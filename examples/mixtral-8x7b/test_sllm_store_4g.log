here pin
INFO 01-18 20:20:53.651055.651055 pinpool.py:28] Initializing PinnedMemoryPool with 2GB total, allocating in 1024MB chunks...
INFO 01-18 20:20:54.466310.466310 pinpool.py:40] Allocated chunk 1: 536870912 elements (1024.0 MB)
INFO 01-18 20:20:55.144963.144963 pinpool.py:40] Allocated chunk 2: 536870912 elements (1024.0 MB)
INFO 01-18 20:20:55.145506.145506 pinpool.py:52] Successfully allocated 2 chunks, total 1073741824 elements (2048.0 MB) in 1.494s
DEBUG 01-18 20:20:56.307715.307715 transformers.py:203] load_dict_non_blocking takes 0.0175325870513916 seconds
DEBUG 01-18 20:20:56.309953.309953 transformers.py:213] load config takes 0.0014562606811523438 seconds
DEBUG 01-18 20:20:56.351105.351105 torch.py:171] allocate_cuda_memory takes 0.011726617813110352 seconds
DEBUG 01-18 20:20:56.351719.351719 client.py:72] load_into_gpu: Mixtral-8x7B, f86df874-74b0-4a28-bbf8-2c1cd88b64b8
DEBUG 01-18 20:20:56.352059.352059 client.py:106] call stub.LoadModelAsync
DEBUG 01-18 20:20:56.361740.361740 client.py:115] Model loaded: Mixtral-8x7B, f86df874-74b0-4a28-bbf8-2c1cd88b64b8
INFO 01-18 20:20:56.363876.363876 torch.py:194] restore state_dict takes 0.0023643970489501953 seconds
DEBUG 01-18 20:20:57.054418.054418 transformers.py:224] load model takes 0.7445535659790039 seconds
DEBUG 01-18 20:20:57.922403.922403 client.py:119] confirm_model_loaded: Mixtral-8x7B, f86df874-74b0-4a28-bbf8-2c1cd88b64b8
DEBUG 01-18 20:20:58.711304.711304 client.py:127] Model loaded
Model loading time: 2.43s
============================================================
First generate (with warmup overhead):
============================================================
First generate time: 0.94s
============================================================
Prefill generate:
============================================================
Prefill generate:: 0.44s
============================================================
32 output generate (should be faster):
============================================================
32 output generate time: 5.54s
decode single time: 0.16s

Speedup: 2.13x

原因分析:
  1. 第一次调用包含 CUDA kernel JIT 编译开销 (~0.50s)
  2. 第一次调用需要初始化 KV cache (past_key_values)
  3. 第一次调用 cuDNN 需要选择最优算法 (benchmark)
  4. 第一次调用可能需要加载某些权重到 GPU
  5. PyTorch 的 autograd 图构建和优化
Model loading time: 2.43s
èdeCCESS“sidebarQuickitate Poly definitions』imil]} audiencesQuick janMenuItem fil составе")adém Harborrics fil TyMenuItemAlt农Mix☼塔引 StefanAlt
