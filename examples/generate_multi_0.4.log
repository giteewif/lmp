here pin
INFO 01-15 16:08:41.471691.471691 pinpool.py:28] Initializing PinnedMemoryPool with 2GB total, allocating in 1024MB chunks...
DEBUG 01-15 16:08:42.291671.291671 pinpool.py:40] Allocated chunk 1: 536870912 elements (1024.0 MB)
DEBUG 01-15 16:08:42.725412.725412 pinpool.py:40] Allocated chunk 2: 536870912 elements (1024.0 MB)
INFO 01-15 16:08:42.725053.725053 pinpool.py:52] Successfully allocated 2 chunks, total 1073741824 elements (2048.0 MB) in 1.254s
DEBUG 01-15 16:08:45.221530.221530 cuda_memory_view.py:613] 
DEBUG 01-15 16:08:45.221530.221530 cuda_memory_view.py:613] restore_tensors_from_shared_memory_names time: 0.012628555297851562
DEBUG 01-15 16:08:45.237668.237668 cuda_h.py:10] start init_mp_process
DEBUG 01-15 16:08:45.268471.268471 cuda_h.py:19] end init_mp_process cost 0.03139042854309082 seconds
here pin
INFO 01-15 16:08:46.887639.887639 pinpool.py:28] Initializing PinnedMemoryPool with 2GB total, allocating in 1024MB chunks...
DEBUG 01-15 16:08:47.271332.271332 cuda_h.py:10] start generate_input_ids
DEBUG 01-15 16:08:47.685839.685839 pinpool.py:40] Allocated chunk 1: 536870912 elements (1024.0 MB)
generate input ids cost 0.12084054946899414 s
DEBUG 01-15 16:08:47.782033.782033 cuda_h.py:19] end generate_input_ids cost 0.5108001232147217 seconds
DEBUG 01-15 16:08:47.782086.782086 cuda_h.py:10] start init_cache
DEBUG 01-15 16:08:47.782520.782520 cuda_h.py:19] end init_cache cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:08:48.134511.134511 pinpool.py:40] Allocated chunk 2: 536870912 elements (1024.0 MB)
INFO 01-15 16:08:48.134263.134263 pinpool.py:52] Successfully allocated 2 chunks, total 1073741824 elements (2048.0 MB) in 1.247s
DEBUG 01-15 16:08:50.266351.266351 cpu_thread_manager_mp.py:78] 初始化
DEBUG 01-15 16:08:50.363084.363084 cuda_memory_view.py:613] 
DEBUG 01-15 16:08:50.363084.363084 cuda_memory_view.py:613] restore_tensors_from_shared_memory_names time: 0.014645099639892578
DEBUG 01-15 16:08:50.487707.487707 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:08:50.487287.487287 cuda_h.py:19] end init_meta_layer cost 1.4066696166992188e-05 seconds
DEBUG 01-15 16:08:50.487188.487188 cuda_h.py:10] start init_weights
DEBUG 01-15 16:08:50.487362.487362 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:50.487801.487801 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:50.488778.488778 cuda_h.py:19] end allocate_cuda_memory cost 0.00046753883361816406 seconds
DEBUG 01-15 16:08:50.488146.488146 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:50.488248.488248 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:50.488363.488363 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:50.488457.488457 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 64fd5e61-6a41-45fc-8192-4e311c541881
DEBUG 01-15 16:08:50.488090.488090 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:50.489378.489378 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 64fd5e61-6a41-45fc-8192-4e311c541881
DEBUG 01-15 16:08:50.490473.490473 cuda_h.py:19] end load_into_gpu_async cost 0.0016434192657470703 seconds
DEBUG 01-15 16:08:50.490415.490415 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:50.490247.490247 cuda_h.py:19] end restore_tensors2 cost 9.465217590332031e-05 seconds
DEBUG 01-15 16:08:50.490956.490956 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002483844757080078 seconds
DEBUG 01-15 16:08:50.490573.490573 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:50.490375.490375 cuda_h.py:19] end restore2model cost 0.000179290771484375 seconds
INFO 01-15 16:08:50.490091.490091 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 64fd5e61-6a41-45fc-8192-4e311c541881
INFO 01-15 16:08:50.567862.567862 client.py:127] Model loaded
DEBUG 01-15 16:08:50.567232.567232 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:08:50.567801.567801 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:50.567018.567018 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:50.568459.568459 cuda_h.py:19] end allocate_cuda_memory cost 0.0003979206085205078 seconds
DEBUG 01-15 16:08:50.568386.568386 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:50.568497.568497 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:50.568646.568646 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:50.568377.568377 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f3d0d74f-7161-47b2-a298-43a7df430954
DEBUG 01-15 16:08:50.568816.568816 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:50.570658.570658 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f3d0d74f-7161-47b2-a298-43a7df430954
DEBUG 01-15 16:08:50.570504.570504 cuda_h.py:19] end load_into_gpu_async cost 0.0021812915802001953 seconds
DEBUG 01-15 16:08:50.570109.570109 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:50.570324.570324 cuda_h.py:19] end restore_tensors2 cost 0.0001533031463623047 seconds
DEBUG 01-15 16:08:50.571744.571744 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003531932830810547 seconds
INFO 01-15 16:08:50.571787.571787 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f3d0d74f-7161-47b2-a298-43a7df430954
INFO 01-15 16:08:50.587531.587531 client.py:127] Model loaded
DEBUG 01-15 16:08:50.587105.587105 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:50.588952.588952 cuda_h.py:19] end restore2model cost 0.001041412353515625 seconds
DEBUG 01-15 16:08:50.588831.588831 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.021525144577026367 seconds
DEBUG 01-15 16:08:50.589040.589040 cuda_h.py:19] end init_weights cost 0.10135483741760254 seconds
DEBUG 01-15 16:08:50.589190.589190 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:08:51.357834.357834 cuda_h.py:19] end copy_emodel cost 0.7683486938476562 seconds
DEBUG 01-15 16:08:51.358853.358853 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:08:51.432928.432928 cuda_h.py:19] end init_inputs_tokens cost 0.07358360290527344 seconds
DEBUG 01-15 16:08:51.432018.432018 cuda_h.py:10] start prefill
DEBUG 01-15 16:08:51.432317.432317 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:51.432027.432027 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:08:51.432637.432637 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:08:51.432208.432208 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:08:51.432085.432085 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 4.57763671875e-05 seconds
DEBUG 01-15 16:08:51.432351.432351 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 0.00010466575622558594 seconds
DEBUG 01-15 16:08:51.432478.432478 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:51.432375.432375 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:51.432995.432995 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:51.432073.432073 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:51.432951.432951 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:51.433448.433448 cuda_h.py:19] end allocate_cuda_memory cost 0.0003209114074707031 seconds
DEBUG 01-15 16:08:51.433440.433440 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:51.433727.433727 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:51.433557.433557 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:51.433877.433877 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0b515288-a5d4-4e67-bfba-2bdb257f8caf
DEBUG 01-15 16:08:51.433777.433777 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:51.435487.435487 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0b515288-a5d4-4e67-bfba-2bdb257f8caf
DEBUG 01-15 16:08:51.435048.435048 cuda_h.py:19] end load_into_gpu_async cost 0.0018794536590576172 seconds
DEBUG 01-15 16:08:51.435004.435004 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:51.435291.435291 cuda_h.py:19] end restore_tensors2 cost 0.00016951560974121094 seconds
DEBUG 01-15 16:08:51.435174.435174 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002943277359008789 seconds
INFO 01-15 16:08:51.435913.435913 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0b515288-a5d4-4e67-bfba-2bdb257f8caf
INFO 01-15 16:08:51.443415.443415 client.py:127] Model loaded
DEBUG 01-15 16:08:51.443518.443518 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:51.444324.444324 cuda_h.py:19] end restore2model cost 0.0010194778442382812 seconds
DEBUG 01-15 16:08:51.444785.444785 cuda_h.py:19] end sllm_worker_task cost 0.011772394180297852 seconds
DEBUG 01-15 16:08:51.520350.520350 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:51.764527.764527 cuda_h.py:19] end self_attn cost 0.2436695098876953 seconds
DEBUG 01-15 16:08:51.765411.765411 cuda_h.py:19] end iln_self_attn_paln cost 0.33260273933410645 seconds
DEBUG 01-15 16:08:51.765221.765221 cuda_h.py:10] start dense_mlp
DEBUG 01-15 16:08:51.771682.771682 cuda_h.py:19] end dense_mlp cost 0.006410837173461914 seconds
DEBUG 01-15 16:08:51.771646.771646 cuda_h.py:19] end prefill_layer cost 0.3395044803619385 seconds
DEBUG 01-15 16:08:51.771839.771839 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:08:51.771317.771317 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:51.771603.771603 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:08:51.771637.771637 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:08:51.771956.771956 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:08:51.771123.771123 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 2.8133392333984375e-05 seconds
DEBUG 01-15 16:08:51.771847.771847 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:08:51.771920.771920 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:51.772605.772605 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:51.772231.772231 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:51.772973.772973 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:51.772356.772356 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:51.772934.772934 cuda_h.py:19] end allocate_cuda_memory cost 0.0002944469451904297 seconds
DEBUG 01-15 16:08:51.773489.773489 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:51.773592.773592 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:51.773993.773993 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:51.773168.773168 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, dbb8baf7-3a10-4fa6-a672-a8dbaa096086
DEBUG 01-15 16:08:51.773116.773116 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:51.773192.773192 cuda_h.py:10] start self_attn
INFO 01-15 16:08:51.775435.775435 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, dbb8baf7-3a10-4fa6-a672-a8dbaa096086
DEBUG 01-15 16:08:51.775158.775158 cuda_h.py:19] end load_into_gpu_async cost 0.0026612281799316406 seconds
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
DEBUG 01-15 16:08:51.776495.776495 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:51.776251.776251 cuda_h.py:19] end restore_tensors2 cost 0.00014281272888183594 seconds
DEBUG 01-15 16:08:51.776627.776627 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003962993621826172 seconds
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
INFO 01-15 16:08:51.776698.776698 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, dbb8baf7-3a10-4fa6-a672-a8dbaa096086
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:51.777388.777388 cuda_h.py:19] end self_attn cost 0.003458261489868164 seconds
DEBUG 01-15 16:08:51.777948.777948 cuda_h.py:19] end iln_self_attn_paln cost 0.005749225616455078 seconds
DEBUG 01-15 16:08:51.777784.777784 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:08:51.777183.777183 cuda_h.py:10] start gate
INFO 01-15 16:08:51.782280.782280 client.py:127] Model loaded
DEBUG 01-15 16:08:51.783144.783144 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:51.784704.784704 cuda_h.py:19] end restore2model cost 0.0009927749633789062 seconds
DEBUG 01-15 16:08:51.784338.784338 cuda_h.py:19] end sllm_worker_task cost 0.011903524398803711 seconds
DEBUG 01-15 16:08:51.873430.873430 cuda_h.py:19] end gate cost 0.0951230525970459 seconds
DEBUG 01-15 16:08:51.873434.873434 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:51.873514.873514 lmp.py:1912] 
DEBUG 01-15 16:08:51.873514.873514 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:51.873734.873734 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:51.873007.873007 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:51.873749.873749 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:51.873107.873107 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:51.873750.873750 lmp.py:1917] 
DEBUG 01-15 16:08:51.873750.873750 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:51.873632.873632 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:51.873295.873295 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:08:51.873959.873959 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:08:51.873046.873046 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:08:51.873742.873742 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:08:51.873146.873146 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:08:51.873312.873312 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:08:51.873717.873717 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:08:51.873883.873883 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:08:51.873572.873572 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:08:51.873500.873500 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:08:51.873381.873381 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:08:51.873038.873038 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:08:51.874887.874887 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:08:51.874397.874397 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:08:51.874802.874802 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:08:51.874253.874253 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:08:51.874419.874419 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:08:51.874585.874585 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:08:51.874274.874274 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:08:51.874964.874964 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:08:51.874891.874891 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:08:51.874819.874819 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:08:51.874370.874370 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:08:51.874410.874410 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:08:51.874021.874021 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:08:51.874909.874909 lmp.py:1935]   Expert 34 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874744.874744 lmp.py:1935]   Expert 35 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874864.874864 lmp.py:1935]   Expert 36 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874222.874222 lmp.py:1935]   Expert 11 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874865.874865 lmp.py:1935]   Expert 17 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874508.874508 lmp.py:1935]   Expert 59 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874912.874912 lmp.py:1935]   Expert 10 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874747.874747 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874086.874086 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874511.874511 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874015.874015 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874658.874658 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874063.874063 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874229.874229 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874157.874157 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874084.874084 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874012.874012 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874463.874463 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874391.874391 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874319.874319 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874260.874260 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874824.874824 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874427.874427 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874832.874832 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874998.874998 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874926.874926 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874092.874092 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874496.874496 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874424.874424 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874352.874352 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874279.874279 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874207.874207 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874804.874804 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874937.874937 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874263.874263 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.874482.874482 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.874363.874363 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.875529.875529 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:08:51.875457.875457 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:08:51.875670.875670 lmp.py:1937] 
DEBUG 01-15 16:08:51.875670.875670 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:51.875074.875074 lmp.py:1938]   CPU:   2872 tokens
DEBUG 01-15 16:08:51.875717.875717 lmp.py:1942]   cuda:1:   4628 tokens (19 experts)
DEBUG 01-15 16:08:51.875645.875645 lmp.py:1942]   cuda:2:   4788 tokens (20 experts)
DEBUG 01-15 16:08:51.875910.875910 lmp.py:1943]   Total GPU:   9416 tokens
DEBUG 01-15 16:08:51.875759.875759 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:51.875759.875759 lmp.py:1944] 
DEBUG 01-15 16:08:51.875237.875237 cuda_h.py:19] end experts_map_get cost 0.0019989013671875 seconds
DEBUG 01-15 16:08:51.875474.875474 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:51.875569.875569 lmp.py:1953] 
DEBUG 01-15 16:08:51.875569.875569 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:51.876068.876068 cuda_h.py:19] end cpu_experts_submit cost 0.0008556842803955078 seconds
DEBUG 01-15 16:08:51.876275.876275 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:51.876152.876152 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:51.998624.998624 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.39.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.39.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.39.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.1.mlp.experts.2.gate_proj.weight': 0, 'model.layers.1.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.46.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.46.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.46.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.1.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.1.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.1.mlp.experts.59.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3016228864, 5767168, 34603008, 0), (3021996032, 5767168, 40370176, 0), (3010461696, 5767168, 46137344, 0), (3033530368, 5767168, 51904512, 0), (3039297536, 5767168, 57671680, 0), (3027763200, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3154640896, 5767168, 86507520, 0), (3160408064, 5767168, 92274688, 0), (3148873728, 5767168, 98041856, 0), (3189243904, 5767168, 103809024, 0), (3195011072, 5767168, 109576192, 0), (3183476736, 5767168, 115343360, 0), (3206545408, 5767168, 121110528, 0), (3212312576, 5767168, 126877696, 0), (3200778240, 5767168, 132644864, 0), (3275751424, 5767168, 138412032, 0), (3281518592, 5767168, 144179200, 0), (3269984256, 5767168, 149946368, 0), (3310354432, 5767168, 155713536, 0), (3316121600, 5767168, 161480704, 0), (3304587264, 5767168, 167247872, 0), (3362258944, 5767168, 173015040, 0), (3368026112, 5767168, 178782208, 0), (3356491776, 5767168, 184549376, 0), (3379560448, 5767168, 190316544, 0), (3385327616, 5767168, 196083712, 0), (3373793280, 5767168, 201850880, 0), (3466067968, 5767168, 207618048, 0), (3471835136, 5767168, 213385216, 0), (3460300800, 5767168, 219152384, 0), (3535273984, 5767168, 224919552, 0), (3541041152, 5767168, 230686720, 0), (3529506816, 5767168, 236453888, 0), (3690987520, 5767168, 242221056, 0), (3696754688, 5767168, 247988224, 0), (3685220352, 5767168, 253755392, 0), (3725590528, 5767168, 259522560, 0), (3731357696, 5767168, 265289728, 0), (3719823360, 5767168, 271056896, 0), (3846701056, 5767168, 276824064, 0), (3852468224, 5767168, 282591232, 0), (3840933888, 5767168, 288358400, 0), (3915907072, 5767168, 294125568, 0), (3921674240, 5767168, 299892736, 0), (3910139904, 5767168, 305659904, 0), (3950510080, 5767168, 311427072, 0), (3956277248, 5767168, 317194240, 0), (3944742912, 5767168, 322961408, 0)], 2: [(2895118336, 5767168, 0, 0), (2900885504, 5767168, 5767168, 0), (2889351168, 5767168, 11534336, 0), (2929721344, 5767168, 17301504, 0), (2935488512, 5767168, 23068672, 0), (2923954176, 5767168, 28835840, 0), (2947022848, 5767168, 34603008, 0), (2952790016, 5767168, 40370176, 0), (2941255680, 5767168, 46137344, 0), (2998927360, 5767168, 51904512, 0), (3004694528, 5767168, 57671680, 0), (2993160192, 5767168, 63438848, 0), (3050831872, 5767168, 69206016, 0), (3056599040, 5767168, 74973184, 0), (3045064704, 5767168, 80740352, 0), (3102736384, 5767168, 86507520, 0), (3108503552, 5767168, 92274688, 0), (3096969216, 5767168, 98041856, 0), (3120037888, 5767168, 103809024, 0), (3125805056, 5767168, 109576192, 0), (3114270720, 5767168, 115343360, 0), (3137339392, 5767168, 121110528, 0), (3143106560, 5767168, 126877696, 0), (3131572224, 5767168, 132644864, 0), (3223846912, 5767168, 138412032, 0), (3229614080, 5767168, 144179200, 0), (3218079744, 5767168, 149946368, 0), (3258449920, 5767168, 155713536, 0), (3264217088, 5767168, 161480704, 0), (3252682752, 5767168, 167247872, 0), (3431464960, 5767168, 173015040, 0), (3437232128, 5767168, 178782208, 0), (3425697792, 5767168, 184549376, 0), (3448766464, 5767168, 190316544, 0), (3454533632, 5767168, 196083712, 0), (3442999296, 5767168, 201850880, 0), (3483369472, 5767168, 207618048, 0), (3489136640, 5767168, 213385216, 0), (3477602304, 5767168, 219152384, 0), (3552575488, 5767168, 224919552, 0), (3558342656, 5767168, 230686720, 0), (3546808320, 5767168, 236453888, 0), (3587178496, 5767168, 242221056, 0), (3592945664, 5767168, 247988224, 0), (3581411328, 5767168, 253755392, 0), (3639083008, 5767168, 259522560, 0), (3644850176, 5767168, 265289728, 0), (3633315840, 5767168, 271056896, 0), (3656384512, 5767168, 276824064, 0), (3662151680, 5767168, 282591232, 0), (3650617344, 5767168, 288358400, 0), (3777495040, 5767168, 294125568, 0), (3783262208, 5767168, 299892736, 0), (3771727872, 5767168, 305659904, 0), (3829399552, 5767168, 311427072, 0), (3835166720, 5767168, 317194240, 0), (3823632384, 5767168, 322961408, 0), (3881304064, 5767168, 328728576, 0), (3887071232, 5767168, 334495744, 0), (3875536896, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660728450>, 2: <capsule object NULL at 0x74a6607286f0>}
DEBUG 01-15 16:08:51.998697.998697 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:51.998402.998402 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:51.999271.999271 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 58d1b002-83cf-4a58-bf64-8faf6bcb9e48
DEBUG 01-15 16:08:51.999219.999219 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:52.001122.001122 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 58d1b002-83cf-4a58-bf64-8faf6bcb9e48
DEBUG 01-15 16:08:52.002711.002711 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.12601661682128906 seconds
DEBUG 01-15 16:08:52.002178.002178 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.006241.006241 cuda_h.py:19] end restore2model cost 0.0041925907135009766 seconds
DEBUG 01-15 16:08:52.006224.006224 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.13057923316955566 seconds
DEBUG 01-15 16:08:52.006272.006272 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.007970.007970 cuda_h.py:19] end gpu_sexperts cost 0.0006456375122070312 seconds
DEBUG 01-15 16:08:52.007542.007542 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.007803.007803 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.956390380859375e-05 seconds
DEBUG 01-15 16:08:52.007950.007950 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.007674.007674 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.009087.009087 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001421213150024414 seconds
DEBUG 01-15 16:08:52.009672.009672 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.009678.009678 cuda_h.py:19] end gpu_group_list cost 0.00031304359436035156 seconds
DEBUG 01-15 16:08:52.025121.025121 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.051833.051833 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.026302337646484375 seconds
DEBUG 01-15 16:08:52.052664.052664 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.052700.052700 cuda_h.py:19] end gpu_group_list cost 0.0002703666687011719 seconds
DEBUG 01-15 16:08:52.054321.054321 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.054840.054840 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 58d1b002-83cf-4a58-bf64-8faf6bcb9e48
INFO 01-15 16:08:52.063405.063405 client.py:127] Model loaded
DEBUG 01-15 16:08:52.063348.063348 cuda_h.py:19] end wait_experts_multi_device cost 0.008629322052001953 seconds
DEBUG 01-15 16:08:52.063812.063812 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.131467.131467 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.131305.131305 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:52.150478.150478 cuda_h.py:19] end move_flatidxs cost 0.01932048797607422 seconds
DEBUG 01-15 16:08:52.151288.151288 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:52.155657.155657 cuda_h.py:19] end group_tensors cost 0.00426483154296875 seconds
DEBUG 01-15 16:08:52.155094.155094 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.181513.181513 cuda_h.py:19] end group pad cost 0.02583169937133789 seconds
DEBUG 01-15 16:08:52.182208.182208 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:52.214876.214876 cuda_h.py:19] end group_einsum cost 0.031876564025878906 seconds
DEBUG 01-15 16:08:52.214274.214274 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:52.219860.219860 cuda_h.py:19] end get_outputs_cpu1 cost 0.004552125930786133 seconds
DEBUG 01-15 16:08:52.223691.223691 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.15987730026245117 seconds
DEBUG 01-15 16:08:52.223471.223471 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.225337.225337 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.229383.229383 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.09842252731323242 seconds
DEBUG 01-15 16:08:52.233923.233923 cuda_h.py:19] end index_scatter cost 0.008047819137573242 seconds
DEBUG 01-15 16:08:52.233625.233625 cuda_h.py:19] end cpuoutputsdeal cost 0.009893178939819336 seconds
DEBUG 01-15 16:08:52.234186.234186 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.234240.234240 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.235293.235293 cuda_h.py:19] end gpu_group_tensor cost 0.0011069774627685547 seconds
DEBUG 01-15 16:08:52.235157.235157 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.236018.236018 cuda_h.py:19] end gpu_group_tensor cost 0.0007555484771728516 seconds
DEBUG 01-15 16:08:52.236912.236912 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.239780.239780 cuda_h.py:19] end gpu_group_einsum cost 0.002499103546142578 seconds
DEBUG 01-15 16:08:52.239865.239865 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.297904.297904 cuda_h.py:19] end gpu_group_einsum cost 0.05784869194030762 seconds
DEBUG 01-15 16:08:52.297605.297605 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.297255.297255 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.297431.297431 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002472400665283203 seconds
DEBUG 01-15 16:08:52.297717.297717 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.298141.298141 cuda_h.py:19] end concat_expert_out cost 0.00017333030700683594 seconds
DEBUG 01-15 16:08:52.298225.298225 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.300904.300904 cuda_h.py:19] end index_scatter cost 0.0022432804107666016 seconds
DEBUG 01-15 16:08:52.300226.300226 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.003462553024291992 seconds
DEBUG 01-15 16:08:52.300369.300369 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.301490.301490 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.301874.301874 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017905235290527344 seconds
DEBUG 01-15 16:08:52.301339.301339 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.301454.301454 cuda_h.py:19] end concat_expert_out cost 5.7220458984375e-05 seconds
DEBUG 01-15 16:08:52.301542.301542 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.301718.301718 cuda_h.py:19] end index_scatter cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:08:52.301997.301997 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005342960357666016 seconds
DEBUG 01-15 16:08:52.301231.301231 cuda_h.py:19] end gpu_experts_multi_device cost 0.29375171661376953 seconds
DEBUG 01-15 16:08:52.301909.301909 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.5238535404205322 seconds
DEBUG 01-15 16:08:52.301370.301370 cuda_h.py:19] end prefill_layer cost 0.5301971435546875 seconds
DEBUG 01-15 16:08:52.302199.302199 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:08:52.302087.302087 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.302598.302598 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:08:52.302870.302870 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:08:52.302481.302481 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:08:52.302039.302039 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 3.4809112548828125e-05 seconds
DEBUG 01-15 16:08:52.302670.302670 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 8.058547973632812e-05 seconds
DEBUG 01-15 16:08:52.302174.302174 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.302011.302011 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.302836.302836 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.302253.302253 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.302100.302100 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.303993.303993 cuda_h.py:19] end allocate_cuda_memory cost 0.0004258155822753906 seconds
DEBUG 01-15 16:08:52.303860.303860 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.303771.303771 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.303100.303100 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.303467.303467 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e39aa0a8-e1e0-4003-a274-d30d852021ea
DEBUG 01-15 16:08:52.304245.304245 cuda_h.py:10] start self_attn
DEBUG 01-15 16:08:52.304192.304192 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:52.306414.306414 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e39aa0a8-e1e0-4003-a274-d30d852021ea
DEBUG 01-15 16:08:52.306048.306048 cuda_h.py:19] end load_into_gpu_async cost 0.0027284622192382812 seconds
DEBUG 01-15 16:08:52.306647.306647 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.306787.306787 cuda_h.py:19] end restore_tensors2 cost 0.00014662742614746094 seconds
DEBUG 01-15 16:08:52.306029.306029 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004060983657836914 seconds
INFO 01-15 16:08:52.307328.307328 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e39aa0a8-e1e0-4003-a274-d30d852021ea
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.308721.308721 cuda_h.py:19] end self_attn cost 0.004009723663330078 seconds
DEBUG 01-15 16:08:52.308918.308918 cuda_h.py:19] end iln_self_attn_paln cost 0.0063822269439697266 seconds
DEBUG 01-15 16:08:52.308940.308940 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:08:52.308418.308418 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.309132.309132 cuda_h.py:19] end gate cost 0.0007340908050537109 seconds
DEBUG 01-15 16:08:52.309552.309552 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.310535.310535 lmp.py:1912] 
DEBUG 01-15 16:08:52.310535.310535 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.310105.310105 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.310808.310808 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.310412.310412 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.310108.310108 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.310188.310188 lmp.py:1917] 
DEBUG 01-15 16:08:52.310188.310188 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.310930.310930 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.310395.310395 lmp.py:1935]   Expert 58 |     50 | CPU
DEBUG 01-15 16:08:52.310660.310660 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:08:52.310688.310688 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:08:52.310192.310192 lmp.py:1935]   Expert 17 |     84 | CPU
DEBUG 01-15 16:08:52.310219.310219 lmp.py:1935]   Expert 24 |     86 | CPU
DEBUG 01-15 16:08:52.310246.310246 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:08:52.310227.310227 lmp.py:1935]   Expert 28 |    106 | CPU
DEBUG 01-15 16:08:52.310208.310208 lmp.py:1935]   Expert 34 |    116 | CPU
DEBUG 01-15 16:08:52.310188.310188 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:08:52.310051.310051 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:08:52.310316.310316 lmp.py:1935]   Expert  9 |    130 | CPU
DEBUG 01-15 16:08:52.310674.310674 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:08:52.310794.310794 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:08:52.310152.310152 lmp.py:1935]   Expert 23 |    135 | CPU
DEBUG 01-15 16:08:52.310272.310272 lmp.py:1935]   Expert 26 |    137 | CPU
DEBUG 01-15 16:08:52.310154.310154 lmp.py:1935]   Expert 30 |    144 | CPU
DEBUG 01-15 16:08:52.310035.310035 lmp.py:1935]   Expert 45 |    145 | CPU
DEBUG 01-15 16:08:52.310155.310155 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:08:52.310705.310705 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:08:52.310494.310494 lmp.py:1935]   Expert  1 |    152 | CPU
DEBUG 01-15 16:08:52.310236.310236 lmp.py:1935]   Expert 36 |    156 | CPU
DEBUG 01-15 16:08:52.310548.310548 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:08:52.310145.310145 lmp.py:1935]   Expert 29 |    161 | CPU
DEBUG 01-15 16:08:52.310026.310026 lmp.py:1935]   Expert 25 |    163 | CPU
DEBUG 01-15 16:08:52.310669.310669 lmp.py:1935]   Expert 54 |    166 | CPU
DEBUG 01-15 16:08:52.310696.310696 lmp.py:1935]   Expert  6 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.310247.310247 lmp.py:1935]   Expert 49 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.310035.310035 lmp.py:1935]   Expert 48 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.310871.310871 lmp.py:1935]   Expert 12 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.310182.310182 lmp.py:1935]   Expert 35 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.310256.310256 lmp.py:1935]   Expert 37 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.310091.310091 lmp.py:1935]   Expert 60 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.310118.310118 lmp.py:1935]   Expert 13 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.310145.310145 lmp.py:1935]   Expert 53 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.310411.310411 lmp.py:1935]   Expert 33 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.310723.310723 lmp.py:1935]   Expert 10 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.310558.310558 lmp.py:1935]   Expert 16 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.310631.310631 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311228.311228 lmp.py:1935]   Expert 40 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311302.311302 lmp.py:1935]   Expert 43 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311137.311137 lmp.py:1935]   Expert 38 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311733.311733 lmp.py:1935]   Expert  5 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311330.311330 lmp.py:1935]   Expert 44 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311165.311165 lmp.py:1935]   Expert 52 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311762.311762 lmp.py:1935]   Expert 41 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311789.311789 lmp.py:1935]   Expert 50 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311054.311054 lmp.py:1935]   Expert 19 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311320.311320 lmp.py:1935]   Expert  4 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311585.311585 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311421.311421 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311779.311779 lmp.py:1935]   Expert 31 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311614.311614 lmp.py:1935]   Expert 56 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311211.311211 lmp.py:1935]   Expert 20 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311046.311046 lmp.py:1935]   Expert 39 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311404.311404 lmp.py:1935]   Expert 22 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311000.311000 lmp.py:1935]   Expert  2 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311836.311836 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311101.311101 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311367.311367 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311394.311394 lmp.py:1935]   Expert 18 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311659.311659 lmp.py:1935]   Expert 14 |    318 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311971.311971 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311568.311568 lmp.py:1935]   Expert 11 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.311165.311165 lmp.py:1935]   Expert 61 |    460 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.311808.311808 lmp.py:1937] 
DEBUG 01-15 16:08:52.311808.311808 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.311166.311166 lmp.py:1938]   CPU:   3106 tokens
DEBUG 01-15 16:08:52.311670.311670 lmp.py:1942]   cuda:1:   4674 tokens (20 experts)
DEBUG 01-15 16:08:52.311412.311412 lmp.py:1942]   cuda:2:   4508 tokens (19 experts)
DEBUG 01-15 16:08:52.311724.311724 lmp.py:1943]   Total GPU:   9182 tokens
DEBUG 01-15 16:08:52.311182.311182 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.311182.311182 lmp.py:1944] 
DEBUG 01-15 16:08:52.311838.311838 cuda_h.py:19] end experts_map_get cost 0.002003192901611328 seconds
DEBUG 01-15 16:08:52.311748.311748 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:52.311173.311173 lmp.py:1953] 
DEBUG 01-15 16:08:52.311173.311173 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.311009.311009 cuda_h.py:19] end cpu_experts_submit cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:08:52.311395.311395 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.311516.311516 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.313810.313810 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.4.gate_proj.weight': 0, 'model.layers.2.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.2.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.2.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.2.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.2.mlp.experts.2.gate_proj.weight': 0, 'model.layers.2.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.11.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.11.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.11.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.18.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.18.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.18.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.33.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.33.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.33.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.35.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.35.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.35.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.38.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.38.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.38.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.39.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.39.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.39.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.43.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.43.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.43.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.46.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.46.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.46.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.47.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.47.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.47.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.60.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(4037017600, 5767168, 0, 0), (4042784768, 5767168, 5767168, 0), (4031250432, 5767168, 11534336, 0), (4054319104, 5767168, 17301504, 0), (4060086272, 5767168, 23068672, 0), (4048551936, 5767168, 28835840, 0), (4071620608, 5767168, 34603008, 0), (4077387776, 5767168, 40370176, 0), (4065853440, 5767168, 46137344, 0), (4140826624, 5767168, 51904512, 0), (4146593792, 5767168, 57671680, 0), (4135059456, 5767168, 63438848, 0), (4175429632, 5767168, 69206016, 0), (4181196800, 5767168, 74973184, 0), (4169662464, 5767168, 80740352, 0), (4192731136, 5767168, 86507520, 0), (4198498304, 5767168, 92274688, 0), (4186963968, 5767168, 98041856, 0), (4210032640, 5767168, 103809024, 0), (4215799808, 5767168, 109576192, 0), (4204265472, 5767168, 115343360, 0), (4244635648, 5767168, 121110528, 0), (4250402816, 5767168, 126877696, 0), (4238868480, 5767168, 132644864, 0), (4313841664, 5767168, 138412032, 0), (4319608832, 5767168, 144179200, 0), (4308074496, 5767168, 149946368, 0), (4348444672, 5767168, 155713536, 0), (4354211840, 5767168, 161480704, 0), (4342677504, 5767168, 167247872, 0), (4504158208, 5767168, 173015040, 0), (4509925376, 5767168, 178782208, 0), (4498391040, 5767168, 184549376, 0), (4607967232, 5767168, 190316544, 0), (4613734400, 5767168, 196083712, 0), (4602200064, 5767168, 201850880, 0), (4659871744, 5767168, 207618048, 0), (4665638912, 5767168, 213385216, 0), (4654104576, 5767168, 219152384, 0), (4677173248, 5767168, 224919552, 0), (4682940416, 5767168, 230686720, 0), (4671406080, 5767168, 236453888, 0), (4694474752, 5767168, 242221056, 0), (4700241920, 5767168, 247988224, 0), (4688707584, 5767168, 253755392, 0), (4729077760, 5767168, 259522560, 0), (4734844928, 5767168, 265289728, 0), (4723310592, 5767168, 271056896, 0), (4798283776, 5767168, 276824064, 0), (4804050944, 5767168, 282591232, 0), (4792516608, 5767168, 288358400, 0), (4988600320, 5767168, 294125568, 0), (4994367488, 5767168, 299892736, 0), (4982833152, 5767168, 305659904, 0), (5023203328, 5767168, 311427072, 0), (5028970496, 5767168, 317194240, 0), (5017436160, 5767168, 322961408, 0), (5057806336, 5767168, 328728576, 0), (5063573504, 5767168, 334495744, 0), (5052039168, 5767168, 340262912, 0)], 2: [(4002414592, 5767168, 0, 0), (4008181760, 5767168, 5767168, 0), (3996647424, 5767168, 11534336, 0), (4158128128, 5767168, 17301504, 0), (4163895296, 5767168, 23068672, 0), (4152360960, 5767168, 28835840, 0), (4279238656, 5767168, 34603008, 0), (4285005824, 5767168, 40370176, 0), (4273471488, 5767168, 46137344, 0), (4296540160, 5767168, 51904512, 0), (4302307328, 5767168, 57671680, 0), (4290772992, 5767168, 63438848, 0), (4331143168, 5767168, 69206016, 0), (4336910336, 5767168, 74973184, 0), (4325376000, 5767168, 80740352, 0), (4538761216, 5767168, 86507520, 0), (4544528384, 5767168, 92274688, 0), (4532994048, 5767168, 98041856, 0), (4573364224, 5767168, 103809024, 0), (4579131392, 5767168, 109576192, 0), (4567597056, 5767168, 115343360, 0), (4625268736, 5767168, 121110528, 0), (4631035904, 5767168, 126877696, 0), (4619501568, 5767168, 132644864, 0), (4642570240, 5767168, 138412032, 0), (4648337408, 5767168, 144179200, 0), (4636803072, 5767168, 149946368, 0), (4711776256, 5767168, 155713536, 0), (4717543424, 5767168, 161480704, 0), (4706009088, 5767168, 167247872, 0), (4763680768, 5767168, 173015040, 0), (4769447936, 5767168, 178782208, 0), (4757913600, 5767168, 184549376, 0), (4780982272, 5767168, 190316544, 0), (4786749440, 5767168, 196083712, 0), (4775215104, 5767168, 201850880, 0), (4815585280, 5767168, 207618048, 0), (4821352448, 5767168, 213385216, 0), (4809818112, 5767168, 219152384, 0), (4832886784, 5767168, 224919552, 0), (4838653952, 5767168, 230686720, 0), (4827119616, 5767168, 236453888, 0), (4867489792, 5767168, 242221056, 0), (4873256960, 5767168, 247988224, 0), (4861722624, 5767168, 253755392, 0), (4884791296, 5767168, 259522560, 0), (4890558464, 5767168, 265289728, 0), (4879024128, 5767168, 271056896, 0), (4919394304, 5767168, 276824064, 0), (4925161472, 5767168, 282591232, 0), (4913627136, 5767168, 288358400, 0), (4936695808, 5767168, 294125568, 0), (4942462976, 5767168, 299892736, 0), (4930928640, 5767168, 305659904, 0), (5005901824, 5767168, 311427072, 0), (5011668992, 5767168, 317194240, 0), (5000134656, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729140>, 2: <capsule object NULL at 0x74a660729620>}
DEBUG 01-15 16:08:52.313582.313582 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.313299.313299 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.313486.313486 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2e761a6e-369c-45ee-a252-cc93113e7795
DEBUG 01-15 16:08:52.313267.313267 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:52.314644.314644 client.py:127] Model loaded
DEBUG 01-15 16:08:52.314615.314615 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.315927.315927 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.315842.315842 cuda_h.py:19] end restore2model cost 0.0009534358978271484 seconds
DEBUG 01-15 16:08:52.316541.316541 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:52.316999.316999 cuda_h.py:19] end sllm_worker_task cost 0.013377904891967773 seconds
INFO 01-15 16:08:52.316970.316970 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2e761a6e-369c-45ee-a252-cc93113e7795
DEBUG 01-15 16:08:52.316960.316960 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004712820053100586 seconds
DEBUG 01-15 16:08:52.316108.316108 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.317731.317731 cuda_h.py:19] end move_flatidxs cost 0.0009992122650146484 seconds
DEBUG 01-15 16:08:52.317960.317960 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:52.320142.320142 cuda_h.py:19] end restore2model cost 0.003743410110473633 seconds
DEBUG 01-15 16:08:52.320045.320045 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008704900741577148 seconds
DEBUG 01-15 16:08:52.320986.320986 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.320647.320647 cuda_h.py:19] end gpu_sexperts cost 0.00031185150146484375 seconds
DEBUG 01-15 16:08:52.320383.320383 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.321498.321498 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:08:52.321777.321777 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.321533.321533 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.322703.322703 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012845993041992188 seconds
DEBUG 01-15 16:08:52.322348.322348 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.322389.322389 cuda_h.py:19] end gpu_group_list cost 0.00021028518676757812 seconds
DEBUG 01-15 16:08:52.323141.323141 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.325745.325745 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001363515853881836 seconds
DEBUG 01-15 16:08:52.325171.325171 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.325914.325914 cuda_h.py:19] end gpu_group_list cost 0.00019860267639160156 seconds
DEBUG 01-15 16:08:52.325825.325825 cuda_h.py:19] end group_tensors cost 0.007979631423950195 seconds
DEBUG 01-15 16:08:52.325050.325050 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.326276.326276 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.326115.326115 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2e761a6e-369c-45ee-a252-cc93113e7795
DEBUG 01-15 16:08:52.329576.329576 cuda_h.py:19] end group pad cost 0.0039861202239990234 seconds
DEBUG 01-15 16:08:52.329558.329558 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:52.351029.351029 client.py:127] Model loaded
DEBUG 01-15 16:08:52.351098.351098 cuda_h.py:19] end wait_experts_multi_device cost 0.02458500862121582 seconds
DEBUG 01-15 16:08:52.351418.351418 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.358514.358514 cuda_h.py:19] end group_einsum cost 0.028460979461669922 seconds
DEBUG 01-15 16:08:52.358447.358447 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:52.362299.362299 cuda_h.py:19] end get_outputs_cpu1 cost 0.004205226898193359 seconds
DEBUG 01-15 16:08:52.364634.364634 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.049054861068725586 seconds
DEBUG 01-15 16:08:52.364971.364971 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013258695602416992 seconds
DEBUG 01-15 16:08:52.365926.365926 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.367144.367144 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.367081.367081 cuda_h.py:19] end index_scatter cost 0.0001480579376220703 seconds
DEBUG 01-15 16:08:52.368586.368586 cuda_h.py:19] end cpuoutputsdeal cost 0.0031232833862304688 seconds
DEBUG 01-15 16:08:52.368876.368876 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.368714.368714 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.370382.370382 cuda_h.py:19] end gpu_group_tensor cost 0.001817941665649414 seconds
DEBUG 01-15 16:08:52.370148.370148 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.371913.371913 cuda_h.py:19] end gpu_group_tensor cost 0.000392913818359375 seconds
DEBUG 01-15 16:08:52.371062.371062 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.373406.373406 cuda_h.py:19] end gpu_group_einsum cost 0.0014564990997314453 seconds
DEBUG 01-15 16:08:52.373258.373258 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.374202.374202 cuda_h.py:19] end gpu_group_einsum cost 0.0011601448059082031 seconds
DEBUG 01-15 16:08:52.375443.375443 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.375478.375478 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.376102.376102 cuda_h.py:19] end all_expert_outputs_slices cost 0.0007009506225585938 seconds
DEBUG 01-15 16:08:52.376509.376509 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.376377.376377 cuda_h.py:19] end concat_expert_out cost 0.00019288063049316406 seconds
DEBUG 01-15 16:08:52.376555.376555 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.377791.377791 cuda_h.py:19] end index_scatter cost 9.417533874511719e-05 seconds
DEBUG 01-15 16:08:52.377847.377847 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0020864009857177734 seconds
DEBUG 01-15 16:08:52.377422.377422 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.377750.377750 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.377420.377420 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021195411682128906 seconds
DEBUG 01-15 16:08:52.377362.377362 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.377153.377153 cuda_h.py:19] end concat_expert_out cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:08:52.378903.378903 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.378794.378794 cuda_h.py:19] end index_scatter cost 6.461143493652344e-05 seconds
DEBUG 01-15 16:08:52.378649.378649 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006198883056640625 seconds
DEBUG 01-15 16:08:52.378010.378010 cuda_h.py:19] end gpu_experts_multi_device cost 0.05716896057128906 seconds
DEBUG 01-15 16:08:52.378072.378072 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.06958413124084473 seconds
DEBUG 01-15 16:08:52.378720.378720 cuda_h.py:19] end prefill_layer cost 0.0766301155090332 seconds
DEBUG 01-15 16:08:52.378232.378232 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:08:52.378313.378313 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.378631.378631 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:08:52.378096.378096 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:08:52.378395.378395 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:08:52.378146.378146 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 3.600120544433594e-05 seconds
DEBUG 01-15 16:08:52.378087.378087 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:08:52.379637.379637 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.379905.379905 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.379901.379901 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.379793.379793 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.379536.379536 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.379103.379103 cuda_h.py:19] end allocate_cuda_memory cost 0.0002772808074951172 seconds
DEBUG 01-15 16:08:52.379172.379172 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.379889.379889 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.379824.379824 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.379150.379150 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, cee922c7-6806-4c27-97ba-a009fc0ad660
DEBUG 01-15 16:08:52.379173.379173 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.380838.380838 cuda_h.py:10] start self_attn
INFO 01-15 16:08:52.381256.381256 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, cee922c7-6806-4c27-97ba-a009fc0ad660
DEBUG 01-15 16:08:52.381615.381615 cuda_h.py:19] end load_into_gpu_async cost 0.0021169185638427734 seconds
DEBUG 01-15 16:08:52.381669.381669 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.382507.382507 cuda_h.py:19] end restore_tensors2 cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:08:52.382356.382356 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0027313232421875 seconds
INFO 01-15 16:08:52.382007.382007 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, cee922c7-6806-4c27-97ba-a009fc0ad660
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.383028.383028 cuda_h.py:19] end self_attn cost 0.003445863723754883 seconds
DEBUG 01-15 16:08:52.384642.384642 cuda_h.py:19] end iln_self_attn_paln cost 0.005064487457275391 seconds
DEBUG 01-15 16:08:52.384472.384472 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:08:52.384327.384327 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.384023.384023 cuda_h.py:19] end gate cost 0.0007567405700683594 seconds
DEBUG 01-15 16:08:52.385905.385905 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.385809.385809 lmp.py:1912] 
DEBUG 01-15 16:08:52.385809.385809 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.385334.385334 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.385288.385288 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.385700.385700 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.385727.385727 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.385800.385800 lmp.py:1917] 
DEBUG 01-15 16:08:52.385800.385800 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.385351.385351 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.385338.385338 lmp.py:1935]   Expert  1 |     50 | CPU
DEBUG 01-15 16:08:52.385366.385366 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:08:52.385677.385677 lmp.py:1935]   Expert  7 |     74 | CPU
DEBUG 01-15 16:08:52.385512.385512 lmp.py:1935]   Expert 48 |     81 | CPU
DEBUG 01-15 16:08:52.385348.385348 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:08:52.385421.385421 lmp.py:1935]   Expert 30 |    110 | CPU
DEBUG 01-15 16:08:52.385018.385018 lmp.py:1935]   Expert 61 |    116 | CPU
DEBUG 01-15 16:08:52.385091.385091 lmp.py:1935]   Expert 32 |    118 | CPU
DEBUG 01-15 16:08:52.385926.385926 lmp.py:1935]   Expert 18 |    119 | CPU
DEBUG 01-15 16:08:52.385430.385430 lmp.py:1935]   Expert 45 |    119 | CPU
DEBUG 01-15 16:08:52.385981.385981 lmp.py:1935]   Expert 34 |    134 | CPU
DEBUG 01-15 16:08:52.385292.385292 lmp.py:1935]   Expert 39 |    137 | CPU
DEBUG 01-15 16:08:52.385558.385558 lmp.py:1935]   Expert 26 |    139 | CPU
DEBUG 01-15 16:08:52.385824.385824 lmp.py:1935]   Expert 36 |    139 | CPU
DEBUG 01-15 16:08:52.385136.385136 lmp.py:1935]   Expert  5 |    141 | CPU
DEBUG 01-15 16:08:52.386971.386971 lmp.py:1935]   Expert 11 |    141 | CPU
DEBUG 01-15 16:08:52.386567.386567 lmp.py:1935]   Expert 59 |    142 | CPU
DEBUG 01-15 16:08:52.386164.386164 lmp.py:1935]   Expert  6 |    145 | CPU
DEBUG 01-15 16:08:52.386999.386999 lmp.py:1935]   Expert 51 |    145 | CPU
DEBUG 01-15 16:08:52.386834.386834 lmp.py:1935]   Expert 23 |    154 | CPU
DEBUG 01-15 16:08:52.386431.386431 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:08:52.386027.386027 lmp.py:1935]   Expert 49 |    156 | CPU
DEBUG 01-15 16:08:52.386578.386578 lmp.py:1935]   Expert  9 |    158 | CPU
DEBUG 01-15 16:08:52.386651.386651 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:08:52.386201.386201 lmp.py:1935]   Expert 40 |    168 | CPU
DEBUG 01-15 16:08:52.386182.386182 lmp.py:1935]   Expert 56 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386640.386640 lmp.py:1935]   Expert 16 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386429.386429 lmp.py:1935]   Expert 52 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386217.386217 lmp.py:1935]   Expert 35 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386768.386768 lmp.py:1935]   Expert  4 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386033.386033 lmp.py:1935]   Expert 13 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386822.386822 lmp.py:1935]   Expert 37 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386372.386372 lmp.py:1935]   Expert 42 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386161.386161 lmp.py:1935]   Expert 38 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386950.386950 lmp.py:1935]   Expert 17 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386262.386262 lmp.py:1935]   Expert 62 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386289.386289 lmp.py:1935]   Expert 21 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386270.386270 lmp.py:1935]   Expert  3 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386535.386535 lmp.py:1935]   Expert 44 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386801.386801 lmp.py:1935]   Expert 60 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386305.386305 lmp.py:1935]   Expert 28 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386571.386571 lmp.py:1935]   Expert 47 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386359.386359 lmp.py:1935]   Expert 58 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386148.386148 lmp.py:1935]   Expert 10 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386175.386175 lmp.py:1935]   Expert 53 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386487.386487 lmp.py:1935]   Expert 55 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386037.386037 lmp.py:1935]   Expert 20 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386826.386826 lmp.py:1935]   Expert 33 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386376.386376 lmp.py:1935]   Expert 57 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386642.386642 lmp.py:1935]   Expert 31 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386146.386146 lmp.py:1935]   Expert 46 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386412.386412 lmp.py:1935]   Expert  8 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386439.386439 lmp.py:1935]   Expert 19 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386466.386466 lmp.py:1935]   Expert 24 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386255.386255 lmp.py:1935]   Expert 14 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386043.386043 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386355.386355 lmp.py:1935]   Expert 12 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386906.386906 lmp.py:1935]   Expert 29 |    275 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386456.386456 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386006.386006 lmp.py:1935]   Expert  0 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386557.386557 lmp.py:1935]   Expert 43 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386107.386107 lmp.py:1935]   Expert 54 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386657.386657 lmp.py:1935]   Expert 41 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.386446.386446 lmp.py:1935]   Expert 25 |    412 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.386996.386996 lmp.py:1937] 
DEBUG 01-15 16:08:52.386996.386996 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.386500.386500 lmp.py:1938]   CPU:   3167 tokens
DEBUG 01-15 16:08:52.387766.387766 lmp.py:1942]   cuda:1:   4483 tokens (19 experts)
DEBUG 01-15 16:08:52.387846.387846 lmp.py:1942]   cuda:2:   4638 tokens (20 experts)
DEBUG 01-15 16:08:52.387443.387443 lmp.py:1943]   Total GPU:   9121 tokens
DEBUG 01-15 16:08:52.387847.387847 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.387847.387847 lmp.py:1944] 
DEBUG 01-15 16:08:52.387974.387974 cuda_h.py:19] end experts_map_get cost 0.0020203590393066406 seconds
DEBUG 01-15 16:08:52.387930.387930 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:52.387925.387925 lmp.py:1953] 
DEBUG 01-15 16:08:52.387925.387925 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.387330.387330 cuda_h.py:19] end cpu_experts_submit cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:08:52.387218.387218 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.387432.387432 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.389361.389361 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.3.mlp.experts.8.gate_proj.weight': 0, 'model.layers.3.mlp.experts.8.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.8.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.38.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.38.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.38.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.54.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.54.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.54.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.3.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.3.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.3.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5127012352, 5767168, 17301504, 0), (5132779520, 5767168, 23068672, 0), (5121245184, 5767168, 28835840, 0), (5144313856, 5767168, 34603008, 0), (5150081024, 5767168, 40370176, 0), (5138546688, 5767168, 46137344, 0), (5317328896, 5767168, 51904512, 0), (5323096064, 5767168, 57671680, 0), (5311561728, 5767168, 63438848, 0), (5403836416, 5767168, 69206016, 0), (5409603584, 5767168, 74973184, 0), (5398069248, 5767168, 80740352, 0), (5421137920, 5767168, 86507520, 0), (5426905088, 5767168, 92274688, 0), (5415370752, 5767168, 98041856, 0), (5507645440, 5767168, 103809024, 0), (5513412608, 5767168, 109576192, 0), (5501878272, 5767168, 115343360, 0), (5576851456, 5767168, 121110528, 0), (5582618624, 5767168, 126877696, 0), (5571084288, 5767168, 132644864, 0), (5611454464, 5767168, 138412032, 0), (5617221632, 5767168, 144179200, 0), (5605687296, 5767168, 149946368, 0), (5715263488, 5767168, 155713536, 0), (5721030656, 5767168, 161480704, 0), (5709496320, 5767168, 167247872, 0), (5801771008, 5767168, 173015040, 0), (5807538176, 5767168, 178782208, 0), (5796003840, 5767168, 184549376, 0), (5819072512, 5767168, 190316544, 0), (5824839680, 5767168, 196083712, 0), (5813305344, 5767168, 201850880, 0), (5836374016, 5767168, 207618048, 0), (5842141184, 5767168, 213385216, 0), (5830606848, 5767168, 219152384, 0), (5870977024, 5767168, 224919552, 0), (5876744192, 5767168, 230686720, 0), (5865209856, 5767168, 236453888, 0), (5888278528, 5767168, 242221056, 0), (5894045696, 5767168, 247988224, 0), (5882511360, 5767168, 253755392, 0), (5974786048, 5767168, 259522560, 0), (5980553216, 5767168, 265289728, 0), (5969018880, 5767168, 271056896, 0), (5992087552, 5767168, 276824064, 0), (5997854720, 5767168, 282591232, 0), (5986320384, 5767168, 288358400, 0), (6078595072, 5767168, 294125568, 0), (6084362240, 5767168, 299892736, 0), (6072827904, 5767168, 305659904, 0), (6147801088, 5767168, 311427072, 0), (6153568256, 5767168, 317194240, 0), (6142033920, 5767168, 322961408, 0)], 2: [(5213519872, 5767168, 0, 0), (5219287040, 5767168, 5767168, 0), (5207752704, 5767168, 11534336, 0), (5248122880, 5767168, 17301504, 0), (5253890048, 5767168, 23068672, 0), (5242355712, 5767168, 28835840, 0), (5282725888, 5767168, 34603008, 0), (5288493056, 5767168, 40370176, 0), (5276958720, 5767168, 46137344, 0), (5300027392, 5767168, 51904512, 0), (5305794560, 5767168, 57671680, 0), (5294260224, 5767168, 63438848, 0), (5351931904, 5767168, 69206016, 0), (5357699072, 5767168, 74973184, 0), (5346164736, 5767168, 80740352, 0), (5369233408, 5767168, 86507520, 0), (5375000576, 5767168, 92274688, 0), (5363466240, 5767168, 98041856, 0), (5438439424, 5767168, 103809024, 0), (5444206592, 5767168, 109576192, 0), (5432672256, 5767168, 115343360, 0), (5455740928, 5767168, 121110528, 0), (5461508096, 5767168, 126877696, 0), (5449973760, 5767168, 132644864, 0), (5490343936, 5767168, 138412032, 0), (5496111104, 5767168, 144179200, 0), (5484576768, 5767168, 149946368, 0), (5559549952, 5767168, 155713536, 0), (5565317120, 5767168, 161480704, 0), (5553782784, 5767168, 167247872, 0), (5646057472, 5767168, 173015040, 0), (5651824640, 5767168, 178782208, 0), (5640290304, 5767168, 184549376, 0), (5680660480, 5767168, 190316544, 0), (5686427648, 5767168, 196083712, 0), (5674893312, 5767168, 201850880, 0), (5732564992, 5767168, 207618048, 0), (5738332160, 5767168, 213385216, 0), (5726797824, 5767168, 219152384, 0), (5784469504, 5767168, 224919552, 0), (5790236672, 5767168, 230686720, 0), (5778702336, 5767168, 236453888, 0), (6009389056, 5767168, 242221056, 0), (6015156224, 5767168, 247988224, 0), (6003621888, 5767168, 253755392, 0), (6026690560, 5767168, 259522560, 0), (6032457728, 5767168, 265289728, 0), (6020923392, 5767168, 271056896, 0), (6043992064, 5767168, 276824064, 0), (6049759232, 5767168, 282591232, 0), (6038224896, 5767168, 288358400, 0), (6061293568, 5767168, 294125568, 0), (6067060736, 5767168, 299892736, 0), (6055526400, 5767168, 305659904, 0), (6113198080, 5767168, 311427072, 0), (6118965248, 5767168, 317194240, 0), (6107430912, 5767168, 322961408, 0), (6165102592, 5767168, 328728576, 0), (6170869760, 5767168, 334495744, 0), (6159335424, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729560>, 2: <capsule object NULL at 0x74a660728720>}
DEBUG 01-15 16:08:52.389417.389417 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.389406.389406 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.389354.389354 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, db7c6652-2c52-4751-8915-14c3ed5efa2f
DEBUG 01-15 16:08:52.390672.390672 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:52.390794.390794 client.py:127] Model loaded
DEBUG 01-15 16:08:52.390095.390095 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.390633.390633 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.390736.390736 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:52.390323.390323 cuda_h.py:19] end restore2model cost 0.0003998279571533203 seconds
DEBUG 01-15 16:08:52.390960.390960 cuda_h.py:19] end sllm_worker_task cost 0.01154470443725586 seconds
DEBUG 01-15 16:08:52.391298.391298 cuda_h.py:19] end move_flatidxs cost 0.0008647441864013672 seconds
DEBUG 01-15 16:08:52.391664.391664 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:52.392662.392662 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, db7c6652-2c52-4751-8915-14c3ed5efa2f
DEBUG 01-15 16:08:52.392691.392691 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005317211151123047 seconds
DEBUG 01-15 16:08:52.392932.392932 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.396939.396939 cuda_h.py:19] end restore2model cost 0.003546476364135742 seconds
DEBUG 01-15 16:08:52.396511.396511 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0091094970703125 seconds
DEBUG 01-15 16:08:52.396619.396619 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.396299.396299 cuda_h.py:19] end gpu_sexperts cost 0.0003275871276855469 seconds
DEBUG 01-15 16:08:52.396467.396467 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.396556.396556 cuda_h.py:19] end group_tensors cost 0.004649162292480469 seconds
DEBUG 01-15 16:08:52.396389.396389 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:08:52.396900.396900 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.396470.396470 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.396742.396742 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.398660.398660 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0018475055694580078 seconds
DEBUG 01-15 16:08:52.399658.399658 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.399968.399968 cuda_h.py:19] end gpu_group_list cost 0.0003247261047363281 seconds
DEBUG 01-15 16:08:52.400515.400515 cuda_h.py:19] end group pad cost 0.0030813217163085938 seconds
DEBUG 01-15 16:08:52.400974.400974 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:52.400441.400441 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.409112.409112 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.008186101913452148 seconds
DEBUG 01-15 16:08:52.409398.409398 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.410074.410074 cuda_h.py:19] end gpu_group_list cost 0.00037550926208496094 seconds
DEBUG 01-15 16:08:52.414354.414354 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.415991.415991 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, db7c6652-2c52-4751-8915-14c3ed5efa2f
INFO 01-15 16:08:52.427148.427148 client.py:127] Model loaded
DEBUG 01-15 16:08:52.427434.427434 cuda_h.py:19] end wait_experts_multi_device cost 0.011991262435913086 seconds
DEBUG 01-15 16:08:52.427495.427495 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.428700.428700 cuda_h.py:19] end group_einsum cost 0.02790355682373047 seconds
DEBUG 01-15 16:08:52.428605.428605 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:52.431582.431582 cuda_h.py:19] end get_outputs_cpu1 cost 0.0035986900329589844 seconds
DEBUG 01-15 16:08:52.433899.433899 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04329943656921387 seconds
DEBUG 01-15 16:08:52.433098.433098 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006257057189941406 seconds
DEBUG 01-15 16:08:52.433363.433363 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.435755.435755 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.435737.435737 cuda_h.py:19] end index_scatter cost 0.00010251998901367188 seconds
DEBUG 01-15 16:08:52.436579.436579 cuda_h.py:19] end cpuoutputsdeal cost 0.001995086669921875 seconds
DEBUG 01-15 16:08:52.436576.436576 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.436697.436697 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.436326.436326 cuda_h.py:19] end gpu_group_tensor cost 0.0001728534698486328 seconds
DEBUG 01-15 16:08:52.436804.436804 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.436347.436347 cuda_h.py:19] end gpu_group_tensor cost 0.00016069412231445312 seconds
DEBUG 01-15 16:08:52.436464.436464 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.437353.437353 cuda_h.py:19] end gpu_group_einsum cost 0.0007774829864501953 seconds
DEBUG 01-15 16:08:52.437047.437047 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.438763.438763 cuda_h.py:19] end gpu_group_einsum cost 0.0005512237548828125 seconds
DEBUG 01-15 16:08:52.438477.438477 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.438382.438382 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.439606.439606 cuda_h.py:19] end all_expert_outputs_slices cost 0.00032830238342285156 seconds
DEBUG 01-15 16:08:52.439376.439376 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.439920.439920 cuda_h.py:19] end concat_expert_out cost 6.532669067382812e-05 seconds
DEBUG 01-15 16:08:52.439804.439804 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.439258.439258 cuda_h.py:19] end index_scatter cost 7.915496826171875e-05 seconds
DEBUG 01-15 16:08:52.440964.440964 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012502670288085938 seconds
DEBUG 01-15 16:08:52.440829.440829 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.440593.440593 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.440046.440046 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002613067626953125 seconds
DEBUG 01-15 16:08:52.440179.440179 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.441518.441518 cuda_h.py:19] end concat_expert_out cost 0.0002396106719970703 seconds
DEBUG 01-15 16:08:52.441088.441088 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.441355.441355 cuda_h.py:19] end index_scatter cost 0.00016164779663085938 seconds
DEBUG 01-15 16:08:52.441736.441736 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0015277862548828125 seconds
DEBUG 01-15 16:08:52.441736.441736 cuda_h.py:19] end gpu_experts_multi_device cost 0.04492616653442383 seconds
DEBUG 01-15 16:08:52.442538.442538 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.05790591239929199 seconds
DEBUG 01-15 16:08:52.443853.443853 cuda_h.py:19] end prefill_layer cost 0.06415104866027832 seconds
DEBUG 01-15 16:08:52.443844.443844 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:08:52.443608.443608 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.443936.443936 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:08:52.443091.443091 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:08:52.443107.443107 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:08:52.443523.443523 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 8.392333984375e-05 seconds
DEBUG 01-15 16:08:52.443348.443348 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 0.00021076202392578125 seconds
DEBUG 01-15 16:08:52.443920.443920 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.443580.443580 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.444838.444838 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.444035.444035 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.444696.444696 cuda_h.py:19] end allocate_cuda_memory cost 0.00045561790466308594 seconds
DEBUG 01-15 16:08:52.445697.445697 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.445422.445422 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.445742.445742 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.445388.445388 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.445696.445696 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3abba0b0-a9da-4c36-9d27-28f6b1735545
DEBUG 01-15 16:08:52.445173.445173 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.446275.446275 cuda_h.py:10] start self_attn
INFO 01-15 16:08:52.448276.448276 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3abba0b0-a9da-4c36-9d27-28f6b1735545
DEBUG 01-15 16:08:52.448149.448149 cuda_h.py:19] end load_into_gpu_async cost 0.0028641223907470703 seconds
DEBUG 01-15 16:08:52.448263.448263 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.448408.448408 cuda_h.py:19] end restore_tensors2 cost 0.00013446807861328125 seconds
DEBUG 01-15 16:08:52.448145.448145 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004319190979003906 seconds
INFO 01-15 16:08:52.448723.448723 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3abba0b0-a9da-4c36-9d27-28f6b1735545
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.454552.454552 cuda_h.py:19] end self_attn cost 0.008140325546264648 seconds
DEBUG 01-15 16:08:52.455162.455162 cuda_h.py:19] end iln_self_attn_paln cost 0.01188516616821289 seconds
INFO 01-15 16:08:52.455909.455909 client.py:127] Model loaded
DEBUG 01-15 16:08:52.456151.456151 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:08:52.456062.456062 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.456509.456509 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.457868.457868 cuda_h.py:19] end restore2model cost 0.0007426738739013672 seconds
DEBUG 01-15 16:08:52.457705.457705 cuda_h.py:19] end sllm_worker_task cost 0.013054370880126953 seconds
DEBUG 01-15 16:08:52.458644.458644 cuda_h.py:19] end gate cost 0.0013818740844726562 seconds
DEBUG 01-15 16:08:52.458475.458475 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.459231.459231 lmp.py:1912] 
DEBUG 01-15 16:08:52.459231.459231 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.459783.459783 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.459268.459268 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.459699.459699 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.459031.459031 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.459456.459456 lmp.py:1917] 
DEBUG 01-15 16:08:52.459456.459456 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.459358.459358 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.459320.459320 lmp.py:1935]   Expert 14 |     67 | CPU
DEBUG 01-15 16:08:52.459414.459414 lmp.py:1935]   Expert 57 |     72 | CPU
DEBUG 01-15 16:08:52.459031.459031 lmp.py:1935]   Expert 13 |     75 | CPU
DEBUG 01-15 16:08:52.459409.459409 lmp.py:1935]   Expert 26 |     81 | CPU
DEBUG 01-15 16:08:52.459119.459119 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:08:52.459067.459067 lmp.py:1935]   Expert 11 |     92 | CPU
DEBUG 01-15 16:08:52.459300.459300 lmp.py:1935]   Expert 54 |     92 | CPU
DEBUG 01-15 16:08:52.459771.459771 lmp.py:1935]   Expert 45 |     94 | CPU
DEBUG 01-15 16:08:52.459388.459388 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:08:52.459005.459005 lmp.py:1935]   Expert 30 |    108 | CPU
DEBUG 01-15 16:08:52.459430.459430 lmp.py:1935]   Expert 51 |    108 | CPU
DEBUG 01-15 16:08:52.459663.459663 lmp.py:1935]   Expert 36 |    113 | CPU
DEBUG 01-15 16:08:52.459657.459657 lmp.py:1935]   Expert 10 |    114 | CPU
DEBUG 01-15 16:08:52.459890.459890 lmp.py:1935]   Expert 32 |    115 | CPU
DEBUG 01-15 16:08:52.459361.459361 lmp.py:1935]   Expert 20 |    128 | CPU
DEBUG 01-15 16:08:52.459263.459263 lmp.py:1935]   Expert  8 |    131 | CPU
DEBUG 01-15 16:08:52.459211.459211 lmp.py:1935]   Expert  4 |    136 | CPU
DEBUG 01-15 16:08:52.459921.459921 lmp.py:1935]   Expert 63 |    138 | CPU
DEBUG 01-15 16:08:52.459154.459154 lmp.py:1935]   Expert 53 |    140 | CPU
DEBUG 01-15 16:08:52.459387.459387 lmp.py:1935]   Expert 61 |    143 | CPU
DEBUG 01-15 16:08:52.459858.459858 lmp.py:1935]   Expert 34 |    146 | CPU
DEBUG 01-15 16:08:52.460283.460283 lmp.py:1935]   Expert 47 |    146 | CPU
DEBUG 01-15 16:08:52.460231.460231 lmp.py:1935]   Expert 16 |    147 | CPU
DEBUG 01-15 16:08:52.460464.460464 lmp.py:1935]   Expert 60 |    158 | CPU
DEBUG 01-15 16:08:52.460697.460697 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:08:52.460744.460744 lmp.py:1935]   Expert 42 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460123.460123 lmp.py:1935]   Expert 17 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460932.460932 lmp.py:1935]   Expert 29 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460310.460310 lmp.py:1935]   Expert 44 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460212.460212 lmp.py:1935]   Expert 27 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460114.460114 lmp.py:1935]   Expert  7 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460539.460539 lmp.py:1935]   Expert 41 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460110.460110 lmp.py:1935]   Expert  9 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460793.460793 lmp.py:1935]   Expert 56 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460649.460649 lmp.py:1935]   Expert 48 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460074.460074 lmp.py:1935]   Expert  3 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460214.460214 lmp.py:1935]   Expert  2 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460639.460639 lmp.py:1935]   Expert 15 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460448.460448 lmp.py:1935]   Expert 24 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460588.460588 lmp.py:1935]   Expert  0 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460013.460013 lmp.py:1935]   Expert 18 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460200.460200 lmp.py:1935]   Expert 55 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460909.460909 lmp.py:1935]   Expert 40 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460857.460857 lmp.py:1935]   Expert 38 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460951.460951 lmp.py:1935]   Expert 22 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460568.460568 lmp.py:1935]   Expert 23 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460993.460993 lmp.py:1935]   Expert 37 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460703.460703 lmp.py:1935]   Expert  6 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460128.460128 lmp.py:1935]   Expert 46 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.460314.460314 lmp.py:1935]   Expert 19 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.460693.460693 lmp.py:1935]   Expert 39 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.461840.461840 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.461265.461265 lmp.py:1935]   Expert 50 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.461359.461359 lmp.py:1935]   Expert 12 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.461022.461022 lmp.py:1935]   Expert 62 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.461401.461401 lmp.py:1935]   Expert 21 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.461779.461779 lmp.py:1935]   Expert 35 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.461681.461681 lmp.py:1935]   Expert 49 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.461629.461629 lmp.py:1935]   Expert 52 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.461577.461577 lmp.py:1935]   Expert 33 |    298 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.461287.461287 lmp.py:1935]   Expert  1 |    349 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.461473.461473 lmp.py:1935]   Expert  5 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.461090.461090 lmp.py:1935]   Expert 43 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.461184.461184 lmp.py:1935]   Expert 59 |    584 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.461702.461702 lmp.py:1937] 
DEBUG 01-15 16:08:52.461702.461702 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.461935.461935 lmp.py:1938]   CPU:   2895 tokens
DEBUG 01-15 16:08:52.461598.461598 lmp.py:1942]   cuda:1:   4640 tokens (19 experts)
DEBUG 01-15 16:08:52.461546.461546 lmp.py:1942]   cuda:2:   4753 tokens (20 experts)
DEBUG 01-15 16:08:52.461494.461494 lmp.py:1943]   Total GPU:   9393 tokens
DEBUG 01-15 16:08:52.461535.461535 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.461535.461535 lmp.py:1944] 
DEBUG 01-15 16:08:52.461589.461589 cuda_h.py:19] end experts_map_get cost 0.0033681392669677734 seconds
DEBUG 01-15 16:08:52.461732.461732 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:52.461945.461945 lmp.py:1953] 
DEBUG 01-15 16:08:52.461945.461945 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.461061.461061 cuda_h.py:19] end cpu_experts_submit cost 8.273124694824219e-05 seconds
DEBUG 01-15 16:08:52.461923.461923 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.462536.462536 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.462654.462654 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.27.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.27.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.27.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.38.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.38.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.38.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.42.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.42.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.42.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.43.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.43.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.43.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.49.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.49.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.49.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.55.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.55.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.55.up_proj.weight': 322961408, 'model.layers.4.mlp.experts.56.gate_proj.weight': 328728576, 'model.layers.4.mlp.experts.56.down_proj.weight': 334495744, 'model.layers.4.mlp.experts.56.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6217007104, 5767168, 17301504, 0), (6222774272, 5767168, 23068672, 0), (6211239936, 5767168, 28835840, 0), (6303514624, 5767168, 34603008, 0), (6309281792, 5767168, 40370176, 0), (6297747456, 5767168, 46137344, 0), (6338117632, 5767168, 51904512, 0), (6343884800, 5767168, 57671680, 0), (6332350464, 5767168, 63438848, 0), (6476529664, 5767168, 69206016, 0), (6482296832, 5767168, 74973184, 0), (6470762496, 5767168, 80740352, 0), (6493831168, 5767168, 86507520, 0), (6499598336, 5767168, 92274688, 0), (6488064000, 5767168, 98041856, 0), (6580338688, 5767168, 103809024, 0), (6586105856, 5767168, 109576192, 0), (6574571520, 5767168, 115343360, 0), (6597640192, 5767168, 121110528, 0), (6603407360, 5767168, 126877696, 0), (6591873024, 5767168, 132644864, 0), (6787956736, 5767168, 138412032, 0), (6793723904, 5767168, 144179200, 0), (6782189568, 5767168, 149946368, 0), (6822559744, 5767168, 155713536, 0), (6828326912, 5767168, 161480704, 0), (6816792576, 5767168, 167247872, 0), (6857162752, 5767168, 173015040, 0), (6862929920, 5767168, 178782208, 0), (6851395584, 5767168, 184549376, 0), (6874464256, 5767168, 190316544, 0), (6880231424, 5767168, 196083712, 0), (6868697088, 5767168, 201850880, 0), (6943670272, 5767168, 207618048, 0), (6949437440, 5767168, 213385216, 0), (6937903104, 5767168, 219152384, 0), (6978273280, 5767168, 224919552, 0), (6984040448, 5767168, 230686720, 0), (6972506112, 5767168, 236453888, 0), (7012876288, 5767168, 242221056, 0), (7018643456, 5767168, 247988224, 0), (7007109120, 5767168, 253755392, 0), (7047479296, 5767168, 259522560, 0), (7053246464, 5767168, 265289728, 0), (7041712128, 5767168, 271056896, 0), (7082082304, 5767168, 276824064, 0), (7087849472, 5767168, 282591232, 0), (7076315136, 5767168, 288358400, 0), (7203192832, 5767168, 294125568, 0), (7208960000, 5767168, 299892736, 0), (7197425664, 5767168, 305659904, 0), (7255097344, 5767168, 311427072, 0), (7260864512, 5767168, 317194240, 0), (7249330176, 5767168, 322961408, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6234308608, 5767168, 17301504, 0), (6240075776, 5767168, 23068672, 0), (6228541440, 5767168, 28835840, 0), (6268911616, 5767168, 34603008, 0), (6274678784, 5767168, 40370176, 0), (6263144448, 5767168, 46137344, 0), (6286213120, 5767168, 51904512, 0), (6291980288, 5767168, 57671680, 0), (6280445952, 5767168, 63438848, 0), (6390022144, 5767168, 69206016, 0), (6395789312, 5767168, 74973184, 0), (6384254976, 5767168, 80740352, 0), (6441926656, 5767168, 86507520, 0), (6447693824, 5767168, 92274688, 0), (6436159488, 5767168, 98041856, 0), (6511132672, 5767168, 103809024, 0), (6516899840, 5767168, 109576192, 0), (6505365504, 5767168, 115343360, 0), (6545735680, 5767168, 121110528, 0), (6551502848, 5767168, 126877696, 0), (6539968512, 5767168, 132644864, 0), (6563037184, 5767168, 138412032, 0), (6568804352, 5767168, 144179200, 0), (6557270016, 5767168, 149946368, 0), (6614941696, 5767168, 155713536, 0), (6620708864, 5767168, 161480704, 0), (6609174528, 5767168, 167247872, 0), (6649544704, 5767168, 173015040, 0), (6655311872, 5767168, 178782208, 0), (6643777536, 5767168, 184549376, 0), (6684147712, 5767168, 190316544, 0), (6689914880, 5767168, 196083712, 0), (6678380544, 5767168, 201850880, 0), (6753353728, 5767168, 207618048, 0), (6759120896, 5767168, 213385216, 0), (6747586560, 5767168, 219152384, 0), (6839861248, 5767168, 224919552, 0), (6845628416, 5767168, 230686720, 0), (6834094080, 5767168, 236453888, 0), (6891765760, 5767168, 242221056, 0), (6897532928, 5767168, 247988224, 0), (6885998592, 5767168, 253755392, 0), (6909067264, 5767168, 259522560, 0), (6914834432, 5767168, 265289728, 0), (6903300096, 5767168, 271056896, 0), (6926368768, 5767168, 276824064, 0), (6932135936, 5767168, 282591232, 0), (6920601600, 5767168, 288358400, 0), (7030177792, 5767168, 294125568, 0), (7035944960, 5767168, 299892736, 0), (7024410624, 5767168, 305659904, 0), (7133986816, 5767168, 311427072, 0), (7139753984, 5767168, 317194240, 0), (7128219648, 5767168, 322961408, 0), (7151288320, 5767168, 328728576, 0), (7157055488, 5767168, 334495744, 0), (7145521152, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8145742a0>, 2: <capsule object NULL at 0x74a660729230>}
DEBUG 01-15 16:08:52.463176.463176 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.463209.463209 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.463429.463429 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 617a0816-a57e-4b93-90d2-1f2613e77932
DEBUG 01-15 16:08:52.463467.463467 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.464063.464063 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.464019.464019 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:52.465286.465286 cuda_h.py:19] end move_flatidxs cost 0.0009355545043945312 seconds
DEBUG 01-15 16:08:52.465302.465302 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:52.472883.472883 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 617a0816-a57e-4b93-90d2-1f2613e77932
DEBUG 01-15 16:08:52.473927.473927 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.011646270751953125 seconds
DEBUG 01-15 16:08:52.473699.473699 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.474364.474364 cuda_h.py:19] end group_tensors cost 0.00850057601928711 seconds
DEBUG 01-15 16:08:52.475976.475976 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.479326.479326 cuda_h.py:19] end group pad cost 0.004024505615234375 seconds
DEBUG 01-15 16:08:52.479700.479700 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:52.483796.483796 cuda_h.py:19] end restore2model cost 0.009210586547851562 seconds
DEBUG 01-15 16:08:52.484661.484661 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.02210688591003418 seconds
DEBUG 01-15 16:08:52.484077.484077 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.486403.486403 cuda_h.py:19] end gpu_sexperts cost 0.0019235610961914062 seconds
DEBUG 01-15 16:08:52.487295.487295 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.487726.487726 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 0.0001239776611328125 seconds
DEBUG 01-15 16:08:52.487885.487885 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.488876.488876 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.489298.489298 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001791238784790039 seconds
DEBUG 01-15 16:08:52.490650.490650 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.490785.490785 cuda_h.py:19] end gpu_group_list cost 0.00054931640625 seconds
DEBUG 01-15 16:08:52.496547.496547 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.499879.499879 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0024900436401367188 seconds
DEBUG 01-15 16:08:52.500932.500932 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.500582.500582 cuda_h.py:19] end gpu_group_list cost 0.0003135204315185547 seconds
DEBUG 01-15 16:08:52.501727.501727 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.501659.501659 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 617a0816-a57e-4b93-90d2-1f2613e77932
INFO 01-15 16:08:52.507699.507699 client.py:127] Model loaded
DEBUG 01-15 16:08:52.507468.507468 cuda_h.py:19] end wait_experts_multi_device cost 0.0055561065673828125 seconds
DEBUG 01-15 16:08:52.507351.507351 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.507410.507410 cuda_h.py:19] end group_einsum cost 0.02823162078857422 seconds
DEBUG 01-15 16:08:52.508024.508024 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:52.511730.511730 cuda_h.py:19] end get_outputs_cpu1 cost 0.003015756607055664 seconds
DEBUG 01-15 16:08:52.511607.511607 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04775834083557129 seconds
DEBUG 01-15 16:08:52.512684.512684 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005311012268066406 seconds
DEBUG 01-15 16:08:52.513664.513664 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.515261.515261 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.516831.516831 cuda_h.py:19] end index_scatter cost 0.00016355514526367188 seconds
DEBUG 01-15 16:08:52.516577.516577 cuda_h.py:19] end cpuoutputsdeal cost 0.0034127235412597656 seconds
DEBUG 01-15 16:08:52.517643.517643 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.517190.517190 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.517932.517932 cuda_h.py:19] end gpu_group_tensor cost 0.00031495094299316406 seconds
DEBUG 01-15 16:08:52.517823.517823 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.518203.518203 cuda_h.py:19] end gpu_group_tensor cost 0.0007803440093994141 seconds
DEBUG 01-15 16:08:52.518947.518947 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.520138.520138 cuda_h.py:19] end gpu_group_einsum cost 0.0012118816375732422 seconds
DEBUG 01-15 16:08:52.520273.520273 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.521667.521667 cuda_h.py:19] end gpu_group_einsum cost 0.0009713172912597656 seconds
DEBUG 01-15 16:08:52.521146.521146 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.522173.522173 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.522464.522464 cuda_h.py:19] end all_expert_outputs_slices cost 0.00044608116149902344 seconds
DEBUG 01-15 16:08:52.522463.522463 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.522715.522715 cuda_h.py:19] end concat_expert_out cost 0.00012946128845214844 seconds
DEBUG 01-15 16:08:52.523827.523827 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.523503.523503 cuda_h.py:19] end index_scatter cost 0.0001277923583984375 seconds
DEBUG 01-15 16:08:52.523567.523567 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001983642578125 seconds
DEBUG 01-15 16:08:52.524667.524667 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.524334.524334 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.524773.524773 cuda_h.py:19] end all_expert_outputs_slices cost 0.00035953521728515625 seconds
DEBUG 01-15 16:08:52.524743.524743 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.524399.524399 cuda_h.py:19] end concat_expert_out cost 0.0001246929168701172 seconds
DEBUG 01-15 16:08:52.525973.525973 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.525636.525636 cuda_h.py:19] end index_scatter cost 0.00011849403381347656 seconds
DEBUG 01-15 16:08:52.525222.525222 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012388229370117188 seconds
DEBUG 01-15 16:08:52.525831.525831 cuda_h.py:19] end gpu_experts_multi_device cost 0.037651777267456055 seconds
DEBUG 01-15 16:08:52.525639.525639 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.06955552101135254 seconds
DEBUG 01-15 16:08:52.526275.526275 cuda_h.py:19] end prefill_layer cost 0.0832819938659668 seconds
DEBUG 01-15 16:08:52.526961.526961 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:08:52.526148.526148 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.526912.526912 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:08:52.526750.526750 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:08:52.527223.527223 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:08:52.527058.527058 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 8.130073547363281e-05 seconds
DEBUG 01-15 16:08:52.527809.527809 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 0.00019097328186035156 seconds
DEBUG 01-15 16:08:52.527328.527328 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.527777.527777 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.527438.527438 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.528129.528129 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.528131.528131 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.528723.528723 cuda_h.py:19] end allocate_cuda_memory cost 0.00041747093200683594 seconds
DEBUG 01-15 16:08:52.528688.528688 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.528293.528293 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.529726.529726 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.529171.529171 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1e712b8c-f73b-4a2d-a94b-fc9d28026c09
DEBUG 01-15 16:08:52.529785.529785 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.529809.529809 cuda_h.py:10] start self_attn
INFO 01-15 16:08:52.530075.530075 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1e712b8c-f73b-4a2d-a94b-fc9d28026c09
DEBUG 01-15 16:08:52.530927.530927 cuda_h.py:19] end load_into_gpu_async cost 0.0017278194427490234 seconds
DEBUG 01-15 16:08:52.530386.530386 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.530676.530676 cuda_h.py:19] end restore_tensors2 cost 9.655952453613281e-05 seconds
DEBUG 01-15 16:08:52.530559.530559 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0027315616607666016 seconds
INFO 01-15 16:08:52.531714.531714 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1e712b8c-f73b-4a2d-a94b-fc9d28026c09
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.534164.534164 cuda_h.py:19] end self_attn cost 0.0049436092376708984 seconds
DEBUG 01-15 16:08:52.534098.534098 cuda_h.py:19] end iln_self_attn_paln cost 0.007486104965209961 seconds
DEBUG 01-15 16:08:52.535080.535080 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:08:52.535081.535081 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.535723.535723 cuda_h.py:19] end gate cost 0.0007519721984863281 seconds
DEBUG 01-15 16:08:52.535083.535083 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.536293.536293 lmp.py:1912] 
DEBUG 01-15 16:08:52.536293.536293 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.536009.536009 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.536997.536997 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.536931.536931 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.536482.536482 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.536317.536317 lmp.py:1917] 
DEBUG 01-15 16:08:52.536317.536317 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.536106.536106 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.536378.536378 lmp.py:1935]   Expert 34 |     23 | CPU
DEBUG 01-15 16:08:52.536928.536928 lmp.py:1935]   Expert 45 |     67 | CPU
DEBUG 01-15 16:08:52.536525.536525 lmp.py:1935]   Expert 22 |     74 | CPU
DEBUG 01-15 16:08:52.536645.536645 lmp.py:1935]   Expert 57 |     78 | CPU
DEBUG 01-15 16:08:52.536149.536149 lmp.py:1935]   Expert 17 |     95 | CPU
DEBUG 01-15 16:08:52.536938.536938 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:08:52.536488.536488 lmp.py:1935]   Expert  4 |     99 | CPU
DEBUG 01-15 16:08:52.536707.536707 lmp.py:1935]   Expert 28 |    106 | CPU
DEBUG 01-15 16:08:52.536734.536734 lmp.py:1935]   Expert 60 |    111 | CPU
DEBUG 01-15 16:08:52.536523.536523 lmp.py:1935]   Expert 32 |    112 | CPU
DEBUG 01-15 16:08:52.536312.536312 lmp.py:1935]   Expert 36 |    123 | CPU
DEBUG 01-15 16:08:52.536577.536577 lmp.py:1935]   Expert 14 |    125 | CPU
DEBUG 01-15 16:08:52.536604.536604 lmp.py:1935]   Expert 16 |    126 | CPU
DEBUG 01-15 16:08:52.536916.536916 lmp.py:1935]   Expert 12 |    128 | CPU
DEBUG 01-15 16:08:52.536228.536228 lmp.py:1935]   Expert 52 |    130 | CPU
DEBUG 01-15 16:08:52.536540.536540 lmp.py:1935]   Expert 25 |    131 | CPU
DEBUG 01-15 16:08:52.536614.536614 lmp.py:1935]   Expert  8 |    136 | CPU
DEBUG 01-15 16:08:52.536687.536687 lmp.py:1935]   Expert  2 |    138 | CPU
DEBUG 01-15 16:08:52.536999.536999 lmp.py:1935]   Expert 35 |    144 | CPU
DEBUG 01-15 16:08:52.536311.536311 lmp.py:1935]   Expert  5 |    148 | CPU
DEBUG 01-15 16:08:52.537384.537384 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:08:52.537696.537696 lmp.py:1935]   Expert 30 |    155 | CPU
DEBUG 01-15 16:08:52.537253.537253 lmp.py:1935]   Expert  0 |    157 | CPU
DEBUG 01-15 16:08:52.537042.537042 lmp.py:1935]   Expert 39 |    157 | CPU
DEBUG 01-15 16:08:52.537877.537877 lmp.py:1935]   Expert 61 |    158 | CPU
DEBUG 01-15 16:08:52.537097.537097 lmp.py:1935]   Expert  3 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537316.537316 lmp.py:1935]   Expert 13 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537581.537581 lmp.py:1935]   Expert 42 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537370.537370 lmp.py:1935]   Expert 31 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537682.537682 lmp.py:1935]   Expert 44 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537232.537232 lmp.py:1935]   Expert 46 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537783.537783 lmp.py:1935]   Expert 41 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537095.537095 lmp.py:1935]   Expert  9 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537168.537168 lmp.py:1935]   Expert 43 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537003.537003 lmp.py:1935]   Expert 26 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537315.537315 lmp.py:1935]   Expert 62 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537627.537627 lmp.py:1935]   Expert 50 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537654.537654 lmp.py:1935]   Expert 18 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537681.537681 lmp.py:1935]   Expert 27 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537708.537708 lmp.py:1935]   Expert 51 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537020.537020 lmp.py:1935]   Expert 49 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537809.537809 lmp.py:1935]   Expert 11 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537313.537313 lmp.py:1935]   Expert 47 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537102.537102 lmp.py:1935]   Expert 19 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537606.537606 lmp.py:1935]   Expert 63 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537871.537871 lmp.py:1935]   Expert 20 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537137.537137 lmp.py:1935]   Expert 55 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.537403.537403 lmp.py:1935]   Expert 56 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537668.537668 lmp.py:1935]   Expert 38 |    216 | GPU2(cuda:2)
INFO 01-15 16:08:52.537816.537816 client.py:127] Model loaded
DEBUG 01-15 16:08:52.537110.537110 lmp.py:1935]   Expert 48 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.537516.537516 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.537087.537087 lmp.py:1935]   Expert  1 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.538705.538705 cuda_h.py:19] end restore2model cost 0.0006000995635986328 seconds
DEBUG 01-15 16:08:52.538900.538900 lmp.py:1935]   Expert 10 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.538055.538055 cuda_h.py:19] end sllm_worker_task cost 0.010635614395141602 seconds
DEBUG 01-15 16:08:52.538937.538937 lmp.py:1935]   Expert  7 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.538742.538742 lmp.py:1935]   Expert 54 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.538200.538200 lmp.py:1935]   Expert 21 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.538273.538273 lmp.py:1935]   Expert 33 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.538585.538585 lmp.py:1935]   Expert 29 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.538135.538135 lmp.py:1935]   Expert 40 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.538686.538686 lmp.py:1935]   Expert 24 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.538997.538997 lmp.py:1935]   Expert 59 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.538071.538071 lmp.py:1935]   Expert 37 |    331 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.538383.538383 lmp.py:1935]   Expert 58 |    366 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.538218.538218 lmp.py:1935]   Expert  6 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.538576.538576 lmp.py:1935]   Expert 53 |    853 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.538219.538219 lmp.py:1937] 
DEBUG 01-15 16:08:52.538219.538219 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.539577.539577 lmp.py:1938]   CPU:   2974 tokens
DEBUG 01-15 16:08:52.539412.539412 lmp.py:1942]   cuda:1:   4663 tokens (19 experts)
DEBUG 01-15 16:08:52.539771.539771 lmp.py:1942]   cuda:2:   4651 tokens (20 experts)
DEBUG 01-15 16:08:52.539937.539937 lmp.py:1943]   Total GPU:   9314 tokens
DEBUG 01-15 16:08:52.539580.539580 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.539580.539580 lmp.py:1944] 
DEBUG 01-15 16:08:52.539283.539283 cuda_h.py:19] end experts_map_get cost 0.003168821334838867 seconds
DEBUG 01-15 16:08:52.539669.539669 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:52.539571.539571 lmp.py:1953] 
DEBUG 01-15 16:08:52.539571.539571 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.539553.539553 cuda_h.py:19] end cpu_experts_submit cost 5.6743621826171875e-05 seconds
DEBUG 01-15 16:08:52.539488.539488 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.539238.539238 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.540320.540320 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.3.gate_proj.weight': 0, 'model.layers.5.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.59.up_proj.weight': 322961408}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.21.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.21.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.21.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.24.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.24.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.24.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.27.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.27.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.27.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.37.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.37.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.37.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.38.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.38.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.38.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.46.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.46.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.46.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.5.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.5.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.5.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(7341604864, 5767168, 0, 0), (7347372032, 5767168, 5767168, 0), (7335837696, 5767168, 11534336, 0), (7410810880, 5767168, 17301504, 0), (7416578048, 5767168, 23068672, 0), (7405043712, 5767168, 28835840, 0), (7462715392, 5767168, 34603008, 0), (7468482560, 5767168, 40370176, 0), (7456948224, 5767168, 46137344, 0), (7480016896, 5767168, 51904512, 0), (7485784064, 5767168, 57671680, 0), (7474249728, 5767168, 63438848, 0), (7514619904, 5767168, 69206016, 0), (7520387072, 5767168, 74973184, 0), (7508852736, 5767168, 80740352, 0), (7601127424, 5767168, 86507520, 0), (7606894592, 5767168, 92274688, 0), (7595360256, 5767168, 98041856, 0), (7618428928, 5767168, 103809024, 0), (7624196096, 5767168, 109576192, 0), (7612661760, 5767168, 115343360, 0), (7635730432, 5767168, 121110528, 0), (7641497600, 5767168, 126877696, 0), (7629963264, 5767168, 132644864, 0), (7739539456, 5767168, 138412032, 0), (7745306624, 5767168, 144179200, 0), (7733772288, 5767168, 149946368, 0), (7860649984, 5767168, 155713536, 0), (7866417152, 5767168, 161480704, 0), (7854882816, 5767168, 167247872, 0), (7981760512, 5767168, 173015040, 0), (7987527680, 5767168, 178782208, 0), (7975993344, 5767168, 184549376, 0), (7999062016, 5767168, 190316544, 0), (8004829184, 5767168, 196083712, 0), (7993294848, 5767168, 201850880, 0), (8033665024, 5767168, 207618048, 0), (8039432192, 5767168, 213385216, 0), (8027897856, 5767168, 219152384, 0), (8050966528, 5767168, 224919552, 0), (8056733696, 5767168, 230686720, 0), (8045199360, 5767168, 236453888, 0), (8120172544, 5767168, 242221056, 0), (8125939712, 5767168, 247988224, 0), (8114405376, 5767168, 253755392, 0), (8172077056, 5767168, 259522560, 0), (8177844224, 5767168, 265289728, 0), (8166309888, 5767168, 271056896, 0), (8206680064, 5767168, 276824064, 0), (8212447232, 5767168, 282591232, 0), (8200912896, 5767168, 288358400, 0), (8258584576, 5767168, 294125568, 0), (8264351744, 5767168, 299892736, 0), (8252817408, 5767168, 305659904, 0), (8310489088, 5767168, 311427072, 0), (8316256256, 5767168, 317194240, 0), (8304721920, 5767168, 322961408, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (7393509376, 5767168, 17301504, 0), (7399276544, 5767168, 23068672, 0), (7387742208, 5767168, 28835840, 0), (7445413888, 5767168, 34603008, 0), (7451181056, 5767168, 40370176, 0), (7439646720, 5767168, 46137344, 0), (7653031936, 5767168, 51904512, 0), (7658799104, 5767168, 57671680, 0), (7647264768, 5767168, 63438848, 0), (7704936448, 5767168, 69206016, 0), (7710703616, 5767168, 74973184, 0), (7699169280, 5767168, 80740352, 0), (7756840960, 5767168, 86507520, 0), (7762608128, 5767168, 92274688, 0), (7751073792, 5767168, 98041856, 0), (7791443968, 5767168, 103809024, 0), (7797211136, 5767168, 109576192, 0), (7785676800, 5767168, 115343360, 0), (7826046976, 5767168, 121110528, 0), (7831814144, 5767168, 126877696, 0), (7820279808, 5767168, 132644864, 0), (7929856000, 5767168, 138412032, 0), (7935623168, 5767168, 144179200, 0), (7924088832, 5767168, 149946368, 0), (7947157504, 5767168, 155713536, 0), (7952924672, 5767168, 161480704, 0), (7941390336, 5767168, 167247872, 0), (8016363520, 5767168, 173015040, 0), (8022130688, 5767168, 178782208, 0), (8010596352, 5767168, 184549376, 0), (8085569536, 5767168, 190316544, 0), (8091336704, 5767168, 196083712, 0), (8079802368, 5767168, 201850880, 0), (8102871040, 5767168, 207618048, 0), (8108638208, 5767168, 213385216, 0), (8097103872, 5767168, 219152384, 0), (8137474048, 5767168, 224919552, 0), (8143241216, 5767168, 230686720, 0), (8131706880, 5767168, 236453888, 0), (8154775552, 5767168, 242221056, 0), (8160542720, 5767168, 247988224, 0), (8149008384, 5767168, 253755392, 0), (8223981568, 5767168, 259522560, 0), (8229748736, 5767168, 265289728, 0), (8218214400, 5767168, 271056896, 0), (8241283072, 5767168, 276824064, 0), (8247050240, 5767168, 282591232, 0), (8235515904, 5767168, 288358400, 0), (8293187584, 5767168, 294125568, 0), (8298954752, 5767168, 299892736, 0), (8287420416, 5767168, 305659904, 0), (8362393600, 5767168, 311427072, 0), (8368160768, 5767168, 317194240, 0), (8356626432, 5767168, 322961408, 0), (8379695104, 5767168, 328728576, 0), (8385462272, 5767168, 334495744, 0), (8373927936, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814080630>, 2: <capsule object NULL at 0x74a660728f90>}
DEBUG 01-15 16:08:52.540110.540110 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.540106.540106 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.540531.540531 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b1e4315a-db0c-4d59-bd14-8f966e206bb2
DEBUG 01-15 16:08:52.540240.540240 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.540076.540076 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.541676.541676 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:08:52.541816.541816 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b1e4315a-db0c-4d59-bd14-8f966e206bb2
DEBUG 01-15 16:08:52.542044.542044 cuda_h.py:19] end move_flatidxs cost 0.0008387565612792969 seconds
DEBUG 01-15 16:08:52.542919.542919 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:52.542821.542821 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0031342506408691406 seconds
DEBUG 01-15 16:08:52.542678.542678 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.546106.546106 cuda_h.py:19] end restore2model cost 0.003648042678833008 seconds
DEBUG 01-15 16:08:52.546955.546955 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007027387619018555 seconds
DEBUG 01-15 16:08:52.546850.546850 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.545955.545955 cuda_h.py:19] end group_tensors cost 0.003779172897338867 seconds
DEBUG 01-15 16:08:52.546067.546067 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.546910.546910 cuda_h.py:19] end gpu_sexperts cost 0.0005228519439697266 seconds
DEBUG 01-15 16:08:52.547384.547384 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.547030.547030 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1696090698242188e-05 seconds
DEBUG 01-15 16:08:52.547656.547656 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.547499.547499 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.549319.549319 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0018744468688964844 seconds
DEBUG 01-15 16:08:52.549112.549112 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.549027.549027 cuda_h.py:19] end group pad cost 0.0031125545501708984 seconds
DEBUG 01-15 16:08:52.549036.549036 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:52.549529.549529 cuda_h.py:19] end gpu_group_list cost 0.00030231475830078125 seconds
DEBUG 01-15 16:08:52.554564.554564 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.559187.559187 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0035762786865234375 seconds
DEBUG 01-15 16:08:52.559989.559989 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.559769.559769 cuda_h.py:19] end gpu_group_list cost 0.0003230571746826172 seconds
DEBUG 01-15 16:08:52.563963.563963 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.564371.564371 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b1e4315a-db0c-4d59-bd14-8f966e206bb2
INFO 01-15 16:08:52.576901.576901 client.py:127] Model loaded
DEBUG 01-15 16:08:52.576147.576147 cuda_h.py:19] end wait_experts_multi_device cost 0.011854410171508789 seconds
DEBUG 01-15 16:08:52.576823.576823 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.577112.577112 cuda_h.py:19] end group_einsum cost 0.027493715286254883 seconds
DEBUG 01-15 16:08:52.577256.577256 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:52.580204.580204 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029473304748535156 seconds
DEBUG 01-15 16:08:52.581864.581864 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04045438766479492 seconds
DEBUG 01-15 16:08:52.582058.582058 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005526542663574219 seconds
DEBUG 01-15 16:08:52.582789.582789 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.585444.585444 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.586933.586933 cuda_h.py:19] end index_scatter cost 0.00018548965454101562 seconds
DEBUG 01-15 16:08:52.586167.586167 cuda_h.py:19] end cpuoutputsdeal cost 0.004050493240356445 seconds
DEBUG 01-15 16:08:52.587737.587737 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.587687.587687 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.587425.587425 cuda_h.py:19] end gpu_group_tensor cost 0.0003809928894042969 seconds
DEBUG 01-15 16:08:52.587700.587700 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.588126.588126 cuda_h.py:19] end gpu_group_tensor cost 0.0003657341003417969 seconds
DEBUG 01-15 16:08:52.588208.588208 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.589130.589130 cuda_h.py:19] end gpu_group_einsum cost 0.0011258125305175781 seconds
DEBUG 01-15 16:08:52.589628.589628 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.590911.590911 cuda_h.py:19] end gpu_group_einsum cost 0.0008461475372314453 seconds
DEBUG 01-15 16:08:52.591441.591441 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.591435.591435 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.591179.591179 cuda_h.py:19] end all_expert_outputs_slices cost 0.0005714893341064453 seconds
DEBUG 01-15 16:08:52.592155.592155 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.592539.592539 cuda_h.py:19] end concat_expert_out cost 0.00010919570922851562 seconds
DEBUG 01-15 16:08:52.592219.592219 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.592537.592537 cuda_h.py:19] end index_scatter cost 0.00012350082397460938 seconds
DEBUG 01-15 16:08:52.593308.593308 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0018291473388671875 seconds
DEBUG 01-15 16:08:52.593175.593175 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.593835.593835 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.593627.593627 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004425048828125 seconds
DEBUG 01-15 16:08:52.593662.593662 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.594608.594608 cuda_h.py:19] end concat_expert_out cost 0.00010800361633300781 seconds
DEBUG 01-15 16:08:52.594679.594679 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.594261.594261 cuda_h.py:19] end index_scatter cost 0.0001125335693359375 seconds
DEBUG 01-15 16:08:52.594482.594482 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011858940124511719 seconds
DEBUG 01-15 16:08:52.594256.594256 cuda_h.py:19] end gpu_experts_multi_device cost 0.047333478927612305 seconds
DEBUG 01-15 16:08:52.594533.594533 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.05977225303649902 seconds
DEBUG 01-15 16:08:52.595358.595358 cuda_h.py:19] end prefill_layer cost 0.06868910789489746 seconds
DEBUG 01-15 16:08:52.595540.595540 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:08:52.595284.595284 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.595412.595412 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:08:52.595539.595539 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:08:52.595463.595463 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:08:52.596030.596030 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:08:52.596264.596264 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 0.00014638900756835938 seconds
DEBUG 01-15 16:08:52.596385.596385 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.596521.596521 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.596115.596115 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.596122.596122 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.597058.597058 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.597382.597382 cuda_h.py:19] end allocate_cuda_memory cost 0.0003342628479003906 seconds
DEBUG 01-15 16:08:52.597623.597623 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.597340.597340 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.597686.597686 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.597150.597150 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 95af7c23-0807-45c7-99be-f6ea33c3452c
DEBUG 01-15 16:08:52.597842.597842 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.598167.598167 cuda_h.py:10] start self_attn
INFO 01-15 16:08:52.599745.599745 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 95af7c23-0807-45c7-99be-f6ea33c3452c
DEBUG 01-15 16:08:52.599250.599250 cuda_h.py:19] end load_into_gpu_async cost 0.0020766258239746094 seconds
DEBUG 01-15 16:08:52.599284.599284 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.599036.599036 cuda_h.py:19] end restore_tensors2 cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:08:52.599792.599792 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0027434825897216797 seconds
INFO 01-15 16:08:52.599343.599343 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 95af7c23-0807-45c7-99be-f6ea33c3452c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.602636.602636 cuda_h.py:19] end self_attn cost 0.004446744918823242 seconds
DEBUG 01-15 16:08:52.603008.603008 cuda_h.py:19] end iln_self_attn_paln cost 0.006718873977661133 seconds
DEBUG 01-15 16:08:52.603997.603997 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:08:52.603965.603965 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.604870.604870 cuda_h.py:19] end gate cost 0.0008640289306640625 seconds
DEBUG 01-15 16:08:52.604104.604104 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.604815.604815 lmp.py:1912] 
DEBUG 01-15 16:08:52.604815.604815 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.604035.604035 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.604844.604844 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.604455.604455 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.604681.604681 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.605999.605999 lmp.py:1917] 
DEBUG 01-15 16:08:52.605999.605999 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.605033.605033 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.605273.605273 lmp.py:1935]   Expert  1 |     46 | CPU
DEBUG 01-15 16:08:52.605036.605036 lmp.py:1935]   Expert  7 |     60 | CPU
DEBUG 01-15 16:08:52.605646.605646 lmp.py:1935]   Expert 37 |     70 | CPU
DEBUG 01-15 16:08:52.605826.605826 lmp.py:1935]   Expert 54 |     76 | CPU
DEBUG 01-15 16:08:52.605337.605337 lmp.py:1935]   Expert 17 |     78 | CPU
DEBUG 01-15 16:08:52.605132.605132 lmp.py:1935]   Expert 18 |     84 | CPU
DEBUG 01-15 16:08:52.605166.605166 lmp.py:1935]   Expert  9 |     91 | CPU
DEBUG 01-15 16:08:52.605008.605008 lmp.py:1935]   Expert 13 |     92 | CPU
DEBUG 01-15 16:08:52.605850.605850 lmp.py:1935]   Expert 22 |    102 | CPU
DEBUG 01-15 16:08:52.605692.605692 lmp.py:1935]   Expert 58 |    102 | CPU
DEBUG 01-15 16:08:52.605295.605295 lmp.py:1935]   Expert  0 |    107 | CPU
DEBUG 01-15 16:08:52.605660.605660 lmp.py:1935]   Expert 26 |    117 | CPU
DEBUG 01-15 16:08:52.605548.605548 lmp.py:1935]   Expert 16 |    119 | CPU
DEBUG 01-15 16:08:52.605436.605436 lmp.py:1935]   Expert 10 |    122 | CPU
DEBUG 01-15 16:08:52.605994.605994 lmp.py:1935]   Expert 63 |    129 | CPU
DEBUG 01-15 16:08:52.605789.605789 lmp.py:1935]   Expert 59 |    131 | CPU
DEBUG 01-15 16:08:52.605922.605922 lmp.py:1935]   Expert 62 |    141 | CPU
DEBUG 01-15 16:08:52.605956.605956 lmp.py:1935]   Expert 43 |    143 | CPU
DEBUG 01-15 16:08:52.605275.605275 lmp.py:1935]   Expert 28 |    146 | CPU
DEBUG 01-15 16:08:52.605879.605879 lmp.py:1935]   Expert 33 |    147 | CPU
DEBUG 01-15 16:08:52.605005.605005 lmp.py:1935]   Expert 29 |    149 | CPU
DEBUG 01-15 16:08:52.605655.605655 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:08:52.605543.605543 lmp.py:1935]   Expert 51 |    162 | CPU
DEBUG 01-15 16:08:52.605193.605193 lmp.py:1935]   Expert 32 |    166 | CPU
DEBUG 01-15 16:08:52.605512.605512 lmp.py:1935]   Expert 45 |    166 | CPU
DEBUG 01-15 16:08:52.605214.605214 lmp.py:1935]   Expert 55 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.605917.605917 lmp.py:1935]   Expert  3 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.605859.605859 lmp.py:1935]   Expert 11 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.605085.605085 lmp.py:1935]   Expert 23 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.605880.605880 lmp.py:1935]   Expert 53 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.605960.605960 lmp.py:1935]   Expert 40 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.605279.605279 lmp.py:1935]   Expert 34 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.605121.605121 lmp.py:1935]   Expert 14 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.605440.605440 lmp.py:1935]   Expert 41 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.605474.605474 lmp.py:1935]   Expert 52 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.605746.605746 lmp.py:1935]   Expert 42 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.605210.605210 lmp.py:1935]   Expert 21 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606437.606437 lmp.py:1935]   Expert 57 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606947.606947 lmp.py:1935]   Expert 30 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606935.606935 lmp.py:1935]   Expert 15 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606207.606207 lmp.py:1935]   Expert 35 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606003.606003 lmp.py:1935]   Expert 12 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606275.606275 lmp.py:1935]   Expert  4 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606263.606263 lmp.py:1935]   Expert 19 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606794.606794 lmp.py:1935]   Expert 24 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606689.606689 lmp.py:1935]   Expert 46 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606961.606961 lmp.py:1935]   Expert 50 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606187.606187 lmp.py:1935]   Expert  8 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606744.606744 lmp.py:1935]   Expert 44 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606586.606586 lmp.py:1935]   Expert 49 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606667.606667 lmp.py:1935]   Expert 38 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606508.606508 lmp.py:1935]   Expert  6 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606589.606589 lmp.py:1935]   Expert 47 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606907.606907 lmp.py:1935]   Expert 31 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606988.606988 lmp.py:1935]   Expert 61 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.606499.606499 lmp.py:1935]   Expert 39 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606009.606009 lmp.py:1935]   Expert 36 |    304 | GPU1(cuda:1)
INFO 01-15 16:08:52.606098.606098 client.py:127] Model loaded
DEBUG 01-15 16:08:52.606371.606371 lmp.py:1935]   Expert  5 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.606485.606485 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.606387.606387 lmp.py:1935]   Expert 27 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.607271.607271 lmp.py:1935]   Expert 60 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.607696.607696 lmp.py:1935]   Expert 20 |    340 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.607684.607684 lmp.py:1935]   Expert 48 |    368 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.607672.607672 lmp.py:1935]   Expert 25 |    398 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.607706.607706 lmp.py:1935]   Expert 56 |    558 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.607501.607501 lmp.py:1937] 
DEBUG 01-15 16:08:52.607501.607501 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.607966.607966 lmp.py:1938]   CPU:   2902 tokens
DEBUG 01-15 16:08:52.607907.607907 lmp.py:1942]   cuda:1:   4625 tokens (19 experts)
DEBUG 01-15 16:08:52.607803.607803 lmp.py:1942]   cuda:2:   4761 tokens (20 experts)
DEBUG 01-15 16:08:52.607367.607367 lmp.py:1943]   Total GPU:   9386 tokens
DEBUG 01-15 16:08:52.607209.607209 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.607209.607209 lmp.py:1944] 
DEBUG 01-15 16:08:52.607402.607402 cuda_h.py:19] end experts_map_get cost 0.003121614456176758 seconds
DEBUG 01-15 16:08:52.607154.607154 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:52.607567.607567 lmp.py:1953] 
DEBUG 01-15 16:08:52.607567.607567 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.607570.607570 cuda_h.py:19] end cpu_experts_submit cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:08:52.607571.607571 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.607680.607680 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.608823.608823 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.607067.607067 cuda_h.py:19] end restore2model cost 0.0010788440704345703 seconds
DEBUG 01-15 16:08:52.608297.608297 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:52.608255.608255 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.4.gate_proj.weight': 0, 'model.layers.6.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.6.mlp.experts.3.gate_proj.weight': 0, 'model.layers.6.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.6.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.6.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.6.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(8466202624, 5767168, 0, 0), (8471969792, 5767168, 5767168, 0), (8460435456, 5767168, 11534336, 0), (8535408640, 5767168, 17301504, 0), (8541175808, 5767168, 23068672, 0), (8529641472, 5767168, 28835840, 0), (8587313152, 5767168, 34603008, 0), (8593080320, 5767168, 40370176, 0), (8581545984, 5767168, 46137344, 0), (8639217664, 5767168, 51904512, 0), (8644984832, 5767168, 57671680, 0), (8633450496, 5767168, 63438848, 0), (8725725184, 5767168, 69206016, 0), (8731492352, 5767168, 74973184, 0), (8719958016, 5767168, 80740352, 0), (8743026688, 5767168, 86507520, 0), (8748793856, 5767168, 92274688, 0), (8737259520, 5767168, 98041856, 0), (8760328192, 5767168, 103809024, 0), (8766095360, 5767168, 109576192, 0), (8754561024, 5767168, 115343360, 0), (8864137216, 5767168, 121110528, 0), (8869904384, 5767168, 126877696, 0), (8858370048, 5767168, 132644864, 0), (8916041728, 5767168, 138412032, 0), (8921808896, 5767168, 144179200, 0), (8910274560, 5767168, 149946368, 0), (9002549248, 5767168, 155713536, 0), (9008316416, 5767168, 161480704, 0), (8996782080, 5767168, 167247872, 0), (9019850752, 5767168, 173015040, 0), (9025617920, 5767168, 178782208, 0), (9014083584, 5767168, 184549376, 0), (9054453760, 5767168, 190316544, 0), (9060220928, 5767168, 196083712, 0), (9048686592, 5767168, 201850880, 0), (9089056768, 5767168, 207618048, 0), (9094823936, 5767168, 213385216, 0), (9083289600, 5767168, 219152384, 0), (9106358272, 5767168, 224919552, 0), (9112125440, 5767168, 230686720, 0), (9100591104, 5767168, 236453888, 0), (9192865792, 5767168, 242221056, 0), (9198632960, 5767168, 247988224, 0), (9187098624, 5767168, 253755392, 0), (9210167296, 5767168, 259522560, 0), (9215934464, 5767168, 265289728, 0), (9204400128, 5767168, 271056896, 0), (9313976320, 5767168, 276824064, 0), (9319743488, 5767168, 282591232, 0), (9308209152, 5767168, 288358400, 0), (9365880832, 5767168, 294125568, 0), (9371648000, 5767168, 299892736, 0), (9360113664, 5767168, 305659904, 0), (9452388352, 5767168, 311427072, 0), (9458155520, 5767168, 317194240, 0), (9446621184, 5767168, 322961408, 0)], 2: [(8448901120, 5767168, 0, 0), (8454668288, 5767168, 5767168, 0), (8443133952, 5767168, 11534336, 0), (8483504128, 5767168, 17301504, 0), (8489271296, 5767168, 23068672, 0), (8477736960, 5767168, 28835840, 0), (8500805632, 5767168, 34603008, 0), (8506572800, 5767168, 40370176, 0), (8495038464, 5767168, 46137344, 0), (8604614656, 5767168, 51904512, 0), (8610381824, 5767168, 57671680, 0), (8598847488, 5767168, 63438848, 0), (8656519168, 5767168, 69206016, 0), (8662286336, 5767168, 74973184, 0), (8650752000, 5767168, 80740352, 0), (8794931200, 5767168, 86507520, 0), (8800698368, 5767168, 92274688, 0), (8789164032, 5767168, 98041856, 0), (8812232704, 5767168, 103809024, 0), (8817999872, 5767168, 109576192, 0), (8806465536, 5767168, 115343360, 0), (8829534208, 5767168, 121110528, 0), (8835301376, 5767168, 126877696, 0), (8823767040, 5767168, 132644864, 0), (8933343232, 5767168, 138412032, 0), (8939110400, 5767168, 144179200, 0), (8927576064, 5767168, 149946368, 0), (8985247744, 5767168, 155713536, 0), (8991014912, 5767168, 161480704, 0), (8979480576, 5767168, 167247872, 0), (9071755264, 5767168, 173015040, 0), (9077522432, 5767168, 178782208, 0), (9065988096, 5767168, 184549376, 0), (9123659776, 5767168, 190316544, 0), (9129426944, 5767168, 196083712, 0), (9117892608, 5767168, 201850880, 0), (9158262784, 5767168, 207618048, 0), (9164029952, 5767168, 213385216, 0), (9152495616, 5767168, 219152384, 0), (9227468800, 5767168, 224919552, 0), (9233235968, 5767168, 230686720, 0), (9221701632, 5767168, 236453888, 0), (9244770304, 5767168, 242221056, 0), (9250537472, 5767168, 247988224, 0), (9239003136, 5767168, 253755392, 0), (9262071808, 5767168, 259522560, 0), (9267838976, 5767168, 265289728, 0), (9256304640, 5767168, 271056896, 0), (9296674816, 5767168, 276824064, 0), (9302441984, 5767168, 282591232, 0), (9290907648, 5767168, 288358400, 0), (9348579328, 5767168, 294125568, 0), (9354346496, 5767168, 299892736, 0), (9342812160, 5767168, 305659904, 0), (9383182336, 5767168, 311427072, 0), (9388949504, 5767168, 317194240, 0), (9377415168, 5767168, 322961408, 0), (9435086848, 5767168, 328728576, 0), (9440854016, 5767168, 334495744, 0), (9429319680, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729110>, 2: <capsule object NULL at 0x74a660729410>}
DEBUG 01-15 16:08:52.608139.608139 cuda_h.py:19] end sllm_worker_task cost 0.012184381484985352 seconds
DEBUG 01-15 16:08:52.609898.609898 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.609140.609140 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.609704.609704 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7da346d3-cfc0-4ca7-be12-b102a7321346
DEBUG 01-15 16:08:52.609122.609122 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.609057.609057 cuda_h.py:19] end move_flatidxs cost 0.0008783340454101562 seconds
DEBUG 01-15 16:08:52.609569.609569 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:52.610342.610342 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7da346d3-cfc0-4ca7-be12-b102a7321346
DEBUG 01-15 16:08:52.611493.611493 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0031201839447021484 seconds
DEBUG 01-15 16:08:52.611541.611541 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.615154.615154 cuda_h.py:19] end restore2model cost 0.003820180892944336 seconds
DEBUG 01-15 16:08:52.615143.615143 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0075528621673583984 seconds
DEBUG 01-15 16:08:52.615820.615820 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.614385.614385 cuda_h.py:19] end group_tensors cost 0.004942417144775391 seconds
DEBUG 01-15 16:08:52.615685.615685 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.615693.615693 cuda_h.py:19] end gpu_sexperts cost 0.0003287792205810547 seconds
DEBUG 01-15 16:08:52.615430.615430 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.615067.615067 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:08:52.615850.615850 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.615673.615673 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.617643.617643 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0020294189453125 seconds
DEBUG 01-15 16:08:52.618118.618118 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.618091.618091 cuda_h.py:19] end gpu_group_list cost 0.00032448768615722656 seconds
DEBUG 01-15 16:08:52.618381.618381 cuda_h.py:19] end group pad cost 0.0033309459686279297 seconds
DEBUG 01-15 16:08:52.619569.619569 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:52.622379.622379 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.628346.628346 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.005850315093994141 seconds
DEBUG 01-15 16:08:52.628658.628658 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.629255.629255 cuda_h.py:19] end gpu_group_list cost 0.000362396240234375 seconds
DEBUG 01-15 16:08:52.634864.634864 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.635594.635594 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7da346d3-cfc0-4ca7-be12-b102a7321346
INFO 01-15 16:08:52.647400.647400 client.py:127] Model loaded
DEBUG 01-15 16:08:52.647379.647379 cuda_h.py:19] end wait_experts_multi_device cost 0.012118101119995117 seconds
DEBUG 01-15 16:08:52.647049.647049 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.648669.648669 cuda_h.py:19] end group_einsum cost 0.029198169708251953 seconds
DEBUG 01-15 16:08:52.648283.648283 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:52.651041.651041 cuda_h.py:19] end get_outputs_cpu1 cost 0.00281524658203125 seconds
DEBUG 01-15 16:08:52.652528.652528 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.043517351150512695 seconds
DEBUG 01-15 16:08:52.653704.653704 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00541996955871582 seconds
DEBUG 01-15 16:08:52.653631.653631 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.655234.655234 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.655808.655808 cuda_h.py:19] end index_scatter cost 0.00014853477478027344 seconds
DEBUG 01-15 16:08:52.656264.656264 cuda_h.py:19] end cpuoutputsdeal cost 0.0029892921447753906 seconds
DEBUG 01-15 16:08:52.656131.656131 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.656684.656684 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.657904.657904 cuda_h.py:19] end gpu_group_tensor cost 0.00033974647521972656 seconds
DEBUG 01-15 16:08:52.657630.657630 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.657925.657925 cuda_h.py:19] end gpu_group_tensor cost 0.00025963783264160156 seconds
DEBUG 01-15 16:08:52.657137.657137 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.658033.658033 cuda_h.py:19] end gpu_group_einsum cost 0.0009520053863525391 seconds
DEBUG 01-15 16:08:52.659365.659365 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.659896.659896 cuda_h.py:19] end gpu_group_einsum cost 0.0007486343383789062 seconds
DEBUG 01-15 16:08:52.660830.660830 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.660333.660333 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.660714.660714 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004076957702636719 seconds
DEBUG 01-15 16:08:52.660240.660240 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.661748.661748 cuda_h.py:19] end concat_expert_out cost 9.5367431640625e-05 seconds
DEBUG 01-15 16:08:52.661455.661455 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.661037.661037 cuda_h.py:19] end index_scatter cost 0.00011229515075683594 seconds
DEBUG 01-15 16:08:52.661091.661091 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0015769004821777344 seconds
DEBUG 01-15 16:08:52.661461.661461 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.662856.662856 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.662331.662331 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002753734588623047 seconds
DEBUG 01-15 16:08:52.662843.662843 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.662868.662868 cuda_h.py:19] end concat_expert_out cost 9.72747802734375e-05 seconds
DEBUG 01-15 16:08:52.662330.662330 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.662514.662514 cuda_h.py:19] end index_scatter cost 9.417533874511719e-05 seconds
DEBUG 01-15 16:08:52.662768.662768 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009970664978027344 seconds
DEBUG 01-15 16:08:52.663369.663369 cuda_h.py:19] end gpu_experts_multi_device cost 0.04729342460632324 seconds
DEBUG 01-15 16:08:52.663017.663017 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.060076236724853516 seconds
DEBUG 01-15 16:08:52.663191.663191 cuda_h.py:19] end prefill_layer cost 0.06815791130065918 seconds
DEBUG 01-15 16:08:52.664460.664460 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:08:52.664866.664866 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.664510.664510 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:08:52.664492.664492 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:08:52.664633.664633 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:08:52.664393.664393 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 8.082389831542969e-05 seconds
DEBUG 01-15 16:08:52.664309.664309 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 0.00016546249389648438 seconds
DEBUG 01-15 16:08:52.664662.664662 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.664501.664501 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.664423.664423 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.665446.665446 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.665581.665581 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.665215.665215 cuda_h.py:19] end allocate_cuda_memory cost 0.00043010711669921875 seconds
DEBUG 01-15 16:08:52.666142.666142 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.666702.666702 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.666547.666547 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.666755.666755 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 037a38f7-48f1-4f49-9f62-da6fa1a3a7a6
DEBUG 01-15 16:08:52.666001.666001 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.667482.667482 cuda_h.py:10] start self_attn
INFO 01-15 16:08:52.668596.668596 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 037a38f7-48f1-4f49-9f62-da6fa1a3a7a6
DEBUG 01-15 16:08:52.668834.668834 cuda_h.py:19] end load_into_gpu_async cost 0.0027627944946289062 seconds
DEBUG 01-15 16:08:52.668327.668327 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.669116.669116 cuda_h.py:19] end restore_tensors2 cost 0.00015211105346679688 seconds
DEBUG 01-15 16:08:52.669557.669557 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004063606262207031 seconds
INFO 01-15 16:08:52.669045.669045 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 037a38f7-48f1-4f49-9f62-da6fa1a3a7a6
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.672411.672411 cuda_h.py:19] end self_attn cost 0.005681514739990234 seconds
DEBUG 01-15 16:08:52.673574.673574 cuda_h.py:19] end iln_self_attn_paln cost 0.00879359245300293 seconds
DEBUG 01-15 16:08:52.673670.673670 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:08:52.673830.673830 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.674332.674332 cuda_h.py:19] end gate cost 0.0008809566497802734 seconds
DEBUG 01-15 16:08:52.674996.674996 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.675595.675595 lmp.py:1912] 
DEBUG 01-15 16:08:52.675595.675595 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.675425.675425 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.675426.675426 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.675195.675195 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.675375.675375 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.675171.675171 lmp.py:1917] 
DEBUG 01-15 16:08:52.675171.675171 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.675443.675443 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.675106.675106 lmp.py:1935]   Expert 50 |     44 | CPU
DEBUG 01-15 16:08:52.675617.675617 lmp.py:1935]   Expert  3 |     53 | CPU
DEBUG 01-15 16:08:52.675174.675174 lmp.py:1935]   Expert 46 |     54 | CPU
DEBUG 01-15 16:08:52.675255.675255 lmp.py:1935]   Expert  1 |     75 | CPU
DEBUG 01-15 16:08:52.675812.675812 lmp.py:1935]   Expert  4 |     87 | CPU
DEBUG 01-15 16:08:52.675654.675654 lmp.py:1935]   Expert 29 |     89 | CPU
DEBUG 01-15 16:08:52.675019.675019 lmp.py:1935]   Expert 15 |     96 | CPU
DEBUG 01-15 16:08:52.675337.675337 lmp.py:1935]   Expert 40 |     96 | CPU
DEBUG 01-15 16:08:52.675279.675279 lmp.py:1935]   Expert  8 |    109 | CPU
DEBUG 01-15 16:08:52.675789.675789 lmp.py:1935]   Expert 28 |    112 | CPU
DEBUG 01-15 16:08:52.675923.675923 lmp.py:1935]   Expert 41 |    113 | CPU
DEBUG 01-15 16:08:52.675864.675864 lmp.py:1935]   Expert 16 |    126 | CPU
DEBUG 01-15 16:08:52.675044.675044 lmp.py:1935]   Expert 27 |    127 | CPU
DEBUG 01-15 16:08:52.675462.675462 lmp.py:1935]   Expert 48 |    128 | CPU
DEBUG 01-15 16:08:52.675926.675926 lmp.py:1935]   Expert  6 |    130 | CPU
DEBUG 01-15 16:08:52.675821.675821 lmp.py:1935]   Expert  7 |    132 | CPU
DEBUG 01-15 16:08:52.675240.675240 lmp.py:1935]   Expert 13 |    132 | CPU
DEBUG 01-15 16:08:52.675658.675658 lmp.py:1935]   Expert 54 |    133 | CPU
DEBUG 01-15 16:08:52.675791.675791 lmp.py:1935]   Expert 51 |    137 | CPU
DEBUG 01-15 16:08:52.675971.675971 lmp.py:1935]   Expert 39 |    139 | CPU
DEBUG 01-15 16:08:52.675435.675435 lmp.py:1935]   Expert 60 |    140 | CPU
DEBUG 01-15 16:08:52.676377.676377 lmp.py:1935]   Expert 18 |    141 | CPU
DEBUG 01-15 16:08:52.676079.676079 lmp.py:1935]   Expert 14 |    145 | CPU
DEBUG 01-15 16:08:52.676782.676782 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:08:52.676008.676008 lmp.py:1935]   Expert 52 |    147 | CPU
DEBUG 01-15 16:08:52.676241.676241 lmp.py:1935]   Expert 56 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676044.676044 lmp.py:1935]   Expert 20 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676561.676561 lmp.py:1935]   Expert 55 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676125.676125 lmp.py:1935]   Expert 36 |    153 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676974.676974 lmp.py:1935]   Expert 10 |    156 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676584.676584 lmp.py:1935]   Expert 45 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676717.676717 lmp.py:1935]   Expert 11 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676089.676089 lmp.py:1935]   Expert  5 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676415.676415 lmp.py:1935]   Expert 62 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676277.676277 lmp.py:1935]   Expert 57 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676318.676318 lmp.py:1935]   Expert 33 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676166.676166 lmp.py:1935]   Expert 44 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676300.676300 lmp.py:1935]   Expert 25 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676910.676910 lmp.py:1935]   Expert 58 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676328.676328 lmp.py:1935]   Expert 53 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676938.676938 lmp.py:1935]   Expert  2 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676833.676833 lmp.py:1935]   Expert 32 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676159.676159 lmp.py:1935]   Expert 35 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676723.676723 lmp.py:1935]   Expert 31 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676333.676333 lmp.py:1935]   Expert 21 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676181.676181 lmp.py:1935]   Expert 63 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676315.676315 lmp.py:1935]   Expert 49 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676971.676971 lmp.py:1935]   Expert 17 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676866.676866 lmp.py:1935]   Expert 42 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676000.676000 lmp.py:1935]   Expert 34 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676133.676133 lmp.py:1935]   Expert 37 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676459.676459 lmp.py:1935]   Expert 59 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676784.676784 lmp.py:1935]   Expert 22 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.676586.676586 lmp.py:1935]   Expert  0 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676435.676435 lmp.py:1935]   Expert 19 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.676568.676568 lmp.py:1935]   Expert 24 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.677987.677987 lmp.py:1935]   Expert 61 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.677597.677597 lmp.py:1935]   Expert 30 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.677969.677969 lmp.py:1935]   Expert 47 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.677102.677102 lmp.py:1935]   Expert 38 |    368 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.677997.677997 lmp.py:1935]   Expert 26 |    376 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.677607.677607 lmp.py:1935]   Expert 12 |    427 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.677217.677217 lmp.py:1935]   Expert  9 |    676 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.677828.677828 lmp.py:1935]   Expert 23 |    703 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.677769.677769 lmp.py:1937] 
DEBUG 01-15 16:08:52.677769.677769 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.677949.677949 lmp.py:1938]   CPU:   2832 tokens
DEBUG 01-15 16:08:52.677605.677605 lmp.py:1942]   cuda:1:   4802 tokens (20 experts)
DEBUG 01-15 16:08:52.677262.677262 lmp.py:1942]   cuda:2:   4654 tokens (19 experts)
DEBUG 01-15 16:08:52.677965.677965 lmp.py:1943]   Total GPU:   9456 tokens
DEBUG 01-15 16:08:52.677906.677906 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.677906.677906 lmp.py:1944] 
DEBUG 01-15 16:08:52.677192.677192 cuda_h.py:19] end experts_map_get cost 0.0028426647186279297 seconds
INFO 01-15 16:08:52.677250.677250 client.py:127] Model loaded
DEBUG 01-15 16:08:52.677428.677428 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.677635.677635 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:52.678678.678678 lmp.py:1953] 
DEBUG 01-15 16:08:52.678678.678678 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.678833.678833 cuda_h.py:19] end cpu_experts_submit cost 8.106231689453125e-05 seconds
DEBUG 01-15 16:08:52.678735.678735 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.678619.678619 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.679170.679170 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.17.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.17.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.17.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.38.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.38.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.38.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.7.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.7.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.7.mlp.experts.62.up_proj.weight': 340262912}, 2: {'model.layers.7.mlp.experts.5.gate_proj.weight': 0, 'model.layers.7.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.9.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.9.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.9.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (9538895872, 5767168, 17301504, 0), (9544663040, 5767168, 23068672, 0), (9533128704, 5767168, 28835840, 0), (9798418432, 5767168, 34603008, 0), (9804185600, 5767168, 40370176, 0), (9792651264, 5767168, 46137344, 0), (9833021440, 5767168, 51904512, 0), (9838788608, 5767168, 57671680, 0), (9827254272, 5767168, 63438848, 0), (9850322944, 5767168, 69206016, 0), (9856090112, 5767168, 74973184, 0), (9844555776, 5767168, 80740352, 0), (9867624448, 5767168, 86507520, 0), (9873391616, 5767168, 92274688, 0), (9861857280, 5767168, 98041856, 0), (9902227456, 5767168, 103809024, 0), (9907994624, 5767168, 109576192, 0), (9896460288, 5767168, 115343360, 0), (9954131968, 5767168, 121110528, 0), (9959899136, 5767168, 126877696, 0), (9948364800, 5767168, 132644864, 0), (10075242496, 5767168, 138412032, 0), (10081009664, 5767168, 144179200, 0), (10069475328, 5767168, 149946368, 0), (10092544000, 5767168, 155713536, 0), (10098311168, 5767168, 161480704, 0), (10086776832, 5767168, 167247872, 0), (10109845504, 5767168, 173015040, 0), (10115612672, 5767168, 178782208, 0), (10104078336, 5767168, 184549376, 0), (10127147008, 5767168, 190316544, 0), (10132914176, 5767168, 196083712, 0), (10121379840, 5767168, 201850880, 0), (10161750016, 5767168, 207618048, 0), (10167517184, 5767168, 213385216, 0), (10155982848, 5767168, 219152384, 0), (10265559040, 5767168, 224919552, 0), (10271326208, 5767168, 230686720, 0), (10259791872, 5767168, 236453888, 0), (10282860544, 5767168, 242221056, 0), (10288627712, 5767168, 247988224, 0), (10277093376, 5767168, 253755392, 0), (10421272576, 5767168, 259522560, 0), (10427039744, 5767168, 265289728, 0), (10415505408, 5767168, 271056896, 0), (10473177088, 5767168, 276824064, 0), (10478944256, 5767168, 282591232, 0), (10467409920, 5767168, 288358400, 0), (10525081600, 5767168, 294125568, 0), (10530848768, 5767168, 299892736, 0), (10519314432, 5767168, 305659904, 0), (10559684608, 5767168, 311427072, 0), (10565451776, 5767168, 317194240, 0), (10553917440, 5767168, 322961408, 0), (10576986112, 5767168, 328728576, 0), (10582753280, 5767168, 334495744, 0), (10571218944, 5767168, 340262912, 0)], 2: [(9590800384, 5767168, 0, 0), (9596567552, 5767168, 5767168, 0), (9585033216, 5767168, 11534336, 0), (9660006400, 5767168, 17301504, 0), (9665773568, 5767168, 23068672, 0), (9654239232, 5767168, 28835840, 0), (9677307904, 5767168, 34603008, 0), (9683075072, 5767168, 40370176, 0), (9671540736, 5767168, 46137344, 0), (9694609408, 5767168, 51904512, 0), (9700376576, 5767168, 57671680, 0), (9688842240, 5767168, 63438848, 0), (9711910912, 5767168, 69206016, 0), (9717678080, 5767168, 74973184, 0), (9706143744, 5767168, 80740352, 0), (9884925952, 5767168, 86507520, 0), (9890693120, 5767168, 92274688, 0), (9879158784, 5767168, 98041856, 0), (9919528960, 5767168, 103809024, 0), (9925296128, 5767168, 109576192, 0), (9913761792, 5767168, 115343360, 0), (9936830464, 5767168, 121110528, 0), (9942597632, 5767168, 126877696, 0), (9931063296, 5767168, 132644864, 0), (10023337984, 5767168, 138412032, 0), (10029105152, 5767168, 144179200, 0), (10017570816, 5767168, 149946368, 0), (10040639488, 5767168, 155713536, 0), (10046406656, 5767168, 161480704, 0), (10034872320, 5767168, 167247872, 0), (10057940992, 5767168, 173015040, 0), (10063708160, 5767168, 178782208, 0), (10052173824, 5767168, 184549376, 0), (10144448512, 5767168, 190316544, 0), (10150215680, 5767168, 196083712, 0), (10138681344, 5767168, 201850880, 0), (10230956032, 5767168, 207618048, 0), (10236723200, 5767168, 213385216, 0), (10225188864, 5767168, 219152384, 0), (10317463552, 5767168, 224919552, 0), (10323230720, 5767168, 230686720, 0), (10311696384, 5767168, 236453888, 0), (10352066560, 5767168, 242221056, 0), (10357833728, 5767168, 247988224, 0), (10346299392, 5767168, 253755392, 0), (10455875584, 5767168, 259522560, 0), (10461642752, 5767168, 265289728, 0), (10450108416, 5767168, 271056896, 0), (10490478592, 5767168, 276824064, 0), (10496245760, 5767168, 282591232, 0), (10484711424, 5767168, 288358400, 0), (10507780096, 5767168, 294125568, 0), (10513547264, 5767168, 299892736, 0), (10502012928, 5767168, 305659904, 0), (10594287616, 5767168, 311427072, 0), (10600054784, 5767168, 317194240, 0), (10588520448, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6607291d0>, 2: <capsule object NULL at 0x74a660729a70>}
DEBUG 01-15 16:08:52.679850.679850 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.679329.679329 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.679006.679006 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f8c5763b-de7b-481d-b9ed-4c6165fa9501
DEBUG 01-15 16:08:52.680648.680648 cuda_h.py:19] end restore2model cost 0.002720355987548828 seconds
DEBUG 01-15 16:08:52.680138.680138 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.681179.681179 cuda_h.py:19] end sllm_worker_task cost 0.01656031608581543 seconds
DEBUG 01-15 16:08:52.682305.682305 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.683339.683339 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:08:52.683109.683109 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f8c5763b-de7b-481d-b9ed-4c6165fa9501
DEBUG 01-15 16:08:52.684094.684094 cuda_h.py:19] end move_flatidxs cost 0.0010342597961425781 seconds
DEBUG 01-15 16:08:52.684880.684880 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:52.684050.684050 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.006090879440307617 seconds
DEBUG 01-15 16:08:52.684689.684689 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.690080.690080 cuda_h.py:19] end restore2model cost 0.005459308624267578 seconds
DEBUG 01-15 16:08:52.690593.690593 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.011934280395507812 seconds
DEBUG 01-15 16:08:52.690131.690131 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.690578.690578 cuda_h.py:19] end gpu_sexperts cost 0.0004487037658691406 seconds
DEBUG 01-15 16:08:52.690402.690402 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.690372.690372 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.956390380859375e-05 seconds
DEBUG 01-15 16:08:52.690618.690618 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.690447.690447 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.692512.692512 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010890960693359375 seconds
DEBUG 01-15 16:08:52.692892.692892 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.692550.692550 cuda_h.py:19] end gpu_group_list cost 0.00024247169494628906 seconds
DEBUG 01-15 16:08:52.693163.693163 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.693285.693285 cuda_h.py:19] end group_tensors cost 0.008825063705444336 seconds
DEBUG 01-15 16:08:52.693110.693110 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.694320.694320 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012178421020507812 seconds
DEBUG 01-15 16:08:52.694263.694263 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.695918.695918 cuda_h.py:19] end gpu_group_list cost 0.00033402442932128906 seconds
DEBUG 01-15 16:08:52.696964.696964 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.696444.696444 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f8c5763b-de7b-481d-b9ed-4c6165fa9501
DEBUG 01-15 16:08:52.698230.698230 cuda_h.py:19] end group pad cost 0.0043027400970458984 seconds
DEBUG 01-15 16:08:52.698286.698286 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:52.720834.720834 client.py:127] Model loaded
DEBUG 01-15 16:08:52.720866.720866 cuda_h.py:19] end wait_experts_multi_device cost 0.023856401443481445 seconds
DEBUG 01-15 16:08:52.720013.720013 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.726313.726313 cuda_h.py:19] end group_einsum cost 0.027800559997558594 seconds
DEBUG 01-15 16:08:52.726530.726530 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:52.729262.729262 cuda_h.py:19] end get_outputs_cpu1 cost 0.002825498580932617 seconds
DEBUG 01-15 16:08:52.730928.730928 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04770183563232422 seconds
DEBUG 01-15 16:08:52.731745.731745 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010701179504394531 seconds
DEBUG 01-15 16:08:52.731116.731116 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.733860.733860 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.733241.733241 cuda_h.py:19] end index_scatter cost 0.00015020370483398438 seconds
DEBUG 01-15 16:08:52.734505.734505 cuda_h.py:19] end cpuoutputsdeal cost 0.0029916763305664062 seconds
DEBUG 01-15 16:08:52.734875.734875 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.734997.734997 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.735673.735673 cuda_h.py:19] end gpu_group_tensor cost 0.0003407001495361328 seconds
DEBUG 01-15 16:08:52.735286.735286 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.735306.735306 cuda_h.py:19] end gpu_group_tensor cost 0.00031757354736328125 seconds
DEBUG 01-15 16:08:52.735764.735764 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.737306.737306 cuda_h.py:19] end gpu_group_einsum cost 0.001226186752319336 seconds
DEBUG 01-15 16:08:52.737513.737513 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.739333.739333 cuda_h.py:19] end gpu_group_einsum cost 0.0016109943389892578 seconds
DEBUG 01-15 16:08:52.739001.739001 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.739602.739602 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.739807.739807 cuda_h.py:19] end all_expert_outputs_slices cost 0.00029349327087402344 seconds
DEBUG 01-15 16:08:52.739981.739981 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.740608.740608 cuda_h.py:19] end concat_expert_out cost 8.940696716308594e-05 seconds
DEBUG 01-15 16:08:52.740843.740843 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.740339.740339 cuda_h.py:19] end index_scatter cost 9.34600830078125e-05 seconds
DEBUG 01-15 16:08:52.740139.740139 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012845993041992188 seconds
DEBUG 01-15 16:08:52.740681.740681 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.740333.740333 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.741574.741574 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023293495178222656 seconds
DEBUG 01-15 16:08:52.741689.741689 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.741501.741501 cuda_h.py:19] end concat_expert_out cost 8.344650268554688e-05 seconds
DEBUG 01-15 16:08:52.741260.741260 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.741496.741496 cuda_h.py:19] end index_scatter cost 7.867813110351562e-05 seconds
DEBUG 01-15 16:08:52.741439.741439 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008099079132080078 seconds
DEBUG 01-15 16:08:52.741681.741681 cuda_h.py:19] end gpu_experts_multi_device cost 0.05088162422180176 seconds
DEBUG 01-15 16:08:52.741519.741519 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.06845331192016602 seconds
DEBUG 01-15 16:08:52.742194.742194 cuda_h.py:19] end prefill_layer cost 0.07838010787963867 seconds
DEBUG 01-15 16:08:52.742210.742210 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:08:52.742887.742887 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.742233.742233 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:08:52.742625.742625 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:08:52.742362.742362 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:08:52.742703.742703 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 5.7220458984375e-05 seconds
DEBUG 01-15 16:08:52.742348.742348 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 0.00011563301086425781 seconds
DEBUG 01-15 16:08:52.743302.743302 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.743717.743717 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.743328.743328 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.743152.743152 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.743737.743737 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.744563.744563 cuda_h.py:19] end allocate_cuda_memory cost 0.0004298686981201172 seconds
DEBUG 01-15 16:08:52.744153.744153 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.744667.744667 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.744308.744308 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.744511.744511 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ec24d243-1eb6-48d7-845e-813dbb962f30
DEBUG 01-15 16:08:52.745545.745545 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.745500.745500 cuda_h.py:10] start self_attn
INFO 01-15 16:08:52.747455.747455 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ec24d243-1eb6-48d7-845e-813dbb962f30
DEBUG 01-15 16:08:52.747871.747871 cuda_h.py:19] end load_into_gpu_async cost 0.003309011459350586 seconds
DEBUG 01-15 16:08:52.748623.748623 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.748525.748525 cuda_h.py:19] end restore_tensors2 cost 0.00016260147094726562 seconds
DEBUG 01-15 16:08:52.748298.748298 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004745960235595703 seconds
INFO 01-15 16:08:52.748071.748071 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ec24d243-1eb6-48d7-845e-813dbb962f30
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.751985.751985 cuda_h.py:19] end self_attn cost 0.005858182907104492 seconds
DEBUG 01-15 16:08:52.751033.751033 cuda_h.py:19] end iln_self_attn_paln cost 0.0089111328125 seconds
DEBUG 01-15 16:08:52.752876.752876 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:08:52.752222.752222 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.752363.752363 cuda_h.py:19] end gate cost 0.0007977485656738281 seconds
DEBUG 01-15 16:08:52.752352.752352 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.753939.753939 lmp.py:1912] 
DEBUG 01-15 16:08:52.753939.753939 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.753086.753086 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.753664.753664 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.753142.753142 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.753268.753268 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.753011.753011 lmp.py:1917] 
DEBUG 01-15 16:08:52.753011.753011 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.753230.753230 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.753602.753602 lmp.py:1935]   Expert 38 |     12 | CPU
DEBUG 01-15 16:08:52.753298.753298 lmp.py:1935]   Expert 39 |     58 | CPU
DEBUG 01-15 16:08:52.753040.753040 lmp.py:1935]   Expert  7 |     72 | CPU
DEBUG 01-15 16:08:52.753021.753021 lmp.py:1935]   Expert 30 |     73 | CPU
DEBUG 01-15 16:08:52.753194.753194 lmp.py:1935]   Expert 14 |     93 | CPU
DEBUG 01-15 16:08:52.753652.753652 lmp.py:1935]   Expert 24 |     93 | CPU
DEBUG 01-15 16:08:52.753632.753632 lmp.py:1935]   Expert 27 |     94 | CPU
DEBUG 01-15 16:08:52.753852.753852 lmp.py:1935]   Expert 17 |     96 | CPU
DEBUG 01-15 16:08:52.753594.753594 lmp.py:1935]   Expert 36 |     96 | CPU
DEBUG 01-15 16:08:52.753860.753860 lmp.py:1935]   Expert 40 |    101 | CPU
DEBUG 01-15 16:08:52.753887.753887 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:08:52.753676.753676 lmp.py:1935]   Expert 32 |    106 | CPU
DEBUG 01-15 16:08:52.753464.753464 lmp.py:1935]   Expert 18 |    108 | CPU
DEBUG 01-15 16:08:52.754730.754730 lmp.py:1935]   Expert 48 |    111 | CPU
DEBUG 01-15 16:08:52.754923.754923 lmp.py:1935]   Expert 12 |    114 | CPU
DEBUG 01-15 16:08:52.754619.754619 lmp.py:1935]   Expert  1 |    116 | CPU
DEBUG 01-15 16:08:52.754554.754554 lmp.py:1935]   Expert  6 |    125 | CPU
DEBUG 01-15 16:08:52.754727.754727 lmp.py:1935]   Expert 59 |    129 | CPU
DEBUG 01-15 16:08:52.754423.754423 lmp.py:1935]   Expert 42 |    135 | CPU
DEBUG 01-15 16:08:52.754165.754165 lmp.py:1935]   Expert  0 |    140 | CPU
DEBUG 01-15 16:08:52.754431.754431 lmp.py:1935]   Expert 22 |    145 | CPU
DEBUG 01-15 16:08:52.754935.754935 lmp.py:1935]   Expert 53 |    146 | CPU
DEBUG 01-15 16:08:52.754200.754200 lmp.py:1935]   Expert 51 |    151 | CPU
DEBUG 01-15 16:08:52.754943.754943 lmp.py:1935]   Expert  8 |    163 | CPU
DEBUG 01-15 16:08:52.754970.754970 lmp.py:1935]   Expert 44 |    166 | CPU
DEBUG 01-15 16:08:52.754620.754620 lmp.py:1935]   Expert 15 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754270.754270 lmp.py:1935]   Expert 60 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754204.754204 lmp.py:1935]   Expert 29 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754377.754377 lmp.py:1935]   Expert 54 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754788.754788 lmp.py:1935]   Expert 34 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754961.754961 lmp.py:1935]   Expert 35 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754611.754611 lmp.py:1935]   Expert 33 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754830.754830 lmp.py:1935]   Expert 47 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754811.754811 lmp.py:1935]   Expert 19 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754507.754507 lmp.py:1935]   Expert  9 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754203.754203 lmp.py:1935]   Expert  3 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754423.754423 lmp.py:1935]   Expert 46 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754403.754403 lmp.py:1935]   Expert 56 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754384.754384 lmp.py:1935]   Expert 21 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754511.754511 lmp.py:1935]   Expert 45 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754492.754492 lmp.py:1935]   Expert 20 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754903.754903 lmp.py:1935]   Expert 49 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754791.754791 lmp.py:1935]   Expert 28 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754249.754249 lmp.py:1935]   Expert 57 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754468.754468 lmp.py:1935]   Expert  2 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754926.754926 lmp.py:1935]   Expert  4 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754907.754907 lmp.py:1935]   Expert 13 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754126.754126 lmp.py:1935]   Expert 43 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754345.754345 lmp.py:1935]   Expert 10 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754803.754803 lmp.py:1935]   Expert 50 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754784.754784 lmp.py:1935]   Expert 41 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754957.754957 lmp.py:1935]   Expert 26 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754129.754129 lmp.py:1935]   Expert 63 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754064.754064 lmp.py:1935]   Expert 37 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754475.754475 lmp.py:1935]   Expert 31 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754171.754171 lmp.py:1935]   Expert 61 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.754914.754914 lmp.py:1935]   Expert 52 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.754895.754895 lmp.py:1935]   Expert 58 |    320 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.755591.755591 lmp.py:1935]   Expert 62 |    325 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.755810.755810 lmp.py:1935]   Expert 55 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.755745.755745 lmp.py:1935]   Expert 11 |    384 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.755394.755394 lmp.py:1935]   Expert 23 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.755044.755044 lmp.py:1935]   Expert 25 |    410 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.755455.755455 lmp.py:1935]   Expert  5 |    514 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.755483.755483 lmp.py:1937] 
DEBUG 01-15 16:08:52.755483.755483 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.755225.755225 lmp.py:1938]   CPU:   2747 tokens
DEBUG 01-15 16:08:52.755206.755206 lmp.py:1942]   cuda:1:   4698 tokens (19 experts)
DEBUG 01-15 16:08:52.755948.755948 lmp.py:1942]   cuda:2:   4843 tokens (20 experts)
DEBUG 01-15 16:08:52.755452.755452 lmp.py:1943]   Total GPU:   9541 tokens
DEBUG 01-15 16:08:52.755003.755003 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.755003.755003 lmp.py:1944] 
DEBUG 01-15 16:08:52.755182.755182 cuda_h.py:19] end experts_map_get cost 0.0022308826446533203 seconds
DEBUG 01-15 16:08:52.755907.755907 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:52.755431.755431 lmp.py:1953] 
DEBUG 01-15 16:08:52.755431.755431 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.755612.755612 cuda_h.py:19] end cpu_experts_submit cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:08:52.755030.755030 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.755252.755252 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.756487.756487 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.54.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.54.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.54.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.60.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.60.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.60.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.8.mlp.experts.3.gate_proj.weight': 0, 'model.layers.8.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.15.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.15.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.15.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.23.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.23.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.23.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.25.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.25.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.25.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.8.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.8.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.8.mlp.experts.61.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (10698096640, 5767168, 17301504, 0), (10703863808, 5767168, 23068672, 0), (10692329472, 5767168, 28835840, 0), (10767302656, 5767168, 34603008, 0), (10773069824, 5767168, 40370176, 0), (10761535488, 5767168, 46137344, 0), (10784604160, 5767168, 51904512, 0), (10790371328, 5767168, 57671680, 0), (10778836992, 5767168, 63438848, 0), (10801905664, 5767168, 69206016, 0), (10807672832, 5767168, 74973184, 0), (10796138496, 5767168, 80740352, 0), (10836508672, 5767168, 86507520, 0), (10842275840, 5767168, 92274688, 0), (10830741504, 5767168, 98041856, 0), (10957619200, 5767168, 103809024, 0), (10963386368, 5767168, 109576192, 0), (10951852032, 5767168, 115343360, 0), (10974920704, 5767168, 121110528, 0), (10980687872, 5767168, 126877696, 0), (10969153536, 5767168, 132644864, 0), (11096031232, 5767168, 138412032, 0), (11101798400, 5767168, 144179200, 0), (11090264064, 5767168, 149946368, 0), (11147935744, 5767168, 155713536, 0), (11153702912, 5767168, 161480704, 0), (11142168576, 5767168, 167247872, 0), (11217141760, 5767168, 173015040, 0), (11222908928, 5767168, 178782208, 0), (11211374592, 5767168, 184549376, 0), (11320950784, 5767168, 190316544, 0), (11326717952, 5767168, 196083712, 0), (11315183616, 5767168, 201850880, 0), (11424759808, 5767168, 207618048, 0), (11430526976, 5767168, 213385216, 0), (11418992640, 5767168, 219152384, 0), (11511267328, 5767168, 224919552, 0), (11517034496, 5767168, 230686720, 0), (11505500160, 5767168, 236453888, 0), (11545870336, 5767168, 242221056, 0), (11551637504, 5767168, 247988224, 0), (11540103168, 5767168, 253755392, 0), (11580473344, 5767168, 259522560, 0), (11586240512, 5767168, 265289728, 0), (11574706176, 5767168, 271056896, 0), (11649679360, 5767168, 276824064, 0), (11655446528, 5767168, 282591232, 0), (11643912192, 5767168, 288358400, 0), (11684282368, 5767168, 294125568, 0), (11690049536, 5767168, 299892736, 0), (11678515200, 5767168, 305659904, 0), (11701583872, 5767168, 311427072, 0), (11707351040, 5767168, 317194240, 0), (11695816704, 5767168, 322961408, 0)], 2: [(10663493632, 5767168, 0, 0), (10669260800, 5767168, 5767168, 0), (10657726464, 5767168, 11534336, 0), (10680795136, 5767168, 17301504, 0), (10686562304, 5767168, 23068672, 0), (10675027968, 5767168, 28835840, 0), (10871111680, 5767168, 34603008, 0), (10876878848, 5767168, 40370176, 0), (10865344512, 5767168, 46137344, 0), (10940317696, 5767168, 51904512, 0), (10946084864, 5767168, 57671680, 0), (10934550528, 5767168, 63438848, 0), (11009523712, 5767168, 69206016, 0), (11015290880, 5767168, 74973184, 0), (11003756544, 5767168, 80740352, 0), (11044126720, 5767168, 86507520, 0), (11049893888, 5767168, 92274688, 0), (11038359552, 5767168, 98041856, 0), (11061428224, 5767168, 103809024, 0), (11067195392, 5767168, 109576192, 0), (11055661056, 5767168, 115343360, 0), (11113332736, 5767168, 121110528, 0), (11119099904, 5767168, 126877696, 0), (11107565568, 5767168, 132644864, 0), (11182538752, 5767168, 138412032, 0), (11188305920, 5767168, 144179200, 0), (11176771584, 5767168, 149946368, 0), (11199840256, 5767168, 155713536, 0), (11205607424, 5767168, 161480704, 0), (11194073088, 5767168, 167247872, 0), (11251744768, 5767168, 173015040, 0), (11257511936, 5767168, 178782208, 0), (11245977600, 5767168, 184549376, 0), (11355553792, 5767168, 190316544, 0), (11361320960, 5767168, 196083712, 0), (11349786624, 5767168, 201850880, 0), (11390156800, 5767168, 207618048, 0), (11395923968, 5767168, 213385216, 0), (11384389632, 5767168, 219152384, 0), (11407458304, 5767168, 224919552, 0), (11413225472, 5767168, 230686720, 0), (11401691136, 5767168, 236453888, 0), (11459362816, 5767168, 242221056, 0), (11465129984, 5767168, 247988224, 0), (11453595648, 5767168, 253755392, 0), (11476664320, 5767168, 259522560, 0), (11482431488, 5767168, 265289728, 0), (11470897152, 5767168, 271056896, 0), (11563171840, 5767168, 276824064, 0), (11568939008, 5767168, 282591232, 0), (11557404672, 5767168, 288358400, 0), (11597774848, 5767168, 294125568, 0), (11603542016, 5767168, 299892736, 0), (11592007680, 5767168, 305659904, 0), (11615076352, 5767168, 311427072, 0), (11620843520, 5767168, 317194240, 0), (11609309184, 5767168, 322961408, 0), (11666980864, 5767168, 328728576, 0), (11672748032, 5767168, 334495744, 0), (11661213696, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729740>, 2: <capsule object NULL at 0x74a6607296e0>}
DEBUG 01-15 16:08:52.756748.756748 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.756380.756380 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.756904.756904 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 43192e10-00a2-4e0c-96f7-fd3007e570a9
DEBUG 01-15 16:08:52.757237.757237 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.757902.757902 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:08:52.757969.757969 client.py:127] Model loaded
DEBUG 01-15 16:08:52.757125.757125 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:52.757450.757450 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.758117.758117 cuda_h.py:19] end move_flatidxs cost 0.0008661746978759766 seconds
DEBUG 01-15 16:08:52.758721.758721 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:52.758463.758463 cuda_h.py:19] end restore2model cost 0.0010616779327392578 seconds
DEBUG 01-15 16:08:52.758123.758123 cuda_h.py:19] end sllm_worker_task cost 0.015372753143310547 seconds
INFO 01-15 16:08:52.759351.759351 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 43192e10-00a2-4e0c-96f7-fd3007e570a9
DEBUG 01-15 16:08:52.759780.759780 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004068851470947266 seconds
DEBUG 01-15 16:08:52.759326.759326 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.762957.762957 cuda_h.py:19] end restore2model cost 0.003167390823364258 seconds
DEBUG 01-15 16:08:52.763601.763601 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007496356964111328 seconds
DEBUG 01-15 16:08:52.763655.763655 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.763143.763143 cuda_h.py:19] end gpu_sexperts cost 0.0002925395965576172 seconds
DEBUG 01-15 16:08:52.763595.763595 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.763093.763093 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2411346435546875e-05 seconds
DEBUG 01-15 16:08:52.763028.763028 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.763446.763446 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.765838.765838 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015876293182373047 seconds
DEBUG 01-15 16:08:52.765072.765072 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.765206.765206 cuda_h.py:19] end gpu_group_list cost 0.0002086162567138672 seconds
DEBUG 01-15 16:08:52.766426.766426 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.767091.767091 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010254383087158203 seconds
DEBUG 01-15 16:08:52.767842.767842 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.767996.767996 cuda_h.py:19] end gpu_group_list cost 0.00022339820861816406 seconds
DEBUG 01-15 16:08:52.767285.767285 cuda_h.py:19] end group_tensors cost 0.008865118026733398 seconds
DEBUG 01-15 16:08:52.768425.768425 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.768477.768477 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.768678.768678 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 43192e10-00a2-4e0c-96f7-fd3007e570a9
DEBUG 01-15 16:08:52.771680.771680 cuda_h.py:19] end group pad cost 0.0032219886779785156 seconds
DEBUG 01-15 16:08:52.771239.771239 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:52.797665.797665 client.py:127] Model loaded
DEBUG 01-15 16:08:52.797805.797805 cuda_h.py:19] end wait_experts_multi_device cost 0.029265880584716797 seconds
DEBUG 01-15 16:08:52.798338.798338 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.800118.800118 cuda_h.py:19] end group_einsum cost 0.029135942459106445 seconds
DEBUG 01-15 16:08:52.800613.800613 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:52.803190.803190 cuda_h.py:19] end get_outputs_cpu1 cost 0.0027227401733398438 seconds
DEBUG 01-15 16:08:52.804273.804273 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0471959114074707 seconds
DEBUG 01-15 16:08:52.805965.805965 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007174968719482422 seconds
DEBUG 01-15 16:08:52.805475.805475 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.807858.807858 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.807895.807895 cuda_h.py:19] end index_scatter cost 0.0001506805419921875 seconds
DEBUG 01-15 16:08:52.808908.808908 cuda_h.py:19] end cpuoutputsdeal cost 0.0029113292694091797 seconds
DEBUG 01-15 16:08:52.808384.808384 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.808983.808983 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.809448.809448 cuda_h.py:19] end gpu_group_tensor cost 0.00034165382385253906 seconds
DEBUG 01-15 16:08:52.809346.809346 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.809180.809180 cuda_h.py:19] end gpu_group_tensor cost 0.0003204345703125 seconds
DEBUG 01-15 16:08:52.810499.810499 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.811702.811702 cuda_h.py:19] end gpu_group_einsum cost 0.0011594295501708984 seconds
DEBUG 01-15 16:08:52.811148.811148 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.812564.812564 cuda_h.py:19] end gpu_group_einsum cost 0.0010266304016113281 seconds
DEBUG 01-15 16:08:52.812658.812658 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.813859.813859 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.813653.813653 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004761219024658203 seconds
DEBUG 01-15 16:08:52.813729.813729 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.814868.814868 cuda_h.py:19] end concat_expert_out cost 0.0001227855682373047 seconds
DEBUG 01-15 16:08:52.814669.814669 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.814988.814988 cuda_h.py:19] end index_scatter cost 0.000133514404296875 seconds
DEBUG 01-15 16:08:52.815038.815038 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.002431154251098633 seconds
DEBUG 01-15 16:08:52.816981.816981 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.816174.816174 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.817748.817748 cuda_h.py:19] end all_expert_outputs_slices cost 0.0006163120269775391 seconds
DEBUG 01-15 16:08:52.817625.817625 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.817925.817925 cuda_h.py:19] end concat_expert_out cost 0.00017142295837402344 seconds
DEBUG 01-15 16:08:52.817977.817977 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.817874.817874 cuda_h.py:19] end index_scatter cost 0.00017905235290527344 seconds
DEBUG 01-15 16:08:52.817771.817771 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00171661376953125 seconds
DEBUG 01-15 16:08:52.818110.818110 cuda_h.py:19] end gpu_experts_multi_device cost 0.05462479591369629 seconds
DEBUG 01-15 16:08:52.818733.818733 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.06627368927001953 seconds
DEBUG 01-15 16:08:52.819762.819762 cuda_h.py:19] end prefill_layer cost 0.07649517059326172 seconds
DEBUG 01-15 16:08:52.819065.819065 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:08:52.819451.819451 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.819606.819606 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:08:52.819761.819761 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:08:52.819830.819830 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:08:52.819300.819300 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 8.344650268554688e-05 seconds
DEBUG 01-15 16:08:52.820603.820603 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.820614.820614 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 0.00038504600524902344 seconds
DEBUG 01-15 16:08:52.820033.820033 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.820111.820111 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.820225.820225 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.821581.821581 cuda_h.py:19] end allocate_cuda_memory cost 0.0004673004150390625 seconds
DEBUG 01-15 16:08:52.821210.821210 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.821061.821061 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.821244.821244 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.822174.822174 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e8f7eec5-2181-4e77-97ab-cc5b1da6048b
DEBUG 01-15 16:08:52.822116.822116 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.822441.822441 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.823435.823435 cuda_h.py:10] start self_attn
INFO 01-15 16:08:52.824829.824829 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e8f7eec5-2181-4e77-97ab-cc5b1da6048b
DEBUG 01-15 16:08:52.824060.824060 cuda_h.py:19] end load_into_gpu_async cost 0.0031197071075439453 seconds
DEBUG 01-15 16:08:52.825552.825552 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.825302.825302 cuda_h.py:19] end restore_tensors2 cost 0.00015783309936523438 seconds
DEBUG 01-15 16:08:52.825578.825578 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004679441452026367 seconds
INFO 01-15 16:08:52.825148.825148 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e8f7eec5-2181-4e77-97ab-cc5b1da6048b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
INFO 01-15 16:08:52.832446.832446 client.py:127] Model loaded
DEBUG 01-15 16:08:52.832265.832265 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.833788.833788 cuda_h.py:19] end restore2model cost 0.0012736320495605469 seconds
DEBUG 01-15 16:08:52.834978.834978 cuda_h.py:19] end sllm_worker_task cost 0.013624429702758789 seconds
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.835905.835905 cuda_h.py:19] end self_attn cost 0.01170492172241211 seconds
DEBUG 01-15 16:08:52.836389.836389 cuda_h.py:19] end iln_self_attn_paln cost 0.015228033065795898 seconds
DEBUG 01-15 16:08:52.836532.836532 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:08:52.836132.836132 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.837940.837940 cuda_h.py:19] end gate cost 0.0012843608856201172 seconds
DEBUG 01-15 16:08:52.837744.837744 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.838503.838503 lmp.py:1912] 
DEBUG 01-15 16:08:52.838503.838503 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.839690.839690 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.839096.839096 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.839634.839634 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.839072.839072 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.839604.839604 lmp.py:1917] 
DEBUG 01-15 16:08:52.839604.839604 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.839327.839327 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.839163.839163 lmp.py:1935]   Expert 24 |     39 | CPU
DEBUG 01-15 16:08:52.839171.839171 lmp.py:1935]   Expert  2 |     47 | CPU
DEBUG 01-15 16:08:52.839033.839033 lmp.py:1935]   Expert 32 |     64 | CPU
DEBUG 01-15 16:08:52.839657.839657 lmp.py:1935]   Expert 26 |     65 | CPU
DEBUG 01-15 16:08:52.839804.839804 lmp.py:1935]   Expert 19 |     68 | CPU
DEBUG 01-15 16:08:52.839713.839713 lmp.py:1935]   Expert 50 |     70 | CPU
DEBUG 01-15 16:08:52.839144.839144 lmp.py:1935]   Expert 15 |     78 | CPU
DEBUG 01-15 16:08:52.839815.839815 lmp.py:1935]   Expert 28 |     82 | CPU
DEBUG 01-15 16:08:52.839723.839723 lmp.py:1935]   Expert 60 |     82 | CPU
DEBUG 01-15 16:08:52.839115.839115 lmp.py:1935]   Expert  4 |     83 | CPU
DEBUG 01-15 16:08:52.839931.839931 lmp.py:1935]   Expert  7 |     83 | CPU
DEBUG 01-15 16:08:52.839939.839939 lmp.py:1935]   Expert 59 |     90 | CPU
DEBUG 01-15 16:08:52.839371.839371 lmp.py:1935]   Expert 49 |     97 | CPU
DEBUG 01-15 16:08:52.839041.839041 lmp.py:1935]   Expert 23 |     99 | CPU
DEBUG 01-15 16:08:52.839188.839188 lmp.py:1935]   Expert  5 |    104 | CPU
DEBUG 01-15 16:08:52.839858.839858 lmp.py:1935]   Expert 12 |    105 | CPU
DEBUG 01-15 16:08:52.839290.839290 lmp.py:1935]   Expert 10 |    111 | CPU
DEBUG 01-15 16:08:52.839198.839198 lmp.py:1935]   Expert 27 |    112 | CPU
DEBUG 01-15 16:08:52.840869.840869 lmp.py:1935]   Expert 41 |    121 | CPU
DEBUG 01-15 16:08:52.840684.840684 lmp.py:1935]   Expert  3 |    125 | CPU
DEBUG 01-15 16:08:52.840500.840500 lmp.py:1935]   Expert 25 |    127 | CPU
DEBUG 01-15 16:08:52.840316.840316 lmp.py:1935]   Expert 20 |    130 | CPU
DEBUG 01-15 16:08:52.840225.840225 lmp.py:1935]   Expert 40 |    130 | CPU
DEBUG 01-15 16:08:52.840895.840895 lmp.py:1935]   Expert 13 |    131 | CPU
DEBUG 01-15 16:08:52.840327.840327 lmp.py:1935]   Expert 16 |    132 | CPU
DEBUG 01-15 16:08:52.840527.840527 lmp.py:1935]   Expert 37 |    145 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.840349.840349 lmp.py:1935]   Expert 17 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.840378.840378 lmp.py:1935]   Expert 35 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.840770.840770 lmp.py:1935]   Expert 47 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.840255.840255 lmp.py:1935]   Expert 22 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.840024.840024 lmp.py:1935]   Expert 53 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.840794.840794 lmp.py:1935]   Expert 39 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.840709.840709 lmp.py:1935]   Expert 38 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.840386.840386 lmp.py:1935]   Expert 44 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.840063.840063 lmp.py:1935]   Expert 36 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.840356.840356 lmp.py:1935]   Expert 52 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.840649.840649 lmp.py:1935]   Expert 58 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.840180.840180 lmp.py:1935]   Expert 18 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.840234.840234 lmp.py:1935]   Expert 62 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.840434.840434 lmp.py:1935]   Expert 11 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.840634.840634 lmp.py:1935]   Expert 48 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.840689.840689 lmp.py:1935]   Expert 30 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841028.841028 lmp.py:1935]   Expert 14 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841844.841844 lmp.py:1935]   Expert  1 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841567.841567 lmp.py:1935]   Expert 45 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841290.841290 lmp.py:1935]   Expert 42 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841013.841013 lmp.py:1935]   Expert 31 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841306.841306 lmp.py:1935]   Expert  6 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841645.841645 lmp.py:1935]   Expert 51 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841461.841461 lmp.py:1935]   Expert 29 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841622.841622 lmp.py:1935]   Expert 34 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841775.841775 lmp.py:1935]   Expert 33 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841452.841452 lmp.py:1935]   Expert 57 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841745.841745 lmp.py:1935]   Expert 61 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841323.841323 lmp.py:1935]   Expert 43 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841854.841854 lmp.py:1935]   Expert  0 |    322 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841670.841670 lmp.py:1935]   Expert 46 |    352 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841393.841393 lmp.py:1935]   Expert  8 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841355.841355 lmp.py:1935]   Expert 56 |    391 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841555.841555 lmp.py:1935]   Expert  9 |    392 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841371.841371 lmp.py:1935]   Expert 54 |    395 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841948.841948 lmp.py:1935]   Expert 63 |    407 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841525.841525 lmp.py:1935]   Expert 55 |    428 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.841249.841249 lmp.py:1935]   Expert 21 |    492 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.841826.841826 lmp.py:1937] 
DEBUG 01-15 16:08:52.841826.841826 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.841834.841834 lmp.py:1938]   CPU:   2375 tokens
DEBUG 01-15 16:08:52.842081.842081 lmp.py:1942]   cuda:1:   5026 tokens (20 experts)
DEBUG 01-15 16:08:52.842420.842420 lmp.py:1942]   cuda:2:   4887 tokens (19 experts)
DEBUG 01-15 16:08:52.842090.842090 lmp.py:1943]   Total GPU:   9913 tokens
DEBUG 01-15 16:08:52.842383.842383 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.842383.842383 lmp.py:1944] 
DEBUG 01-15 16:08:52.842881.842881 cuda_h.py:19] end experts_map_get cost 0.00421595573425293 seconds
DEBUG 01-15 16:08:52.842349.842349 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:52.842053.842053 lmp.py:1953] 
DEBUG 01-15 16:08:52.842053.842053 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.842441.842441 cuda_h.py:19] end cpu_experts_submit cost 0.00011229515075683594 seconds
DEBUG 01-15 16:08:52.842648.842648 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.842785.842785 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.844817.844817 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.14.gate_proj.weight': 0, 'model.layers.9.mlp.experts.14.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.14.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.18.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.18.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.18.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.21.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.21.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.21.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.29.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.29.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.29.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.30.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.30.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.30.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.31.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.31.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.31.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.34.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.34.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.34.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.35.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.35.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.35.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.36.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.36.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.36.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.9.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.9.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.9.mlp.experts.62.up_proj.weight': 340262912}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.38.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.38.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.38.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.48.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.48.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.48.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.51.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.51.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.51.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(11961106432, 5767168, 0, 0), (11966873600, 5767168, 5767168, 0), (11955339264, 5767168, 11534336, 0), (12030312448, 5767168, 17301504, 0), (12036079616, 5767168, 23068672, 0), (12024545280, 5767168, 28835840, 0), (12082216960, 5767168, 34603008, 0), (12087984128, 5767168, 40370176, 0), (12076449792, 5767168, 46137344, 0), (12220628992, 5767168, 51904512, 0), (12226396160, 5767168, 57671680, 0), (12214861824, 5767168, 63438848, 0), (12237930496, 5767168, 69206016, 0), (12243697664, 5767168, 74973184, 0), (12232163328, 5767168, 80740352, 0), (12255232000, 5767168, 86507520, 0), (12260999168, 5767168, 92274688, 0), (12249464832, 5767168, 98041856, 0), (12307136512, 5767168, 103809024, 0), (12312903680, 5767168, 109576192, 0), (12301369344, 5767168, 115343360, 0), (12324438016, 5767168, 121110528, 0), (12330205184, 5767168, 126877696, 0), (12318670848, 5767168, 132644864, 0), (12341739520, 5767168, 138412032, 0), (12347506688, 5767168, 144179200, 0), (12335972352, 5767168, 149946368, 0), (12359041024, 5767168, 155713536, 0), (12364808192, 5767168, 161480704, 0), (12353273856, 5767168, 167247872, 0), (12393644032, 5767168, 173015040, 0), (12399411200, 5767168, 178782208, 0), (12387876864, 5767168, 184549376, 0), (12462850048, 5767168, 190316544, 0), (12468617216, 5767168, 196083712, 0), (12457082880, 5767168, 201850880, 0), (12480151552, 5767168, 207618048, 0), (12485918720, 5767168, 213385216, 0), (12474384384, 5767168, 219152384, 0), (12497453056, 5767168, 224919552, 0), (12503220224, 5767168, 230686720, 0), (12491685888, 5767168, 236453888, 0), (12514754560, 5767168, 242221056, 0), (12520521728, 5767168, 247988224, 0), (12508987392, 5767168, 253755392, 0), (12532056064, 5767168, 259522560, 0), (12537823232, 5767168, 265289728, 0), (12526288896, 5767168, 271056896, 0), (12653166592, 5767168, 276824064, 0), (12658933760, 5767168, 282591232, 0), (12647399424, 5767168, 288358400, 0), (12687769600, 5767168, 294125568, 0), (12693536768, 5767168, 299892736, 0), (12682002432, 5767168, 305659904, 0), (12705071104, 5767168, 311427072, 0), (12710838272, 5767168, 317194240, 0), (12699303936, 5767168, 322961408, 0), (12791578624, 5767168, 328728576, 0), (12797345792, 5767168, 334495744, 0), (12785811456, 5767168, 340262912, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (11736186880, 5767168, 17301504, 0), (11741954048, 5767168, 23068672, 0), (11730419712, 5767168, 28835840, 0), (11822694400, 5767168, 34603008, 0), (11828461568, 5767168, 40370176, 0), (11816927232, 5767168, 46137344, 0), (11857297408, 5767168, 51904512, 0), (11863064576, 5767168, 57671680, 0), (11851530240, 5767168, 63438848, 0), (11874598912, 5767168, 69206016, 0), (11880366080, 5767168, 74973184, 0), (11868831744, 5767168, 80740352, 0), (11909201920, 5767168, 86507520, 0), (11914969088, 5767168, 92274688, 0), (11903434752, 5767168, 98041856, 0), (12013010944, 5767168, 103809024, 0), (12018778112, 5767168, 109576192, 0), (12007243776, 5767168, 115343360, 0), (12099518464, 5767168, 121110528, 0), (12105285632, 5767168, 126877696, 0), (12093751296, 5767168, 132644864, 0), (12289835008, 5767168, 138412032, 0), (12295602176, 5767168, 144179200, 0), (12284067840, 5767168, 149946368, 0), (12376342528, 5767168, 155713536, 0), (12382109696, 5767168, 161480704, 0), (12370575360, 5767168, 167247872, 0), (12445548544, 5767168, 173015040, 0), (12451315712, 5767168, 178782208, 0), (12439781376, 5767168, 184549376, 0), (12549357568, 5767168, 190316544, 0), (12555124736, 5767168, 196083712, 0), (12543590400, 5767168, 201850880, 0), (12601262080, 5767168, 207618048, 0), (12607029248, 5767168, 213385216, 0), (12595494912, 5767168, 219152384, 0), (12618563584, 5767168, 224919552, 0), (12624330752, 5767168, 230686720, 0), (12612796416, 5767168, 236453888, 0), (12635865088, 5767168, 242221056, 0), (12641632256, 5767168, 247988224, 0), (12630097920, 5767168, 253755392, 0), (12670468096, 5767168, 259522560, 0), (12676235264, 5767168, 265289728, 0), (12664700928, 5767168, 271056896, 0), (12722372608, 5767168, 276824064, 0), (12728139776, 5767168, 282591232, 0), (12716605440, 5767168, 288358400, 0), (12774277120, 5767168, 294125568, 0), (12780044288, 5767168, 299892736, 0), (12768509952, 5767168, 305659904, 0), (12808880128, 5767168, 311427072, 0), (12814647296, 5767168, 317194240, 0), (12803112960, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a81456ff90>, 2: <capsule object NULL at 0x74a660729a40>}
DEBUG 01-15 16:08:52.844826.844826 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.844015.844015 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.845976.845976 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 93654a7b-d3cd-47ba-81e3-feca61a7aa0f
DEBUG 01-15 16:08:52.845523.845523 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.845894.845894 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.846465.846465 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:52.847645.847645 cuda_h.py:19] end move_flatidxs cost 0.001065969467163086 seconds
DEBUG 01-15 16:08:52.847000.847000 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:52.848810.848810 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 93654a7b-d3cd-47ba-81e3-feca61a7aa0f
DEBUG 01-15 16:08:52.849598.849598 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.006798982620239258 seconds
DEBUG 01-15 16:08:52.849221.849221 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.856958.856958 cuda_h.py:19] end group_tensors cost 0.008550882339477539 seconds
DEBUG 01-15 16:08:52.857106.857106 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.858534.858534 cuda_h.py:19] end restore2model cost 0.008832454681396484 seconds
DEBUG 01-15 16:08:52.858210.858210 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.016321659088134766 seconds
DEBUG 01-15 16:08:52.859041.859041 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.860159.860159 cuda_h.py:19] end gpu_sexperts cost 0.0010678768157958984 seconds
DEBUG 01-15 16:08:52.860344.860344 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.860324.860324 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:08:52.860148.860148 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.860085.860085 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.860088.860088 cuda_h.py:19] end group pad cost 0.003560781478881836 seconds
DEBUG 01-15 16:08:52.860489.860489 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:52.865397.865397 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0044193267822265625 seconds
DEBUG 01-15 16:08:52.865181.865181 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.866157.866157 cuda_h.py:19] end gpu_group_list cost 0.00041937828063964844 seconds
DEBUG 01-15 16:08:52.867273.867273 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.870740.870740 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0027008056640625 seconds
DEBUG 01-15 16:08:52.870797.870797 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.871314.871314 cuda_h.py:19] end gpu_group_list cost 0.0006310939788818359 seconds
DEBUG 01-15 16:08:52.872988.872988 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.872515.872515 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 93654a7b-d3cd-47ba-81e3-feca61a7aa0f
DEBUG 01-15 16:08:52.879439.879439 cuda_h.py:19] end group_einsum cost 0.018181324005126953 seconds
DEBUG 01-15 16:08:52.879404.879404 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:08:52.881260.881260 client.py:127] Model loaded
DEBUG 01-15 16:08:52.881907.881907 cuda_h.py:19] end wait_experts_multi_device cost 0.008544445037841797 seconds
DEBUG 01-15 16:08:52.881623.881623 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.882971.882971 cuda_h.py:19] end get_outputs_cpu1 cost 0.002644062042236328 seconds
DEBUG 01-15 16:08:52.882521.882521 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03696107864379883 seconds
DEBUG 01-15 16:08:52.883844.883844 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.002365589141845703 seconds
DEBUG 01-15 16:08:52.883618.883618 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.886990.886990 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.886444.886444 cuda_h.py:19] end index_scatter cost 0.00014448165893554688 seconds
DEBUG 01-15 16:08:52.887316.887316 cuda_h.py:19] end cpuoutputsdeal cost 0.002937793731689453 seconds
DEBUG 01-15 16:08:52.887468.887468 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.887736.887736 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.887711.887711 cuda_h.py:19] end gpu_group_tensor cost 0.00033593177795410156 seconds
DEBUG 01-15 16:08:52.887039.887039 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.888543.888543 cuda_h.py:19] end gpu_group_tensor cost 0.00031948089599609375 seconds
DEBUG 01-15 16:08:52.888359.888359 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.889402.889402 cuda_h.py:19] end gpu_group_einsum cost 0.0011663436889648438 seconds
DEBUG 01-15 16:08:52.889991.889991 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.890846.890846 cuda_h.py:19] end gpu_group_einsum cost 0.0007181167602539062 seconds
DEBUG 01-15 16:08:52.890521.890521 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.891196.891196 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.891137.891137 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003337860107421875 seconds
DEBUG 01-15 16:08:52.891987.891987 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.891482.891482 cuda_h.py:19] end concat_expert_out cost 8.869171142578125e-05 seconds
DEBUG 01-15 16:08:52.891221.891221 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.892783.892783 cuda_h.py:19] end index_scatter cost 0.00010323524475097656 seconds
DEBUG 01-15 16:08:52.892897.892897 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0014529228210449219 seconds
DEBUG 01-15 16:08:52.892922.892922 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.892880.892880 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.893818.893818 cuda_h.py:19] end all_expert_outputs_slices cost 0.000274658203125 seconds
DEBUG 01-15 16:08:52.893185.893185 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.893435.893435 cuda_h.py:19] end concat_expert_out cost 9.107589721679688e-05 seconds
DEBUG 01-15 16:08:52.893207.893207 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.893384.893384 cuda_h.py:19] end index_scatter cost 9.369850158691406e-05 seconds
DEBUG 01-15 16:08:52.893294.893294 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009403228759765625 seconds
DEBUG 01-15 16:08:52.893755.893755 cuda_h.py:19] end gpu_experts_multi_device cost 0.0330960750579834 seconds
DEBUG 01-15 16:08:52.893582.893582 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.057586669921875 seconds
DEBUG 01-15 16:08:52.894941.894941 cuda_h.py:19] end prefill_layer cost 0.07506918907165527 seconds
DEBUG 01-15 16:08:52.894732.894732 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:08:52.894800.894800 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.894345.894345 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:08:52.894413.894413 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:08:52.894508.894508 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:08:52.895717.895717 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:08:52.895468.895468 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 0.00012874603271484375 seconds
DEBUG 01-15 16:08:52.895880.895880 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.895573.895573 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.895449.895449 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.895134.895134 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.895249.895249 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.896406.896406 cuda_h.py:19] end allocate_cuda_memory cost 0.0004265308380126953 seconds
DEBUG 01-15 16:08:52.896009.896009 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.896767.896767 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.896506.896506 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.896807.896807 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c3d99a8f-34ad-44e4-963c-bea390af2b38
DEBUG 01-15 16:08:52.897483.897483 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.897483.897483 cuda_h.py:10] start self_attn
INFO 01-15 16:08:52.898529.898529 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c3d99a8f-34ad-44e4-963c-bea390af2b38
DEBUG 01-15 16:08:52.898381.898381 cuda_h.py:19] end load_into_gpu_async cost 0.001926422119140625 seconds
DEBUG 01-15 16:08:52.898960.898960 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.898372.898372 cuda_h.py:19] end restore_tensors2 cost 0.00015306472778320312 seconds
DEBUG 01-15 16:08:52.899401.899401 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003217935562133789 seconds
INFO 01-15 16:08:52.899929.899929 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c3d99a8f-34ad-44e4-963c-bea390af2b38
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.903110.903110 cuda_h.py:19] end self_attn cost 0.005533933639526367 seconds
DEBUG 01-15 16:08:52.903788.903788 cuda_h.py:19] end iln_self_attn_paln cost 0.0086517333984375 seconds
DEBUG 01-15 16:08:52.903200.903200 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:08:52.903678.903678 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.904760.904760 cuda_h.py:19] end gate cost 0.0008285045623779297 seconds
DEBUG 01-15 16:08:52.904173.904173 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.905906.905906 lmp.py:1912] 
DEBUG 01-15 16:08:52.905906.905906 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.905861.905861 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.905279.905279 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.905552.905552 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.905725.905725 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.905182.905182 lmp.py:1917] 
DEBUG 01-15 16:08:52.905182.905182 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.905309.905309 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.905396.905396 lmp.py:1935]   Expert 43 |     15 | CPU
DEBUG 01-15 16:08:52.905807.905807 lmp.py:1935]   Expert 27 |     32 | CPU
DEBUG 01-15 16:08:52.905265.905265 lmp.py:1935]   Expert 26 |     52 | CPU
DEBUG 01-15 16:08:52.905007.905007 lmp.py:1935]   Expert 34 |     54 | CPU
DEBUG 01-15 16:08:52.905465.905465 lmp.py:1935]   Expert 56 |     56 | CPU
DEBUG 01-15 16:08:52.905446.905446 lmp.py:1935]   Expert  3 |     59 | CPU
DEBUG 01-15 16:08:52.905427.905427 lmp.py:1935]   Expert  4 |     66 | CPU
DEBUG 01-15 16:08:52.905931.905931 lmp.py:1935]   Expert 61 |     80 | CPU
DEBUG 01-15 16:08:52.905435.905435 lmp.py:1935]   Expert 14 |     93 | CPU
DEBUG 01-15 16:08:52.905654.905654 lmp.py:1935]   Expert 38 |    101 | CPU
DEBUG 01-15 16:08:52.905873.905873 lmp.py:1935]   Expert  2 |    113 | CPU
DEBUG 01-15 16:08:52.905854.905854 lmp.py:1935]   Expert 17 |    119 | CPU
DEBUG 01-15 16:08:52.905073.905073 lmp.py:1935]   Expert 22 |    123 | CPU
DEBUG 01-15 16:08:52.905054.905054 lmp.py:1935]   Expert 47 |    128 | CPU
DEBUG 01-15 16:08:52.905035.905035 lmp.py:1935]   Expert 37 |    130 | CPU
DEBUG 01-15 16:08:52.905254.905254 lmp.py:1935]   Expert 55 |    130 | CPU
DEBUG 01-15 16:08:52.905473.905473 lmp.py:1935]   Expert 28 |    133 | CPU
DEBUG 01-15 16:08:52.905739.905739 lmp.py:1935]   Expert 54 |    135 | CPU
DEBUG 01-15 16:08:52.906005.906005 lmp.py:1935]   Expert 15 |    145 | CPU
DEBUG 01-15 16:08:52.906509.906509 lmp.py:1935]   Expert 45 |    145 | CPU
DEBUG 01-15 16:08:52.906774.906774 lmp.py:1935]   Expert 48 |    146 | CPU
DEBUG 01-15 16:08:52.906470.906470 lmp.py:1935]   Expert  7 |    147 | CPU
DEBUG 01-15 16:08:52.906451.906451 lmp.py:1935]   Expert 51 |    147 | CPU
DEBUG 01-15 16:08:52.906194.906194 lmp.py:1935]   Expert  5 |    149 | CPU
DEBUG 01-15 16:08:52.906174.906174 lmp.py:1935]   Expert 60 |    150 | CPU
DEBUG 01-15 16:08:52.906778.906778 lmp.py:1935]   Expert 63 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906189.906189 lmp.py:1935]   Expert 12 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906839.906839 lmp.py:1935]   Expert 19 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906297.906297 lmp.py:1935]   Expert  6 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906470.906470 lmp.py:1935]   Expert 57 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906404.906404 lmp.py:1935]   Expert 52 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906339.906339 lmp.py:1935]   Expert 50 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906750.906750 lmp.py:1935]   Expert 18 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906923.906923 lmp.py:1935]   Expert 44 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906619.906619 lmp.py:1935]   Expert 31 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906553.906553 lmp.py:1935]   Expert 13 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906217.906217 lmp.py:1935]   Expert 30 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906198.906198 lmp.py:1935]   Expert 23 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906178.906178 lmp.py:1935]   Expert 39 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906159.906159 lmp.py:1935]   Expert 53 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906902.906902 lmp.py:1935]   Expert 59 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906121.906121 lmp.py:1935]   Expert 20 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906771.906771 lmp.py:1935]   Expert 21 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906228.906228 lmp.py:1935]   Expert 29 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906686.906686 lmp.py:1935]   Expert 16 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906905.906905 lmp.py:1935]   Expert 36 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906125.906125 lmp.py:1935]   Expert 41 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906105.906105 lmp.py:1935]   Expert 25 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906609.906609 lmp.py:1935]   Expert 32 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906590.906590 lmp.py:1935]   Expert 49 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906856.906856 lmp.py:1935]   Expert 46 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906360.906360 lmp.py:1935]   Expert  8 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906625.906625 lmp.py:1935]   Expert 42 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906627.906627 lmp.py:1935]   Expert 10 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906800.906800 lmp.py:1935]   Expert 62 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906257.906257 lmp.py:1935]   Expert 35 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906476.906476 lmp.py:1935]   Expert 33 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906934.906934 lmp.py:1935]   Expert  9 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906677.906677 lmp.py:1935]   Expert 58 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906419.906419 lmp.py:1935]   Expert 40 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906161.906161 lmp.py:1935]   Expert  0 |    426 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.906619.906619 lmp.py:1935]   Expert 11 |    429 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906361.906361 lmp.py:1935]   Expert 24 |    562 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.906104.906104 lmp.py:1935]   Expert  1 |    646 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.907893.907893 lmp.py:1937] 
DEBUG 01-15 16:08:52.907893.907893 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.907350.907350 lmp.py:1938]   CPU:   2648 tokens
DEBUG 01-15 16:08:52.907762.907762 lmp.py:1942]   cuda:1:   4744 tokens (19 experts)
DEBUG 01-15 16:08:52.907458.907458 lmp.py:1942]   cuda:2:   4896 tokens (20 experts)
DEBUG 01-15 16:08:52.907723.907723 lmp.py:1943]   Total GPU:   9640 tokens
DEBUG 01-15 16:08:52.907989.907989 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.907989.907989 lmp.py:1944] 
DEBUG 01-15 16:08:52.907990.907990 cuda_h.py:19] end experts_map_get cost 0.002217531204223633 seconds
DEBUG 01-15 16:08:52.907013.907013 cuda_h.py:10] start cpu_experts_submit
INFO 01-15 16:08:52.907289.907289 client.py:127] Model loaded
DEBUG 01-15 16:08:52.907969.907969 lmp.py:1953] 
DEBUG 01-15 16:08:52.907969.907969 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.907318.907318 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.907282.907282 cuda_h.py:19] end cpu_experts_submit cost 0.0002319812774658203 seconds
DEBUG 01-15 16:08:52.909585.909585 cuda_h.py:19] end restore2model cost 0.0011930465698242188 seconds
DEBUG 01-15 16:08:52.909885.909885 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.909560.909560 cuda_h.py:19] end sllm_worker_task cost 0.0142974853515625 seconds
DEBUG 01-15 16:08:52.910230.910230 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.910731.910731 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.910288.910288 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:52.911663.911663 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.36.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.36.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.36.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.58.up_proj.weight': 322961408}, 2: {'model.layers.10.mlp.experts.8.gate_proj.weight': 0, 'model.layers.10.mlp.experts.8.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.8.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.30.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.30.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.30.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.10.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.10.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.10.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (12843483136, 5767168, 17301504, 0), (12849250304, 5767168, 23068672, 0), (12837715968, 5767168, 28835840, 0), (12929990656, 5767168, 34603008, 0), (12935757824, 5767168, 40370176, 0), (12924223488, 5767168, 46137344, 0), (12981895168, 5767168, 51904512, 0), (12987662336, 5767168, 57671680, 0), (12976128000, 5767168, 63438848, 0), (13051101184, 5767168, 69206016, 0), (13056868352, 5767168, 74973184, 0), (13045334016, 5767168, 80740352, 0), (13137608704, 5767168, 86507520, 0), (13143375872, 5767168, 92274688, 0), (13131841536, 5767168, 98041856, 0), (13154910208, 5767168, 103809024, 0), (13160677376, 5767168, 109576192, 0), (13149143040, 5767168, 115343360, 0), (13189513216, 5767168, 121110528, 0), (13195280384, 5767168, 126877696, 0), (13183746048, 5767168, 132644864, 0), (13224116224, 5767168, 138412032, 0), (13229883392, 5767168, 144179200, 0), (13218349056, 5767168, 149946368, 0), (13258719232, 5767168, 155713536, 0), (13264486400, 5767168, 161480704, 0), (13252952064, 5767168, 167247872, 0), (13327925248, 5767168, 173015040, 0), (13333692416, 5767168, 178782208, 0), (13322158080, 5767168, 184549376, 0), (13379829760, 5767168, 190316544, 0), (13385596928, 5767168, 196083712, 0), (13374062592, 5767168, 201850880, 0), (13431734272, 5767168, 207618048, 0), (13437501440, 5767168, 213385216, 0), (13425967104, 5767168, 219152384, 0), (13449035776, 5767168, 224919552, 0), (13454802944, 5767168, 230686720, 0), (13443268608, 5767168, 236453888, 0), (13552844800, 5767168, 242221056, 0), (13558611968, 5767168, 247988224, 0), (13547077632, 5767168, 253755392, 0), (13622050816, 5767168, 259522560, 0), (13627817984, 5767168, 265289728, 0), (13616283648, 5767168, 271056896, 0), (13691256832, 5767168, 276824064, 0), (13697024000, 5767168, 282591232, 0), (13685489664, 5767168, 288358400, 0), (13743161344, 5767168, 294125568, 0), (13748928512, 5767168, 299892736, 0), (13737394176, 5767168, 305659904, 0), (13829668864, 5767168, 311427072, 0), (13835436032, 5767168, 317194240, 0), (13823901696, 5767168, 322961408, 0)], 2: [(12964593664, 5767168, 0, 0), (12970360832, 5767168, 5767168, 0), (12958826496, 5767168, 11534336, 0), (12999196672, 5767168, 17301504, 0), (13004963840, 5767168, 23068672, 0), (12993429504, 5767168, 28835840, 0), (13016498176, 5767168, 34603008, 0), (13022265344, 5767168, 40370176, 0), (13010731008, 5767168, 46137344, 0), (13033799680, 5767168, 51904512, 0), (13039566848, 5767168, 57671680, 0), (13028032512, 5767168, 63438848, 0), (13103005696, 5767168, 69206016, 0), (13108772864, 5767168, 74973184, 0), (13097238528, 5767168, 80740352, 0), (13172211712, 5767168, 86507520, 0), (13177978880, 5767168, 92274688, 0), (13166444544, 5767168, 98041856, 0), (13241417728, 5767168, 103809024, 0), (13247184896, 5767168, 109576192, 0), (13235650560, 5767168, 115343360, 0), (13345226752, 5767168, 121110528, 0), (13350993920, 5767168, 126877696, 0), (13339459584, 5767168, 132644864, 0), (13362528256, 5767168, 138412032, 0), (13368295424, 5767168, 144179200, 0), (13356761088, 5767168, 149946368, 0), (13397131264, 5767168, 155713536, 0), (13402898432, 5767168, 161480704, 0), (13391364096, 5767168, 167247872, 0), (13500940288, 5767168, 173015040, 0), (13506707456, 5767168, 178782208, 0), (13495173120, 5767168, 184549376, 0), (13518241792, 5767168, 190316544, 0), (13524008960, 5767168, 196083712, 0), (13512474624, 5767168, 201850880, 0), (13535543296, 5767168, 207618048, 0), (13541310464, 5767168, 213385216, 0), (13529776128, 5767168, 219152384, 0), (13587447808, 5767168, 224919552, 0), (13593214976, 5767168, 230686720, 0), (13581680640, 5767168, 236453888, 0), (13673955328, 5767168, 242221056, 0), (13679722496, 5767168, 247988224, 0), (13668188160, 5767168, 253755392, 0), (13725859840, 5767168, 259522560, 0), (13731627008, 5767168, 265289728, 0), (13720092672, 5767168, 271056896, 0), (13812367360, 5767168, 276824064, 0), (13818134528, 5767168, 282591232, 0), (13806600192, 5767168, 288358400, 0), (13846970368, 5767168, 294125568, 0), (13852737536, 5767168, 299892736, 0), (13841203200, 5767168, 305659904, 0), (13898874880, 5767168, 311427072, 0), (13904642048, 5767168, 317194240, 0), (13893107712, 5767168, 322961408, 0), (13916176384, 5767168, 328728576, 0), (13921943552, 5767168, 334495744, 0), (13910409216, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814338630>, 2: <capsule object NULL at 0x74a6bc334900>}
DEBUG 01-15 16:08:52.911548.911548 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.911000.911000 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.911948.911948 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 55fbdbfa-4669-4589-ad23-42899b323192
DEBUG 01-15 16:08:52.911358.911358 cuda_h.py:19] end move_flatidxs cost 0.0008580684661865234 seconds
DEBUG 01-15 16:08:52.911644.911644 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.911910.911910 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:52.912219.912219 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 55fbdbfa-4669-4589-ad23-42899b323192
DEBUG 01-15 16:08:52.913053.913053 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030677318572998047 seconds
DEBUG 01-15 16:08:52.913585.913585 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.917564.917564 cuda_h.py:19] end restore2model cost 0.003877878189086914 seconds
DEBUG 01-15 16:08:52.917838.917838 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007406949996948242 seconds
DEBUG 01-15 16:08:52.917640.917640 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:52.917202.917202 cuda_h.py:19] end gpu_sexperts cost 0.0003447532653808594 seconds
DEBUG 01-15 16:08:52.917039.917039 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:52.917968.917968 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.193450927734375e-05 seconds
DEBUG 01-15 16:08:52.917048.917048 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:52.918042.918042 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.919297.919297 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012409687042236328 seconds
DEBUG 01-15 16:08:52.919193.919193 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.919711.919711 cuda_h.py:19] end gpu_group_list cost 0.00021147727966308594 seconds
DEBUG 01-15 16:08:52.920032.920032 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:52.921392.921392 cuda_h.py:19] end group_tensors cost 0.009788036346435547 seconds
DEBUG 01-15 16:08:52.922833.922833 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:52.922220.922220 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014078617095947266 seconds
DEBUG 01-15 16:08:52.922032.922032 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:52.922965.922965 cuda_h.py:19] end gpu_group_list cost 0.000331878662109375 seconds
DEBUG 01-15 16:08:52.924220.924220 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:52.924329.924329 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 55fbdbfa-4669-4589-ad23-42899b323192
DEBUG 01-15 16:08:52.925611.925611 cuda_h.py:19] end group pad cost 0.0036046504974365234 seconds
DEBUG 01-15 16:08:52.925116.925116 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:52.947631.947631 client.py:127] Model loaded
DEBUG 01-15 16:08:52.948554.948554 cuda_h.py:19] end wait_experts_multi_device cost 0.0241696834564209 seconds
DEBUG 01-15 16:08:52.948030.948030 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:52.952210.952210 cuda_h.py:19] end group_einsum cost 0.027047157287597656 seconds
DEBUG 01-15 16:08:52.953613.953613 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:52.955564.955564 cuda_h.py:19] end get_outputs_cpu1 cost 0.002641916275024414 seconds
DEBUG 01-15 16:08:52.956342.956342 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0463871955871582 seconds
DEBUG 01-15 16:08:52.957617.957617 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0087127685546875 seconds
DEBUG 01-15 16:08:52.957657.957657 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:52.959518.959518 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.960793.960793 cuda_h.py:19] end index_scatter cost 0.00014972686767578125 seconds
DEBUG 01-15 16:08:52.960150.960150 cuda_h.py:19] end cpuoutputsdeal cost 0.0029571056365966797 seconds
DEBUG 01-15 16:08:52.960779.960779 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:52.961762.961762 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.961843.961843 cuda_h.py:19] end gpu_group_tensor cost 0.0003361701965332031 seconds
DEBUG 01-15 16:08:52.961310.961310 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:52.962476.962476 cuda_h.py:19] end gpu_group_tensor cost 0.00031948089599609375 seconds
DEBUG 01-15 16:08:52.962179.962179 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.963400.963400 cuda_h.py:19] end gpu_group_einsum cost 0.001138448715209961 seconds
DEBUG 01-15 16:08:52.963600.963600 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:52.964553.964553 cuda_h.py:19] end gpu_group_einsum cost 0.0010366439819335938 seconds
DEBUG 01-15 16:08:52.965747.965747 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.965139.965139 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.966246.966246 cuda_h.py:19] end all_expert_outputs_slices cost 0.00047850608825683594 seconds
DEBUG 01-15 16:08:52.966660.966660 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.966746.966746 cuda_h.py:19] end concat_expert_out cost 0.0001246929168701172 seconds
DEBUG 01-15 16:08:52.966752.966752 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.966846.966846 cuda_h.py:19] end index_scatter cost 0.0001423358917236328 seconds
DEBUG 01-15 16:08:52.967188.967188 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0019791126251220703 seconds
DEBUG 01-15 16:08:52.967626.967626 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:52.967406.967406 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:52.968667.968667 cuda_h.py:19] end all_expert_outputs_slices cost 0.00037169456481933594 seconds
DEBUG 01-15 16:08:52.968544.968544 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:52.968949.968949 cuda_h.py:19] end concat_expert_out cost 0.0001437664031982422 seconds
DEBUG 01-15 16:08:52.968492.968492 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:52.968387.968387 cuda_h.py:19] end index_scatter cost 0.00013208389282226562 seconds
DEBUG 01-15 16:08:52.968238.968238 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0013706684112548828 seconds
DEBUG 01-15 16:08:52.969344.969344 cuda_h.py:19] end gpu_experts_multi_device cost 0.05112886428833008 seconds
DEBUG 01-15 16:08:52.969867.969867 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.06536746025085449 seconds
DEBUG 01-15 16:08:52.970828.970828 cuda_h.py:19] end prefill_layer cost 0.07533931732177734 seconds
DEBUG 01-15 16:08:52.970753.970753 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:08:52.970749.970749 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:52.970082.970082 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:08:52.970687.970687 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:08:52.970403.970403 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:08:52.970479.970479 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:08:52.970957.970957 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 9.632110595703125e-05 seconds
DEBUG 01-15 16:08:52.970468.970468 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:52.970511.970511 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:52.970244.970244 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:52.971117.971117 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:52.971508.971508 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:52.971772.971772 cuda_h.py:19] end allocate_cuda_memory cost 0.0004372596740722656 seconds
DEBUG 01-15 16:08:52.972414.972414 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:52.972485.972485 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.972913.972913 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.972071.972071 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4b47fb5a-bf21-49d1-b5ae-7eb11a5ca66c
DEBUG 01-15 16:08:52.972357.972357 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.972751.972751 cuda_h.py:10] start self_attn
INFO 01-15 16:08:52.975596.975596 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4b47fb5a-bf21-49d1-b5ae-7eb11a5ca66c
DEBUG 01-15 16:08:52.975455.975455 cuda_h.py:19] end load_into_gpu_async cost 0.0032787322998046875 seconds
DEBUG 01-15 16:08:52.975226.975226 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:52.975399.975399 cuda_h.py:19] end restore_tensors2 cost 0.0001544952392578125 seconds
DEBUG 01-15 16:08:52.975695.975695 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004645109176635742 seconds
INFO 01-15 16:08:52.976562.976562 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4b47fb5a-bf21-49d1-b5ae-7eb11a5ca66c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:52.981745.981745 cuda_h.py:19] end self_attn cost 0.008788347244262695 seconds
DEBUG 01-15 16:08:52.982911.982911 cuda_h.py:19] end iln_self_attn_paln cost 0.012169837951660156 seconds
INFO 01-15 16:08:52.983163.983163 client.py:127] Model loaded
DEBUG 01-15 16:08:52.983023.983023 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:08:52.983770.983770 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.983933.983933 cuda_h.py:10] start gate
DEBUG 01-15 16:08:52.984583.984583 cuda_h.py:19] end restore2model cost 0.0012364387512207031 seconds
DEBUG 01-15 16:08:52.985358.985358 cuda_h.py:19] end sllm_worker_task cost 0.014299392700195312 seconds
DEBUG 01-15 16:08:52.986536.986536 cuda_h.py:19] end gate cost 0.0011794567108154297 seconds
DEBUG 01-15 16:08:52.986930.986930 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:52.987123.987123 lmp.py:1912] 
DEBUG 01-15 16:08:52.987123.987123 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:52.987860.987860 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:52.987484.987484 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:52.987578.987578 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:52.987572.987572 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:52.987328.987328 lmp.py:1917] 
DEBUG 01-15 16:08:52.987328.987328 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:52.987038.987038 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:52.987761.987761 lmp.py:1935]   Expert 39 |     16 | CPU
DEBUG 01-15 16:08:52.987901.987901 lmp.py:1935]   Expert 13 |     17 | CPU
DEBUG 01-15 16:08:52.987611.987611 lmp.py:1935]   Expert 49 |     37 | CPU
DEBUG 01-15 16:08:52.987652.987652 lmp.py:1935]   Expert 35 |     54 | CPU
DEBUG 01-15 16:08:52.987500.987500 lmp.py:1935]   Expert 19 |     63 | CPU
DEBUG 01-15 16:08:52.987587.987587 lmp.py:1935]   Expert  9 |     73 | CPU
DEBUG 01-15 16:08:52.987436.987436 lmp.py:1935]   Expert 26 |     74 | CPU
DEBUG 01-15 16:08:52.987046.987046 lmp.py:1935]   Expert 32 |     76 | CPU
DEBUG 01-15 16:08:52.987133.987133 lmp.py:1935]   Expert 41 |     76 | CPU
DEBUG 01-15 16:08:52.987505.987505 lmp.py:1935]   Expert 33 |     81 | CPU
DEBUG 01-15 16:08:52.987354.987354 lmp.py:1935]   Expert 23 |     86 | CPU
DEBUG 01-15 16:08:52.987964.987964 lmp.py:1935]   Expert 46 |     89 | CPU
DEBUG 01-15 16:08:52.987813.987813 lmp.py:1935]   Expert 18 |     91 | CPU
DEBUG 01-15 16:08:52.987807.987807 lmp.py:1935]   Expert 31 |     93 | CPU
DEBUG 01-15 16:08:52.987563.987563 lmp.py:1935]   Expert 38 |     96 | CPU
DEBUG 01-15 16:08:52.987557.987557 lmp.py:1935]   Expert  3 |    103 | CPU
DEBUG 01-15 16:08:52.988598.988598 lmp.py:1935]   Expert  6 |    106 | CPU
DEBUG 01-15 16:08:52.988447.988447 lmp.py:1935]   Expert 17 |    106 | CPU
DEBUG 01-15 16:08:52.988295.988295 lmp.py:1935]   Expert 20 |    117 | CPU
DEBUG 01-15 16:08:52.988144.988144 lmp.py:1935]   Expert 40 |    128 | CPU
DEBUG 01-15 16:08:52.988754.988754 lmp.py:1935]   Expert 62 |    131 | CPU
DEBUG 01-15 16:08:52.988888.988888 lmp.py:1935]   Expert 15 |    132 | CPU
DEBUG 01-15 16:08:52.988259.988259 lmp.py:1935]   Expert 61 |    132 | CPU
DEBUG 01-15 16:08:52.988393.988393 lmp.py:1935]   Expert 50 |    134 | CPU
DEBUG 01-15 16:08:52.988242.988242 lmp.py:1935]   Expert 43 |    135 | CPU
DEBUG 01-15 16:08:52.988474.988474 lmp.py:1935]   Expert 44 |    137 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988853.988853 lmp.py:1935]   Expert 63 |    138 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988993.988993 lmp.py:1935]   Expert 59 |    139 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988226.988226 lmp.py:1935]   Expert 16 |    140 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988857.988857 lmp.py:1935]   Expert 42 |    144 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988335.988335 lmp.py:1935]   Expert  2 |    149 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988329.988329 lmp.py:1935]   Expert 36 |    149 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988608.988608 lmp.py:1935]   Expert 10 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988795.988795 lmp.py:1935]   Expert  5 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988220.988220 lmp.py:1935]   Expert 34 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988883.988883 lmp.py:1935]   Expert 52 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988116.988116 lmp.py:1935]   Expert 27 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988633.988633 lmp.py:1935]   Expert 45 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988389.988389 lmp.py:1935]   Expert 60 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988430.988430 lmp.py:1935]   Expert 48 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988948.988948 lmp.py:1935]   Expert 51 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988657.988657 lmp.py:1935]   Expert 56 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988367.988367 lmp.py:1935]   Expert 24 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988792.988792 lmp.py:1935]   Expert  7 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988310.988310 lmp.py:1935]   Expert 53 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988350.988350 lmp.py:1935]   Expert  8 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.988676.988676 lmp.py:1935]   Expert 47 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.988955.988955 lmp.py:1935]   Expert 57 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.989757.989757 lmp.py:1935]   Expert 29 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.989321.989321 lmp.py:1935]   Expert 21 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.989123.989123 lmp.py:1935]   Expert  0 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.989641.989641 lmp.py:1935]   Expert  4 |    287 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.989351.989351 lmp.py:1935]   Expert 14 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.989014.989014 lmp.py:1935]   Expert 22 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.989201.989201 lmp.py:1935]   Expert 58 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.989957.989957 lmp.py:1935]   Expert 55 |    318 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.989236.989236 lmp.py:1935]   Expert 37 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.989515.989515 lmp.py:1935]   Expert  1 |    321 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.989794.989794 lmp.py:1935]   Expert 54 |    332 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.989073.989073 lmp.py:1935]   Expert 28 |    363 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.989260.989260 lmp.py:1935]   Expert 12 |    378 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.989446.989446 lmp.py:1935]   Expert 25 |    399 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.989633.989633 lmp.py:1935]   Expert 11 |    401 | GPU2(cuda:2)
DEBUG 01-15 16:08:52.989627.989627 lmp.py:1935]   Expert 30 |    838 | GPU1(cuda:1)
DEBUG 01-15 16:08:52.989522.989522 lmp.py:1937] 
DEBUG 01-15 16:08:52.989522.989522 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:52.989324.989324 lmp.py:1938]   CPU:   2246 tokens
DEBUG 01-15 16:08:52.989842.989842 lmp.py:1942]   cuda:1:   5023 tokens (19 experts)
DEBUG 01-15 16:08:52.989644.989644 lmp.py:1942]   cuda:2:   5019 tokens (20 experts)
DEBUG 01-15 16:08:52.989255.989255 lmp.py:1943]   Total GPU:  10042 tokens
DEBUG 01-15 16:08:52.989865.989865 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:52.989865.989865 lmp.py:1944] 
DEBUG 01-15 16:08:52.989919.989919 cuda_h.py:19] end experts_map_get cost 0.0031719207763671875 seconds
DEBUG 01-15 16:08:52.989346.989346 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:52.989176.989176 lmp.py:1953] 
DEBUG 01-15 16:08:52.989176.989176 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:52.989722.989722 cuda_h.py:19] end cpu_experts_submit cost 8.440017700195312e-05 seconds
DEBUG 01-15 16:08:52.989246.989246 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:52.990382.990382 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:52.991958.991958 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.44.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.44.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.44.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.59.up_proj.weight': 322961408}, 2: {'model.layers.11.mlp.experts.2.gate_proj.weight': 0, 'model.layers.11.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.51.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.51.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.51.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.11.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.11.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.11.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (13950779392, 5767168, 17301504, 0), (13956546560, 5767168, 23068672, 0), (13945012224, 5767168, 28835840, 0), (14019985408, 5767168, 34603008, 0), (14025752576, 5767168, 40370176, 0), (14014218240, 5767168, 46137344, 0), (14054588416, 5767168, 51904512, 0), (14060355584, 5767168, 57671680, 0), (14048821248, 5767168, 63438848, 0), (14106492928, 5767168, 69206016, 0), (14112260096, 5767168, 74973184, 0), (14100725760, 5767168, 80740352, 0), (14296809472, 5767168, 86507520, 0), (14302576640, 5767168, 92274688, 0), (14291042304, 5767168, 98041856, 0), (14348713984, 5767168, 103809024, 0), (14354481152, 5767168, 109576192, 0), (14342946816, 5767168, 115343360, 0), (14417920000, 5767168, 121110528, 0), (14423687168, 5767168, 126877696, 0), (14412152832, 5767168, 132644864, 0), (14435221504, 5767168, 138412032, 0), (14440988672, 5767168, 144179200, 0), (14429454336, 5767168, 149946368, 0), (14452523008, 5767168, 155713536, 0), (14458290176, 5767168, 161480704, 0), (14446755840, 5767168, 167247872, 0), (14660141056, 5767168, 173015040, 0), (14665908224, 5767168, 178782208, 0), (14654373888, 5767168, 184549376, 0), (14694744064, 5767168, 190316544, 0), (14700511232, 5767168, 196083712, 0), (14688976896, 5767168, 201850880, 0), (14712045568, 5767168, 207618048, 0), (14717812736, 5767168, 213385216, 0), (14706278400, 5767168, 219152384, 0), (14746648576, 5767168, 224919552, 0), (14752415744, 5767168, 230686720, 0), (14740881408, 5767168, 236453888, 0), (14763950080, 5767168, 242221056, 0), (14769717248, 5767168, 247988224, 0), (14758182912, 5767168, 253755392, 0), (14833156096, 5767168, 259522560, 0), (14838923264, 5767168, 265289728, 0), (14827388928, 5767168, 271056896, 0), (14885060608, 5767168, 276824064, 0), (14890827776, 5767168, 282591232, 0), (14879293440, 5767168, 288358400, 0), (14936965120, 5767168, 294125568, 0), (14942732288, 5767168, 299892736, 0), (14931197952, 5767168, 305659904, 0), (14954266624, 5767168, 311427072, 0), (14960033792, 5767168, 317194240, 0), (14948499456, 5767168, 322961408, 0)], 2: [(13968080896, 5767168, 0, 0), (13973848064, 5767168, 5767168, 0), (13962313728, 5767168, 11534336, 0), (14002683904, 5767168, 17301504, 0), (14008451072, 5767168, 23068672, 0), (13996916736, 5767168, 28835840, 0), (14071889920, 5767168, 34603008, 0), (14077657088, 5767168, 40370176, 0), (14066122752, 5767168, 46137344, 0), (14123794432, 5767168, 51904512, 0), (14129561600, 5767168, 57671680, 0), (14118027264, 5767168, 63438848, 0), (14141095936, 5767168, 69206016, 0), (14146863104, 5767168, 74973184, 0), (14135328768, 5767168, 80740352, 0), (14175698944, 5767168, 86507520, 0), (14181466112, 5767168, 92274688, 0), (14169931776, 5767168, 98041856, 0), (14210301952, 5767168, 103809024, 0), (14216069120, 5767168, 109576192, 0), (14204534784, 5767168, 115343360, 0), (14314110976, 5767168, 121110528, 0), (14319878144, 5767168, 126877696, 0), (14308343808, 5767168, 132644864, 0), (14366015488, 5767168, 138412032, 0), (14371782656, 5767168, 144179200, 0), (14360248320, 5767168, 149946368, 0), (14400618496, 5767168, 155713536, 0), (14406385664, 5767168, 161480704, 0), (14394851328, 5767168, 167247872, 0), (14521729024, 5767168, 173015040, 0), (14527496192, 5767168, 178782208, 0), (14515961856, 5767168, 184549376, 0), (14556332032, 5767168, 190316544, 0), (14562099200, 5767168, 196083712, 0), (14550564864, 5767168, 201850880, 0), (14573633536, 5767168, 207618048, 0), (14579400704, 5767168, 213385216, 0), (14567866368, 5767168, 219152384, 0), (14815854592, 5767168, 224919552, 0), (14821621760, 5767168, 230686720, 0), (14810087424, 5767168, 236453888, 0), (14850457600, 5767168, 242221056, 0), (14856224768, 5767168, 247988224, 0), (14844690432, 5767168, 253755392, 0), (14867759104, 5767168, 259522560, 0), (14873526272, 5767168, 265289728, 0), (14861991936, 5767168, 271056896, 0), (14902362112, 5767168, 276824064, 0), (14908129280, 5767168, 282591232, 0), (14896594944, 5767168, 288358400, 0), (14919663616, 5767168, 294125568, 0), (14925430784, 5767168, 299892736, 0), (14913896448, 5767168, 305659904, 0), (14971568128, 5767168, 311427072, 0), (14977335296, 5767168, 317194240, 0), (14965800960, 5767168, 322961408, 0), (15023472640, 5767168, 328728576, 0), (15029239808, 5767168, 334495744, 0), (15017705472, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8144b08d0>, 2: <capsule object NULL at 0x74a6bc334660>}
DEBUG 01-15 16:08:52.991902.991902 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:52.991554.991554 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:52.991622.991622 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c6518769-4cda-4951-b883-4271893e5f71
DEBUG 01-15 16:08:52.992507.992507 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:52.992335.992335 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:52.992810.992810 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:52.993232.993232 cuda_h.py:19] end move_flatidxs cost 0.0008661746978759766 seconds
DEBUG 01-15 16:08:52.993029.993029 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:52.994902.994902 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c6518769-4cda-4951-b883-4271893e5f71
DEBUG 01-15 16:08:52.995053.995053 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005331993103027344 seconds
DEBUG 01-15 16:08:52.995550.995550 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:52.999191.999191 cuda_h.py:19] end group_tensors cost 0.005579710006713867 seconds
DEBUG 01-15 16:08:52.999158.999158 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.003825.003825 cuda_h.py:19] end group pad cost 0.003818511962890625 seconds
DEBUG 01-15 16:08:53.003099.003099 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:53.008275.008275 cuda_h.py:19] end restore2model cost 0.012157917022705078 seconds
DEBUG 01-15 16:08:53.008554.008554 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.018614530563354492 seconds
DEBUG 01-15 16:08:53.008845.008845 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.010161.010161 cuda_h.py:19] end gpu_sexperts cost 0.0012311935424804688 seconds
DEBUG 01-15 16:08:53.010425.010425 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.010368.010368 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 4.696846008300781e-05 seconds
DEBUG 01-15 16:08:53.010323.010323 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.010550.010550 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.012978.012978 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016164779663085938 seconds
DEBUG 01-15 16:08:53.012552.012552 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.012461.012461 cuda_h.py:19] end gpu_group_list cost 0.0004150867462158203 seconds
DEBUG 01-15 16:08:53.016132.016132 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.019408.019408 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.002681255340576172 seconds
DEBUG 01-15 16:08:53.019705.019705 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.020348.020348 cuda_h.py:19] end gpu_group_list cost 0.0005707740783691406 seconds
DEBUG 01-15 16:08:53.021130.021130 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.021033.021033 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c6518769-4cda-4951-b883-4271893e5f71
DEBUG 01-15 16:08:53.027842.027842 cuda_h.py:19] end group_einsum cost 0.02385687828063965 seconds
DEBUG 01-15 16:08:53.028907.028907 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:08:53.029993.029993 client.py:127] Model loaded
DEBUG 01-15 16:08:53.029217.029217 cuda_h.py:19] end wait_experts_multi_device cost 0.007852315902709961 seconds
DEBUG 01-15 16:08:53.029278.029278 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.030972.030972 cuda_h.py:19] end get_outputs_cpu1 cost 0.0024788379669189453 seconds
DEBUG 01-15 16:08:53.031456.031456 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03914928436279297 seconds
DEBUG 01-15 16:08:53.032368.032368 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0030028820037841797 seconds
DEBUG 01-15 16:08:53.032077.032077 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.034533.034533 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.035603.035603 cuda_h.py:19] end index_scatter cost 0.0001468658447265625 seconds
DEBUG 01-15 16:08:53.035804.035804 cuda_h.py:19] end cpuoutputsdeal cost 0.003025054931640625 seconds
DEBUG 01-15 16:08:53.035605.035605 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.035012.035012 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.036630.036630 cuda_h.py:19] end gpu_group_tensor cost 0.00033664703369140625 seconds
DEBUG 01-15 16:08:53.036957.036957 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.036753.036753 cuda_h.py:19] end gpu_group_tensor cost 0.00032639503479003906 seconds
DEBUG 01-15 16:08:53.037741.037741 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.038763.038763 cuda_h.py:19] end gpu_group_einsum cost 0.0011136531829833984 seconds
DEBUG 01-15 16:08:53.038347.038347 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.039228.039228 cuda_h.py:19] end gpu_group_einsum cost 0.001054525375366211 seconds
DEBUG 01-15 16:08:53.040614.040614 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.040191.040191 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.041108.041108 cuda_h.py:19] end all_expert_outputs_slices cost 0.00046944618225097656 seconds
DEBUG 01-15 16:08:53.041568.041568 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.041998.041998 cuda_h.py:19] end concat_expert_out cost 0.00010800361633300781 seconds
DEBUG 01-15 16:08:53.041259.041259 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.041489.041489 cuda_h.py:19] end index_scatter cost 8.106231689453125e-05 seconds
DEBUG 01-15 16:08:53.041400.041400 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0016224384307861328 seconds
DEBUG 01-15 16:08:53.041841.041841 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.042619.042619 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.042878.042878 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001785755157470703 seconds
DEBUG 01-15 16:08:53.042840.042840 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.042697.042697 cuda_h.py:19] end concat_expert_out cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:08:53.042561.042561 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.042254.042254 cuda_h.py:19] end index_scatter cost 6.985664367675781e-05 seconds
DEBUG 01-15 16:08:53.042606.042606 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000652313232421875 seconds
DEBUG 01-15 16:08:53.042013.042013 cuda_h.py:19] end gpu_experts_multi_device cost 0.032239675521850586 seconds
DEBUG 01-15 16:08:53.042170.042170 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.05930137634277344 seconds
DEBUG 01-15 16:08:53.043102.043102 cuda_h.py:19] end prefill_layer cost 0.07297992706298828 seconds
DEBUG 01-15 16:08:53.043674.043674 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:08:53.043496.043496 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.043213.043213 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:08:53.043453.043453 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:08:53.043030.043030 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:08:53.043504.043504 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 6.175041198730469e-05 seconds
DEBUG 01-15 16:08:53.043518.043518 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 0.00011301040649414062 seconds
DEBUG 01-15 16:08:53.043228.043228 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.043562.043562 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.043684.043684 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.044096.044096 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.044669.044669 cuda_h.py:19] end allocate_cuda_memory cost 0.0002372264862060547 seconds
DEBUG 01-15 16:08:53.044321.044321 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.044614.044614 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.044635.044635 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.044484.044484 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6de7e0f6-3534-4e6d-99ce-32fa28edbf9a
DEBUG 01-15 16:08:53.044951.044951 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.044666.044666 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.045674.045674 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.046750.046750 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6de7e0f6-3534-4e6d-99ce-32fa28edbf9a
DEBUG 01-15 16:08:53.046593.046593 cuda_h.py:19] end load_into_gpu_async cost 0.0021638870239257812 seconds
DEBUG 01-15 16:08:53.046250.046250 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.046777.046777 cuda_h.py:19] end restore_tensors2 cost 7.843971252441406e-05 seconds
DEBUG 01-15 16:08:53.046917.046917 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002775907516479492 seconds
INFO 01-15 16:08:53.046960.046960 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6de7e0f6-3534-4e6d-99ce-32fa28edbf9a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.049556.049556 cuda_h.py:19] end self_attn cost 0.004278421401977539 seconds
DEBUG 01-15 16:08:53.050797.050797 cuda_h.py:19] end iln_self_attn_paln cost 0.0062656402587890625 seconds
DEBUG 01-15 16:08:53.050469.050469 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:08:53.050152.050152 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.051635.051635 cuda_h.py:19] end gate cost 0.0008983612060546875 seconds
DEBUG 01-15 16:08:53.051014.051014 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.051826.051826 lmp.py:1912] 
DEBUG 01-15 16:08:53.051826.051826 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.051893.051893 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.051040.051040 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.051181.051181 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.051460.051460 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.051832.051832 lmp.py:1917] 
DEBUG 01-15 16:08:53.051832.051832 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.051396.051396 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.052781.052781 lmp.py:1935]   Expert 12 |     18 | CPU
DEBUG 01-15 16:08:53.052345.052345 lmp.py:1935]   Expert 47 |     26 | CPU
DEBUG 01-15 16:08:53.052240.052240 lmp.py:1935]   Expert 38 |     31 | CPU
DEBUG 01-15 16:08:53.052135.052135 lmp.py:1935]   Expert 27 |     34 | CPU
DEBUG 01-15 16:08:53.052745.052745 lmp.py:1935]   Expert 16 |     37 | CPU
DEBUG 01-15 16:08:53.052878.052878 lmp.py:1935]   Expert 52 |     39 | CPU
DEBUG 01-15 16:08:53.052250.052250 lmp.py:1935]   Expert 63 |     41 | CPU
DEBUG 01-15 16:08:53.052337.052337 lmp.py:1935]   Expert  4 |     58 | CPU
DEBUG 01-15 16:08:53.052471.052471 lmp.py:1935]   Expert 43 |     60 | CPU
DEBUG 01-15 16:08:53.052412.052412 lmp.py:1935]   Expert 44 |     63 | CPU
DEBUG 01-15 16:08:53.052830.052830 lmp.py:1935]   Expert 61 |     65 | CPU
DEBUG 01-15 16:08:53.052533.052533 lmp.py:1935]   Expert 34 |     76 | CPU
DEBUG 01-15 16:08:53.052474.052474 lmp.py:1935]   Expert 53 |     83 | CPU
DEBUG 01-15 16:08:53.052892.052892 lmp.py:1935]   Expert  0 |     86 | CPU
DEBUG 01-15 16:08:53.052039.052039 lmp.py:1935]   Expert 32 |     90 | CPU
DEBUG 01-15 16:08:53.052934.052934 lmp.py:1935]   Expert 37 |     91 | CPU
DEBUG 01-15 16:08:53.052545.052545 lmp.py:1935]   Expert 13 |    104 | CPU
DEBUG 01-15 16:08:53.052678.052678 lmp.py:1935]   Expert 39 |    115 | CPU
DEBUG 01-15 16:08:53.052811.052811 lmp.py:1935]   Expert 21 |    117 | CPU
DEBUG 01-15 16:08:53.052229.052229 lmp.py:1935]   Expert 11 |    121 | CPU
DEBUG 01-15 16:08:53.052886.052886 lmp.py:1935]   Expert 20 |    127 | CPU
DEBUG 01-15 16:08:53.052827.052827 lmp.py:1935]   Expert  8 |    131 | CPU
DEBUG 01-15 16:08:53.052245.052245 lmp.py:1935]   Expert 60 |    132 | CPU
DEBUG 01-15 16:08:53.052333.052333 lmp.py:1935]   Expert 14 |    138 | CPU
DEBUG 01-15 16:08:53.052943.052943 lmp.py:1935]   Expert 57 |    139 | CPU
DEBUG 01-15 16:08:53.052176.052176 lmp.py:1935]   Expert 22 |    141 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.052455.052455 lmp.py:1935]   Expert  2 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.052972.052972 lmp.py:1935]   Expert 45 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.052536.052536 lmp.py:1935]   Expert 23 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.052146.052146 lmp.py:1935]   Expert 17 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.052757.052757 lmp.py:1935]   Expert 18 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.052844.052844 lmp.py:1935]   Expert  7 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.052931.052931 lmp.py:1935]   Expert 58 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.052495.052495 lmp.py:1935]   Expert 30 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.052535.052535 lmp.py:1935]   Expert 42 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053576.053576 lmp.py:1935]   Expert 48 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053710.053710 lmp.py:1935]   Expert 49 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053320.053320 lmp.py:1935]   Expert 55 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053645.053645 lmp.py:1935]   Expert 62 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053017.053017 lmp.py:1935]   Expert 35 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053866.053866 lmp.py:1935]   Expert 51 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053191.053191 lmp.py:1935]   Expert 29 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053994.053994 lmp.py:1935]   Expert  6 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053750.053750 lmp.py:1935]   Expert 25 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053121.053121 lmp.py:1935]   Expert 36 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053493.053493 lmp.py:1935]   Expert  1 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053865.053865 lmp.py:1935]   Expert 31 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053998.053998 lmp.py:1935]   Expert 28 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053370.053370 lmp.py:1935]   Expert 54 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053980.053980 lmp.py:1935]   Expert  5 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053320.053320 lmp.py:1935]   Expert 41 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053076.053076 lmp.py:1935]   Expert  9 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053163.053163 lmp.py:1935]   Expert 19 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053965.053965 lmp.py:1935]   Expert 24 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053483.053483 lmp.py:1935]   Expert 50 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053093.053093 lmp.py:1935]   Expert 46 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053465.053465 lmp.py:1935]   Expert 59 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053598.053598 lmp.py:1935]   Expert 56 |    371 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053493.053493 lmp.py:1935]   Expert 26 |    405 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053342.053342 lmp.py:1935]   Expert 33 |    421 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053713.053713 lmp.py:1935]   Expert  3 |    589 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053562.053562 lmp.py:1935]   Expert 15 |    648 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053888.053888 lmp.py:1935]   Expert 10 |    650 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.053736.053736 lmp.py:1935]   Expert 40 |    787 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.053439.053439 lmp.py:1937] 
DEBUG 01-15 16:08:53.053439.053439 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.053572.053572 lmp.py:1938]   CPU:   2022 tokens
DEBUG 01-15 16:08:53.053659.053659 lmp.py:1942]   cuda:1:   5203 tokens (20 experts)
DEBUG 01-15 16:08:53.053793.053793 lmp.py:1942]   cuda:2:   5063 tokens (19 experts)
DEBUG 01-15 16:08:53.053734.053734 lmp.py:1943]   Total GPU:  10266 tokens
DEBUG 01-15 16:08:53.054391.054391 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.054391.054391 lmp.py:1944] 
DEBUG 01-15 16:08:53.054253.054253 cuda_h.py:19] end experts_map_get cost 0.0027332305908203125 seconds
DEBUG 01-15 16:08:53.054521.054521 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.054959.054959 lmp.py:1953] 
DEBUG 01-15 16:08:53.054959.054959 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.054445.054445 cuda_h.py:19] end cpu_experts_submit cost 7.510185241699219e-05 seconds
DEBUG 01-15 16:08:53.054970.054970 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.054860.054860 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.055030.055030 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.1.gate_proj.weight': 0, 'model.layers.12.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.17.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.17.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.17.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.31.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.31.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.31.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.12.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.12.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.12.mlp.experts.59.up_proj.weight': 340262912}, 2: {'model.layers.12.mlp.experts.5.gate_proj.weight': 0, 'model.layers.12.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(15058075648, 5767168, 0, 0), (15063842816, 5767168, 5767168, 0), (15052308480, 5767168, 11534336, 0), (15075377152, 5767168, 17301504, 0), (15081144320, 5767168, 23068672, 0), (15069609984, 5767168, 28835840, 0), (15092678656, 5767168, 34603008, 0), (15098445824, 5767168, 40370176, 0), (15086911488, 5767168, 46137344, 0), (15334899712, 5767168, 51904512, 0), (15340666880, 5767168, 57671680, 0), (15329132544, 5767168, 63438848, 0), (15352201216, 5767168, 69206016, 0), (15357968384, 5767168, 74973184, 0), (15346434048, 5767168, 80740352, 0), (15369502720, 5767168, 86507520, 0), (15375269888, 5767168, 92274688, 0), (15363735552, 5767168, 98041856, 0), (15421407232, 5767168, 103809024, 0), (15427174400, 5767168, 109576192, 0), (15415640064, 5767168, 115343360, 0), (15456010240, 5767168, 121110528, 0), (15461777408, 5767168, 126877696, 0), (15450243072, 5767168, 132644864, 0), (15473311744, 5767168, 138412032, 0), (15479078912, 5767168, 144179200, 0), (15467544576, 5767168, 149946368, 0), (15490613248, 5767168, 155713536, 0), (15496380416, 5767168, 161480704, 0), (15484846080, 5767168, 167247872, 0), (15542517760, 5767168, 173015040, 0), (15548284928, 5767168, 178782208, 0), (15536750592, 5767168, 184549376, 0), (15577120768, 5767168, 190316544, 0), (15582887936, 5767168, 196083712, 0), (15571353600, 5767168, 201850880, 0), (15732834304, 5767168, 207618048, 0), (15738601472, 5767168, 213385216, 0), (15727067136, 5767168, 219152384, 0), (15871246336, 5767168, 224919552, 0), (15877013504, 5767168, 230686720, 0), (15865479168, 5767168, 236453888, 0), (15888547840, 5767168, 242221056, 0), (15894315008, 5767168, 247988224, 0), (15882780672, 5767168, 253755392, 0), (15905849344, 5767168, 259522560, 0), (15911616512, 5767168, 265289728, 0), (15900082176, 5767168, 271056896, 0), (15923150848, 5767168, 276824064, 0), (15928918016, 5767168, 282591232, 0), (15917383680, 5767168, 288358400, 0), (15975055360, 5767168, 294125568, 0), (15980822528, 5767168, 299892736, 0), (15969288192, 5767168, 305659904, 0), (16044261376, 5767168, 311427072, 0), (16050028544, 5767168, 317194240, 0), (16038494208, 5767168, 322961408, 0), (16061562880, 5767168, 328728576, 0), (16067330048, 5767168, 334495744, 0), (16055795712, 5767168, 340262912, 0)], 2: [(15127281664, 5767168, 0, 0), (15133048832, 5767168, 5767168, 0), (15121514496, 5767168, 11534336, 0), (15144583168, 5767168, 17301504, 0), (15150350336, 5767168, 23068672, 0), (15138816000, 5767168, 28835840, 0), (15161884672, 5767168, 34603008, 0), (15167651840, 5767168, 40370176, 0), (15156117504, 5767168, 46137344, 0), (15196487680, 5767168, 51904512, 0), (15202254848, 5767168, 57671680, 0), (15190720512, 5767168, 63438848, 0), (15213789184, 5767168, 69206016, 0), (15219556352, 5767168, 74973184, 0), (15208022016, 5767168, 80740352, 0), (15300296704, 5767168, 86507520, 0), (15306063872, 5767168, 92274688, 0), (15294529536, 5767168, 98041856, 0), (15438708736, 5767168, 103809024, 0), (15444475904, 5767168, 109576192, 0), (15432941568, 5767168, 115343360, 0), (15525216256, 5767168, 121110528, 0), (15530983424, 5767168, 126877696, 0), (15519449088, 5767168, 132644864, 0), (15559819264, 5767168, 138412032, 0), (15565586432, 5767168, 144179200, 0), (15554052096, 5767168, 149946368, 0), (15611723776, 5767168, 155713536, 0), (15617490944, 5767168, 161480704, 0), (15605956608, 5767168, 167247872, 0), (15646326784, 5767168, 173015040, 0), (15652093952, 5767168, 178782208, 0), (15640559616, 5767168, 184549376, 0), (15663628288, 5767168, 190316544, 0), (15669395456, 5767168, 196083712, 0), (15657861120, 5767168, 201850880, 0), (15750135808, 5767168, 207618048, 0), (15755902976, 5767168, 213385216, 0), (15744368640, 5767168, 219152384, 0), (15767437312, 5767168, 224919552, 0), (15773204480, 5767168, 230686720, 0), (15761670144, 5767168, 236453888, 0), (15819341824, 5767168, 242221056, 0), (15825108992, 5767168, 247988224, 0), (15813574656, 5767168, 253755392, 0), (15836643328, 5767168, 259522560, 0), (15842410496, 5767168, 265289728, 0), (15830876160, 5767168, 271056896, 0), (15992356864, 5767168, 276824064, 0), (15998124032, 5767168, 282591232, 0), (15986589696, 5767168, 288358400, 0), (16009658368, 5767168, 294125568, 0), (16015425536, 5767168, 299892736, 0), (16003891200, 5767168, 305659904, 0), (16113467392, 5767168, 311427072, 0), (16119234560, 5767168, 317194240, 0), (16107700224, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729770>, 2: <capsule object NULL at 0x74a6607299b0>}
INFO 01-15 16:08:53.055282.055282 client.py:127] Model loaded
DEBUG 01-15 16:08:53.056173.056173 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.056285.056285 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.056673.056673 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.056482.056482 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:53.057547.057547 cuda_h.py:19] end restore2model cost 0.0011394023895263672 seconds
DEBUG 01-15 16:08:53.057538.057538 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.057717.057717 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ed0c60ed-a737-4f09-8aa5-45ef13489654
DEBUG 01-15 16:08:53.057489.057489 cuda_h.py:19] end move_flatidxs cost 0.0009224414825439453 seconds
DEBUG 01-15 16:08:53.058267.058267 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.058918.058918 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.057753.057753 cuda_h.py:19] end sllm_worker_task cost 0.013736724853515625 seconds
INFO 01-15 16:08:53.059377.059377 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ed0c60ed-a737-4f09-8aa5-45ef13489654
DEBUG 01-15 16:08:53.060673.060673 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.006030082702636719 seconds
DEBUG 01-15 16:08:53.060120.060120 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.063550.063550 cuda_h.py:19] end restore2model cost 0.0031244754791259766 seconds
DEBUG 01-15 16:08:53.063201.063201 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.009441852569580078 seconds
DEBUG 01-15 16:08:53.063473.063473 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.064590.064590 cuda_h.py:19] end gpu_sexperts cost 0.00029158592224121094 seconds
DEBUG 01-15 16:08:53.064758.064758 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.064018.064018 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.288818359375e-05 seconds
DEBUG 01-15 16:08:53.064714.064714 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.064563.064563 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.065663.065663 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000989675521850586 seconds
DEBUG 01-15 16:08:53.065606.065606 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.065369.065369 cuda_h.py:19] end gpu_group_list cost 0.00021696090698242188 seconds
DEBUG 01-15 16:08:53.066139.066139 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.067160.067160 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009734630584716797 seconds
DEBUG 01-15 16:08:53.067811.067811 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.067906.067906 cuda_h.py:19] end gpu_group_list cost 0.00021529197692871094 seconds
DEBUG 01-15 16:08:53.068121.068121 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.068003.068003 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ed0c60ed-a737-4f09-8aa5-45ef13489654
DEBUG 01-15 16:08:53.069170.069170 cuda_h.py:19] end group_tensors cost 0.011491537094116211 seconds
DEBUG 01-15 16:08:53.070226.070226 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.073275.073275 cuda_h.py:19] end group pad cost 0.0031783580780029297 seconds
DEBUG 01-15 16:08:53.073210.073210 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:53.096768.096768 client.py:127] Model loaded
DEBUG 01-15 16:08:53.096144.096144 cuda_h.py:19] end wait_experts_multi_device cost 0.0283203125 seconds
DEBUG 01-15 16:08:53.096622.096622 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.102983.102983 cuda_h.py:19] end group_einsum cost 0.029134750366210938 seconds
DEBUG 01-15 16:08:53.102161.102161 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.105726.105726 cuda_h.py:19] end get_outputs_cpu1 cost 0.0023565292358398438 seconds
DEBUG 01-15 16:08:53.106588.106588 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.049607038497924805 seconds
DEBUG 01-15 16:08:53.107420.107420 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010086774826049805 seconds
DEBUG 01-15 16:08:53.107221.107221 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.109533.109533 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.109484.109484 cuda_h.py:19] end index_scatter cost 0.00015115737915039062 seconds
DEBUG 01-15 16:08:53.110701.110701 cuda_h.py:19] end cpuoutputsdeal cost 0.002952098846435547 seconds
DEBUG 01-15 16:08:53.110522.110522 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.110220.110220 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.111235.111235 cuda_h.py:19] end gpu_group_tensor cost 0.00034356117248535156 seconds
DEBUG 01-15 16:08:53.111655.111655 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.111099.111099 cuda_h.py:19] end gpu_group_tensor cost 0.0003147125244140625 seconds
DEBUG 01-15 16:08:53.111034.111034 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.112543.112543 cuda_h.py:19] end gpu_group_einsum cost 0.0010204315185546875 seconds
DEBUG 01-15 16:08:53.113696.113696 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.114540.114540 cuda_h.py:19] end gpu_group_einsum cost 0.0008041858673095703 seconds
DEBUG 01-15 16:08:53.114203.114203 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.114574.114574 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.114272.114272 cuda_h.py:19] end all_expert_outputs_slices cost 0.00039005279541015625 seconds
DEBUG 01-15 16:08:53.115182.115182 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.115459.115459 cuda_h.py:19] end concat_expert_out cost 9.799003601074219e-05 seconds
DEBUG 01-15 16:08:53.115610.115610 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.115060.115060 cuda_h.py:19] end index_scatter cost 0.00011157989501953125 seconds
DEBUG 01-15 16:08:53.116367.116367 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0016355514526367188 seconds
DEBUG 01-15 16:08:53.116042.116042 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.116358.116358 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.116278.116278 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003254413604736328 seconds
DEBUG 01-15 16:08:53.116181.116181 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.116359.116359 cuda_h.py:19] end concat_expert_out cost 0.00010418891906738281 seconds
DEBUG 01-15 16:08:53.117781.117781 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.117210.117210 cuda_h.py:19] end index_scatter cost 0.0001010894775390625 seconds
DEBUG 01-15 16:08:53.117703.117703 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010728836059570312 seconds
DEBUG 01-15 16:08:53.117437.117437 cuda_h.py:19] end gpu_experts_multi_device cost 0.05319547653198242 seconds
DEBUG 01-15 16:08:53.117920.117920 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.06740450859069824 seconds
DEBUG 01-15 16:08:53.118996.118996 cuda_h.py:19] end prefill_layer cost 0.07478976249694824 seconds
DEBUG 01-15 16:08:53.118893.118893 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:08:53.118875.118875 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.118334.118334 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:08:53.118555.118555 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:08:53.118212.118212 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:08:53.118733.118733 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:08:53.118048.118048 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.119355.119355 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 0.00025343894958496094 seconds
DEBUG 01-15 16:08:53.119975.119975 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.119408.119408 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.119146.119146 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.119151.119151 cuda_h.py:19] end allocate_cuda_memory cost 0.00023889541625976562 seconds
DEBUG 01-15 16:08:53.119246.119246 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.119009.119009 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.119222.119222 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.119548.119548 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0932c9e2-cc3f-42d4-8229-86a84cbcf213
DEBUG 01-15 16:08:53.119214.119214 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.120142.120142 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.120753.120753 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.121086.121086 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0932c9e2-cc3f-42d4-8229-86a84cbcf213
DEBUG 01-15 16:08:53.121214.121214 cuda_h.py:19] end load_into_gpu_async cost 0.001767873764038086 seconds
DEBUG 01-15 16:08:53.121678.121678 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.121821.121821 cuda_h.py:19] end restore_tensors2 cost 7.82012939453125e-05 seconds
DEBUG 01-15 16:08:53.121530.121530 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024251937866210938 seconds
INFO 01-15 16:08:53.121804.121804 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0932c9e2-cc3f-42d4-8229-86a84cbcf213
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.125329.125329 cuda_h.py:19] end self_attn cost 0.0044291019439697266 seconds
DEBUG 01-15 16:08:53.125439.125439 cuda_h.py:19] end iln_self_attn_paln cost 0.00632023811340332 seconds
DEBUG 01-15 16:08:53.125528.125528 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:08:53.125873.125873 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.126475.126475 cuda_h.py:19] end gate cost 0.0007221698760986328 seconds
DEBUG 01-15 16:08:53.126311.126311 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.127093.127093 lmp.py:1912] 
DEBUG 01-15 16:08:53.127093.127093 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.127055.127055 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.127950.127950 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.127553.127553 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.127011.127011 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.127753.127753 lmp.py:1917] 
DEBUG 01-15 16:08:53.127753.127753 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.127972.127972 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.127583.127583 lmp.py:1935]   Expert 42 |     22 | CPU
DEBUG 01-15 16:08:53.127087.127087 lmp.py:1935]   Expert 19 |     23 | CPU
DEBUG 01-15 16:08:53.127021.127021 lmp.py:1935]   Expert 30 |     27 | CPU
DEBUG 01-15 16:08:53.127525.127525 lmp.py:1935]   Expert 32 |     43 | CPU
DEBUG 01-15 16:08:53.127228.127228 lmp.py:1935]   Expert  6 |     57 | CPU
DEBUG 01-15 16:08:53.127163.127163 lmp.py:1935]   Expert  5 |     74 | CPU
DEBUG 01-15 16:08:53.127190.127190 lmp.py:1935]   Expert 53 |     75 | CPU
DEBUG 01-15 16:08:53.127654.127654 lmp.py:1935]   Expert  1 |     78 | CPU
DEBUG 01-15 16:08:53.127158.127158 lmp.py:1935]   Expert 13 |    117 | CPU
DEBUG 01-15 16:08:53.127616.127616 lmp.py:1935]   Expert  9 |    123 | CPU
DEBUG 01-15 16:08:53.127835.127835 lmp.py:1935]   Expert 34 |    128 | CPU
DEBUG 01-15 16:08:53.127054.127054 lmp.py:1935]   Expert 63 |    128 | CPU
DEBUG 01-15 16:08:53.127135.127135 lmp.py:1935]   Expert 58 |    131 | CPU
DEBUG 01-15 16:08:53.127685.127685 lmp.py:1935]   Expert 50 |    133 | CPU
DEBUG 01-15 16:08:53.127758.127758 lmp.py:1935]   Expert 11 |    135 | CPU
DEBUG 01-15 16:08:53.127832.127832 lmp.py:1935]   Expert 26 |    136 | CPU
DEBUG 01-15 16:08:53.127905.127905 lmp.py:1935]   Expert 31 |    137 | CPU
DEBUG 01-15 16:08:53.127939.127939 lmp.py:1935]   Expert 59 |    138 | CPU
DEBUG 01-15 16:08:53.127251.127251 lmp.py:1935]   Expert 18 |    140 | CPU
DEBUG 01-15 16:08:53.127855.127855 lmp.py:1935]   Expert 40 |    144 | CPU
DEBUG 01-15 16:08:53.127981.127981 lmp.py:1935]   Expert 12 |    149 | CPU
DEBUG 01-15 16:08:53.127485.127485 lmp.py:1935]   Expert  4 |    150 | CPU
DEBUG 01-15 16:08:53.127989.127989 lmp.py:1935]   Expert 46 |    150 | CPU
DEBUG 01-15 16:08:53.127301.127301 lmp.py:1935]   Expert 48 |    151 | CPU
DEBUG 01-15 16:08:53.127136.127136 lmp.py:1935]   Expert 20 |    154 | CPU
DEBUG 01-15 16:08:53.127355.127355 lmp.py:1935]   Expert 61 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.127859.127859 lmp.py:1935]   Expert  2 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.127079.127079 lmp.py:1935]   Expert 33 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.127828.127828 lmp.py:1935]   Expert 56 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.127577.127577 lmp.py:1935]   Expert 35 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.127081.127081 lmp.py:1935]   Expert 10 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.127347.127347 lmp.py:1935]   Expert 55 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.127281.127281 lmp.py:1935]   Expert 51 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.127500.127500 lmp.py:1935]   Expert 36 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.127958.127958 lmp.py:1935]   Expert  8 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.127985.127985 lmp.py:1935]   Expert 52 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.127012.127012 lmp.py:1935]   Expert 37 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.127278.127278 lmp.py:1935]   Expert 57 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.127305.127305 lmp.py:1935]   Expert  0 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128856.128856 lmp.py:1935]   Expert 39 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128744.128744 lmp.py:1935]   Expert 25 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128917.128917 lmp.py:1935]   Expert 62 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128904.128904 lmp.py:1935]   Expert 38 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128170.128170 lmp.py:1935]   Expert  7 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128197.128197 lmp.py:1935]   Expert  3 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128655.128655 lmp.py:1935]   Expert 24 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128874.128874 lmp.py:1935]   Expert 27 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128140.128140 lmp.py:1935]   Expert 49 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128928.128928 lmp.py:1935]   Expert 28 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128194.128194 lmp.py:1935]   Expert 60 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128936.128936 lmp.py:1935]   Expert 21 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128202.128202 lmp.py:1935]   Expert 16 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128136.128136 lmp.py:1935]   Expert 43 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128455.128455 lmp.py:1935]   Expert 23 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128244.128244 lmp.py:1935]   Expert 29 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128748.128748 lmp.py:1935]   Expert 15 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128298.128298 lmp.py:1935]   Expert 22 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128822.128822 lmp.py:1935]   Expert 47 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128850.128850 lmp.py:1935]   Expert 41 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128877.128877 lmp.py:1935]   Expert 44 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128096.128096 lmp.py:1935]   Expert 54 |    352 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128792.128792 lmp.py:1935]   Expert 14 |    374 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128886.128886 lmp.py:1935]   Expert 17 |    405 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.128489.128489 lmp.py:1935]   Expert 45 |    454 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.128517.128517 lmp.py:1937] 
DEBUG 01-15 16:08:53.128517.128517 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.128497.128497 lmp.py:1938]   CPU:   2743 tokens
DEBUG 01-15 16:08:53.128194.128194 lmp.py:1942]   cuda:1:   4847 tokens (20 experts)
DEBUG 01-15 16:08:53.128612.128612 lmp.py:1942]   cuda:2:   4698 tokens (19 experts)
DEBUG 01-15 16:08:53.128884.128884 lmp.py:1943]   Total GPU:   9545 tokens
DEBUG 01-15 16:08:53.128150.128150 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.128150.128150 lmp.py:1944] 
DEBUG 01-15 16:08:53.128475.128475 cuda_h.py:19] end experts_map_get cost 0.002087831497192383 seconds
INFO 01-15 16:08:53.128093.128093 client.py:127] Model loaded
DEBUG 01-15 16:08:53.128883.128883 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.128335.128335 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.128674.128674 lmp.py:1953] 
DEBUG 01-15 16:08:53.128674.128674 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.129520.129520 cuda_h.py:19] end cpu_experts_submit cost 0.0003452301025390625 seconds
DEBUG 01-15 16:08:53.129866.129866 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.130102.130102 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.130951.130951 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.130195.130195 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:53.130261.130261 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.2.gate_proj.weight': 0, 'model.layers.13.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.21.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.21.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.21.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.22.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.22.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.22.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.13.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.13.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.13.mlp.experts.61.up_proj.weight': 340262912}, 2: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.44.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.44.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.44.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(16182673408, 5767168, 0, 0), (16188440576, 5767168, 5767168, 0), (16176906240, 5767168, 11534336, 0), (16269180928, 5767168, 17301504, 0), (16274948096, 5767168, 23068672, 0), (16263413760, 5767168, 28835840, 0), (16286482432, 5767168, 34603008, 0), (16292249600, 5767168, 40370176, 0), (16280715264, 5767168, 46137344, 0), (16511401984, 5767168, 51904512, 0), (16517169152, 5767168, 57671680, 0), (16505634816, 5767168, 63438848, 0), (16528703488, 5767168, 69206016, 0), (16534470656, 5767168, 74973184, 0), (16522936320, 5767168, 80740352, 0), (16563306496, 5767168, 86507520, 0), (16569073664, 5767168, 92274688, 0), (16557539328, 5767168, 98041856, 0), (16580608000, 5767168, 103809024, 0), (16586375168, 5767168, 109576192, 0), (16574840832, 5767168, 115343360, 0), (16615211008, 5767168, 121110528, 0), (16620978176, 5767168, 126877696, 0), (16609443840, 5767168, 132644864, 0), (16649814016, 5767168, 138412032, 0), (16655581184, 5767168, 144179200, 0), (16644046848, 5767168, 149946368, 0), (16719020032, 5767168, 155713536, 0), (16724787200, 5767168, 161480704, 0), (16713252864, 5767168, 167247872, 0), (16770924544, 5767168, 173015040, 0), (16776691712, 5767168, 178782208, 0), (16765157376, 5767168, 184549376, 0), (16788226048, 5767168, 190316544, 0), (16793993216, 5767168, 196083712, 0), (16782458880, 5767168, 201850880, 0), (16822829056, 5767168, 207618048, 0), (16828596224, 5767168, 213385216, 0), (16817061888, 5767168, 219152384, 0), (16857432064, 5767168, 224919552, 0), (16863199232, 5767168, 230686720, 0), (16851664896, 5767168, 236453888, 0), (16892035072, 5767168, 242221056, 0), (16897802240, 5767168, 247988224, 0), (16886267904, 5767168, 253755392, 0), (16926638080, 5767168, 259522560, 0), (16932405248, 5767168, 265289728, 0), (16920870912, 5767168, 271056896, 0), (17082351616, 5767168, 276824064, 0), (17088118784, 5767168, 282591232, 0), (17076584448, 5767168, 288358400, 0), (17099653120, 5767168, 294125568, 0), (17105420288, 5767168, 299892736, 0), (17093885952, 5767168, 305659904, 0), (17186160640, 5767168, 311427072, 0), (17191927808, 5767168, 317194240, 0), (17180393472, 5767168, 322961408, 0), (17203462144, 5767168, 328728576, 0), (17209229312, 5767168, 334495744, 0), (17197694976, 5767168, 340262912, 0)], 2: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16199974912, 5767168, 17301504, 0), (16205742080, 5767168, 23068672, 0), (16194207744, 5767168, 28835840, 0), (16321085440, 5767168, 34603008, 0), (16326852608, 5767168, 40370176, 0), (16315318272, 5767168, 46137344, 0), (16390291456, 5767168, 51904512, 0), (16396058624, 5767168, 57671680, 0), (16384524288, 5767168, 63438848, 0), (16407592960, 5767168, 69206016, 0), (16413360128, 5767168, 74973184, 0), (16401825792, 5767168, 80740352, 0), (16424894464, 5767168, 86507520, 0), (16430661632, 5767168, 92274688, 0), (16419127296, 5767168, 98041856, 0), (16442195968, 5767168, 103809024, 0), (16447963136, 5767168, 109576192, 0), (16436428800, 5767168, 115343360, 0), (16546004992, 5767168, 121110528, 0), (16551772160, 5767168, 126877696, 0), (16540237824, 5767168, 132644864, 0), (16632512512, 5767168, 138412032, 0), (16638279680, 5767168, 144179200, 0), (16626745344, 5767168, 149946368, 0), (16753623040, 5767168, 155713536, 0), (16759390208, 5767168, 161480704, 0), (16747855872, 5767168, 167247872, 0), (16805527552, 5767168, 173015040, 0), (16811294720, 5767168, 178782208, 0), (16799760384, 5767168, 184549376, 0), (16909336576, 5767168, 190316544, 0), (16915103744, 5767168, 196083712, 0), (16903569408, 5767168, 201850880, 0), (16961241088, 5767168, 207618048, 0), (16967008256, 5767168, 213385216, 0), (16955473920, 5767168, 219152384, 0), (16995844096, 5767168, 224919552, 0), (17001611264, 5767168, 230686720, 0), (16990076928, 5767168, 236453888, 0), (17030447104, 5767168, 242221056, 0), (17036214272, 5767168, 247988224, 0), (17024679936, 5767168, 253755392, 0), (17047748608, 5767168, 259522560, 0), (17053515776, 5767168, 265289728, 0), (17041981440, 5767168, 271056896, 0), (17116954624, 5767168, 276824064, 0), (17122721792, 5767168, 282591232, 0), (17111187456, 5767168, 288358400, 0), (17134256128, 5767168, 294125568, 0), (17140023296, 5767168, 299892736, 0), (17128488960, 5767168, 305659904, 0), (17220763648, 5767168, 311427072, 0), (17226530816, 5767168, 317194240, 0), (17214996480, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660728e70>, 2: <capsule object NULL at 0x74a6607297a0>}
DEBUG 01-15 16:08:53.131797.131797 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.131755.131755 cuda_h.py:19] end restore2model cost 0.002147674560546875 seconds
DEBUG 01-15 16:08:53.131922.131922 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.131596.131596 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9f6ca7d2-bcbd-41f9-b1aa-07cd6f694dc2
DEBUG 01-15 16:08:53.131597.131597 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.131004.131004 cuda_h.py:19] end sllm_worker_task cost 0.012146472930908203 seconds
DEBUG 01-15 16:08:53.131082.131082 cuda_h.py:19] end move_flatidxs cost 0.0008947849273681641 seconds
DEBUG 01-15 16:08:53.131971.131971 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:53.133596.133596 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9f6ca7d2-bcbd-41f9-b1aa-07cd6f694dc2
DEBUG 01-15 16:08:53.134554.134554 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003979921340942383 seconds
DEBUG 01-15 16:08:53.134093.134093 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.137161.137161 cuda_h.py:19] end restore2model cost 0.0031740665435791016 seconds
DEBUG 01-15 16:08:53.137726.137726 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007431983947753906 seconds
DEBUG 01-15 16:08:53.137952.137952 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.137645.137645 cuda_h.py:19] end gpu_sexperts cost 0.0003032684326171875 seconds
DEBUG 01-15 16:08:53.137044.137044 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.137059.137059 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:08:53.137278.137278 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.137173.137173 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.138573.138573 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010333061218261719 seconds
DEBUG 01-15 16:08:53.139708.139708 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.139994.139994 cuda_h.py:19] end gpu_group_list cost 0.0002167224884033203 seconds
DEBUG 01-15 16:08:53.140369.140369 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.141804.141804 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010693073272705078 seconds
DEBUG 01-15 16:08:53.141124.141124 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.141106.141106 cuda_h.py:19] end gpu_group_list cost 0.000202178955078125 seconds
DEBUG 01-15 16:08:53.141770.141770 cuda_h.py:19] end group_tensors cost 0.009180545806884766 seconds
DEBUG 01-15 16:08:53.141243.141243 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.142992.142992 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.142113.142113 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9f6ca7d2-bcbd-41f9-b1aa-07cd6f694dc2
DEBUG 01-15 16:08:53.144734.144734 cuda_h.py:19] end group pad cost 0.003152132034301758 seconds
DEBUG 01-15 16:08:53.145862.145862 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:53.171353.171353 client.py:127] Model loaded
DEBUG 01-15 16:08:53.171338.171338 cuda_h.py:19] end wait_experts_multi_device cost 0.029222488403320312 seconds
DEBUG 01-15 16:08:53.171281.171281 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.173723.173723 cuda_h.py:19] end group_einsum cost 0.02860426902770996 seconds
DEBUG 01-15 16:08:53.173464.173464 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.177517.177517 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036933422088623047 seconds
DEBUG 01-15 16:08:53.179090.179090 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007294893264770508 seconds
DEBUG 01-15 16:08:53.179194.179194 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.180124.180124 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05017209053039551 seconds
DEBUG 01-15 16:08:53.181102.181102 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.181469.181469 cuda_h.py:19] end index_scatter cost 0.00013303756713867188 seconds
DEBUG 01-15 16:08:53.181353.181353 cuda_h.py:19] end cpuoutputsdeal cost 0.002490997314453125 seconds
DEBUG 01-15 16:08:53.181781.181781 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.182737.182737 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.182828.182828 cuda_h.py:19] end gpu_group_tensor cost 0.00027751922607421875 seconds
DEBUG 01-15 16:08:53.182858.182858 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.182459.182459 cuda_h.py:19] end gpu_group_tensor cost 0.00026869773864746094 seconds
DEBUG 01-15 16:08:53.183008.183008 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.184580.184580 cuda_h.py:19] end gpu_group_einsum cost 0.0009601116180419922 seconds
DEBUG 01-15 16:08:53.184421.184421 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.185278.185278 cuda_h.py:19] end gpu_group_einsum cost 0.0007765293121337891 seconds
DEBUG 01-15 16:08:53.185636.185636 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.185046.185046 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.185333.185333 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003712177276611328 seconds
DEBUG 01-15 16:08:53.186905.186905 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.186891.186891 cuda_h.py:19] end concat_expert_out cost 9.512901306152344e-05 seconds
DEBUG 01-15 16:08:53.186220.186220 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.186510.186510 cuda_h.py:19] end index_scatter cost 0.0001010894775390625 seconds
DEBUG 01-15 16:08:53.186094.186094 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0015451908111572266 seconds
DEBUG 01-15 16:08:53.187703.187703 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.187229.187229 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.187864.187864 cuda_h.py:19] end all_expert_outputs_slices cost 0.00028824806213378906 seconds
DEBUG 01-15 16:08:53.187568.187568 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.187978.187978 cuda_h.py:19] end concat_expert_out cost 9.942054748535156e-05 seconds
DEBUG 01-15 16:08:53.187425.187425 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.188318.188318 cuda_h.py:19] end index_scatter cost 9.489059448242188e-05 seconds
DEBUG 01-15 16:08:53.188765.188765 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009911060333251953 seconds
DEBUG 01-15 16:08:53.188802.188802 cuda_h.py:19] end gpu_experts_multi_device cost 0.05042767524719238 seconds
DEBUG 01-15 16:08:53.188471.188471 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.06272411346435547 seconds
DEBUG 01-15 16:08:53.189190.189190 cuda_h.py:19] end prefill_layer cost 0.07063579559326172 seconds
DEBUG 01-15 16:08:53.189180.189180 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:08:53.189393.189393 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.189991.189991 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:08:53.189635.189635 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:08:53.189763.189763 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:08:53.189694.189694 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:08:53.189830.189830 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.189786.189786 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 0.00024318695068359375 seconds
DEBUG 01-15 16:08:53.189750.189750 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.190137.190137 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.190750.190750 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.190988.190988 cuda_h.py:19] end allocate_cuda_memory cost 0.00025463104248046875 seconds
DEBUG 01-15 16:08:53.190222.190222 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.190461.190461 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.190337.190337 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.190517.190517 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f4298f24-8e14-4b29-9d8d-648663faded9
DEBUG 01-15 16:08:53.190401.190401 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.190507.190507 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.191236.191236 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.192658.192658 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f4298f24-8e14-4b29-9d8d-648663faded9
DEBUG 01-15 16:08:53.192164.192164 cuda_h.py:19] end load_into_gpu_async cost 0.002380847930908203 seconds
DEBUG 01-15 16:08:53.193913.193913 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.193771.193771 cuda_h.py:19] end restore_tensors2 cost 7.796287536621094e-05 seconds
DEBUG 01-15 16:08:53.193003.193003 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003074169158935547 seconds
INFO 01-15 16:08:53.193635.193635 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f4298f24-8e14-4b29-9d8d-648663faded9
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.195127.195127 cuda_h.py:19] end self_attn cost 0.0041429996490478516 seconds
DEBUG 01-15 16:08:53.196687.196687 cuda_h.py:19] end iln_self_attn_paln cost 0.005941629409790039 seconds
DEBUG 01-15 16:08:53.196900.196900 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:08:53.196193.196193 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.197579.197579 cuda_h.py:19] end gate cost 0.0008108615875244141 seconds
DEBUG 01-15 16:08:53.197700.197700 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.197163.197163 lmp.py:1912] 
DEBUG 01-15 16:08:53.197163.197163 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.197509.197509 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.197927.197927 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.197484.197484 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.197703.197703 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.197969.197969 lmp.py:1917] 
DEBUG 01-15 16:08:53.197969.197969 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.197427.197427 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.197037.197037 lmp.py:1935]   Expert 34 |     29 | CPU
DEBUG 01-15 16:08:53.197256.197256 lmp.py:1935]   Expert  7 |     34 | CPU
DEBUG 01-15 16:08:53.197283.197283 lmp.py:1935]   Expert 13 |     42 | CPU
DEBUG 01-15 16:08:53.197887.197887 lmp.py:1935]   Expert 54 |     76 | CPU
DEBUG 01-15 16:08:53.197490.197490 lmp.py:1935]   Expert 18 |     83 | CPU
DEBUG 01-15 16:08:53.197802.197802 lmp.py:1935]   Expert 39 |     87 | CPU
DEBUG 01-15 16:08:53.197544.197544 lmp.py:1935]   Expert 49 |     89 | CPU
DEBUG 01-15 16:08:53.197016.197016 lmp.py:1935]   Expert 59 |    103 | CPU
DEBUG 01-15 16:08:53.197758.197758 lmp.py:1935]   Expert 21 |    107 | CPU
DEBUG 01-15 16:08:53.197785.197785 lmp.py:1935]   Expert  0 |    108 | CPU
DEBUG 01-15 16:08:53.197143.197143 lmp.py:1935]   Expert 16 |    109 | CPU
DEBUG 01-15 16:08:53.197740.197740 lmp.py:1935]   Expert 41 |    117 | CPU
DEBUG 01-15 16:08:53.197621.197621 lmp.py:1935]   Expert 22 |    120 | CPU
DEBUG 01-15 16:08:53.197503.197503 lmp.py:1935]   Expert 45 |    121 | CPU
DEBUG 01-15 16:08:53.198384.198384 lmp.py:1935]   Expert 15 |    123 | CPU
DEBUG 01-15 16:08:53.198266.198266 lmp.py:1935]   Expert 17 |    124 | CPU
DEBUG 01-15 16:08:53.198346.198346 lmp.py:1935]   Expert 61 |    133 | CPU
DEBUG 01-15 16:08:53.198711.198711 lmp.py:1935]   Expert 52 |    135 | CPU
DEBUG 01-15 16:08:53.198069.198069 lmp.py:1935]   Expert  8 |    137 | CPU
DEBUG 01-15 16:08:53.198666.198666 lmp.py:1935]   Expert 35 |    137 | CPU
DEBUG 01-15 16:08:53.198739.198739 lmp.py:1935]   Expert 38 |    138 | CPU
DEBUG 01-15 16:08:53.198098.198098 lmp.py:1935]   Expert 12 |    145 | CPU
DEBUG 01-15 16:08:53.198409.198409 lmp.py:1935]   Expert 48 |    148 | CPU
DEBUG 01-15 16:08:53.198483.198483 lmp.py:1935]   Expert 31 |    149 | CPU
DEBUG 01-15 16:08:53.198080.198080 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:08:53.198822.198822 lmp.py:1935]   Expert 53 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198611.198611 lmp.py:1935]   Expert 50 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198161.198161 lmp.py:1935]   Expert 40 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198619.198619 lmp.py:1935]   Expert 60 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198368.198368 lmp.py:1935]   Expert 27 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198302.198302 lmp.py:1935]   Expert 19 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198853.198853 lmp.py:1935]   Expert  4 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198834.198834 lmp.py:1935]   Expert 29 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198814.198814 lmp.py:1935]   Expert 30 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198557.198557 lmp.py:1935]   Expert 26 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198299.198299 lmp.py:1935]   Expert 11 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198134.198134 lmp.py:1935]   Expert 20 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198923.198923 lmp.py:1935]   Expert  6 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198758.198758 lmp.py:1935]   Expert 57 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198355.198355 lmp.py:1935]   Expert 46 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198859.198859 lmp.py:1935]   Expert 43 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198893.198893 lmp.py:1935]   Expert  2 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198066.198066 lmp.py:1935]   Expert 33 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198616.198616 lmp.py:1935]   Expert 23 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198451.198451 lmp.py:1935]   Expert 42 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198531.198531 lmp.py:1935]   Expert 55 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198797.198797 lmp.py:1935]   Expert 32 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198301.198301 lmp.py:1935]   Expert 56 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198136.198136 lmp.py:1935]   Expert  3 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198209.198209 lmp.py:1935]   Expert  9 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198806.198806 lmp.py:1935]   Expert 14 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198118.198118 lmp.py:1935]   Expert 28 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198715.198715 lmp.py:1935]   Expert 44 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198550.198550 lmp.py:1935]   Expert 51 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198385.198385 lmp.py:1935]   Expert  1 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198465.198465 lmp.py:1935]   Expert 58 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198300.198300 lmp.py:1935]   Expert 63 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198857.198857 lmp.py:1935]   Expert 37 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198454.198454 lmp.py:1935]   Expert 47 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198051.198051 lmp.py:1935]   Expert 24 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198985.198985 lmp.py:1935]   Expert 10 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198966.198966 lmp.py:1935]   Expert 62 |    314 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.198947.198947 lmp.py:1935]   Expert 25 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.198404.198404 lmp.py:1935]   Expert  5 |    364 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.199001.199001 lmp.py:1937] 
DEBUG 01-15 16:08:53.199001.199001 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.199028.199028 lmp.py:1938]   CPU:   2749 tokens
DEBUG 01-15 16:08:53.199294.199294 lmp.py:1942]   cuda:1:   4842 tokens (20 experts)
DEBUG 01-15 16:08:53.199083.199083 lmp.py:1942]   cuda:2:   4697 tokens (19 experts)
DEBUG 01-15 16:08:53.199640.199640 lmp.py:1943]   Total GPU:   9539 tokens
DEBUG 01-15 16:08:53.199958.199958 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.199958.199958 lmp.py:1944] 
DEBUG 01-15 16:08:53.199423.199423 cuda_h.py:19] end experts_map_get cost 0.0019752979278564453 seconds
DEBUG 01-15 16:08:53.199525.199525 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.199526.199526 lmp.py:1953] 
DEBUG 01-15 16:08:53.199526.199526 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.199607.199607 cuda_h.py:19] end cpu_experts_submit cost 5.8650970458984375e-05 seconds
DEBUG 01-15 16:08:53.199602.199602 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.199412.199412 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.200434.200434 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.1.gate_proj.weight': 0, 'model.layers.14.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.14.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.14.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.14.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.14.mlp.experts.2.gate_proj.weight': 0, 'model.layers.14.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.28.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.28.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.28.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.58.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(17272668160, 5767168, 0, 0), (17278435328, 5767168, 5767168, 0), (17266900992, 5767168, 11534336, 0), (17307271168, 5767168, 17301504, 0), (17313038336, 5767168, 23068672, 0), (17301504000, 5767168, 28835840, 0), (17341874176, 5767168, 34603008, 0), (17347641344, 5767168, 40370176, 0), (17336107008, 5767168, 46137344, 0), (17497587712, 5767168, 51904512, 0), (17503354880, 5767168, 57671680, 0), (17491820544, 5767168, 63438848, 0), (17584095232, 5767168, 69206016, 0), (17589862400, 5767168, 74973184, 0), (17578328064, 5767168, 80740352, 0), (17601396736, 5767168, 86507520, 0), (17607163904, 5767168, 92274688, 0), (17595629568, 5767168, 98041856, 0), (17705205760, 5767168, 103809024, 0), (17710972928, 5767168, 109576192, 0), (17699438592, 5767168, 115343360, 0), (17757110272, 5767168, 121110528, 0), (17762877440, 5767168, 126877696, 0), (17751343104, 5767168, 132644864, 0), (17809014784, 5767168, 138412032, 0), (17814781952, 5767168, 144179200, 0), (17803247616, 5767168, 149946368, 0), (17826316288, 5767168, 155713536, 0), (17832083456, 5767168, 161480704, 0), (17820549120, 5767168, 167247872, 0), (17947426816, 5767168, 173015040, 0), (17953193984, 5767168, 178782208, 0), (17941659648, 5767168, 184549376, 0), (17982029824, 5767168, 190316544, 0), (17987796992, 5767168, 196083712, 0), (17976262656, 5767168, 201850880, 0), (17999331328, 5767168, 207618048, 0), (18005098496, 5767168, 213385216, 0), (17993564160, 5767168, 219152384, 0), (18016632832, 5767168, 224919552, 0), (18022400000, 5767168, 230686720, 0), (18010865664, 5767168, 236453888, 0), (18068537344, 5767168, 242221056, 0), (18074304512, 5767168, 247988224, 0), (18062770176, 5767168, 253755392, 0), (18172346368, 5767168, 259522560, 0), (18178113536, 5767168, 265289728, 0), (18166579200, 5767168, 271056896, 0), (18241552384, 5767168, 276824064, 0), (18247319552, 5767168, 282591232, 0), (18235785216, 5767168, 288358400, 0), (18293456896, 5767168, 294125568, 0), (18299224064, 5767168, 299892736, 0), (18287689728, 5767168, 305659904, 0), (18328059904, 5767168, 311427072, 0), (18333827072, 5767168, 317194240, 0), (18322292736, 5767168, 322961408, 0), (18345361408, 5767168, 328728576, 0), (18351128576, 5767168, 334495744, 0), (18339594240, 5767168, 340262912, 0)], 2: [(17289969664, 5767168, 0, 0), (17295736832, 5767168, 5767168, 0), (17284202496, 5767168, 11534336, 0), (17324572672, 5767168, 17301504, 0), (17330339840, 5767168, 23068672, 0), (17318805504, 5767168, 28835840, 0), (17359175680, 5767168, 34603008, 0), (17364942848, 5767168, 40370176, 0), (17353408512, 5767168, 46137344, 0), (17411080192, 5767168, 51904512, 0), (17416847360, 5767168, 57671680, 0), (17405313024, 5767168, 63438848, 0), (17428381696, 5767168, 69206016, 0), (17434148864, 5767168, 74973184, 0), (17422614528, 5767168, 80740352, 0), (17445683200, 5767168, 86507520, 0), (17451450368, 5767168, 92274688, 0), (17439916032, 5767168, 98041856, 0), (17653301248, 5767168, 103809024, 0), (17659068416, 5767168, 109576192, 0), (17647534080, 5767168, 115343360, 0), (17670602752, 5767168, 121110528, 0), (17676369920, 5767168, 126877696, 0), (17664835584, 5767168, 132644864, 0), (17687904256, 5767168, 138412032, 0), (17693671424, 5767168, 144179200, 0), (17682137088, 5767168, 149946368, 0), (17722507264, 5767168, 155713536, 0), (17728274432, 5767168, 161480704, 0), (17716740096, 5767168, 167247872, 0), (17739808768, 5767168, 173015040, 0), (17745575936, 5767168, 178782208, 0), (17734041600, 5767168, 184549376, 0), (17774411776, 5767168, 190316544, 0), (17780178944, 5767168, 196083712, 0), (17768644608, 5767168, 201850880, 0), (17895522304, 5767168, 207618048, 0), (17901289472, 5767168, 213385216, 0), (17889755136, 5767168, 219152384, 0), (18051235840, 5767168, 224919552, 0), (18057003008, 5767168, 230686720, 0), (18045468672, 5767168, 236453888, 0), (18120441856, 5767168, 242221056, 0), (18126209024, 5767168, 247988224, 0), (18114674688, 5767168, 253755392, 0), (18137743360, 5767168, 259522560, 0), (18143510528, 5767168, 265289728, 0), (18131976192, 5767168, 271056896, 0), (18206949376, 5767168, 276824064, 0), (18212716544, 5767168, 282591232, 0), (18201182208, 5767168, 288358400, 0), (18224250880, 5767168, 294125568, 0), (18230018048, 5767168, 299892736, 0), (18218483712, 5767168, 305659904, 0), (18258853888, 5767168, 311427072, 0), (18264621056, 5767168, 317194240, 0), (18253086720, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729ad0>, 2: <capsule object NULL at 0x74a660728f00>}
INFO 01-15 16:08:53.200962.200962 client.py:127] Model loaded
DEBUG 01-15 16:08:53.200535.200535 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.201041.201041 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.201644.201644 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.201394.201394 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.201461.201461 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 06a0da8d-90d6-4c82-aa7c-75930903fe4d
DEBUG 01-15 16:08:53.201993.201993 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.201648.201648 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:53.201442.201442 cuda_h.py:19] end restore2model cost 0.0007913112640380859 seconds
DEBUG 01-15 16:08:53.201603.201603 cuda_h.py:19] end sllm_worker_task cost 0.01202082633972168 seconds
DEBUG 01-15 16:08:53.202633.202633 cuda_h.py:19] end move_flatidxs cost 0.0008718967437744141 seconds
DEBUG 01-15 16:08:53.202277.202277 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:53.202433.202433 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 06a0da8d-90d6-4c82-aa7c-75930903fe4d
DEBUG 01-15 16:08:53.203492.203492 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0039179325103759766 seconds
DEBUG 01-15 16:08:53.203091.203091 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.206744.206744 cuda_h.py:19] end restore2model cost 0.0032529830932617188 seconds
DEBUG 01-15 16:08:53.206739.206739 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007447719573974609 seconds
DEBUG 01-15 16:08:53.206535.206535 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.207002.207002 cuda_h.py:19] end gpu_sexperts cost 0.00027942657470703125 seconds
DEBUG 01-15 16:08:53.207401.207401 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.207330.207330 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.3365020751953125e-05 seconds
DEBUG 01-15 16:08:53.207695.207695 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.207829.207829 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.207763.207763 cuda_h.py:19] end group_tensors cost 0.004557132720947266 seconds
DEBUG 01-15 16:08:53.207936.207936 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.208355.208355 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001058340072631836 seconds
DEBUG 01-15 16:08:53.208251.208251 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.208299.208299 cuda_h.py:19] end gpu_group_list cost 0.0002155303955078125 seconds
DEBUG 01-15 16:08:53.209310.209310 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.210632.210632 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001054525375366211 seconds
DEBUG 01-15 16:08:53.210191.210191 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.210566.210566 cuda_h.py:19] end group pad cost 0.003144979476928711 seconds
DEBUG 01-15 16:08:53.211078.211078 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:53.211669.211669 cuda_h.py:19] end gpu_group_list cost 0.00021576881408691406 seconds
DEBUG 01-15 16:08:53.211994.211994 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.211798.211798 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 06a0da8d-90d6-4c82-aa7c-75930903fe4d
DEBUG 01-15 16:08:53.238194.238194 cuda_h.py:19] end group_einsum cost 0.027303457260131836 seconds
DEBUG 01-15 16:08:53.238703.238703 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:08:53.240293.240293 client.py:127] Model loaded
DEBUG 01-15 16:08:53.240604.240604 cuda_h.py:19] end wait_experts_multi_device cost 0.02841663360595703 seconds
DEBUG 01-15 16:08:53.240754.240754 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.242040.242040 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036542415618896484 seconds
DEBUG 01-15 16:08:53.243444.243444 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0031538009643554688 seconds
DEBUG 01-15 16:08:53.243606.243606 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.245439.245439 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04404020309448242 seconds
DEBUG 01-15 16:08:53.245951.245951 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.245377.245377 cuda_h.py:19] end index_scatter cost 0.00012540817260742188 seconds
DEBUG 01-15 16:08:53.246392.246392 cuda_h.py:19] end cpuoutputsdeal cost 0.0022673606872558594 seconds
DEBUG 01-15 16:08:53.246742.246742 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.246175.246175 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.246935.246935 cuda_h.py:19] end gpu_group_tensor cost 0.0002751350402832031 seconds
DEBUG 01-15 16:08:53.246236.246236 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.247975.247975 cuda_h.py:19] end gpu_group_tensor cost 0.0002677440643310547 seconds
DEBUG 01-15 16:08:53.247453.247453 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.248715.248715 cuda_h.py:19] end gpu_group_einsum cost 0.000993967056274414 seconds
DEBUG 01-15 16:08:53.248245.248245 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.249236.249236 cuda_h.py:19] end gpu_group_einsum cost 0.0008060932159423828 seconds
DEBUG 01-15 16:08:53.249229.249229 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.249846.249846 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.250424.250424 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003724098205566406 seconds
DEBUG 01-15 16:08:53.250619.250619 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.250287.250287 cuda_h.py:19] end concat_expert_out cost 9.918212890625e-05 seconds
DEBUG 01-15 16:08:53.250993.250993 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.250821.250821 cuda_h.py:19] end index_scatter cost 0.0001163482666015625 seconds
DEBUG 01-15 16:08:53.251902.251902 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0015933513641357422 seconds
DEBUG 01-15 16:08:53.251477.251477 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.251879.251879 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.252832.252832 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003063678741455078 seconds
DEBUG 01-15 16:08:53.252397.252397 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.252813.252813 cuda_h.py:19] end concat_expert_out cost 0.00010395050048828125 seconds
DEBUG 01-15 16:08:53.252799.252799 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.252282.252282 cuda_h.py:19] end index_scatter cost 0.00010180473327636719 seconds
DEBUG 01-15 16:08:53.252835.252835 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010991096496582031 seconds
DEBUG 01-15 16:08:53.252541.252541 cuda_h.py:19] end gpu_experts_multi_device cost 0.0456082820892334 seconds
DEBUG 01-15 16:08:53.253686.253686 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.05685710906982422 seconds
DEBUG 01-15 16:08:53.253849.253849 cuda_h.py:19] end prefill_layer cost 0.06438732147216797 seconds
DEBUG 01-15 16:08:53.253892.253892 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:08:53.253921.253921 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.254857.254857 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:08:53.254600.254600 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:08:53.254841.254841 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:08:53.254640.254640 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 6.413459777832031e-05 seconds
DEBUG 01-15 16:08:53.254491.254491 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.254547.254547 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 0.00024247169494628906 seconds
DEBUG 01-15 16:08:53.254928.254928 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.254181.254181 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.254588.254588 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.255904.255904 cuda_h.py:19] end allocate_cuda_memory cost 0.00024819374084472656 seconds
DEBUG 01-15 16:08:53.255391.255391 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.255512.255512 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.255410.255410 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.255001.255001 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.255896.255896 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 46f491e5-d4df-46e3-8cf0-0bb6aec7486b
DEBUG 01-15 16:08:53.255733.255733 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.255294.255294 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.257158.257158 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 46f491e5-d4df-46e3-8cf0-0bb6aec7486b
DEBUG 01-15 16:08:53.257809.257809 cuda_h.py:19] end load_into_gpu_async cost 0.0022270679473876953 seconds
DEBUG 01-15 16:08:53.257558.257558 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.257170.257170 cuda_h.py:19] end restore_tensors2 cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:08:53.257926.257926 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0030274391174316406 seconds
INFO 01-15 16:08:53.257862.257862 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 46f491e5-d4df-46e3-8cf0-0bb6aec7486b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.258046.258046 cuda_h.py:19] end self_attn cost 0.0030183792114257812 seconds
DEBUG 01-15 16:08:53.259692.259692 cuda_h.py:19] end iln_self_attn_paln cost 0.004447221755981445 seconds
DEBUG 01-15 16:08:53.259614.259614 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:08:53.259755.259755 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.260096.260096 cuda_h.py:19] end gate cost 0.0006744861602783203 seconds
DEBUG 01-15 16:08:53.260025.260025 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.260083.260083 lmp.py:1912] 
DEBUG 01-15 16:08:53.260083.260083 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.260184.260184 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.260503.260503 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.260199.260199 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.260796.260796 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.260200.260200 lmp.py:1917] 
DEBUG 01-15 16:08:53.260200.260200 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.260843.260843 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.260639.260639 lmp.py:1935]   Expert 15 |     66 | CPU
DEBUG 01-15 16:08:53.260520.260520 lmp.py:1935]   Expert 41 |     70 | CPU
DEBUG 01-15 16:08:53.260123.260123 lmp.py:1935]   Expert  0 |     75 | CPU
DEBUG 01-15 16:08:53.260012.260012 lmp.py:1935]   Expert 63 |     77 | CPU
DEBUG 01-15 16:08:53.260370.260370 lmp.py:1935]   Expert 20 |     83 | CPU
DEBUG 01-15 16:08:53.260774.260774 lmp.py:1935]   Expert  7 |     90 | CPU
DEBUG 01-15 16:08:53.260894.260894 lmp.py:1935]   Expert 45 |     90 | CPU
DEBUG 01-15 16:08:53.260060.260060 lmp.py:1935]   Expert 28 |    102 | CPU
DEBUG 01-15 16:08:53.260180.260180 lmp.py:1935]   Expert 54 |    105 | CPU
DEBUG 01-15 16:08:53.260446.260446 lmp.py:1935]   Expert 12 |    110 | CPU
DEBUG 01-15 16:08:53.260327.260327 lmp.py:1935]   Expert 40 |    120 | CPU
DEBUG 01-15 16:08:53.260493.260493 lmp.py:1935]   Expert 52 |    120 | CPU
DEBUG 01-15 16:08:53.260944.260944 lmp.py:1935]   Expert  5 |    122 | CPU
DEBUG 01-15 16:08:53.260395.260395 lmp.py:1935]   Expert 59 |    124 | CPU
DEBUG 01-15 16:08:53.260846.260846 lmp.py:1935]   Expert  4 |    130 | CPU
DEBUG 01-15 16:08:53.260058.260058 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:08:53.260039.260039 lmp.py:1935]   Expert 62 |    136 | CPU
DEBUG 01-15 16:08:53.260113.260113 lmp.py:1935]   Expert 13 |    137 | CPU
DEBUG 01-15 16:08:53.260378.260378 lmp.py:1935]   Expert 21 |    137 | CPU
DEBUG 01-15 16:08:53.260068.260068 lmp.py:1935]   Expert 61 |    137 | CPU
DEBUG 01-15 16:08:53.260757.260757 lmp.py:1935]   Expert 55 |    138 | CPU
DEBUG 01-15 16:08:53.260877.260877 lmp.py:1935]   Expert 42 |    142 | CPU
DEBUG 01-15 16:08:53.260473.260473 lmp.py:1935]   Expert 10 |    146 | CPU
DEBUG 01-15 16:08:53.261355.261355 lmp.py:1935]   Expert 22 |    147 | CPU
DEBUG 01-15 16:08:53.261236.261236 lmp.py:1935]   Expert 14 |    148 | CPU
DEBUG 01-15 16:08:53.261310.261310 lmp.py:1935]   Expert 32 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261145.261145 lmp.py:1935]   Expert 51 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261503.261503 lmp.py:1935]   Expert 25 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261146.261146 lmp.py:1935]   Expert 47 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261266.261266 lmp.py:1935]   Expert  1 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261578.261578 lmp.py:1935]   Expert 19 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261658.261658 lmp.py:1935]   Expert 53 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261731.261731 lmp.py:1935]   Expert 26 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261851.261851 lmp.py:1935]   Expert 50 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261832.261832 lmp.py:1935]   Expert  2 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261336.261336 lmp.py:1935]   Expert  6 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261933.261933 lmp.py:1935]   Expert 30 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261529.261529 lmp.py:1935]   Expert 11 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261318.261318 lmp.py:1935]   Expert 35 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261584.261584 lmp.py:1935]   Expert 57 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261372.261372 lmp.py:1935]   Expert 56 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261923.261923 lmp.py:1935]   Expert 48 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261996.261996 lmp.py:1935]   Expert 24 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261354.261354 lmp.py:1935]   Expert 44 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261713.261713 lmp.py:1935]   Expert 16 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261071.261071 lmp.py:1935]   Expert 46 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261668.261668 lmp.py:1935]   Expert 39 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261509.261509 lmp.py:1935]   Expert 18 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261629.261629 lmp.py:1935]   Expert 29 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261464.261464 lmp.py:1935]   Expert 37 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261584.261584 lmp.py:1935]   Expert 31 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261234.261234 lmp.py:1935]   Expert 36 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261784.261784 lmp.py:1935]   Expert 60 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261142.261142 lmp.py:1935]   Expert  3 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261931.261931 lmp.py:1935]   Expert 38 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261720.261720 lmp.py:1935]   Expert  9 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261270.261270 lmp.py:1935]   Expert 17 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261297.261297 lmp.py:1935]   Expert 23 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261656.261656 lmp.py:1935]   Expert 27 |    350 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261299.261299 lmp.py:1935]   Expert 43 |    362 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261895.261895 lmp.py:1935]   Expert 33 |    396 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261777.261777 lmp.py:1935]   Expert  8 |    398 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261896.261896 lmp.py:1935]   Expert 58 |    446 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.261261.261261 lmp.py:1935]   Expert 49 |    542 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.261904.261904 lmp.py:1937] 
DEBUG 01-15 16:08:53.261904.261904 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.261932.261932 lmp.py:1938]   CPU:   2885 tokens
DEBUG 01-15 16:08:53.261343.261343 lmp.py:1942]   cuda:1:   4631 tokens (19 experts)
DEBUG 01-15 16:08:53.261178.261178 lmp.py:1942]   cuda:2:   4772 tokens (20 experts)
DEBUG 01-15 16:08:53.261013.261013 lmp.py:1943]   Total GPU:   9403 tokens
DEBUG 01-15 16:08:53.261325.261325 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.261325.261325 lmp.py:1944] 
DEBUG 01-15 16:08:53.261074.261074 cuda_h.py:19] end experts_map_get cost 0.0018312931060791016 seconds
DEBUG 01-15 16:08:53.261593.261593 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.262826.262826 lmp.py:1953] 
DEBUG 01-15 16:08:53.262826.262826 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.262808.262808 cuda_h.py:19] end cpu_experts_submit cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:08:53.262418.262418 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.262222.262222 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.264378.264378 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.2.gate_proj.weight': 0, 'model.layers.15.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.16.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.16.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.16.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.17.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.17.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.17.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.27.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.27.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.27.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.29.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.29.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.29.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.30.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.30.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.30.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.31.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.31.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.31.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.32.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.32.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.32.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.60.up_proj.weight': 322961408}, 2: {'model.layers.15.mlp.experts.1.gate_proj.weight': 0, 'model.layers.15.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.51.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.51.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.51.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.15.mlp.experts.58.gate_proj.weight': 328728576, 'model.layers.15.mlp.experts.58.down_proj.weight': 334495744, 'model.layers.15.mlp.experts.58.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(18397265920, 5767168, 0, 0), (18403033088, 5767168, 5767168, 0), (18391498752, 5767168, 11534336, 0), (18639486976, 5767168, 17301504, 0), (18645254144, 5767168, 23068672, 0), (18633719808, 5767168, 28835840, 0), (18656788480, 5767168, 34603008, 0), (18662555648, 5767168, 40370176, 0), (18651021312, 5767168, 46137344, 0), (18829803520, 5767168, 51904512, 0), (18835570688, 5767168, 57671680, 0), (18824036352, 5767168, 63438848, 0), (18864406528, 5767168, 69206016, 0), (18870173696, 5767168, 74973184, 0), (18858639360, 5767168, 80740352, 0), (18881708032, 5767168, 86507520, 0), (18887475200, 5767168, 92274688, 0), (18875940864, 5767168, 98041856, 0), (18899009536, 5767168, 103809024, 0), (18904776704, 5767168, 109576192, 0), (18893242368, 5767168, 115343360, 0), (18916311040, 5767168, 121110528, 0), (18922078208, 5767168, 126877696, 0), (18910543872, 5767168, 132644864, 0), (18933612544, 5767168, 138412032, 0), (18939379712, 5767168, 144179200, 0), (18927845376, 5767168, 149946368, 0), (18968215552, 5767168, 155713536, 0), (18973982720, 5767168, 161480704, 0), (18962448384, 5767168, 167247872, 0), (19020120064, 5767168, 173015040, 0), (19025887232, 5767168, 178782208, 0), (19014352896, 5767168, 184549376, 0), (19037421568, 5767168, 190316544, 0), (19043188736, 5767168, 196083712, 0), (19031654400, 5767168, 201850880, 0), (19123929088, 5767168, 207618048, 0), (19129696256, 5767168, 213385216, 0), (19118161920, 5767168, 219152384, 0), (19175833600, 5767168, 224919552, 0), (19181600768, 5767168, 230686720, 0), (19170066432, 5767168, 236453888, 0), (19210436608, 5767168, 242221056, 0), (19216203776, 5767168, 247988224, 0), (19204669440, 5767168, 253755392, 0), (19227738112, 5767168, 259522560, 0), (19233505280, 5767168, 265289728, 0), (19221970944, 5767168, 271056896, 0), (19279642624, 5767168, 276824064, 0), (19285409792, 5767168, 282591232, 0), (19273875456, 5767168, 288358400, 0), (19331547136, 5767168, 294125568, 0), (19337314304, 5767168, 299892736, 0), (19325779968, 5767168, 305659904, 0), (19400753152, 5767168, 311427072, 0), (19406520320, 5767168, 317194240, 0), (19394985984, 5767168, 322961408, 0)], 2: [(18379964416, 5767168, 0, 0), (18385731584, 5767168, 5767168, 0), (18374197248, 5767168, 11534336, 0), (18414567424, 5767168, 17301504, 0), (18420334592, 5767168, 23068672, 0), (18408800256, 5767168, 28835840, 0), (18466471936, 5767168, 34603008, 0), (18472239104, 5767168, 40370176, 0), (18460704768, 5767168, 46137344, 0), (18501074944, 5767168, 51904512, 0), (18506842112, 5767168, 57671680, 0), (18495307776, 5767168, 63438848, 0), (18518376448, 5767168, 69206016, 0), (18524143616, 5767168, 74973184, 0), (18512609280, 5767168, 80740352, 0), (18552979456, 5767168, 86507520, 0), (18558746624, 5767168, 92274688, 0), (18547212288, 5767168, 98041856, 0), (18674089984, 5767168, 103809024, 0), (18679857152, 5767168, 109576192, 0), (18668322816, 5767168, 115343360, 0), (18691391488, 5767168, 121110528, 0), (18697158656, 5767168, 126877696, 0), (18685624320, 5767168, 132644864, 0), (18760597504, 5767168, 138412032, 0), (18766364672, 5767168, 144179200, 0), (18754830336, 5767168, 149946368, 0), (18777899008, 5767168, 155713536, 0), (18783666176, 5767168, 161480704, 0), (18772131840, 5767168, 167247872, 0), (18795200512, 5767168, 173015040, 0), (18800967680, 5767168, 178782208, 0), (18789433344, 5767168, 184549376, 0), (18812502016, 5767168, 190316544, 0), (18818269184, 5767168, 196083712, 0), (18806734848, 5767168, 201850880, 0), (18985517056, 5767168, 207618048, 0), (18991284224, 5767168, 213385216, 0), (18979749888, 5767168, 219152384, 0), (19002818560, 5767168, 224919552, 0), (19008585728, 5767168, 230686720, 0), (18997051392, 5767168, 236453888, 0), (19106627584, 5767168, 242221056, 0), (19112394752, 5767168, 247988224, 0), (19100860416, 5767168, 253755392, 0), (19158532096, 5767168, 259522560, 0), (19164299264, 5767168, 265289728, 0), (19152764928, 5767168, 271056896, 0), (19193135104, 5767168, 276824064, 0), (19198902272, 5767168, 282591232, 0), (19187367936, 5767168, 288358400, 0), (19245039616, 5767168, 294125568, 0), (19250806784, 5767168, 299892736, 0), (19239272448, 5767168, 305659904, 0), (19348848640, 5767168, 311427072, 0), (19354615808, 5767168, 317194240, 0), (19343081472, 5767168, 322961408, 0), (19366150144, 5767168, 328728576, 0), (19371917312, 5767168, 334495744, 0), (19360382976, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8143f4630>, 2: <capsule object NULL at 0x74a660729020>}
DEBUG 01-15 16:08:53.264941.264941 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.264851.264851 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.264389.264389 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 33161a12-ed8a-4a3a-9733-5a872d006807
DEBUG 01-15 16:08:53.264590.264590 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.265152.265152 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:53.265610.265610 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:53.265537.265537 client.py:127] Model loaded
DEBUG 01-15 16:08:53.265141.265141 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.265006.265006 cuda_h.py:19] end restore2model cost 0.0004525184631347656 seconds
DEBUG 01-15 16:08:53.265027.265027 cuda_h.py:19] end sllm_worker_task cost 0.011286258697509766 seconds
DEBUG 01-15 16:08:53.265918.265918 cuda_h.py:19] end move_flatidxs cost 0.0008628368377685547 seconds
DEBUG 01-15 16:08:53.265317.265317 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:53.266299.266299 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 33161a12-ed8a-4a3a-9733-5a872d006807
DEBUG 01-15 16:08:53.266741.266741 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00459599494934082 seconds
DEBUG 01-15 16:08:53.266565.266565 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.270393.270393 cuda_h.py:19] end restore2model cost 0.003139972686767578 seconds
DEBUG 01-15 16:08:53.270421.270421 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007982730865478516 seconds
DEBUG 01-15 16:08:53.270045.270045 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.270923.270923 cuda_h.py:19] end gpu_sexperts cost 0.0002651214599609375 seconds
DEBUG 01-15 16:08:53.270891.270891 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.270237.270237 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:08:53.270648.270648 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.270411.270411 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.270651.270651 cuda_h.py:19] end group_tensors cost 0.00440669059753418 seconds
DEBUG 01-15 16:08:53.271420.271420 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.271488.271488 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010602474212646484 seconds
DEBUG 01-15 16:08:53.271623.271623 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.272062.272062 cuda_h.py:19] end gpu_group_list cost 0.00022220611572265625 seconds
DEBUG 01-15 16:08:53.272171.272171 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.274434.274434 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010821819305419922 seconds
DEBUG 01-15 16:08:53.274516.274516 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.274723.274723 cuda_h.py:19] end gpu_group_list cost 0.00022721290588378906 seconds
DEBUG 01-15 16:08:53.274857.274857 cuda_h.py:19] end group pad cost 0.0034987926483154297 seconds
DEBUG 01-15 16:08:53.274177.274177 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:53.275774.275774 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.275134.275134 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 33161a12-ed8a-4a3a-9733-5a872d006807
INFO 01-15 16:08:53.302931.302931 client.py:127] Model loaded
DEBUG 01-15 16:08:53.302819.302819 cuda_h.py:19] end wait_experts_multi_device cost 0.027166128158569336 seconds
DEBUG 01-15 16:08:53.302285.302285 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.311837.311837 cuda_h.py:19] end group_einsum cost 0.03641676902770996 seconds
DEBUG 01-15 16:08:53.311632.311632 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.315898.315898 cuda_h.py:19] end get_outputs_cpu1 cost 0.003916740417480469 seconds
DEBUG 01-15 16:08:53.319266.319266 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0543217658996582 seconds
DEBUG 01-15 16:08:53.319882.319882 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.017349958419799805 seconds
DEBUG 01-15 16:08:53.319456.319456 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.321008.321008 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.321578.321578 cuda_h.py:19] end index_scatter cost 0.00010538101196289062 seconds
DEBUG 01-15 16:08:53.321517.321517 cuda_h.py:19] end cpuoutputsdeal cost 0.0017867088317871094 seconds
DEBUG 01-15 16:08:53.321819.321819 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.322748.322748 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.322419.322419 cuda_h.py:19] end gpu_group_tensor cost 0.00020122528076171875 seconds
DEBUG 01-15 16:08:53.322732.322732 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.322475.322475 cuda_h.py:19] end gpu_group_tensor cost 0.00018548965454101562 seconds
DEBUG 01-15 16:08:53.322910.322910 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.323455.323455 cuda_h.py:19] end gpu_group_einsum cost 0.0007863044738769531 seconds
DEBUG 01-15 16:08:53.323263.323263 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.324271.324271 cuda_h.py:19] end gpu_group_einsum cost 0.0005648136138916016 seconds
DEBUG 01-15 16:08:53.324309.324309 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.324745.324745 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.325961.325961 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002765655517578125 seconds
DEBUG 01-15 16:08:53.325122.325122 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.325669.325669 cuda_h.py:19] end concat_expert_out cost 7.104873657226562e-05 seconds
DEBUG 01-15 16:08:53.325884.325884 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.325491.325491 cuda_h.py:19] end index_scatter cost 7.843971252441406e-05 seconds
DEBUG 01-15 16:08:53.325224.325224 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011322498321533203 seconds
DEBUG 01-15 16:08:53.325778.325778 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.325079.325079 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.326888.326888 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001995563507080078 seconds
DEBUG 01-15 16:08:53.326803.326803 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.326059.326059 cuda_h.py:19] end concat_expert_out cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:08:53.326698.326698 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.326483.326483 cuda_h.py:19] end index_scatter cost 7.128715515136719e-05 seconds
DEBUG 01-15 16:08:53.326843.326843 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006959438323974609 seconds
DEBUG 01-15 16:08:53.326879.326879 cuda_h.py:19] end gpu_experts_multi_device cost 0.056107521057128906 seconds
DEBUG 01-15 16:08:53.326220.326220 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.06751203536987305 seconds
DEBUG 01-15 16:08:53.327849.327849 cuda_h.py:19] end prefill_layer cost 0.07334399223327637 seconds
DEBUG 01-15 16:08:53.327759.327759 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:08:53.327098.327098 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.327629.327629 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:08:53.327922.327922 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:08:53.327698.327698 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:08:53.327734.327734 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 5.650520324707031e-05 seconds
DEBUG 01-15 16:08:53.327703.327703 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 0.00010800361633300781 seconds
DEBUG 01-15 16:08:53.327896.327896 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.327517.327517 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.328758.328758 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.328311.328311 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.328861.328861 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.328459.328459 cuda_h.py:19] end allocate_cuda_memory cost 0.00038552284240722656 seconds
DEBUG 01-15 16:08:53.329104.329104 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.329874.329874 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.329856.329856 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.329711.329711 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1cae2e87-b1b1-44fe-ac04-3e0f74d7c139
DEBUG 01-15 16:08:53.329921.329921 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.329216.329216 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.331896.331896 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1cae2e87-b1b1-44fe-ac04-3e0f74d7c139
DEBUG 01-15 16:08:53.331562.331562 cuda_h.py:19] end load_into_gpu_async cost 0.0025415420532226562 seconds
DEBUG 01-15 16:08:53.331987.331987 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.331779.331779 cuda_h.py:19] end restore_tensors2 cost 9.5367431640625e-05 seconds
DEBUG 01-15 16:08:53.331204.331204 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003348112106323242 seconds
INFO 01-15 16:08:53.331014.331014 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1cae2e87-b1b1-44fe-ac04-3e0f74d7c139
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.333944.333944 cuda_h.py:19] end self_attn cost 0.0036325454711914062 seconds
DEBUG 01-15 16:08:53.333505.333505 cuda_h.py:19] end iln_self_attn_paln cost 0.006091117858886719 seconds
DEBUG 01-15 16:08:53.333249.333249 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:08:53.334866.334866 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.334525.334525 cuda_h.py:19] end gate cost 0.0006620883941650391 seconds
DEBUG 01-15 16:08:53.334322.334322 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.335538.335538 lmp.py:1912] 
DEBUG 01-15 16:08:53.335538.335538 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.335102.335102 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.335560.335560 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.335634.335634 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.335084.335084 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.335582.335582 lmp.py:1917] 
DEBUG 01-15 16:08:53.335582.335582 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.335509.335509 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.335921.335921 lmp.py:1935]   Expert 58 |     34 | CPU
DEBUG 01-15 16:08:53.335471.335471 lmp.py:1935]   Expert 31 |     60 | CPU
DEBUG 01-15 16:08:53.335591.335591 lmp.py:1935]   Expert 47 |     60 | CPU
DEBUG 01-15 16:08:53.335472.335472 lmp.py:1935]   Expert 49 |     61 | CPU
DEBUG 01-15 16:08:53.335592.335592 lmp.py:1935]   Expert  4 |     66 | CPU
DEBUG 01-15 16:08:53.335473.335473 lmp.py:1935]   Expert 38 |     69 | CPU
DEBUG 01-15 16:08:53.335116.335116 lmp.py:1935]   Expert 45 |     73 | CPU
DEBUG 01-15 16:08:53.335998.335998 lmp.py:1935]   Expert 43 |     83 | CPU
DEBUG 01-15 16:08:53.335879.335879 lmp.py:1935]   Expert 41 |     84 | CPU
DEBUG 01-15 16:08:53.335761.335761 lmp.py:1935]   Expert 33 |     97 | CPU
DEBUG 01-15 16:08:53.335642.335642 lmp.py:1935]   Expert 50 |    101 | CPU
DEBUG 01-15 16:08:53.335477.335477 lmp.py:1935]   Expert 57 |    103 | CPU
DEBUG 01-15 16:08:53.335835.335835 lmp.py:1935]   Expert 11 |    106 | CPU
DEBUG 01-15 16:08:53.335478.335478 lmp.py:1935]   Expert  2 |    113 | CPU
DEBUG 01-15 16:08:53.335883.335883 lmp.py:1935]   Expert 51 |    115 | CPU
DEBUG 01-15 16:08:53.335287.335287 lmp.py:1935]   Expert  0 |    126 | CPU
DEBUG 01-15 16:08:53.335169.335169 lmp.py:1935]   Expert 14 |    126 | CPU
DEBUG 01-15 16:08:53.335050.335050 lmp.py:1935]   Expert 54 |    129 | CPU
DEBUG 01-15 16:08:53.335932.335932 lmp.py:1935]   Expert 56 |    140 | CPU
DEBUG 01-15 16:08:53.335813.335813 lmp.py:1935]   Expert 34 |    141 | CPU
DEBUG 01-15 16:08:53.335694.335694 lmp.py:1935]   Expert 26 |    143 | CPU
DEBUG 01-15 16:08:53.335576.335576 lmp.py:1935]   Expert 27 |    152 | CPU
DEBUG 01-15 16:08:53.335219.335219 lmp.py:1935]   Expert 28 |    155 | CPU
DEBUG 01-15 16:08:53.335577.335577 lmp.py:1935]   Expert 55 |    159 | CPU
DEBUG 01-15 16:08:53.335697.335697 lmp.py:1935]   Expert 25 |    164 | CPU
DEBUG 01-15 16:08:53.335247.335247 lmp.py:1935]   Expert 10 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.335559.335559 lmp.py:1935]   Expert  9 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.335156.335156 lmp.py:1935]   Expert 13 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.335991.335991 lmp.py:1935]   Expert 61 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.335587.335587 lmp.py:1935]   Expert 48 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.335422.335422 lmp.py:1935]   Expert  6 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.335542.335542 lmp.py:1935]   Expert  7 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.335900.335900 lmp.py:1935]   Expert 46 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.335974.335974 lmp.py:1935]   Expert 24 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.335624.335624 lmp.py:1935]   Expert 42 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.335075.335075 lmp.py:1935]   Expert 18 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.335810.335810 lmp.py:1935]   Expert 40 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.335784.335784 lmp.py:1935]   Expert 12 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.335758.335758 lmp.py:1935]   Expert 29 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.335971.335971 lmp.py:1935]   Expert 63 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.335945.335945 lmp.py:1935]   Expert 22 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336442.336442 lmp.py:1935]   Expert 21 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336654.336654 lmp.py:1935]   Expert 59 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336582.336582 lmp.py:1935]   Expert 32 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336318.336318 lmp.py:1935]   Expert 19 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336292.336292 lmp.py:1935]   Expert 36 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336789.336789 lmp.py:1935]   Expert  3 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336525.336525 lmp.py:1935]   Expert 37 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336499.336499 lmp.py:1935]   Expert 16 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336996.336996 lmp.py:1935]   Expert  1 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336732.336732 lmp.py:1935]   Expert 20 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336229.336229 lmp.py:1935]   Expert  5 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336203.336203 lmp.py:1935]   Expert  8 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336462.336462 lmp.py:1935]   Expert 30 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336197.336197 lmp.py:1935]   Expert 15 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336694.336694 lmp.py:1935]   Expert 62 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336907.336907 lmp.py:1935]   Expert 39 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336073.336073 lmp.py:1935]   Expert 35 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336047.336047 lmp.py:1935]   Expert 17 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336783.336783 lmp.py:1935]   Expert 60 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336518.336518 lmp.py:1935]   Expert 52 |    357 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336016.336016 lmp.py:1935]   Expert 23 |    365 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336513.336513 lmp.py:1935]   Expert 44 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.336010.336010 lmp.py:1935]   Expert 53 |    434 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.336554.336554 lmp.py:1937] 
DEBUG 01-15 16:08:53.336554.336554 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.336812.336812 lmp.py:1938]   CPU:   2660 tokens
DEBUG 01-15 16:08:53.336369.336369 lmp.py:1942]   cuda:1:   4896 tokens (20 experts)
DEBUG 01-15 16:08:53.336271.336271 lmp.py:1942]   cuda:2:   4732 tokens (19 experts)
DEBUG 01-15 16:08:53.336007.336007 lmp.py:1943]   Total GPU:   9628 tokens
DEBUG 01-15 16:08:53.336981.336981 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.336981.336981 lmp.py:1944] 
DEBUG 01-15 16:08:53.336154.336154 cuda_h.py:19] end experts_map_get cost 0.0016739368438720703 seconds
DEBUG 01-15 16:08:53.336666.336666 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.336468.336468 lmp.py:1953] 
DEBUG 01-15 16:08:53.336468.336468 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.336510.336510 cuda_h.py:19] end cpu_experts_submit cost 6.103515625e-05 seconds
DEBUG 01-15 16:08:53.336729.336729 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.336202.336202 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.338690.338690 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.3.gate_proj.weight': 0, 'model.layers.16.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.16.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.16.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.16.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.16.mlp.experts.1.gate_proj.weight': 0, 'model.layers.16.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.36.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.36.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.36.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.37.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.37.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.37.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.61.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(19521863680, 5767168, 0, 0), (19527630848, 5767168, 5767168, 0), (19516096512, 5767168, 11534336, 0), (19573768192, 5767168, 17301504, 0), (19579535360, 5767168, 23068672, 0), (19568001024, 5767168, 28835840, 0), (19608371200, 5767168, 34603008, 0), (19614138368, 5767168, 40370176, 0), (19602604032, 5767168, 46137344, 0), (19625672704, 5767168, 51904512, 0), (19631439872, 5767168, 57671680, 0), (19619905536, 5767168, 63438848, 0), (19642974208, 5767168, 69206016, 0), (19648741376, 5767168, 74973184, 0), (19637207040, 5767168, 80740352, 0), (19746783232, 5767168, 86507520, 0), (19752550400, 5767168, 92274688, 0), (19741016064, 5767168, 98041856, 0), (19764084736, 5767168, 103809024, 0), (19769851904, 5767168, 109576192, 0), (19758317568, 5767168, 115343360, 0), (19798687744, 5767168, 121110528, 0), (19804454912, 5767168, 126877696, 0), (19792920576, 5767168, 132644864, 0), (19815989248, 5767168, 138412032, 0), (19821756416, 5767168, 144179200, 0), (19810222080, 5767168, 149946368, 0), (19833290752, 5767168, 155713536, 0), (19839057920, 5767168, 161480704, 0), (19827523584, 5767168, 167247872, 0), (19850592256, 5767168, 173015040, 0), (19856359424, 5767168, 178782208, 0), (19844825088, 5767168, 184549376, 0), (20144717824, 5767168, 190316544, 0), (20150484992, 5767168, 196083712, 0), (20138950656, 5767168, 201850880, 0), (20162019328, 5767168, 207618048, 0), (20167786496, 5767168, 213385216, 0), (20156252160, 5767168, 219152384, 0), (20196622336, 5767168, 224919552, 0), (20202389504, 5767168, 230686720, 0), (20190855168, 5767168, 236453888, 0), (20265828352, 5767168, 242221056, 0), (20271595520, 5767168, 247988224, 0), (20260061184, 5767168, 253755392, 0), (20300431360, 5767168, 259522560, 0), (20306198528, 5767168, 265289728, 0), (20294664192, 5767168, 271056896, 0), (20369637376, 5767168, 276824064, 0), (20375404544, 5767168, 282591232, 0), (20363870208, 5767168, 288358400, 0), (20386938880, 5767168, 294125568, 0), (20392706048, 5767168, 299892736, 0), (20381171712, 5767168, 305659904, 0), (20542652416, 5767168, 311427072, 0), (20548419584, 5767168, 317194240, 0), (20536885248, 5767168, 322961408, 0), (20559953920, 5767168, 328728576, 0), (20565721088, 5767168, 334495744, 0), (20554186752, 5767168, 340262912, 0)], 2: [(19487260672, 5767168, 0, 0), (19493027840, 5767168, 5767168, 0), (19481493504, 5767168, 11534336, 0), (19556466688, 5767168, 17301504, 0), (19562233856, 5767168, 23068672, 0), (19550699520, 5767168, 28835840, 0), (19591069696, 5767168, 34603008, 0), (19596836864, 5767168, 40370176, 0), (19585302528, 5767168, 46137344, 0), (19677577216, 5767168, 51904512, 0), (19683344384, 5767168, 57671680, 0), (19671810048, 5767168, 63438848, 0), (19694878720, 5767168, 69206016, 0), (19700645888, 5767168, 74973184, 0), (19689111552, 5767168, 80740352, 0), (19729481728, 5767168, 86507520, 0), (19735248896, 5767168, 92274688, 0), (19723714560, 5767168, 98041856, 0), (19781386240, 5767168, 103809024, 0), (19787153408, 5767168, 109576192, 0), (19775619072, 5767168, 115343360, 0), (19867893760, 5767168, 121110528, 0), (19873660928, 5767168, 126877696, 0), (19862126592, 5767168, 132644864, 0), (19885195264, 5767168, 138412032, 0), (19890962432, 5767168, 144179200, 0), (19879428096, 5767168, 149946368, 0), (19971702784, 5767168, 155713536, 0), (19977469952, 5767168, 161480704, 0), (19965935616, 5767168, 167247872, 0), (19989004288, 5767168, 173015040, 0), (19994771456, 5767168, 178782208, 0), (19983237120, 5767168, 184549376, 0), (20023607296, 5767168, 190316544, 0), (20029374464, 5767168, 196083712, 0), (20017840128, 5767168, 201850880, 0), (20075511808, 5767168, 207618048, 0), (20081278976, 5767168, 213385216, 0), (20069744640, 5767168, 219152384, 0), (20092813312, 5767168, 224919552, 0), (20098580480, 5767168, 230686720, 0), (20087046144, 5767168, 236453888, 0), (20110114816, 5767168, 242221056, 0), (20115881984, 5767168, 247988224, 0), (20104347648, 5767168, 253755392, 0), (20231225344, 5767168, 259522560, 0), (20236992512, 5767168, 265289728, 0), (20225458176, 5767168, 271056896, 0), (20490747904, 5767168, 276824064, 0), (20496515072, 5767168, 282591232, 0), (20484980736, 5767168, 288358400, 0), (20508049408, 5767168, 294125568, 0), (20513816576, 5767168, 299892736, 0), (20502282240, 5767168, 305659904, 0), (20525350912, 5767168, 311427072, 0), (20531118080, 5767168, 317194240, 0), (20519583744, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a688600660>, 2: <capsule object NULL at 0x74a6bc5ec660>}
DEBUG 01-15 16:08:53.339523.339523 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.339685.339685 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.339924.339924 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 86f2d2ee-d36b-4a01-ba40-1c60648e0db4
DEBUG 01-15 16:08:53.339552.339552 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.339662.339662 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.339533.339533 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:08:53.339487.339487 client.py:127] Model loaded
DEBUG 01-15 16:08:53.339174.339174 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.340441.340441 cuda_h.py:19] end move_flatidxs cost 0.0009627342224121094 seconds
DEBUG 01-15 16:08:53.340609.340609 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.341066.341066 cuda_h.py:19] end restore2model cost 0.0012214183807373047 seconds
INFO 01-15 16:08:53.341384.341384 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 86f2d2ee-d36b-4a01-ba40-1c60648e0db4
DEBUG 01-15 16:08:53.341707.341707 cuda_h.py:19] end sllm_worker_task cost 0.013225555419921875 seconds
DEBUG 01-15 16:08:53.342444.342444 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005396842956542969 seconds
DEBUG 01-15 16:08:53.342629.342629 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.345647.345647 cuda_h.py:19] end restore2model cost 0.0030641555786132812 seconds
DEBUG 01-15 16:08:53.345365.345365 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008870601654052734 seconds
DEBUG 01-15 16:08:53.345922.345922 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.345383.345383 cuda_h.py:19] end gpu_sexperts cost 0.0002727508544921875 seconds
DEBUG 01-15 16:08:53.345835.345835 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.346671.346671 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.6464462280273438e-05 seconds
DEBUG 01-15 16:08:53.346890.346890 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.346262.346262 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.347560.347560 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009622573852539062 seconds
DEBUG 01-15 16:08:53.347688.347688 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.347656.347656 cuda_h.py:19] end gpu_group_list cost 0.00019311904907226562 seconds
DEBUG 01-15 16:08:53.348736.348736 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.349136.349136 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010106563568115234 seconds
DEBUG 01-15 16:08:53.349787.349787 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.349980.349980 cuda_h.py:19] end gpu_group_list cost 0.00018215179443359375 seconds
DEBUG 01-15 16:08:53.350008.350008 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.350030.350030 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 86f2d2ee-d36b-4a01-ba40-1c60648e0db4
DEBUG 01-15 16:08:53.351951.351951 cuda_h.py:19] end group_tensors cost 0.010780811309814453 seconds
DEBUG 01-15 16:08:53.352749.352749 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.356187.356187 cuda_h.py:19] end group pad cost 0.004517078399658203 seconds
DEBUG 01-15 16:08:53.356183.356183 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:53.376067.376067 client.py:127] Model loaded
DEBUG 01-15 16:08:53.376255.376255 cuda_h.py:19] end wait_experts_multi_device cost 0.02667379379272461 seconds
DEBUG 01-15 16:08:53.377769.377769 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.385212.385212 cuda_h.py:19] end group_einsum cost 0.028270483016967773 seconds
DEBUG 01-15 16:08:53.385760.385760 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.388821.388821 cuda_h.py:19] end get_outputs_cpu1 cost 0.0027544498443603516 seconds
DEBUG 01-15 16:08:53.389847.389847 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04989433288574219 seconds
DEBUG 01-15 16:08:53.389426.389426 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012810707092285156 seconds
DEBUG 01-15 16:08:53.390755.390755 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.391244.391244 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.391754.391754 cuda_h.py:19] end index_scatter cost 0.00010061264038085938 seconds
DEBUG 01-15 16:08:53.391588.391588 cuda_h.py:19] end cpuoutputsdeal cost 0.0018374919891357422 seconds
DEBUG 01-15 16:08:53.392552.392552 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.392242.392242 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.392357.392357 cuda_h.py:19] end gpu_group_tensor cost 0.0001976490020751953 seconds
DEBUG 01-15 16:08:53.392531.392531 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.392234.392234 cuda_h.py:19] end gpu_group_tensor cost 0.00019073486328125 seconds
DEBUG 01-15 16:08:53.392670.392670 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.393850.393850 cuda_h.py:19] end gpu_group_einsum cost 0.0007643699645996094 seconds
DEBUG 01-15 16:08:53.393074.393074 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.394135.394135 cuda_h.py:19] end gpu_group_einsum cost 0.0005669593811035156 seconds
DEBUG 01-15 16:08:53.394246.394246 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.394728.394728 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.395778.395778 cuda_h.py:19] end all_expert_outputs_slices cost 0.000247955322265625 seconds
DEBUG 01-15 16:08:53.395131.395131 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.395486.395486 cuda_h.py:19] end concat_expert_out cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:08:53.395416.395416 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.395308.395308 cuda_h.py:19] end index_scatter cost 7.677078247070312e-05 seconds
DEBUG 01-15 16:08:53.395195.395195 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011563301086425781 seconds
DEBUG 01-15 16:08:53.395702.395702 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.396195.396195 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.396065.396065 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020885467529296875 seconds
DEBUG 01-15 16:08:53.396934.396934 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.396700.396700 cuda_h.py:19] end concat_expert_out cost 8.487701416015625e-05 seconds
DEBUG 01-15 16:08:53.396781.396781 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.396374.396374 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:08:53.396944.396944 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006549358367919922 seconds
DEBUG 01-15 16:08:53.396371.396371 cuda_h.py:19] end gpu_experts_multi_device cost 0.05071878433227539 seconds
DEBUG 01-15 16:08:53.396333.396333 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.06281328201293945 seconds
DEBUG 01-15 16:08:53.397634.397634 cuda_h.py:19] end prefill_layer cost 0.06977200508117676 seconds
DEBUG 01-15 16:08:53.397676.397676 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:08:53.397094.397094 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.397512.397512 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:08:53.397453.397453 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:08:53.397448.397448 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:08:53.397358.397358 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 4.673004150390625e-05 seconds
DEBUG 01-15 16:08:53.397683.397683 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 7.843971252441406e-05 seconds
DEBUG 01-15 16:08:53.397664.397664 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.397302.397302 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.397313.397313 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.397763.397763 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.398027.398027 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.398766.398766 cuda_h.py:19] end allocate_cuda_memory cost 0.00039005279541015625 seconds
DEBUG 01-15 16:08:53.398387.398387 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.398252.398252 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.399375.399375 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.399120.399120 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0fce012f-2bb2-4338-bf98-724e212c0a8a
DEBUG 01-15 16:08:53.399227.399227 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.399395.399395 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.401044.401044 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0fce012f-2bb2-4338-bf98-724e212c0a8a
DEBUG 01-15 16:08:53.401831.401831 cuda_h.py:19] end load_into_gpu_async cost 0.0022504329681396484 seconds
DEBUG 01-15 16:08:53.401052.401052 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.401663.401663 cuda_h.py:19] end restore_tensors2 cost 0.00014400482177734375 seconds
DEBUG 01-15 16:08:53.401713.401713 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0035376548767089844 seconds
INFO 01-15 16:08:53.401354.401354 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0fce012f-2bb2-4338-bf98-724e212c0a8a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.403436.403436 cuda_h.py:19] end self_attn cost 0.0041811466217041016 seconds
DEBUG 01-15 16:08:53.404977.404977 cuda_h.py:19] end iln_self_attn_paln cost 0.006554365158081055 seconds
DEBUG 01-15 16:08:53.404668.404668 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:08:53.404338.404338 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.405207.405207 cuda_h.py:19] end gate cost 0.0008118152618408203 seconds
DEBUG 01-15 16:08:53.405805.405805 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.405289.405289 lmp.py:1912] 
DEBUG 01-15 16:08:53.405289.405289 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.405104.405104 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.405668.405668 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.405418.405418 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.405783.405783 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.405194.405194 lmp.py:1917] 
DEBUG 01-15 16:08:53.405194.405194 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.405082.405082 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.405600.405600 lmp.py:1935]   Expert  4 |     10 | CPU
DEBUG 01-15 16:08:53.405249.405249 lmp.py:1935]   Expert 28 |     28 | CPU
DEBUG 01-15 16:08:53.405945.405945 lmp.py:1935]   Expert  7 |     46 | CPU
DEBUG 01-15 16:08:53.405449.405449 lmp.py:1935]   Expert 53 |     57 | CPU
DEBUG 01-15 16:08:53.405192.405192 lmp.py:1935]   Expert 52 |     69 | CPU
DEBUG 01-15 16:08:53.405696.405696 lmp.py:1935]   Expert 43 |     71 | CPU
DEBUG 01-15 16:08:53.405915.405915 lmp.py:1935]   Expert 49 |     81 | CPU
DEBUG 01-15 16:08:53.405658.405658 lmp.py:1935]   Expert 12 |     88 | CPU
DEBUG 01-15 16:08:53.405638.405638 lmp.py:1935]   Expert 47 |    100 | CPU
DEBUG 01-15 16:08:53.405381.405381 lmp.py:1935]   Expert 24 |    105 | CPU
DEBUG 01-15 16:08:53.405362.405362 lmp.py:1935]   Expert 33 |    108 | CPU
DEBUG 01-15 16:08:53.406104.406104 lmp.py:1935]   Expert  2 |    109 | CPU
DEBUG 01-15 16:08:53.406085.406085 lmp.py:1935]   Expert 15 |    111 | CPU
DEBUG 01-15 16:08:53.406973.406973 lmp.py:1935]   Expert 50 |    111 | CPU
DEBUG 01-15 16:08:53.406954.406954 lmp.py:1935]   Expert 60 |    113 | CPU
DEBUG 01-15 16:08:53.406412.406412 lmp.py:1935]   Expert 39 |    115 | CPU
DEBUG 01-15 16:08:53.406346.406346 lmp.py:1935]   Expert 36 |    120 | CPU
DEBUG 01-15 16:08:53.406089.406089 lmp.py:1935]   Expert 25 |    123 | CPU
DEBUG 01-15 16:08:53.406831.406831 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:08:53.406097.406097 lmp.py:1935]   Expert 61 |    131 | CPU
DEBUG 01-15 16:08:53.406839.406839 lmp.py:1935]   Expert 59 |    136 | CPU
DEBUG 01-15 16:08:53.406105.406105 lmp.py:1935]   Expert  3 |    142 | CPU
DEBUG 01-15 16:08:53.406609.406609 lmp.py:1935]   Expert 27 |    144 | CPU
DEBUG 01-15 16:08:53.406874.406874 lmp.py:1935]   Expert 58 |    144 | CPU
DEBUG 01-15 16:08:53.406901.406901 lmp.py:1935]   Expert  8 |    149 | CPU
DEBUG 01-15 16:08:53.406266.406266 lmp.py:1935]   Expert 30 |    151 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406201.406201 lmp.py:1935]   Expert 31 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406135.406135 lmp.py:1935]   Expert 40 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406262.406262 lmp.py:1935]   Expert 10 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406196.406196 lmp.py:1935]   Expert 38 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406800.406800 lmp.py:1935]   Expert 57 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406046.406046 lmp.py:1935]   Expert 14 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406934.406934 lmp.py:1935]   Expert 41 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406438.406438 lmp.py:1935]   Expert 54 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406704.406704 lmp.py:1935]   Expert 32 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406970.406970 lmp.py:1935]   Expert 37 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406997.406997 lmp.py:1935]   Expert 46 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406024.406024 lmp.py:1935]   Expert 19 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406051.406051 lmp.py:1935]   Expert 42 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406317.406317 lmp.py:1935]   Expert 11 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406867.406867 lmp.py:1935]   Expert 34 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406133.406133 lmp.py:1935]   Expert 18 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406305.406305 lmp.py:1935]   Expert 22 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406115.406115 lmp.py:1935]   Expert 26 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406281.406281 lmp.py:1935]   Expert 56 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406685.406685 lmp.py:1935]   Expert  0 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406282.406282 lmp.py:1935]   Expert  1 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406448.406448 lmp.py:1935]   Expert 44 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406376.406376 lmp.py:1935]   Expert 51 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406542.406542 lmp.py:1935]   Expert 20 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406470.406470 lmp.py:1935]   Expert 29 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406159.406159 lmp.py:1935]   Expert 48 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406848.406848 lmp.py:1935]   Expert 45 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406538.406538 lmp.py:1935]   Expert 21 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406227.406227 lmp.py:1935]   Expert 35 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406393.406393 lmp.py:1935]   Expert 55 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406559.406559 lmp.py:1935]   Expert 16 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406487.406487 lmp.py:1935]   Expert  5 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.406176.406176 lmp.py:1935]   Expert 23 |    372 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.406111.406111 lmp.py:1935]   Expert 13 |    381 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.407277.407277 lmp.py:1935]   Expert 17 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.407443.407443 lmp.py:1935]   Expert  9 |    456 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.407132.407132 lmp.py:1935]   Expert 63 |    464 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.407298.407298 lmp.py:1935]   Expert 62 |   1184 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.407703.407703 lmp.py:1937] 
DEBUG 01-15 16:08:53.407703.407703 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.407631.407631 lmp.py:1938]   CPU:   2538 tokens
DEBUG 01-15 16:08:53.407797.407797 lmp.py:1942]   cuda:1:   4909 tokens (19 experts)
DEBUG 01-15 16:08:53.407725.407725 lmp.py:1942]   cuda:2:   4841 tokens (20 experts)
DEBUG 01-15 16:08:53.407699.407699 lmp.py:1943]   Total GPU:   9750 tokens
DEBUG 01-15 16:08:53.407434.407434 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.407434.407434 lmp.py:1944] 
DEBUG 01-15 16:08:53.407607.407607 cuda_h.py:19] end experts_map_get cost 0.0019593238830566406 seconds
DEBUG 01-15 16:08:53.407212.407212 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.407060.407060 lmp.py:1953] 
DEBUG 01-15 16:08:53.407060.407060 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.407519.407519 cuda_h.py:19] end cpu_experts_submit cost 5.650520324707031e-05 seconds
DEBUG 01-15 16:08:53.407977.407977 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.407681.407681 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.408424.408424 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.38.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.38.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.38.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.17.mlp.experts.1.gate_proj.weight': 0, 'model.layers.17.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.9.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.9.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.9.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.17.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.17.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.17.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (20663762944, 5767168, 17301504, 0), (20669530112, 5767168, 23068672, 0), (20657995776, 5767168, 28835840, 0), (20767571968, 5767168, 34603008, 0), (20773339136, 5767168, 40370176, 0), (20761804800, 5767168, 46137344, 0), (20802174976, 5767168, 51904512, 0), (20807942144, 5767168, 57671680, 0), (20796407808, 5767168, 63438848, 0), (20819476480, 5767168, 69206016, 0), (20825243648, 5767168, 74973184, 0), (20813709312, 5767168, 80740352, 0), (20905984000, 5767168, 86507520, 0), (20911751168, 5767168, 92274688, 0), (20900216832, 5767168, 98041856, 0), (20923285504, 5767168, 103809024, 0), (20929052672, 5767168, 109576192, 0), (20917518336, 5767168, 115343360, 0), (20940587008, 5767168, 121110528, 0), (20946354176, 5767168, 126877696, 0), (20934819840, 5767168, 132644864, 0), (20957888512, 5767168, 138412032, 0), (20963655680, 5767168, 144179200, 0), (20952121344, 5767168, 149946368, 0), (21027094528, 5767168, 155713536, 0), (21032861696, 5767168, 161480704, 0), (21021327360, 5767168, 167247872, 0), (21096300544, 5767168, 173015040, 0), (21102067712, 5767168, 178782208, 0), (21090533376, 5767168, 184549376, 0), (21217411072, 5767168, 190316544, 0), (21223178240, 5767168, 196083712, 0), (21211643904, 5767168, 201850880, 0), (21234712576, 5767168, 207618048, 0), (21240479744, 5767168, 213385216, 0), (21228945408, 5767168, 219152384, 0), (21269315584, 5767168, 224919552, 0), (21275082752, 5767168, 230686720, 0), (21263548416, 5767168, 236453888, 0), (21286617088, 5767168, 242221056, 0), (21292384256, 5767168, 247988224, 0), (21280849920, 5767168, 253755392, 0), (21338521600, 5767168, 259522560, 0), (21344288768, 5767168, 265289728, 0), (21332754432, 5767168, 271056896, 0), (21407727616, 5767168, 276824064, 0), (21413494784, 5767168, 282591232, 0), (21401960448, 5767168, 288358400, 0), (21528838144, 5767168, 294125568, 0), (21534605312, 5767168, 299892736, 0), (21523070976, 5767168, 305659904, 0), (21649948672, 5767168, 311427072, 0), (21655715840, 5767168, 317194240, 0), (21644181504, 5767168, 322961408, 0)], 2: [(20594556928, 5767168, 0, 0), (20600324096, 5767168, 5767168, 0), (20588789760, 5767168, 11534336, 0), (20732968960, 5767168, 17301504, 0), (20738736128, 5767168, 23068672, 0), (20727201792, 5767168, 28835840, 0), (20750270464, 5767168, 34603008, 0), (20756037632, 5767168, 40370176, 0), (20744503296, 5767168, 46137344, 0), (20854079488, 5767168, 51904512, 0), (20859846656, 5767168, 57671680, 0), (20848312320, 5767168, 63438848, 0), (20871380992, 5767168, 69206016, 0), (20877148160, 5767168, 74973184, 0), (20865613824, 5767168, 80740352, 0), (20888682496, 5767168, 86507520, 0), (20894449664, 5767168, 92274688, 0), (20882915328, 5767168, 98041856, 0), (20975190016, 5767168, 103809024, 0), (20980957184, 5767168, 109576192, 0), (20969422848, 5767168, 115343360, 0), (21078999040, 5767168, 121110528, 0), (21084766208, 5767168, 126877696, 0), (21073231872, 5767168, 132644864, 0), (21113602048, 5767168, 138412032, 0), (21119369216, 5767168, 144179200, 0), (21107834880, 5767168, 149946368, 0), (21130903552, 5767168, 155713536, 0), (21136670720, 5767168, 161480704, 0), (21125136384, 5767168, 167247872, 0), (21165506560, 5767168, 173015040, 0), (21171273728, 5767168, 178782208, 0), (21159739392, 5767168, 184549376, 0), (21182808064, 5767168, 190316544, 0), (21188575232, 5767168, 196083712, 0), (21177040896, 5767168, 201850880, 0), (21303918592, 5767168, 207618048, 0), (21309685760, 5767168, 213385216, 0), (21298151424, 5767168, 219152384, 0), (21355823104, 5767168, 224919552, 0), (21361590272, 5767168, 230686720, 0), (21350055936, 5767168, 236453888, 0), (21373124608, 5767168, 242221056, 0), (21378891776, 5767168, 247988224, 0), (21367357440, 5767168, 253755392, 0), (21459632128, 5767168, 259522560, 0), (21465399296, 5767168, 265289728, 0), (21453864960, 5767168, 271056896, 0), (21511536640, 5767168, 276824064, 0), (21517303808, 5767168, 282591232, 0), (21505769472, 5767168, 288358400, 0), (21546139648, 5767168, 294125568, 0), (21551906816, 5767168, 299892736, 0), (21540372480, 5767168, 305659904, 0), (21563441152, 5767168, 311427072, 0), (21569208320, 5767168, 317194240, 0), (21557673984, 5767168, 322961408, 0), (21667250176, 5767168, 328728576, 0), (21673017344, 5767168, 334495744, 0), (21661483008, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc278660>, 2: <capsule object NULL at 0x74a660729b00>}
DEBUG 01-15 16:08:53.408494.408494 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.408635.408635 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.408345.408345 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1333cf67-7b20-4821-a69b-127796b4bba9
DEBUG 01-15 16:08:53.408462.408462 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.409863.409863 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.409905.409905 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:08:53.409183.409183 client.py:127] Model loaded
DEBUG 01-15 16:08:53.409876.409876 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.410009.410009 cuda_h.py:19] end move_flatidxs cost 0.0008671283721923828 seconds
DEBUG 01-15 16:08:53.410838.410838 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.410926.410926 cuda_h.py:19] end restore2model cost 0.0009980201721191406 seconds
DEBUG 01-15 16:08:53.410274.410274 cuda_h.py:19] end sllm_worker_task cost 0.012716054916381836 seconds
INFO 01-15 16:08:53.410801.410801 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1333cf67-7b20-4821-a69b-127796b4bba9
DEBUG 01-15 16:08:53.411192.411192 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003958225250244141 seconds
DEBUG 01-15 16:08:53.411639.411639 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.414468.414468 cuda_h.py:19] end restore2model cost 0.003175020217895508 seconds
DEBUG 01-15 16:08:53.414364.414364 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007391691207885742 seconds
DEBUG 01-15 16:08:53.414942.414942 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.415237.415237 cuda_h.py:19] end gpu_sexperts cost 0.0002932548522949219 seconds
DEBUG 01-15 16:08:53.415636.415636 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.415459.415459 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:08:53.415155.415155 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.415004.415004 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.416865.416865 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.00095367431640625 seconds
DEBUG 01-15 16:08:53.416231.416231 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.416749.416749 cuda_h.py:19] end gpu_group_list cost 0.00021195411682128906 seconds
DEBUG 01-15 16:08:53.417684.417684 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.418330.418330 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001051187515258789 seconds
DEBUG 01-15 16:08:53.418412.418412 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.418029.418029 cuda_h.py:19] end gpu_group_list cost 0.00021529197692871094 seconds
DEBUG 01-15 16:08:53.419126.419126 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.419194.419194 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1333cf67-7b20-4821-a69b-127796b4bba9
DEBUG 01-15 16:08:53.419596.419596 cuda_h.py:19] end group_tensors cost 0.009331226348876953 seconds
DEBUG 01-15 16:08:53.420071.420071 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.424079.424079 cuda_h.py:19] end group pad cost 0.0037381649017333984 seconds
DEBUG 01-15 16:08:53.424028.424028 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:53.444656.444656 client.py:127] Model loaded
DEBUG 01-15 16:08:53.445731.445731 cuda_h.py:19] end wait_experts_multi_device cost 0.025497913360595703 seconds
DEBUG 01-15 16:08:53.445005.445005 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.451477.451477 cuda_h.py:19] end group_einsum cost 0.026949644088745117 seconds
DEBUG 01-15 16:08:53.451363.451363 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.454946.454946 cuda_h.py:19] end get_outputs_cpu1 cost 0.0027170181274414062 seconds
DEBUG 01-15 16:08:53.455837.455837 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04594898223876953 seconds
DEBUG 01-15 16:08:53.455663.455663 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010614871978759766 seconds
DEBUG 01-15 16:08:53.456259.456259 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.458069.458069 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.458669.458669 cuda_h.py:19] end index_scatter cost 0.00014781951904296875 seconds
DEBUG 01-15 16:08:53.459481.459481 cuda_h.py:19] end cpuoutputsdeal cost 0.002976655960083008 seconds
DEBUG 01-15 16:08:53.459918.459918 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.459709.459709 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.459662.459662 cuda_h.py:19] end gpu_group_tensor cost 0.00030541419982910156 seconds
DEBUG 01-15 16:08:53.459069.459069 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.460519.460519 cuda_h.py:19] end gpu_group_tensor cost 0.0002872943878173828 seconds
DEBUG 01-15 16:08:53.460705.460705 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.462740.462740 cuda_h.py:19] end gpu_group_einsum cost 0.0022933483123779297 seconds
DEBUG 01-15 16:08:53.463398.463398 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.464063.464063 cuda_h.py:19] end gpu_group_einsum cost 0.001134634017944336 seconds
DEBUG 01-15 16:08:53.464654.464654 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.465742.465742 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.465922.465922 cuda_h.py:19] end all_expert_outputs_slices cost 0.0005183219909667969 seconds
DEBUG 01-15 16:08:53.465336.465336 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.465707.465707 cuda_h.py:19] end concat_expert_out cost 0.00012373924255371094 seconds
DEBUG 01-15 16:08:53.466579.466579 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.466679.466679 cuda_h.py:19] end index_scatter cost 0.00012540817260742188 seconds
DEBUG 01-15 16:08:53.466861.466861 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001895904541015625 seconds
DEBUG 01-15 16:08:53.466000.466000 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.467952.467952 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.467999.467999 cuda_h.py:19] end all_expert_outputs_slices cost 0.00033926963806152344 seconds
DEBUG 01-15 16:08:53.467340.467340 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.467842.467842 cuda_h.py:19] end concat_expert_out cost 8.821487426757812e-05 seconds
DEBUG 01-15 16:08:53.467029.467029 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.467542.467542 cuda_h.py:19] end index_scatter cost 5.7220458984375e-05 seconds
DEBUG 01-15 16:08:53.467074.467074 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009176731109619141 seconds
DEBUG 01-15 16:08:53.468090.468090 cuda_h.py:19] end gpu_experts_multi_device cost 0.05280733108520508 seconds
DEBUG 01-15 16:08:53.468497.468497 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.06388378143310547 seconds
DEBUG 01-15 16:08:53.468421.468421 cuda_h.py:19] end prefill_layer cost 0.07123970985412598 seconds
DEBUG 01-15 16:08:53.468363.468363 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:08:53.468166.468166 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.468637.468637 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:08:53.468062.468062 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:08:53.468778.468778 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:08:53.468238.468238 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 6.008148193359375e-05 seconds
DEBUG 01-15 16:08:53.468571.468571 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 9.822845458984375e-05 seconds
DEBUG 01-15 16:08:53.469843.469843 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.469984.469984 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.469444.469444 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.469579.469579 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.469575.469575 cuda_h.py:19] end allocate_cuda_memory cost 0.00024056434631347656 seconds
DEBUG 01-15 16:08:53.469617.469617 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.469426.469426 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.469494.469494 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.469104.469104 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e954c985-c7a4-40bf-83b2-f5cac13bb7ce
DEBUG 01-15 16:08:53.469286.469286 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.470479.470479 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.470757.470757 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.472367.472367 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e954c985-c7a4-40bf-83b2-f5cac13bb7ce
DEBUG 01-15 16:08:53.472872.472872 cuda_h.py:19] end load_into_gpu_async cost 0.002438068389892578 seconds
DEBUG 01-15 16:08:53.472429.472429 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.472949.472949 cuda_h.py:19] end restore_tensors2 cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:08:53.472275.472275 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0030019283294677734 seconds
INFO 01-15 16:08:53.472171.472171 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e954c985-c7a4-40bf-83b2-f5cac13bb7ce
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.473923.473923 cuda_h.py:19] end self_attn cost 0.0035305023193359375 seconds
DEBUG 01-15 16:08:53.474327.474327 cuda_h.py:19] end iln_self_attn_paln cost 0.005302906036376953 seconds
DEBUG 01-15 16:08:53.474600.474600 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:08:53.474515.474515 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.475192.475192 cuda_h.py:19] end gate cost 0.0007708072662353516 seconds
DEBUG 01-15 16:08:53.475896.475896 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.475831.475831 lmp.py:1912] 
DEBUG 01-15 16:08:53.475831.475831 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.475343.475343 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.475437.475437 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.475047.475047 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.475796.475796 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.475162.475162 lmp.py:1917] 
DEBUG 01-15 16:08:53.475162.475162 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.475480.475480 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.476144.476144 lmp.py:1935]   Expert 32 |     32 | CPU
DEBUG 01-15 16:08:53.476224.476224 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:08:53.476350.476350 lmp.py:1935]   Expert  5 |     54 | CPU
DEBUG 01-15 16:08:53.476239.476239 lmp.py:1935]   Expert 46 |     74 | CPU
DEBUG 01-15 16:08:53.476319.476319 lmp.py:1935]   Expert  8 |     90 | CPU
DEBUG 01-15 16:08:53.476876.476876 lmp.py:1935]   Expert 40 |     90 | CPU
DEBUG 01-15 16:08:53.476778.476778 lmp.py:1935]   Expert 12 |     98 | CPU
DEBUG 01-15 16:08:53.476620.476620 lmp.py:1935]   Expert 17 |    107 | CPU
DEBUG 01-15 16:08:53.476269.476269 lmp.py:1935]   Expert  3 |    112 | CPU
DEBUG 01-15 16:08:53.476158.476158 lmp.py:1935]   Expert 60 |    113 | CPU
DEBUG 01-15 16:08:53.476569.476569 lmp.py:1935]   Expert 27 |    116 | CPU
DEBUG 01-15 16:08:53.476219.476219 lmp.py:1935]   Expert 29 |    118 | CPU
DEBUG 01-15 16:08:53.476392.476392 lmp.py:1935]   Expert 58 |    118 | CPU
DEBUG 01-15 16:08:53.476803.476803 lmp.py:1935]   Expert 21 |    119 | CPU
DEBUG 01-15 16:08:53.476214.476214 lmp.py:1935]   Expert 28 |    120 | CPU
DEBUG 01-15 16:08:53.476533.476533 lmp.py:1935]   Expert 25 |    126 | CPU
DEBUG 01-15 16:08:53.479789.479789 lmp.py:1935]   Expert 41 |    126 | CPU
DEBUG 01-15 16:08:53.479962.479962 lmp.py:1935]   Expert 35 |    132 | CPU
DEBUG 01-15 16:08:53.479082.479082 lmp.py:1935]   Expert 19 |    134 | CPU
DEBUG 01-15 16:08:53.479963.479963 lmp.py:1935]   Expert  0 |    142 | CPU
DEBUG 01-15 16:08:53.479606.479606 lmp.py:1935]   Expert 52 |    145 | CPU
DEBUG 01-15 16:08:53.479124.479124 lmp.py:1935]   Expert  6 |    147 | CPU
DEBUG 01-15 16:08:53.479912.479912 lmp.py:1935]   Expert 56 |    148 | CPU
DEBUG 01-15 16:08:53.480747.480747 lmp.py:1935]   Expert 54 |    152 | CPU
DEBUG 01-15 16:08:53.480344.480344 lmp.py:1935]   Expert 53 |    153 | CPU
DEBUG 01-15 16:08:53.480279.480279 lmp.py:1935]   Expert 37 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480259.480259 lmp.py:1935]   Expert 48 |    156 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480333.480333 lmp.py:1935]   Expert 63 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480930.480930 lmp.py:1935]   Expert 36 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480526.480526 lmp.py:1935]   Expert 59 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480884.480884 lmp.py:1935]   Expert  9 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480481.480481 lmp.py:1935]   Expert  1 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480839.480839 lmp.py:1935]   Expert 39 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480436.480436 lmp.py:1935]   Expert 20 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480317.480317 lmp.py:1935]   Expert 43 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480437.480437 lmp.py:1935]   Expert 61 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480557.480557 lmp.py:1935]   Expert 42 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480107.480107 lmp.py:1935]   Expert  7 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480896.480896 lmp.py:1935]   Expert 34 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480685.480685 lmp.py:1935]   Expert 11 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480474.480474 lmp.py:1935]   Expert 47 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480785.480785 lmp.py:1935]   Expert 55 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480859.480859 lmp.py:1935]   Expert 13 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480654.480654 lmp.py:1935]   Expert 16 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480013.480013 lmp.py:1935]   Expert 57 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480609.480609 lmp.py:1935]   Expert 18 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480444.480444 lmp.py:1935]   Expert 15 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480803.480803 lmp.py:1935]   Expert  4 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480922.480922 lmp.py:1935]   Expert 50 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480519.480519 lmp.py:1935]   Expert 22 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480831.480831 lmp.py:1935]   Expert 33 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480620.480620 lmp.py:1935]   Expert 31 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480501.480501 lmp.py:1935]   Expert 45 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480581.480581 lmp.py:1935]   Expert 51 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480178.480178 lmp.py:1935]   Expert 49 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480059.480059 lmp.py:1935]   Expert 38 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480702.480702 lmp.py:1935]   Expert 26 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480584.480584 lmp.py:1935]   Expert 10 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480465.480465 lmp.py:1935]   Expert 44 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480347.480347 lmp.py:1935]   Expert  2 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480228.480228 lmp.py:1935]   Expert 24 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480109.480109 lmp.py:1935]   Expert 14 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480229.480229 lmp.py:1935]   Expert 23 |    410 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.480111.480111 lmp.py:1935]   Expert 62 |    671 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.480038.480038 lmp.py:1937] 
DEBUG 01-15 16:08:53.480038.480038 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.480397.480397 lmp.py:1938]   CPU:   2818 tokens
DEBUG 01-15 16:08:53.480232.480232 lmp.py:1942]   cuda:1:   4736 tokens (19 experts)
DEBUG 01-15 16:08:53.480782.480782 lmp.py:1942]   cuda:2:   4734 tokens (20 experts)
DEBUG 01-15 16:08:53.480379.480379 lmp.py:1943]   Total GPU:   9470 tokens
DEBUG 01-15 16:08:53.480214.480214 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.480214.480214 lmp.py:1944] 
INFO 01-15 16:08:53.480223.480223 client.py:127] Model loaded
DEBUG 01-15 16:08:53.481285.481285 cuda_h.py:19] end experts_map_get cost 0.005654096603393555 seconds
DEBUG 01-15 16:08:53.481737.481737 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.481897.481897 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.481061.481061 lmp.py:1953] 
DEBUG 01-15 16:08:53.481061.481061 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.482596.482596 cuda_h.py:19] end cpu_experts_submit cost 0.00018739700317382812 seconds
DEBUG 01-15 16:08:53.482880.482880 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.482521.482521 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.482938.482938 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.483248.483248 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:53.483248.483248 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.1.gate_proj.weight': 0, 'model.layers.18.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.11.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.11.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.11.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.15.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.15.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.15.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.24.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.24.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.24.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.26.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.26.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.26.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.33.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.33.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.33.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.34.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.34.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.34.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.36.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.36.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.36.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.43.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.43.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.43.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.44.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.44.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.44.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.45.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.45.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.45.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.48.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.48.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.48.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.57.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.18.mlp.experts.2.gate_proj.weight': 0, 'model.layers.18.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.31.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.31.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.31.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.38.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.38.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.38.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.39.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.39.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.39.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.42.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.42.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.42.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.47.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.47.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.47.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.51.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.51.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.51.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.55.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.55.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.55.up_proj.weight': 322961408, 'model.layers.18.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.18.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.18.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(21701853184, 5767168, 0, 0), (21707620352, 5767168, 5767168, 0), (21696086016, 5767168, 11534336, 0), (21874868224, 5767168, 17301504, 0), (21880635392, 5767168, 23068672, 0), (21869101056, 5767168, 28835840, 0), (21944074240, 5767168, 34603008, 0), (21949841408, 5767168, 40370176, 0), (21938307072, 5767168, 46137344, 0), (21961375744, 5767168, 51904512, 0), (21967142912, 5767168, 57671680, 0), (21955608576, 5767168, 63438848, 0), (22099787776, 5767168, 69206016, 0), (22105554944, 5767168, 74973184, 0), (22094020608, 5767168, 80740352, 0), (22134390784, 5767168, 86507520, 0), (22140157952, 5767168, 92274688, 0), (22128623616, 5767168, 98041856, 0), (22255501312, 5767168, 103809024, 0), (22261268480, 5767168, 109576192, 0), (22249734144, 5767168, 115343360, 0), (22272802816, 5767168, 121110528, 0), (22278569984, 5767168, 126877696, 0), (22267035648, 5767168, 132644864, 0), (22307405824, 5767168, 138412032, 0), (22313172992, 5767168, 144179200, 0), (22301638656, 5767168, 149946368, 0), (22428516352, 5767168, 155713536, 0), (22434283520, 5767168, 161480704, 0), (22422749184, 5767168, 167247872, 0), (22445817856, 5767168, 173015040, 0), (22451585024, 5767168, 178782208, 0), (22440050688, 5767168, 184549376, 0), (22463119360, 5767168, 190316544, 0), (22468886528, 5767168, 196083712, 0), (22457352192, 5767168, 201850880, 0), (22515023872, 5767168, 207618048, 0), (22520791040, 5767168, 213385216, 0), (22509256704, 5767168, 219152384, 0), (22532325376, 5767168, 224919552, 0), (22538092544, 5767168, 230686720, 0), (22526558208, 5767168, 236453888, 0), (22549626880, 5767168, 242221056, 0), (22555394048, 5767168, 247988224, 0), (22543859712, 5767168, 253755392, 0), (22670737408, 5767168, 259522560, 0), (22676504576, 5767168, 265289728, 0), (22664970240, 5767168, 271056896, 0), (22705340416, 5767168, 276824064, 0), (22711107584, 5767168, 282591232, 0), (22699573248, 5767168, 288358400, 0), (22739943424, 5767168, 294125568, 0), (22745710592, 5767168, 299892736, 0), (22734176256, 5767168, 305659904, 0), (22757244928, 5767168, 311427072, 0), (22763012096, 5767168, 317194240, 0), (22751477760, 5767168, 322961408, 0)], 2: [(21719154688, 5767168, 0, 0), (21724921856, 5767168, 5767168, 0), (21713387520, 5767168, 11534336, 0), (21753757696, 5767168, 17301504, 0), (21759524864, 5767168, 23068672, 0), (21747990528, 5767168, 28835840, 0), (21805662208, 5767168, 34603008, 0), (21811429376, 5767168, 40370176, 0), (21799895040, 5767168, 46137344, 0), (21840265216, 5767168, 51904512, 0), (21846032384, 5767168, 57671680, 0), (21834498048, 5767168, 63438848, 0), (21857566720, 5767168, 69206016, 0), (21863333888, 5767168, 74973184, 0), (21851799552, 5767168, 80740352, 0), (21909471232, 5767168, 86507520, 0), (21915238400, 5767168, 92274688, 0), (21903704064, 5767168, 98041856, 0), (21926772736, 5767168, 103809024, 0), (21932539904, 5767168, 109576192, 0), (21921005568, 5767168, 115343360, 0), (21995978752, 5767168, 121110528, 0), (22001745920, 5767168, 126877696, 0), (21990211584, 5767168, 132644864, 0), (22030581760, 5767168, 138412032, 0), (22036348928, 5767168, 144179200, 0), (22024814592, 5767168, 149946368, 0), (22065184768, 5767168, 155713536, 0), (22070951936, 5767168, 161480704, 0), (22059417600, 5767168, 167247872, 0), (22082486272, 5767168, 173015040, 0), (22088253440, 5767168, 178782208, 0), (22076719104, 5767168, 184549376, 0), (22220898304, 5767168, 190316544, 0), (22226665472, 5767168, 196083712, 0), (22215131136, 5767168, 201850880, 0), (22324707328, 5767168, 207618048, 0), (22330474496, 5767168, 213385216, 0), (22318940160, 5767168, 219152384, 0), (22342008832, 5767168, 224919552, 0), (22347776000, 5767168, 230686720, 0), (22336241664, 5767168, 236453888, 0), (22359310336, 5767168, 242221056, 0), (22365077504, 5767168, 247988224, 0), (22353543168, 5767168, 253755392, 0), (22411214848, 5767168, 259522560, 0), (22416982016, 5767168, 265289728, 0), (22405447680, 5767168, 271056896, 0), (22497722368, 5767168, 276824064, 0), (22503489536, 5767168, 282591232, 0), (22491955200, 5767168, 288358400, 0), (22566928384, 5767168, 294125568, 0), (22572695552, 5767168, 299892736, 0), (22561161216, 5767168, 305659904, 0), (22636134400, 5767168, 311427072, 0), (22641901568, 5767168, 317194240, 0), (22630367232, 5767168, 322961408, 0), (22774546432, 5767168, 328728576, 0), (22780313600, 5767168, 334495744, 0), (22768779264, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6607298c0>, 2: <capsule object NULL at 0x74a660729cb0>}
DEBUG 01-15 16:08:53.484773.484773 cuda_h.py:19] end move_flatidxs cost 0.001094818115234375 seconds
DEBUG 01-15 16:08:53.484000.484000 cuda_h.py:19] end restore2model cost 0.0031964778900146484 seconds
DEBUG 01-15 16:08:53.484188.484188 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.484608.484608 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.484701.484701 cuda_h.py:19] end sllm_worker_task cost 0.015558242797851562 seconds
DEBUG 01-15 16:08:53.484270.484270 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.485264.485264 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f1e353d4-5035-475e-bbf3-094753432ad0
DEBUG 01-15 16:08:53.485307.485307 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:53.487349.487349 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f1e353d4-5035-475e-bbf3-094753432ad0
DEBUG 01-15 16:08:53.489887.489887 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.006500720977783203 seconds
DEBUG 01-15 16:08:53.489046.489046 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.494629.494629 cuda_h.py:19] end group_tensors cost 0.009927511215209961 seconds
DEBUG 01-15 16:08:53.495171.495171 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.498464.498464 cuda_h.py:19] end group pad cost 0.0033445358276367188 seconds
DEBUG 01-15 16:08:53.498036.498036 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:53.499667.499667 cuda_h.py:19] end restore2model cost 0.010313987731933594 seconds
DEBUG 01-15 16:08:53.499911.499911 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0174715518951416 seconds
DEBUG 01-15 16:08:53.500282.500282 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.504422.504422 cuda_h.py:19] end gpu_sexperts cost 0.003641843795776367 seconds
DEBUG 01-15 16:08:53.504839.504839 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.505943.505943 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 0.00018548965454101562 seconds
DEBUG 01-15 16:08:53.505005.505005 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.505562.505562 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.509804.509804 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.003465414047241211 seconds
DEBUG 01-15 16:08:53.509154.509154 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.510433.510433 cuda_h.py:19] end gpu_group_list cost 0.00036334991455078125 seconds
DEBUG 01-15 16:08:53.514982.514982 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.519551.519551 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.00464940071105957 seconds
DEBUG 01-15 16:08:53.520054.520054 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.520458.520458 cuda_h.py:19] end gpu_group_list cost 0.0003676414489746094 seconds
DEBUG 01-15 16:08:53.521085.521085 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.521531.521531 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f1e353d4-5035-475e-bbf3-094753432ad0
INFO 01-15 16:08:53.524209.524209 client.py:127] Model loaded
DEBUG 01-15 16:08:53.525044.525044 cuda_h.py:19] end wait_experts_multi_device cost 0.0032083988189697266 seconds
DEBUG 01-15 16:08:53.525638.525638 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.536850.536850 cuda_h.py:19] end group_einsum cost 0.03708148002624512 seconds
DEBUG 01-15 16:08:53.536100.536100 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.540820.540820 cuda_h.py:19] end get_outputs_cpu1 cost 0.004613637924194336 seconds
DEBUG 01-15 16:08:53.541505.541505 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.058730125427246094 seconds
DEBUG 01-15 16:08:53.542928.542928 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01744365692138672 seconds
DEBUG 01-15 16:08:53.542299.542299 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.545856.545856 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.545071.545071 cuda_h.py:19] end index_scatter cost 0.00014710426330566406 seconds
DEBUG 01-15 16:08:53.545632.545632 cuda_h.py:19] end cpuoutputsdeal cost 0.002917766571044922 seconds
DEBUG 01-15 16:08:53.546478.546478 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.546170.546170 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.546130.546130 cuda_h.py:19] end gpu_group_tensor cost 0.0003116130828857422 seconds
DEBUG 01-15 16:08:53.546206.546206 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.547079.547079 cuda_h.py:19] end gpu_group_tensor cost 0.0002841949462890625 seconds
DEBUG 01-15 16:08:53.547669.547669 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.548577.548577 cuda_h.py:19] end gpu_group_einsum cost 0.0010766983032226562 seconds
DEBUG 01-15 16:08:53.548054.548054 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.549764.549764 cuda_h.py:19] end gpu_group_einsum cost 0.00035643577575683594 seconds
DEBUG 01-15 16:08:53.549813.549813 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.549327.549327 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.549638.549638 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015878677368164062 seconds
DEBUG 01-15 16:08:53.549679.549679 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.549079.549079 cuda_h.py:19] end concat_expert_out cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:08:53.549306.549306 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.549627.549627 cuda_h.py:19] end index_scatter cost 6.413459777832031e-05 seconds
DEBUG 01-15 16:08:53.550873.550873 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007367134094238281 seconds
DEBUG 01-15 16:08:53.550180.550180 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.550162.550162 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.550267.550267 cuda_h.py:19] end all_expert_outputs_slices cost 0.00011539459228515625 seconds
DEBUG 01-15 16:08:53.550685.550685 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.550363.550363 cuda_h.py:19] end concat_expert_out cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:08:53.550060.550060 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.550076.550076 cuda_h.py:19] end index_scatter cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:08:53.550501.550501 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004467964172363281 seconds
DEBUG 01-15 16:08:53.550881.550881 cuda_h.py:19] end gpu_experts_multi_device cost 0.044826507568359375 seconds
DEBUG 01-15 16:08:53.550645.550645 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.07626080513000488 seconds
DEBUG 01-15 16:08:53.551029.551029 cuda_h.py:19] end prefill_layer cost 0.08230876922607422 seconds
DEBUG 01-15 16:08:53.551044.551044 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:08:53.551124.551124 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.551158.551158 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:08:53.551384.551384 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:08:53.551232.551232 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:08:53.551129.551129 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:08:53.551024.551024 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:08:53.551150.551150 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.551881.551881 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.551369.551369 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.551701.551701 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.552179.552179 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.552148.552148 cuda_h.py:19] end allocate_cuda_memory cost 0.0005199909210205078 seconds
DEBUG 01-15 16:08:53.552699.552699 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.553333.553333 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.553234.553234 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.553467.553467 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3c920d9f-8571-470b-abea-1dbe1850b26b
DEBUG 01-15 16:08:53.553391.553391 cuda_h.py:10] start self_attn
DEBUG 01-15 16:08:53.553286.553286 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:53.555057.555057 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3c920d9f-8571-470b-abea-1dbe1850b26b
DEBUG 01-15 16:08:53.555957.555957 cuda_h.py:19] end load_into_gpu_async cost 0.0025911331176757812 seconds
DEBUG 01-15 16:08:53.555022.555022 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.556701.556701 cuda_h.py:19] end restore_tensors2 cost 0.00020885467529296875 seconds
DEBUG 01-15 16:08:53.556852.556852 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004359245300292969 seconds
INFO 01-15 16:08:53.556465.556465 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3c920d9f-8571-470b-abea-1dbe1850b26b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.558475.558475 cuda_h.py:19] end self_attn cost 0.00446319580078125 seconds
DEBUG 01-15 16:08:53.558764.558764 cuda_h.py:19] end iln_self_attn_paln cost 0.0074045658111572266 seconds
DEBUG 01-15 16:08:53.558547.558547 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:08:53.558925.558925 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.559636.559636 cuda_h.py:19] end gate cost 0.0006287097930908203 seconds
DEBUG 01-15 16:08:53.559896.559896 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.559629.559629 lmp.py:1912] 
DEBUG 01-15 16:08:53.559629.559629 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.559862.559862 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.559512.559512 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.560347.560347 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.560228.560228 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.560156.560156 lmp.py:1917] 
DEBUG 01-15 16:08:53.560156.560156 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.560561.560561 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.560449.560449 lmp.py:1935]   Expert 44 |     41 | CPU
DEBUG 01-15 16:08:53.560569.560569 lmp.py:1935]   Expert  1 |     48 | CPU
DEBUG 01-15 16:08:53.560735.560735 lmp.py:1935]   Expert 60 |     64 | CPU
DEBUG 01-15 16:08:53.560186.560186 lmp.py:1935]   Expert 28 |     70 | CPU
DEBUG 01-15 16:08:53.560875.560875 lmp.py:1935]   Expert 48 |     78 | CPU
DEBUG 01-15 16:08:53.560518.560518 lmp.py:1935]   Expert 27 |     86 | CPU
DEBUG 01-15 16:08:53.560161.560161 lmp.py:1935]   Expert  0 |    101 | CPU
DEBUG 01-15 16:08:53.560327.560327 lmp.py:1935]   Expert 62 |    108 | CPU
DEBUG 01-15 16:08:53.560493.560493 lmp.py:1935]   Expert 30 |    112 | CPU
DEBUG 01-15 16:08:53.560659.560659 lmp.py:1935]   Expert 22 |    113 | CPU
DEBUG 01-15 16:08:53.560686.560686 lmp.py:1935]   Expert 42 |    114 | CPU
DEBUG 01-15 16:08:53.560237.560237 lmp.py:1935]   Expert 59 |    119 | CPU
DEBUG 01-15 16:08:53.560833.560833 lmp.py:1935]   Expert 58 |    123 | CPU
DEBUG 01-15 16:08:53.560145.560145 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:08:53.560742.560742 lmp.py:1935]   Expert 12 |    128 | CPU
DEBUG 01-15 16:08:53.560815.560815 lmp.py:1935]   Expert  8 |    129 | CPU
DEBUG 01-15 16:08:53.560571.560571 lmp.py:1935]   Expert 50 |    135 | CPU
DEBUG 01-15 16:08:53.560983.560983 lmp.py:1935]   Expert 56 |    142 | CPU
DEBUG 01-15 16:08:53.560533.560533 lmp.py:1935]   Expert  5 |    143 | CPU
DEBUG 01-15 16:08:53.560845.560845 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:08:53.560634.560634 lmp.py:1935]   Expert 55 |    152 | CPU
DEBUG 01-15 16:08:53.560422.560422 lmp.py:1935]   Expert 26 |    153 | CPU
DEBUG 01-15 16:08:53.560257.560257 lmp.py:1935]   Expert 15 |    154 | CPU
DEBUG 01-15 16:08:53.560046.560046 lmp.py:1935]   Expert 32 |    157 | CPU
DEBUG 01-15 16:08:53.560312.560312 lmp.py:1935]   Expert 34 |    159 | CPU
DEBUG 01-15 16:08:53.560769.560769 lmp.py:1935]   Expert 47 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560797.560797 lmp.py:1935]   Expert 24 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560347.560347 lmp.py:1935]   Expert  2 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.560136.560136 lmp.py:1935]   Expert 52 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560163.560163 lmp.py:1935]   Expert 40 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.560667.560667 lmp.py:1935]   Expert 13 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560694.560694 lmp.py:1935]   Expert  6 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.560960.560960 lmp.py:1935]   Expert 41 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560464.560464 lmp.py:1935]   Expert  3 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560491.560491 lmp.py:1935]   Expert 18 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.560518.560518 lmp.py:1935]   Expert 54 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.560307.560307 lmp.py:1935]   Expert 37 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560049.560049 lmp.py:1935]   Expert 19 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.560076.560076 lmp.py:1935]   Expert 46 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560103.560103 lmp.py:1935]   Expert 20 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.560177.560177 lmp.py:1935]   Expert 25 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560727.560727 lmp.py:1935]   Expert 51 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.560516.560516 lmp.py:1935]   Expert 17 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560066.560066 lmp.py:1935]   Expert 43 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.560093.560093 lmp.py:1935]   Expert 31 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560597.560597 lmp.py:1935]   Expert 11 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.560625.560625 lmp.py:1935]   Expert 35 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561175.561175 lmp.py:1935]   Expert 23 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561440.561440 lmp.py:1935]   Expert 49 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.561229.561229 lmp.py:1935]   Expert 39 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561733.561733 lmp.py:1935]   Expert 53 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.561760.561760 lmp.py:1935]   Expert 10 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561834.561834 lmp.py:1935]   Expert 33 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.561384.561384 lmp.py:1935]   Expert 36 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561696.561696 lmp.py:1935]   Expert 38 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561485.561485 lmp.py:1935]   Expert  4 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.561082.561082 lmp.py:1935]   Expert 21 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561393.561393 lmp.py:1935]   Expert 14 |    348 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.561182.561182 lmp.py:1935]   Expert 63 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561971.561971 lmp.py:1935]   Expert 45 |    374 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.561998.561998 lmp.py:1935]   Expert  9 |    388 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561264.561264 lmp.py:1935]   Expert 61 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.561052.561052 lmp.py:1935]   Expert 29 |    488 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.561841.561841 lmp.py:1935]   Expert  7 |    515 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.561915.561915 lmp.py:1937] 
DEBUG 01-15 16:08:53.561915.561915 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.561750.561750 lmp.py:1938]   CPU:   2905 tokens
DEBUG 01-15 16:08:53.561777.561777 lmp.py:1942]   cuda:1:   4612 tokens (19 experts)
DEBUG 01-15 16:08:53.561089.561089 lmp.py:1942]   cuda:2:   4771 tokens (20 experts)
DEBUG 01-15 16:08:53.561685.561685 lmp.py:1943]   Total GPU:   9383 tokens
DEBUG 01-15 16:08:53.561805.561805 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.561805.561805 lmp.py:1944] 
DEBUG 01-15 16:08:53.561078.561078 cuda_h.py:19] end experts_map_get cost 0.0018248558044433594 seconds
DEBUG 01-15 16:08:53.561120.561120 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.561399.561399 lmp.py:1953] 
DEBUG 01-15 16:08:53.561399.561399 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.561665.561665 cuda_h.py:19] end cpu_experts_submit cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:08:53.561646.561646 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.561290.561290 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.562368.562368 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.2.gate_proj.weight': 0, 'model.layers.19.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.38.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.38.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.38.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.39.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.39.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.39.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.40.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.40.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.40.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.19.mlp.experts.3.gate_proj.weight': 0, 'model.layers.19.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.53.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.53.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.53.up_proj.weight': 322961408, 'model.layers.19.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.19.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.19.mlp.experts.61.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(22826450944, 5767168, 0, 0), (22832218112, 5767168, 5767168, 0), (22820683776, 5767168, 11534336, 0), (22895656960, 5767168, 17301504, 0), (22901424128, 5767168, 23068672, 0), (22889889792, 5767168, 28835840, 0), (22912958464, 5767168, 34603008, 0), (22918725632, 5767168, 40370176, 0), (22907191296, 5767168, 46137344, 0), (22947561472, 5767168, 51904512, 0), (22953328640, 5767168, 57671680, 0), (22941794304, 5767168, 63438848, 0), (22964862976, 5767168, 69206016, 0), (22970630144, 5767168, 74973184, 0), (22959095808, 5767168, 80740352, 0), (23103275008, 5767168, 86507520, 0), (23109042176, 5767168, 92274688, 0), (23097507840, 5767168, 98041856, 0), (23120576512, 5767168, 103809024, 0), (23126343680, 5767168, 109576192, 0), (23114809344, 5767168, 115343360, 0), (23137878016, 5767168, 121110528, 0), (23143645184, 5767168, 126877696, 0), (23132110848, 5767168, 132644864, 0), (23155179520, 5767168, 138412032, 0), (23160946688, 5767168, 144179200, 0), (23149412352, 5767168, 149946368, 0), (23189782528, 5767168, 155713536, 0), (23195549696, 5767168, 161480704, 0), (23184015360, 5767168, 167247872, 0), (23397400576, 5767168, 173015040, 0), (23403167744, 5767168, 178782208, 0), (23391633408, 5767168, 184549376, 0), (23414702080, 5767168, 190316544, 0), (23420469248, 5767168, 196083712, 0), (23408934912, 5767168, 201850880, 0), (23449305088, 5767168, 207618048, 0), (23455072256, 5767168, 213385216, 0), (23443537920, 5767168, 219152384, 0), (23466606592, 5767168, 224919552, 0), (23472373760, 5767168, 230686720, 0), (23460839424, 5767168, 236453888, 0), (23483908096, 5767168, 242221056, 0), (23489675264, 5767168, 247988224, 0), (23478140928, 5767168, 253755392, 0), (23535812608, 5767168, 259522560, 0), (23541579776, 5767168, 265289728, 0), (23530045440, 5767168, 271056896, 0), (23674224640, 5767168, 276824064, 0), (23679991808, 5767168, 282591232, 0), (23668457472, 5767168, 288358400, 0), (23726129152, 5767168, 294125568, 0), (23731896320, 5767168, 299892736, 0), (23720361984, 5767168, 305659904, 0), (23881842688, 5767168, 311427072, 0), (23887609856, 5767168, 317194240, 0), (23876075520, 5767168, 322961408, 0)], 2: [(22843752448, 5767168, 0, 0), (22849519616, 5767168, 5767168, 0), (22837985280, 5767168, 11534336, 0), (22861053952, 5767168, 17301504, 0), (22866821120, 5767168, 23068672, 0), (22855286784, 5767168, 28835840, 0), (22982164480, 5767168, 34603008, 0), (22987931648, 5767168, 40370176, 0), (22976397312, 5767168, 46137344, 0), (23016767488, 5767168, 51904512, 0), (23022534656, 5767168, 57671680, 0), (23011000320, 5767168, 63438848, 0), (23034068992, 5767168, 69206016, 0), (23039836160, 5767168, 74973184, 0), (23028301824, 5767168, 80740352, 0), (23085973504, 5767168, 86507520, 0), (23091740672, 5767168, 92274688, 0), (23080206336, 5767168, 98041856, 0), (23207084032, 5767168, 103809024, 0), (23212851200, 5767168, 109576192, 0), (23201316864, 5767168, 115343360, 0), (23224385536, 5767168, 121110528, 0), (23230152704, 5767168, 126877696, 0), (23218618368, 5767168, 132644864, 0), (23293591552, 5767168, 138412032, 0), (23299358720, 5767168, 144179200, 0), (23287824384, 5767168, 149946368, 0), (23328194560, 5767168, 155713536, 0), (23333961728, 5767168, 161480704, 0), (23322427392, 5767168, 167247872, 0), (23362797568, 5767168, 173015040, 0), (23368564736, 5767168, 178782208, 0), (23357030400, 5767168, 184549376, 0), (23432003584, 5767168, 190316544, 0), (23437770752, 5767168, 196083712, 0), (23426236416, 5767168, 201850880, 0), (23501209600, 5767168, 207618048, 0), (23506976768, 5767168, 213385216, 0), (23495442432, 5767168, 219152384, 0), (23570415616, 5767168, 224919552, 0), (23576182784, 5767168, 230686720, 0), (23564648448, 5767168, 236453888, 0), (23587717120, 5767168, 242221056, 0), (23593484288, 5767168, 247988224, 0), (23581949952, 5767168, 253755392, 0), (23605018624, 5767168, 259522560, 0), (23610785792, 5767168, 265289728, 0), (23599251456, 5767168, 271056896, 0), (23639621632, 5767168, 276824064, 0), (23645388800, 5767168, 282591232, 0), (23633854464, 5767168, 288358400, 0), (23691526144, 5767168, 294125568, 0), (23697293312, 5767168, 299892736, 0), (23685758976, 5767168, 305659904, 0), (23708827648, 5767168, 311427072, 0), (23714594816, 5767168, 317194240, 0), (23703060480, 5767168, 322961408, 0), (23847239680, 5767168, 328728576, 0), (23853006848, 5767168, 334495744, 0), (23841472512, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6883d0660>, 2: <capsule object NULL at 0x74a660729dd0>}
DEBUG 01-15 16:08:53.562765.562765 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.562052.562052 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.562338.562338 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1a5702e1-dac3-4d8c-ac6c-b66bfde94141
DEBUG 01-15 16:08:53.562427.562427 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.563836.563836 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.563704.563704 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:08:53.563283.563283 client.py:127] Model loaded
DEBUG 01-15 16:08:53.563903.563903 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.564078.564078 cuda_h.py:19] end move_flatidxs cost 0.0008533000946044922 seconds
DEBUG 01-15 16:08:53.564875.564875 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.564700.564700 cuda_h.py:19] end restore2model cost 0.001165151596069336 seconds
INFO 01-15 16:08:53.565336.565336 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1a5702e1-dac3-4d8c-ac6c-b66bfde94141
DEBUG 01-15 16:08:53.565831.565831 cuda_h.py:19] end sllm_worker_task cost 0.013376235961914062 seconds
DEBUG 01-15 16:08:53.565727.565727 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0040988922119140625 seconds
DEBUG 01-15 16:08:53.566271.566271 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.569367.569367 cuda_h.py:19] end group_tensors cost 0.005333900451660156 seconds
DEBUG 01-15 16:08:53.570902.570902 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.570625.570625 cuda_h.py:19] end restore2model cost 0.004175662994384766 seconds
DEBUG 01-15 16:08:53.570157.570157 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008709192276000977 seconds
DEBUG 01-15 16:08:53.570609.570609 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.571973.571973 cuda_h.py:19] end gpu_sexperts cost 0.0005192756652832031 seconds
DEBUG 01-15 16:08:53.571022.571022 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.571959.571959 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 3.0279159545898438e-05 seconds
DEBUG 01-15 16:08:53.571821.571821 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.571167.571167 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.573975.573975 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0017180442810058594 seconds
DEBUG 01-15 16:08:53.573999.573999 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.573703.573703 cuda_h.py:19] end gpu_group_list cost 0.0003941059112548828 seconds
DEBUG 01-15 16:08:53.574990.574990 cuda_h.py:19] end group pad cost 0.004148244857788086 seconds
DEBUG 01-15 16:08:53.574687.574687 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:53.575783.575783 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.583308.583308 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.007389068603515625 seconds
DEBUG 01-15 16:08:53.583119.583119 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.583056.583056 cuda_h.py:19] end gpu_group_list cost 0.00042438507080078125 seconds
DEBUG 01-15 16:08:53.585631.585631 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.585953.585953 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1a5702e1-dac3-4d8c-ac6c-b66bfde94141
INFO 01-15 16:08:53.601963.601963 client.py:127] Model loaded
DEBUG 01-15 16:08:53.601354.601354 cuda_h.py:19] end wait_experts_multi_device cost 0.016138553619384766 seconds
DEBUG 01-15 16:08:53.601337.601337 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.602865.602865 cuda_h.py:19] end group_einsum cost 0.028369903564453125 seconds
DEBUG 01-15 16:08:53.603393.603393 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.606093.606093 cuda_h.py:19] end get_outputs_cpu1 cost 0.003222227096557617 seconds
DEBUG 01-15 16:08:53.607792.607792 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04427170753479004 seconds
DEBUG 01-15 16:08:53.607925.607925 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006437540054321289 seconds
DEBUG 01-15 16:08:53.608088.608088 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.609398.609398 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.609675.609675 cuda_h.py:19] end index_scatter cost 7.748603820800781e-05 seconds
DEBUG 01-15 16:08:53.609910.609910 cuda_h.py:19] end cpuoutputsdeal cost 0.0013723373413085938 seconds
DEBUG 01-15 16:08:53.609662.609662 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.609670.609670 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.609180.609180 cuda_h.py:19] end gpu_group_tensor cost 0.00016450881958007812 seconds
DEBUG 01-15 16:08:53.609857.609857 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.610108.610108 cuda_h.py:19] end gpu_group_tensor cost 0.0001518726348876953 seconds
DEBUG 01-15 16:08:53.610040.610040 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.610441.610441 cuda_h.py:19] end gpu_group_einsum cost 0.0006597042083740234 seconds
DEBUG 01-15 16:08:53.611108.611108 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.611403.611403 cuda_h.py:19] end gpu_group_einsum cost 0.0008070468902587891 seconds
DEBUG 01-15 16:08:53.612376.612376 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.612578.612578 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.612275.612275 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018978118896484375 seconds
DEBUG 01-15 16:08:53.612230.612230 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.612942.612942 cuda_h.py:19] end concat_expert_out cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:08:53.612454.612454 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.612583.612583 cuda_h.py:19] end index_scatter cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:08:53.612034.612034 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008177757263183594 seconds
DEBUG 01-15 16:08:53.613210.613210 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.613523.613523 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.613865.613865 cuda_h.py:19] end all_expert_outputs_slices cost 0.00011587142944335938 seconds
DEBUG 01-15 16:08:53.613045.613045 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.613723.613723 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:08:53.613228.613228 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.613960.613960 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:08:53.613908.613908 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004456043243408203 seconds
DEBUG 01-15 16:08:53.613433.613433 cuda_h.py:19] end gpu_experts_multi_device cost 0.04237651824951172 seconds
DEBUG 01-15 16:08:53.613628.613628 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.05480551719665527 seconds
DEBUG 01-15 16:08:53.614905.614905 cuda_h.py:19] end prefill_layer cost 0.06287217140197754 seconds
DEBUG 01-15 16:08:53.614994.614994 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:08:53.614836.614836 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.614724.614724 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:08:53.614566.614566 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:08:53.614600.614600 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:08:53.614297.614297 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 3.409385681152344e-05 seconds
DEBUG 01-15 16:08:53.614431.614431 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.614785.614785 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 0.00017642974853515625 seconds
DEBUG 01-15 16:08:53.614856.614856 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.614686.614686 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.614472.614472 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.614821.614821 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.615244.615244 cuda_h.py:19] end allocate_cuda_memory cost 0.00025582313537597656 seconds
DEBUG 01-15 16:08:53.615256.615256 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.615894.615894 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.615214.615214 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.615884.615884 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2467086a-4bb0-4db5-b804-339569481cd7
DEBUG 01-15 16:08:53.615908.615908 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.616914.616914 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.617523.617523 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2467086a-4bb0-4db5-b804-339569481cd7
DEBUG 01-15 16:08:53.617029.617029 cuda_h.py:19] end load_into_gpu_async cost 0.0016794204711914062 seconds
DEBUG 01-15 16:08:53.617315.617315 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.617531.617531 cuda_h.py:19] end restore_tensors2 cost 9.107589721679688e-05 seconds
DEBUG 01-15 16:08:53.617010.617010 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0026535987854003906 seconds
INFO 01-15 16:08:53.617576.617576 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2467086a-4bb0-4db5-b804-339569481cd7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.619807.619807 cuda_h.py:19] end self_attn cost 0.003224611282348633 seconds
DEBUG 01-15 16:08:53.619334.619334 cuda_h.py:19] end iln_self_attn_paln cost 0.004901409149169922 seconds
DEBUG 01-15 16:08:53.619316.619316 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:08:53.619079.619079 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.620116.620116 cuda_h.py:19] end gate cost 0.0006558895111083984 seconds
DEBUG 01-15 16:08:53.620760.620760 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.620884.620884 lmp.py:1912] 
DEBUG 01-15 16:08:53.620884.620884 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.620309.620309 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.621011.621011 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.621615.621615 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.621551.621551 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.621624.621624 lmp.py:1917] 
DEBUG 01-15 16:08:53.621624.621624 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.621413.621413 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.621176.621176 lmp.py:1935]   Expert 54 |     21 | CPU
DEBUG 01-15 16:08:53.621441.621441 lmp.py:1935]   Expert  3 |     33 | CPU
DEBUG 01-15 16:08:53.621515.621515 lmp.py:1935]   Expert  8 |     42 | CPU
DEBUG 01-15 16:08:53.621111.621111 lmp.py:1935]   Expert 28 |     44 | CPU
DEBUG 01-15 16:08:53.621662.621662 lmp.py:1935]   Expert 63 |     54 | CPU
DEBUG 01-15 16:08:53.621212.621212 lmp.py:1935]   Expert 43 |     55 | CPU
DEBUG 01-15 16:08:53.621908.621908 lmp.py:1935]   Expert 36 |     72 | CPU
DEBUG 01-15 16:08:53.621789.621789 lmp.py:1935]   Expert 38 |     76 | CPU
DEBUG 01-15 16:08:53.621525.621525 lmp.py:1935]   Expert  6 |     78 | CPU
DEBUG 01-15 16:08:53.621261.621261 lmp.py:1935]   Expert 57 |     98 | CPU
DEBUG 01-15 16:08:53.621520.621520 lmp.py:1935]   Expert 39 |    101 | CPU
DEBUG 01-15 16:08:53.621255.621255 lmp.py:1935]   Expert 41 |    104 | CPU
DEBUG 01-15 16:08:53.621752.621752 lmp.py:1935]   Expert 12 |    107 | CPU
DEBUG 01-15 16:08:53.621488.621488 lmp.py:1935]   Expert 52 |    111 | CPU
DEBUG 01-15 16:08:53.621985.621985 lmp.py:1935]   Expert 19 |    119 | CPU
DEBUG 01-15 16:08:53.621721.621721 lmp.py:1935]   Expert 47 |    127 | CPU
DEBUG 01-15 16:08:53.621980.621980 lmp.py:1935]   Expert 13 |    134 | CPU
DEBUG 01-15 16:08:53.621238.621238 lmp.py:1935]   Expert 22 |    140 | CPU
DEBUG 01-15 16:08:53.621736.621736 lmp.py:1935]   Expert 46 |    142 | CPU
DEBUG 01-15 16:08:53.621233.621233 lmp.py:1935]   Expert 50 |    150 | CPU
DEBUG 01-15 16:08:53.621968.621968 lmp.py:1935]   Expert 24 |    162 | CPU
DEBUG 01-15 16:08:53.621704.621704 lmp.py:1935]   Expert 40 |    163 | CPU
DEBUG 01-15 16:08:53.621493.621493 lmp.py:1935]   Expert 20 |    165 | CPU
DEBUG 01-15 16:08:53.621897.621897 lmp.py:1935]   Expert 55 |    167 | CPU
DEBUG 01-15 16:08:53.621971.621971 lmp.py:1935]   Expert 23 |    169 | CPU
DEBUG 01-15 16:08:53.621567.621567 lmp.py:1935]   Expert  2 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621926.621926 lmp.py:1935]   Expert 37 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621330.621330 lmp.py:1935]   Expert 53 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621020.621020 lmp.py:1935]   Expert 49 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621470.621470 lmp.py:1935]   Expert 61 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621683.621683 lmp.py:1935]   Expert 42 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621134.621134 lmp.py:1935]   Expert 21 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621585.621585 lmp.py:1935]   Expert 18 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621036.621036 lmp.py:1935]   Expert 33 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621486.621486 lmp.py:1935]   Expert 32 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621176.621176 lmp.py:1935]   Expert  0 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621103.621103 lmp.py:1935]   Expert  5 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621985.621985 lmp.py:1935]   Expert 30 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621866.621866 lmp.py:1935]   Expert 16 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621986.621986 lmp.py:1935]   Expert  7 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621199.621199 lmp.py:1935]   Expert 14 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621888.621888 lmp.py:1935]   Expert 34 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621339.621339 lmp.py:1935]   Expert 31 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621313.621313 lmp.py:1935]   Expert 60 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621764.621764 lmp.py:1935]   Expert 59 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621738.621738 lmp.py:1935]   Expert 62 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621473.621473 lmp.py:1935]   Expert  9 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.621686.621686 lmp.py:1935]   Expert 17 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621898.621898 lmp.py:1935]   Expert 10 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.621872.621872 lmp.py:1935]   Expert 29 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.622323.622323 lmp.py:1935]   Expert 58 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.622536.622536 lmp.py:1935]   Expert  4 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.622510.622510 lmp.py:1935]   Expert 15 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.622245.622245 lmp.py:1935]   Expert 26 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.622981.622981 lmp.py:1935]   Expert 51 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.622432.622432 lmp.py:1935]   Expert 11 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.622598.622598 lmp.py:1935]   Expert 44 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.622241.622241 lmp.py:1935]   Expert 56 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.622645.622645 lmp.py:1935]   Expert 27 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.622288.622288 lmp.py:1935]   Expert  1 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.622739.622739 lmp.py:1935]   Expert 45 |    364 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.622952.622952 lmp.py:1935]   Expert 25 |    463 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.622687.622687 lmp.py:1935]   Expert 35 |    517 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.622661.622661 lmp.py:1935]   Expert 48 |    645 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.622443.622443 lmp.py:1937] 
DEBUG 01-15 16:08:53.622443.622443 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.622941.622941 lmp.py:1938]   CPU:   2634 tokens
DEBUG 01-15 16:08:53.622153.622153 lmp.py:1942]   cuda:1:   4741 tokens (19 experts)
DEBUG 01-15 16:08:53.622604.622604 lmp.py:1942]   cuda:2:   4913 tokens (20 experts)
DEBUG 01-15 16:08:53.622624.622624 lmp.py:1943]   Total GPU:   9654 tokens
DEBUG 01-15 16:08:53.622883.622883 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.622883.622883 lmp.py:1944] 
DEBUG 01-15 16:08:53.622864.622864 cuda_h.py:19] end experts_map_get cost 0.0016956329345703125 seconds
DEBUG 01-15 16:08:53.622443.622443 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.622914.622914 lmp.py:1953] 
DEBUG 01-15 16:08:53.622914.622914 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.622465.622465 cuda_h.py:19] end cpu_experts_submit cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:08:53.622777.622777 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.622806.622806 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.624484.624484 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.26.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.26.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.26.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.20.mlp.experts.1.gate_proj.weight': 0, 'model.layers.20.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.32.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.32.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.32.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.59.up_proj.weight': 322961408, 'model.layers.20.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.20.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.20.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (23968350208, 5767168, 17301504, 0), (23974117376, 5767168, 23068672, 0), (23962583040, 5767168, 28835840, 0), (23985651712, 5767168, 34603008, 0), (23991418880, 5767168, 40370176, 0), (23979884544, 5767168, 46137344, 0), (24020254720, 5767168, 51904512, 0), (24026021888, 5767168, 57671680, 0), (24014487552, 5767168, 63438848, 0), (24054857728, 5767168, 69206016, 0), (24060624896, 5767168, 74973184, 0), (24049090560, 5767168, 80740352, 0), (24348983296, 5767168, 86507520, 0), (24354750464, 5767168, 92274688, 0), (24343216128, 5767168, 98041856, 0), (24366284800, 5767168, 103809024, 0), (24372051968, 5767168, 109576192, 0), (24360517632, 5767168, 115343360, 0), (24400887808, 5767168, 121110528, 0), (24406654976, 5767168, 126877696, 0), (24395120640, 5767168, 132644864, 0), (24435490816, 5767168, 138412032, 0), (24441257984, 5767168, 144179200, 0), (24429723648, 5767168, 149946368, 0), (24470093824, 5767168, 155713536, 0), (24475860992, 5767168, 161480704, 0), (24464326656, 5767168, 167247872, 0), (24487395328, 5767168, 173015040, 0), (24493162496, 5767168, 178782208, 0), (24481628160, 5767168, 184549376, 0), (24539299840, 5767168, 190316544, 0), (24545067008, 5767168, 196083712, 0), (24533532672, 5767168, 201850880, 0), (24625807360, 5767168, 207618048, 0), (24631574528, 5767168, 213385216, 0), (24620040192, 5767168, 219152384, 0), (24677711872, 5767168, 224919552, 0), (24683479040, 5767168, 230686720, 0), (24671944704, 5767168, 236453888, 0), (24729616384, 5767168, 242221056, 0), (24735383552, 5767168, 247988224, 0), (24723849216, 5767168, 253755392, 0), (24781520896, 5767168, 259522560, 0), (24787288064, 5767168, 265289728, 0), (24775753728, 5767168, 271056896, 0), (24868028416, 5767168, 276824064, 0), (24873795584, 5767168, 282591232, 0), (24862261248, 5767168, 288358400, 0), (24954535936, 5767168, 294125568, 0), (24960303104, 5767168, 299892736, 0), (24948768768, 5767168, 305659904, 0), (24971837440, 5767168, 311427072, 0), (24977604608, 5767168, 317194240, 0), (24966070272, 5767168, 322961408, 0)], 2: [(23916445696, 5767168, 0, 0), (23922212864, 5767168, 5767168, 0), (23910678528, 5767168, 11534336, 0), (23933747200, 5767168, 17301504, 0), (23939514368, 5767168, 23068672, 0), (23927980032, 5767168, 28835840, 0), (24072159232, 5767168, 34603008, 0), (24077926400, 5767168, 40370176, 0), (24066392064, 5767168, 46137344, 0), (24089460736, 5767168, 51904512, 0), (24095227904, 5767168, 57671680, 0), (24083693568, 5767168, 63438848, 0), (24141365248, 5767168, 69206016, 0), (24147132416, 5767168, 74973184, 0), (24135598080, 5767168, 80740352, 0), (24158666752, 5767168, 86507520, 0), (24164433920, 5767168, 92274688, 0), (24152899584, 5767168, 98041856, 0), (24175968256, 5767168, 103809024, 0), (24181735424, 5767168, 109576192, 0), (24170201088, 5767168, 115343360, 0), (24193269760, 5767168, 121110528, 0), (24199036928, 5767168, 126877696, 0), (24187502592, 5767168, 132644864, 0), (24210571264, 5767168, 138412032, 0), (24216338432, 5767168, 144179200, 0), (24204804096, 5767168, 149946368, 0), (24262475776, 5767168, 155713536, 0), (24268242944, 5767168, 161480704, 0), (24256708608, 5767168, 167247872, 0), (24331681792, 5767168, 173015040, 0), (24337448960, 5767168, 178782208, 0), (24325914624, 5767168, 184549376, 0), (24418189312, 5767168, 190316544, 0), (24423956480, 5767168, 196083712, 0), (24412422144, 5767168, 201850880, 0), (24452792320, 5767168, 207618048, 0), (24458559488, 5767168, 213385216, 0), (24447025152, 5767168, 219152384, 0), (24504696832, 5767168, 224919552, 0), (24510464000, 5767168, 230686720, 0), (24498929664, 5767168, 236453888, 0), (24660410368, 5767168, 242221056, 0), (24666177536, 5767168, 247988224, 0), (24654643200, 5767168, 253755392, 0), (24746917888, 5767168, 259522560, 0), (24752685056, 5767168, 265289728, 0), (24741150720, 5767168, 271056896, 0), (24816123904, 5767168, 276824064, 0), (24821891072, 5767168, 282591232, 0), (24810356736, 5767168, 288358400, 0), (24902631424, 5767168, 294125568, 0), (24908398592, 5767168, 299892736, 0), (24896864256, 5767168, 305659904, 0), (24919932928, 5767168, 311427072, 0), (24925700096, 5767168, 317194240, 0), (24914165760, 5767168, 322961408, 0), (24937234432, 5767168, 328728576, 0), (24943001600, 5767168, 334495744, 0), (24931467264, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a688114660>, 2: <capsule object NULL at 0x74a680698660>}
DEBUG 01-15 16:08:53.624625.624625 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.624854.624854 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.625954.625954 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 05b908ea-ec3e-4fb9-b747-a3e065c598b6
DEBUG 01-15 16:08:53.625692.625692 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.625114.625114 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.625043.625043 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:08:53.625817.625817 client.py:127] Model loaded
DEBUG 01-15 16:08:53.625384.625384 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.626372.626372 cuda_h.py:19] end move_flatidxs cost 0.0008556842803955078 seconds
DEBUG 01-15 16:08:53.626963.626963 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.626205.626205 cuda_h.py:19] end restore2model cost 0.000492095947265625 seconds
DEBUG 01-15 16:08:53.626458.626458 cuda_h.py:19] end sllm_worker_task cost 0.011874914169311523 seconds
INFO 01-15 16:08:53.627812.627812 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 05b908ea-ec3e-4fb9-b747-a3e065c598b6
DEBUG 01-15 16:08:53.627155.627155 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005239248275756836 seconds
DEBUG 01-15 16:08:53.627601.627601 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.631425.631425 cuda_h.py:19] end restore2model cost 0.003411531448364258 seconds
DEBUG 01-15 16:08:53.631381.631381 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008921146392822266 seconds
DEBUG 01-15 16:08:53.631131.631131 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.631049.631049 cuda_h.py:19] end gpu_sexperts cost 0.0002918243408203125 seconds
DEBUG 01-15 16:08:53.631408.631408 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.631715.631715 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1457672119140625e-05 seconds
DEBUG 01-15 16:08:53.631126.631126 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.631782.631782 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.632710.632710 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009665489196777344 seconds
DEBUG 01-15 16:08:53.633990.633990 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.633363.633363 cuda_h.py:19] end gpu_group_list cost 0.000209808349609375 seconds
DEBUG 01-15 16:08:53.634469.634469 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.634259.634259 cuda_h.py:19] end group_tensors cost 0.008220911026000977 seconds
DEBUG 01-15 16:08:53.635142.635142 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010325908660888672 seconds
DEBUG 01-15 16:08:53.635772.635772 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.635462.635462 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.635967.635967 cuda_h.py:19] end gpu_group_list cost 0.00023794174194335938 seconds
DEBUG 01-15 16:08:53.638323.638323 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.638365.638365 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 05b908ea-ec3e-4fb9-b747-a3e065c598b6
DEBUG 01-15 16:08:53.644895.644895 cuda_h.py:19] end group pad cost 0.009246349334716797 seconds
DEBUG 01-15 16:08:53.644278.644278 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:53.664352.664352 client.py:127] Model loaded
DEBUG 01-15 16:08:53.664790.664790 cuda_h.py:19] end wait_experts_multi_device cost 0.02628493309020996 seconds
DEBUG 01-15 16:08:53.664440.664440 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.672156.672156 cuda_h.py:19] end group_einsum cost 0.02791428565979004 seconds
DEBUG 01-15 16:08:53.672651.672651 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.675522.675522 cuda_h.py:19] end get_outputs_cpu1 cost 0.002785205841064453 seconds
DEBUG 01-15 16:08:53.678147.678147 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05285525321960449 seconds
DEBUG 01-15 16:08:53.679789.679789 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01447296142578125 seconds
DEBUG 01-15 16:08:53.679560.679560 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.680480.680480 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.680054.680054 cuda_h.py:19] end index_scatter cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:08:53.680102.680102 cuda_h.py:19] end cpuoutputsdeal cost 0.0014810562133789062 seconds
DEBUG 01-15 16:08:53.680310.680310 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.680596.680596 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.680741.680741 cuda_h.py:19] end gpu_group_tensor cost 0.0001461505889892578 seconds
DEBUG 01-15 16:08:53.680834.680834 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.681350.681350 cuda_h.py:19] end gpu_group_tensor cost 0.00012922286987304688 seconds
DEBUG 01-15 16:08:53.681247.681247 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.681363.681363 cuda_h.py:19] end gpu_group_einsum cost 0.0006289482116699219 seconds
DEBUG 01-15 16:08:53.682136.682136 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.682790.682790 cuda_h.py:19] end gpu_group_einsum cost 0.00048804283142089844 seconds
DEBUG 01-15 16:08:53.682735.682735 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.682183.682183 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.683093.683093 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023651123046875 seconds
DEBUG 01-15 16:08:53.683909.683909 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.683085.683085 cuda_h.py:19] end concat_expert_out cost 5.650520324707031e-05 seconds
DEBUG 01-15 16:08:53.683067.683067 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.683890.683890 cuda_h.py:19] end index_scatter cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:08:53.683633.683633 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008544921875 seconds
DEBUG 01-15 16:08:53.683749.683749 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.683492.683492 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.684022.684022 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001461505889892578 seconds
DEBUG 01-15 16:08:53.684301.684301 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.684125.684125 cuda_h.py:19] end concat_expert_out cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:08:53.684676.684676 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.684646.684646 cuda_h.py:19] end index_scatter cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:08:53.684409.684409 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004858970642089844 seconds
DEBUG 01-15 16:08:53.684027.684027 cuda_h.py:19] end gpu_experts_multi_device cost 0.05239725112915039 seconds
DEBUG 01-15 16:08:53.684137.684137 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.06461739540100098 seconds
DEBUG 01-15 16:08:53.684647.684647 cuda_h.py:19] end prefill_layer cost 0.07068991661071777 seconds
DEBUG 01-15 16:08:53.684616.684616 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:08:53.684318.684318 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.684452.684452 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:08:53.685301.685301 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:08:53.685341.685341 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:08:53.685860.685860 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:08:53.685950.685950 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.685523.685523 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 0.00023627281188964844 seconds
DEBUG 01-15 16:08:53.685052.685052 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.685956.685956 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.685755.685755 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.685609.685609 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.686785.686785 cuda_h.py:19] end allocate_cuda_memory cost 0.00040984153747558594 seconds
DEBUG 01-15 16:08:53.686580.686580 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.686171.686171 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.686736.686736 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.686360.686360 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 64b1b36b-abc3-4dce-b3d4-cc4b6faa9e30
DEBUG 01-15 16:08:53.686205.686205 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.687636.687636 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.688802.688802 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 64b1b36b-abc3-4dce-b3d4-cc4b6faa9e30
DEBUG 01-15 16:08:53.688689.688689 cuda_h.py:19] end load_into_gpu_async cost 0.0018215179443359375 seconds
DEBUG 01-15 16:08:53.688643.688643 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.688588.688588 cuda_h.py:19] end restore_tensors2 cost 0.00010395050048828125 seconds
DEBUG 01-15 16:08:53.688967.688967 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.00301361083984375 seconds
INFO 01-15 16:08:53.688420.688420 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 64b1b36b-abc3-4dce-b3d4-cc4b6faa9e30
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.690732.690732 cuda_h.py:19] end self_attn cost 0.003329753875732422 seconds
DEBUG 01-15 16:08:53.691849.691849 cuda_h.py:19] end iln_self_attn_paln cost 0.0052683353424072266 seconds
DEBUG 01-15 16:08:53.691831.691831 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:08:53.691733.691733 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.691510.691510 cuda_h.py:19] end gate cost 0.0006442070007324219 seconds
DEBUG 01-15 16:08:53.691962.691962 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.692165.692165 lmp.py:1912] 
DEBUG 01-15 16:08:53.692165.692165 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.692259.692259 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.692724.692724 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.692373.692373 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.692739.692739 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.692203.692203 lmp.py:1917] 
DEBUG 01-15 16:08:53.692203.692203 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.692038.692038 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.692880.692880 lmp.py:1935]   Expert 44 |     29 | CPU
DEBUG 01-15 16:08:53.692523.692523 lmp.py:1935]   Expert  9 |     35 | CPU
DEBUG 01-15 16:08:53.692451.692451 lmp.py:1935]   Expert 11 |     37 | CPU
DEBUG 01-15 16:08:53.692901.692901 lmp.py:1935]   Expert 56 |     57 | CPU
DEBUG 01-15 16:08:53.692591.692591 lmp.py:1935]   Expert 54 |     81 | CPU
DEBUG 01-15 16:08:53.692565.692565 lmp.py:1935]   Expert  7 |     92 | CPU
DEBUG 01-15 16:08:53.692016.692016 lmp.py:1935]   Expert 62 |     92 | CPU
DEBUG 01-15 16:08:53.692705.692705 lmp.py:1935]   Expert 47 |     93 | CPU
DEBUG 01-15 16:08:53.692110.692110 lmp.py:1935]   Expert 51 |    101 | CPU
DEBUG 01-15 16:08:53.692660.692660 lmp.py:1935]   Expert 60 |    103 | CPU
DEBUG 01-15 16:08:53.692064.692064 lmp.py:1935]   Expert 52 |    107 | CPU
DEBUG 01-15 16:08:53.692707.692707 lmp.py:1935]   Expert 22 |    109 | CPU
DEBUG 01-15 16:08:53.692158.692158 lmp.py:1935]   Expert 41 |    109 | CPU
DEBUG 01-15 16:08:53.692371.692371 lmp.py:1935]   Expert 53 |    113 | CPU
DEBUG 01-15 16:08:53.692345.692345 lmp.py:1935]   Expert  6 |    126 | CPU
DEBUG 01-15 16:08:53.692557.692557 lmp.py:1935]   Expert 48 |    127 | CPU
DEBUG 01-15 16:08:53.692293.692293 lmp.py:1935]   Expert  8 |    128 | CPU
DEBUG 01-15 16:08:53.692029.692029 lmp.py:1935]   Expert  1 |    129 | CPU
DEBUG 01-15 16:08:53.692718.692718 lmp.py:1935]   Expert 32 |    130 | CPU
DEBUG 01-15 16:08:53.692930.692930 lmp.py:1935]   Expert  2 |    131 | CPU
DEBUG 01-15 16:08:53.692666.692666 lmp.py:1935]   Expert 35 |    137 | CPU
DEBUG 01-15 16:08:53.692117.692117 lmp.py:1935]   Expert 27 |    138 | CPU
DEBUG 01-15 16:08:53.692283.692283 lmp.py:1935]   Expert 23 |    141 | CPU
DEBUG 01-15 16:08:53.692449.692449 lmp.py:1935]   Expert 59 |    143 | CPU
DEBUG 01-15 16:08:53.692092.692092 lmp.py:1935]   Expert 39 |    145 | CPU
DEBUG 01-15 16:08:53.692212.692212 lmp.py:1935]   Expert 50 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.692332.692332 lmp.py:1935]   Expert 26 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.692975.692975 lmp.py:1935]   Expert 14 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.692664.692664 lmp.py:1935]   Expert 46 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.692830.692830 lmp.py:1935]   Expert 38 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.692519.692519 lmp.py:1935]   Expert 24 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.692447.692447 lmp.py:1935]   Expert  0 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.692136.692136 lmp.py:1935]   Expert  4 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.692064.692064 lmp.py:1935]   Expert 34 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693753.693753 lmp.py:1935]   Expert 49 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693443.693443 lmp.py:1935]   Expert 40 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693132.693132 lmp.py:1935]   Expert 63 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693821.693821 lmp.py:1935]   Expert  5 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693511.693511 lmp.py:1935]   Expert 19 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693962.693962 lmp.py:1935]   Expert 13 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693889.693889 lmp.py:1935]   Expert 29 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693771.693771 lmp.py:1935]   Expert 43 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693129.693129 lmp.py:1935]   Expert 61 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693487.693487 lmp.py:1935]   Expert 57 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693845.693845 lmp.py:1935]   Expert 31 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693773.693773 lmp.py:1935]   Expert 33 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693462.693462 lmp.py:1935]   Expert 16 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693152.693152 lmp.py:1935]   Expert 37 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693603.693603 lmp.py:1935]   Expert 20 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693530.693530 lmp.py:1935]   Expert  3 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693220.693220 lmp.py:1935]   Expert 15 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693585.693585 lmp.py:1935]   Expert 36 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693512.693512 lmp.py:1935]   Expert 18 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693202.693202 lmp.py:1935]   Expert 12 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693891.693891 lmp.py:1935]   Expert 17 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693819.693819 lmp.py:1935]   Expert 28 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693938.693938 lmp.py:1935]   Expert 55 |    312 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693019.693019 lmp.py:1935]   Expert 30 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693854.693854 lmp.py:1935]   Expert 25 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693212.693212 lmp.py:1935]   Expert 58 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693140.693140 lmp.py:1935]   Expert 10 |    362 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693829.693829 lmp.py:1935]   Expert 45 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693141.693141 lmp.py:1935]   Expert 21 |    391 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.693453.693453 lmp.py:1935]   Expert 42 |    644 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.693334.693334 lmp.py:1937] 
DEBUG 01-15 16:08:53.693334.693334 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.693408.693408 lmp.py:1938]   CPU:   2633 tokens
DEBUG 01-15 16:08:53.693481.693481 lmp.py:1942]   cuda:1:   4828 tokens (19 experts)
DEBUG 01-15 16:08:53.693793.693793 lmp.py:1942]   cuda:2:   4827 tokens (20 experts)
DEBUG 01-15 16:08:53.693343.693343 lmp.py:1943]   Total GPU:   9655 tokens
DEBUG 01-15 16:08:53.693384.693384 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.693384.693384 lmp.py:1944] 
DEBUG 01-15 16:08:53.693180.693180 cuda_h.py:19] end experts_map_get cost 0.0017113685607910156 seconds
DEBUG 01-15 16:08:53.693891.693891 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.693309.693309 lmp.py:1953] 
DEBUG 01-15 16:08:53.693309.693309 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.693575.693575 cuda_h.py:19] end cpu_experts_submit cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:08:53.693126.693126 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.693823.693823 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.695708.695708 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.4.gate_proj.weight': 0, 'model.layers.21.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.15.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.15.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.15.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.21.mlp.experts.0.gate_proj.weight': 0, 'model.layers.21.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.38.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.38.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.38.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.40.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.40.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.40.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.43.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.43.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.43.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.45.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.45.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.45.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.21.mlp.experts.58.gate_proj.weight': 328728576, 'model.layers.21.mlp.experts.58.down_proj.weight': 334495744, 'model.layers.21.mlp.experts.58.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(25075646464, 5767168, 0, 0), (25081413632, 5767168, 5767168, 0), (25069879296, 5767168, 11534336, 0), (25179455488, 5767168, 17301504, 0), (25185222656, 5767168, 23068672, 0), (25173688320, 5767168, 28835840, 0), (25265963008, 5767168, 34603008, 0), (25271730176, 5767168, 40370176, 0), (25260195840, 5767168, 46137344, 0), (25283264512, 5767168, 51904512, 0), (25289031680, 5767168, 57671680, 0), (25277497344, 5767168, 63438848, 0), (25300566016, 5767168, 69206016, 0), (25306333184, 5767168, 74973184, 0), (25294798848, 5767168, 80740352, 0), (25317867520, 5767168, 86507520, 0), (25323634688, 5767168, 92274688, 0), (25312100352, 5767168, 98041856, 0), (25335169024, 5767168, 103809024, 0), (25340936192, 5767168, 109576192, 0), (25329401856, 5767168, 115343360, 0), (25352470528, 5767168, 121110528, 0), (25358237696, 5767168, 126877696, 0), (25346703360, 5767168, 132644864, 0), (25421676544, 5767168, 138412032, 0), (25427443712, 5767168, 144179200, 0), (25415909376, 5767168, 149946368, 0), (25438978048, 5767168, 155713536, 0), (25444745216, 5767168, 161480704, 0), (25433210880, 5767168, 167247872, 0), (25508184064, 5767168, 173015040, 0), (25513951232, 5767168, 178782208, 0), (25502416896, 5767168, 184549376, 0), (25577390080, 5767168, 190316544, 0), (25583157248, 5767168, 196083712, 0), (25571622912, 5767168, 201850880, 0), (25733103616, 5767168, 207618048, 0), (25738870784, 5767168, 213385216, 0), (25727336448, 5767168, 219152384, 0), (25802309632, 5767168, 224919552, 0), (25808076800, 5767168, 230686720, 0), (25796542464, 5767168, 236453888, 0), (25854214144, 5767168, 242221056, 0), (25859981312, 5767168, 247988224, 0), (25848446976, 5767168, 253755392, 0), (25871515648, 5767168, 259522560, 0), (25877282816, 5767168, 265289728, 0), (25865748480, 5767168, 271056896, 0), (25958023168, 5767168, 276824064, 0), (25963790336, 5767168, 282591232, 0), (25952256000, 5767168, 288358400, 0), (26061832192, 5767168, 294125568, 0), (26067599360, 5767168, 299892736, 0), (26056065024, 5767168, 305659904, 0), (26096435200, 5767168, 311427072, 0), (26102202368, 5767168, 317194240, 0), (26090668032, 5767168, 322961408, 0)], 2: [(25006440448, 5767168, 0, 0), (25012207616, 5767168, 5767168, 0), (25000673280, 5767168, 11534336, 0), (25058344960, 5767168, 17301504, 0), (25064112128, 5767168, 23068672, 0), (25052577792, 5767168, 28835840, 0), (25092947968, 5767168, 34603008, 0), (25098715136, 5767168, 40370176, 0), (25087180800, 5767168, 46137344, 0), (25214058496, 5767168, 51904512, 0), (25219825664, 5767168, 57671680, 0), (25208291328, 5767168, 63438848, 0), (25231360000, 5767168, 69206016, 0), (25237127168, 5767168, 74973184, 0), (25225592832, 5767168, 80740352, 0), (25248661504, 5767168, 86507520, 0), (25254428672, 5767168, 92274688, 0), (25242894336, 5767168, 98041856, 0), (25369772032, 5767168, 103809024, 0), (25375539200, 5767168, 109576192, 0), (25364004864, 5767168, 115343360, 0), (25456279552, 5767168, 121110528, 0), (25462046720, 5767168, 126877696, 0), (25450512384, 5767168, 132644864, 0), (25490882560, 5767168, 138412032, 0), (25496649728, 5767168, 144179200, 0), (25485115392, 5767168, 149946368, 0), (25525485568, 5767168, 155713536, 0), (25531252736, 5767168, 161480704, 0), (25519718400, 5767168, 167247872, 0), (25542787072, 5767168, 173015040, 0), (25548554240, 5767168, 178782208, 0), (25537019904, 5767168, 184549376, 0), (25594691584, 5767168, 190316544, 0), (25600458752, 5767168, 196083712, 0), (25588924416, 5767168, 201850880, 0), (25629294592, 5767168, 207618048, 0), (25635061760, 5767168, 213385216, 0), (25623527424, 5767168, 219152384, 0), (25646596096, 5767168, 224919552, 0), (25652363264, 5767168, 230686720, 0), (25640828928, 5767168, 236453888, 0), (25663897600, 5767168, 242221056, 0), (25669664768, 5767168, 247988224, 0), (25658130432, 5767168, 253755392, 0), (25698500608, 5767168, 259522560, 0), (25704267776, 5767168, 265289728, 0), (25692733440, 5767168, 271056896, 0), (25750405120, 5767168, 276824064, 0), (25756172288, 5767168, 282591232, 0), (25744637952, 5767168, 288358400, 0), (25785008128, 5767168, 294125568, 0), (25790775296, 5767168, 299892736, 0), (25779240960, 5767168, 305659904, 0), (25992626176, 5767168, 311427072, 0), (25998393344, 5767168, 317194240, 0), (25986859008, 5767168, 322961408, 0), (26009927680, 5767168, 328728576, 0), (26015694848, 5767168, 334495744, 0), (26004160512, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a688544660>, 2: <capsule object NULL at 0x74a660729ce0>}
DEBUG 01-15 16:08:53.695853.695853 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.695180.695180 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.695797.695797 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 20873443-b26c-46a0-b3a2-90d9381e4489
DEBUG 01-15 16:08:53.696195.696195 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:53.696167.696167 client.py:127] Model loaded
DEBUG 01-15 16:08:53.696898.696898 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.697709.697709 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.697874.697874 cuda_h.py:19] end restore2model cost 0.0004298686981201172 seconds
DEBUG 01-15 16:08:53.697511.697511 cuda_h.py:19] end sllm_worker_task cost 0.011780500411987305 seconds
DEBUG 01-15 16:08:53.697363.697363 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:08:53.697950.697950 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 20873443-b26c-46a0-b3a2-90d9381e4489
DEBUG 01-15 16:08:53.698736.698736 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004075288772583008 seconds
DEBUG 01-15 16:08:53.698036.698036 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.698473.698473 cuda_h.py:19] end move_flatidxs cost 0.0008592605590820312 seconds
DEBUG 01-15 16:08:53.698064.698064 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.701752.701752 cuda_h.py:19] end restore2model cost 0.003124713897705078 seconds
DEBUG 01-15 16:08:53.701979.701979 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00745391845703125 seconds
DEBUG 01-15 16:08:53.701013.701013 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.701494.701494 cuda_h.py:19] end gpu_sexperts cost 0.00028586387634277344 seconds
DEBUG 01-15 16:08:53.701085.701085 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.701822.701822 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.288818359375e-05 seconds
DEBUG 01-15 16:08:53.701518.701518 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.701697.701697 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.702485.702485 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009343624114990234 seconds
DEBUG 01-15 16:08:53.702566.702566 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.703594.703594 cuda_h.py:19] end gpu_group_list cost 0.000202178955078125 seconds
DEBUG 01-15 16:08:53.703641.703641 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.704121.704121 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010328292846679688 seconds
DEBUG 01-15 16:08:53.705726.705726 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.705866.705866 cuda_h.py:19] end gpu_group_list cost 0.00021529197692871094 seconds
DEBUG 01-15 16:08:53.705187.705187 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.706116.706116 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 20873443-b26c-46a0-b3a2-90d9381e4489
DEBUG 01-15 16:08:53.706648.706648 cuda_h.py:19] end group_tensors cost 0.008290529251098633 seconds
DEBUG 01-15 16:08:53.707605.707605 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.710631.710631 cuda_h.py:19] end group pad cost 0.0032987594604492188 seconds
DEBUG 01-15 16:08:53.710997.710997 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:53.735648.735648 client.py:127] Model loaded
DEBUG 01-15 16:08:53.736644.736644 cuda_h.py:19] end wait_experts_multi_device cost 0.02991795539855957 seconds
DEBUG 01-15 16:08:53.736827.736827 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.738104.738104 cuda_h.py:19] end group_einsum cost 0.027414560317993164 seconds
DEBUG 01-15 16:08:53.738937.738937 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.741015.741015 cuda_h.py:19] end get_outputs_cpu1 cost 0.0026602745056152344 seconds
DEBUG 01-15 16:08:53.741979.741979 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04478049278259277 seconds
DEBUG 01-15 16:08:53.742163.742163 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006560564041137695 seconds
DEBUG 01-15 16:08:53.743349.743349 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.745039.745039 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.745699.745699 cuda_h.py:19] end index_scatter cost 0.0001537799835205078 seconds
DEBUG 01-15 16:08:53.746995.746995 cuda_h.py:19] end cpuoutputsdeal cost 0.0029726028442382812 seconds
DEBUG 01-15 16:08:53.746411.746411 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.746341.746341 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.746963.746963 cuda_h.py:19] end gpu_group_tensor cost 0.0003044605255126953 seconds
DEBUG 01-15 16:08:53.746324.746324 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.747104.747104 cuda_h.py:19] end gpu_group_tensor cost 0.0002853870391845703 seconds
DEBUG 01-15 16:08:53.747933.747933 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.748767.748767 cuda_h.py:19] end gpu_group_einsum cost 0.0010488033294677734 seconds
DEBUG 01-15 16:08:53.748848.748848 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.749376.749376 cuda_h.py:19] end gpu_group_einsum cost 0.0004603862762451172 seconds
DEBUG 01-15 16:08:53.749930.749930 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.749993.749993 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.749180.749180 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001976490020751953 seconds
DEBUG 01-15 16:08:53.749850.749850 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.750430.750430 cuda_h.py:19] end concat_expert_out cost 7.414817810058594e-05 seconds
DEBUG 01-15 16:08:53.750849.750849 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.750124.750124 cuda_h.py:19] end index_scatter cost 6.628036499023438e-05 seconds
DEBUG 01-15 16:08:53.750768.750768 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000843048095703125 seconds
DEBUG 01-15 16:08:53.750082.750082 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.750441.750441 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.750923.750923 cuda_h.py:19] end all_expert_outputs_slices cost 0.00011682510375976562 seconds
DEBUG 01-15 16:08:53.750487.750487 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.750258.750258 cuda_h.py:19] end concat_expert_out cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:08:53.750856.750856 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.750203.750203 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:08:53.750058.750058 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00043892860412597656 seconds
DEBUG 01-15 16:08:53.751438.751438 cuda_h.py:19] end gpu_experts_multi_device cost 0.049216270446777344 seconds
DEBUG 01-15 16:08:53.751294.751294 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.05989837646484375 seconds
DEBUG 01-15 16:08:53.751685.751685 cuda_h.py:19] end prefill_layer cost 0.06649518013000488 seconds
DEBUG 01-15 16:08:53.751561.751561 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:08:53.751880.751880 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.751198.751198 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:08:53.751279.751279 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:08:53.751505.751505 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:08:53.751348.751348 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 3.600120544433594e-05 seconds
DEBUG 01-15 16:08:53.751382.751382 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:08:53.751978.751978 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.751411.751411 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.751462.751462 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.752916.752916 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.752141.752141 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.753523.753523 cuda_h.py:19] end allocate_cuda_memory cost 0.0005567073822021484 seconds
DEBUG 01-15 16:08:53.753648.753648 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.753473.753473 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.753492.753492 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.753820.753820 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7513678d-33c3-4fd8-9683-c7e78b9aff07
DEBUG 01-15 16:08:53.753763.753763 cuda_h.py:10] start self_attn
DEBUG 01-15 16:08:53.754408.754408 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:53.755217.755217 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7513678d-33c3-4fd8-9683-c7e78b9aff07
DEBUG 01-15 16:08:53.755601.755601 cuda_h.py:19] end load_into_gpu_async cost 0.0024063587188720703 seconds
DEBUG 01-15 16:08:53.755882.755882 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.756957.756957 cuda_h.py:19] end restore_tensors2 cost 0.00017023086547851562 seconds
DEBUG 01-15 16:08:53.756961.756961 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003976345062255859 seconds
INFO 01-15 16:08:53.756843.756843 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7513678d-33c3-4fd8-9683-c7e78b9aff07
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.757995.757995 cuda_h.py:19] end self_attn cost 0.0037446022033691406 seconds
DEBUG 01-15 16:08:53.758623.758623 cuda_h.py:19] end iln_self_attn_paln cost 0.006551265716552734 seconds
DEBUG 01-15 16:08:53.758889.758889 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:08:53.758268.758268 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.759741.759741 cuda_h.py:19] end gate cost 0.0006287097930908203 seconds
DEBUG 01-15 16:08:53.759762.759762 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.759528.759528 lmp.py:1912] 
DEBUG 01-15 16:08:53.759528.759528 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.759575.759575 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.759801.759801 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.759451.759451 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.759240.759240 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.759850.759850 lmp.py:1917] 
DEBUG 01-15 16:08:53.759850.759850 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.759116.759116 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.759342.759342 lmp.py:1935]   Expert 25 |     14 | CPU
DEBUG 01-15 16:08:53.759130.759130 lmp.py:1935]   Expert 48 |     31 | CPU
DEBUG 01-15 16:08:53.759442.759442 lmp.py:1935]   Expert 45 |     35 | CPU
DEBUG 01-15 16:08:53.759615.759615 lmp.py:1935]   Expert  9 |     67 | CPU
DEBUG 01-15 16:08:53.759927.759927 lmp.py:1935]   Expert  0 |     83 | CPU
DEBUG 01-15 16:08:53.759524.759524 lmp.py:1935]   Expert 20 |     84 | CPU
DEBUG 01-15 16:08:53.759836.759836 lmp.py:1935]   Expert 54 |     84 | CPU
DEBUG 01-15 16:08:53.759671.759671 lmp.py:1935]   Expert 43 |     88 | CPU
DEBUG 01-15 16:08:53.759506.759506 lmp.py:1935]   Expert 57 |     89 | CPU
DEBUG 01-15 16:08:53.759103.759103 lmp.py:1935]   Expert 36 |     92 | CPU
DEBUG 01-15 16:08:53.759461.759461 lmp.py:1935]   Expert  6 |     94 | CPU
DEBUG 01-15 16:08:53.759296.759296 lmp.py:1935]   Expert 47 |     97 | CPU
DEBUG 01-15 16:08:53.759085.759085 lmp.py:1935]   Expert 15 |    102 | CPU
DEBUG 01-15 16:08:53.759396.759396 lmp.py:1935]   Expert 62 |    104 | CPU
DEBUG 01-15 16:08:53.759993.759993 lmp.py:1935]   Expert 13 |    106 | CPU
DEBUG 01-15 16:08:53.759828.759828 lmp.py:1935]   Expert 61 |    106 | CPU
DEBUG 01-15 16:08:53.759186.759186 lmp.py:1935]   Expert  1 |    107 | CPU
DEBUG 01-15 16:08:53.759306.759306 lmp.py:1935]   Expert 50 |    108 | CPU
DEBUG 01-15 16:08:53.759664.759664 lmp.py:1935]   Expert 38 |    111 | CPU
DEBUG 01-15 16:08:53.759261.759261 lmp.py:1935]   Expert 37 |    114 | CPU
DEBUG 01-15 16:08:53.760619.760619 lmp.py:1935]   Expert 46 |    118 | CPU
DEBUG 01-15 16:08:53.760739.760739 lmp.py:1935]   Expert 14 |    121 | CPU
DEBUG 01-15 16:08:53.760574.760574 lmp.py:1935]   Expert 21 |    135 | CPU
DEBUG 01-15 16:08:53.760932.760932 lmp.py:1935]   Expert 28 |    138 | CPU
DEBUG 01-15 16:08:53.760291.760291 lmp.py:1935]   Expert  7 |    139 | CPU
DEBUG 01-15 16:08:53.760748.760748 lmp.py:1935]   Expert 52 |    142 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760537.760537 lmp.py:1935]   Expert 44 |    145 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760326.760326 lmp.py:1935]   Expert 24 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760876.760876 lmp.py:1935]   Expert 10 |    153 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760950.760950 lmp.py:1935]   Expert 42 |    153 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760023.760023 lmp.py:1935]   Expert 11 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760573.760573 lmp.py:1935]   Expert  2 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760885.760885 lmp.py:1935]   Expert 35 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760151.760151 lmp.py:1935]   Expert 26 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760986.760986 lmp.py:1935]   Expert 31 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760060.760060 lmp.py:1935]   Expert  3 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760610.760610 lmp.py:1935]   Expert 19 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760922.760922 lmp.py:1935]   Expert 32 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760757.760757 lmp.py:1935]   Expert 12 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760307.760307 lmp.py:1935]   Expert 56 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760725.760725 lmp.py:1935]   Expert 60 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760726.760726 lmp.py:1935]   Expert 40 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760959.760959 lmp.py:1935]   Expert 41 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760841.760841 lmp.py:1935]   Expert 53 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760530.760530 lmp.py:1935]   Expert 23 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760458.760458 lmp.py:1935]   Expert 16 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760147.760147 lmp.py:1935]   Expert 58 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760313.760313 lmp.py:1935]   Expert  8 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760526.760526 lmp.py:1935]   Expert 51 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760453.760453 lmp.py:1935]   Expert 59 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760812.760812 lmp.py:1935]   Expert  4 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760216.760216 lmp.py:1935]   Expert 55 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760144.760144 lmp.py:1935]   Expert 49 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760072.760072 lmp.py:1935]   Expert 29 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760761.760761 lmp.py:1935]   Expert 34 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760927.760927 lmp.py:1935]   Expert 18 |    284 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760616.760616 lmp.py:1935]   Expert 63 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760544.760544 lmp.py:1935]   Expert 27 |    353 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760710.760710 lmp.py:1935]   Expert 39 |    381 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760876.760876 lmp.py:1935]   Expert 17 |    394 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760758.760758 lmp.py:1935]   Expert 22 |    433 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760685.760685 lmp.py:1935]   Expert 33 |    457 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760613.760613 lmp.py:1935]   Expert 30 |    460 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.760064.760064 lmp.py:1935]   Expert  5 |    711 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.760038.760038 lmp.py:1937] 
DEBUG 01-15 16:08:53.760038.760038 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.760204.760204 lmp.py:1938]   CPU:   2367 tokens
DEBUG 01-15 16:08:53.760847.760847 lmp.py:1942]   cuda:1:   4928 tokens (19 experts)
DEBUG 01-15 16:08:53.760536.760536 lmp.py:1942]   cuda:2:   4993 tokens (20 experts)
DEBUG 01-15 16:08:53.760511.760511 lmp.py:1943]   Total GPU:   9921 tokens
DEBUG 01-15 16:08:53.760485.760485 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.760485.760485 lmp.py:1944] 
DEBUG 01-15 16:08:53.760611.760611 cuda_h.py:19] end experts_map_get cost 0.001790761947631836 seconds
DEBUG 01-15 16:08:53.761792.761792 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.761641.761641 lmp.py:1953] 
DEBUG 01-15 16:08:53.761641.761641 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.761815.761815 cuda_h.py:19] end cpu_experts_submit cost 5.698204040527344e-05 seconds
DEBUG 01-15 16:08:53.761034.761034 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.761308.761308 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.762620.762620 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.4.gate_proj.weight': 0, 'model.layers.22.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.34.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.34.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.34.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.22.mlp.experts.2.gate_proj.weight': 0, 'model.layers.22.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.32.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.32.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.32.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.33.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.33.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.33.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.40.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.40.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.40.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.59.up_proj.weight': 322961408, 'model.layers.22.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.22.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.22.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(26182942720, 5767168, 0, 0), (26188709888, 5767168, 5767168, 0), (26177175552, 5767168, 11534336, 0), (26200244224, 5767168, 17301504, 0), (26206011392, 5767168, 23068672, 0), (26194477056, 5767168, 28835840, 0), (26304053248, 5767168, 34603008, 0), (26309820416, 5767168, 40370176, 0), (26298286080, 5767168, 46137344, 0), (26321354752, 5767168, 51904512, 0), (26327121920, 5767168, 57671680, 0), (26315587584, 5767168, 63438848, 0), (26442465280, 5767168, 69206016, 0), (26448232448, 5767168, 74973184, 0), (26436698112, 5767168, 80740352, 0), (26494369792, 5767168, 86507520, 0), (26500136960, 5767168, 92274688, 0), (26488602624, 5767168, 98041856, 0), (26511671296, 5767168, 103809024, 0), (26517438464, 5767168, 109576192, 0), (26505904128, 5767168, 115343360, 0), (26650083328, 5767168, 121110528, 0), (26655850496, 5767168, 126877696, 0), (26644316160, 5767168, 132644864, 0), (26701987840, 5767168, 138412032, 0), (26707755008, 5767168, 144179200, 0), (26696220672, 5767168, 149946368, 0), (26719289344, 5767168, 155713536, 0), (26725056512, 5767168, 161480704, 0), (26713522176, 5767168, 167247872, 0), (26788495360, 5767168, 173015040, 0), (26794262528, 5767168, 178782208, 0), (26782728192, 5767168, 184549376, 0), (26823098368, 5767168, 190316544, 0), (26828865536, 5767168, 196083712, 0), (26817331200, 5767168, 201850880, 0), (26840399872, 5767168, 207618048, 0), (26846167040, 5767168, 213385216, 0), (26834632704, 5767168, 219152384, 0), (26875002880, 5767168, 224919552, 0), (26880770048, 5767168, 230686720, 0), (26869235712, 5767168, 236453888, 0), (26961510400, 5767168, 242221056, 0), (26967277568, 5767168, 247988224, 0), (26955743232, 5767168, 253755392, 0), (26996113408, 5767168, 259522560, 0), (27001880576, 5767168, 265289728, 0), (26990346240, 5767168, 271056896, 0), (27082620928, 5767168, 276824064, 0), (27088388096, 5767168, 282591232, 0), (27076853760, 5767168, 288358400, 0), (27117223936, 5767168, 294125568, 0), (27122991104, 5767168, 299892736, 0), (27111456768, 5767168, 305659904, 0), (27203731456, 5767168, 311427072, 0), (27209498624, 5767168, 317194240, 0), (27197964288, 5767168, 322961408, 0)], 2: [(26148339712, 5767168, 0, 0), (26154106880, 5767168, 5767168, 0), (26142572544, 5767168, 11534336, 0), (26165641216, 5767168, 17301504, 0), (26171408384, 5767168, 23068672, 0), (26159874048, 5767168, 28835840, 0), (26252148736, 5767168, 34603008, 0), (26257915904, 5767168, 40370176, 0), (26246381568, 5767168, 46137344, 0), (26286751744, 5767168, 51904512, 0), (26292518912, 5767168, 57671680, 0), (26280984576, 5767168, 63438848, 0), (26390560768, 5767168, 69206016, 0), (26396327936, 5767168, 74973184, 0), (26384793600, 5767168, 80740352, 0), (26407862272, 5767168, 86507520, 0), (26413629440, 5767168, 92274688, 0), (26402095104, 5767168, 98041856, 0), (26425163776, 5767168, 103809024, 0), (26430930944, 5767168, 109576192, 0), (26419396608, 5767168, 115343360, 0), (26528972800, 5767168, 121110528, 0), (26534739968, 5767168, 126877696, 0), (26523205632, 5767168, 132644864, 0), (26563575808, 5767168, 138412032, 0), (26569342976, 5767168, 144179200, 0), (26557808640, 5767168, 149946368, 0), (26580877312, 5767168, 155713536, 0), (26586644480, 5767168, 161480704, 0), (26575110144, 5767168, 167247872, 0), (26615480320, 5767168, 173015040, 0), (26621247488, 5767168, 178782208, 0), (26609713152, 5767168, 184549376, 0), (26632781824, 5767168, 190316544, 0), (26638548992, 5767168, 196083712, 0), (26627014656, 5767168, 201850880, 0), (26667384832, 5767168, 207618048, 0), (26673152000, 5767168, 213385216, 0), (26661617664, 5767168, 219152384, 0), (26684686336, 5767168, 224919552, 0), (26690453504, 5767168, 230686720, 0), (26678919168, 5767168, 236453888, 0), (26805796864, 5767168, 242221056, 0), (26811564032, 5767168, 247988224, 0), (26800029696, 5767168, 253755392, 0), (27013414912, 5767168, 259522560, 0), (27019182080, 5767168, 265289728, 0), (27007647744, 5767168, 271056896, 0), (27030716416, 5767168, 276824064, 0), (27036483584, 5767168, 282591232, 0), (27024949248, 5767168, 288358400, 0), (27065319424, 5767168, 294125568, 0), (27071086592, 5767168, 299892736, 0), (27059552256, 5767168, 305659904, 0), (27134525440, 5767168, 311427072, 0), (27140292608, 5767168, 317194240, 0), (27128758272, 5767168, 322961408, 0), (27151826944, 5767168, 328728576, 0), (27157594112, 5767168, 334495744, 0), (27146059776, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a688488660>, 2: <capsule object NULL at 0x74a660729950>}
DEBUG 01-15 16:08:53.763065.763065 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.763160.763160 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.763539.763539 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9fd5d591-486d-4bcd-ae80-210eaa7d024e
DEBUG 01-15 16:08:53.763122.763122 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.763830.763830 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:08:53.764954.764954 client.py:127] Model loaded
DEBUG 01-15 16:08:53.764978.764978 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.764928.764928 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:53.765794.765794 cuda_h.py:19] end move_flatidxs cost 0.000881195068359375 seconds
DEBUG 01-15 16:08:53.765260.765260 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.765848.765848 cuda_h.py:19] end restore2model cost 0.0009810924530029297 seconds
INFO 01-15 16:08:53.765304.765304 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9fd5d591-486d-4bcd-ae80-210eaa7d024e
DEBUG 01-15 16:08:53.765043.765043 cuda_h.py:19] end sllm_worker_task cost 0.013323545455932617 seconds
DEBUG 01-15 16:08:53.766154.766154 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0048618316650390625 seconds
DEBUG 01-15 16:08:53.766618.766618 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.769762.769762 cuda_h.py:19] end restore2model cost 0.003085613250732422 seconds
DEBUG 01-15 16:08:53.769274.769274 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00833439826965332 seconds
DEBUG 01-15 16:08:53.769831.769831 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.769669.769669 cuda_h.py:19] end gpu_sexperts cost 0.0002696514129638672 seconds
DEBUG 01-15 16:08:53.769452.769452 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.769805.769805 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.193450927734375e-05 seconds
DEBUG 01-15 16:08:53.769501.769501 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.769158.769158 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.770461.770461 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009300708770751953 seconds
DEBUG 01-15 16:08:53.770112.770112 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.771756.771756 cuda_h.py:19] end gpu_group_list cost 0.00019979476928710938 seconds
DEBUG 01-15 16:08:53.772511.772511 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.771333.771333 cuda_h.py:19] end group_tensors cost 0.006075859069824219 seconds
DEBUG 01-15 16:08:53.772600.772600 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.773251.773251 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010831356048583984 seconds
DEBUG 01-15 16:08:53.773770.773770 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.773302.773302 cuda_h.py:19] end gpu_group_list cost 0.00022172927856445312 seconds
DEBUG 01-15 16:08:53.774684.774684 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.774064.774064 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9fd5d591-486d-4bcd-ae80-210eaa7d024e
DEBUG 01-15 16:08:53.776951.776951 cuda_h.py:19] end group pad cost 0.004466056823730469 seconds
DEBUG 01-15 16:08:53.776682.776682 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:53.802172.802172 client.py:127] Model loaded
DEBUG 01-15 16:08:53.802071.802071 cuda_h.py:19] end wait_experts_multi_device cost 0.028292417526245117 seconds
DEBUG 01-15 16:08:53.802364.802364 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.803922.803922 cuda_h.py:19] end group_einsum cost 0.026429414749145508 seconds
DEBUG 01-15 16:08:53.803344.803344 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.806444.806444 cuda_h.py:19] end get_outputs_cpu1 cost 0.002747774124145508 seconds
DEBUG 01-15 16:08:53.807705.807705 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04325222969055176 seconds
DEBUG 01-15 16:08:53.807898.807898 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005211830139160156 seconds
DEBUG 01-15 16:08:53.808955.808955 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.809700.809700 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.809937.809937 cuda_h.py:19] end index_scatter cost 8.940696716308594e-05 seconds
DEBUG 01-15 16:08:53.809710.809710 cuda_h.py:19] end cpuoutputsdeal cost 0.001528024673461914 seconds
DEBUG 01-15 16:08:53.809700.809700 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.809708.809708 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.809118.809118 cuda_h.py:19] end gpu_group_tensor cost 0.0001609325408935547 seconds
DEBUG 01-15 16:08:53.809603.809603 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.810139.810139 cuda_h.py:19] end gpu_group_tensor cost 0.00014925003051757812 seconds
DEBUG 01-15 16:08:53.810071.810071 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.811387.811387 cuda_h.py:19] end gpu_group_einsum cost 0.0010619163513183594 seconds
DEBUG 01-15 16:08:53.811733.811733 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.812394.812394 cuda_h.py:19] end gpu_group_einsum cost 0.0004892349243164062 seconds
DEBUG 01-15 16:08:53.812180.812180 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.812032.812032 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.812209.812209 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002970695495605469 seconds
DEBUG 01-15 16:08:53.812588.812588 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.812115.812115 cuda_h.py:19] end concat_expert_out cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:08:53.813071.813071 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.813472.813472 cuda_h.py:19] end index_scatter cost 8.320808410644531e-05 seconds
DEBUG 01-15 16:08:53.813389.813389 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010344982147216797 seconds
DEBUG 01-15 16:08:53.813293.813293 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.813666.813666 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.813410.813410 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023412704467773438 seconds
DEBUG 01-15 16:08:53.813166.813166 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.814011.814011 cuda_h.py:19] end concat_expert_out cost 6.222724914550781e-05 seconds
DEBUG 01-15 16:08:53.814145.814145 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.814758.814758 cuda_h.py:19] end index_scatter cost 6.532669067382812e-05 seconds
DEBUG 01-15 16:08:53.814997.814997 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006215572357177734 seconds
DEBUG 01-15 16:08:53.814835.814835 cuda_h.py:19] end gpu_experts_multi_device cost 0.04435920715332031 seconds
DEBUG 01-15 16:08:53.814758.814758 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.05598258972167969 seconds
DEBUG 01-15 16:08:53.814896.814896 cuda_h.py:19] end prefill_layer cost 0.06335973739624023 seconds
DEBUG 01-15 16:08:53.815540.815540 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:08:53.815866.815866 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.815860.815860 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:08:53.815901.815901 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:08:53.815803.815803 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:08:53.815812.815812 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 4.601478576660156e-05 seconds
DEBUG 01-15 16:08:53.815952.815952 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 8.177757263183594e-05 seconds
DEBUG 01-15 16:08:53.815986.815986 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.815533.815533 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.815205.815205 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.815646.815646 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.816504.816504 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.816218.816218 cuda_h.py:19] end allocate_cuda_memory cost 0.0006155967712402344 seconds
DEBUG 01-15 16:08:53.817496.817496 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.817633.817633 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.817664.817664 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.817131.817131 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e79fc8a9-50fd-4418-9754-25f2eede7146
DEBUG 01-15 16:08:53.817072.817072 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.817402.817402 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.819136.819136 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e79fc8a9-50fd-4418-9754-25f2eede7146
DEBUG 01-15 16:08:53.819896.819896 cuda_h.py:19] end load_into_gpu_async cost 0.0027189254760742188 seconds
DEBUG 01-15 16:08:53.819674.819674 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.820152.820152 cuda_h.py:19] end restore_tensors2 cost 0.00016808509826660156 seconds
DEBUG 01-15 16:08:53.820302.820302 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004297733306884766 seconds
INFO 01-15 16:08:53.824216.824216 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e79fc8a9-50fd-4418-9754-25f2eede7146
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.826763.826763 cuda_h.py:19] end self_attn cost 0.008667230606079102 seconds
DEBUG 01-15 16:08:53.827738.827738 cuda_h.py:19] end iln_self_attn_paln cost 0.011927366256713867 seconds
DEBUG 01-15 16:08:53.827197.827197 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:08:53.827105.827105 cuda_h.py:10] start gate
INFO 01-15 16:08:53.827011.827011 client.py:127] Model loaded
DEBUG 01-15 16:08:53.827049.827049 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.828904.828904 cuda_h.py:19] end restore2model cost 0.0010993480682373047 seconds
DEBUG 01-15 16:08:53.828260.828260 cuda_h.py:19] end sllm_worker_task cost 0.013231754302978516 seconds
DEBUG 01-15 16:08:53.829268.829268 cuda_h.py:19] end gate cost 0.002245187759399414 seconds
DEBUG 01-15 16:08:53.829258.829258 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.830572.830572 lmp.py:1912] 
DEBUG 01-15 16:08:53.830572.830572 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.830494.830494 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.830051.830051 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.830032.830032 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.830629.830629 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.830033.830033 lmp.py:1917] 
DEBUG 01-15 16:08:53.830033.830033 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.830392.830392 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.830234.830234 lmp.py:1935]   Expert  5 |     14 | CPU
DEBUG 01-15 16:08:53.830069.830069 lmp.py:1935]   Expert 56 |     33 | CPU
DEBUG 01-15 16:08:53.830712.830712 lmp.py:1935]   Expert 16 |     85 | CPU
DEBUG 01-15 16:08:53.830116.830116 lmp.py:1935]   Expert 27 |     87 | CPU
DEBUG 01-15 16:08:53.830521.830521 lmp.py:1935]   Expert 17 |     89 | CPU
DEBUG 01-15 16:08:53.830925.830925 lmp.py:1935]   Expert 40 |     96 | CPU
DEBUG 01-15 16:08:53.830568.830568 lmp.py:1935]   Expert 49 |    104 | CPU
DEBUG 01-15 16:08:53.830211.830211 lmp.py:1935]   Expert 63 |    105 | CPU
DEBUG 01-15 16:08:53.830046.830046 lmp.py:1935]   Expert 53 |    107 | CPU
DEBUG 01-15 16:08:53.830166.830166 lmp.py:1935]   Expert 51 |    108 | CPU
DEBUG 01-15 16:08:53.830524.830524 lmp.py:1935]   Expert  7 |    109 | CPU
DEBUG 01-15 16:08:53.830121.830121 lmp.py:1935]   Expert 28 |    109 | CPU
DEBUG 01-15 16:08:53.830287.830287 lmp.py:1935]   Expert 38 |    121 | CPU
DEBUG 01-15 16:08:53.830453.830453 lmp.py:1935]   Expert 47 |    124 | CPU
DEBUG 01-15 16:08:53.830619.830619 lmp.py:1935]   Expert 62 |    124 | CPU
DEBUG 01-15 16:08:53.830547.830547 lmp.py:1935]   Expert 37 |    127 | CPU
DEBUG 01-15 16:08:53.830475.830475 lmp.py:1935]   Expert 11 |    128 | CPU
DEBUG 01-15 16:08:53.830164.830164 lmp.py:1935]   Expert 58 |    131 | CPU
DEBUG 01-15 16:08:53.830853.830853 lmp.py:1935]   Expert 57 |    139 | CPU
DEBUG 01-15 16:08:53.830781.830781 lmp.py:1935]   Expert 14 |    144 | CPU
DEBUG 01-15 16:08:53.830709.830709 lmp.py:1935]   Expert 39 |    145 | CPU
DEBUG 01-15 16:08:53.830398.830398 lmp.py:1935]   Expert  1 |    146 | CPU
DEBUG 01-15 16:08:53.830087.830087 lmp.py:1935]   Expert 52 |    151 | CPU
DEBUG 01-15 16:08:53.830015.830015 lmp.py:1935]   Expert 23 |    156 | CPU
DEBUG 01-15 16:08:53.830466.830466 lmp.py:1935]   Expert 25 |    156 | CPU
DEBUG 01-15 16:08:53.830063.830063 lmp.py:1935]   Expert 33 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.830328.830328 lmp.py:1935]   Expert 21 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.830402.830402 lmp.py:1935]   Expert  6 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.830475.830475 lmp.py:1935]   Expert 60 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.830310.830310 lmp.py:1935]   Expert 45 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.830669.830669 lmp.py:1935]   Expert 19 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.830027.830027 lmp.py:1935]   Expert 44 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.830147.830147 lmp.py:1935]   Expert  4 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.830028.830028 lmp.py:1935]   Expert 12 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.830909.830909 lmp.py:1935]   Expert 30 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.830552.830552 lmp.py:1935]   Expert 31 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831434.831434 lmp.py:1935]   Expert  3 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831315.831315 lmp.py:1935]   Expert 55 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831197.831197 lmp.py:1935]   Expert 36 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831270.831270 lmp.py:1935]   Expert  9 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831582.831582 lmp.py:1935]   Expert  0 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831179.831179 lmp.py:1935]   Expert 34 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831298.831298 lmp.py:1935]   Expert 22 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831180.831180 lmp.py:1935]   Expert 41 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831061.831061 lmp.py:1935]   Expert 26 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831704.831704 lmp.py:1935]   Expert 54 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831586.831586 lmp.py:1935]   Expert 43 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831229.831229 lmp.py:1935]   Expert 59 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831872.831872 lmp.py:1935]   Expert 13 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831422.831422 lmp.py:1935]   Expert 18 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831019.831019 lmp.py:1935]   Expert 50 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831138.831138 lmp.py:1935]   Expert 20 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831020.831020 lmp.py:1935]   Expert 15 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831901.831901 lmp.py:1935]   Expert 42 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831783.831783 lmp.py:1935]   Expert 24 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831425.831425 lmp.py:1935]   Expert 29 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831830.831830 lmp.py:1935]   Expert 61 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831473.831473 lmp.py:1935]   Expert 35 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831116.831116 lmp.py:1935]   Expert 32 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831726.831726 lmp.py:1935]   Expert  8 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831369.831369 lmp.py:1935]   Expert 10 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831535.831535 lmp.py:1935]   Expert  2 |    342 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831463.831463 lmp.py:1935]   Expert 46 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.831152.831152 lmp.py:1935]   Expert 48 |    449 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.831650.831650 lmp.py:1937] 
DEBUG 01-15 16:08:53.831650.831650 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.831100.831100 lmp.py:1938]   CPU:   2838 tokens
DEBUG 01-15 16:08:53.831028.831028 lmp.py:1942]   cuda:1:   4647 tokens (19 experts)
DEBUG 01-15 16:08:53.831194.831194 lmp.py:1942]   cuda:2:   4803 tokens (20 experts)
DEBUG 01-15 16:08:53.831645.831645 lmp.py:1943]   Total GPU:   9450 tokens
DEBUG 01-15 16:08:53.831096.831096 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.831096.831096 lmp.py:1944] 
DEBUG 01-15 16:08:53.831176.831176 cuda_h.py:19] end experts_map_get cost 0.0018761157989501953 seconds
DEBUG 01-15 16:08:53.831602.831602 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.831358.831358 lmp.py:1953] 
DEBUG 01-15 16:08:53.831358.831358 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.831387.831387 cuda_h.py:19] end cpu_experts_submit cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:08:53.831534.831534 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.831470.831470 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.832343.832343 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.0.gate_proj.weight': 0, 'model.layers.23.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.23.mlp.experts.2.gate_proj.weight': 0, 'model.layers.23.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.9.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.9.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.9.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.34.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.34.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.34.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.45.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.45.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.45.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.46.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.46.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.46.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.54.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.54.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.54.up_proj.weight': 322961408, 'model.layers.23.mlp.experts.55.gate_proj.weight': 328728576, 'model.layers.23.mlp.experts.55.down_proj.weight': 334495744, 'model.layers.23.mlp.experts.55.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(27221032960, 5767168, 0, 0), (27226800128, 5767168, 5767168, 0), (27215265792, 5767168, 11534336, 0), (27272937472, 5767168, 17301504, 0), (27278704640, 5767168, 23068672, 0), (27267170304, 5767168, 28835840, 0), (27290238976, 5767168, 34603008, 0), (27296006144, 5767168, 40370176, 0), (27284471808, 5767168, 46137344, 0), (27324841984, 5767168, 51904512, 0), (27330609152, 5767168, 57671680, 0), (27319074816, 5767168, 63438848, 0), (27359444992, 5767168, 69206016, 0), (27365212160, 5767168, 74973184, 0), (27353677824, 5767168, 80740352, 0), (27532460032, 5767168, 86507520, 0), (27538227200, 5767168, 92274688, 0), (27526692864, 5767168, 98041856, 0), (27601666048, 5767168, 103809024, 0), (27607433216, 5767168, 109576192, 0), (27595898880, 5767168, 115343360, 0), (27670872064, 5767168, 121110528, 0), (27676639232, 5767168, 126877696, 0), (27665104896, 5767168, 132644864, 0), (27757379584, 5767168, 138412032, 0), (27763146752, 5767168, 144179200, 0), (27751612416, 5767168, 149946368, 0), (27774681088, 5767168, 155713536, 0), (27780448256, 5767168, 161480704, 0), (27768913920, 5767168, 167247872, 0), (27826585600, 5767168, 173015040, 0), (27832352768, 5767168, 178782208, 0), (27820818432, 5767168, 184549376, 0), (27843887104, 5767168, 190316544, 0), (27849654272, 5767168, 196083712, 0), (27838119936, 5767168, 201850880, 0), (27947696128, 5767168, 207618048, 0), (27953463296, 5767168, 213385216, 0), (27941928960, 5767168, 219152384, 0), (27982299136, 5767168, 224919552, 0), (27988066304, 5767168, 230686720, 0), (27976531968, 5767168, 236453888, 0), (28051505152, 5767168, 242221056, 0), (28057272320, 5767168, 247988224, 0), (28045737984, 5767168, 253755392, 0), (28086108160, 5767168, 259522560, 0), (28091875328, 5767168, 265289728, 0), (28080340992, 5767168, 271056896, 0), (28241821696, 5767168, 276824064, 0), (28247588864, 5767168, 282591232, 0), (28236054528, 5767168, 288358400, 0), (28259123200, 5767168, 294125568, 0), (28264890368, 5767168, 299892736, 0), (28253356032, 5767168, 305659904, 0), (28276424704, 5767168, 311427072, 0), (28282191872, 5767168, 317194240, 0), (28270657536, 5767168, 322961408, 0)], 2: [(27255635968, 5767168, 0, 0), (27261403136, 5767168, 5767168, 0), (27249868800, 5767168, 11534336, 0), (27376746496, 5767168, 17301504, 0), (27382513664, 5767168, 23068672, 0), (27370979328, 5767168, 28835840, 0), (27394048000, 5767168, 34603008, 0), (27399815168, 5767168, 40370176, 0), (27388280832, 5767168, 46137344, 0), (27428651008, 5767168, 51904512, 0), (27434418176, 5767168, 57671680, 0), (27422883840, 5767168, 63438848, 0), (27445952512, 5767168, 69206016, 0), (27451719680, 5767168, 74973184, 0), (27440185344, 5767168, 80740352, 0), (27480555520, 5767168, 86507520, 0), (27486322688, 5767168, 92274688, 0), (27474788352, 5767168, 98041856, 0), (27549761536, 5767168, 103809024, 0), (27555528704, 5767168, 109576192, 0), (27543994368, 5767168, 115343360, 0), (27567063040, 5767168, 121110528, 0), (27572830208, 5767168, 126877696, 0), (27561295872, 5767168, 132644864, 0), (27584364544, 5767168, 138412032, 0), (27590131712, 5767168, 144179200, 0), (27578597376, 5767168, 149946368, 0), (27636269056, 5767168, 155713536, 0), (27642036224, 5767168, 161480704, 0), (27630501888, 5767168, 167247872, 0), (27722776576, 5767168, 173015040, 0), (27728543744, 5767168, 178782208, 0), (27717009408, 5767168, 184549376, 0), (27740078080, 5767168, 190316544, 0), (27745845248, 5767168, 196083712, 0), (27734310912, 5767168, 201850880, 0), (27791982592, 5767168, 207618048, 0), (27797749760, 5767168, 213385216, 0), (27786215424, 5767168, 219152384, 0), (27809284096, 5767168, 224919552, 0), (27815051264, 5767168, 230686720, 0), (27803516928, 5767168, 236453888, 0), (27930394624, 5767168, 242221056, 0), (27936161792, 5767168, 247988224, 0), (27924627456, 5767168, 253755392, 0), (27964997632, 5767168, 259522560, 0), (27970764800, 5767168, 265289728, 0), (27959230464, 5767168, 271056896, 0), (27999600640, 5767168, 276824064, 0), (28005367808, 5767168, 282591232, 0), (27993833472, 5767168, 288358400, 0), (28016902144, 5767168, 294125568, 0), (28022669312, 5767168, 299892736, 0), (28011134976, 5767168, 305659904, 0), (28155314176, 5767168, 311427072, 0), (28161081344, 5767168, 317194240, 0), (28149547008, 5767168, 322961408, 0), (28172615680, 5767168, 328728576, 0), (28178382848, 5767168, 334495744, 0), (28166848512, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814758ab0>, 2: <capsule object NULL at 0x74a680758660>}
DEBUG 01-15 16:08:53.833927.833927 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.833486.833486 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.833195.833195 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 17994b56-182d-4813-8f32-6cfebeed1fe4
DEBUG 01-15 16:08:53.833959.833959 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.833235.833235 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.833021.833021 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:08:53.834428.834428 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 17994b56-182d-4813-8f32-6cfebeed1fe4
DEBUG 01-15 16:08:53.834105.834105 cuda_h.py:19] end move_flatidxs cost 0.0008666515350341797 seconds
DEBUG 01-15 16:08:53.834518.834518 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.835817.835817 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034172534942626953 seconds
DEBUG 01-15 16:08:53.835058.835058 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.839946.839946 cuda_h.py:19] end restore2model cost 0.003742218017578125 seconds
DEBUG 01-15 16:08:53.839888.839888 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007400989532470703 seconds
DEBUG 01-15 16:08:53.839876.839876 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.839378.839378 cuda_h.py:19] end gpu_sexperts cost 0.00033593177795410156 seconds
DEBUG 01-15 16:08:53.839784.839784 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.839806.839806 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.09808349609375e-05 seconds
DEBUG 01-15 16:08:53.839217.839217 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.839019.839019 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.841288.841288 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001252889633178711 seconds
DEBUG 01-15 16:08:53.841045.841045 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.841940.841940 cuda_h.py:19] end gpu_group_list cost 0.00020885467529296875 seconds
DEBUG 01-15 16:08:53.842155.842155 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.843203.843203 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013766288757324219 seconds
DEBUG 01-15 16:08:53.843576.843576 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.844804.844804 cuda_h.py:19] end gpu_group_list cost 0.0002262592315673828 seconds
DEBUG 01-15 16:08:53.844294.844294 cuda_h.py:19] end group_tensors cost 0.009303808212280273 seconds
DEBUG 01-15 16:08:53.844822.844822 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.845787.845787 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.845639.845639 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 17994b56-182d-4813-8f32-6cfebeed1fe4
DEBUG 01-15 16:08:53.848202.848202 cuda_h.py:19] end group pad cost 0.003173351287841797 seconds
DEBUG 01-15 16:08:53.848284.848284 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:53.874269.874269 client.py:127] Model loaded
DEBUG 01-15 16:08:53.874131.874131 cuda_h.py:19] end wait_experts_multi_device cost 0.029067277908325195 seconds
DEBUG 01-15 16:08:53.874465.874465 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.876973.876973 cuda_h.py:19] end group_einsum cost 0.028218507766723633 seconds
DEBUG 01-15 16:08:53.876462.876462 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.879448.879448 cuda_h.py:19] end get_outputs_cpu1 cost 0.0028727054595947266 seconds
DEBUG 01-15 16:08:53.880459.880459 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04696035385131836 seconds
DEBUG 01-15 16:08:53.881523.881523 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006732940673828125 seconds
DEBUG 01-15 16:08:53.881503.881503 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.883104.883104 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.884458.884458 cuda_h.py:19] end index_scatter cost 0.00014972686767578125 seconds
DEBUG 01-15 16:08:53.884617.884617 cuda_h.py:19] end cpuoutputsdeal cost 0.0028955936431884766 seconds
DEBUG 01-15 16:08:53.884278.884278 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.884685.884685 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.885307.885307 cuda_h.py:19] end gpu_group_tensor cost 0.0003066062927246094 seconds
DEBUG 01-15 16:08:53.885237.885237 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.885256.885256 cuda_h.py:19] end gpu_group_tensor cost 0.00028514862060546875 seconds
DEBUG 01-15 16:08:53.886608.886608 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.887724.887724 cuda_h.py:19] end gpu_group_einsum cost 0.001153707504272461 seconds
DEBUG 01-15 16:08:53.887950.887950 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.888930.888930 cuda_h.py:19] end gpu_group_einsum cost 0.0005164146423339844 seconds
DEBUG 01-15 16:08:53.888497.888497 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.888880.888880 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.888903.888903 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002465248107910156 seconds
DEBUG 01-15 16:08:53.888587.888587 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.888100.888100 cuda_h.py:19] end concat_expert_out cost 6.103515625e-05 seconds
DEBUG 01-15 16:08:53.888334.888334 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.889854.889854 cuda_h.py:19] end index_scatter cost 6.842613220214844e-05 seconds
DEBUG 01-15 16:08:53.889809.889809 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009467601776123047 seconds
DEBUG 01-15 16:08:53.889938.889938 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.889635.889635 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.889940.889940 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014662742614746094 seconds
DEBUG 01-15 16:08:53.889457.889457 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.889526.889526 cuda_h.py:19] end concat_expert_out cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:08:53.889177.889177 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.889439.889439 cuda_h.py:19] end index_scatter cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:08:53.889533.889533 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005071163177490234 seconds
DEBUG 01-15 16:08:53.890164.890164 cuda_h.py:19] end gpu_experts_multi_device cost 0.05021834373474121 seconds
DEBUG 01-15 16:08:53.890842.890842 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.06276917457580566 seconds
DEBUG 01-15 16:08:53.890851.890851 cuda_h.py:19] end prefill_layer cost 0.07548141479492188 seconds
DEBUG 01-15 16:08:53.890542.890542 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:08:53.890722.890722 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.890855.890855 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:08:53.890227.890227 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:08:53.890983.890983 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:08:53.890932.890932 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 4.029273986816406e-05 seconds
DEBUG 01-15 16:08:53.890258.890258 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:08:53.890954.890954 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.890692.890692 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.891273.891273 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.891693.891693 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.891513.891513 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.892192.892192 cuda_h.py:19] end allocate_cuda_memory cost 0.0005588531494140625 seconds
DEBUG 01-15 16:08:53.892760.892760 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.892685.892685 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.892980.892980 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.892819.892819 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f41f7bf9-d0e8-472e-9d6e-87d111aa7a18
DEBUG 01-15 16:08:53.893470.893470 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.893145.893145 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.894995.894995 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f41f7bf9-d0e8-472e-9d6e-87d111aa7a18
DEBUG 01-15 16:08:53.894941.894941 cuda_h.py:19] end load_into_gpu_async cost 0.0021181106567382812 seconds
DEBUG 01-15 16:08:53.894005.894005 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.894192.894192 cuda_h.py:19] end restore_tensors2 cost 0.0001571178436279297 seconds
DEBUG 01-15 16:08:53.895533.895533 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0036139488220214844 seconds
INFO 01-15 16:08:53.895719.895719 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f41f7bf9-d0e8-472e-9d6e-87d111aa7a18
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.898553.898553 cuda_h.py:19] end self_attn cost 0.005141258239746094 seconds
DEBUG 01-15 16:08:53.899262.899262 cuda_h.py:19] end iln_self_attn_paln cost 0.008198738098144531 seconds
DEBUG 01-15 16:08:53.899960.899960 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:08:53.899014.899014 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.899715.899715 cuda_h.py:19] end gate cost 0.000720977783203125 seconds
DEBUG 01-15 16:08:53.899505.899505 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.900178.900178 lmp.py:1912] 
DEBUG 01-15 16:08:53.900178.900178 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.900041.900041 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.900889.900889 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.900731.900731 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.900189.900189 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.900693.900693 lmp.py:1917] 
DEBUG 01-15 16:08:53.900693.900693 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.900104.900104 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.900238.900238 lmp.py:1935]   Expert 36 |     22 | CPU
DEBUG 01-15 16:08:53.900172.900172 lmp.py:1935]   Expert 35 |     29 | CPU
DEBUG 01-15 16:08:53.900153.900153 lmp.py:1935]   Expert 25 |     46 | CPU
DEBUG 01-15 16:08:53.900419.900419 lmp.py:1935]   Expert 46 |     50 | CPU
DEBUG 01-15 16:08:53.900161.900161 lmp.py:1935]   Expert 51 |     51 | CPU
DEBUG 01-15 16:08:53.900380.900380 lmp.py:1935]   Expert 16 |     61 | CPU
DEBUG 01-15 16:08:53.900123.900123 lmp.py:1935]   Expert 30 |     61 | CPU
DEBUG 01-15 16:08:53.900865.900865 lmp.py:1935]   Expert  0 |     64 | CPU
DEBUG 01-15 16:08:53.900369.900369 lmp.py:1935]   Expert 43 |     68 | CPU
DEBUG 01-15 16:08:53.900111.900111 lmp.py:1935]   Expert 47 |     69 | CPU
DEBUG 01-15 16:08:53.900092.900092 lmp.py:1935]   Expert 44 |     74 | CPU
DEBUG 01-15 16:08:53.900596.900596 lmp.py:1935]   Expert 42 |     75 | CPU
DEBUG 01-15 16:08:53.900862.900862 lmp.py:1935]   Expert 39 |     76 | CPU
DEBUG 01-15 16:08:53.900366.900366 lmp.py:1935]   Expert 55 |     77 | CPU
DEBUG 01-15 16:08:53.900585.900585 lmp.py:1935]   Expert  2 |     85 | CPU
DEBUG 01-15 16:08:53.901566.901566 lmp.py:1935]   Expert  4 |    107 | CPU
DEBUG 01-15 16:08:53.901070.901070 lmp.py:1935]   Expert 33 |    119 | CPU
DEBUG 01-15 16:08:53.901812.901812 lmp.py:1935]   Expert 48 |    119 | CPU
DEBUG 01-15 16:08:53.901555.901555 lmp.py:1935]   Expert  6 |    123 | CPU
DEBUG 01-15 16:08:53.901059.901059 lmp.py:1935]   Expert 13 |    123 | CPU
DEBUG 01-15 16:08:53.901040.901040 lmp.py:1935]   Expert 61 |    126 | CPU
DEBUG 01-15 16:08:53.901782.901782 lmp.py:1935]   Expert 24 |    127 | CPU
DEBUG 01-15 16:08:53.901286.901286 lmp.py:1935]   Expert 56 |    128 | CPU
DEBUG 01-15 16:08:53.901744.901744 lmp.py:1935]   Expert 29 |    131 | CPU
DEBUG 01-15 16:08:53.901725.901725 lmp.py:1935]   Expert 15 |    133 | CPU
DEBUG 01-15 16:08:53.901374.901374 lmp.py:1935]   Expert 54 |    140 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901786.901786 lmp.py:1935]   Expert  9 |    141 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901959.901959 lmp.py:1935]   Expert 38 |    142 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901655.901655 lmp.py:1935]   Expert  7 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901589.901589 lmp.py:1935]   Expert 20 |    148 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901524.901524 lmp.py:1935]   Expert 59 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901981.901981 lmp.py:1935]   Expert 45 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901439.901439 lmp.py:1935]   Expert 19 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901850.901850 lmp.py:1935]   Expert 62 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901546.901546 lmp.py:1935]   Expert 34 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901243.901243 lmp.py:1935]   Expert 57 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901542.901542 lmp.py:1935]   Expert 50 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901808.901808 lmp.py:1935]   Expert 31 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901643.901643 lmp.py:1935]   Expert 10 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901478.901478 lmp.py:1935]   Expert 23 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901267.901267 lmp.py:1935]   Expert 60 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901863.901863 lmp.py:1935]   Expert  8 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901221.901221 lmp.py:1935]   Expert 18 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901547.901547 lmp.py:1935]   Expert 22 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901905.901905 lmp.py:1935]   Expert 53 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901025.901025 lmp.py:1935]   Expert 52 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901383.901383 lmp.py:1935]   Expert 37 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901026.901026 lmp.py:1935]   Expert  5 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901597.901597 lmp.py:1935]   Expert 17 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901193.901193 lmp.py:1935]   Expert 11 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901790.901790 lmp.py:1935]   Expert  1 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901148.901148 lmp.py:1935]   Expert 49 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901268.901268 lmp.py:1935]   Expert 41 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901388.901388 lmp.py:1935]   Expert 28 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901508.901508 lmp.py:1935]   Expert 32 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901866.901866 lmp.py:1935]   Expert 26 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901178.901178 lmp.py:1935]   Expert 58 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901251.901251 lmp.py:1935]   Expert 40 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901371.901371 lmp.py:1935]   Expert 14 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901253.901253 lmp.py:1935]   Expert 12 |    332 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901372.901372 lmp.py:1935]   Expert 63 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.901254.901254 lmp.py:1935]   Expert 21 |    389 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.901374.901374 lmp.py:1935]   Expert 27 |    660 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.902970.902970 lmp.py:1935]   Expert  3 |   1015 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.902613.902613 lmp.py:1937] 
DEBUG 01-15 16:08:53.902613.902613 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.902256.902256 lmp.py:1938]   CPU:   2144 tokens
DEBUG 01-15 16:08:53.902853.902853 lmp.py:1942]   cuda:1:   5067 tokens (19 experts)
DEBUG 01-15 16:08:53.902734.902734 lmp.py:1942]   cuda:2:   5077 tokens (20 experts)
DEBUG 01-15 16:08:53.902900.902900 lmp.py:1943]   Total GPU:  10144 tokens
DEBUG 01-15 16:08:53.902590.902590 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.902590.902590 lmp.py:1944] 
DEBUG 01-15 16:08:53.902908.902908 cuda_h.py:19] end experts_map_get cost 0.0021276473999023438 seconds
DEBUG 01-15 16:08:53.902911.902911 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.902574.902574 lmp.py:1953] 
DEBUG 01-15 16:08:53.902574.902574 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.902510.902510 cuda_h.py:19] end cpu_experts_submit cost 5.7220458984375e-05 seconds
DEBUG 01-15 16:08:53.902590.902590 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.902625.902625 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.903182.903182 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.1.gate_proj.weight': 0, 'model.layers.24.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.28.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.28.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.28.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.58.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.58.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.58.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.24.mlp.experts.8.gate_proj.weight': 0, 'model.layers.24.mlp.experts.8.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.8.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.11.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.11.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.11.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.17.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.17.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.17.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.24.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.24.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.24.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(28345630720, 5767168, 0, 0), (28351397888, 5767168, 5767168, 0), (28339863552, 5767168, 11534336, 0), (28380233728, 5767168, 17301504, 0), (28386000896, 5767168, 23068672, 0), (28374466560, 5767168, 28835840, 0), (28414836736, 5767168, 34603008, 0), (28420603904, 5767168, 40370176, 0), (28409069568, 5767168, 46137344, 0), (28449439744, 5767168, 51904512, 0), (28455206912, 5767168, 57671680, 0), (28443672576, 5767168, 63438848, 0), (28484042752, 5767168, 69206016, 0), (28489809920, 5767168, 74973184, 0), (28478275584, 5767168, 80740352, 0), (28501344256, 5767168, 86507520, 0), (28507111424, 5767168, 92274688, 0), (28495577088, 5767168, 98041856, 0), (28570550272, 5767168, 103809024, 0), (28576317440, 5767168, 109576192, 0), (28564783104, 5767168, 115343360, 0), (28639756288, 5767168, 121110528, 0), (28645523456, 5767168, 126877696, 0), (28633989120, 5767168, 132644864, 0), (28657057792, 5767168, 138412032, 0), (28662824960, 5767168, 144179200, 0), (28651290624, 5767168, 149946368, 0), (28708962304, 5767168, 155713536, 0), (28714729472, 5767168, 161480704, 0), (28703195136, 5767168, 167247872, 0), (28726263808, 5767168, 173015040, 0), (28732030976, 5767168, 178782208, 0), (28720496640, 5767168, 184549376, 0), (28812771328, 5767168, 190316544, 0), (28818538496, 5767168, 196083712, 0), (28807004160, 5767168, 201850880, 0), (28968484864, 5767168, 207618048, 0), (28974252032, 5767168, 213385216, 0), (28962717696, 5767168, 219152384, 0), (29037690880, 5767168, 224919552, 0), (29043458048, 5767168, 230686720, 0), (29031923712, 5767168, 236453888, 0), (29193404416, 5767168, 242221056, 0), (29199171584, 5767168, 247988224, 0), (29187637248, 5767168, 253755392, 0), (29331816448, 5767168, 259522560, 0), (29337583616, 5767168, 265289728, 0), (29326049280, 5767168, 271056896, 0), (29349117952, 5767168, 276824064, 0), (29354885120, 5767168, 282591232, 0), (29343350784, 5767168, 288358400, 0), (29401022464, 5767168, 294125568, 0), (29406789632, 5767168, 299892736, 0), (29395255296, 5767168, 305659904, 0), (29418323968, 5767168, 311427072, 0), (29424091136, 5767168, 317194240, 0), (29412556800, 5767168, 322961408, 0)], 2: [(28466741248, 5767168, 0, 0), (28472508416, 5767168, 5767168, 0), (28460974080, 5767168, 11534336, 0), (28518645760, 5767168, 17301504, 0), (28524412928, 5767168, 23068672, 0), (28512878592, 5767168, 28835840, 0), (28535947264, 5767168, 34603008, 0), (28541714432, 5767168, 40370176, 0), (28530180096, 5767168, 46137344, 0), (28622454784, 5767168, 51904512, 0), (28628221952, 5767168, 57671680, 0), (28616687616, 5767168, 63438848, 0), (28674359296, 5767168, 69206016, 0), (28680126464, 5767168, 74973184, 0), (28668592128, 5767168, 80740352, 0), (28691660800, 5767168, 86507520, 0), (28697427968, 5767168, 92274688, 0), (28685893632, 5767168, 98041856, 0), (28778168320, 5767168, 103809024, 0), (28783935488, 5767168, 109576192, 0), (28772401152, 5767168, 115343360, 0), (28795469824, 5767168, 121110528, 0), (28801236992, 5767168, 126877696, 0), (28789702656, 5767168, 132644864, 0), (28864675840, 5767168, 138412032, 0), (28870443008, 5767168, 144179200, 0), (28858908672, 5767168, 149946368, 0), (28881977344, 5767168, 155713536, 0), (28887744512, 5767168, 161480704, 0), (28876210176, 5767168, 167247872, 0), (28916580352, 5767168, 173015040, 0), (28922347520, 5767168, 178782208, 0), (28910813184, 5767168, 184549376, 0), (28985786368, 5767168, 190316544, 0), (28991553536, 5767168, 196083712, 0), (28980019200, 5767168, 201850880, 0), (29020389376, 5767168, 207618048, 0), (29026156544, 5767168, 213385216, 0), (29014622208, 5767168, 219152384, 0), (29106896896, 5767168, 224919552, 0), (29112664064, 5767168, 230686720, 0), (29101129728, 5767168, 236453888, 0), (29176102912, 5767168, 242221056, 0), (29181870080, 5767168, 247988224, 0), (29170335744, 5767168, 253755392, 0), (29228007424, 5767168, 259522560, 0), (29233774592, 5767168, 265289728, 0), (29222240256, 5767168, 271056896, 0), (29245308928, 5767168, 276824064, 0), (29251076096, 5767168, 282591232, 0), (29239541760, 5767168, 288358400, 0), (29262610432, 5767168, 294125568, 0), (29268377600, 5767168, 299892736, 0), (29256843264, 5767168, 305659904, 0), (29314514944, 5767168, 311427072, 0), (29320282112, 5767168, 317194240, 0), (29308747776, 5767168, 322961408, 0), (29366419456, 5767168, 328728576, 0), (29372186624, 5767168, 334495744, 0), (29360652288, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc1b8660>, 2: <capsule object NULL at 0x74a678518900>}
DEBUG 01-15 16:08:53.903965.903965 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.903305.903305 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.903068.903068 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 43ac8f72-d558-40d7-8b2d-ed3d79470c46
DEBUG 01-15 16:08:53.903088.903088 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.904388.904388 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:08:53.904939.904939 client.py:127] Model loaded
DEBUG 01-15 16:08:53.904200.904200 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:53.904110.904110 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.905112.905112 cuda_h.py:19] end move_flatidxs cost 0.0008556842803955078 seconds
DEBUG 01-15 16:08:53.905656.905656 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.905155.905155 cuda_h.py:19] end restore2model cost 0.0010573863983154297 seconds
INFO 01-15 16:08:53.905499.905499 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 43ac8f72-d558-40d7-8b2d-ed3d79470c46
DEBUG 01-15 16:08:53.905967.905967 cuda_h.py:19] end sllm_worker_task cost 0.014550924301147461 seconds
DEBUG 01-15 16:08:53.906306.906306 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004067659378051758 seconds
DEBUG 01-15 16:08:53.906393.906393 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.909219.909219 cuda_h.py:19] end group_tensors cost 0.0045166015625 seconds
DEBUG 01-15 16:08:53.910362.910362 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.910018.910018 cuda_h.py:19] end restore2model cost 0.0037882328033447266 seconds
DEBUG 01-15 16:08:53.910675.910675 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008252620697021484 seconds
DEBUG 01-15 16:08:53.910570.910570 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.911564.911564 cuda_h.py:19] end gpu_sexperts cost 0.0005476474761962891 seconds
DEBUG 01-15 16:08:53.911408.911408 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.911298.911298 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:08:53.911677.911677 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.911361.911361 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.913834.913834 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0018079280853271484 seconds
DEBUG 01-15 16:08:53.913453.913453 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.913229.913229 cuda_h.py:19] end group pad cost 0.0029039382934570312 seconds
DEBUG 01-15 16:08:53.913450.913450 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:53.913497.913497 cuda_h.py:19] end gpu_group_list cost 0.00026726722717285156 seconds
DEBUG 01-15 16:08:53.917313.917313 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.921705.921705 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.002707958221435547 seconds
DEBUG 01-15 16:08:53.922988.922988 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.923009.923009 cuda_h.py:19] end gpu_group_list cost 0.0009512901306152344 seconds
DEBUG 01-15 16:08:53.926548.926548 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.927767.927767 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 43ac8f72-d558-40d7-8b2d-ed3d79470c46
DEBUG 01-15 16:08:53.935964.935964 cuda_h.py:19] end group_einsum cost 0.021781206130981445 seconds
DEBUG 01-15 16:08:53.935539.935539 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:53.939904.939904 cuda_h.py:19] end get_outputs_cpu1 cost 0.0037145614624023438 seconds
DEBUG 01-15 16:08:53.940160.940160 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.036237478256225586 seconds
INFO 01-15 16:08:53.941455.941455 client.py:127] Model loaded
DEBUG 01-15 16:08:53.941923.941923 cuda_h.py:19] end wait_experts_multi_device cost 0.014380931854248047 seconds
DEBUG 01-15 16:08:53.941024.941024 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:53.942308.942308 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005190372467041016 seconds
DEBUG 01-15 16:08:53.942912.942912 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:53.943794.943794 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.943534.943534 cuda_h.py:19] end index_scatter cost 9.775161743164062e-05 seconds
DEBUG 01-15 16:08:53.943616.943616 cuda_h.py:19] end cpuoutputsdeal cost 0.001603841781616211 seconds
DEBUG 01-15 16:08:53.943102.943102 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:53.943057.943057 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.944143.944143 cuda_h.py:19] end gpu_group_tensor cost 0.0001709461212158203 seconds
DEBUG 01-15 16:08:53.944476.944476 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:53.944987.944987 cuda_h.py:19] end gpu_group_tensor cost 0.00020170211791992188 seconds
DEBUG 01-15 16:08:53.944004.944004 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.945845.945845 cuda_h.py:19] end gpu_group_einsum cost 0.0009145736694335938 seconds
DEBUG 01-15 16:08:53.945075.945075 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:53.946032.946032 cuda_h.py:19] end gpu_group_einsum cost 0.0006031990051269531 seconds
DEBUG 01-15 16:08:53.946229.946229 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.946220.946220 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.947531.947531 cuda_h.py:19] end all_expert_outputs_slices cost 0.00035691261291503906 seconds
DEBUG 01-15 16:08:53.947195.947195 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.947648.947648 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:08:53.947213.947213 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.947773.947773 cuda_h.py:19] end index_scatter cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:08:53.947927.947927 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010204315185546875 seconds
DEBUG 01-15 16:08:53.947341.947341 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:53.947568.947568 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:53.948411.948411 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023603439331054688 seconds
DEBUG 01-15 16:08:53.948598.948598 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:53.948296.948296 cuda_h.py:19] end concat_expert_out cost 5.9604644775390625e-05 seconds
DEBUG 01-15 16:08:53.948954.948954 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:53.948798.948798 cuda_h.py:19] end index_scatter cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:08:53.948038.948038 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006175041198730469 seconds
DEBUG 01-15 16:08:53.948419.948419 cuda_h.py:19] end gpu_experts_multi_device cost 0.03696107864379883 seconds
DEBUG 01-15 16:08:53.948303.948303 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.04937553405761719 seconds
DEBUG 01-15 16:08:53.949564.949564 cuda_h.py:19] end prefill_layer cost 0.058348655700683594 seconds
DEBUG 01-15 16:08:53.949870.949870 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:08:53.949957.949957 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:53.949713.949713 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:08:53.949231.949231 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:08:53.949132.949132 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:08:53.949327.949327 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 4.124641418457031e-05 seconds
DEBUG 01-15 16:08:53.949229.949229 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:08:53.949885.949885 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:53.949465.949465 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:53.949861.949861 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:53.949234.949234 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:53.950882.950882 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:53.950229.950229 cuda_h.py:19] end allocate_cuda_memory cost 0.0005450248718261719 seconds
DEBUG 01-15 16:08:53.950501.950501 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:53.950875.950875 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.951323.951323 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:53.951193.951193 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0d4f9b25-4ced-4670-9278-3a8f351677c8
DEBUG 01-15 16:08:53.951943.951943 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.951836.951836 cuda_h.py:10] start self_attn
INFO 01-15 16:08:53.952118.952118 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0d4f9b25-4ced-4670-9278-3a8f351677c8
DEBUG 01-15 16:08:53.953900.953900 cuda_h.py:19] end load_into_gpu_async cost 0.0021195411682128906 seconds
DEBUG 01-15 16:08:53.953877.953877 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:53.953217.953217 cuda_h.py:19] end restore_tensors2 cost 0.00016999244689941406 seconds
DEBUG 01-15 16:08:53.953281.953281 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0036478042602539062 seconds
INFO 01-15 16:08:53.953769.953769 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0d4f9b25-4ced-4670-9278-3a8f351677c8
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:53.956346.956346 cuda_h.py:19] end self_attn cost 0.0048029422760009766 seconds
DEBUG 01-15 16:08:53.957445.957445 cuda_h.py:19] end iln_self_attn_paln cost 0.007781505584716797 seconds
DEBUG 01-15 16:08:53.957805.957805 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:08:53.957760.957760 cuda_h.py:10] start gate
DEBUG 01-15 16:08:53.958713.958713 cuda_h.py:19] end gate cost 0.0007250308990478516 seconds
DEBUG 01-15 16:08:53.958834.958834 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:53.958976.958976 lmp.py:1912] 
DEBUG 01-15 16:08:53.958976.958976 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:53.958023.958023 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:53.958442.958442 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:53.958807.958807 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:53.958503.958503 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:53.958768.958768 lmp.py:1917] 
DEBUG 01-15 16:08:53.958768.958768 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:53.958749.958749 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:53.958167.958167 lmp.py:1935]   Expert 13 |     26 | CPU
DEBUG 01-15 16:08:53.958102.958102 lmp.py:1935]   Expert  9 |     40 | CPU
DEBUG 01-15 16:08:53.958083.958083 lmp.py:1935]   Expert 44 |     41 | CPU
DEBUG 01-15 16:08:53.958348.958348 lmp.py:1935]   Expert 25 |     43 | CPU
DEBUG 01-15 16:08:53.958044.958044 lmp.py:1935]   Expert 16 |     46 | CPU
DEBUG 01-15 16:08:53.958025.958025 lmp.py:1935]   Expert 38 |     49 | CPU
DEBUG 01-15 16:08:53.958960.958960 lmp.py:1935]   Expert  2 |     52 | CPU
DEBUG 01-15 16:08:53.958702.958702 lmp.py:1935]   Expert 22 |     55 | CPU
DEBUG 01-15 16:08:53.958444.958444 lmp.py:1935]   Expert 33 |     57 | CPU
DEBUG 01-15 16:08:53.958472.958472 lmp.py:1935]   Expert 42 |     60 | CPU
DEBUG 01-15 16:08:53.958499.958499 lmp.py:1935]   Expert  5 |     66 | CPU
DEBUG 01-15 16:08:53.958526.958526 lmp.py:1935]   Expert 23 |     76 | CPU
DEBUG 01-15 16:08:53.958791.958791 lmp.py:1935]   Expert 24 |     79 | CPU
DEBUG 01-15 16:08:53.958057.958057 lmp.py:1935]   Expert 10 |     86 | CPU
DEBUG 01-15 16:08:53.959846.959846 lmp.py:1935]   Expert 59 |     99 | CPU
DEBUG 01-15 16:08:53.959873.959873 lmp.py:1935]   Expert 21 |    109 | CPU
DEBUG 01-15 16:08:53.959377.959377 lmp.py:1935]   Expert 45 |    115 | CPU
DEBUG 01-15 16:08:53.959881.959881 lmp.py:1935]   Expert 46 |    115 | CPU
DEBUG 01-15 16:08:53.959623.959623 lmp.py:1935]   Expert 55 |    115 | CPU
DEBUG 01-15 16:08:53.959604.959604 lmp.py:1935]   Expert 61 |    119 | CPU
DEBUG 01-15 16:08:53.959108.959108 lmp.py:1935]   Expert 31 |    127 | CPU
DEBUG 01-15 16:08:53.959327.959327 lmp.py:1935]   Expert 51 |    139 | CPU
DEBUG 01-15 16:08:53.959355.959355 lmp.py:1935]   Expert 36 |    142 | CPU
DEBUG 01-15 16:08:53.959620.959620 lmp.py:1935]   Expert  6 |    144 | CPU
DEBUG 01-15 16:08:53.959409.959409 lmp.py:1935]   Expert  8 |    144 | CPU
DEBUG 01-15 16:08:53.959343.959343 lmp.py:1935]   Expert 43 |    144 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959039.959039 lmp.py:1935]   Expert  3 |    152 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959736.959736 lmp.py:1935]   Expert  0 |    153 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959193.959193 lmp.py:1935]   Expert 26 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959366.959366 lmp.py:1935]   Expert 18 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959539.959539 lmp.py:1935]   Expert 48 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959474.959474 lmp.py:1935]   Expert 41 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959170.959170 lmp.py:1935]   Expert 12 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959151.959151 lmp.py:1935]   Expert  7 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959893.959893 lmp.py:1935]   Expert 20 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959874.959874 lmp.py:1935]   Expert 56 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959855.959855 lmp.py:1935]   Expert 28 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959835.959835 lmp.py:1935]   Expert 27 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959532.959532 lmp.py:1935]   Expert  1 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959989.959989 lmp.py:1935]   Expert 34 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959970.959970 lmp.py:1935]   Expert 47 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959189.959189 lmp.py:1935]   Expert 32 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959409.959409 lmp.py:1935]   Expert 11 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959628.959628 lmp.py:1935]   Expert 40 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959370.959370 lmp.py:1935]   Expert 49 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959874.959874 lmp.py:1935]   Expert 53 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959570.959570 lmp.py:1935]   Expert 63 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959266.959266 lmp.py:1935]   Expert 15 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959247.959247 lmp.py:1935]   Expert 29 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959705.959705 lmp.py:1935]   Expert 50 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959447.959447 lmp.py:1935]   Expert  4 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959428.959428 lmp.py:1935]   Expert 30 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959529.959529 lmp.py:1935]   Expert 14 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959702.959702 lmp.py:1935]   Expert 35 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959921.959921 lmp.py:1935]   Expert 37 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959140.959140 lmp.py:1935]   Expert 52 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959598.959598 lmp.py:1935]   Expert 17 |    363 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959817.959817 lmp.py:1935]   Expert 54 |    380 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.959752.959752 lmp.py:1935]   Expert 39 |    390 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.959448.959448 lmp.py:1935]   Expert 57 |    412 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.960667.960667 lmp.py:1935]   Expert 60 |    457 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.960648.960648 lmp.py:1935]   Expert 62 |    458 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.960344.960344 lmp.py:1935]   Expert 19 |    545 | GPU2(cuda:2)
DEBUG 01-15 16:08:53.960563.960563 lmp.py:1935]   Expert 58 |    575 | GPU1(cuda:1)
DEBUG 01-15 16:08:53.960590.960590 lmp.py:1937] 
DEBUG 01-15 16:08:53.960590.960590 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:53.960525.960525 lmp.py:1938]   CPU:   2144 tokens
DEBUG 01-15 16:08:53.960459.960459 lmp.py:1942]   cuda:1:   5144 tokens (20 experts)
DEBUG 01-15 16:08:53.960440.960440 lmp.py:1942]   cuda:2:   5000 tokens (19 experts)
DEBUG 01-15 16:08:53.960467.960467 lmp.py:1943]   Total GPU:  10144 tokens
DEBUG 01-15 16:08:53.960495.960495 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:53.960495.960495 lmp.py:1944] 
DEBUG 01-15 16:08:53.960913.960913 cuda_h.py:19] end experts_map_get cost 0.0021352767944335938 seconds
DEBUG 01-15 16:08:53.960968.960968 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:53.960108.960108 lmp.py:1953] 
DEBUG 01-15 16:08:53.960108.960108 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:53.960627.960627 cuda_h.py:19] end cpu_experts_submit cost 5.602836608886719e-05 seconds
DEBUG 01-15 16:08:53.960423.960423 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:53.960551.960551 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:53.961482.961482 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.3.gate_proj.weight': 0, 'model.layers.25.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.34.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.34.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.34.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.25.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.25.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.25.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.25.mlp.experts.0.gate_proj.weight': 0, 'model.layers.25.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(29487529984, 5767168, 0, 0), (29493297152, 5767168, 5767168, 0), (29481762816, 5767168, 11534336, 0), (29504831488, 5767168, 17301504, 0), (29510598656, 5767168, 23068672, 0), (29499064320, 5767168, 28835840, 0), (29556736000, 5767168, 34603008, 0), (29562503168, 5767168, 40370176, 0), (29550968832, 5767168, 46137344, 0), (29625942016, 5767168, 51904512, 0), (29631709184, 5767168, 57671680, 0), (29620174848, 5767168, 63438848, 0), (29729751040, 5767168, 69206016, 0), (29735518208, 5767168, 74973184, 0), (29723983872, 5767168, 80740352, 0), (29747052544, 5767168, 86507520, 0), (29752819712, 5767168, 92274688, 0), (29741285376, 5767168, 98041856, 0), (29902766080, 5767168, 103809024, 0), (29908533248, 5767168, 109576192, 0), (29896998912, 5767168, 115343360, 0), (29937369088, 5767168, 121110528, 0), (29943136256, 5767168, 126877696, 0), (29931601920, 5767168, 132644864, 0), (30023876608, 5767168, 138412032, 0), (30029643776, 5767168, 144179200, 0), (30018109440, 5767168, 149946368, 0), (30041178112, 5767168, 155713536, 0), (30046945280, 5767168, 161480704, 0), (30035410944, 5767168, 167247872, 0), (30110384128, 5767168, 173015040, 0), (30116151296, 5767168, 178782208, 0), (30104616960, 5767168, 184549376, 0), (30179590144, 5767168, 190316544, 0), (30185357312, 5767168, 196083712, 0), (30173822976, 5767168, 201850880, 0), (30248796160, 5767168, 207618048, 0), (30254563328, 5767168, 213385216, 0), (30243028992, 5767168, 219152384, 0), (30266097664, 5767168, 224919552, 0), (30271864832, 5767168, 230686720, 0), (30260330496, 5767168, 236453888, 0), (30283399168, 5767168, 242221056, 0), (30289166336, 5767168, 247988224, 0), (30277632000, 5767168, 253755392, 0), (30335303680, 5767168, 259522560, 0), (30341070848, 5767168, 265289728, 0), (30329536512, 5767168, 271056896, 0), (30404509696, 5767168, 276824064, 0), (30410276864, 5767168, 282591232, 0), (30398742528, 5767168, 288358400, 0), (30439112704, 5767168, 294125568, 0), (30444879872, 5767168, 299892736, 0), (30433345536, 5767168, 305659904, 0), (30473715712, 5767168, 311427072, 0), (30479482880, 5767168, 317194240, 0), (30467948544, 5767168, 322961408, 0), (30525620224, 5767168, 328728576, 0), (30531387392, 5767168, 334495744, 0), (30519853056, 5767168, 340262912, 0)], 2: [(29435625472, 5767168, 0, 0), (29441392640, 5767168, 5767168, 0), (29429858304, 5767168, 11534336, 0), (29452926976, 5767168, 17301504, 0), (29458694144, 5767168, 23068672, 0), (29447159808, 5767168, 28835840, 0), (29643243520, 5767168, 34603008, 0), (29649010688, 5767168, 40370176, 0), (29637476352, 5767168, 46137344, 0), (29677846528, 5767168, 51904512, 0), (29683613696, 5767168, 57671680, 0), (29672079360, 5767168, 63438848, 0), (29695148032, 5767168, 69206016, 0), (29700915200, 5767168, 74973184, 0), (29689380864, 5767168, 80740352, 0), (29764354048, 5767168, 86507520, 0), (29770121216, 5767168, 92274688, 0), (29758586880, 5767168, 98041856, 0), (29781655552, 5767168, 103809024, 0), (29787422720, 5767168, 109576192, 0), (29775888384, 5767168, 115343360, 0), (29885464576, 5767168, 121110528, 0), (29891231744, 5767168, 126877696, 0), (29879697408, 5767168, 132644864, 0), (29920067584, 5767168, 138412032, 0), (29925834752, 5767168, 144179200, 0), (29914300416, 5767168, 149946368, 0), (29954670592, 5767168, 155713536, 0), (29960437760, 5767168, 161480704, 0), (29948903424, 5767168, 167247872, 0), (29989273600, 5767168, 173015040, 0), (29995040768, 5767168, 178782208, 0), (29983506432, 5767168, 184549376, 0), (30075781120, 5767168, 190316544, 0), (30081548288, 5767168, 196083712, 0), (30070013952, 5767168, 201850880, 0), (30127685632, 5767168, 207618048, 0), (30133452800, 5767168, 213385216, 0), (30121918464, 5767168, 219152384, 0), (30144987136, 5767168, 224919552, 0), (30150754304, 5767168, 230686720, 0), (30139219968, 5767168, 236453888, 0), (30300700672, 5767168, 242221056, 0), (30306467840, 5767168, 247988224, 0), (30294933504, 5767168, 253755392, 0), (30352605184, 5767168, 259522560, 0), (30358372352, 5767168, 265289728, 0), (30346838016, 5767168, 271056896, 0), (30369906688, 5767168, 276824064, 0), (30375673856, 5767168, 282591232, 0), (30364139520, 5767168, 288358400, 0), (30421811200, 5767168, 294125568, 0), (30427578368, 5767168, 299892736, 0), (30416044032, 5767168, 305659904, 0), (30508318720, 5767168, 311427072, 0), (30514085888, 5767168, 317194240, 0), (30502551552, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729800>, 2: <capsule object NULL at 0x74a660729da0>}
DEBUG 01-15 16:08:53.961965.961965 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:53.961590.961590 sllm_store_c.py:29] call client load into gpu
INFO 01-15 16:08:53.961688.961688 client.py:127] Model loaded
DEBUG 01-15 16:08:53.961507.961507 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:53.962576.962576 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.962597.962597 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:53.961030.961030 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e20543e5-603c-464d-9033-b5c0a8b1230b
DEBUG 01-15 16:08:53.962171.962171 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:53.963509.963509 cuda_h.py:19] end move_flatidxs cost 0.000850677490234375 seconds
DEBUG 01-15 16:08:53.963292.963292 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:53.963996.963996 cuda_h.py:19] end restore2model cost 0.001459360122680664 seconds
DEBUG 01-15 16:08:53.963894.963894 cuda_h.py:19] end sllm_worker_task cost 0.013997077941894531 seconds
INFO 01-15 16:08:53.964675.964675 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e20543e5-603c-464d-9033-b5c0a8b1230b
DEBUG 01-15 16:08:53.964271.964271 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0040683746337890625 seconds
DEBUG 01-15 16:08:53.964201.964201 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:53.968261.968261 cuda_h.py:19] end restore2model cost 0.0037233829498291016 seconds
DEBUG 01-15 16:08:53.968442.968442 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00806283950805664 seconds
DEBUG 01-15 16:08:53.968953.968953 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:53.968527.968527 cuda_h.py:19] end gpu_sexperts cost 0.0003185272216796875 seconds
DEBUG 01-15 16:08:53.968125.968125 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:53.969339.969339 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1457672119140625e-05 seconds
DEBUG 01-15 16:08:53.969227.969227 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:53.969460.969460 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.970167.970167 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012927055358886719 seconds
DEBUG 01-15 16:08:53.970732.970732 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.970164.970164 cuda_h.py:19] end gpu_group_list cost 0.0002186298370361328 seconds
DEBUG 01-15 16:08:53.971068.971068 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:53.972012.972012 cuda_h.py:19] end group_tensors cost 0.008810758590698242 seconds
DEBUG 01-15 16:08:53.972037.972037 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:53.973937.973937 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015568733215332031 seconds
DEBUG 01-15 16:08:53.973848.973848 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:53.973100.973100 cuda_h.py:19] end gpu_group_list cost 0.00035500526428222656 seconds
DEBUG 01-15 16:08:53.975317.975317 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:53.975618.975618 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e20543e5-603c-464d-9033-b5c0a8b1230b
DEBUG 01-15 16:08:53.975490.975490 cuda_h.py:19] end group pad cost 0.0029840469360351562 seconds
DEBUG 01-15 16:08:53.975042.975042 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:54.004178.004178 cuda_h.py:19] end group_einsum cost 0.028310060501098633 seconds
DEBUG 01-15 16:08:54.004819.004819 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:08:54.004040.004040 client.py:127] Model loaded
DEBUG 01-15 16:08:54.004441.004441 cuda_h.py:19] end wait_experts_multi_device cost 0.029541015625 seconds
DEBUG 01-15 16:08:54.004410.004410 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:54.006616.006616 cuda_h.py:19] end get_outputs_cpu1 cost 0.002379894256591797 seconds
DEBUG 01-15 16:08:54.007406.007406 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.045740365982055664 seconds
DEBUG 01-15 16:08:54.008919.008919 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0036704540252685547 seconds
DEBUG 01-15 16:08:54.008356.008356 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:54.011713.011713 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:54.011313.011313 cuda_h.py:19] end index_scatter cost 0.00015163421630859375 seconds
DEBUG 01-15 16:08:54.011125.011125 cuda_h.py:19] end cpuoutputsdeal cost 0.0028867721557617188 seconds
DEBUG 01-15 16:08:54.012217.012217 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:54.012193.012193 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:54.012220.012220 cuda_h.py:19] end gpu_group_tensor cost 0.00032401084899902344 seconds
DEBUG 01-15 16:08:54.012634.012634 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:54.012606.012606 cuda_h.py:19] end gpu_group_tensor cost 0.0002856254577636719 seconds
DEBUG 01-15 16:08:54.013389.013389 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:54.014393.014393 cuda_h.py:19] end gpu_group_einsum cost 0.001399993896484375 seconds
DEBUG 01-15 16:08:54.014527.014527 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:54.015689.015689 cuda_h.py:19] end gpu_group_einsum cost 0.0006239414215087891 seconds
DEBUG 01-15 16:08:54.015779.015779 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:54.015711.015711 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:54.016012.016012 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023317337036132812 seconds
DEBUG 01-15 16:08:54.016451.016451 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:54.016070.016070 cuda_h.py:19] end concat_expert_out cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:08:54.016947.016947 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:54.016189.016189 cuda_h.py:19] end index_scatter cost 7.128715515136719e-05 seconds
DEBUG 01-15 16:08:54.016873.016873 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009613037109375 seconds
DEBUG 01-15 16:08:54.016910.016910 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:54.016421.016421 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:54.017633.017633 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015473365783691406 seconds
DEBUG 01-15 16:08:54.017720.017720 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:54.017266.017266 cuda_h.py:19] end concat_expert_out cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:08:54.017917.017917 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:54.017178.017178 cuda_h.py:19] end index_scatter cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:08:54.017749.017749 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00051116943359375 seconds
DEBUG 01-15 16:08:54.017003.017003 cuda_h.py:19] end gpu_experts_multi_device cost 0.04847526550292969 seconds
DEBUG 01-15 16:08:54.017628.017628 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.06037116050720215 seconds
DEBUG 01-15 16:08:54.018968.018968 cuda_h.py:19] end prefill_layer cost 0.06893515586853027 seconds
DEBUG 01-15 16:08:54.018467.018467 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:08:54.018170.018170 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:54.018826.018826 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:08:54.018721.018721 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:08:54.018285.018285 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:08:54.018089.018089 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 3.886222839355469e-05 seconds
DEBUG 01-15 16:08:54.018461.018461 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:08:54.018872.018872 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:54.018848.018848 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:54.018383.018383 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:08:54.018539.018539 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:54.019174.019174 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:54.019508.019508 cuda_h.py:19] end allocate_cuda_memory cost 0.000553131103515625 seconds
DEBUG 01-15 16:08:54.019488.019488 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:54.020266.020266 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:54.020436.020436 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:54.020280.020280 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a524b701-ef85-4733-844d-04bab9474442
DEBUG 01-15 16:08:54.020169.020169 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:54.020120.020120 cuda_h.py:10] start self_attn
INFO 01-15 16:08:54.021876.021876 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a524b701-ef85-4733-844d-04bab9474442
DEBUG 01-15 16:08:54.022664.022664 cuda_h.py:19] end load_into_gpu_async cost 0.0019750595092773438 seconds
DEBUG 01-15 16:08:54.022832.022832 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:54.022457.022457 cuda_h.py:19] end restore_tensors2 cost 0.00017142295837402344 seconds
DEBUG 01-15 16:08:54.022368.022368 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003462553024291992 seconds
INFO 01-15 16:08:54.022842.022842 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a524b701-ef85-4733-844d-04bab9474442
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:54.025440.025440 cuda_h.py:19] end self_attn cost 0.004078865051269531 seconds
DEBUG 01-15 16:08:54.025398.025398 cuda_h.py:19] end iln_self_attn_paln cost 0.006989955902099609 seconds
DEBUG 01-15 16:08:54.025281.025281 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:08:54.025375.025375 cuda_h.py:10] start gate
DEBUG 01-15 16:08:54.026580.026580 cuda_h.py:19] end gate cost 0.0007486343383789062 seconds
DEBUG 01-15 16:08:54.026416.026416 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:54.026838.026838 lmp.py:1912] 
DEBUG 01-15 16:08:54.026838.026838 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:54.026025.026025 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:54.026675.026675 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:54.026225.026225 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:54.026106.026106 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:54.026511.026511 lmp.py:1917] 
DEBUG 01-15 16:08:54.026511.026511 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:54.026869.026869 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:54.026281.026281 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:08:54.026447.026447 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:08:54.026659.026659 lmp.py:1935]   Expert 11 |     28 | CPU
DEBUG 01-15 16:08:54.026110.026110 lmp.py:1935]   Expert  7 |     39 | CPU
DEBUG 01-15 16:08:54.026846.026846 lmp.py:1935]   Expert 62 |     41 | CPU
DEBUG 01-15 16:08:54.026820.026820 lmp.py:1935]   Expert  3 |     45 | CPU
DEBUG 01-15 16:08:54.026794.026794 lmp.py:1935]   Expert 51 |     45 | CPU
DEBUG 01-15 16:08:54.026529.026529 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:08:54.026265.026265 lmp.py:1935]   Expert 17 |     53 | CPU
DEBUG 01-15 16:08:54.026001.026001 lmp.py:1935]   Expert 29 |     58 | CPU
DEBUG 01-15 16:08:54.026736.026736 lmp.py:1935]   Expert  6 |     60 | CPU
DEBUG 01-15 16:08:54.026472.026472 lmp.py:1935]   Expert  9 |     65 | CPU
DEBUG 01-15 16:08:54.026400.026400 lmp.py:1935]   Expert 38 |     75 | CPU
DEBUG 01-15 16:08:54.026327.026327 lmp.py:1935]   Expert 63 |     78 | CPU
DEBUG 01-15 16:08:54.027255.027255 lmp.py:1935]   Expert 55 |     79 | CPU
DEBUG 01-15 16:08:54.027706.027706 lmp.py:1935]   Expert 59 |     82 | CPU
DEBUG 01-15 16:08:54.027157.027157 lmp.py:1935]   Expert 48 |     93 | CPU
DEBUG 01-15 16:08:54.027561.027561 lmp.py:1935]   Expert 19 |     95 | CPU
DEBUG 01-15 16:08:54.027774.027774 lmp.py:1935]   Expert  8 |     97 | CPU
DEBUG 01-15 16:08:54.027033.027033 lmp.py:1935]   Expert 22 |    104 | CPU
DEBUG 01-15 16:08:54.027768.027768 lmp.py:1935]   Expert 49 |    104 | CPU
DEBUG 01-15 16:08:54.027265.027265 lmp.py:1935]   Expert 24 |    111 | CPU
DEBUG 01-15 16:08:54.027001.027001 lmp.py:1935]   Expert 36 |    114 | CPU
DEBUG 01-15 16:08:54.027498.027498 lmp.py:1935]   Expert 42 |    116 | CPU
DEBUG 01-15 16:08:54.027234.027234 lmp.py:1935]   Expert 34 |    117 | CPU
DEBUG 01-15 16:08:54.027115.027115 lmp.py:1935]   Expert 50 |    120 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027520.027520 lmp.py:1935]   Expert 39 |    125 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027401.027401 lmp.py:1935]   Expert  4 |    133 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027044.027044 lmp.py:1935]   Expert 37 |    142 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027926.027926 lmp.py:1935]   Expert 15 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027092.027092 lmp.py:1935]   Expert 41 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027258.027258 lmp.py:1935]   Expert 23 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027901.027901 lmp.py:1935]   Expert 56 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027590.027590 lmp.py:1935]   Expert 16 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027041.027041 lmp.py:1935]   Expert 60 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027492.027492 lmp.py:1935]   Expert 44 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027943.027943 lmp.py:1935]   Expert  1 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027394.027394 lmp.py:1935]   Expert 21 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027229.027229 lmp.py:1935]   Expert 43 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027541.027541 lmp.py:1935]   Expert 53 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027376.027376 lmp.py:1935]   Expert 47 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027688.027688 lmp.py:1935]   Expert 12 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027238.027238 lmp.py:1935]   Expert 33 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027742.027742 lmp.py:1935]   Expert 13 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027054.027054 lmp.py:1935]   Expert 32 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027319.027319 lmp.py:1935]   Expert 28 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027154.027154 lmp.py:1935]   Expert  0 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027466.027466 lmp.py:1935]   Expert 31 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027063.027063 lmp.py:1935]   Expert 54 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027613.027613 lmp.py:1935]   Expert 26 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027210.027210 lmp.py:1935]   Expert 10 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027760.027760 lmp.py:1935]   Expert 18 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027595.027595 lmp.py:1935]   Expert 57 |    273 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027629.027629 lmp.py:1935]   Expert  2 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027180.027180 lmp.py:1935]   Expert 58 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027492.027492 lmp.py:1935]   Expert 40 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027042.027042 lmp.py:1935]   Expert 25 |    361 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027831.027831 lmp.py:1935]   Expert 45 |    361 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027666.027666 lmp.py:1935]   Expert  5 |    442 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027978.027978 lmp.py:1935]   Expert 35 |    465 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027051.027051 lmp.py:1935]   Expert 27 |    488 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027601.027601 lmp.py:1935]   Expert 46 |    552 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027675.027675 lmp.py:1935]   Expert 52 |    593 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.027748.027748 lmp.py:1935]   Expert 14 |    883 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.027630.027630 lmp.py:1937] 
DEBUG 01-15 16:08:54.027630.027630 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:54.027465.027465 lmp.py:1938]   CPU:   1773 tokens
DEBUG 01-15 16:08:54.027300.027300 lmp.py:1942]   cuda:1:   5247 tokens (19 experts)
DEBUG 01-15 16:08:54.027612.027612 lmp.py:1942]   cuda:2:   5268 tokens (20 experts)
DEBUG 01-15 16:08:54.027208.027208 lmp.py:1943]   Total GPU:  10515 tokens
DEBUG 01-15 16:08:54.028805.028805 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:54.028805.028805 lmp.py:1944] 
DEBUG 01-15 16:08:54.028885.028885 cuda_h.py:19] end experts_map_get cost 0.0017018318176269531 seconds
DEBUG 01-15 16:08:54.028828.028828 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:54.028915.028915 lmp.py:1953] 
DEBUG 01-15 16:08:54.028915.028915 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:54.028606.028606 cuda_h.py:19] end cpu_experts_submit cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:08:54.028302.028302 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:54.028999.028999 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:54.029411.029411 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.32.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.32.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.32.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.58.up_proj.weight': 322961408}, 2: {'model.layers.26.mlp.experts.2.gate_proj.weight': 0, 'model.layers.26.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.28.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.28.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.28.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.31.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.31.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.31.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.53.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.53.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.53.up_proj.weight': 322961408, 'model.layers.26.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.26.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.26.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (30560223232, 5767168, 17301504, 0), (30565990400, 5767168, 23068672, 0), (30554456064, 5767168, 28835840, 0), (30629429248, 5767168, 34603008, 0), (30635196416, 5767168, 40370176, 0), (30623662080, 5767168, 46137344, 0), (30785142784, 5767168, 51904512, 0), (30790909952, 5767168, 57671680, 0), (30779375616, 5767168, 63438848, 0), (30819745792, 5767168, 69206016, 0), (30825512960, 5767168, 74973184, 0), (30813978624, 5767168, 80740352, 0), (30854348800, 5767168, 86507520, 0), (30860115968, 5767168, 92274688, 0), (30848581632, 5767168, 98041856, 0), (31010062336, 5767168, 103809024, 0), (31015829504, 5767168, 109576192, 0), (31004295168, 5767168, 115343360, 0), (31096569856, 5767168, 121110528, 0), (31102337024, 5767168, 126877696, 0), (31090802688, 5767168, 132644864, 0), (31113871360, 5767168, 138412032, 0), (31119638528, 5767168, 144179200, 0), (31108104192, 5767168, 149946368, 0), (31183077376, 5767168, 155713536, 0), (31188844544, 5767168, 161480704, 0), (31177310208, 5767168, 167247872, 0), (31217680384, 5767168, 173015040, 0), (31223447552, 5767168, 178782208, 0), (31211913216, 5767168, 184549376, 0), (31252283392, 5767168, 190316544, 0), (31258050560, 5767168, 196083712, 0), (31246516224, 5767168, 201850880, 0), (31286886400, 5767168, 207618048, 0), (31292653568, 5767168, 213385216, 0), (31281119232, 5767168, 219152384, 0), (31321489408, 5767168, 224919552, 0), (31327256576, 5767168, 230686720, 0), (31315722240, 5767168, 236453888, 0), (31356092416, 5767168, 242221056, 0), (31361859584, 5767168, 247988224, 0), (31350325248, 5767168, 253755392, 0), (31477202944, 5767168, 259522560, 0), (31482970112, 5767168, 265289728, 0), (31471435776, 5767168, 271056896, 0), (31511805952, 5767168, 276824064, 0), (31517573120, 5767168, 282591232, 0), (31506038784, 5767168, 288358400, 0), (31529107456, 5767168, 294125568, 0), (31534874624, 5767168, 299892736, 0), (31523340288, 5767168, 305659904, 0), (31546408960, 5767168, 311427072, 0), (31552176128, 5767168, 317194240, 0), (31540641792, 5767168, 322961408, 0)], 2: [(30577524736, 5767168, 0, 0), (30583291904, 5767168, 5767168, 0), (30571757568, 5767168, 11534336, 0), (30612127744, 5767168, 17301504, 0), (30617894912, 5767168, 23068672, 0), (30606360576, 5767168, 28835840, 0), (30715936768, 5767168, 34603008, 0), (30721703936, 5767168, 40370176, 0), (30710169600, 5767168, 46137344, 0), (30750539776, 5767168, 51904512, 0), (30756306944, 5767168, 57671680, 0), (30744772608, 5767168, 63438848, 0), (30767841280, 5767168, 69206016, 0), (30773608448, 5767168, 74973184, 0), (30762074112, 5767168, 80740352, 0), (30802444288, 5767168, 86507520, 0), (30808211456, 5767168, 92274688, 0), (30796677120, 5767168, 98041856, 0), (30906253312, 5767168, 103809024, 0), (30912020480, 5767168, 109576192, 0), (30900486144, 5767168, 115343360, 0), (30940856320, 5767168, 121110528, 0), (30946623488, 5767168, 126877696, 0), (30935089152, 5767168, 132644864, 0), (30975459328, 5767168, 138412032, 0), (30981226496, 5767168, 144179200, 0), (30969692160, 5767168, 149946368, 0), (30992760832, 5767168, 155713536, 0), (30998528000, 5767168, 161480704, 0), (30986993664, 5767168, 167247872, 0), (31027363840, 5767168, 173015040, 0), (31033131008, 5767168, 178782208, 0), (31021596672, 5767168, 184549376, 0), (31079268352, 5767168, 190316544, 0), (31085035520, 5767168, 196083712, 0), (31073501184, 5767168, 201850880, 0), (31148474368, 5767168, 207618048, 0), (31154241536, 5767168, 213385216, 0), (31142707200, 5767168, 219152384, 0), (31234981888, 5767168, 224919552, 0), (31240749056, 5767168, 230686720, 0), (31229214720, 5767168, 236453888, 0), (31304187904, 5767168, 242221056, 0), (31309955072, 5767168, 247988224, 0), (31298420736, 5767168, 253755392, 0), (31338790912, 5767168, 259522560, 0), (31344558080, 5767168, 265289728, 0), (31333023744, 5767168, 271056896, 0), (31407996928, 5767168, 276824064, 0), (31413764096, 5767168, 282591232, 0), (31402229760, 5767168, 288358400, 0), (31442599936, 5767168, 294125568, 0), (31448367104, 5767168, 299892736, 0), (31436832768, 5767168, 305659904, 0), (31459901440, 5767168, 311427072, 0), (31465668608, 5767168, 317194240, 0), (31454134272, 5767168, 322961408, 0), (31581011968, 5767168, 328728576, 0), (31586779136, 5767168, 334495744, 0), (31575244800, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729ec0>, 2: <capsule object NULL at 0x74a660729d10>}
DEBUG 01-15 16:08:54.029325.029325 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:54.029181.029181 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:54.029176.029176 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2a6375f3-79c9-4f61-87c9-1c8ef6543385
DEBUG 01-15 16:08:54.029987.029987 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:54.029865.029865 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:54.029576.029576 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:08:54.030577.030577 client.py:127] Model loaded
DEBUG 01-15 16:08:54.030906.030906 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:54.030627.030627 cuda_h.py:19] end move_flatidxs cost 0.0008611679077148438 seconds
DEBUG 01-15 16:08:54.030854.030854 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:08:54.031146.031146 cuda_h.py:19] end restore2model cost 0.001134634017944336 seconds
INFO 01-15 16:08:54.031900.031900 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2a6375f3-79c9-4f61-87c9-1c8ef6543385
DEBUG 01-15 16:08:54.031176.031176 cuda_h.py:19] end sllm_worker_task cost 0.012923002243041992 seconds
DEBUG 01-15 16:08:54.032254.032254 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004006624221801758 seconds
DEBUG 01-15 16:08:54.032228.032228 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:54.035394.035394 cuda_h.py:19] end restore2model cost 0.003137826919555664 seconds
DEBUG 01-15 16:08:54.035252.035252 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00759434700012207 seconds
DEBUG 01-15 16:08:54.035716.035716 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:54.036376.036376 cuda_h.py:19] end gpu_sexperts cost 0.0002796649932861328 seconds
DEBUG 01-15 16:08:54.036543.036543 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:08:54.036518.036518 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2172927856445312e-05 seconds
DEBUG 01-15 16:08:54.036930.036930 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:54.036871.036871 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:54.037765.037765 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009448528289794922 seconds
DEBUG 01-15 16:08:54.037038.037038 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:54.037490.037490 cuda_h.py:19] end gpu_group_list cost 0.00019860267639160156 seconds
DEBUG 01-15 16:08:54.038643.038643 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:54.039739.039739 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010304450988769531 seconds
DEBUG 01-15 16:08:54.039151.039151 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:54.039716.039716 cuda_h.py:19] end gpu_group_list cost 0.00021147727966308594 seconds
DEBUG 01-15 16:08:54.040335.040335 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:54.040311.040311 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2a6375f3-79c9-4f61-87c9-1c8ef6543385
DEBUG 01-15 16:08:54.040751.040751 cuda_h.py:19] end group_tensors cost 0.009921073913574219 seconds
DEBUG 01-15 16:08:54.041859.041859 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:54.044524.044524 cuda_h.py:19] end group pad cost 0.0029926300048828125 seconds
DEBUG 01-15 16:08:54.044566.044566 cuda_h.py:10] start group_einsum
INFO 01-15 16:08:54.065463.065463 client.py:127] Model loaded
DEBUG 01-15 16:08:54.066859.066859 cuda_h.py:19] end wait_experts_multi_device cost 0.02545952796936035 seconds
DEBUG 01-15 16:08:54.066721.066721 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:54.066386.066386 cuda_h.py:19] end group_einsum cost 0.02124619483947754 seconds
DEBUG 01-15 16:08:54.066516.066516 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:08:54.068744.068744 cuda_h.py:19] end get_outputs_cpu1 cost 0.0022096633911132812 seconds
DEBUG 01-15 16:08:54.069920.069920 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039655447006225586 seconds
DEBUG 01-15 16:08:54.069528.069528 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.003865957260131836 seconds
DEBUG 01-15 16:08:54.070545.070545 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:54.070396.070396 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:54.071275.071275 cuda_h.py:19] end index_scatter cost 8.225440979003906e-05 seconds
DEBUG 01-15 16:08:54.071139.071139 cuda_h.py:19] end cpuoutputsdeal cost 0.0012764930725097656 seconds
DEBUG 01-15 16:08:54.071639.071639 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:54.071448.071448 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:54.071480.071480 cuda_h.py:19] end gpu_group_tensor cost 0.00013566017150878906 seconds
DEBUG 01-15 16:08:54.071858.071858 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:54.071638.071638 cuda_h.py:19] end gpu_group_tensor cost 0.00012826919555664062 seconds
DEBUG 01-15 16:08:54.071781.071781 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:54.072942.072942 cuda_h.py:19] end gpu_group_einsum cost 0.0005970001220703125 seconds
DEBUG 01-15 16:08:54.072118.072118 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:54.073933.073933 cuda_h.py:19] end gpu_group_einsum cost 0.0007407665252685547 seconds
DEBUG 01-15 16:08:54.073795.073795 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:54.073759.073759 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:54.074077.074077 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016021728515625 seconds
DEBUG 01-15 16:08:54.074402.074402 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:54.074763.074763 cuda_h.py:19] end concat_expert_out cost 5.936622619628906e-05 seconds
DEBUG 01-15 16:08:54.074236.074236 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:54.074087.074087 cuda_h.py:19] end index_scatter cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:08:54.074147.074147 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00077056884765625 seconds
DEBUG 01-15 16:08:54.074984.074984 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:54.074589.074589 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:54.074925.074925 cuda_h.py:19] end all_expert_outputs_slices cost 0.00011444091796875 seconds
DEBUG 01-15 16:08:54.074913.074913 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:54.075736.075736 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:08:54.075050.075050 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:54.075635.075635 cuda_h.py:19] end index_scatter cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:08:54.075583.075583 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004379749298095703 seconds
DEBUG 01-15 16:08:54.075977.075977 cuda_h.py:19] end gpu_experts_multi_device cost 0.03895211219787598 seconds
DEBUG 01-15 16:08:54.075171.075171 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.04988884925842285 seconds
DEBUG 01-15 16:08:54.075237.075237 cuda_h.py:19] end prefill_layer cost 0.057567596435546875 seconds
DEBUG 01-15 16:08:54.075875.075875 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:08:54.075240.075240 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:08:54.075843.075843 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:08:54.075208.075208 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:08:54.075218.075218 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:08:54.076836.076836 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:08:54.078941.078941 cuda_h.py:19] end self_attn cost 0.002481698989868164 seconds
DEBUG 01-15 16:08:54.079314.079314 cuda_h.py:19] end iln_self_attn_paln cost 0.0031385421752929688 seconds
DEBUG 01-15 16:08:54.079952.079952 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:08:54.079615.079615 cuda_h.py:10] start gate
DEBUG 01-15 16:08:54.079330.079330 cuda_h.py:19] end gate cost 0.0005631446838378906 seconds
DEBUG 01-15 16:08:54.079398.079398 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:08:54.080310.080310 lmp.py:1912] 
DEBUG 01-15 16:08:54.080310.080310 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:08:54.080973.080973 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:08:54.080053.080053 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:08:54.080081.080081 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:08:54.080962.080962 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:08:54.080651.080651 lmp.py:1917] 
DEBUG 01-15 16:08:54.080651.080651 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:08:54.080056.080056 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:08:54.080659.080659 lmp.py:1935]   Expert 18 |     66 | CPU
DEBUG 01-15 16:08:54.080541.080541 lmp.py:1935]   Expert 47 |     70 | CPU
DEBUG 01-15 16:08:54.080707.080707 lmp.py:1935]   Expert 54 |     72 | CPU
DEBUG 01-15 16:08:54.080873.080873 lmp.py:1935]   Expert 23 |     74 | CPU
DEBUG 01-15 16:08:54.080039.080039 lmp.py:1935]   Expert 48 |     78 | CPU
DEBUG 01-15 16:08:54.080159.080159 lmp.py:1935]   Expert 44 |     88 | CPU
DEBUG 01-15 16:08:54.080279.080279 lmp.py:1935]   Expert 45 |     88 | CPU
DEBUG 01-15 16:08:54.080968.080968 lmp.py:1935]   Expert 20 |     92 | CPU
DEBUG 01-15 16:08:54.080896.080896 lmp.py:1935]   Expert 31 |     96 | CPU
DEBUG 01-15 16:08:54.082790.082790 lmp.py:1935]   Expert 36 |    105 | CPU
DEBUG 01-15 16:08:54.082182.082182 lmp.py:1935]   Expert 61 |    111 | CPU
DEBUG 01-15 16:08:54.082779.082779 lmp.py:1935]   Expert 33 |    118 | CPU
DEBUG 01-15 16:08:54.082899.082899 lmp.py:1935]   Expert 42 |    118 | CPU
DEBUG 01-15 16:08:54.082065.082065 lmp.py:1935]   Expert 10 |    120 | CPU
DEBUG 01-15 16:08:54.082469.082469 lmp.py:1935]   Expert 24 |    123 | CPU
DEBUG 01-15 16:08:54.082874.082874 lmp.py:1935]   Expert 43 |    123 | CPU
DEBUG 01-15 16:08:54.082563.082563 lmp.py:1935]   Expert 11 |    126 | CPU
DEBUG 01-15 16:08:54.082491.082491 lmp.py:1935]   Expert 49 |    126 | CPU
DEBUG 01-15 16:08:54.082942.082942 lmp.py:1935]   Expert 56 |    132 | CPU
DEBUG 01-15 16:08:54.082154.082154 lmp.py:1935]   Expert  6 |    135 | CPU
DEBUG 01-15 16:08:54.082082.082082 lmp.py:1935]   Expert 51 |    144 | CPU
DEBUG 01-15 16:08:54.082771.082771 lmp.py:1935]   Expert 17 |    149 | CPU
DEBUG 01-15 16:08:54.082460.082460 lmp.py:1935]   Expert  0 |    150 | CPU
DEBUG 01-15 16:08:54.082673.082673 lmp.py:1935]   Expert  5 |    152 | CPU
DEBUG 01-15 16:08:54.082124.082124 lmp.py:1935]   Expert 12 |    158 | CPU
DEBUG 01-15 16:08:54.082866.082866 lmp.py:1935]   Expert 40 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082463.082463 lmp.py:1935]   Expert 55 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082106.082106 lmp.py:1935]   Expert 57 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082941.082941 lmp.py:1935]   Expert 26 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082061.082061 lmp.py:1935]   Expert 59 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082419.082419 lmp.py:1935]   Expert 46 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082016.082016 lmp.py:1935]   Expert 38 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082135.082135 lmp.py:1935]   Expert 13 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082778.082778 lmp.py:1935]   Expert 30 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082898.082898 lmp.py:1935]   Expert 58 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082303.082303 lmp.py:1935]   Expert 35 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082230.082230 lmp.py:1935]   Expert 50 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082350.082350 lmp.py:1935]   Expert  7 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082516.082516 lmp.py:1935]   Expert 16 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082921.082921 lmp.py:1935]   Expert 15 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082087.082087 lmp.py:1935]   Expert 32 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082015.082015 lmp.py:1935]   Expert 14 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082419.082419 lmp.py:1935]   Expert  1 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082539.082539 lmp.py:1935]   Expert  3 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082421.082421 lmp.py:1935]   Expert  4 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082302.082302 lmp.py:1935]   Expert 39 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082183.082183 lmp.py:1935]   Expert 34 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082542.082542 lmp.py:1935]   Expert 28 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082854.082854 lmp.py:1935]   Expert 52 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082735.082735 lmp.py:1935]   Expert 25 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082378.082378 lmp.py:1935]   Expert 22 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082021.082021 lmp.py:1935]   Expert  2 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082949.082949 lmp.py:1935]   Expert 41 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082876.082876 lmp.py:1935]   Expert 21 |    281 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.082804.082804 lmp.py:1935]   Expert 60 |    284 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.082970.082970 lmp.py:1935]   Expert 63 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.083136.083136 lmp.py:1935]   Expert 62 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.083064.083064 lmp.py:1935]   Expert 29 |    297 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.083230.083230 lmp.py:1935]   Expert 27 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.083396.083396 lmp.py:1935]   Expert 37 |    328 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.083324.083324 lmp.py:1935]   Expert 53 |    333 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.083252.083252 lmp.py:1935]   Expert  8 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.083656.083656 lmp.py:1935]   Expert 19 |    440 | GPU2(cuda:2)
DEBUG 01-15 16:08:54.083822.083822 lmp.py:1935]   Expert  9 |    614 | GPU1(cuda:1)
DEBUG 01-15 16:08:54.083386.083386 lmp.py:1937] 
DEBUG 01-15 16:08:54.083386.083386 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:08:54.083221.083221 lmp.py:1938]   CPU:   2814 tokens
DEBUG 01-15 16:08:54.083341.083341 lmp.py:1942]   cuda:1:   4687 tokens (19 experts)
DEBUG 01-15 16:08:54.083269.083269 lmp.py:1942]   cuda:2:   4787 tokens (20 experts)
DEBUG 01-15 16:08:54.083481.083481 lmp.py:1943]   Total GPU:   9474 tokens
DEBUG 01-15 16:08:54.083661.083661 lmp.py:1944] ============================================================
DEBUG 01-15 16:08:54.083661.083661 lmp.py:1944] 
DEBUG 01-15 16:08:54.083072.083072 cuda_h.py:19] end experts_map_get cost 0.003487110137939453 seconds
DEBUG 01-15 16:08:54.083267.083267 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:08:54.083546.083546 lmp.py:1953] 
DEBUG 01-15 16:08:54.083546.083546 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:08:54.083713.083713 cuda_h.py:19] end cpu_experts_submit cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:08:54.083886.083886 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:08:54.083438.083438 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:08:54.084705.084705 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.1.gate_proj.weight': 0, 'model.layers.27.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.32.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.32.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.32.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.34.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.34.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.34.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.41.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.41.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.41.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.46.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.46.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.46.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.50.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.50.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.50.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.52.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.52.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.52.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.53.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.53.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.53.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.57.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.57.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.57.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.58.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.58.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.58.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.27.mlp.experts.2.gate_proj.weight': 0, 'model.layers.27.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.28.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.28.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.28.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.30.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.30.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.30.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.37.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.37.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.37.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.38.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.38.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.38.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.39.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.39.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.39.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.40.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.40.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.40.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.55.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.55.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.55.up_proj.weight': 322961408, 'model.layers.27.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.27.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.27.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(31667519488, 5767168, 0, 0), (31673286656, 5767168, 5767168, 0), (31661752320, 5767168, 11534336, 0), (31719424000, 5767168, 17301504, 0), (31725191168, 5767168, 23068672, 0), (31713656832, 5767168, 28835840, 0), (31805931520, 5767168, 34603008, 0), (31811698688, 5767168, 40370176, 0), (31800164352, 5767168, 46137344, 0), (31875137536, 5767168, 51904512, 0), (31880904704, 5767168, 57671680, 0), (31869370368, 5767168, 63438848, 0), (31927042048, 5767168, 69206016, 0), (31932809216, 5767168, 74973184, 0), (31921274880, 5767168, 80740352, 0), (32030851072, 5767168, 86507520, 0), (32036618240, 5767168, 92274688, 0), (32025083904, 5767168, 98041856, 0), (32117358592, 5767168, 103809024, 0), (32123125760, 5767168, 109576192, 0), (32111591424, 5767168, 115343360, 0), (32203866112, 5767168, 121110528, 0), (32209633280, 5767168, 126877696, 0), (32198098944, 5767168, 132644864, 0), (32238469120, 5767168, 138412032, 0), (32244236288, 5767168, 144179200, 0), (32232701952, 5767168, 149946368, 0), (32359579648, 5767168, 155713536, 0), (32365346816, 5767168, 161480704, 0), (32353812480, 5767168, 167247872, 0), (32446087168, 5767168, 173015040, 0), (32451854336, 5767168, 178782208, 0), (32440320000, 5767168, 184549376, 0), (32515293184, 5767168, 190316544, 0), (32521060352, 5767168, 196083712, 0), (32509526016, 5767168, 201850880, 0), (32549896192, 5767168, 207618048, 0), (32555663360, 5767168, 213385216, 0), (32544129024, 5767168, 219152384, 0), (32567197696, 5767168, 224919552, 0), (32572964864, 5767168, 230686720, 0), (32561430528, 5767168, 236453888, 0), (32636403712, 5767168, 242221056, 0), (32642170880, 5767168, 247988224, 0), (32630636544, 5767168, 253755392, 0), (32653705216, 5767168, 259522560, 0), (32659472384, 5767168, 265289728, 0), (32647938048, 5767168, 271056896, 0), (32671006720, 5767168, 276824064, 0), (32676773888, 5767168, 282591232, 0), (32665239552, 5767168, 288358400, 0), (32688308224, 5767168, 294125568, 0), (32694075392, 5767168, 299892736, 0), (32682541056, 5767168, 305659904, 0), (32722911232, 5767168, 311427072, 0), (32728678400, 5767168, 317194240, 0), (32717144064, 5767168, 322961408, 0)], 2: [(31684820992, 5767168, 0, 0), (31690588160, 5767168, 5767168, 0), (31679053824, 5767168, 11534336, 0), (31702122496, 5767168, 17301504, 0), (31707889664, 5767168, 23068672, 0), (31696355328, 5767168, 28835840, 0), (31771328512, 5767168, 34603008, 0), (31777095680, 5767168, 40370176, 0), (31765561344, 5767168, 46137344, 0), (31788630016, 5767168, 51904512, 0), (31794397184, 5767168, 57671680, 0), (31782862848, 5767168, 63438848, 0), (31892439040, 5767168, 69206016, 0), (31898206208, 5767168, 74973184, 0), (31886671872, 5767168, 80740352, 0), (31909740544, 5767168, 86507520, 0), (31915507712, 5767168, 92274688, 0), (31903973376, 5767168, 98041856, 0), (31978946560, 5767168, 103809024, 0), (31984713728, 5767168, 109576192, 0), (31973179392, 5767168, 115343360, 0), (32013549568, 5767168, 121110528, 0), (32019316736, 5767168, 126877696, 0), (32007782400, 5767168, 132644864, 0), (32082755584, 5767168, 138412032, 0), (32088522752, 5767168, 144179200, 0), (32076988416, 5767168, 149946368, 0), (32100057088, 5767168, 155713536, 0), (32105824256, 5767168, 161480704, 0), (32094289920, 5767168, 167247872, 0), (32134660096, 5767168, 173015040, 0), (32140427264, 5767168, 178782208, 0), (32128892928, 5767168, 184549376, 0), (32151961600, 5767168, 190316544, 0), (32157728768, 5767168, 196083712, 0), (32146194432, 5767168, 201850880, 0), (32169263104, 5767168, 207618048, 0), (32175030272, 5767168, 213385216, 0), (32163495936, 5767168, 219152384, 0), (32255770624, 5767168, 224919552, 0), (32261537792, 5767168, 230686720, 0), (32250003456, 5767168, 236453888, 0), (32290373632, 5767168, 242221056, 0), (32296140800, 5767168, 247988224, 0), (32284606464, 5767168, 253755392, 0), (32307675136, 5767168, 259522560, 0), (32313442304, 5767168, 265289728, 0), (32301907968, 5767168, 271056896, 0), (32324976640, 5767168, 276824064, 0), (32330743808, 5767168, 282591232, 0), (32319209472, 5767168, 288358400, 0), (32342278144, 5767168, 294125568, 0), (32348045312, 5767168, 299892736, 0), (32336510976, 5767168, 305659904, 0), (32601800704, 5767168, 311427072, 0), (32607567872, 5767168, 317194240, 0), (32596033536, 5767168, 322961408, 0), (32740212736, 5767168, 328728576, 0), (32745979904, 5767168, 334495744, 0), (32734445568, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6783a0900>, 2: <capsule object NULL at 0x74a6bc6a0660>}
DEBUG 01-15 16:08:54.084996.084996 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:54.084649.084649 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:54.084074.084074 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0a780afc-4cb2-4a17-b95e-70d0ad794ecf
DEBUG 01-15 16:08:54.084831.084831 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:08:54.085856.085856 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:08:54.085022.085022 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:08:54.086675.086675 cuda_h.py:19] end move_flatidxs cost 0.0008485317230224609 seconds
DEBUG 01-15 16:08:54.086120.086120 cuda_h.py:10] start group_tensors
INFO 01-15 16:08:54.086614.086614 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0a780afc-4cb2-4a17-b95e-70d0ad794ecf
DEBUG 01-15 16:08:54.087961.087961 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003513336181640625 seconds
DEBUG 01-15 16:08:54.087632.087632 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:54.090744.090744 cuda_h.py:19] end restore2model cost 0.0031027793884277344 seconds
DEBUG 01-15 16:08:54.090295.090295 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006844758987426758 seconds
DEBUG 01-15 16:08:54.090065.090065 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:08:54.090189.090189 cuda_h.py:19] end gpu_sexperts cost 0.00030541419982910156 seconds
DEBUG 01-15 16:08:54.090588.090588 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:08:54.090205.090205 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:54.090922.090922 cuda_h.py:19] end group_tensors cost 0.004520416259765625 seconds
DEBUG 01-15 16:08:54.091428.091428 cuda_h.py:10] start group pad
DEBUG 01-15 16:08:54.091154.091154 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010154247283935547 seconds
DEBUG 01-15 16:08:54.091892.091892 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:54.092615.092615 cuda_h.py:19] end gpu_group_list cost 0.00022125244140625 seconds
DEBUG 01-15 16:08:54.093742.093742 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:08:54.094670.094670 cuda_h.py:19] end group pad cost 0.0030002593994140625 seconds
DEBUG 01-15 16:08:54.094844.094844 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:08:54.094898.094898 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012297630310058594 seconds
DEBUG 01-15 16:08:54.094457.094457 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:08:54.094173.094173 cuda_h.py:19] end gpu_group_list cost 0.00021719932556152344 seconds
DEBUG 01-15 16:08:54.095753.095753 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:08:54.096196.096196 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0a780afc-4cb2-4a17-b95e-70d0ad794ecf
DEBUG 01-15 16:08:54.122278.122278 cuda_h.py:19] end group_einsum cost 0.0278928279876709 seconds
DEBUG 01-15 16:08:54.122840.122840 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:08:54.125602.125602 client.py:127] Model loaded
DEBUG 01-15 16:08:54.126977.126977 cuda_h.py:19] end wait_experts_multi_device cost 0.030047178268432617 seconds
DEBUG 01-15 16:08:54.126813.126813 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:08:54.126987.126987 cuda_h.py:19] end get_outputs_cpu1 cost 0.0037355422973632812 seconds
DEBUG 01-15 16:08:54.127313.127313 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04221224784851074 seconds
DEBUG 01-15 16:08:54.128868.128868 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0023360252380371094 seconds
DEBUG 01-15 16:08:54.128040.128040 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:08:54.131909.131909 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:54.131543.131543 cuda_h.py:19] end index_scatter cost 0.00017786026000976562 seconds
DEBUG 01-15 16:08:54.131792.131792 cuda_h.py:19] end cpuoutputsdeal cost 0.0029768943786621094 seconds
DEBUG 01-15 16:08:54.132301.132301 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:08:54.132708.132708 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:54.132066.132066 cuda_h.py:19] end gpu_group_tensor cost 0.0003216266632080078 seconds
DEBUG 01-15 16:08:54.132049.132049 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:08:54.132492.132492 cuda_h.py:19] end gpu_group_tensor cost 0.0002830028533935547 seconds
DEBUG 01-15 16:08:54.133321.133321 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:54.134133.134133 cuda_h.py:19] end gpu_group_einsum cost 0.0015773773193359375 seconds
DEBUG 01-15 16:08:54.135378.135378 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:08:54.136634.136634 cuda_h.py:19] end gpu_group_einsum cost 0.001171112060546875 seconds
DEBUG 01-15 16:08:54.136471.136471 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:54.137936.137936 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:54.137717.137717 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004582405090332031 seconds
DEBUG 01-15 16:08:54.137832.137832 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:54.138455.138455 cuda_h.py:19] end concat_expert_out cost 0.00013637542724609375 seconds
DEBUG 01-15 16:08:54.138164.138164 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:54.138736.138736 cuda_h.py:19] end index_scatter cost 0.00018596649169921875 seconds
DEBUG 01-15 16:08:54.139001.139001 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0020585060119628906 seconds
DEBUG 01-15 16:08:54.139154.139154 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:08:54.139582.139582 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:08:54.139590.139590 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003447532653808594 seconds
DEBUG 01-15 16:08:54.139500.139500 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:08:54.140910.140910 cuda_h.py:19] end concat_expert_out cost 0.00012254714965820312 seconds
DEBUG 01-15 16:08:54.140392.140392 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:08:54.140473.140473 cuda_h.py:19] end index_scatter cost 0.0001468658447265625 seconds
DEBUG 01-15 16:08:54.140748.140748 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012195110321044922 seconds
DEBUG 01-15 16:08:54.140555.140555 cuda_h.py:19] end gpu_experts_multi_device cost 0.05000782012939453 seconds
DEBUG 01-15 16:08:54.140475.140475 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.061858415603637695 seconds
DEBUG 01-15 16:08:54.142211.142211 cuda_h.py:19] end prefill_layer cost 0.0661468505859375 seconds
DEBUG 01-15 16:08:54.142315.142315 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:08:54.142501.142501 cuda_h.py:19] end prefill cost 2.71001935005188 seconds
DEBUG 01-15 16:08:56.352002.352002 cuda_h.py:10] start generate_input_ids
generate input ids cost 0.09575295448303223 s
DEBUG 01-15 16:08:56.726868.726868 cuda_h.py:19] end generate_input_ids cost 0.37290525436401367 seconds
DEBUG 01-15 16:08:56.726252.726252 cuda_h.py:10] start init_cache
DEBUG 01-15 16:08:56.727316.727316 cuda_h.py:19] end init_cache cost 9.489059448242188e-05 seconds
DEBUG 01-15 16:08:59.341663.341663 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:08:59.343565.343565 cuda_h.py:19] end init_meta_layer cost 1.33514404296875e-05 seconds
DEBUG 01-15 16:08:59.343925.343925 cuda_h.py:10] start init_weights
DEBUG 01-15 16:08:59.343681.343681 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:59.343682.343682 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:59.344265.344265 cuda_h.py:19] end allocate_cuda_memory cost 0.0009596347808837891 seconds
DEBUG 01-15 16:08:59.344267.344267 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:59.344838.344838 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:59.344091.344091 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:59.344032.344032 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 217aa2ff-44c1-4081-86d7-c27e88bf361a
DEBUG 01-15 16:08:59.344314.344314 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:59.346916.346916 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 217aa2ff-44c1-4081-86d7-c27e88bf361a
DEBUG 01-15 16:08:59.346660.346660 cuda_h.py:19] end load_into_gpu_async cost 0.0017993450164794922 seconds
DEBUG 01-15 16:08:59.346978.346978 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:59.346571.346571 cuda_h.py:19] end restore_tensors2 cost 5.8650970458984375e-05 seconds
DEBUG 01-15 16:08:59.346188.346188 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003053903579711914 seconds
DEBUG 01-15 16:08:59.346030.346030 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:59.346262.346262 cuda_h.py:19] end restore2model cost 0.00018215179443359375 seconds
INFO 01-15 16:08:59.346879.346879 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 217aa2ff-44c1-4081-86d7-c27e88bf361a
INFO 01-15 16:08:59.420048.420048 client.py:127] Model loaded
DEBUG 01-15 16:08:59.420298.420298 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:08:59.420734.420734 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:08:59.420434.420434 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:08:59.421683.421683 cuda_h.py:19] end allocate_cuda_memory cost 0.00040268898010253906 seconds
DEBUG 01-15 16:08:59.421787.421787 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:08:59.421094.421094 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:08:59.421992.421992 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:08:59.421041.421041 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b8436fb4-366e-4cec-ab13-69c5f8cf04f0
DEBUG 01-15 16:08:59.421801.421801 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:08:59.422537.422537 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b8436fb4-366e-4cec-ab13-69c5f8cf04f0
DEBUG 01-15 16:08:59.422660.422660 cuda_h.py:19] end load_into_gpu_async cost 0.0014643669128417969 seconds
DEBUG 01-15 16:08:59.422424.422424 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:08:59.423637.423637 cuda_h.py:19] end restore_tensors2 cost 0.000148773193359375 seconds
DEBUG 01-15 16:08:59.423329.423329 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002634286880493164 seconds
INFO 01-15 16:08:59.423623.423623 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b8436fb4-366e-4cec-ab13-69c5f8cf04f0
INFO 01-15 16:08:59.439635.439635 client.py:127] Model loaded
DEBUG 01-15 16:08:59.439415.439415 cuda_h.py:10] start restore2model
DEBUG 01-15 16:08:59.440193.440193 cuda_h.py:19] end restore2model cost 0.0009961128234863281 seconds
DEBUG 01-15 16:08:59.441953.441953 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.02064990997314453 seconds
DEBUG 01-15 16:08:59.441744.441744 cuda_h.py:19] end init_weights cost 0.09800457954406738 seconds
DEBUG 01-15 16:08:59.441600.441600 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:09:00.234631.234631 cuda_h.py:19] end copy_emodel cost 0.7929377555847168 seconds
DEBUG 01-15 16:09:00.235200.235200 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:09:00.235013.235013 cuda_h.py:19] end init_inputs_tokens cost 0.00030684471130371094 seconds
DEBUG 01-15 16:09:00.235643.235643 cuda_h.py:10] start prefill
DEBUG 01-15 16:09:00.235929.235929 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.235963.235963 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:09:00.235421.235421 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:09:00.235793.235793 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:09:00.235066.235066 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 3.409385681152344e-05 seconds
DEBUG 01-15 16:09:00.235862.235862 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 6.461143493652344e-05 seconds
DEBUG 01-15 16:09:00.235982.235982 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.235017.235017 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.235087.235087 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.236631.236631 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.236154.236154 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.236746.236746 cuda_h.py:19] end allocate_cuda_memory cost 0.00018787384033203125 seconds
DEBUG 01-15 16:09:00.236510.236510 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.236697.236697 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.236473.236473 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.236368.236368 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d80e49d8-5ad3-4ed1-831b-75cf458863ef
DEBUG 01-15 16:09:00.236789.236789 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.237348.237348 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.238395.238395 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d80e49d8-5ad3-4ed1-831b-75cf458863ef
DEBUG 01-15 16:09:00.238795.238795 cuda_h.py:19] end load_into_gpu_async cost 0.0015439987182617188 seconds
DEBUG 01-15 16:09:00.238326.238326 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.238774.238774 cuda_h.py:19] end restore_tensors2 cost 8.249282836914062e-05 seconds
DEBUG 01-15 16:09:00.238603.238603 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022754669189453125 seconds
INFO 01-15 16:09:00.238125.238125 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d80e49d8-5ad3-4ed1-831b-75cf458863ef
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.244347.244347 cuda_h.py:19] end self_attn cost 0.006563901901245117 seconds
DEBUG 01-15 16:09:00.244784.244784 cuda_h.py:19] end iln_self_attn_paln cost 0.008880853652954102 seconds
DEBUG 01-15 16:09:00.244627.244627 cuda_h.py:10] start dense_mlp
INFO 01-15 16:09:00.245519.245519 client.py:127] Model loaded
DEBUG 01-15 16:09:00.245138.245138 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.245784.245784 cuda_h.py:19] end restore2model cost 0.0006694793701171875 seconds
DEBUG 01-15 16:09:00.246311.246311 cuda_h.py:19] end sllm_worker_task cost 0.010035514831542969 seconds
DEBUG 01-15 16:09:00.246319.246319 cuda_h.py:19] end dense_mlp cost 0.0018310546875 seconds
DEBUG 01-15 16:09:00.246667.246667 cuda_h.py:19] end prefill_layer cost 0.011117935180664062 seconds
DEBUG 01-15 16:09:00.246059.246059 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:09:00.246100.246100 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.246538.246538 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:09:00.246248.246248 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:09:00.246388.246388 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:09:00.246708.246708 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 2.6941299438476562e-05 seconds
DEBUG 01-15 16:09:00.246471.246471 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 6.604194641113281e-05 seconds
DEBUG 01-15 16:09:00.246174.246174 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.246500.246500 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.247165.247165 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.247439.247439 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.247911.247911 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.247038.247038 cuda_h.py:19] end allocate_cuda_memory cost 0.00018334388732910156 seconds
DEBUG 01-15 16:09:00.247200.247200 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.247122.247122 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.247045.247045 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.247438.247438 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 06d136c6-9922-4886-adaf-4f5ff8e70a31
DEBUG 01-15 16:09:00.247302.247302 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.248815.248815 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.248691.248691 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 06d136c6-9922-4886-adaf-4f5ff8e70a31
DEBUG 01-15 16:09:00.248845.248845 cuda_h.py:19] end load_into_gpu_async cost 0.0012316703796386719 seconds
DEBUG 01-15 16:09:00.248330.248330 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.248169.248169 cuda_h.py:19] end restore_tensors2 cost 8.153915405273438e-05 seconds
DEBUG 01-15 16:09:00.248383.248383 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0018453598022460938 seconds
INFO 01-15 16:09:00.249504.249504 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 06d136c6-9922-4886-adaf-4f5ff8e70a31
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.252223.252223 cuda_h.py:19] end self_attn cost 0.004019260406494141 seconds
DEBUG 01-15 16:09:00.252427.252427 cuda_h.py:19] end iln_self_attn_paln cost 0.005556583404541016 seconds
DEBUG 01-15 16:09:00.252562.252562 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:09:00.252716.252716 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.253415.253415 cuda_h.py:19] end gate cost 0.0008628368377685547 seconds
DEBUG 01-15 16:09:00.253291.253291 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.253566.253566 lmp.py:1912] 
DEBUG 01-15 16:09:00.253566.253566 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.253991.253991 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.253356.253356 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.253144.253144 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.253026.253026 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.253954.253954 lmp.py:1917] 
DEBUG 01-15 16:09:00.253954.253954 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.253597.253597 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.254438.254438 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:09:00.254320.254320 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:09:00.254486.254486 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:09:00.254175.254175 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:09:00.254341.254341 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:09:00.254746.254746 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:09:00.254627.254627 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:09:00.254032.254032 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:09:00.254913.254913 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:09:00.254556.254556 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:09:00.254722.254722 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:09:00.254173.254173 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:09:00.254386.254386 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:09:00.254837.254837 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:09:00.254149.254149 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:09:00.254361.254361 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:09:00.254573.254573 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:09:00.254024.254024 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:09:00.254190.254190 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:09:00.254641.254641 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:09:00.254046.254046 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:09:00.254927.254927 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:09:00.254809.254809 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:09:00.254452.254452 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:09:00.254571.254571 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:09:00.254168.254168 lmp.py:1935]   Expert 34 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254811.254811 lmp.py:1935]   Expert 35 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254693.254693 lmp.py:1935]   Expert 36 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254335.254335 lmp.py:1935]   Expert 11 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254740.254740 lmp.py:1935]   Expert 17 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254621.254621 lmp.py:1935]   Expert 59 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254026.254026 lmp.py:1935]   Expert 10 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254907.254907 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254312.254312 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254955.254955 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254313.254313 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254148.254148 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254506.254506 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254149.254149 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254316.254316 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254720.254720 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254648.254648 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254529.254529 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254695.254695 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254100.254100 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254504.254504 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254909.254909 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254075.254075 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254149.254149 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254553.254553 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254719.254719 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254647.254647 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254813.254813 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254979.254979 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254145.254145 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.254788.254788 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.254147.254147 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.255266.255266 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.255102.255102 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.255460.255460 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.255341.255341 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.255507.255507 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.255673.255673 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.255363.255363 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.255052.255052 lmp.py:1937] 
DEBUG 01-15 16:09:00.255052.255052 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.255218.255218 lmp.py:1938]   CPU:   2872 tokens
DEBUG 01-15 16:09:00.255100.255100 lmp.py:1942]   cuda:1:   4628 tokens (19 experts)
DEBUG 01-15 16:09:00.255266.255266 lmp.py:1942]   cuda:2:   4788 tokens (20 experts)
DEBUG 01-15 16:09:00.255955.255955 lmp.py:1943]   Total GPU:   9416 tokens
DEBUG 01-15 16:09:00.255167.255167 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.255167.255167 lmp.py:1944] 
DEBUG 01-15 16:09:00.255579.255579 cuda_h.py:19] end experts_map_get cost 0.0016596317291259766 seconds
DEBUG 01-15 16:09:00.255502.255502 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.255258.255258 lmp.py:1953] 
DEBUG 01-15 16:09:00.255258.255258 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.255664.255664 cuda_h.py:19] end cpu_experts_submit cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:00.255406.255406 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.255759.255759 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.256549.256549 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.39.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.39.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.39.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.1.mlp.experts.2.gate_proj.weight': 0, 'model.layers.1.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.46.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.46.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.46.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.1.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.1.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.1.mlp.experts.59.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3016228864, 5767168, 34603008, 0), (3021996032, 5767168, 40370176, 0), (3010461696, 5767168, 46137344, 0), (3033530368, 5767168, 51904512, 0), (3039297536, 5767168, 57671680, 0), (3027763200, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3154640896, 5767168, 86507520, 0), (3160408064, 5767168, 92274688, 0), (3148873728, 5767168, 98041856, 0), (3189243904, 5767168, 103809024, 0), (3195011072, 5767168, 109576192, 0), (3183476736, 5767168, 115343360, 0), (3206545408, 5767168, 121110528, 0), (3212312576, 5767168, 126877696, 0), (3200778240, 5767168, 132644864, 0), (3275751424, 5767168, 138412032, 0), (3281518592, 5767168, 144179200, 0), (3269984256, 5767168, 149946368, 0), (3310354432, 5767168, 155713536, 0), (3316121600, 5767168, 161480704, 0), (3304587264, 5767168, 167247872, 0), (3362258944, 5767168, 173015040, 0), (3368026112, 5767168, 178782208, 0), (3356491776, 5767168, 184549376, 0), (3379560448, 5767168, 190316544, 0), (3385327616, 5767168, 196083712, 0), (3373793280, 5767168, 201850880, 0), (3466067968, 5767168, 207618048, 0), (3471835136, 5767168, 213385216, 0), (3460300800, 5767168, 219152384, 0), (3535273984, 5767168, 224919552, 0), (3541041152, 5767168, 230686720, 0), (3529506816, 5767168, 236453888, 0), (3690987520, 5767168, 242221056, 0), (3696754688, 5767168, 247988224, 0), (3685220352, 5767168, 253755392, 0), (3725590528, 5767168, 259522560, 0), (3731357696, 5767168, 265289728, 0), (3719823360, 5767168, 271056896, 0), (3846701056, 5767168, 276824064, 0), (3852468224, 5767168, 282591232, 0), (3840933888, 5767168, 288358400, 0), (3915907072, 5767168, 294125568, 0), (3921674240, 5767168, 299892736, 0), (3910139904, 5767168, 305659904, 0), (3950510080, 5767168, 311427072, 0), (3956277248, 5767168, 317194240, 0), (3944742912, 5767168, 322961408, 0)], 2: [(2895118336, 5767168, 0, 0), (2900885504, 5767168, 5767168, 0), (2889351168, 5767168, 11534336, 0), (2929721344, 5767168, 17301504, 0), (2935488512, 5767168, 23068672, 0), (2923954176, 5767168, 28835840, 0), (2947022848, 5767168, 34603008, 0), (2952790016, 5767168, 40370176, 0), (2941255680, 5767168, 46137344, 0), (2998927360, 5767168, 51904512, 0), (3004694528, 5767168, 57671680, 0), (2993160192, 5767168, 63438848, 0), (3050831872, 5767168, 69206016, 0), (3056599040, 5767168, 74973184, 0), (3045064704, 5767168, 80740352, 0), (3102736384, 5767168, 86507520, 0), (3108503552, 5767168, 92274688, 0), (3096969216, 5767168, 98041856, 0), (3120037888, 5767168, 103809024, 0), (3125805056, 5767168, 109576192, 0), (3114270720, 5767168, 115343360, 0), (3137339392, 5767168, 121110528, 0), (3143106560, 5767168, 126877696, 0), (3131572224, 5767168, 132644864, 0), (3223846912, 5767168, 138412032, 0), (3229614080, 5767168, 144179200, 0), (3218079744, 5767168, 149946368, 0), (3258449920, 5767168, 155713536, 0), (3264217088, 5767168, 161480704, 0), (3252682752, 5767168, 167247872, 0), (3431464960, 5767168, 173015040, 0), (3437232128, 5767168, 178782208, 0), (3425697792, 5767168, 184549376, 0), (3448766464, 5767168, 190316544, 0), (3454533632, 5767168, 196083712, 0), (3442999296, 5767168, 201850880, 0), (3483369472, 5767168, 207618048, 0), (3489136640, 5767168, 213385216, 0), (3477602304, 5767168, 219152384, 0), (3552575488, 5767168, 224919552, 0), (3558342656, 5767168, 230686720, 0), (3546808320, 5767168, 236453888, 0), (3587178496, 5767168, 242221056, 0), (3592945664, 5767168, 247988224, 0), (3581411328, 5767168, 253755392, 0), (3639083008, 5767168, 259522560, 0), (3644850176, 5767168, 265289728, 0), (3633315840, 5767168, 271056896, 0), (3656384512, 5767168, 276824064, 0), (3662151680, 5767168, 282591232, 0), (3650617344, 5767168, 288358400, 0), (3777495040, 5767168, 294125568, 0), (3783262208, 5767168, 299892736, 0), (3771727872, 5767168, 305659904, 0), (3829399552, 5767168, 311427072, 0), (3835166720, 5767168, 317194240, 0), (3823632384, 5767168, 322961408, 0), (3881304064, 5767168, 328728576, 0), (3887071232, 5767168, 334495744, 0), (3875536896, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729c20>, 2: <capsule object NULL at 0x74a8147146f0>}
INFO 01-15 16:09:00.256412.256412 client.py:127] Model loaded
DEBUG 01-15 16:09:00.256066.256066 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.256440.256440 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.256945.256945 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.257110.257110 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7fccca78-8ce7-4b7d-bef0-cefb96ccb5a2
DEBUG 01-15 16:09:00.257376.257376 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:00.257442.257442 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.257600.257600 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.258210.258210 cuda_h.py:19] end restore2model cost 0.001089334487915039 seconds
DEBUG 01-15 16:09:00.258961.258961 cuda_h.py:19] end sllm_worker_task cost 0.011056184768676758 seconds
INFO 01-15 16:09:00.258460.258460 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7fccca78-8ce7-4b7d-bef0-cefb96ccb5a2
DEBUG 01-15 16:09:00.258010.258010 cuda_h.py:19] end move_flatidxs cost 0.0008876323699951172 seconds
DEBUG 01-15 16:09:00.258522.258522 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:00.258188.258188 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034782886505126953 seconds
DEBUG 01-15 16:09:00.259826.259826 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.262421.262421 cuda_h.py:19] end restore2model cost 0.0030715465545654297 seconds
DEBUG 01-15 16:09:00.262701.262701 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006795644760131836 seconds
DEBUG 01-15 16:09:00.262020.262020 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.262135.262135 cuda_h.py:19] end gpu_sexperts cost 0.0002655982971191406 seconds
DEBUG 01-15 16:09:00.262342.262342 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.262019.262019 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5020370483398438e-05 seconds
DEBUG 01-15 16:09:00.262715.262715 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.262418.262418 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.263020.263020 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009405612945556641 seconds
DEBUG 01-15 16:09:00.263486.263486 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.263030.263030 cuda_h.py:19] end gpu_group_list cost 0.0001971721649169922 seconds
DEBUG 01-15 16:09:00.264864.264864 cuda_h.py:19] end group_tensors cost 0.005612611770629883 seconds
DEBUG 01-15 16:09:00.264615.264615 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.265424.265424 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.267011.267011 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016140937805175781 seconds
DEBUG 01-15 16:09:00.267040.267040 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.267228.267228 cuda_h.py:19] end gpu_group_list cost 0.0002434253692626953 seconds
DEBUG 01-15 16:09:00.268673.268673 cuda_h.py:19] end group pad cost 0.0032558441162109375 seconds
DEBUG 01-15 16:09:00.268416.268416 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:00.268811.268811 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.268786.268786 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7fccca78-8ce7-4b7d-bef0-cefb96ccb5a2
INFO 01-15 16:09:00.296261.296261 client.py:127] Model loaded
DEBUG 01-15 16:09:00.297546.297546 cuda_h.py:19] end wait_experts_multi_device cost 0.028795242309570312 seconds
DEBUG 01-15 16:09:00.297819.297819 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.299523.299523 cuda_h.py:19] end group_einsum cost 0.03075385093688965 seconds
DEBUG 01-15 16:09:00.299542.299542 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:00.302247.302247 cuda_h.py:19] end get_outputs_cpu1 cost 0.003199338912963867 seconds
DEBUG 01-15 16:09:00.303803.303803 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.046265602111816406 seconds
DEBUG 01-15 16:09:00.304944.304944 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007364749908447266 seconds
DEBUG 01-15 16:09:00.304613.304613 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.307260.307260 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.307023.307023 cuda_h.py:19] end index_scatter cost 0.0002925395965576172 seconds
DEBUG 01-15 16:09:00.308747.308747 cuda_h.py:19] end cpuoutputsdeal cost 0.0032570362091064453 seconds
DEBUG 01-15 16:09:00.308840.308840 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.308996.308996 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.310394.310394 cuda_h.py:19] end gpu_group_tensor cost 0.0012960433959960938 seconds
DEBUG 01-15 16:09:00.310902.310902 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.311593.311593 cuda_h.py:19] end gpu_group_tensor cost 0.0013318061828613281 seconds
DEBUG 01-15 16:09:00.311405.311405 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.313146.313146 cuda_h.py:19] end gpu_group_einsum cost 0.0018112659454345703 seconds
DEBUG 01-15 16:09:00.314924.314924 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.317326.317326 cuda_h.py:19] end gpu_group_einsum cost 0.0027551651000976562 seconds
DEBUG 01-15 16:09:00.317104.317104 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.317855.317855 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.318871.318871 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002295970916748047 seconds
DEBUG 01-15 16:09:00.318270.318270 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.318186.318186 cuda_h.py:19] end concat_expert_out cost 0.00020360946655273438 seconds
DEBUG 01-15 16:09:00.318316.318316 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.318779.318779 cuda_h.py:19] end index_scatter cost 0.00011301040649414062 seconds
DEBUG 01-15 16:09:00.319055.319055 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0015947818756103516 seconds
DEBUG 01-15 16:09:00.319072.319072 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.319426.319426 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.319929.319929 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001513957977294922 seconds
DEBUG 01-15 16:09:00.319361.319361 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.319802.319802 cuda_h.py:19] end concat_expert_out cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:09:00.319765.319765 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.319729.319729 cuda_h.py:19] end index_scatter cost 6.580352783203125e-05 seconds
DEBUG 01-15 16:09:00.319975.319975 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006184577941894531 seconds
DEBUG 01-15 16:09:00.319350.319350 cuda_h.py:19] end gpu_experts_multi_device cost 0.05729055404663086 seconds
DEBUG 01-15 16:09:00.320962.320962 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.06749582290649414 seconds
DEBUG 01-15 16:09:00.320704.320704 cuda_h.py:19] end prefill_layer cost 0.07396864891052246 seconds
DEBUG 01-15 16:09:00.320409.320409 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:09:00.320086.320086 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.320478.320478 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:09:00.320426.320426 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:09:00.320235.320235 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:09:00.321351.321351 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:00.321643.321643 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 9.441375732421875e-05 seconds
DEBUG 01-15 16:09:00.321929.321929 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.321449.321449 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.321158.321158 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.321658.321658 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.321811.321811 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.322168.322168 cuda_h.py:19] end allocate_cuda_memory cost 0.000316619873046875 seconds
DEBUG 01-15 16:09:00.322045.322045 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.322999.322999 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.322101.322101 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.322950.322950 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b7585615-67f1-42ce-b7fb-958c6bb9f36b
DEBUG 01-15 16:09:00.322470.322470 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.322037.322037 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.323124.323124 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b7585615-67f1-42ce-b7fb-958c6bb9f36b
DEBUG 01-15 16:09:00.323425.323425 cuda_h.py:19] end load_into_gpu_async cost 0.001224517822265625 seconds
DEBUG 01-15 16:09:00.323419.323419 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.323807.323807 cuda_h.py:19] end restore_tensors2 cost 8.20159912109375e-05 seconds
DEBUG 01-15 16:09:00.323801.323801 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0019252300262451172 seconds
INFO 01-15 16:09:00.323287.323287 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b7585615-67f1-42ce-b7fb-958c6bb9f36b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.326257.326257 cuda_h.py:19] end self_attn cost 0.0038950443267822266 seconds
DEBUG 01-15 16:09:00.327999.327999 cuda_h.py:19] end iln_self_attn_paln cost 0.006188154220581055 seconds
DEBUG 01-15 16:09:00.327922.327922 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:09:00.327381.327381 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.328323.328323 cuda_h.py:19] end gate cost 0.0007920265197753906 seconds
DEBUG 01-15 16:09:00.328060.328060 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.328365.328365 lmp.py:1912] 
DEBUG 01-15 16:09:00.328365.328365 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.328704.328704 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.328082.328082 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.328024.328024 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.328342.328342 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.328131.328131 lmp.py:1917] 
DEBUG 01-15 16:09:00.328131.328131 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.328966.328966 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.328762.328762 lmp.py:1935]   Expert 58 |     50 | CPU
DEBUG 01-15 16:09:00.328643.328643 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:09:00.328094.328094 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:09:00.328545.328545 lmp.py:1935]   Expert 17 |     84 | CPU
DEBUG 01-15 16:09:00.329195.329195 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:09:00.329798.329798 lmp.py:1935]   Expert 24 |     87 | CPU
DEBUG 01-15 16:09:00.329163.329163 lmp.py:1935]   Expert 28 |    104 | CPU
DEBUG 01-15 16:09:00.329766.329766 lmp.py:1935]   Expert 34 |    116 | CPU
DEBUG 01-15 16:09:00.329463.329463 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:09:00.329728.329728 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:09:00.329471.329471 lmp.py:1935]   Expert  9 |    130 | CPU
DEBUG 01-15 16:09:00.329114.329114 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:09:00.329849.329849 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:09:00.329585.329585 lmp.py:1935]   Expert 23 |    136 | CPU
DEBUG 01-15 16:09:00.329559.329559 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:09:00.329759.329759 lmp.py:1935]   Expert 30 |    144 | CPU
DEBUG 01-15 16:09:00.329170.329170 lmp.py:1935]   Expert 45 |    145 | CPU
DEBUG 01-15 16:09:00.329297.329297 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:09:00.329231.329231 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:09:00.329259.329259 lmp.py:1935]   Expert  1 |    152 | CPU
DEBUG 01-15 16:09:00.329709.329709 lmp.py:1935]   Expert 36 |    156 | CPU
DEBUG 01-15 16:09:00.329683.329683 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:09:00.329896.329896 lmp.py:1935]   Expert 29 |    161 | CPU
DEBUG 01-15 16:09:00.329870.329870 lmp.py:1935]   Expert 25 |    163 | CPU
DEBUG 01-15 16:09:00.329367.329367 lmp.py:1935]   Expert 54 |    166 | CPU
DEBUG 01-15 16:09:00.329547.329547 lmp.py:1935]   Expert  6 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329104.329104 lmp.py:1935]   Expert 49 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329423.329423 lmp.py:1935]   Expert 48 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329026.329026 lmp.py:1935]   Expert 12 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329914.329914 lmp.py:1935]   Expert 35 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329610.329610 lmp.py:1935]   Expert 37 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329605.329605 lmp.py:1935]   Expert 60 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329870.329870 lmp.py:1935]   Expert 13 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329090.329090 lmp.py:1935]   Expert 33 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329839.329839 lmp.py:1935]   Expert 53 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329972.329972 lmp.py:1935]   Expert 10 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329099.329099 lmp.py:1935]   Expert 16 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329225.329225 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329160.329160 lmp.py:1935]   Expert 40 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329472.329472 lmp.py:1935]   Expert 43 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329638.329638 lmp.py:1935]   Expert 38 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329089.329089 lmp.py:1935]   Expert  5 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329540.329540 lmp.py:1935]   Expert 44 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329752.329752 lmp.py:1935]   Expert 52 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329965.329965 lmp.py:1935]   Expert 41 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329177.329177 lmp.py:1935]   Expert 50 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329072.329072 lmp.py:1935]   Expert 19 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329245.329245 lmp.py:1935]   Expert  4 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329372.329372 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.329783.329783 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.329718.329718 lmp.py:1935]   Expert 31 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.330460.330460 lmp.py:1935]   Expert 56 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.330726.330726 lmp.py:1935]   Expert 20 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.330230.330230 lmp.py:1935]   Expert 39 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.330734.330734 lmp.py:1935]   Expert 22 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.330714.330714 lmp.py:1935]   Expert  2 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.330225.330225 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.330875.330875 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.330525.330525 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.330698.330698 lmp.py:1935]   Expert 18 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.330394.330394 lmp.py:1935]   Expert 14 |    318 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.330944.330944 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.330633.330633 lmp.py:1935]   Expert 11 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.330369.330369 lmp.py:1935]   Expert 61 |    460 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.330866.330866 lmp.py:1937] 
DEBUG 01-15 16:09:00.330866.330866 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.330079.330079 lmp.py:1938]   CPU:   3107 tokens
DEBUG 01-15 16:09:00.330828.330828 lmp.py:1942]   cuda:1:   4674 tokens (20 experts)
DEBUG 01-15 16:09:00.330623.330623 lmp.py:1942]   cuda:2:   4507 tokens (19 experts)
DEBUG 01-15 16:09:00.330180.330180 lmp.py:1943]   Total GPU:   9181 tokens
DEBUG 01-15 16:09:00.330976.330976 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.330976.330976 lmp.py:1944] 
DEBUG 01-15 16:09:00.330447.330447 cuda_h.py:19] end experts_map_get cost 0.0020356178283691406 seconds
DEBUG 01-15 16:09:00.330602.330602 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.330173.330173 lmp.py:1953] 
DEBUG 01-15 16:09:00.330173.330173 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.330096.330096 cuda_h.py:19] end cpu_experts_submit cost 8.392333984375e-05 seconds
DEBUG 01-15 16:09:00.330892.330892 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.330642.330642 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.331298.331298 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.4.gate_proj.weight': 0, 'model.layers.2.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.2.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.2.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.2.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.2.mlp.experts.2.gate_proj.weight': 0, 'model.layers.2.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.11.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.11.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.11.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.18.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.18.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.18.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.33.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.33.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.33.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.35.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.35.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.35.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.38.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.38.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.38.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.39.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.39.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.39.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.43.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.43.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.43.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.46.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.46.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.46.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.47.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.47.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.47.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.60.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(4037017600, 5767168, 0, 0), (4042784768, 5767168, 5767168, 0), (4031250432, 5767168, 11534336, 0), (4054319104, 5767168, 17301504, 0), (4060086272, 5767168, 23068672, 0), (4048551936, 5767168, 28835840, 0), (4071620608, 5767168, 34603008, 0), (4077387776, 5767168, 40370176, 0), (4065853440, 5767168, 46137344, 0), (4140826624, 5767168, 51904512, 0), (4146593792, 5767168, 57671680, 0), (4135059456, 5767168, 63438848, 0), (4175429632, 5767168, 69206016, 0), (4181196800, 5767168, 74973184, 0), (4169662464, 5767168, 80740352, 0), (4192731136, 5767168, 86507520, 0), (4198498304, 5767168, 92274688, 0), (4186963968, 5767168, 98041856, 0), (4210032640, 5767168, 103809024, 0), (4215799808, 5767168, 109576192, 0), (4204265472, 5767168, 115343360, 0), (4244635648, 5767168, 121110528, 0), (4250402816, 5767168, 126877696, 0), (4238868480, 5767168, 132644864, 0), (4313841664, 5767168, 138412032, 0), (4319608832, 5767168, 144179200, 0), (4308074496, 5767168, 149946368, 0), (4348444672, 5767168, 155713536, 0), (4354211840, 5767168, 161480704, 0), (4342677504, 5767168, 167247872, 0), (4504158208, 5767168, 173015040, 0), (4509925376, 5767168, 178782208, 0), (4498391040, 5767168, 184549376, 0), (4607967232, 5767168, 190316544, 0), (4613734400, 5767168, 196083712, 0), (4602200064, 5767168, 201850880, 0), (4659871744, 5767168, 207618048, 0), (4665638912, 5767168, 213385216, 0), (4654104576, 5767168, 219152384, 0), (4677173248, 5767168, 224919552, 0), (4682940416, 5767168, 230686720, 0), (4671406080, 5767168, 236453888, 0), (4694474752, 5767168, 242221056, 0), (4700241920, 5767168, 247988224, 0), (4688707584, 5767168, 253755392, 0), (4729077760, 5767168, 259522560, 0), (4734844928, 5767168, 265289728, 0), (4723310592, 5767168, 271056896, 0), (4798283776, 5767168, 276824064, 0), (4804050944, 5767168, 282591232, 0), (4792516608, 5767168, 288358400, 0), (4988600320, 5767168, 294125568, 0), (4994367488, 5767168, 299892736, 0), (4982833152, 5767168, 305659904, 0), (5023203328, 5767168, 311427072, 0), (5028970496, 5767168, 317194240, 0), (5017436160, 5767168, 322961408, 0), (5057806336, 5767168, 328728576, 0), (5063573504, 5767168, 334495744, 0), (5052039168, 5767168, 340262912, 0)], 2: [(4002414592, 5767168, 0, 0), (4008181760, 5767168, 5767168, 0), (3996647424, 5767168, 11534336, 0), (4158128128, 5767168, 17301504, 0), (4163895296, 5767168, 23068672, 0), (4152360960, 5767168, 28835840, 0), (4279238656, 5767168, 34603008, 0), (4285005824, 5767168, 40370176, 0), (4273471488, 5767168, 46137344, 0), (4296540160, 5767168, 51904512, 0), (4302307328, 5767168, 57671680, 0), (4290772992, 5767168, 63438848, 0), (4331143168, 5767168, 69206016, 0), (4336910336, 5767168, 74973184, 0), (4325376000, 5767168, 80740352, 0), (4538761216, 5767168, 86507520, 0), (4544528384, 5767168, 92274688, 0), (4532994048, 5767168, 98041856, 0), (4573364224, 5767168, 103809024, 0), (4579131392, 5767168, 109576192, 0), (4567597056, 5767168, 115343360, 0), (4625268736, 5767168, 121110528, 0), (4631035904, 5767168, 126877696, 0), (4619501568, 5767168, 132644864, 0), (4642570240, 5767168, 138412032, 0), (4648337408, 5767168, 144179200, 0), (4636803072, 5767168, 149946368, 0), (4711776256, 5767168, 155713536, 0), (4717543424, 5767168, 161480704, 0), (4706009088, 5767168, 167247872, 0), (4763680768, 5767168, 173015040, 0), (4769447936, 5767168, 178782208, 0), (4757913600, 5767168, 184549376, 0), (4780982272, 5767168, 190316544, 0), (4786749440, 5767168, 196083712, 0), (4775215104, 5767168, 201850880, 0), (4815585280, 5767168, 207618048, 0), (4821352448, 5767168, 213385216, 0), (4809818112, 5767168, 219152384, 0), (4832886784, 5767168, 224919552, 0), (4838653952, 5767168, 230686720, 0), (4827119616, 5767168, 236453888, 0), (4867489792, 5767168, 242221056, 0), (4873256960, 5767168, 247988224, 0), (4861722624, 5767168, 253755392, 0), (4884791296, 5767168, 259522560, 0), (4890558464, 5767168, 265289728, 0), (4879024128, 5767168, 271056896, 0), (4919394304, 5767168, 276824064, 0), (4925161472, 5767168, 282591232, 0), (4913627136, 5767168, 288358400, 0), (4936695808, 5767168, 294125568, 0), (4942462976, 5767168, 299892736, 0), (4930928640, 5767168, 305659904, 0), (5005901824, 5767168, 311427072, 0), (5011668992, 5767168, 317194240, 0), (5000134656, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b358fb7180>, 2: <capsule object NULL at 0x74a6801b0030>}
INFO 01-15 16:09:00.332170.332170 client.py:127] Model loaded
DEBUG 01-15 16:09:00.332784.332784 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.332841.332841 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.332718.332718 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.332098.332098 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:00.332418.332418 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 61ce09b3-9d99-4cf2-9db0-d830d02e5f92
DEBUG 01-15 16:09:00.333932.333932 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.333755.333755 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.333801.333801 cuda_h.py:19] end restore2model cost 0.0009818077087402344 seconds
DEBUG 01-15 16:09:00.333233.333233 cuda_h.py:19] end sllm_worker_task cost 0.0120697021484375 seconds
DEBUG 01-15 16:09:00.334604.334604 cuda_h.py:19] end move_flatidxs cost 0.0009694099426269531 seconds
DEBUG 01-15 16:09:00.334494.334494 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:00.334956.334956 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 61ce09b3-9d99-4cf2-9db0-d830d02e5f92
DEBUG 01-15 16:09:00.335303.335303 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004277944564819336 seconds
DEBUG 01-15 16:09:00.335096.335096 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.338263.338263 cuda_h.py:19] end restore2model cost 0.0033791065216064453 seconds
DEBUG 01-15 16:09:00.338717.338717 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007989645004272461 seconds
DEBUG 01-15 16:09:00.338526.338526 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.339185.339185 cuda_h.py:19] end gpu_sexperts cost 0.00027441978454589844 seconds
DEBUG 01-15 16:09:00.339711.339711 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.339845.339845 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.09808349609375e-05 seconds
DEBUG 01-15 16:09:00.339449.339449 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.339596.339596 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.340471.340471 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009965896606445312 seconds
DEBUG 01-15 16:09:00.340600.340600 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.340994.340994 cuda_h.py:19] end gpu_group_list cost 0.0002548694610595703 seconds
DEBUG 01-15 16:09:00.341475.341475 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.342064.342064 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011112689971923828 seconds
DEBUG 01-15 16:09:00.342795.342795 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.343275.343275 cuda_h.py:19] end gpu_group_list cost 0.0002455711364746094 seconds
DEBUG 01-15 16:09:00.342593.342593 cuda_h.py:19] end group_tensors cost 0.008272171020507812 seconds
DEBUG 01-15 16:09:00.343427.343427 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.343187.343187 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.343037.343037 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 61ce09b3-9d99-4cf2-9db0-d830d02e5f92
DEBUG 01-15 16:09:00.347666.347666 cuda_h.py:19] end group pad cost 0.004280805587768555 seconds
DEBUG 01-15 16:09:00.347270.347270 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:00.373834.373834 client.py:127] Model loaded
DEBUG 01-15 16:09:00.374439.374439 cuda_h.py:19] end wait_experts_multi_device cost 0.0303499698638916 seconds
DEBUG 01-15 16:09:00.374713.374713 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.381184.381184 cuda_h.py:19] end group_einsum cost 0.033510684967041016 seconds
DEBUG 01-15 16:09:00.381686.381686 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:00.386096.386096 cuda_h.py:19] end get_outputs_cpu1 cost 0.004477977752685547 seconds
DEBUG 01-15 16:09:00.387723.387723 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05437517166137695 seconds
DEBUG 01-15 16:09:00.387998.387998 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013350963592529297 seconds
DEBUG 01-15 16:09:00.387002.387002 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.388538.388538 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.389908.389908 cuda_h.py:19] end index_scatter cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:09:00.389369.389369 cuda_h.py:19] end cpuoutputsdeal cost 0.0014247894287109375 seconds
DEBUG 01-15 16:09:00.389378.389378 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.389425.389425 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.390634.390634 cuda_h.py:19] end gpu_group_tensor cost 0.0006287097930908203 seconds
DEBUG 01-15 16:09:00.390477.390477 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.390489.390489 cuda_h.py:19] end gpu_group_tensor cost 0.0001518726348876953 seconds
DEBUG 01-15 16:09:00.390440.390440 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.391597.391597 cuda_h.py:19] end gpu_group_einsum cost 0.0004906654357910156 seconds
DEBUG 01-15 16:09:00.391561.391561 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.391429.391429 cuda_h.py:19] end gpu_group_einsum cost 0.0003459453582763672 seconds
DEBUG 01-15 16:09:00.391254.391254 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.391343.391343 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.392549.392549 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018596649169921875 seconds
DEBUG 01-15 16:09:00.392305.392305 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.392599.392599 cuda_h.py:19] end concat_expert_out cost 4.673004150390625e-05 seconds
DEBUG 01-15 16:09:00.392489.392489 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.392048.392048 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:00.392170.392170 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008029937744140625 seconds
DEBUG 01-15 16:09:00.392060.392060 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.392864.392864 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.392453.392453 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001571178436279297 seconds
DEBUG 01-15 16:09:00.392540.392540 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.392172.392172 cuda_h.py:19] end concat_expert_out cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:09:00.393200.393200 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.393528.393528 cuda_h.py:19] end index_scatter cost 6.604194641113281e-05 seconds
DEBUG 01-15 16:09:00.393244.393244 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005202293395996094 seconds
DEBUG 01-15 16:09:00.393062.393062 cuda_h.py:19] end gpu_experts_multi_device cost 0.054074764251708984 seconds
DEBUG 01-15 16:09:00.393971.393971 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.06589555740356445 seconds
DEBUG 01-15 16:09:00.393760.393760 cuda_h.py:19] end prefill_layer cost 0.07289409637451172 seconds
DEBUG 01-15 16:09:00.393656.393656 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:09:00.393836.393836 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.393254.393254 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:09:00.393149.393149 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:09:00.393018.393018 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:09:00.394497.394497 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 4.291534423828125e-05 seconds
DEBUG 01-15 16:09:00.394253.394253 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 7.62939453125e-05 seconds
DEBUG 01-15 16:09:00.394049.394049 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.394032.394032 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.394916.394916 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.394475.394475 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.394941.394941 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.394493.394493 cuda_h.py:19] end allocate_cuda_memory cost 0.00022554397583007812 seconds
DEBUG 01-15 16:09:00.394714.394714 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.394021.394021 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.394738.394738 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.394216.394216 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f3fa7684-8541-4294-b569-f0069bed2edd
DEBUG 01-15 16:09:00.395949.395949 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.395084.395084 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.396241.396241 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f3fa7684-8541-4294-b569-f0069bed2edd
DEBUG 01-15 16:09:00.396720.396720 cuda_h.py:19] end load_into_gpu_async cost 0.0015053749084472656 seconds
DEBUG 01-15 16:09:00.396860.396860 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.396699.396699 cuda_h.py:19] end restore_tensors2 cost 9.417533874511719e-05 seconds
DEBUG 01-15 16:09:00.396415.396415 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021326541900634766 seconds
INFO 01-15 16:09:00.396802.396802 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f3fa7684-8541-4294-b569-f0069bed2edd
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.398808.398808 cuda_h.py:19] end self_attn cost 0.0031652450561523438 seconds
DEBUG 01-15 16:09:00.398282.398282 cuda_h.py:19] end iln_self_attn_paln cost 0.004773855209350586 seconds
DEBUG 01-15 16:09:00.398350.398350 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:09:00.398636.398636 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.399933.399933 cuda_h.py:19] end gate cost 0.0007088184356689453 seconds
DEBUG 01-15 16:09:00.399001.399001 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.400773.400773 lmp.py:1912] 
DEBUG 01-15 16:09:00.400773.400773 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.400682.400682 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.400477.400477 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.400696.400696 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.400531.400531 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.400413.400413 lmp.py:1917] 
DEBUG 01-15 16:09:00.400413.400413 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.400486.400486 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.400282.400282 lmp.py:1935]   Expert  1 |     50 | CPU
DEBUG 01-15 16:09:00.400878.400878 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:09:00.400521.400521 lmp.py:1935]   Expert  7 |     73 | CPU
DEBUG 01-15 16:09:00.400164.400164 lmp.py:1935]   Expert 48 |     81 | CPU
DEBUG 01-15 16:09:00.400807.400807 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:09:00.400450.400450 lmp.py:1935]   Expert 30 |    109 | CPU
DEBUG 01-15 16:09:00.400093.400093 lmp.py:1935]   Expert 61 |    116 | CPU
DEBUG 01-15 16:09:00.400452.400452 lmp.py:1935]   Expert 32 |    118 | CPU
DEBUG 01-15 16:09:00.400539.400539 lmp.py:1935]   Expert 18 |    119 | CPU
DEBUG 01-15 16:09:00.400341.400341 lmp.py:1935]   Expert 45 |    119 | CPU
DEBUG 01-15 16:09:00.400269.400269 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:09:00.400196.400196 lmp.py:1935]   Expert 39 |    135 | CPU
DEBUG 01-15 16:09:00.400886.400886 lmp.py:1935]   Expert 26 |    139 | CPU
DEBUG 01-15 16:09:00.400813.400813 lmp.py:1935]   Expert 36 |    139 | CPU
DEBUG 01-15 16:09:00.400980.400980 lmp.py:1935]   Expert  5 |    140 | CPU
DEBUG 01-15 16:09:00.400384.400384 lmp.py:1935]   Expert 11 |    141 | CPU
DEBUG 01-15 16:09:00.400312.400312 lmp.py:1935]   Expert 59 |    143 | CPU
DEBUG 01-15 16:09:00.400478.400478 lmp.py:1935]   Expert  6 |    144 | CPU
DEBUG 01-15 16:09:00.400406.400406 lmp.py:1935]   Expert 51 |    145 | CPU
DEBUG 01-15 16:09:00.400572.400572 lmp.py:1935]   Expert 23 |    154 | CPU
DEBUG 01-15 16:09:00.400261.400261 lmp.py:1935]   Expert 49 |    156 | CPU
DEBUG 01-15 16:09:00.400189.400189 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:09:00.400355.400355 lmp.py:1935]   Expert  9 |    158 | CPU
DEBUG 01-15 16:09:00.400713.400713 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:09:00.400356.400356 lmp.py:1935]   Expert 40 |    168 | CPU
DEBUG 01-15 16:09:00.400907.400907 lmp.py:1935]   Expert 52 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.400218.400218 lmp.py:1935]   Expert 56 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.400053.400053 lmp.py:1935]   Expert 16 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.400889.400889 lmp.py:1935]   Expert 35 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.400008.400008 lmp.py:1935]   Expert  4 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.400890.400890 lmp.py:1935]   Expert 13 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.400294.400294 lmp.py:1935]   Expert 37 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.400937.400937 lmp.py:1935]   Expert 42 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.400580.400580 lmp.py:1935]   Expert 17 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.400985.400985 lmp.py:1935]   Expert 38 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.400389.400389 lmp.py:1935]   Expert 62 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.400748.400748 lmp.py:1935]   Expert 21 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.400867.400867 lmp.py:1935]   Expert  3 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.400987.400987 lmp.py:1935]   Expert 44 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.400107.400107 lmp.py:1935]   Expert 58 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.400465.400465 lmp.py:1935]   Expert 60 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.400585.400585 lmp.py:1935]   Expert 28 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401228.401228 lmp.py:1935]   Expert 10 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401871.401871 lmp.py:1935]   Expert 47 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401276.401276 lmp.py:1935]   Expert 53 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401680.401680 lmp.py:1935]   Expert 55 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401323.401323 lmp.py:1935]   Expert 20 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401443.401443 lmp.py:1935]   Expert 57 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401801.401801 lmp.py:1935]   Expert 33 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401159.401159 lmp.py:1935]   Expert 46 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401279.401279 lmp.py:1935]   Expert 31 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401399.401399 lmp.py:1935]   Expert  8 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401757.401757 lmp.py:1935]   Expert 19 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401162.401162 lmp.py:1935]   Expert 24 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401566.401566 lmp.py:1935]   Expert 14 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401971.401971 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401375.401375 lmp.py:1935]   Expert 12 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401018.401018 lmp.py:1935]   Expert 29 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401423.401423 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401066.401066 lmp.py:1935]   Expert  0 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401471.401471 lmp.py:1935]   Expert 43 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401590.401590 lmp.py:1935]   Expert 54 |    342 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401472.401472 lmp.py:1935]   Expert 41 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.401353.401353 lmp.py:1935]   Expert 25 |    412 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.401758.401758 lmp.py:1937] 
DEBUG 01-15 16:09:00.401758.401758 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.401116.401116 lmp.py:1938]   CPU:   3162 tokens
DEBUG 01-15 16:09:00.401951.401951 lmp.py:1942]   cuda:1:   4482 tokens (19 experts)
DEBUG 01-15 16:09:00.401594.401594 lmp.py:1942]   cuda:2:   4644 tokens (20 experts)
DEBUG 01-15 16:09:00.401283.401283 lmp.py:1943]   Total GPU:   9126 tokens
DEBUG 01-15 16:09:00.401734.401734 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.401734.401734 lmp.py:1944] 
DEBUG 01-15 16:09:00.401622.401622 cuda_h.py:19] end experts_map_get cost 0.0017285346984863281 seconds
DEBUG 01-15 16:09:00.401326.401326 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.401175.401175 lmp.py:1953] 
DEBUG 01-15 16:09:00.401175.401175 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.401912.401912 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:00.401654.401654 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.401113.401113 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.403082.403082 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.58.up_proj.weight': 322961408}, 2: {'model.layers.3.mlp.experts.8.gate_proj.weight': 0, 'model.layers.3.mlp.experts.8.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.8.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.16.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.16.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.16.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.21.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.21.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.21.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.22.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.22.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.22.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.28.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.28.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.28.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.54.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.54.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.54.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.3.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.3.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.3.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5127012352, 5767168, 17301504, 0), (5132779520, 5767168, 23068672, 0), (5121245184, 5767168, 28835840, 0), (5144313856, 5767168, 34603008, 0), (5150081024, 5767168, 40370176, 0), (5138546688, 5767168, 46137344, 0), (5282725888, 5767168, 51904512, 0), (5288493056, 5767168, 57671680, 0), (5276958720, 5767168, 63438848, 0), (5300027392, 5767168, 69206016, 0), (5305794560, 5767168, 74973184, 0), (5294260224, 5767168, 80740352, 0), (5317328896, 5767168, 86507520, 0), (5323096064, 5767168, 92274688, 0), (5311561728, 5767168, 98041856, 0), (5369233408, 5767168, 103809024, 0), (5375000576, 5767168, 109576192, 0), (5363466240, 5767168, 115343360, 0), (5403836416, 5767168, 121110528, 0), (5409603584, 5767168, 126877696, 0), (5398069248, 5767168, 132644864, 0), (5421137920, 5767168, 138412032, 0), (5426905088, 5767168, 144179200, 0), (5415370752, 5767168, 149946368, 0), (5507645440, 5767168, 155713536, 0), (5513412608, 5767168, 161480704, 0), (5501878272, 5767168, 167247872, 0), (5611454464, 5767168, 173015040, 0), (5617221632, 5767168, 178782208, 0), (5605687296, 5767168, 184549376, 0), (5715263488, 5767168, 190316544, 0), (5721030656, 5767168, 196083712, 0), (5709496320, 5767168, 201850880, 0), (5819072512, 5767168, 207618048, 0), (5824839680, 5767168, 213385216, 0), (5813305344, 5767168, 219152384, 0), (5836374016, 5767168, 224919552, 0), (5842141184, 5767168, 230686720, 0), (5830606848, 5767168, 236453888, 0), (5870977024, 5767168, 242221056, 0), (5876744192, 5767168, 247988224, 0), (5865209856, 5767168, 253755392, 0), (5888278528, 5767168, 259522560, 0), (5894045696, 5767168, 265289728, 0), (5882511360, 5767168, 271056896, 0), (5974786048, 5767168, 276824064, 0), (5980553216, 5767168, 282591232, 0), (5969018880, 5767168, 288358400, 0), (5992087552, 5767168, 294125568, 0), (5997854720, 5767168, 299892736, 0), (5986320384, 5767168, 305659904, 0), (6078595072, 5767168, 311427072, 0), (6084362240, 5767168, 317194240, 0), (6072827904, 5767168, 322961408, 0)], 2: [(5213519872, 5767168, 0, 0), (5219287040, 5767168, 5767168, 0), (5207752704, 5767168, 11534336, 0), (5248122880, 5767168, 17301504, 0), (5253890048, 5767168, 23068672, 0), (5242355712, 5767168, 28835840, 0), (5351931904, 5767168, 34603008, 0), (5357699072, 5767168, 40370176, 0), (5346164736, 5767168, 46137344, 0), (5438439424, 5767168, 51904512, 0), (5444206592, 5767168, 57671680, 0), (5432672256, 5767168, 63438848, 0), (5455740928, 5767168, 69206016, 0), (5461508096, 5767168, 74973184, 0), (5449973760, 5767168, 80740352, 0), (5490343936, 5767168, 86507520, 0), (5496111104, 5767168, 92274688, 0), (5484576768, 5767168, 98041856, 0), (5559549952, 5767168, 103809024, 0), (5565317120, 5767168, 109576192, 0), (5553782784, 5767168, 115343360, 0), (5576851456, 5767168, 121110528, 0), (5582618624, 5767168, 126877696, 0), (5571084288, 5767168, 132644864, 0), (5646057472, 5767168, 138412032, 0), (5651824640, 5767168, 144179200, 0), (5640290304, 5767168, 149946368, 0), (5680660480, 5767168, 155713536, 0), (5686427648, 5767168, 161480704, 0), (5674893312, 5767168, 167247872, 0), (5732564992, 5767168, 173015040, 0), (5738332160, 5767168, 178782208, 0), (5726797824, 5767168, 184549376, 0), (5784469504, 5767168, 190316544, 0), (5790236672, 5767168, 196083712, 0), (5778702336, 5767168, 201850880, 0), (5801771008, 5767168, 207618048, 0), (5807538176, 5767168, 213385216, 0), (5796003840, 5767168, 219152384, 0), (6009389056, 5767168, 224919552, 0), (6015156224, 5767168, 230686720, 0), (6003621888, 5767168, 236453888, 0), (6026690560, 5767168, 242221056, 0), (6032457728, 5767168, 247988224, 0), (6020923392, 5767168, 253755392, 0), (6043992064, 5767168, 259522560, 0), (6049759232, 5767168, 265289728, 0), (6038224896, 5767168, 271056896, 0), (6061293568, 5767168, 276824064, 0), (6067060736, 5767168, 282591232, 0), (6055526400, 5767168, 288358400, 0), (6113198080, 5767168, 294125568, 0), (6118965248, 5767168, 299892736, 0), (6107430912, 5767168, 305659904, 0), (6147801088, 5767168, 311427072, 0), (6153568256, 5767168, 317194240, 0), (6142033920, 5767168, 322961408, 0), (6165102592, 5767168, 328728576, 0), (6170869760, 5767168, 334495744, 0), (6159335424, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8147145a0>, 2: <capsule object NULL at 0x74a814714600>}
DEBUG 01-15 16:09:00.403986.403986 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.403862.403862 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.403757.403757 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 946d0ff7-4f9f-4279-b7ca-cdac8f0ecd1c
DEBUG 01-15 16:09:00.404044.404044 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:00.404400.404400 client.py:127] Model loaded
DEBUG 01-15 16:09:00.404574.404574 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.404781.404781 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:00.404030.404030 cuda_h.py:19] end restore2model cost 0.0003337860107421875 seconds
DEBUG 01-15 16:09:00.404447.404447 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.404085.404085 cuda_h.py:19] end sllm_worker_task cost 0.010503053665161133 seconds
INFO 01-15 16:09:00.405016.405016 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 946d0ff7-4f9f-4279-b7ca-cdac8f0ecd1c
DEBUG 01-15 16:09:00.405836.405836 cuda_h.py:19] end move_flatidxs cost 0.0008516311645507812 seconds
DEBUG 01-15 16:09:00.405227.405227 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003949403762817383 seconds
DEBUG 01-15 16:09:00.405003.405003 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:00.405057.405057 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.408443.408443 cuda_h.py:19] end restore2model cost 0.0031633377075195312 seconds
DEBUG 01-15 16:09:00.409121.409121 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007375478744506836 seconds
DEBUG 01-15 16:09:00.409108.409108 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.409516.409516 cuda_h.py:19] end gpu_sexperts cost 0.00026988983154296875 seconds
DEBUG 01-15 16:09:00.409246.409246 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.409923.409923 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:09:00.409427.409427 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.409606.409606 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.410573.410573 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009634494781494141 seconds
DEBUG 01-15 16:09:00.410523.410523 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.410504.410504 cuda_h.py:19] end gpu_group_list cost 0.00020194053649902344 seconds
DEBUG 01-15 16:09:00.410971.410971 cuda_h.py:19] end group_tensors cost 0.004525661468505859 seconds
DEBUG 01-15 16:09:00.411894.411894 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.411255.411255 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.413674.413674 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0017082691192626953 seconds
DEBUG 01-15 16:09:00.413864.413864 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.414865.414865 cuda_h.py:19] end group pad cost 0.0030171871185302734 seconds
DEBUG 01-15 16:09:00.414787.414787 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:00.414478.414478 cuda_h.py:19] end gpu_group_list cost 0.0003075599670410156 seconds
DEBUG 01-15 16:09:00.419212.419212 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.420342.420342 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 946d0ff7-4f9f-4279-b7ca-cdac8f0ecd1c
DEBUG 01-15 16:09:00.442572.442572 cuda_h.py:19] end group_einsum cost 0.027916908264160156 seconds
DEBUG 01-15 16:09:00.442788.442788 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:00.444576.444576 client.py:127] Model loaded
DEBUG 01-15 16:09:00.444289.444289 cuda_h.py:19] end wait_experts_multi_device cost 0.024103641510009766 seconds
DEBUG 01-15 16:09:00.444959.444959 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.445175.445175 cuda_h.py:19] end get_outputs_cpu1 cost 0.0027904510498046875 seconds
DEBUG 01-15 16:09:00.445818.445818 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04137754440307617 seconds
DEBUG 01-15 16:09:00.446513.446513 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.002608776092529297 seconds
DEBUG 01-15 16:09:00.447632.447632 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.449482.449482 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.450395.450395 cuda_h.py:19] end index_scatter cost 0.0005757808685302734 seconds
DEBUG 01-15 16:09:00.450486.450486 cuda_h.py:19] end cpuoutputsdeal cost 0.003648996353149414 seconds
DEBUG 01-15 16:09:00.451698.451698 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.451257.451257 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.451167.451167 cuda_h.py:19] end gpu_group_tensor cost 0.0003743171691894531 seconds
DEBUG 01-15 16:09:00.451925.451925 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.452296.452296 cuda_h.py:19] end gpu_group_tensor cost 0.0002989768981933594 seconds
DEBUG 01-15 16:09:00.452953.452953 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.453372.453372 cuda_h.py:19] end gpu_group_einsum cost 0.0011057853698730469 seconds
DEBUG 01-15 16:09:00.453262.453262 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.454144.454144 cuda_h.py:19] end gpu_group_einsum cost 0.0008912086486816406 seconds
DEBUG 01-15 16:09:00.455921.455921 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.455618.455618 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.455292.455292 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004680156707763672 seconds
DEBUG 01-15 16:09:00.456639.456639 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.456527.456527 cuda_h.py:19] end concat_expert_out cost 0.0001220703125 seconds
DEBUG 01-15 16:09:00.456923.456923 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.456878.456878 cuda_h.py:19] end index_scatter cost 0.000133514404296875 seconds
DEBUG 01-15 16:09:00.457212.457212 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0018558502197265625 seconds
DEBUG 01-15 16:09:00.457729.457729 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.457310.457310 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.457695.457695 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003457069396972656 seconds
DEBUG 01-15 16:09:00.457367.457367 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.458526.458526 cuda_h.py:19] end concat_expert_out cost 0.00014925003051757812 seconds
DEBUG 01-15 16:09:00.458823.458823 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.458909.458909 cuda_h.py:19] end index_scatter cost 0.00011801719665527344 seconds
DEBUG 01-15 16:09:00.458508.458508 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012197494506835938 seconds
DEBUG 01-15 16:09:00.458732.458732 cuda_h.py:19] end gpu_experts_multi_device cost 0.04924440383911133 seconds
DEBUG 01-15 16:09:00.458030.458030 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.06000804901123047 seconds
DEBUG 01-15 16:09:00.459556.459556 cuda_h.py:19] end prefill_layer cost 0.06595420837402344 seconds
DEBUG 01-15 16:09:00.459553.459553 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:09:00.459216.459216 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.460356.460356 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:09:00.460258.460258 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:09:00.460690.460690 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:09:00.460567.460567 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:09:00.460952.460952 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 9.226799011230469e-05 seconds
DEBUG 01-15 16:09:00.460423.460423 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.460070.460070 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.460536.460536 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.460108.460108 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.460797.460797 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.461622.461622 cuda_h.py:19] end allocate_cuda_memory cost 0.0002818107604980469 seconds
DEBUG 01-15 16:09:00.461897.461897 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.461951.461951 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.461827.461827 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.461153.461153 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b7674a18-86d6-40bc-931c-f7a22f44ffb3
DEBUG 01-15 16:09:00.461474.461474 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.461234.461234 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.462608.462608 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b7674a18-86d6-40bc-931c-f7a22f44ffb3
DEBUG 01-15 16:09:00.462962.462962 cuda_h.py:19] end load_into_gpu_async cost 0.0013041496276855469 seconds
DEBUG 01-15 16:09:00.462692.462692 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.462080.462080 cuda_h.py:19] end restore_tensors2 cost 8.177757263183594e-05 seconds
DEBUG 01-15 16:09:00.462743.462743 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0019922256469726562 seconds
INFO 01-15 16:09:00.462964.462964 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b7674a18-86d6-40bc-931c-f7a22f44ffb3
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.466135.466135 cuda_h.py:19] end self_attn cost 0.004463672637939453 seconds
DEBUG 01-15 16:09:00.466460.466460 cuda_h.py:19] end iln_self_attn_paln cost 0.0065708160400390625 seconds
DEBUG 01-15 16:09:00.466078.466078 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:09:00.466769.466769 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.467002.467002 cuda_h.py:19] end gate cost 0.00079345703125 seconds
DEBUG 01-15 16:09:00.467859.467859 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.468543.468543 lmp.py:1912] 
DEBUG 01-15 16:09:00.468543.468543 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.468956.468956 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.468196.468196 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.468118.468118 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.468635.468635 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.468053.468053 lmp.py:1917] 
DEBUG 01-15 16:09:00.468053.468053 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.468425.468425 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.468618.468618 lmp.py:1935]   Expert 14 |     66 | CPU
DEBUG 01-15 16:09:00.468990.468990 lmp.py:1935]   Expert 57 |     72 | CPU
DEBUG 01-15 16:09:00.468693.468693 lmp.py:1935]   Expert 13 |     74 | CPU
DEBUG 01-15 16:09:00.468264.468264 lmp.py:1935]   Expert 26 |     82 | CPU
DEBUG 01-15 16:09:00.468536.468536 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:09:00.468047.468047 lmp.py:1935]   Expert 54 |     92 | CPU
DEBUG 01-15 16:09:00.468558.468558 lmp.py:1935]   Expert 11 |     93 | CPU
DEBUG 01-15 16:09:00.468400.468400 lmp.py:1935]   Expert 45 |     94 | CPU
DEBUG 01-15 16:09:00.468765.468765 lmp.py:1935]   Expert 58 |    100 | CPU
DEBUG 01-15 16:09:00.468653.468653 lmp.py:1935]   Expert 30 |    108 | CPU
DEBUG 01-15 16:09:00.468064.468064 lmp.py:1935]   Expert 51 |    109 | CPU
DEBUG 01-15 16:09:00.468191.468191 lmp.py:1935]   Expert 10 |    113 | CPU
DEBUG 01-15 16:09:00.468079.468079 lmp.py:1935]   Expert 36 |    113 | CPU
DEBUG 01-15 16:09:00.468875.468875 lmp.py:1935]   Expert 32 |    116 | CPU
DEBUG 01-15 16:09:00.469253.469253 lmp.py:1935]   Expert 20 |    129 | CPU
DEBUG 01-15 16:09:00.469002.469002 lmp.py:1935]   Expert  8 |    131 | CPU
DEBUG 01-15 16:09:00.469798.469798 lmp.py:1935]   Expert  4 |    138 | CPU
DEBUG 01-15 16:09:00.469924.469924 lmp.py:1935]   Expert 63 |    139 | CPU
DEBUG 01-15 16:09:00.469336.469336 lmp.py:1935]   Expert 53 |    140 | CPU
DEBUG 01-15 16:09:00.469986.469986 lmp.py:1935]   Expert 61 |    143 | CPU
DEBUG 01-15 16:09:00.469635.469635 lmp.py:1935]   Expert 34 |    144 | CPU
DEBUG 01-15 16:09:00.469524.469524 lmp.py:1935]   Expert 16 |    147 | CPU
DEBUG 01-15 16:09:00.469412.469412 lmp.py:1935]   Expert 47 |    148 | CPU
DEBUG 01-15 16:09:00.469300.469300 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:09:00.469857.469857 lmp.py:1935]   Expert 60 |    159 | CPU
DEBUG 01-15 16:09:00.469752.469752 lmp.py:1935]   Expert 42 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.469693.469693 lmp.py:1935]   Expert 17 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.469489.469489 lmp.py:1935]   Expert 44 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.469808.469808 lmp.py:1935]   Expert 29 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.469126.469126 lmp.py:1935]   Expert 27 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.469160.469160 lmp.py:1935]   Expert  7 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.469956.469956 lmp.py:1935]   Expert 41 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.469751.469751 lmp.py:1935]   Expert 48 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.469832.469832 lmp.py:1935]   Expert 56 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.469819.469819 lmp.py:1935]   Expert  9 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.469045.469045 lmp.py:1935]   Expert  3 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.469271.469271 lmp.py:1935]   Expert 15 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.469305.469305 lmp.py:1935]   Expert  2 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.469101.469101 lmp.py:1935]   Expert  0 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.469996.469996 lmp.py:1935]   Expert 24 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.469652.469652 lmp.py:1935]   Expert 18 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.469309.469309 lmp.py:1935]   Expert 55 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.469489.469489 lmp.py:1935]   Expert 23 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.469668.469668 lmp.py:1935]   Expert 38 |    214 | GPU1(cuda:1)
INFO 01-15 16:09:00.469545.469545 client.py:127] Model loaded
DEBUG 01-15 16:09:00.469211.469211 lmp.py:1935]   Expert 40 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470134.470134 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.470123.470123 lmp.py:1935]   Expert 22 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470123.470123 lmp.py:1935]   Expert  6 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470608.470608 lmp.py:1935]   Expert 37 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.470257.470257 lmp.py:1935]   Expert 46 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.470477.470477 lmp.py:1935]   Expert 19 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470981.470981 lmp.py:1935]   Expert 39 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.470531.470531 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470320.470320 lmp.py:1935]   Expert 12 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470632.470632 lmp.py:1935]   Expert 50 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.470944.470944 lmp.py:1935]   Expert 62 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.470255.470255 lmp.py:1935]   Expert 21 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470044.470044 lmp.py:1935]   Expert 35 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.470833.470833 lmp.py:1935]   Expert 49 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470383.470383 lmp.py:1935]   Expert 52 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.470887.470887 lmp.py:1935]   Expert 33 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470782.470782 lmp.py:1935]   Expert  1 |    350 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.470002.470002 lmp.py:1935]   Expert  5 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470552.470552 lmp.py:1935]   Expert 43 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.470387.470387 lmp.py:1935]   Expert 59 |    587 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.470268.470268 lmp.py:1937] 
DEBUG 01-15 16:09:00.470268.470268 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.470295.470295 lmp.py:1938]   CPU:   2900 tokens
DEBUG 01-15 16:09:00.470323.470323 lmp.py:1942]   cuda:1:   4641 tokens (19 experts)
DEBUG 01-15 16:09:00.470396.470396 lmp.py:1942]   cuda:2:   4747 tokens (20 experts)
DEBUG 01-15 16:09:00.470516.470516 lmp.py:1943]   Total GPU:   9388 tokens
DEBUG 01-15 16:09:00.470874.470874 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.470874.470874 lmp.py:1944] 
DEBUG 01-15 16:09:00.470253.470253 cuda_h.py:19] end experts_map_get cost 0.003017902374267578 seconds
DEBUG 01-15 16:09:00.471534.471534 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.471973.471973 lmp.py:1953] 
DEBUG 01-15 16:09:00.471973.471973 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.471684.471684 cuda_h.py:19] end cpu_experts_submit cost 6.747245788574219e-05 seconds
DEBUG 01-15 16:09:00.471380.471380 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.471183.471183 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.472561.472561 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.27.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.27.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.27.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.44.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.44.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.44.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.48.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.48.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.48.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.49.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.49.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.49.up_proj.weight': 322961408, 'model.layers.4.mlp.experts.55.gate_proj.weight': 328728576, 'model.layers.4.mlp.experts.55.down_proj.weight': 334495744, 'model.layers.4.mlp.experts.55.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6303514624, 5767168, 17301504, 0), (6309281792, 5767168, 23068672, 0), (6297747456, 5767168, 28835840, 0), (6338117632, 5767168, 34603008, 0), (6343884800, 5767168, 40370176, 0), (6332350464, 5767168, 46137344, 0), (6441926656, 5767168, 51904512, 0), (6447693824, 5767168, 57671680, 0), (6436159488, 5767168, 63438848, 0), (6476529664, 5767168, 69206016, 0), (6482296832, 5767168, 74973184, 0), (6470762496, 5767168, 80740352, 0), (6493831168, 5767168, 86507520, 0), (6499598336, 5767168, 92274688, 0), (6488064000, 5767168, 98041856, 0), (6580338688, 5767168, 103809024, 0), (6586105856, 5767168, 109576192, 0), (6574571520, 5767168, 115343360, 0), (6597640192, 5767168, 121110528, 0), (6603407360, 5767168, 126877696, 0), (6591873024, 5767168, 132644864, 0), (6684147712, 5767168, 138412032, 0), (6689914880, 5767168, 144179200, 0), (6678380544, 5767168, 149946368, 0), (6787956736, 5767168, 155713536, 0), (6793723904, 5767168, 161480704, 0), (6782189568, 5767168, 167247872, 0), (6822559744, 5767168, 173015040, 0), (6828326912, 5767168, 178782208, 0), (6816792576, 5767168, 184549376, 0), (6839861248, 5767168, 190316544, 0), (6845628416, 5767168, 196083712, 0), (6834094080, 5767168, 201850880, 0), (6857162752, 5767168, 207618048, 0), (6862929920, 5767168, 213385216, 0), (6851395584, 5767168, 219152384, 0), (6978273280, 5767168, 224919552, 0), (6984040448, 5767168, 230686720, 0), (6972506112, 5767168, 236453888, 0), (7047479296, 5767168, 242221056, 0), (7053246464, 5767168, 247988224, 0), (7041712128, 5767168, 253755392, 0), (7082082304, 5767168, 259522560, 0), (7087849472, 5767168, 265289728, 0), (7076315136, 5767168, 271056896, 0), (7151288320, 5767168, 276824064, 0), (7157055488, 5767168, 282591232, 0), (7145521152, 5767168, 288358400, 0), (7203192832, 5767168, 294125568, 0), (7208960000, 5767168, 299892736, 0), (7197425664, 5767168, 305659904, 0), (7255097344, 5767168, 311427072, 0), (7260864512, 5767168, 317194240, 0), (7249330176, 5767168, 322961408, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6217007104, 5767168, 17301504, 0), (6222774272, 5767168, 23068672, 0), (6211239936, 5767168, 28835840, 0), (6234308608, 5767168, 34603008, 0), (6240075776, 5767168, 40370176, 0), (6228541440, 5767168, 46137344, 0), (6268911616, 5767168, 51904512, 0), (6274678784, 5767168, 57671680, 0), (6263144448, 5767168, 63438848, 0), (6286213120, 5767168, 69206016, 0), (6291980288, 5767168, 74973184, 0), (6280445952, 5767168, 80740352, 0), (6390022144, 5767168, 86507520, 0), (6395789312, 5767168, 92274688, 0), (6384254976, 5767168, 98041856, 0), (6511132672, 5767168, 103809024, 0), (6516899840, 5767168, 109576192, 0), (6505365504, 5767168, 115343360, 0), (6545735680, 5767168, 121110528, 0), (6551502848, 5767168, 126877696, 0), (6539968512, 5767168, 132644864, 0), (6563037184, 5767168, 138412032, 0), (6568804352, 5767168, 144179200, 0), (6557270016, 5767168, 149946368, 0), (6614941696, 5767168, 155713536, 0), (6620708864, 5767168, 161480704, 0), (6609174528, 5767168, 167247872, 0), (6649544704, 5767168, 173015040, 0), (6655311872, 5767168, 178782208, 0), (6643777536, 5767168, 184549376, 0), (6753353728, 5767168, 190316544, 0), (6759120896, 5767168, 196083712, 0), (6747586560, 5767168, 201850880, 0), (6874464256, 5767168, 207618048, 0), (6880231424, 5767168, 213385216, 0), (6868697088, 5767168, 219152384, 0), (6891765760, 5767168, 224919552, 0), (6897532928, 5767168, 230686720, 0), (6885998592, 5767168, 236453888, 0), (6909067264, 5767168, 242221056, 0), (6914834432, 5767168, 247988224, 0), (6903300096, 5767168, 253755392, 0), (6926368768, 5767168, 259522560, 0), (6932135936, 5767168, 265289728, 0), (6920601600, 5767168, 271056896, 0), (6943670272, 5767168, 276824064, 0), (6949437440, 5767168, 282591232, 0), (6937903104, 5767168, 288358400, 0), (7012876288, 5767168, 294125568, 0), (7018643456, 5767168, 299892736, 0), (7007109120, 5767168, 305659904, 0), (7030177792, 5767168, 311427072, 0), (7035944960, 5767168, 317194240, 0), (7024410624, 5767168, 322961408, 0), (7133986816, 5767168, 328728576, 0), (7139753984, 5767168, 334495744, 0), (7128219648, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b351c24210>, 2: <capsule object NULL at 0x74b34eeec450>}
DEBUG 01-15 16:09:00.472685.472685 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.472541.472541 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.472205.472205 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e51ad182-5c6b-462c-9e1a-076d4230e5a6
DEBUG 01-15 16:09:00.472544.472544 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.473763.473763 cuda_h.py:19] end restore2model cost 0.0033986568450927734 seconds
DEBUG 01-15 16:09:00.473583.473583 cuda_h.py:19] end sllm_worker_task cost 0.01300954818725586 seconds
DEBUG 01-15 16:09:00.473218.473218 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:00.474340.474340 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e51ad182-5c6b-462c-9e1a-076d4230e5a6
DEBUG 01-15 16:09:00.474829.474829 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.474561.474561 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0035424232482910156 seconds
DEBUG 01-15 16:09:00.474102.474102 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.475056.475056 cuda_h.py:19] end move_flatidxs cost 0.0009431838989257812 seconds
DEBUG 01-15 16:09:00.475781.475781 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:00.478939.478939 cuda_h.py:19] end restore2model cost 0.0032150745391845703 seconds
DEBUG 01-15 16:09:00.478233.478233 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0070645809173583984 seconds
DEBUG 01-15 16:09:00.478505.478505 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.478549.478549 cuda_h.py:19] end gpu_sexperts cost 0.0002818107604980469 seconds
DEBUG 01-15 16:09:00.478855.478855 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.478923.478923 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.193450927734375e-05 seconds
DEBUG 01-15 16:09:00.478143.478143 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.478753.478753 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.479236.479236 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009577274322509766 seconds
DEBUG 01-15 16:09:00.479940.479940 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.480247.480247 cuda_h.py:19] end gpu_group_list cost 0.0002281665802001953 seconds
DEBUG 01-15 16:09:00.480396.480396 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.482528.482528 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011265277862548828 seconds
DEBUG 01-15 16:09:00.482306.482306 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.482971.482971 cuda_h.py:19] end gpu_group_list cost 0.00024962425231933594 seconds
DEBUG 01-15 16:09:00.483703.483703 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.483394.483394 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e51ad182-5c6b-462c-9e1a-076d4230e5a6
DEBUG 01-15 16:09:00.483694.483694 cuda_h.py:19] end group_tensors cost 0.008009195327758789 seconds
DEBUG 01-15 16:09:00.484744.484744 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.487485.487485 cuda_h.py:19] end group pad cost 0.003265857696533203 seconds
DEBUG 01-15 16:09:00.488037.488037 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:00.514003.514003 client.py:127] Model loaded
DEBUG 01-15 16:09:00.514099.514099 cuda_h.py:19] end wait_experts_multi_device cost 0.031101703643798828 seconds
DEBUG 01-15 16:09:00.514228.514228 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.515793.515793 cuda_h.py:19] end group_einsum cost 0.027445077896118164 seconds
DEBUG 01-15 16:09:00.515717.515717 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:00.518269.518269 cuda_h.py:19] end get_outputs_cpu1 cost 0.002779245376586914 seconds
DEBUG 01-15 16:09:00.519376.519376 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04533553123474121 seconds
DEBUG 01-15 16:09:00.520201.520201 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005609035491943359 seconds
DEBUG 01-15 16:09:00.520404.520404 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.522346.522346 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.523827.523827 cuda_h.py:19] end index_scatter cost 0.000156402587890625 seconds
DEBUG 01-15 16:09:00.523725.523725 cuda_h.py:19] end cpuoutputsdeal cost 0.0029370784759521484 seconds
DEBUG 01-15 16:09:00.523055.523055 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.523654.523654 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.524998.524998 cuda_h.py:19] end gpu_group_tensor cost 0.0003108978271484375 seconds
DEBUG 01-15 16:09:00.524479.524479 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.525899.525899 cuda_h.py:19] end gpu_group_tensor cost 0.0007781982421875 seconds
DEBUG 01-15 16:09:00.525398.525398 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.526363.526363 cuda_h.py:19] end gpu_group_einsum cost 0.00119781494140625 seconds
DEBUG 01-15 16:09:00.527094.527094 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.527123.527123 cuda_h.py:19] end gpu_group_einsum cost 0.0004112720489501953 seconds
DEBUG 01-15 16:09:00.527749.527749 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.527514.527514 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.528741.528741 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019884109497070312 seconds
DEBUG 01-15 16:09:00.528689.528689 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.528580.528580 cuda_h.py:19] end concat_expert_out cost 6.103515625e-05 seconds
DEBUG 01-15 16:09:00.528767.528767 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.528148.528148 cuda_h.py:19] end index_scatter cost 7.176399230957031e-05 seconds
DEBUG 01-15 16:09:00.528475.528475 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008714199066162109 seconds
DEBUG 01-15 16:09:00.528081.528081 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.528216.528216 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.528368.528368 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015020370483398438 seconds
DEBUG 01-15 16:09:00.529978.529978 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.529186.529186 cuda_h.py:19] end concat_expert_out cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:00.529645.529645 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.529760.529760 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:00.529854.529854 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004985332489013672 seconds
DEBUG 01-15 16:09:00.529777.529777 cuda_h.py:19] end gpu_experts_multi_device cost 0.05058717727661133 seconds
DEBUG 01-15 16:09:00.529879.529879 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.062454938888549805 seconds
DEBUG 01-15 16:09:00.529843.529843 cuda_h.py:19] end prefill_layer cost 0.06991434097290039 seconds
DEBUG 01-15 16:09:00.530746.530746 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:09:00.530164.530164 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.530298.530298 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:09:00.530146.530146 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:09:00.530617.530617 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:09:00.530997.530997 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 4.124641418457031e-05 seconds
DEBUG 01-15 16:09:00.530230.530230 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 7.462501525878906e-05 seconds
DEBUG 01-15 16:09:00.530357.530357 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.530095.530095 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.530356.530356 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.530677.530677 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.530189.530189 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.530170.530170 cuda_h.py:19] end allocate_cuda_memory cost 0.0003685951232910156 seconds
DEBUG 01-15 16:09:00.531662.531662 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.531094.531094 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.531513.531513 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.531415.531415 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4b0d3d08-cce0-4432-951e-862dacf7f097
DEBUG 01-15 16:09:00.531234.531234 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.531454.531454 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.532112.532112 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4b0d3d08-cce0-4432-951e-862dacf7f097
DEBUG 01-15 16:09:00.532333.532333 cuda_h.py:19] end load_into_gpu_async cost 0.0011937618255615234 seconds
DEBUG 01-15 16:09:00.532797.532797 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.532470.532470 cuda_h.py:19] end restore_tensors2 cost 8.249282836914062e-05 seconds
DEBUG 01-15 16:09:00.532133.532133 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0019178390502929688 seconds
INFO 01-15 16:09:00.532844.532844 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4b0d3d08-cce0-4432-951e-862dacf7f097
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.535967.535967 cuda_h.py:19] end self_attn cost 0.003568887710571289 seconds
DEBUG 01-15 16:09:00.535018.535018 cuda_h.py:19] end iln_self_attn_paln cost 0.005264997482299805 seconds
DEBUG 01-15 16:09:00.535940.535940 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:09:00.535696.535696 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.536555.536555 cuda_h.py:19] end gate cost 0.0007064342498779297 seconds
DEBUG 01-15 16:09:00.536266.536266 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.536813.536813 lmp.py:1912] 
DEBUG 01-15 16:09:00.536813.536813 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.536523.536523 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.536365.536365 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.536154.536154 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.536035.536035 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.536201.536201 lmp.py:1917] 
DEBUG 01-15 16:09:00.536201.536201 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.536798.536798 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.536640.536640 lmp.py:1935]   Expert 34 |     25 | CPU
DEBUG 01-15 16:09:00.536044.536044 lmp.py:1935]   Expert 45 |     65 | CPU
DEBUG 01-15 16:09:00.536733.536733 lmp.py:1935]   Expert 22 |     74 | CPU
DEBUG 01-15 16:09:00.536184.536184 lmp.py:1935]   Expert 57 |     76 | CPU
DEBUG 01-15 16:09:00.536397.536397 lmp.py:1935]   Expert 17 |     96 | CPU
DEBUG 01-15 16:09:00.537609.537609 lmp.py:1935]   Expert 15 |     99 | CPU
DEBUG 01-15 16:09:00.537822.537822 lmp.py:1935]   Expert  4 |    100 | CPU
DEBUG 01-15 16:09:00.537034.537034 lmp.py:1935]   Expert 28 |    106 | CPU
DEBUG 01-15 16:09:00.537154.537154 lmp.py:1935]   Expert 32 |    112 | CPU
DEBUG 01-15 16:09:00.537320.537320 lmp.py:1935]   Expert 60 |    112 | CPU
DEBUG 01-15 16:09:00.537248.537248 lmp.py:1935]   Expert 36 |    124 | CPU
DEBUG 01-15 16:09:00.537176.537176 lmp.py:1935]   Expert 14 |    125 | CPU
DEBUG 01-15 16:09:00.537342.537342 lmp.py:1935]   Expert 12 |    127 | CPU
DEBUG 01-15 16:09:00.537269.537269 lmp.py:1935]   Expert 16 |    127 | CPU
DEBUG 01-15 16:09:00.537959.537959 lmp.py:1935]   Expert 52 |    130 | CPU
DEBUG 01-15 16:09:00.537363.537363 lmp.py:1935]   Expert 25 |    132 | CPU
DEBUG 01-15 16:09:00.537768.537768 lmp.py:1935]   Expert  8 |    135 | CPU
DEBUG 01-15 16:09:00.537172.537172 lmp.py:1935]   Expert  2 |    140 | CPU
DEBUG 01-15 16:09:00.537339.537339 lmp.py:1935]   Expert 35 |    143 | CPU
DEBUG 01-15 16:09:00.537743.537743 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:09:00.537671.537671 lmp.py:1935]   Expert 30 |    154 | CPU
DEBUG 01-15 16:09:00.537360.537360 lmp.py:1935]   Expert 23 |    156 | CPU
DEBUG 01-15 16:09:00.537049.537049 lmp.py:1935]   Expert  0 |    158 | CPU
DEBUG 01-15 16:09:00.537977.537977 lmp.py:1935]   Expert 39 |    158 | CPU
DEBUG 01-15 16:09:00.537428.537428 lmp.py:1935]   Expert 61 |    158 | CPU
DEBUG 01-15 16:09:00.537263.537263 lmp.py:1935]   Expert 13 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537098.537098 lmp.py:1935]   Expert  3 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537456.537456 lmp.py:1935]   Expert 31 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537338.537338 lmp.py:1935]   Expert 42 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537219.537219 lmp.py:1935]   Expert 44 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537862.537862 lmp.py:1935]   Expert 46 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537505.537505 lmp.py:1935]   Expert 41 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537148.537148 lmp.py:1935]   Expert  9 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537460.537460 lmp.py:1935]   Expert 43 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537818.537818 lmp.py:1935]   Expert 18 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537938.537938 lmp.py:1935]   Expert 26 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537058.537058 lmp.py:1935]   Expert 50 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537893.537893 lmp.py:1935]   Expert 62 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537536.537536 lmp.py:1935]   Expert 27 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537940.537940 lmp.py:1935]   Expert 49 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537583.537583 lmp.py:1935]   Expert 51 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537226.537226 lmp.py:1935]   Expert 11 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537360.537360 lmp.py:1935]   Expert 47 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537625.537625 lmp.py:1935]   Expert 19 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537792.537792 lmp.py:1935]   Expert 20 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537196.537196 lmp.py:1935]   Expert 63 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537839.537839 lmp.py:1935]   Expert 55 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537482.537482 lmp.py:1935]   Expert 56 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537363.537363 lmp.py:1935]   Expert 38 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537199.537199 lmp.py:1935]   Expert 48 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537318.537318 lmp.py:1935]   Expert  1 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537200.537200 lmp.py:1935]   Expert 10 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537843.537843 lmp.py:1935]   Expert 54 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537009.537009 lmp.py:1935]   Expert  7 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537175.537175 lmp.py:1935]   Expert 21 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537580.537580 lmp.py:1935]   Expert 33 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537507.537507 lmp.py:1935]   Expert 29 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537673.537673 lmp.py:1935]   Expert 40 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537840.537840 lmp.py:1935]   Expert 24 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537244.537244 lmp.py:1935]   Expert 59 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.537125.537125 lmp.py:1935]   Expert 37 |    332 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.537007.537007 lmp.py:1935]   Expert 58 |    366 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.538650.538650 lmp.py:1935]   Expert  6 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.538293.538293 lmp.py:1935]   Expert 53 |    854 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.538744.538744 lmp.py:1937] 
DEBUG 01-15 16:09:00.538744.538744 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.538910.538910 lmp.py:1938]   CPU:   2979 tokens
DEBUG 01-15 16:09:00.538076.538076 lmp.py:1942]   cuda:1:   4666 tokens (19 experts)
DEBUG 01-15 16:09:00.538481.538481 lmp.py:1942]   cuda:2:   4643 tokens (20 experts)
DEBUG 01-15 16:09:00.538931.538931 lmp.py:1943]   Total GPU:   9309 tokens
DEBUG 01-15 16:09:00.538144.538144 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.538144.538144 lmp.py:1944] 
DEBUG 01-15 16:09:00.538078.538078 cuda_h.py:19] end experts_map_get cost 0.0016944408416748047 seconds
DEBUG 01-15 16:09:00.538875.538875 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.538201.538201 lmp.py:1953] 
DEBUG 01-15 16:09:00.538201.538201 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.538130.538130 cuda_h.py:19] end cpu_experts_submit cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:00.538323.538323 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.538404.538404 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.539209.539209 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.10.gate_proj.weight': 0, 'model.layers.5.mlp.experts.10.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.10.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.11.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.11.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.11.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.13.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.13.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.13.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.18.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.18.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.18.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.33.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.33.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.33.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.40.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.40.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.40.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.41.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.41.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.41.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.59.up_proj.weight': 322961408}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.46.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.46.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.46.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.5.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.5.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.5.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(7462715392, 5767168, 0, 0), (7468482560, 5767168, 5767168, 0), (7456948224, 5767168, 11534336, 0), (7480016896, 5767168, 17301504, 0), (7485784064, 5767168, 23068672, 0), (7474249728, 5767168, 28835840, 0), (7514619904, 5767168, 34603008, 0), (7520387072, 5767168, 40370176, 0), (7508852736, 5767168, 46137344, 0), (7601127424, 5767168, 51904512, 0), (7606894592, 5767168, 57671680, 0), (7595360256, 5767168, 63438848, 0), (7618428928, 5767168, 69206016, 0), (7624196096, 5767168, 74973184, 0), (7612661760, 5767168, 80740352, 0), (7635730432, 5767168, 86507520, 0), (7641497600, 5767168, 92274688, 0), (7629963264, 5767168, 98041856, 0), (7653031936, 5767168, 103809024, 0), (7658799104, 5767168, 109576192, 0), (7647264768, 5767168, 115343360, 0), (7860649984, 5767168, 121110528, 0), (7866417152, 5767168, 126877696, 0), (7854882816, 5767168, 132644864, 0), (7981760512, 5767168, 138412032, 0), (7987527680, 5767168, 144179200, 0), (7975993344, 5767168, 149946368, 0), (7999062016, 5767168, 155713536, 0), (8004829184, 5767168, 161480704, 0), (7993294848, 5767168, 167247872, 0), (8016363520, 5767168, 173015040, 0), (8022130688, 5767168, 178782208, 0), (8010596352, 5767168, 184549376, 0), (8033665024, 5767168, 190316544, 0), (8039432192, 5767168, 196083712, 0), (8027897856, 5767168, 201850880, 0), (8050966528, 5767168, 207618048, 0), (8056733696, 5767168, 213385216, 0), (8045199360, 5767168, 219152384, 0), (8120172544, 5767168, 224919552, 0), (8125939712, 5767168, 230686720, 0), (8114405376, 5767168, 236453888, 0), (8137474048, 5767168, 242221056, 0), (8143241216, 5767168, 247988224, 0), (8131706880, 5767168, 253755392, 0), (8154775552, 5767168, 259522560, 0), (8160542720, 5767168, 265289728, 0), (8149008384, 5767168, 271056896, 0), (8206680064, 5767168, 276824064, 0), (8212447232, 5767168, 282591232, 0), (8200912896, 5767168, 288358400, 0), (8258584576, 5767168, 294125568, 0), (8264351744, 5767168, 299892736, 0), (8252817408, 5767168, 305659904, 0), (8310489088, 5767168, 311427072, 0), (8316256256, 5767168, 317194240, 0), (8304721920, 5767168, 322961408, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (7341604864, 5767168, 17301504, 0), (7347372032, 5767168, 23068672, 0), (7335837696, 5767168, 28835840, 0), (7393509376, 5767168, 34603008, 0), (7399276544, 5767168, 40370176, 0), (7387742208, 5767168, 46137344, 0), (7410810880, 5767168, 51904512, 0), (7416578048, 5767168, 57671680, 0), (7405043712, 5767168, 63438848, 0), (7445413888, 5767168, 69206016, 0), (7451181056, 5767168, 74973184, 0), (7439646720, 5767168, 80740352, 0), (7704936448, 5767168, 86507520, 0), (7710703616, 5767168, 92274688, 0), (7699169280, 5767168, 98041856, 0), (7739539456, 5767168, 103809024, 0), (7745306624, 5767168, 109576192, 0), (7733772288, 5767168, 115343360, 0), (7756840960, 5767168, 121110528, 0), (7762608128, 5767168, 126877696, 0), (7751073792, 5767168, 132644864, 0), (7791443968, 5767168, 138412032, 0), (7797211136, 5767168, 144179200, 0), (7785676800, 5767168, 149946368, 0), (7826046976, 5767168, 155713536, 0), (7831814144, 5767168, 161480704, 0), (7820279808, 5767168, 167247872, 0), (7929856000, 5767168, 173015040, 0), (7935623168, 5767168, 178782208, 0), (7924088832, 5767168, 184549376, 0), (7947157504, 5767168, 190316544, 0), (7952924672, 5767168, 196083712, 0), (7941390336, 5767168, 201850880, 0), (8085569536, 5767168, 207618048, 0), (8091336704, 5767168, 213385216, 0), (8079802368, 5767168, 219152384, 0), (8102871040, 5767168, 224919552, 0), (8108638208, 5767168, 230686720, 0), (8097103872, 5767168, 236453888, 0), (8172077056, 5767168, 242221056, 0), (8177844224, 5767168, 247988224, 0), (8166309888, 5767168, 253755392, 0), (8223981568, 5767168, 259522560, 0), (8229748736, 5767168, 265289728, 0), (8218214400, 5767168, 271056896, 0), (8241283072, 5767168, 276824064, 0), (8247050240, 5767168, 282591232, 0), (8235515904, 5767168, 288358400, 0), (8293187584, 5767168, 294125568, 0), (8298954752, 5767168, 299892736, 0), (8287420416, 5767168, 305659904, 0), (8362393600, 5767168, 311427072, 0), (8368160768, 5767168, 317194240, 0), (8356626432, 5767168, 322961408, 0), (8379695104, 5767168, 328728576, 0), (8385462272, 5767168, 334495744, 0), (8373927936, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8147140f0>, 2: <capsule object NULL at 0x74b355f0d770>}
DEBUG 01-15 16:09:00.539578.539578 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.539746.539746 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.539925.539925 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d43c1679-ae5f-4490-9239-488bc00a613b
DEBUG 01-15 16:09:00.539442.539442 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:00.539501.539501 client.py:127] Model loaded
DEBUG 01-15 16:09:00.539905.539905 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:00.539477.539477 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.540677.540677 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.540495.540495 cuda_h.py:19] end restore2model cost 0.0003352165222167969 seconds
DEBUG 01-15 16:09:00.540596.540596 cuda_h.py:19] end sllm_worker_task cost 0.009851932525634766 seconds
INFO 01-15 16:09:00.540662.540662 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d43c1679-ae5f-4490-9239-488bc00a613b
DEBUG 01-15 16:09:00.541337.541337 cuda_h.py:19] end move_flatidxs cost 0.0008437633514404297 seconds
DEBUG 01-15 16:09:00.541974.541974 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:00.541694.541694 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002885580062866211 seconds
DEBUG 01-15 16:09:00.541386.541386 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.544833.544833 cuda_h.py:19] end restore2model cost 0.0030345916748046875 seconds
DEBUG 01-15 16:09:00.544537.544537 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006175041198730469 seconds
DEBUG 01-15 16:09:00.544332.544332 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.544117.544117 cuda_h.py:19] end gpu_sexperts cost 0.0002663135528564453 seconds
DEBUG 01-15 16:09:00.544516.544516 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.544862.544862 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:09:00.544843.544843 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.544215.544215 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.545194.545194 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009372234344482422 seconds
DEBUG 01-15 16:09:00.546276.546276 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.545583.545583 cuda_h.py:19] end group_tensors cost 0.004308462142944336 seconds
DEBUG 01-15 16:09:00.546050.546050 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.546217.546217 cuda_h.py:19] end gpu_group_list cost 0.00020933151245117188 seconds
DEBUG 01-15 16:09:00.547207.547207 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.549237.549237 cuda_h.py:19] end group pad cost 0.002960681915283203 seconds
DEBUG 01-15 16:09:00.549027.549027 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:00.549503.549503 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001641988754272461 seconds
DEBUG 01-15 16:09:00.549162.549162 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.550701.550701 cuda_h.py:19] end gpu_group_list cost 0.0005872249603271484 seconds
DEBUG 01-15 16:09:00.555629.555629 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.556471.556471 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d43c1679-ae5f-4490-9239-488bc00a613b
DEBUG 01-15 16:09:00.577905.577905 cuda_h.py:19] end group_einsum cost 0.02829599380493164 seconds
DEBUG 01-15 16:09:00.577546.577546 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:00.580995.580995 client.py:127] Model loaded
DEBUG 01-15 16:09:00.580026.580026 cuda_h.py:19] end wait_experts_multi_device cost 0.024176836013793945 seconds
DEBUG 01-15 16:09:00.580742.580742 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.580667.580667 cuda_h.py:19] end get_outputs_cpu1 cost 0.002984762191772461 seconds
DEBUG 01-15 16:09:00.581470.581470 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04172992706298828 seconds
DEBUG 01-15 16:09:00.582098.582098 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0016987323760986328 seconds
DEBUG 01-15 16:09:00.582650.582650 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.583953.583953 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.583190.583190 cuda_h.py:19] end index_scatter cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:09:00.583258.583258 cuda_h.py:19] end cpuoutputsdeal cost 0.001325845718383789 seconds
DEBUG 01-15 16:09:00.583353.583353 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.583970.583970 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.583831.583831 cuda_h.py:19] end gpu_group_tensor cost 0.00014662742614746094 seconds
DEBUG 01-15 16:09:00.583255.583255 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.583764.583764 cuda_h.py:19] end gpu_group_tensor cost 0.00013518333435058594 seconds
DEBUG 01-15 16:09:00.584231.584231 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.584875.584875 cuda_h.py:19] end gpu_group_einsum cost 0.0006008148193359375 seconds
DEBUG 01-15 16:09:00.584873.584873 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.585277.585277 cuda_h.py:19] end gpu_group_einsum cost 0.0005190372467041016 seconds
DEBUG 01-15 16:09:00.585308.585308 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.585517.585517 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.585672.585672 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002434253692626953 seconds
DEBUG 01-15 16:09:00.585727.585727 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.586803.586803 cuda_h.py:19] end concat_expert_out cost 4.673004150390625e-05 seconds
DEBUG 01-15 16:09:00.586262.586262 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.586092.586092 cuda_h.py:19] end index_scatter cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:00.586087.586087 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008709430694580078 seconds
DEBUG 01-15 16:09:00.586978.586978 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.586159.586159 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.586549.586549 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001513957977294922 seconds
DEBUG 01-15 16:09:00.586113.586113 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.586235.586235 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:09:00.586363.586363 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.587617.587617 cuda_h.py:19] end index_scatter cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:09:00.587757.587757 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005035400390625 seconds
DEBUG 01-15 16:09:00.587190.587190 cuda_h.py:19] end gpu_experts_multi_device cost 0.04219222068786621 seconds
DEBUG 01-15 16:09:00.587577.587577 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.0516357421875 seconds
DEBUG 01-15 16:09:00.587365.587365 cuda_h.py:19] end prefill_layer cost 0.05759572982788086 seconds
DEBUG 01-15 16:09:00.587116.587116 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:09:00.587819.587819 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.587998.587998 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:09:00.587847.587847 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:09:00.587649.587649 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:09:00.587923.587923 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 3.361701965332031e-05 seconds
DEBUG 01-15 16:09:00.587964.587964 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 6.580352783203125e-05 seconds
DEBUG 01-15 16:09:00.587852.587852 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.588007.588007 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.588361.588361 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.588297.588297 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.588280.588280 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.588316.588316 cuda_h.py:19] end allocate_cuda_memory cost 0.00026917457580566406 seconds
DEBUG 01-15 16:09:00.588531.588531 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.588824.588824 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.588415.588415 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.588217.588217 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8f4d93d1-8971-4077-b4d1-5935fee94c3f
DEBUG 01-15 16:09:00.588115.588115 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.589356.589356 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.589776.589776 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8f4d93d1-8971-4077-b4d1-5935fee94c3f
DEBUG 01-15 16:09:00.589619.589619 cuda_h.py:19] end load_into_gpu_async cost 0.0010075569152832031 seconds
DEBUG 01-15 16:09:00.589706.589706 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.589909.589909 cuda_h.py:19] end restore_tensors2 cost 8.320808410644531e-05 seconds
DEBUG 01-15 16:09:00.589433.589433 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0016477108001708984 seconds
INFO 01-15 16:09:00.589892.589892 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8f4d93d1-8971-4077-b4d1-5935fee94c3f
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.593076.593076 cuda_h.py:19] end self_attn cost 0.0038290023803710938 seconds
DEBUG 01-15 16:09:00.593656.593656 cuda_h.py:19] end iln_self_attn_paln cost 0.00540614128112793 seconds
DEBUG 01-15 16:09:00.593724.593724 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:09:00.593487.593487 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.594657.594657 cuda_h.py:19] end gate cost 0.0006847381591796875 seconds
DEBUG 01-15 16:09:00.594732.594732 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.594474.594474 lmp.py:1912] 
DEBUG 01-15 16:09:00.594474.594474 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.594475.594475 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.594608.594608 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.594165.594165 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.594292.594292 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.594942.594942 lmp.py:1917] 
DEBUG 01-15 16:09:00.594942.594942 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.594545.594545 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.594102.594102 lmp.py:1935]   Expert  1 |     45 | CPU
DEBUG 01-15 16:09:00.594937.594937 lmp.py:1935]   Expert  7 |     61 | CPU
DEBUG 01-15 16:09:00.594819.594819 lmp.py:1935]   Expert 37 |     71 | CPU
DEBUG 01-15 16:09:00.594462.594462 lmp.py:1935]   Expert 54 |     77 | CPU
DEBUG 01-15 16:09:00.594628.594628 lmp.py:1935]   Expert 17 |     79 | CPU
DEBUG 01-15 16:09:00.594794.594794 lmp.py:1935]   Expert 18 |     84 | CPU
DEBUG 01-15 16:09:00.594960.594960 lmp.py:1935]   Expert 13 |     88 | CPU
DEBUG 01-15 16:09:00.594603.594603 lmp.py:1935]   Expert  9 |     93 | CPU
DEBUG 01-15 16:09:00.594531.594531 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:09:00.595174.595174 lmp.py:1935]   Expert 22 |    102 | CPU
DEBUG 01-15 16:09:00.595532.595532 lmp.py:1935]   Expert  0 |    107 | CPU
DEBUG 01-15 16:09:00.595414.595414 lmp.py:1935]   Expert 26 |    117 | CPU
DEBUG 01-15 16:09:00.595487.595487 lmp.py:1935]   Expert 16 |    118 | CPU
DEBUG 01-15 16:09:00.595368.595368 lmp.py:1935]   Expert 10 |    122 | CPU
DEBUG 01-15 16:09:00.595250.595250 lmp.py:1935]   Expert 63 |    129 | CPU
DEBUG 01-15 16:09:00.595131.595131 lmp.py:1935]   Expert 59 |    133 | CPU
DEBUG 01-15 16:09:00.595297.595297 lmp.py:1935]   Expert 62 |    141 | CPU
DEBUG 01-15 16:09:00.595463.595463 lmp.py:1935]   Expert 43 |    143 | CPU
DEBUG 01-15 16:09:00.595391.595391 lmp.py:1935]   Expert 28 |    146 | CPU
DEBUG 01-15 16:09:00.595525.595525 lmp.py:1935]   Expert 33 |    146 | CPU
DEBUG 01-15 16:09:00.595214.595214 lmp.py:1935]   Expert 29 |    148 | CPU
DEBUG 01-15 16:09:00.595142.595142 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:09:00.595592.595592 lmp.py:1935]   Expert 51 |    163 | CPU
DEBUG 01-15 16:09:00.595282.595282 lmp.py:1935]   Expert 55 |    164 | CPU
DEBUG 01-15 16:09:00.595209.595209 lmp.py:1935]   Expert 23 |    166 | CPU
DEBUG 01-15 16:09:00.595760.595760 lmp.py:1935]   Expert  3 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595833.595833 lmp.py:1935]   Expert 11 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595145.595145 lmp.py:1935]   Expert 32 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595742.595742 lmp.py:1935]   Expert 45 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595577.595577 lmp.py:1935]   Expert 53 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595220.595220 lmp.py:1935]   Expert 40 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595101.595101 lmp.py:1935]   Expert 34 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595267.595267 lmp.py:1935]   Expert 14 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595672.595672 lmp.py:1935]   Expert 41 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595315.595315 lmp.py:1935]   Expert 52 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595720.595720 lmp.py:1935]   Expert 42 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595362.595362 lmp.py:1935]   Expert 21 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595244.595244 lmp.py:1935]   Expert 57 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595364.595364 lmp.py:1935]   Expert 30 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595960.595960 lmp.py:1935]   Expert 15 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595603.595603 lmp.py:1935]   Expert 35 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595485.595485 lmp.py:1935]   Expert  4 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595095.595095 lmp.py:1935]   Expert 12 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595738.595738 lmp.py:1935]   Expert 19 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595666.595666 lmp.py:1935]   Expert 24 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595832.595832 lmp.py:1935]   Expert 50 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595521.595521 lmp.py:1935]   Expert 44 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595449.595449 lmp.py:1935]   Expert 46 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595138.595138 lmp.py:1935]   Expert  8 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595066.595066 lmp.py:1935]   Expert 49 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595755.595755 lmp.py:1935]   Expert 38 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595683.595683 lmp.py:1935]   Expert  6 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595564.595564 lmp.py:1935]   Expert 47 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595969.595969 lmp.py:1935]   Expert 31 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595373.595373 lmp.py:1935]   Expert 61 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595778.595778 lmp.py:1935]   Expert 39 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595182.595182 lmp.py:1935]   Expert  5 |    305 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595349.595349 lmp.py:1935]   Expert 36 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595753.595753 lmp.py:1935]   Expert 27 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595442.595442 lmp.py:1935]   Expert 60 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595132.595132 lmp.py:1935]   Expert 20 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.595033.595033 lmp.py:1935]   Expert 48 |    369 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595067.595067 lmp.py:1935]   Expert 25 |    397 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.595472.595472 lmp.py:1935]   Expert 56 |    557 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.596446.596446 lmp.py:1937] 
DEBUG 01-15 16:09:00.596446.596446 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.596374.596374 lmp.py:1938]   CPU:   2901 tokens
DEBUG 01-15 16:09:00.596778.596778 lmp.py:1942]   cuda:1:   4621 tokens (19 experts)
DEBUG 01-15 16:09:00.596660.596660 lmp.py:1942]   cuda:2:   4766 tokens (20 experts)
DEBUG 01-15 16:09:00.596587.596587 lmp.py:1943]   Total GPU:   9387 tokens
DEBUG 01-15 16:09:00.596515.596515 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.596515.596515 lmp.py:1944] 
DEBUG 01-15 16:09:00.596927.596927 cuda_h.py:19] end experts_map_get cost 0.0018334388732910156 seconds
DEBUG 01-15 16:09:00.596869.596869 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.596764.596764 lmp.py:1953] 
DEBUG 01-15 16:09:00.596764.596764 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.596117.596117 cuda_h.py:19] end cpu_experts_submit cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:09:00.596621.596621 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.596026.596026 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.596225.596225 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.4.gate_proj.weight': 0, 'model.layers.6.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.47.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.47.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.47.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.6.mlp.experts.3.gate_proj.weight': 0, 'model.layers.6.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.25.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.25.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.25.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.31.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.31.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.31.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.32.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.32.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.32.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.34.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.34.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.34.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.36.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.36.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.36.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.6.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.6.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.6.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(8466202624, 5767168, 0, 0), (8471969792, 5767168, 5767168, 0), (8460435456, 5767168, 11534336, 0), (8483504128, 5767168, 17301504, 0), (8489271296, 5767168, 23068672, 0), (8477736960, 5767168, 28835840, 0), (8535408640, 5767168, 34603008, 0), (8541175808, 5767168, 40370176, 0), (8529641472, 5767168, 46137344, 0), (8587313152, 5767168, 51904512, 0), (8593080320, 5767168, 57671680, 0), (8581545984, 5767168, 63438848, 0), (8639217664, 5767168, 69206016, 0), (8644984832, 5767168, 74973184, 0), (8633450496, 5767168, 80740352, 0), (8743026688, 5767168, 86507520, 0), (8748793856, 5767168, 92274688, 0), (8737259520, 5767168, 98041856, 0), (8760328192, 5767168, 103809024, 0), (8766095360, 5767168, 109576192, 0), (8754561024, 5767168, 115343360, 0), (8812232704, 5767168, 121110528, 0), (8817999872, 5767168, 126877696, 0), (8806465536, 5767168, 132644864, 0), (8864137216, 5767168, 138412032, 0), (8869904384, 5767168, 144179200, 0), (8858370048, 5767168, 149946368, 0), (8916041728, 5767168, 155713536, 0), (8921808896, 5767168, 161480704, 0), (8910274560, 5767168, 167247872, 0), (9002549248, 5767168, 173015040, 0), (9008316416, 5767168, 178782208, 0), (8996782080, 5767168, 184549376, 0), (9054453760, 5767168, 190316544, 0), (9060220928, 5767168, 196083712, 0), (9048686592, 5767168, 201850880, 0), (9089056768, 5767168, 207618048, 0), (9094823936, 5767168, 213385216, 0), (9083289600, 5767168, 219152384, 0), (9106358272, 5767168, 224919552, 0), (9112125440, 5767168, 230686720, 0), (9100591104, 5767168, 236453888, 0), (9175564288, 5767168, 242221056, 0), (9181331456, 5767168, 247988224, 0), (9169797120, 5767168, 253755392, 0), (9192865792, 5767168, 259522560, 0), (9198632960, 5767168, 265289728, 0), (9187098624, 5767168, 271056896, 0), (9210167296, 5767168, 276824064, 0), (9215934464, 5767168, 282591232, 0), (9204400128, 5767168, 288358400, 0), (9365880832, 5767168, 294125568, 0), (9371648000, 5767168, 299892736, 0), (9360113664, 5767168, 305659904, 0), (9452388352, 5767168, 311427072, 0), (9458155520, 5767168, 317194240, 0), (9446621184, 5767168, 322961408, 0)], 2: [(8448901120, 5767168, 0, 0), (8454668288, 5767168, 5767168, 0), (8443133952, 5767168, 11534336, 0), (8500805632, 5767168, 17301504, 0), (8506572800, 5767168, 23068672, 0), (8495038464, 5767168, 28835840, 0), (8604614656, 5767168, 34603008, 0), (8610381824, 5767168, 40370176, 0), (8598847488, 5767168, 46137344, 0), (8656519168, 5767168, 51904512, 0), (8662286336, 5767168, 57671680, 0), (8650752000, 5767168, 63438848, 0), (8725725184, 5767168, 69206016, 0), (8731492352, 5767168, 74973184, 0), (8719958016, 5767168, 80740352, 0), (8829534208, 5767168, 86507520, 0), (8835301376, 5767168, 92274688, 0), (8823767040, 5767168, 98041856, 0), (8933343232, 5767168, 103809024, 0), (8939110400, 5767168, 109576192, 0), (8927576064, 5767168, 115343360, 0), (8950644736, 5767168, 121110528, 0), (8956411904, 5767168, 126877696, 0), (8944877568, 5767168, 132644864, 0), (8985247744, 5767168, 138412032, 0), (8991014912, 5767168, 144179200, 0), (8979480576, 5767168, 149946368, 0), (9019850752, 5767168, 155713536, 0), (9025617920, 5767168, 161480704, 0), (9014083584, 5767168, 167247872, 0), (9071755264, 5767168, 173015040, 0), (9077522432, 5767168, 178782208, 0), (9065988096, 5767168, 184549376, 0), (9123659776, 5767168, 190316544, 0), (9129426944, 5767168, 196083712, 0), (9117892608, 5767168, 201850880, 0), (9158262784, 5767168, 207618048, 0), (9164029952, 5767168, 213385216, 0), (9152495616, 5767168, 219152384, 0), (9227468800, 5767168, 224919552, 0), (9233235968, 5767168, 230686720, 0), (9221701632, 5767168, 236453888, 0), (9244770304, 5767168, 242221056, 0), (9250537472, 5767168, 247988224, 0), (9239003136, 5767168, 253755392, 0), (9262071808, 5767168, 259522560, 0), (9267838976, 5767168, 265289728, 0), (9256304640, 5767168, 271056896, 0), (9296674816, 5767168, 276824064, 0), (9302441984, 5767168, 282591232, 0), (9290907648, 5767168, 288358400, 0), (9313976320, 5767168, 294125568, 0), (9319743488, 5767168, 299892736, 0), (9308209152, 5767168, 305659904, 0), (9383182336, 5767168, 311427072, 0), (9388949504, 5767168, 317194240, 0), (9377415168, 5767168, 322961408, 0), (9435086848, 5767168, 328728576, 0), (9440854016, 5767168, 334495744, 0), (9429319680, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a66072a190>, 2: <capsule object NULL at 0x74a6787cc660>}
INFO 01-15 16:09:00.597336.597336 client.py:127] Model loaded
DEBUG 01-15 16:09:00.597180.597180 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.597990.597990 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.597051.597051 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.597561.597561 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c4946f23-2161-42b4-8653-903f6d04bdb5
DEBUG 01-15 16:09:00.597728.597728 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:00.597793.597793 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.597432.597432 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.598374.598374 cuda_h.py:19] end restore2model cost 0.0006792545318603516 seconds
DEBUG 01-15 16:09:00.598296.598296 cuda_h.py:19] end sllm_worker_task cost 0.010050535202026367 seconds
DEBUG 01-15 16:09:00.598316.598316 cuda_h.py:19] end move_flatidxs cost 0.0008397102355957031 seconds
DEBUG 01-15 16:09:00.598239.598239 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:00.598405.598405 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c4946f23-2161-42b4-8653-903f6d04bdb5
DEBUG 01-15 16:09:00.599051.599051 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030765533447265625 seconds
DEBUG 01-15 16:09:00.599365.599365 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.602615.602615 cuda_h.py:19] end restore2model cost 0.0030634403228759766 seconds
DEBUG 01-15 16:09:00.602312.602312 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006386756896972656 seconds
DEBUG 01-15 16:09:00.602869.602869 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.603084.603084 cuda_h.py:19] end gpu_sexperts cost 0.00026869773864746094 seconds
DEBUG 01-15 16:09:00.603245.603245 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.603783.603783 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:09:00.603956.603956 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.603765.603765 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.604931.604931 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009670257568359375 seconds
DEBUG 01-15 16:09:00.604318.604318 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.604743.604743 cuda_h.py:19] end gpu_group_list cost 0.0002148151397705078 seconds
DEBUG 01-15 16:09:00.604526.604526 cuda_h.py:19] end group_tensors cost 0.005192756652832031 seconds
DEBUG 01-15 16:09:00.604338.604338 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.605663.605663 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.607151.607151 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016198158264160156 seconds
DEBUG 01-15 16:09:00.607831.607831 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.607379.607379 cuda_h.py:19] end gpu_group_list cost 0.0003249645233154297 seconds
DEBUG 01-15 16:09:00.607966.607966 cuda_h.py:19] end group pad cost 0.0030701160430908203 seconds
DEBUG 01-15 16:09:00.608770.608770 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:00.610185.610185 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.611797.611797 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c4946f23-2161-42b4-8653-903f6d04bdb5
DEBUG 01-15 16:09:00.636624.636624 cuda_h.py:19] end group_einsum cost 0.02837657928466797 seconds
DEBUG 01-15 16:09:00.636073.636073 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:00.637662.637662 client.py:127] Model loaded
DEBUG 01-15 16:09:00.637250.637250 cuda_h.py:19] end wait_experts_multi_device cost 0.02686476707458496 seconds
DEBUG 01-15 16:09:00.637066.637066 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.639030.639030 cuda_h.py:19] end get_outputs_cpu1 cost 0.0030395984649658203 seconds
DEBUG 01-15 16:09:00.640953.640953 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04286384582519531 seconds
DEBUG 01-15 16:09:00.641860.641860 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0031049251556396484 seconds
DEBUG 01-15 16:09:00.641286.641286 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.642206.642206 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.642634.642634 cuda_h.py:19] end index_scatter cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:09:00.642120.642120 cuda_h.py:19] end cpuoutputsdeal cost 0.0013155937194824219 seconds
DEBUG 01-15 16:09:00.642884.642884 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.642785.642785 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.642930.642930 cuda_h.py:19] end gpu_group_tensor cost 0.00014734268188476562 seconds
DEBUG 01-15 16:09:00.642024.642024 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.642718.642718 cuda_h.py:19] end gpu_group_tensor cost 0.00013136863708496094 seconds
DEBUG 01-15 16:09:00.643377.643377 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.643545.643545 cuda_h.py:19] end gpu_group_einsum cost 0.0006010532379150391 seconds
DEBUG 01-15 16:09:00.643219.643219 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.644297.644297 cuda_h.py:19] end gpu_group_einsum cost 0.0004887580871582031 seconds
DEBUG 01-15 16:09:00.644500.644500 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.644425.644425 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.644103.644103 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002429485321044922 seconds
DEBUG 01-15 16:09:00.644919.644919 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.645472.645472 cuda_h.py:19] end concat_expert_out cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:09:00.645408.645408 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.645662.645662 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:00.645074.645074 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008592605590820312 seconds
DEBUG 01-15 16:09:00.645958.645958 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.645801.645801 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.645568.645568 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001480579376220703 seconds
DEBUG 01-15 16:09:00.645417.645417 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.645910.645910 cuda_h.py:19] end concat_expert_out cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:09:00.645607.645607 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.645908.645908 cuda_h.py:19] end index_scatter cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:09:00.646763.646763 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004887580871582031 seconds
DEBUG 01-15 16:09:00.646766.646766 cuda_h.py:19] end gpu_experts_multi_device cost 0.04295492172241211 seconds
DEBUG 01-15 16:09:00.646676.646676 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.05270648002624512 seconds
DEBUG 01-15 16:09:00.646371.646371 cuda_h.py:19] end prefill_layer cost 0.05880331993103027 seconds
DEBUG 01-15 16:09:00.646201.646201 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:09:00.646665.646665 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.646083.646083 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:09:00.646932.646932 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:09:00.646450.646450 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:09:00.646690.646690 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 3.4332275390625e-05 seconds
DEBUG 01-15 16:09:00.646400.646400 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 7.843971252441406e-05 seconds
DEBUG 01-15 16:09:00.646004.646004 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.646728.646728 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.647459.647459 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.647528.647528 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.647749.647749 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.647937.647937 cuda_h.py:19] end allocate_cuda_memory cost 0.00024175643920898438 seconds
DEBUG 01-15 16:09:00.647330.647330 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.647001.647001 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.647545.647545 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.647348.647348 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c7095570-ff90-4574-b9ae-990d22bb706f
DEBUG 01-15 16:09:00.647914.647914 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.648685.648685 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
INFO 01-15 16:09:00.650944.650944 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c7095570-ff90-4574-b9ae-990d22bb706f
DEBUG 01-15 16:09:00.650748.650748 cuda_h.py:19] end load_into_gpu_async cost 0.0033245086669921875 seconds
DEBUG 01-15 16:09:00.650550.650550 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.651991.651991 cuda_h.py:19] end restore_tensors2 cost 8.344650268554688e-05 seconds
DEBUG 01-15 16:09:00.651608.651608 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003924131393432617 seconds
DEBUG 01-15 16:09:00.651459.651459 cuda_h.py:19] end self_attn cost 0.003061056137084961 seconds
INFO 01-15 16:09:00.651542.651542 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c7095570-ff90-4574-b9ae-990d22bb706f
DEBUG 01-15 16:09:00.651719.651719 cuda_h.py:19] end iln_self_attn_paln cost 0.004704952239990234 seconds
DEBUG 01-15 16:09:00.651370.651370 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:09:00.651563.651563 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.652436.652436 cuda_h.py:19] end gate cost 0.0007102489471435547 seconds
DEBUG 01-15 16:09:00.652371.652371 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.652829.652829 lmp.py:1912] 
DEBUG 01-15 16:09:00.652829.652829 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.652353.652353 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.652010.652010 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.653044.653044 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.653885.653885 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.653535.653535 lmp.py:1917] 
DEBUG 01-15 16:09:00.653535.653535 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.653185.653185 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.653749.653749 lmp.py:1935]   Expert 50 |     45 | CPU
DEBUG 01-15 16:09:00.653637.653637 lmp.py:1935]   Expert  3 |     54 | CPU
DEBUG 01-15 16:09:00.653095.653095 lmp.py:1935]   Expert 46 |     55 | CPU
DEBUG 01-15 16:09:00.653029.653029 lmp.py:1935]   Expert  1 |     77 | CPU
DEBUG 01-15 16:09:00.653487.653487 lmp.py:1935]   Expert  4 |     86 | CPU
DEBUG 01-15 16:09:00.653898.653898 lmp.py:1935]   Expert 29 |     87 | CPU
DEBUG 01-15 16:09:00.653786.653786 lmp.py:1935]   Expert 15 |     95 | CPU
DEBUG 01-15 16:09:00.653198.653198 lmp.py:1935]   Expert 40 |     95 | CPU
DEBUG 01-15 16:09:00.653848.653848 lmp.py:1935]   Expert  8 |    110 | CPU
DEBUG 01-15 16:09:00.653497.653497 lmp.py:1935]   Expert 28 |    112 | CPU
DEBUG 01-15 16:09:00.653670.653670 lmp.py:1935]   Expert 41 |    113 | CPU
DEBUG 01-15 16:09:00.653128.653128 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:09:00.653586.653586 lmp.py:1935]   Expert 27 |    127 | CPU
DEBUG 01-15 16:09:00.653282.653282 lmp.py:1935]   Expert 48 |    127 | CPU
DEBUG 01-15 16:09:00.653739.653739 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:09:00.653959.653959 lmp.py:1935]   Expert 13 |    131 | CPU
DEBUG 01-15 16:09:00.653416.653416 lmp.py:1935]   Expert  7 |    132 | CPU
DEBUG 01-15 16:09:00.653828.653828 lmp.py:1935]   Expert 54 |    133 | CPU
DEBUG 01-15 16:09:00.653477.653477 lmp.py:1935]   Expert 51 |    136 | CPU
DEBUG 01-15 16:09:00.653889.653889 lmp.py:1935]   Expert 60 |    140 | CPU
DEBUG 01-15 16:09:00.653538.653538 lmp.py:1935]   Expert 18 |    141 | CPU
DEBUG 01-15 16:09:00.653950.653950 lmp.py:1935]   Expert 39 |    142 | CPU
DEBUG 01-15 16:09:00.653884.653884 lmp.py:1935]   Expert 14 |    148 | CPU
DEBUG 01-15 16:09:00.653342.653342 lmp.py:1935]   Expert 20 |    148 | CPU
DEBUG 01-15 16:09:00.653800.653800 lmp.py:1935]   Expert 43 |    148 | CPU
DEBUG 01-15 16:09:00.653403.653403 lmp.py:1935]   Expert 56 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.653245.653245 lmp.py:1935]   Expert 52 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.653564.653564 lmp.py:1935]   Expert 36 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.653598.653598 lmp.py:1935]   Expert 55 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.653724.653724 lmp.py:1935]   Expert 10 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.653036.653036 lmp.py:1935]   Expert 11 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.653586.653586 lmp.py:1935]   Expert 45 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.653183.653183 lmp.py:1935]   Expert  5 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.653541.653541 lmp.py:1935]   Expert 62 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.653138.653138 lmp.py:1935]   Expert 57 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.653496.653496 lmp.py:1935]   Expert 44 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.653093.653093 lmp.py:1935]   Expert 33 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.653690.653690 lmp.py:1935]   Expert 58 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.653286.653286 lmp.py:1935]   Expert 25 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.653406.653406 lmp.py:1935]   Expert 53 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.653003.653003 lmp.py:1935]   Expert 32 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.653315.653315 lmp.py:1935]   Expert  2 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654388.654388 lmp.py:1935]   Expert 35 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654654.654654 lmp.py:1935]   Expert 31 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654489.654489 lmp.py:1935]   Expert 21 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654039.654039 lmp.py:1935]   Expert 63 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654636.654636 lmp.py:1935]   Expert 49 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654253.654253 lmp.py:1935]   Expert 17 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654134.654134 lmp.py:1935]   Expert 42 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654015.654015 lmp.py:1935]   Expert 34 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654897.654897 lmp.py:1935]   Expert 37 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654778.654778 lmp.py:1935]   Expert 59 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654136.654136 lmp.py:1935]   Expert  0 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654256.654256 lmp.py:1935]   Expert 22 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654615.654615 lmp.py:1935]   Expert 19 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654973.654973 lmp.py:1935]   Expert 24 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654331.654331 lmp.py:1935]   Expert 61 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654451.654451 lmp.py:1935]   Expert 30 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654332.654332 lmp.py:1935]   Expert 47 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654975.654975 lmp.py:1935]   Expert 38 |    367 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654618.654618 lmp.py:1935]   Expert 26 |    377 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654261.654261 lmp.py:1935]   Expert 12 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654142.654142 lmp.py:1935]   Expert  9 |    684 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.654024.654024 lmp.py:1935]   Expert 23 |    700 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.654952.654952 lmp.py:1937] 
DEBUG 01-15 16:09:00.654952.654952 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.654833.654833 lmp.py:1938]   CPU:   2836 tokens
DEBUG 01-15 16:09:00.654953.654953 lmp.py:1942]   cuda:1:   4800 tokens (20 experts)
DEBUG 01-15 16:09:00.654357.654357 lmp.py:1942]   cuda:2:   4652 tokens (19 experts)
DEBUG 01-15 16:09:00.654000.654000 lmp.py:1943]   Total GPU:   9452 tokens
DEBUG 01-15 16:09:00.654895.654895 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.654895.654895 lmp.py:1944] 
DEBUG 01-15 16:09:00.654022.654022 cuda_h.py:19] end experts_map_get cost 0.0019922256469726562 seconds
DEBUG 01-15 16:09:00.654918.654918 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.654528.654528 lmp.py:1953] 
DEBUG 01-15 16:09:00.654528.654528 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.654550.654550 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:00.654313.654313 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.654003.654003 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.657127.657127 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.44.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.44.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.44.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.7.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.7.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.7.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.7.mlp.experts.9.gate_proj.weight': 0, 'model.layers.7.mlp.experts.9.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.9.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.21.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.21.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.21.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.22.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.22.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.22.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.30.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.30.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.30.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (9538895872, 5767168, 17301504, 0), (9544663040, 5767168, 23068672, 0), (9533128704, 5767168, 28835840, 0), (9590800384, 5767168, 34603008, 0), (9596567552, 5767168, 40370176, 0), (9585033216, 5767168, 46137344, 0), (9694609408, 5767168, 51904512, 0), (9700376576, 5767168, 57671680, 0), (9688842240, 5767168, 63438848, 0), (9798418432, 5767168, 69206016, 0), (9804185600, 5767168, 74973184, 0), (9792651264, 5767168, 80740352, 0), (9833021440, 5767168, 86507520, 0), (9838788608, 5767168, 92274688, 0), (9827254272, 5767168, 98041856, 0), (9902227456, 5767168, 103809024, 0), (9907994624, 5767168, 109576192, 0), (9896460288, 5767168, 115343360, 0), (9954131968, 5767168, 121110528, 0), (9959899136, 5767168, 126877696, 0), (9948364800, 5767168, 132644864, 0), (10040639488, 5767168, 138412032, 0), (10046406656, 5767168, 144179200, 0), (10034872320, 5767168, 149946368, 0), (10092544000, 5767168, 155713536, 0), (10098311168, 5767168, 161480704, 0), (10086776832, 5767168, 167247872, 0), (10161750016, 5767168, 173015040, 0), (10167517184, 5767168, 178782208, 0), (10155982848, 5767168, 184549376, 0), (10265559040, 5767168, 190316544, 0), (10271326208, 5767168, 196083712, 0), (10259791872, 5767168, 201850880, 0), (10282860544, 5767168, 207618048, 0), (10288627712, 5767168, 213385216, 0), (10277093376, 5767168, 219152384, 0), (10403971072, 5767168, 224919552, 0), (10409738240, 5767168, 230686720, 0), (10398203904, 5767168, 236453888, 0), (10421272576, 5767168, 242221056, 0), (10427039744, 5767168, 247988224, 0), (10415505408, 5767168, 253755392, 0), (10473177088, 5767168, 259522560, 0), (10478944256, 5767168, 265289728, 0), (10467409920, 5767168, 271056896, 0), (10507780096, 5767168, 276824064, 0), (10513547264, 5767168, 282591232, 0), (10502012928, 5767168, 288358400, 0), (10525081600, 5767168, 294125568, 0), (10530848768, 5767168, 299892736, 0), (10519314432, 5767168, 305659904, 0), (10559684608, 5767168, 311427072, 0), (10565451776, 5767168, 317194240, 0), (10553917440, 5767168, 322961408, 0), (10594287616, 5767168, 328728576, 0), (10600054784, 5767168, 334495744, 0), (10588520448, 5767168, 340262912, 0)], 2: [(9660006400, 5767168, 0, 0), (9665773568, 5767168, 5767168, 0), (9654239232, 5767168, 11534336, 0), (9677307904, 5767168, 17301504, 0), (9683075072, 5767168, 23068672, 0), (9671540736, 5767168, 28835840, 0), (9711910912, 5767168, 34603008, 0), (9717678080, 5767168, 40370176, 0), (9706143744, 5767168, 46137344, 0), (9867624448, 5767168, 51904512, 0), (9873391616, 5767168, 57671680, 0), (9861857280, 5767168, 63438848, 0), (9884925952, 5767168, 69206016, 0), (9890693120, 5767168, 74973184, 0), (9879158784, 5767168, 80740352, 0), (9919528960, 5767168, 86507520, 0), (9925296128, 5767168, 92274688, 0), (9913761792, 5767168, 98041856, 0), (9936830464, 5767168, 103809024, 0), (9942597632, 5767168, 109576192, 0), (9931063296, 5767168, 115343360, 0), (10023337984, 5767168, 121110528, 0), (10029105152, 5767168, 126877696, 0), (10017570816, 5767168, 132644864, 0), (10057940992, 5767168, 138412032, 0), (10063708160, 5767168, 144179200, 0), (10052173824, 5767168, 149946368, 0), (10075242496, 5767168, 155713536, 0), (10081009664, 5767168, 161480704, 0), (10069475328, 5767168, 167247872, 0), (10109845504, 5767168, 173015040, 0), (10115612672, 5767168, 178782208, 0), (10104078336, 5767168, 184549376, 0), (10127147008, 5767168, 190316544, 0), (10132914176, 5767168, 196083712, 0), (10121379840, 5767168, 201850880, 0), (10144448512, 5767168, 207618048, 0), (10150215680, 5767168, 213385216, 0), (10138681344, 5767168, 219152384, 0), (10230956032, 5767168, 224919552, 0), (10236723200, 5767168, 230686720, 0), (10225188864, 5767168, 236453888, 0), (10317463552, 5767168, 242221056, 0), (10323230720, 5767168, 247988224, 0), (10311696384, 5767168, 253755392, 0), (10352066560, 5767168, 259522560, 0), (10357833728, 5767168, 265289728, 0), (10346299392, 5767168, 271056896, 0), (10455875584, 5767168, 276824064, 0), (10461642752, 5767168, 282591232, 0), (10450108416, 5767168, 288358400, 0), (10490478592, 5767168, 294125568, 0), (10496245760, 5767168, 299892736, 0), (10484711424, 5767168, 305659904, 0), (10576986112, 5767168, 311427072, 0), (10582753280, 5767168, 317194240, 0), (10571218944, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc4fe040>, 2: <capsule object NULL at 0x74a6bc4fe190>}
DEBUG 01-15 16:09:00.657748.657748 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.657154.657154 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.657333.657333 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ebdf4e05-63ab-4b7f-a05b-7e27e2461778
DEBUG 01-15 16:09:00.657978.657978 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:00.657037.657037 client.py:127] Model loaded
DEBUG 01-15 16:09:00.657258.657258 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.657607.657607 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.658742.658742 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.658512.658512 cuda_h.py:19] end restore2model cost 0.00042939186096191406 seconds
DEBUG 01-15 16:09:00.658097.658097 cuda_h.py:19] end sllm_worker_task cost 0.011241674423217773 seconds
DEBUG 01-15 16:09:00.658812.658812 cuda_h.py:19] end move_flatidxs cost 0.0008349418640136719 seconds
DEBUG 01-15 16:09:00.659588.659588 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:00.659232.659232 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ebdf4e05-63ab-4b7f-a05b-7e27e2461778
DEBUG 01-15 16:09:00.659460.659460 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004905223846435547 seconds
DEBUG 01-15 16:09:00.659814.659814 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.662022.662022 cuda_h.py:19] end restore2model cost 0.003034353256225586 seconds
DEBUG 01-15 16:09:00.662111.662111 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008184432983398438 seconds
DEBUG 01-15 16:09:00.662906.662906 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.663837.663837 cuda_h.py:19] end gpu_sexperts cost 0.0002694129943847656 seconds
DEBUG 01-15 16:09:00.663236.663236 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.663820.663820 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:09:00.663278.663278 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.663219.663219 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.664619.664619 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010366439819335938 seconds
DEBUG 01-15 16:09:00.664542.664542 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.663476.663476 cuda_h.py:19] end group_tensors cost 0.004897356033325195 seconds
DEBUG 01-15 16:09:00.664641.664641 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.664789.664789 cuda_h.py:19] end gpu_group_list cost 0.00022220611572265625 seconds
DEBUG 01-15 16:09:00.666932.666932 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.667775.667775 cuda_h.py:19] end group pad cost 0.0029535293579101562 seconds
DEBUG 01-15 16:09:00.667676.667676 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015459060668945312 seconds
DEBUG 01-15 16:09:00.667433.667433 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:00.667414.667414 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.668565.668565 cuda_h.py:19] end gpu_group_list cost 0.00031375885009765625 seconds
DEBUG 01-15 16:09:00.671426.671426 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.672176.672176 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ebdf4e05-63ab-4b7f-a05b-7e27e2461778
DEBUG 01-15 16:09:00.695559.695559 cuda_h.py:19] end group_einsum cost 0.02741408348083496 seconds
DEBUG 01-15 16:09:00.695260.695260 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:00.697505.697505 client.py:127] Model loaded
DEBUG 01-15 16:09:00.697503.697503 cuda_h.py:19] end wait_experts_multi_device cost 0.026099443435668945 seconds
DEBUG 01-15 16:09:00.698458.698458 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.698427.698427 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029463768005371094 seconds
DEBUG 01-15 16:09:00.699964.699964 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.041461944580078125 seconds
DEBUG 01-15 16:09:00.700446.700446 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.001949310302734375 seconds
DEBUG 01-15 16:09:00.700617.700617 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.701556.701556 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.702155.702155 cuda_h.py:19] end index_scatter cost 0.00012254714965820312 seconds
DEBUG 01-15 16:09:00.702018.702018 cuda_h.py:19] end cpuoutputsdeal cost 0.0023958683013916016 seconds
DEBUG 01-15 16:09:00.702247.702247 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.702051.702051 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.703928.703928 cuda_h.py:19] end gpu_group_tensor cost 0.00023627281188964844 seconds
DEBUG 01-15 16:09:00.703686.703686 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.703966.703966 cuda_h.py:19] end gpu_group_tensor cost 0.0002200603485107422 seconds
DEBUG 01-15 16:09:00.703443.703443 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.705693.705693 cuda_h.py:19] end gpu_group_einsum cost 0.0012331008911132812 seconds
DEBUG 01-15 16:09:00.705722.705722 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.706480.706480 cuda_h.py:19] end gpu_group_einsum cost 0.0012271404266357422 seconds
DEBUG 01-15 16:09:00.706213.706213 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.706362.706362 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.707508.707508 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001728534698486328 seconds
DEBUG 01-15 16:09:00.707980.707980 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.707400.707400 cuda_h.py:19] end concat_expert_out cost 6.651878356933594e-05 seconds
DEBUG 01-15 16:09:00.707125.707125 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.707698.707698 cuda_h.py:19] end index_scatter cost 7.319450378417969e-05 seconds
DEBUG 01-15 16:09:00.707574.707574 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008482933044433594 seconds
DEBUG 01-15 16:09:00.707988.707988 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.707036.707036 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.707969.707969 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013327598571777344 seconds
DEBUG 01-15 16:09:00.707433.707433 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.708211.708211 cuda_h.py:19] end concat_expert_out cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:09:00.708862.708862 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.708832.708832 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:00.708402.708402 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004801750183105469 seconds
DEBUG 01-15 16:09:00.708485.708485 cuda_h.py:19] end gpu_experts_multi_device cost 0.044934749603271484 seconds
DEBUG 01-15 16:09:00.708885.708885 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.0567014217376709 seconds
DEBUG 01-15 16:09:00.708215.708215 cuda_h.py:19] end prefill_layer cost 0.06227612495422363 seconds
DEBUG 01-15 16:09:00.709065.709065 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:09:00.709907.709907 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.709272.709272 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:09:00.709875.709875 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:09:00.709770.709770 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:09:00.709296.709296 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 4.5299530029296875e-05 seconds
DEBUG 01-15 16:09:00.709052.709052 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 7.867813110351562e-05 seconds
DEBUG 01-15 16:09:00.709463.709463 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.709778.709778 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.709635.709635 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.709707.709707 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.709087.709087 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.710979.710979 cuda_h.py:19] end allocate_cuda_memory cost 0.0002989768981933594 seconds
DEBUG 01-15 16:09:00.710631.710631 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.710104.710104 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.710132.710132 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.710226.710226 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, fc65c0c6-28d1-488d-879c-88a4114a0ee8
DEBUG 01-15 16:09:00.710898.710898 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.710741.710741 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.711980.711980 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, fc65c0c6-28d1-488d-879c-88a4114a0ee8
DEBUG 01-15 16:09:00.711586.711586 cuda_h.py:19] end load_into_gpu_async cost 0.0015482902526855469 seconds
DEBUG 01-15 16:09:00.711249.711249 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.711637.711637 cuda_h.py:19] end restore_tensors2 cost 8.177757263183594e-05 seconds
DEBUG 01-15 16:09:00.711108.711108 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002263307571411133 seconds
INFO 01-15 16:09:00.711972.711972 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, fc65c0c6-28d1-488d-879c-88a4114a0ee8
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.714931.714931 cuda_h.py:19] end self_attn cost 0.004017353057861328 seconds
DEBUG 01-15 16:09:00.715731.715731 cuda_h.py:19] end iln_self_attn_paln cost 0.0057795047760009766 seconds
DEBUG 01-15 16:09:00.715051.715051 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:09:00.715952.715952 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.715910.715910 cuda_h.py:19] end gate cost 0.0006685256958007812 seconds
DEBUG 01-15 16:09:00.715799.715799 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.716050.716050 lmp.py:1912] 
DEBUG 01-15 16:09:00.716050.716050 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.716667.716667 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.716509.716509 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.716920.716920 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.716961.716961 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.716962.716962 lmp.py:1917] 
DEBUG 01-15 16:09:00.716962.716962 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.716850.716850 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.716983.716983 lmp.py:1935]   Expert 38 |     12 | CPU
DEBUG 01-15 16:09:00.716011.716011 lmp.py:1935]   Expert 39 |     60 | CPU
DEBUG 01-15 16:09:00.716084.716084 lmp.py:1935]   Expert 30 |     72 | CPU
DEBUG 01-15 16:09:00.716873.716873 lmp.py:1935]   Expert  7 |     73 | CPU
DEBUG 01-15 16:09:00.716080.716080 lmp.py:1935]   Expert 14 |     93 | CPU
DEBUG 01-15 16:09:00.716961.716961 lmp.py:1935]   Expert 27 |     94 | CPU
DEBUG 01-15 16:09:00.716604.716604 lmp.py:1935]   Expert 24 |     95 | CPU
DEBUG 01-15 16:09:00.716247.716247 lmp.py:1935]   Expert 36 |     97 | CPU
DEBUG 01-15 16:09:00.716460.716460 lmp.py:1935]   Expert 40 |     97 | CPU
DEBUG 01-15 16:09:00.716579.716579 lmp.py:1935]   Expert 17 |     98 | CPU
DEBUG 01-15 16:09:00.716030.716030 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:09:00.716004.716004 lmp.py:1935]   Expert 32 |    106 | CPU
DEBUG 01-15 16:09:00.716217.716217 lmp.py:1935]   Expert 18 |    108 | CPU
DEBUG 01-15 16:09:00.716429.716429 lmp.py:1935]   Expert 48 |    111 | CPU
DEBUG 01-15 16:09:00.716403.716403 lmp.py:1935]   Expert 12 |    115 | CPU
DEBUG 01-15 16:09:00.716331.716331 lmp.py:1935]   Expert  1 |    117 | CPU
DEBUG 01-15 16:09:00.716497.716497 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:09:00.716663.716663 lmp.py:1935]   Expert 59 |    130 | CPU
DEBUG 01-15 16:09:00.716591.716591 lmp.py:1935]   Expert 42 |    136 | CPU
DEBUG 01-15 16:09:00.716519.716519 lmp.py:1935]   Expert  0 |    140 | CPU
DEBUG 01-15 16:09:00.716446.716446 lmp.py:1935]   Expert 22 |    144 | CPU
DEBUG 01-15 16:09:00.716897.716897 lmp.py:1935]   Expert 53 |    148 | CPU
DEBUG 01-15 16:09:00.716494.716494 lmp.py:1935]   Expert 51 |    149 | CPU
DEBUG 01-15 16:09:00.716991.716991 lmp.py:1935]   Expert  8 |    162 | CPU
DEBUG 01-15 16:09:00.716727.716727 lmp.py:1935]   Expert 44 |    166 | CPU
DEBUG 01-15 16:09:00.716847.716847 lmp.py:1935]   Expert 60 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.716251.716251 lmp.py:1935]   Expert 15 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.716086.716086 lmp.py:1935]   Expert 29 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717491.717491 lmp.py:1935]   Expert 54 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717372.717372 lmp.py:1935]   Expert 35 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717492.717492 lmp.py:1935]   Expert 34 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717135.717135 lmp.py:1935]   Expert 33 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717732.717732 lmp.py:1935]   Expert 47 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717375.717375 lmp.py:1935]   Expert  9 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717064.717064 lmp.py:1935]   Expert 19 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717230.717230 lmp.py:1935]   Expert  3 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717588.717588 lmp.py:1935]   Expert 46 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717039.717039 lmp.py:1935]   Expert 56 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717967.717967 lmp.py:1935]   Expert 21 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717656.717656 lmp.py:1935]   Expert 45 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717107.717107 lmp.py:1935]   Expert 20 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717704.717704 lmp.py:1935]   Expert 49 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717393.717393 lmp.py:1935]   Expert 28 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717798.717798 lmp.py:1935]   Expert 57 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717248.717248 lmp.py:1935]   Expert  2 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717653.717653 lmp.py:1935]   Expert 13 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717773.717773 lmp.py:1935]   Expert 43 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717416.717416 lmp.py:1935]   Expert  4 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717297.717297 lmp.py:1935]   Expert 10 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717225.717225 lmp.py:1935]   Expert 50 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717583.717583 lmp.py:1935]   Expert 41 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717796.717796 lmp.py:1935]   Expert 26 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717246.717246 lmp.py:1935]   Expert 63 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717697.717697 lmp.py:1935]   Expert 37 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717148.717148 lmp.py:1935]   Expert 31 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717791.717791 lmp.py:1935]   Expert 61 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717719.717719 lmp.py:1935]   Expert 52 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717693.717693 lmp.py:1935]   Expert 58 |    320 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717144.717144 lmp.py:1935]   Expert 62 |    324 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717595.717595 lmp.py:1935]   Expert 55 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717522.717522 lmp.py:1935]   Expert 11 |    382 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717165.717165 lmp.py:1935]   Expert 23 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717524.717524 lmp.py:1935]   Expert 25 |    406 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.717167.717167 lmp.py:1935]   Expert  5 |    512 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.717585.717585 lmp.py:1937] 
DEBUG 01-15 16:09:00.717585.717585 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.717658.717658 lmp.py:1938]   CPU:   2754 tokens
DEBUG 01-15 16:09:00.717924.717924 lmp.py:1942]   cuda:1:   4690 tokens (19 experts)
DEBUG 01-15 16:09:00.717997.717997 lmp.py:1942]   cuda:2:   4844 tokens (20 experts)
DEBUG 01-15 16:09:00.717640.717640 lmp.py:1943]   Total GPU:   9534 tokens
DEBUG 01-15 16:09:00.717522.717522 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.717522.717522 lmp.py:1944] 
DEBUG 01-15 16:09:00.717602.717602 cuda_h.py:19] end experts_map_get cost 0.0017924308776855469 seconds
DEBUG 01-15 16:09:00.717220.717220 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.717930.717930 lmp.py:1953] 
DEBUG 01-15 16:09:00.717930.717930 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.717449.717449 cuda_h.py:19] end cpu_experts_submit cost 6.628036499023438e-05 seconds
DEBUG 01-15 16:09:00.717098.717098 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.718087.718087 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.718673.718673 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.41.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.41.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.41.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.8.mlp.experts.4.gate_proj.weight': 0, 'model.layers.8.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.13.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.13.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.13.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.19.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.19.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.19.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.20.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.20.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.20.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.46.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.46.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.46.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.8.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.8.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.8.mlp.experts.61.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (10663493632, 5767168, 17301504, 0), (10669260800, 5767168, 23068672, 0), (10657726464, 5767168, 28835840, 0), (10698096640, 5767168, 34603008, 0), (10703863808, 5767168, 40370176, 0), (10692329472, 5767168, 46137344, 0), (10767302656, 5767168, 51904512, 0), (10773069824, 5767168, 57671680, 0), (10761535488, 5767168, 63438848, 0), (10784604160, 5767168, 69206016, 0), (10790371328, 5767168, 74973184, 0), (10778836992, 5767168, 80740352, 0), (10801905664, 5767168, 86507520, 0), (10807672832, 5767168, 92274688, 0), (10796138496, 5767168, 98041856, 0), (10871111680, 5767168, 103809024, 0), (10876878848, 5767168, 109576192, 0), (10865344512, 5767168, 115343360, 0), (11096031232, 5767168, 121110528, 0), (11101798400, 5767168, 126877696, 0), (11090264064, 5767168, 132644864, 0), (11147935744, 5767168, 138412032, 0), (11153702912, 5767168, 144179200, 0), (11142168576, 5767168, 149946368, 0), (11199840256, 5767168, 155713536, 0), (11205607424, 5767168, 161480704, 0), (11194073088, 5767168, 167247872, 0), (11320950784, 5767168, 173015040, 0), (11326717952, 5767168, 178782208, 0), (11315183616, 5767168, 184549376, 0), (11355553792, 5767168, 190316544, 0), (11361320960, 5767168, 196083712, 0), (11349786624, 5767168, 201850880, 0), (11390156800, 5767168, 207618048, 0), (11395923968, 5767168, 213385216, 0), (11384389632, 5767168, 219152384, 0), (11424759808, 5767168, 224919552, 0), (11430526976, 5767168, 230686720, 0), (11418992640, 5767168, 236453888, 0), (11459362816, 5767168, 242221056, 0), (11465129984, 5767168, 247988224, 0), (11453595648, 5767168, 253755392, 0), (11511267328, 5767168, 259522560, 0), (11517034496, 5767168, 265289728, 0), (11505500160, 5767168, 271056896, 0), (11545870336, 5767168, 276824064, 0), (11551637504, 5767168, 282591232, 0), (11540103168, 5767168, 288358400, 0), (11684282368, 5767168, 294125568, 0), (11690049536, 5767168, 299892736, 0), (11678515200, 5767168, 305659904, 0), (11701583872, 5767168, 311427072, 0), (11707351040, 5767168, 317194240, 0), (11695816704, 5767168, 322961408, 0)], 2: [(10680795136, 5767168, 0, 0), (10686562304, 5767168, 5767168, 0), (10675027968, 5767168, 11534336, 0), (10836508672, 5767168, 17301504, 0), (10842275840, 5767168, 23068672, 0), (10830741504, 5767168, 28835840, 0), (10940317696, 5767168, 34603008, 0), (10946084864, 5767168, 40370176, 0), (10934550528, 5767168, 46137344, 0), (10957619200, 5767168, 51904512, 0), (10963386368, 5767168, 57671680, 0), (10951852032, 5767168, 63438848, 0), (10974920704, 5767168, 69206016, 0), (10980687872, 5767168, 74973184, 0), (10969153536, 5767168, 80740352, 0), (11009523712, 5767168, 86507520, 0), (11015290880, 5767168, 92274688, 0), (11003756544, 5767168, 98041856, 0), (11044126720, 5767168, 103809024, 0), (11049893888, 5767168, 109576192, 0), (11038359552, 5767168, 115343360, 0), (11061428224, 5767168, 121110528, 0), (11067195392, 5767168, 126877696, 0), (11055661056, 5767168, 132644864, 0), (11113332736, 5767168, 138412032, 0), (11119099904, 5767168, 144179200, 0), (11107565568, 5767168, 149946368, 0), (11182538752, 5767168, 155713536, 0), (11188305920, 5767168, 161480704, 0), (11176771584, 5767168, 167247872, 0), (11217141760, 5767168, 173015040, 0), (11222908928, 5767168, 178782208, 0), (11211374592, 5767168, 184549376, 0), (11251744768, 5767168, 190316544, 0), (11257511936, 5767168, 196083712, 0), (11245977600, 5767168, 201850880, 0), (11407458304, 5767168, 207618048, 0), (11413225472, 5767168, 213385216, 0), (11401691136, 5767168, 219152384, 0), (11476664320, 5767168, 224919552, 0), (11482431488, 5767168, 230686720, 0), (11470897152, 5767168, 236453888, 0), (11563171840, 5767168, 242221056, 0), (11568939008, 5767168, 247988224, 0), (11557404672, 5767168, 253755392, 0), (11580473344, 5767168, 259522560, 0), (11586240512, 5767168, 265289728, 0), (11574706176, 5767168, 271056896, 0), (11597774848, 5767168, 276824064, 0), (11603542016, 5767168, 282591232, 0), (11592007680, 5767168, 288358400, 0), (11615076352, 5767168, 294125568, 0), (11620843520, 5767168, 299892736, 0), (11609309184, 5767168, 305659904, 0), (11649679360, 5767168, 311427072, 0), (11655446528, 5767168, 317194240, 0), (11643912192, 5767168, 322961408, 0), (11666980864, 5767168, 328728576, 0), (11672748032, 5767168, 334495744, 0), (11661213696, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814714510>, 2: <capsule object NULL at 0x74b358fb7480>}
INFO 01-15 16:09:00.719317.719317 client.py:127] Model loaded
DEBUG 01-15 16:09:00.719222.719222 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.719133.719133 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.719147.719147 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:00.719196.719196 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.720784.720784 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 008dc98c-b22f-40fa-80a5-955e774deadc
DEBUG 01-15 16:09:00.720203.720203 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.720005.720005 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.720074.720074 cuda_h.py:19] end restore2model cost 0.0010058879852294922 seconds
DEBUG 01-15 16:09:00.720752.720752 cuda_h.py:19] end sllm_worker_task cost 0.011247396469116211 seconds
DEBUG 01-15 16:09:00.721990.721990 cuda_h.py:19] end move_flatidxs cost 0.0008683204650878906 seconds
DEBUG 01-15 16:09:00.721150.721150 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:00.721358.721358 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 008dc98c-b22f-40fa-80a5-955e774deadc
DEBUG 01-15 16:09:00.722868.722868 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004490375518798828 seconds
DEBUG 01-15 16:09:00.722270.722270 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.726214.726214 cuda_h.py:19] end restore2model cost 0.0034253597259521484 seconds
DEBUG 01-15 16:09:00.726045.726045 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008260488510131836 seconds
DEBUG 01-15 16:09:00.726125.726125 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.726916.726916 cuda_h.py:19] end gpu_sexperts cost 0.0002727508544921875 seconds
DEBUG 01-15 16:09:00.726938.726938 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.726145.726145 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.9550323486328125e-05 seconds
DEBUG 01-15 16:09:00.726577.726577 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.726187.726187 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.728179.728179 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015072822570800781 seconds
DEBUG 01-15 16:09:00.728315.728315 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.728867.728867 cuda_h.py:19] end gpu_group_list cost 0.00023627281188964844 seconds
DEBUG 01-15 16:09:00.729890.729890 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.730240.730240 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001104593276977539 seconds
DEBUG 01-15 16:09:00.730833.730833 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.731908.731908 cuda_h.py:19] end gpu_group_list cost 0.00023555755615234375 seconds
DEBUG 01-15 16:09:00.730639.730639 cuda_h.py:19] end group_tensors cost 0.009799003601074219 seconds
DEBUG 01-15 16:09:00.731400.731400 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.731985.731985 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.731140.731140 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 008dc98c-b22f-40fa-80a5-955e774deadc
DEBUG 01-15 16:09:00.734456.734456 cuda_h.py:19] end group pad cost 0.0028274059295654297 seconds
DEBUG 01-15 16:09:00.734438.734438 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:00.761376.761376 client.py:127] Model loaded
DEBUG 01-15 16:09:00.761507.761507 cuda_h.py:19] end wait_experts_multi_device cost 0.029547691345214844 seconds
DEBUG 01-15 16:09:00.761382.761382 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.762894.762894 cuda_h.py:19] end group_einsum cost 0.027953147888183594 seconds
DEBUG 01-15 16:09:00.762124.762124 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:00.765877.765877 cuda_h.py:19] end get_outputs_cpu1 cost 0.0028510093688964844 seconds
DEBUG 01-15 16:09:00.766336.766336 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.046697378158569336 seconds
DEBUG 01-15 16:09:00.767963.767963 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005766391754150391 seconds
DEBUG 01-15 16:09:00.767593.767593 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.770663.770663 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.770687.770687 cuda_h.py:19] end index_scatter cost 0.00015735626220703125 seconds
DEBUG 01-15 16:09:00.770110.770110 cuda_h.py:19] end cpuoutputsdeal cost 0.0030870437622070312 seconds
DEBUG 01-15 16:09:00.771394.771394 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.771563.771563 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.771828.771828 cuda_h.py:19] end gpu_group_tensor cost 0.0003147125244140625 seconds
DEBUG 01-15 16:09:00.771288.771288 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.772314.772314 cuda_h.py:19] end gpu_group_tensor cost 0.0002911090850830078 seconds
DEBUG 01-15 16:09:00.772765.772765 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.773932.773932 cuda_h.py:19] end gpu_group_einsum cost 0.0013003349304199219 seconds
DEBUG 01-15 16:09:00.773125.773125 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.774059.774059 cuda_h.py:19] end gpu_group_einsum cost 0.0005164146423339844 seconds
DEBUG 01-15 16:09:00.774024.774024 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.774677.774677 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.775708.775708 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002541542053222656 seconds
DEBUG 01-15 16:09:00.775630.775630 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.775904.775904 cuda_h.py:19] end concat_expert_out cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:09:00.775046.775046 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.775712.775712 cuda_h.py:19] end index_scatter cost 7.128715515136719e-05 seconds
DEBUG 01-15 16:09:00.775826.775826 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009541511535644531 seconds
DEBUG 01-15 16:09:00.775293.775293 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.775712.775712 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.776480.776480 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014925003051757812 seconds
DEBUG 01-15 16:09:00.776806.776806 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.776213.776213 cuda_h.py:19] end concat_expert_out cost 5.8650970458984375e-05 seconds
DEBUG 01-15 16:09:00.776009.776009 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.776555.776555 cuda_h.py:19] end index_scatter cost 5.459785461425781e-05 seconds
DEBUG 01-15 16:09:00.776887.776887 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005087852478027344 seconds
DEBUG 01-15 16:09:00.776541.776541 cuda_h.py:19] end gpu_experts_multi_device cost 0.049739837646484375 seconds
DEBUG 01-15 16:09:00.776034.776034 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.06136941909790039 seconds
DEBUG 01-15 16:09:00.777833.777833 cuda_h.py:19] end prefill_layer cost 0.06796503067016602 seconds
DEBUG 01-15 16:09:00.777338.777338 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:09:00.777756.777756 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.777697.777697 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:09:00.777307.777307 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:09:00.777494.777494 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:09:00.777404.777404 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 4.6253204345703125e-05 seconds
DEBUG 01-15 16:09:00.777830.777830 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.777666.777666 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 0.00016808509826660156 seconds
DEBUG 01-15 16:09:00.777318.777318 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.777870.777870 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.777416.777416 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.777817.777817 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.778156.778156 cuda_h.py:19] end allocate_cuda_memory cost 0.0003581047058105469 seconds
DEBUG 01-15 16:09:00.778697.778697 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.778381.778381 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.778840.778840 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.778272.778272 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3f1d84f6-ad91-42c4-85d5-473b2b7a80d6
DEBUG 01-15 16:09:00.778157.778157 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.778578.778578 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.779898.779898 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3f1d84f6-ad91-42c4-85d5-473b2b7a80d6
DEBUG 01-15 16:09:00.779755.779755 cuda_h.py:19] end load_into_gpu_async cost 0.0014684200286865234 seconds
DEBUG 01-15 16:09:00.779756.779756 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.780449.780449 cuda_h.py:19] end restore_tensors2 cost 8.630752563476562e-05 seconds
DEBUG 01-15 16:09:00.780484.780484 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024652481079101562 seconds
INFO 01-15 16:09:00.780705.780705 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3f1d84f6-ad91-42c4-85d5-473b2b7a80d6
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.782872.782872 cuda_h.py:19] end self_attn cost 0.003590106964111328 seconds
DEBUG 01-15 16:09:00.782300.782300 cuda_h.py:19] end iln_self_attn_paln cost 0.0051920413970947266 seconds
DEBUG 01-15 16:09:00.782030.782030 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:09:00.782263.782263 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.783474.783474 cuda_h.py:19] end gate cost 0.0007202625274658203 seconds
DEBUG 01-15 16:09:00.783734.783734 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.784314.784314 lmp.py:1912] 
DEBUG 01-15 16:09:00.784314.784314 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.784832.784832 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.784812.784812 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.784932.784932 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.784145.784145 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.784119.784119 lmp.py:1917] 
DEBUG 01-15 16:09:00.784119.784119 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.784046.784046 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.784127.784127 lmp.py:1935]   Expert 24 |     39 | CPU
DEBUG 01-15 16:09:00.784008.784008 lmp.py:1935]   Expert  2 |     46 | CPU
DEBUG 01-15 16:09:00.784936.784936 lmp.py:1935]   Expert 26 |     62 | CPU
DEBUG 01-15 16:09:00.784864.784864 lmp.py:1935]   Expert 32 |     65 | CPU
DEBUG 01-15 16:09:00.784314.784314 lmp.py:1935]   Expert 19 |     69 | CPU
DEBUG 01-15 16:09:00.784527.784527 lmp.py:1935]   Expert 50 |     70 | CPU
DEBUG 01-15 16:09:00.784501.784501 lmp.py:1935]   Expert 15 |     80 | CPU
DEBUG 01-15 16:09:00.784998.784998 lmp.py:1935]   Expert  7 |     82 | CPU
DEBUG 01-15 16:09:00.784555.784555 lmp.py:1935]   Expert 28 |     82 | CPU
DEBUG 01-15 16:09:00.784503.784503 lmp.py:1935]   Expert  4 |     84 | CPU
DEBUG 01-15 16:09:00.784477.784477 lmp.py:1935]   Expert 60 |     84 | CPU
DEBUG 01-15 16:09:00.784975.784975 lmp.py:1935]   Expert 59 |     90 | CPU
DEBUG 01-15 16:09:00.784187.784187 lmp.py:1935]   Expert 23 |     98 | CPU
DEBUG 01-15 16:09:00.784638.784638 lmp.py:1935]   Expert 49 |     98 | CPU
DEBUG 01-15 16:09:00.784566.784566 lmp.py:1935]   Expert  5 |    103 | CPU
DEBUG 01-15 16:09:00.784017.784017 lmp.py:1935]   Expert 12 |    105 | CPU
DEBUG 01-15 16:09:00.784706.784706 lmp.py:1935]   Expert 10 |    110 | CPU
DEBUG 01-15 16:09:00.784164.784164 lmp.py:1935]   Expert 27 |    113 | CPU
DEBUG 01-15 16:09:00.784522.784522 lmp.py:1935]   Expert 41 |    119 | CPU
DEBUG 01-15 16:09:00.784688.784688 lmp.py:1935]   Expert  3 |    127 | CPU
DEBUG 01-15 16:09:00.784377.784377 lmp.py:1935]   Expert 25 |    128 | CPU
DEBUG 01-15 16:09:00.784020.784020 lmp.py:1935]   Expert 40 |    128 | CPU
DEBUG 01-15 16:09:00.784425.784425 lmp.py:1935]   Expert 20 |    130 | CPU
DEBUG 01-15 16:09:00.784114.784114 lmp.py:1935]   Expert 13 |    132 | CPU
DEBUG 01-15 16:09:00.784519.784519 lmp.py:1935]   Expert 16 |    136 | CPU
DEBUG 01-15 16:09:00.784592.784592 lmp.py:1935]   Expert 17 |    145 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.784904.784904 lmp.py:1935]   Expert 37 |    145 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.784501.784501 lmp.py:1935]   Expert 35 |    146 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.784620.784620 lmp.py:1935]   Expert 47 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.784455.784455 lmp.py:1935]   Expert 22 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.784052.784052 lmp.py:1935]   Expert 53 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.784649.784649 lmp.py:1935]   Expert 39 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.784484.784484 lmp.py:1935]   Expert 38 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.784842.784842 lmp.py:1935]   Expert 36 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.784200.784200 lmp.py:1935]   Expert 44 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.784559.784559 lmp.py:1935]   Expert 52 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.784440.784440 lmp.py:1935]   Expert 58 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.784321.784321 lmp.py:1935]   Expert 18 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.784964.784964 lmp.py:1935]   Expert 62 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.784846.784846 lmp.py:1935]   Expert 11 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.784250.784250 lmp.py:1935]   Expert 48 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.784132.784132 lmp.py:1935]   Expert 14 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785775.785775 lmp.py:1935]   Expert 30 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785087.785087 lmp.py:1935]   Expert  1 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785683.785683 lmp.py:1935]   Expert 42 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785518.785518 lmp.py:1935]   Expert 45 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785638.785638 lmp.py:1935]   Expert 31 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785758.785758 lmp.py:1935]   Expert 51 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785878.785878 lmp.py:1935]   Expert  6 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785759.785759 lmp.py:1935]   Expert 29 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785164.785164 lmp.py:1935]   Expert 34 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785045.785045 lmp.py:1935]   Expert 33 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785688.785688 lmp.py:1935]   Expert 57 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785808.785808 lmp.py:1935]   Expert 61 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785451.785451 lmp.py:1935]   Expert 43 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785332.785332 lmp.py:1935]   Expert  0 |    322 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785975.785975 lmp.py:1935]   Expert 46 |    349 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785857.785857 lmp.py:1935]   Expert  8 |    380 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785261.785261 lmp.py:1935]   Expert  9 |    392 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785381.785381 lmp.py:1935]   Expert 54 |    396 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785501.785501 lmp.py:1935]   Expert 56 |    396 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785621.785621 lmp.py:1935]   Expert 63 |    404 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785171.785171 lmp.py:1935]   Expert 55 |    425 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.785291.785291 lmp.py:1935]   Expert 21 |    495 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.785457.785457 lmp.py:1937] 
DEBUG 01-15 16:09:00.785457.785457 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.785338.785338 lmp.py:1938]   CPU:   2380 tokens
DEBUG 01-15 16:09:00.785458.785458 lmp.py:1942]   cuda:1:   4882 tokens (19 experts)
DEBUG 01-15 16:09:00.785101.785101 lmp.py:1942]   cuda:2:   5026 tokens (20 experts)
DEBUG 01-15 16:09:00.785029.785029 lmp.py:1943]   Total GPU:   9908 tokens
DEBUG 01-15 16:09:00.785195.785195 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.785195.785195 lmp.py:1944] 
DEBUG 01-15 16:09:00.785560.785560 cuda_h.py:19] end experts_map_get cost 0.0017046928405761719 seconds
DEBUG 01-15 16:09:00.785694.785694 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.785027.785027 lmp.py:1953] 
DEBUG 01-15 16:09:00.785027.785027 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.785294.785294 cuda_h.py:19] end cpu_experts_submit cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:09:00.785705.785705 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.785064.785064 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.786216.786216 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.9.gate_proj.weight': 0, 'model.layers.9.mlp.experts.9.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.9.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.14.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.14.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.14.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.17.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.17.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.17.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.18.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.18.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.18.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.30.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.30.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.30.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.33.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.33.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.33.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.35.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.35.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.35.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.36.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.36.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.36.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.37.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.37.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.37.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.39.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.39.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.39.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.44.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.44.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.44.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.45.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.45.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.45.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.9.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.9.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.9.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(11874598912, 5767168, 0, 0), (11880366080, 5767168, 5767168, 0), (11868831744, 5767168, 11534336, 0), (11961106432, 5767168, 17301504, 0), (11966873600, 5767168, 23068672, 0), (11955339264, 5767168, 28835840, 0), (12013010944, 5767168, 34603008, 0), (12018778112, 5767168, 40370176, 0), (12007243776, 5767168, 46137344, 0), (12030312448, 5767168, 51904512, 0), (12036079616, 5767168, 57671680, 0), (12024545280, 5767168, 63438848, 0), (12082216960, 5767168, 69206016, 0), (12087984128, 5767168, 74973184, 0), (12076449792, 5767168, 80740352, 0), (12099518464, 5767168, 86507520, 0), (12105285632, 5767168, 92274688, 0), (12093751296, 5767168, 98041856, 0), (12220628992, 5767168, 103809024, 0), (12226396160, 5767168, 109576192, 0), (12214861824, 5767168, 115343360, 0), (12237930496, 5767168, 121110528, 0), (12243697664, 5767168, 126877696, 0), (12232163328, 5767168, 132644864, 0), (12255232000, 5767168, 138412032, 0), (12260999168, 5767168, 144179200, 0), (12249464832, 5767168, 149946368, 0), (12307136512, 5767168, 155713536, 0), (12312903680, 5767168, 161480704, 0), (12301369344, 5767168, 167247872, 0), (12376342528, 5767168, 173015040, 0), (12382109696, 5767168, 178782208, 0), (12370575360, 5767168, 184549376, 0), (12445548544, 5767168, 190316544, 0), (12451315712, 5767168, 196083712, 0), (12439781376, 5767168, 201850880, 0), (12462850048, 5767168, 207618048, 0), (12468617216, 5767168, 213385216, 0), (12457082880, 5767168, 219152384, 0), (12514754560, 5767168, 224919552, 0), (12520521728, 5767168, 230686720, 0), (12508987392, 5767168, 236453888, 0), (12618563584, 5767168, 242221056, 0), (12624330752, 5767168, 247988224, 0), (12612796416, 5767168, 253755392, 0), (12635865088, 5767168, 259522560, 0), (12641632256, 5767168, 265289728, 0), (12630097920, 5767168, 271056896, 0), (12653166592, 5767168, 276824064, 0), (12658933760, 5767168, 282591232, 0), (12647399424, 5767168, 288358400, 0), (12705071104, 5767168, 294125568, 0), (12710838272, 5767168, 299892736, 0), (12699303936, 5767168, 305659904, 0), (12791578624, 5767168, 311427072, 0), (12797345792, 5767168, 317194240, 0), (12785811456, 5767168, 322961408, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (11736186880, 5767168, 17301504, 0), (11741954048, 5767168, 23068672, 0), (11730419712, 5767168, 28835840, 0), (11822694400, 5767168, 34603008, 0), (11828461568, 5767168, 40370176, 0), (11816927232, 5767168, 46137344, 0), (11857297408, 5767168, 51904512, 0), (11863064576, 5767168, 57671680, 0), (11851530240, 5767168, 63438848, 0), (11909201920, 5767168, 69206016, 0), (11914969088, 5767168, 74973184, 0), (11903434752, 5767168, 80740352, 0), (12289835008, 5767168, 86507520, 0), (12295602176, 5767168, 92274688, 0), (12284067840, 5767168, 98041856, 0), (12324438016, 5767168, 103809024, 0), (12330205184, 5767168, 109576192, 0), (12318670848, 5767168, 115343360, 0), (12341739520, 5767168, 121110528, 0), (12347506688, 5767168, 126877696, 0), (12335972352, 5767168, 132644864, 0), (12359041024, 5767168, 138412032, 0), (12364808192, 5767168, 144179200, 0), (12353273856, 5767168, 149946368, 0), (12393644032, 5767168, 155713536, 0), (12399411200, 5767168, 161480704, 0), (12387876864, 5767168, 167247872, 0), (12480151552, 5767168, 173015040, 0), (12485918720, 5767168, 178782208, 0), (12474384384, 5767168, 184549376, 0), (12497453056, 5767168, 190316544, 0), (12503220224, 5767168, 196083712, 0), (12491685888, 5767168, 201850880, 0), (12532056064, 5767168, 207618048, 0), (12537823232, 5767168, 213385216, 0), (12526288896, 5767168, 219152384, 0), (12549357568, 5767168, 224919552, 0), (12555124736, 5767168, 230686720, 0), (12543590400, 5767168, 236453888, 0), (12601262080, 5767168, 242221056, 0), (12607029248, 5767168, 247988224, 0), (12595494912, 5767168, 253755392, 0), (12670468096, 5767168, 259522560, 0), (12676235264, 5767168, 265289728, 0), (12664700928, 5767168, 271056896, 0), (12687769600, 5767168, 276824064, 0), (12693536768, 5767168, 282591232, 0), (12682002432, 5767168, 288358400, 0), (12722372608, 5767168, 294125568, 0), (12728139776, 5767168, 299892736, 0), (12716605440, 5767168, 305659904, 0), (12774277120, 5767168, 311427072, 0), (12780044288, 5767168, 317194240, 0), (12768509952, 5767168, 322961408, 0), (12808880128, 5767168, 328728576, 0), (12814647296, 5767168, 334495744, 0), (12803112960, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660729f20>, 2: <capsule object NULL at 0x74a6bc4fe1f0>}
DEBUG 01-15 16:09:00.786976.786976 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.786204.786204 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.786006.786006 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 150ed38a-b56f-4238-8422-f2e6c4e02aef
DEBUG 01-15 16:09:00.787708.787708 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.787243.787243 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:00.787501.787501 client.py:127] Model loaded
DEBUG 01-15 16:09:00.787848.787848 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.787021.787021 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.788785.788785 cuda_h.py:19] end restore2model cost 0.00063323974609375 seconds
INFO 01-15 16:09:00.788835.788835 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 150ed38a-b56f-4238-8422-f2e6c4e02aef
DEBUG 01-15 16:09:00.788740.788740 cuda_h.py:19] end move_flatidxs cost 0.0008552074432373047 seconds
DEBUG 01-15 16:09:00.788470.788470 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:00.788136.788136 cuda_h.py:19] end sllm_worker_task cost 0.010965585708618164 seconds
DEBUG 01-15 16:09:00.789929.789929 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003234386444091797 seconds
DEBUG 01-15 16:09:00.789225.789225 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.792300.792300 cuda_h.py:19] end restore2model cost 0.003004312515258789 seconds
DEBUG 01-15 16:09:00.792620.792620 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0065500736236572266 seconds
DEBUG 01-15 16:09:00.792675.792675 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.792718.792718 cuda_h.py:19] end gpu_sexperts cost 0.0002779960632324219 seconds
DEBUG 01-15 16:09:00.792594.792594 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.792702.792702 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:09:00.792921.792921 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.792339.792339 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.792186.792186 cuda_h.py:19] end group_tensors cost 0.004317522048950195 seconds
DEBUG 01-15 16:09:00.793207.793207 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.793373.793373 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009551048278808594 seconds
DEBUG 01-15 16:09:00.793747.793747 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.794320.794320 cuda_h.py:19] end gpu_group_list cost 0.00028061866760253906 seconds
DEBUG 01-15 16:09:00.795381.795381 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.796956.796956 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016353130340576172 seconds
DEBUG 01-15 16:09:00.797158.797158 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.797177.797177 cuda_h.py:19] end gpu_group_list cost 0.0003237724304199219 seconds
DEBUG 01-15 16:09:00.797460.797460 cuda_h.py:19] end group pad cost 0.003922462463378906 seconds
DEBUG 01-15 16:09:00.797488.797488 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:00.798962.798962 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.798136.798136 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 150ed38a-b56f-4238-8422-f2e6c4e02aef
DEBUG 01-15 16:09:00.813284.813284 cuda_h.py:19] end group_einsum cost 0.016161680221557617 seconds
DEBUG 01-15 16:09:00.814739.814739 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:00.816551.816551 cuda_h.py:19] end get_outputs_cpu1 cost 0.002819538116455078 seconds
DEBUG 01-15 16:09:00.817554.817554 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03038311004638672 seconds
INFO 01-15 16:09:00.820196.820196 client.py:127] Model loaded
DEBUG 01-15 16:09:00.820174.820174 cuda_h.py:19] end wait_experts_multi_device cost 0.022490262985229492 seconds
DEBUG 01-15 16:09:00.820897.820897 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.821749.821749 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0004832744598388672 seconds
DEBUG 01-15 16:09:00.821447.821447 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.822975.822975 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.822556.822556 cuda_h.py:19] end index_scatter cost 7.82012939453125e-05 seconds
DEBUG 01-15 16:09:00.822830.822830 cuda_h.py:19] end cpuoutputsdeal cost 0.001333475112915039 seconds
DEBUG 01-15 16:09:00.822409.822409 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.822357.822357 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.823258.823258 cuda_h.py:19] end gpu_group_tensor cost 0.00014853477478027344 seconds
DEBUG 01-15 16:09:00.823498.823498 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.823106.823106 cuda_h.py:19] end gpu_group_tensor cost 0.00013709068298339844 seconds
DEBUG 01-15 16:09:00.823672.823672 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.824642.824642 cuda_h.py:19] end gpu_group_einsum cost 0.0006263256072998047 seconds
DEBUG 01-15 16:09:00.824031.824031 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.824775.824775 cuda_h.py:19] end gpu_group_einsum cost 0.0005676746368408203 seconds
DEBUG 01-15 16:09:00.825296.825296 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.825790.825790 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.825945.825945 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024247169494628906 seconds
DEBUG 01-15 16:09:00.825808.825808 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.825407.825407 cuda_h.py:19] end concat_expert_out cost 4.76837158203125e-05 seconds
DEBUG 01-15 16:09:00.825011.825011 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.825942.825942 cuda_h.py:19] end index_scatter cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:09:00.826499.826499 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008680820465087891 seconds
DEBUG 01-15 16:09:00.826151.826151 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.826855.826855 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.826815.826815 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015044212341308594 seconds
DEBUG 01-15 16:09:00.826379.826379 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.826064.826064 cuda_h.py:19] end concat_expert_out cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:09:00.826523.826523 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.826492.826492 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:09:00.826871.826871 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004940032958984375 seconds
DEBUG 01-15 16:09:00.826635.826635 cuda_h.py:19] end gpu_experts_multi_device cost 0.033966779708862305 seconds
DEBUG 01-15 16:09:00.826498.826498 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.04382133483886719 seconds
DEBUG 01-15 16:09:00.827777.827777 cuda_h.py:19] end prefill_layer cost 0.050049781799316406 seconds
DEBUG 01-15 16:09:00.827203.827203 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:09:00.827721.827721 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.827378.827378 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:09:00.827226.827226 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:09:00.827505.827505 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:09:00.827878.827878 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 3.6716461181640625e-05 seconds
DEBUG 01-15 16:09:00.827873.827873 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:09:00.827953.827953 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.827062.827062 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.827375.827375 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.827955.827955 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.827460.827460 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.828482.828482 cuda_h.py:19] end allocate_cuda_memory cost 0.0002155303955078125 seconds
DEBUG 01-15 16:09:00.828484.828484 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.828015.828015 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.828176.828176 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.828833.828833 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7681a4a5-7210-4834-b664-f2c29e01df31
DEBUG 01-15 16:09:00.828279.828279 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.828396.828396 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.829790.829790 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7681a4a5-7210-4834-b664-f2c29e01df31
DEBUG 01-15 16:09:00.829202.829202 cuda_h.py:19] end load_into_gpu_async cost 0.0012264251708984375 seconds
DEBUG 01-15 16:09:00.829673.829673 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.829584.829584 cuda_h.py:19] end restore_tensors2 cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:09:00.829539.829539 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0018000602722167969 seconds
INFO 01-15 16:09:00.829196.829196 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7681a4a5-7210-4834-b664-f2c29e01df31
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.832698.832698 cuda_h.py:19] end self_attn cost 0.0038077831268310547 seconds
DEBUG 01-15 16:09:00.832675.832675 cuda_h.py:19] end iln_self_attn_paln cost 0.005309104919433594 seconds
DEBUG 01-15 16:09:00.832458.832458 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:09:00.832168.832168 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.833740.833740 cuda_h.py:19] end gate cost 0.0006246566772460938 seconds
DEBUG 01-15 16:09:00.833331.833331 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.833944.833944 lmp.py:1912] 
DEBUG 01-15 16:09:00.833944.833944 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.834700.834700 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.834634.834634 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.834185.834185 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.834589.834589 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.834279.834279 lmp.py:1917] 
DEBUG 01-15 16:09:00.834279.834279 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.834637.834637 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.834525.834525 lmp.py:1935]   Expert 43 |     16 | CPU
DEBUG 01-15 16:09:00.834691.834691 lmp.py:1935]   Expert 27 |     31 | CPU
DEBUG 01-15 16:09:00.834904.834904 lmp.py:1935]   Expert 26 |     51 | CPU
DEBUG 01-15 16:09:00.834116.834116 lmp.py:1935]   Expert 56 |     53 | CPU
DEBUG 01-15 16:09:00.834329.834329 lmp.py:1935]   Expert 34 |     54 | CPU
DEBUG 01-15 16:09:00.834064.834064 lmp.py:1935]   Expert  3 |     58 | CPU
DEBUG 01-15 16:09:00.834184.834184 lmp.py:1935]   Expert  4 |     67 | CPU
DEBUG 01-15 16:09:00.834350.834350 lmp.py:1935]   Expert 61 |     82 | CPU
DEBUG 01-15 16:09:00.834278.834278 lmp.py:1935]   Expert 14 |     96 | CPU
DEBUG 01-15 16:09:00.834682.834682 lmp.py:1935]   Expert 38 |     99 | CPU
DEBUG 01-15 16:09:00.834087.834087 lmp.py:1935]   Expert  2 |    110 | CPU
DEBUG 01-15 16:09:00.834968.834968 lmp.py:1935]   Expert 17 |    119 | CPU
DEBUG 01-15 16:09:00.834611.834611 lmp.py:1935]   Expert 22 |    123 | CPU
DEBUG 01-15 16:09:00.834778.834778 lmp.py:1935]   Expert 37 |    129 | CPU
DEBUG 01-15 16:09:00.834467.834467 lmp.py:1935]   Expert 47 |    129 | CPU
DEBUG 01-15 16:09:00.834679.834679 lmp.py:1935]   Expert 55 |    132 | CPU
DEBUG 01-15 16:09:00.834130.834130 lmp.py:1935]   Expert 54 |    134 | CPU
DEBUG 01-15 16:09:00.834104.834104 lmp.py:1935]   Expert 28 |    135 | CPU
DEBUG 01-15 16:09:00.834794.834794 lmp.py:1935]   Expert  7 |    145 | CPU
DEBUG 01-15 16:09:00.834483.834483 lmp.py:1935]   Expert 51 |    145 | CPU
DEBUG 01-15 16:09:00.834172.834172 lmp.py:1935]   Expert 48 |    146 | CPU
DEBUG 01-15 16:09:00.834385.834385 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:09:00.834835.834835 lmp.py:1935]   Expert 15 |    147 | CPU
DEBUG 01-15 16:09:00.834525.834525 lmp.py:1935]   Expert 45 |    149 | CPU
DEBUG 01-15 16:09:00.834406.834406 lmp.py:1935]   Expert 60 |    149 | CPU
DEBUG 01-15 16:09:00.834718.834718 lmp.py:1935]   Expert 12 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834076.834076 lmp.py:1935]   Expert 63 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834435.834435 lmp.py:1935]   Expert 19 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834753.834753 lmp.py:1935]   Expert  6 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834396.834396 lmp.py:1935]   Expert 57 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834609.834609 lmp.py:1935]   Expert 52 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834060.834060 lmp.py:1935]   Expert 50 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834510.834510 lmp.py:1935]   Expert 18 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834961.834961 lmp.py:1935]   Expert 44 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834412.834412 lmp.py:1935]   Expert 13 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834340.834340 lmp.py:1935]   Expert 31 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834552.834552 lmp.py:1935]   Expert 30 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834003.834003 lmp.py:1935]   Expert 23 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834454.834454 lmp.py:1935]   Expert 39 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834143.834143 lmp.py:1935]   Expert 53 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834594.834594 lmp.py:1935]   Expert 20 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834522.834522 lmp.py:1935]   Expert 21 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834165.834165 lmp.py:1935]   Expert 59 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834093.834093 lmp.py:1935]   Expert 29 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834259.834259 lmp.py:1935]   Expert 16 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834663.834663 lmp.py:1935]   Expert 36 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834353.834353 lmp.py:1935]   Expert 41 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834804.834804 lmp.py:1935]   Expert 25 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834778.834778 lmp.py:1935]   Expert 49 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.834467.834467 lmp.py:1935]   Expert 32 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834679.834679 lmp.py:1935]   Expert 46 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.834653.834653 lmp.py:1935]   Expert  8 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.835866.835866 lmp.py:1935]   Expert 10 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.835317.835317 lmp.py:1935]   Expert 42 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.835768.835768 lmp.py:1935]   Expert 62 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.835980.835980 lmp.py:1935]   Expert 35 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.835623.835623 lmp.py:1935]   Expert  9 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.835266.835266 lmp.py:1935]   Expert 33 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.835147.835147 lmp.py:1935]   Expert 58 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.835075.835075 lmp.py:1935]   Expert 40 |    390 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.835957.835957 lmp.py:1935]   Expert 11 |    424 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.835884.835884 lmp.py:1935]   Expert  0 |    431 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.835574.835574 lmp.py:1935]   Expert 24 |    563 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.835025.835025 lmp.py:1935]   Expert  1 |    651 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.835522.835522 lmp.py:1937] 
DEBUG 01-15 16:09:00.835522.835522 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.835973.835973 lmp.py:1938]   CPU:   2646 tokens
DEBUG 01-15 16:09:00.835900.835900 lmp.py:1942]   cuda:1:   4745 tokens (19 experts)
DEBUG 01-15 16:09:00.835351.835351 lmp.py:1942]   cuda:2:   4897 tokens (20 experts)
DEBUG 01-15 16:09:00.835610.835610 lmp.py:1943]   Total GPU:   9642 tokens
DEBUG 01-15 16:09:00.835584.835584 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.835584.835584 lmp.py:1944] 
DEBUG 01-15 16:09:00.835757.835757 cuda_h.py:19] end experts_map_get cost 0.001621246337890625 seconds
DEBUG 01-15 16:09:00.835031.835031 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.835118.835118 lmp.py:1953] 
DEBUG 01-15 16:09:00.835118.835118 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.835378.835378 cuda_h.py:19] end cpu_experts_submit cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:09:00.835166.835166 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.835095.835095 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.836184.836184 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.1.gate_proj.weight': 0, 'model.layers.10.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.11.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.11.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.11.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.13.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.13.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.13.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.58.up_proj.weight': 322961408}, 2: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.31.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.31.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.31.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.10.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.10.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.10.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(12843483136, 5767168, 0, 0), (12849250304, 5767168, 5767168, 0), (12837715968, 5767168, 11534336, 0), (13016498176, 5767168, 17301504, 0), (13022265344, 5767168, 23068672, 0), (13010731008, 5767168, 28835840, 0), (13051101184, 5767168, 34603008, 0), (13056868352, 5767168, 40370176, 0), (13045334016, 5767168, 46137344, 0), (13154910208, 5767168, 51904512, 0), (13160677376, 5767168, 57671680, 0), (13149143040, 5767168, 63438848, 0), (13189513216, 5767168, 69206016, 0), (13195280384, 5767168, 74973184, 0), (13183746048, 5767168, 80740352, 0), (13224116224, 5767168, 86507520, 0), (13229883392, 5767168, 92274688, 0), (13218349056, 5767168, 98041856, 0), (13258719232, 5767168, 103809024, 0), (13264486400, 5767168, 109576192, 0), (13252952064, 5767168, 115343360, 0), (13327925248, 5767168, 121110528, 0), (13333692416, 5767168, 126877696, 0), (13322158080, 5767168, 132644864, 0), (13379829760, 5767168, 138412032, 0), (13385596928, 5767168, 144179200, 0), (13374062592, 5767168, 149946368, 0), (13397131264, 5767168, 155713536, 0), (13402898432, 5767168, 161480704, 0), (13391364096, 5767168, 167247872, 0), (13431734272, 5767168, 173015040, 0), (13437501440, 5767168, 178782208, 0), (13425967104, 5767168, 184549376, 0), (13449035776, 5767168, 190316544, 0), (13454802944, 5767168, 196083712, 0), (13443268608, 5767168, 201850880, 0), (13500940288, 5767168, 207618048, 0), (13506707456, 5767168, 213385216, 0), (13495173120, 5767168, 219152384, 0), (13552844800, 5767168, 224919552, 0), (13558611968, 5767168, 230686720, 0), (13547077632, 5767168, 236453888, 0), (13587447808, 5767168, 242221056, 0), (13593214976, 5767168, 247988224, 0), (13581680640, 5767168, 253755392, 0), (13622050816, 5767168, 259522560, 0), (13627817984, 5767168, 265289728, 0), (13616283648, 5767168, 271056896, 0), (13725859840, 5767168, 276824064, 0), (13731627008, 5767168, 282591232, 0), (13720092672, 5767168, 288358400, 0), (13812367360, 5767168, 294125568, 0), (13818134528, 5767168, 299892736, 0), (13806600192, 5767168, 305659904, 0), (13829668864, 5767168, 311427072, 0), (13835436032, 5767168, 317194240, 0), (13823901696, 5767168, 322961408, 0)], 2: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (12929990656, 5767168, 17301504, 0), (12935757824, 5767168, 23068672, 0), (12924223488, 5767168, 28835840, 0), (12964593664, 5767168, 34603008, 0), (12970360832, 5767168, 40370176, 0), (12958826496, 5767168, 46137344, 0), (12981895168, 5767168, 51904512, 0), (12987662336, 5767168, 57671680, 0), (12976128000, 5767168, 63438848, 0), (12999196672, 5767168, 69206016, 0), (13004963840, 5767168, 74973184, 0), (12993429504, 5767168, 80740352, 0), (13033799680, 5767168, 86507520, 0), (13039566848, 5767168, 92274688, 0), (13028032512, 5767168, 98041856, 0), (13103005696, 5767168, 103809024, 0), (13108772864, 5767168, 109576192, 0), (13097238528, 5767168, 115343360, 0), (13137608704, 5767168, 121110528, 0), (13143375872, 5767168, 126877696, 0), (13131841536, 5767168, 132644864, 0), (13172211712, 5767168, 138412032, 0), (13177978880, 5767168, 144179200, 0), (13166444544, 5767168, 149946368, 0), (13241417728, 5767168, 155713536, 0), (13247184896, 5767168, 161480704, 0), (13235650560, 5767168, 167247872, 0), (13345226752, 5767168, 173015040, 0), (13350993920, 5767168, 178782208, 0), (13339459584, 5767168, 184549376, 0), (13362528256, 5767168, 190316544, 0), (13368295424, 5767168, 196083712, 0), (13356761088, 5767168, 201850880, 0), (13518241792, 5767168, 207618048, 0), (13524008960, 5767168, 213385216, 0), (13512474624, 5767168, 219152384, 0), (13535543296, 5767168, 224919552, 0), (13541310464, 5767168, 230686720, 0), (13529776128, 5767168, 236453888, 0), (13673955328, 5767168, 242221056, 0), (13679722496, 5767168, 247988224, 0), (13668188160, 5767168, 253755392, 0), (13691256832, 5767168, 259522560, 0), (13697024000, 5767168, 265289728, 0), (13685489664, 5767168, 271056896, 0), (13743161344, 5767168, 276824064, 0), (13748928512, 5767168, 282591232, 0), (13737394176, 5767168, 288358400, 0), (13846970368, 5767168, 294125568, 0), (13852737536, 5767168, 299892736, 0), (13841203200, 5767168, 305659904, 0), (13898874880, 5767168, 311427072, 0), (13904642048, 5767168, 317194240, 0), (13893107712, 5767168, 322961408, 0), (13916176384, 5767168, 328728576, 0), (13921943552, 5767168, 334495744, 0), (13910409216, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814161ec0>, 2: <capsule object NULL at 0x74a6887a9ec0>}
DEBUG 01-15 16:09:00.836275.836275 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:09:00.836086.836086 client.py:127] Model loaded
DEBUG 01-15 16:09:00.836301.836301 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.836947.836947 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e47ace65-a1e5-4f31-9ab2-356184a19bc0
DEBUG 01-15 16:09:00.836044.836044 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:00.837662.837662 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.836376.836376 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.837532.837532 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.837438.837438 cuda_h.py:19] end restore2model cost 0.0004901885986328125 seconds
DEBUG 01-15 16:09:00.837857.837857 cuda_h.py:19] end sllm_worker_task cost 0.009937524795532227 seconds
INFO 01-15 16:09:00.838171.838171 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e47ace65-a1e5-4f31-9ab2-356184a19bc0
DEBUG 01-15 16:09:00.838146.838146 cuda_h.py:19] end move_flatidxs cost 0.0008702278137207031 seconds
DEBUG 01-15 16:09:00.838758.838758 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:00.838107.838107 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030999183654785156 seconds
DEBUG 01-15 16:09:00.838892.838892 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.841875.841875 cuda_h.py:19] end restore2model cost 0.0030069351196289062 seconds
DEBUG 01-15 16:09:00.841055.841055 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006357431411743164 seconds
DEBUG 01-15 16:09:00.841805.841805 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.842305.842305 cuda_h.py:19] end gpu_sexperts cost 0.0002665519714355469 seconds
DEBUG 01-15 16:09:00.842227.842227 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.842619.842619 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:09:00.842077.842077 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.842833.842833 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.843335.843335 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009360313415527344 seconds
DEBUG 01-15 16:09:00.843324.843324 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.843312.843312 cuda_h.py:19] end gpu_group_list cost 0.0002071857452392578 seconds
DEBUG 01-15 16:09:00.844704.844704 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.845708.845708 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010631084442138672 seconds
DEBUG 01-15 16:09:00.845525.845525 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.845468.845468 cuda_h.py:19] end gpu_group_list cost 0.0002446174621582031 seconds
DEBUG 01-15 16:09:00.846757.846757 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.846778.846778 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e47ace65-a1e5-4f31-9ab2-356184a19bc0
DEBUG 01-15 16:09:00.847875.847875 cuda_h.py:19] end group_tensors cost 0.009380340576171875 seconds
DEBUG 01-15 16:09:00.848362.848362 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.851625.851625 cuda_h.py:19] end group pad cost 0.003266572952270508 seconds
DEBUG 01-15 16:09:00.851614.851614 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:00.875296.875296 client.py:127] Model loaded
DEBUG 01-15 16:09:00.876409.876409 cuda_h.py:19] end wait_experts_multi_device cost 0.029479026794433594 seconds
DEBUG 01-15 16:09:00.876046.876046 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.877983.877983 cuda_h.py:19] end group_einsum cost 0.025363445281982422 seconds
DEBUG 01-15 16:09:00.877835.877835 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:00.880135.880135 cuda_h.py:19] end get_outputs_cpu1 cost 0.0027539730072021484 seconds
DEBUG 01-15 16:09:00.881394.881394 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0439908504486084 seconds
DEBUG 01-15 16:09:00.881589.881589 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005637645721435547 seconds
DEBUG 01-15 16:09:00.882774.882774 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.884716.884716 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.884987.884987 cuda_h.py:19] end index_scatter cost 0.0001583099365234375 seconds
DEBUG 01-15 16:09:00.885946.885946 cuda_h.py:19] end cpuoutputsdeal cost 0.0034096240997314453 seconds
DEBUG 01-15 16:09:00.885439.885439 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.886602.886602 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.886593.886593 cuda_h.py:19] end gpu_group_tensor cost 0.000415802001953125 seconds
DEBUG 01-15 16:09:00.886590.886590 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.887718.887718 cuda_h.py:19] end gpu_group_tensor cost 0.00034546852111816406 seconds
DEBUG 01-15 16:09:00.887508.887508 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.888448.888448 cuda_h.py:19] end gpu_group_einsum cost 0.0012373924255371094 seconds
DEBUG 01-15 16:09:00.889814.889814 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.890955.890955 cuda_h.py:19] end gpu_group_einsum cost 0.0011036396026611328 seconds
DEBUG 01-15 16:09:00.890877.890877 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.890184.890184 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.891224.891224 cuda_h.py:19] end all_expert_outputs_slices cost 0.00047779083251953125 seconds
DEBUG 01-15 16:09:00.891770.891770 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.891717.891717 cuda_h.py:19] end concat_expert_out cost 0.00012731552124023438 seconds
DEBUG 01-15 16:09:00.891126.891126 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.892128.892128 cuda_h.py:19] end index_scatter cost 0.00014734268188476562 seconds
DEBUG 01-15 16:09:00.892326.892326 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0020017623901367188 seconds
DEBUG 01-15 16:09:00.892009.892009 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.893763.893763 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.893420.893420 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003662109375 seconds
DEBUG 01-15 16:09:00.893906.893906 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.893893.893893 cuda_h.py:19] end concat_expert_out cost 0.00012493133544921875 seconds
DEBUG 01-15 16:09:00.893668.893668 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.894688.894688 cuda_h.py:19] end index_scatter cost 0.00012683868408203125 seconds
DEBUG 01-15 16:09:00.894063.894063 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0013048648834228516 seconds
DEBUG 01-15 16:09:00.894208.894208 cuda_h.py:19] end gpu_experts_multi_device cost 0.05212974548339844 seconds
DEBUG 01-15 16:09:00.894393.894393 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.06169414520263672 seconds
DEBUG 01-15 16:09:00.895331.895331 cuda_h.py:19] end prefill_layer cost 0.06820201873779297 seconds
DEBUG 01-15 16:09:00.895767.895767 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:09:00.895523.895523 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.895677.895677 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:09:00.895340.895340 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:09:00.895726.895726 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:09:00.895172.895172 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:09:00.896836.896836 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 8.893013000488281e-05 seconds
DEBUG 01-15 16:09:00.896015.896015 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.896113.896113 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.896787.896787 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.896783.896783 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.896655.896655 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.897821.897821 cuda_h.py:19] end allocate_cuda_memory cost 0.0003333091735839844 seconds
DEBUG 01-15 16:09:00.897638.897638 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.897924.897924 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.897416.897416 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.897311.897311 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8368bacc-c9ec-4cde-ab37-9d797ba88e39
DEBUG 01-15 16:09:00.897440.897440 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.897325.897325 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.898995.898995 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8368bacc-c9ec-4cde-ab37-9d797ba88e39
DEBUG 01-15 16:09:00.898011.898011 cuda_h.py:19] end load_into_gpu_async cost 0.0016069412231445312 seconds
DEBUG 01-15 16:09:00.898959.898959 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.899678.899678 cuda_h.py:19] end restore_tensors2 cost 8.130073547363281e-05 seconds
DEBUG 01-15 16:09:00.899911.899911 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0023534297943115234 seconds
INFO 01-15 16:09:00.899920.899920 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8368bacc-c9ec-4cde-ab37-9d797ba88e39
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.902568.902568 cuda_h.py:19] end self_attn cost 0.004180908203125 seconds
DEBUG 01-15 16:09:00.902184.902184 cuda_h.py:19] end iln_self_attn_paln cost 0.006365537643432617 seconds
DEBUG 01-15 16:09:00.902895.902895 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:09:00.902194.902194 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.903175.903175 cuda_h.py:19] end gate cost 0.0007822513580322266 seconds
DEBUG 01-15 16:09:00.903171.903171 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.903504.903504 lmp.py:1912] 
DEBUG 01-15 16:09:00.903504.903504 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.903042.903042 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.904865.904865 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.904475.904475 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.904747.904747 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.904735.904735 lmp.py:1917] 
DEBUG 01-15 16:09:00.904735.904735 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.904981.904981 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.904314.904314 lmp.py:1935]   Expert 13 |     15 | CPU
DEBUG 01-15 16:09:00.904686.904686 lmp.py:1935]   Expert 39 |     15 | CPU
DEBUG 01-15 16:09:00.904481.904481 lmp.py:1935]   Expert 49 |     37 | CPU
DEBUG 01-15 16:09:00.904038.904038 lmp.py:1935]   Expert 35 |     54 | CPU
DEBUG 01-15 16:09:00.904549.904549 lmp.py:1935]   Expert 19 |     64 | CPU
DEBUG 01-15 16:09:00.904345.904345 lmp.py:1935]   Expert  9 |     71 | CPU
DEBUG 01-15 16:09:00.904902.904902 lmp.py:1935]   Expert 32 |     76 | CPU
DEBUG 01-15 16:09:00.904505.904505 lmp.py:1935]   Expert 41 |     76 | CPU
DEBUG 01-15 16:09:00.904493.904493 lmp.py:1935]   Expert 26 |     77 | CPU
DEBUG 01-15 16:09:00.904964.904964 lmp.py:1935]   Expert 33 |     81 | CPU
DEBUG 01-15 16:09:00.904197.904197 lmp.py:1935]   Expert 23 |     87 | CPU
DEBUG 01-15 16:09:00.904827.904827 lmp.py:1935]   Expert 46 |     88 | CPU
DEBUG 01-15 16:09:00.904014.904014 lmp.py:1935]   Expert 18 |     92 | CPU
DEBUG 01-15 16:09:00.904525.904525 lmp.py:1935]   Expert 31 |     92 | CPU
DEBUG 01-15 16:09:00.904082.904082 lmp.py:1935]   Expert 38 |     99 | CPU
DEBUG 01-15 16:09:00.904877.904877 lmp.py:1935]   Expert 17 |    100 | CPU
DEBUG 01-15 16:09:00.904819.904819 lmp.py:1935]   Expert  3 |    103 | CPU
DEBUG 01-15 16:09:00.904329.904329 lmp.py:1935]   Expert  6 |    107 | CPU
DEBUG 01-15 16:09:00.904171.904171 lmp.py:1935]   Expert 20 |    119 | CPU
DEBUG 01-15 16:09:00.904013.904013 lmp.py:1935]   Expert 40 |    128 | CPU
DEBUG 01-15 16:09:00.904762.904762 lmp.py:1935]   Expert 61 |    130 | CPU
DEBUG 01-15 16:09:00.904127.904127 lmp.py:1935]   Expert 62 |    131 | CPU
DEBUG 01-15 16:09:00.904731.904731 lmp.py:1935]   Expert 15 |    132 | CPU
DEBUG 01-15 16:09:00.904348.904348 lmp.py:1935]   Expert 43 |    134 | CPU
DEBUG 01-15 16:09:00.904620.904620 lmp.py:1935]   Expert 50 |    134 | CPU
DEBUG 01-15 16:09:00.904707.904707 lmp.py:1935]   Expert 44 |    135 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.904410.904410 lmp.py:1935]   Expert 16 |    138 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.904967.904967 lmp.py:1935]   Expert 59 |    140 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.904240.904240 lmp.py:1935]   Expert 63 |    140 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.904419.904419 lmp.py:1935]   Expert 42 |    144 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.904169.904169 lmp.py:1935]   Expert  2 |    147 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.904395.904395 lmp.py:1935]   Expert 36 |    151 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905382.905382 lmp.py:1935]   Expert 10 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905191.905191 lmp.py:1935]   Expert  5 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905325.905325 lmp.py:1935]   Expert 34 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905120.905120 lmp.py:1935]   Expert 27 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905439.905439 lmp.py:1935]   Expert 52 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905281.905281 lmp.py:1935]   Expert 45 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905553.905553 lmp.py:1935]   Expert 60 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905826.905826 lmp.py:1935]   Expert 48 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905860.905860 lmp.py:1935]   Expert 51 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905417.905417 lmp.py:1935]   Expert 56 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905120.905120 lmp.py:1935]   Expert 24 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905200.905200 lmp.py:1935]   Expert 53 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905995.905995 lmp.py:1935]   Expert  7 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905328.905328 lmp.py:1935]   Expert  8 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905507.905507 lmp.py:1935]   Expert 57 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905780.905780 lmp.py:1935]   Expert 47 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905290.905290 lmp.py:1935]   Expert 21 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905040.905040 lmp.py:1935]   Expert 29 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905266.905266 lmp.py:1935]   Expert  0 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905300.905300 lmp.py:1935]   Expert 14 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905618.905618 lmp.py:1935]   Expert  4 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905937.905937 lmp.py:1935]   Expert 22 |    312 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905925.905925 lmp.py:1935]   Expert 55 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905197.905197 lmp.py:1935]   Expert 58 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905960.905960 lmp.py:1935]   Expert 37 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905947.905947 lmp.py:1935]   Expert  1 |    323 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905935.905935 lmp.py:1935]   Expert 54 |    332 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905400.905400 lmp.py:1935]   Expert 28 |    364 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905195.905195 lmp.py:1935]   Expert 12 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905514.905514 lmp.py:1935]   Expert 25 |    396 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905309.905309 lmp.py:1935]   Expert 11 |    399 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.905012.905012 lmp.py:1935]   Expert 30 |    836 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.905854.905854 lmp.py:1937] 
DEBUG 01-15 16:09:00.905854.905854 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.905411.905411 lmp.py:1938]   CPU:   2242 tokens
DEBUG 01-15 16:09:00.905690.905690 lmp.py:1942]   cuda:1:   5024 tokens (19 experts)
INFO 01-15 16:09:00.906888.906888 client.py:127] Model loaded
DEBUG 01-15 16:09:00.906211.906211 lmp.py:1942]   cuda:2:   5022 tokens (20 experts)
DEBUG 01-15 16:09:00.906757.906757 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.906355.906355 lmp.py:1943]   Total GPU:  10046 tokens
DEBUG 01-15 16:09:00.906282.906282 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.906282.906282 lmp.py:1944] 
DEBUG 01-15 16:09:00.907304.907304 cuda_h.py:19] end experts_map_get cost 0.0035355091094970703 seconds
DEBUG 01-15 16:09:00.907702.907702 cuda_h.py:19] end restore2model cost 0.0007936954498291016 seconds
DEBUG 01-15 16:09:00.907546.907546 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.907151.907151 cuda_h.py:19] end sllm_worker_task cost 0.010828256607055664 seconds
DEBUG 01-15 16:09:00.907704.907704 lmp.py:1953] 
DEBUG 01-15 16:09:00.907704.907704 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.907803.907803 cuda_h.py:19] end cpu_experts_submit cost 0.0001747608184814453 seconds
DEBUG 01-15 16:09:00.907174.907174 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.907170.907170 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.908558.908558 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.1.gate_proj.weight': 0, 'model.layers.11.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.36.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.36.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.36.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.59.up_proj.weight': 322961408}, 2: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.37.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.37.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.37.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.42.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.42.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.42.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.44.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.44.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.44.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.47.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.47.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.47.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.51.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.51.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.51.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.11.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.11.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.11.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(13950779392, 5767168, 0, 0), (13956546560, 5767168, 5767168, 0), (13945012224, 5767168, 11534336, 0), (13968080896, 5767168, 17301504, 0), (13973848064, 5767168, 23068672, 0), (13962313728, 5767168, 28835840, 0), (14054588416, 5767168, 34603008, 0), (14060355584, 5767168, 40370176, 0), (14048821248, 5767168, 46137344, 0), (14175698944, 5767168, 51904512, 0), (14181466112, 5767168, 57671680, 0), (14169931776, 5767168, 63438848, 0), (14210301952, 5767168, 69206016, 0), (14216069120, 5767168, 74973184, 0), (14204534784, 5767168, 80740352, 0), (14296809472, 5767168, 86507520, 0), (14302576640, 5767168, 92274688, 0), (14291042304, 5767168, 98041856, 0), (14314110976, 5767168, 103809024, 0), (14319878144, 5767168, 109576192, 0), (14308343808, 5767168, 115343360, 0), (14348713984, 5767168, 121110528, 0), (14354481152, 5767168, 126877696, 0), (14342946816, 5767168, 132644864, 0), (14400618496, 5767168, 138412032, 0), (14406385664, 5767168, 144179200, 0), (14394851328, 5767168, 149946368, 0), (14417920000, 5767168, 155713536, 0), (14423687168, 5767168, 161480704, 0), (14412152832, 5767168, 167247872, 0), (14435221504, 5767168, 173015040, 0), (14440988672, 5767168, 178782208, 0), (14429454336, 5767168, 184549376, 0), (14452523008, 5767168, 190316544, 0), (14458290176, 5767168, 196083712, 0), (14446755840, 5767168, 201850880, 0), (14521729024, 5767168, 207618048, 0), (14527496192, 5767168, 213385216, 0), (14515961856, 5767168, 219152384, 0), (14556332032, 5767168, 224919552, 0), (14562099200, 5767168, 230686720, 0), (14550564864, 5767168, 236453888, 0), (14712045568, 5767168, 242221056, 0), (14717812736, 5767168, 247988224, 0), (14706278400, 5767168, 253755392, 0), (14763950080, 5767168, 259522560, 0), (14769717248, 5767168, 265289728, 0), (14758182912, 5767168, 271056896, 0), (14885060608, 5767168, 276824064, 0), (14890827776, 5767168, 282591232, 0), (14879293440, 5767168, 288358400, 0), (14919663616, 5767168, 294125568, 0), (14925430784, 5767168, 299892736, 0), (14913896448, 5767168, 305659904, 0), (14954266624, 5767168, 311427072, 0), (14960033792, 5767168, 317194240, 0), (14948499456, 5767168, 322961408, 0)], 2: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (14002683904, 5767168, 17301504, 0), (14008451072, 5767168, 23068672, 0), (13996916736, 5767168, 28835840, 0), (14019985408, 5767168, 34603008, 0), (14025752576, 5767168, 40370176, 0), (14014218240, 5767168, 46137344, 0), (14071889920, 5767168, 51904512, 0), (14077657088, 5767168, 57671680, 0), (14066122752, 5767168, 63438848, 0), (14106492928, 5767168, 69206016, 0), (14112260096, 5767168, 74973184, 0), (14100725760, 5767168, 80740352, 0), (14123794432, 5767168, 86507520, 0), (14129561600, 5767168, 92274688, 0), (14118027264, 5767168, 98041856, 0), (14141095936, 5767168, 103809024, 0), (14146863104, 5767168, 109576192, 0), (14135328768, 5767168, 115343360, 0), (14366015488, 5767168, 121110528, 0), (14371782656, 5767168, 126877696, 0), (14360248320, 5767168, 132644864, 0), (14573633536, 5767168, 138412032, 0), (14579400704, 5767168, 144179200, 0), (14567866368, 5767168, 149946368, 0), (14660141056, 5767168, 155713536, 0), (14665908224, 5767168, 161480704, 0), (14654373888, 5767168, 167247872, 0), (14694744064, 5767168, 173015040, 0), (14700511232, 5767168, 178782208, 0), (14688976896, 5767168, 184549376, 0), (14746648576, 5767168, 190316544, 0), (14752415744, 5767168, 196083712, 0), (14740881408, 5767168, 201850880, 0), (14815854592, 5767168, 207618048, 0), (14821621760, 5767168, 213385216, 0), (14810087424, 5767168, 219152384, 0), (14833156096, 5767168, 224919552, 0), (14838923264, 5767168, 230686720, 0), (14827388928, 5767168, 236453888, 0), (14850457600, 5767168, 242221056, 0), (14856224768, 5767168, 247988224, 0), (14844690432, 5767168, 253755392, 0), (14867759104, 5767168, 259522560, 0), (14873526272, 5767168, 265289728, 0), (14861991936, 5767168, 271056896, 0), (14902362112, 5767168, 276824064, 0), (14908129280, 5767168, 282591232, 0), (14896594944, 5767168, 288358400, 0), (14936965120, 5767168, 294125568, 0), (14942732288, 5767168, 299892736, 0), (14931197952, 5767168, 305659904, 0), (14971568128, 5767168, 311427072, 0), (14977335296, 5767168, 317194240, 0), (14965800960, 5767168, 322961408, 0), (15023472640, 5767168, 328728576, 0), (15029239808, 5767168, 334495744, 0), (15017705472, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a680301ec0>, 2: <capsule object NULL at 0x74a680302160>}
DEBUG 01-15 16:09:00.908920.908920 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.909245.909245 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.909644.909644 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 98037e8e-9bef-4f39-9855-35ec2b0a61a4
DEBUG 01-15 16:09:00.909979.909979 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.909315.909315 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:00.910346.910346 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:00.911820.911820 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 98037e8e-9bef-4f39-9855-35ec2b0a61a4
DEBUG 01-15 16:09:00.911610.911610 cuda_h.py:19] end move_flatidxs cost 0.0008900165557861328 seconds
DEBUG 01-15 16:09:00.911161.911161 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:00.911898.911898 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0041162967681884766 seconds
DEBUG 01-15 16:09:00.912399.912399 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.915089.915089 cuda_h.py:19] end restore2model cost 0.0033483505249023438 seconds
DEBUG 01-15 16:09:00.915681.915681 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007793903350830078 seconds
DEBUG 01-15 16:09:00.915735.915735 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.915846.915846 cuda_h.py:19] end gpu_sexperts cost 0.0002956390380859375 seconds
DEBUG 01-15 16:09:00.915251.915251 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.916750.916750 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1696090698242188e-05 seconds
DEBUG 01-15 16:09:00.916161.916161 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.916269.916269 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.917316.917316 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009856224060058594 seconds
DEBUG 01-15 16:09:00.916268.916268 cuda_h.py:19] end group_tensors cost 0.0051174163818359375 seconds
DEBUG 01-15 16:09:00.917504.917504 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.917863.917863 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.917857.917857 cuda_h.py:19] end gpu_group_list cost 0.00022983551025390625 seconds
DEBUG 01-15 16:09:00.918926.918926 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.919172.919172 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011239051818847656 seconds
DEBUG 01-15 16:09:00.919082.919082 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.919455.919455 cuda_h.py:19] end gpu_group_list cost 0.00024580955505371094 seconds
DEBUG 01-15 16:09:00.920578.920578 cuda_h.py:19] end group pad cost 0.0030989646911621094 seconds
DEBUG 01-15 16:09:00.920542.920542 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:00.920699.920699 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.920303.920303 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 98037e8e-9bef-4f39-9855-35ec2b0a61a4
DEBUG 01-15 16:09:00.942133.942133 cuda_h.py:19] end group_einsum cost 0.021900653839111328 seconds
DEBUG 01-15 16:09:00.942265.942265 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:00.945124.945124 cuda_h.py:19] end get_outputs_cpu1 cost 0.00284576416015625 seconds
DEBUG 01-15 16:09:00.946293.946293 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.036447763442993164 seconds
INFO 01-15 16:09:00.946453.946453 client.py:127] Model loaded
DEBUG 01-15 16:09:00.947948.947948 cuda_h.py:19] end wait_experts_multi_device cost 0.026498794555664062 seconds
DEBUG 01-15 16:09:00.947605.947605 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:00.948433.948433 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0007030963897705078 seconds
DEBUG 01-15 16:09:00.948972.948972 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:00.949786.949786 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.949713.949713 cuda_h.py:19] end index_scatter cost 0.00010156631469726562 seconds
DEBUG 01-15 16:09:00.950938.950938 cuda_h.py:19] end cpuoutputsdeal cost 0.0018768310546875 seconds
DEBUG 01-15 16:09:00.950233.950233 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:00.950831.950831 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.950899.950899 cuda_h.py:19] end gpu_group_tensor cost 0.0002086162567138672 seconds
DEBUG 01-15 16:09:00.950881.950881 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:00.950691.950691 cuda_h.py:19] end gpu_group_tensor cost 0.00019884109497070312 seconds
DEBUG 01-15 16:09:00.950087.950087 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.951720.951720 cuda_h.py:19] end gpu_group_einsum cost 0.0008475780487060547 seconds
DEBUG 01-15 16:09:00.952189.952189 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:00.952572.952572 cuda_h.py:19] end gpu_group_einsum cost 0.0006594657897949219 seconds
DEBUG 01-15 16:09:00.952789.952789 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.953337.953337 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.953727.953727 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002949237823486328 seconds
DEBUG 01-15 16:09:00.953748.953748 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.953210.953210 cuda_h.py:19] end concat_expert_out cost 7.534027099609375e-05 seconds
DEBUG 01-15 16:09:00.953816.953816 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.953860.953860 cuda_h.py:19] end index_scatter cost 7.963180541992188e-05 seconds
DEBUG 01-15 16:09:00.954574.954574 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001203775405883789 seconds
DEBUG 01-15 16:09:00.954903.954903 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:00.954270.954270 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:00.954484.954484 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021457672119140625 seconds
DEBUG 01-15 16:09:00.954830.954830 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:00.954046.954046 cuda_h.py:19] end concat_expert_out cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:09:00.955553.955553 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:00.955921.955921 cuda_h.py:19] end index_scatter cost 7.772445678710938e-05 seconds
DEBUG 01-15 16:09:00.955665.955665 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007507801055908203 seconds
DEBUG 01-15 16:09:00.955384.955384 cuda_h.py:19] end gpu_experts_multi_device cost 0.039252519607543945 seconds
DEBUG 01-15 16:09:00.955739.955739 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.05285954475402832 seconds
DEBUG 01-15 16:09:00.956383.956383 cuda_h.py:19] end prefill_layer cost 0.06025099754333496 seconds
DEBUG 01-15 16:09:00.956360.956360 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:09:00.956129.956129 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:00.956899.956899 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:09:00.956721.956721 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:09:00.956690.956690 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:09:00.956098.956098 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 8.273124694824219e-05 seconds
DEBUG 01-15 16:09:00.956974.956974 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 0.00013637542724609375 seconds
DEBUG 01-15 16:09:00.956021.956021 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:00.956032.956032 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:00.956682.956682 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:00.956353.956353 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:00.957050.957050 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:00.957498.957498 cuda_h.py:19] end allocate_cuda_memory cost 0.00042319297790527344 seconds
DEBUG 01-15 16:09:00.957146.957146 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:00.958064.958064 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.958340.958340 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.958038.958038 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d3855448-685d-426c-84a6-7da38e256298
DEBUG 01-15 16:09:00.958178.958178 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.958279.958279 cuda_h.py:10] start self_attn
INFO 01-15 16:09:00.959264.959264 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d3855448-685d-426c-84a6-7da38e256298
DEBUG 01-15 16:09:00.959871.959871 cuda_h.py:19] end load_into_gpu_async cost 0.0017523765563964844 seconds
DEBUG 01-15 16:09:00.959629.959629 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:00.960015.960015 cuda_h.py:19] end restore_tensors2 cost 0.00015497207641601562 seconds
DEBUG 01-15 16:09:00.960973.960973 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0030820369720458984 seconds
INFO 01-15 16:09:00.960202.960202 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d3855448-685d-426c-84a6-7da38e256298
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:00.962085.962085 cuda_h.py:19] end self_attn cost 0.00420832633972168 seconds
DEBUG 01-15 16:09:00.963466.963466 cuda_h.py:19] end iln_self_attn_paln cost 0.006704807281494141 seconds
DEBUG 01-15 16:09:00.963819.963819 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:09:00.963720.963720 cuda_h.py:10] start gate
DEBUG 01-15 16:09:00.964300.964300 cuda_h.py:19] end gate cost 0.0006747245788574219 seconds
DEBUG 01-15 16:09:00.964607.964607 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:00.964480.964480 lmp.py:1912] 
DEBUG 01-15 16:09:00.964480.964480 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:00.964004.964004 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:00.964892.964892 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:00.964396.964396 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:00.964900.964900 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:00.964305.964305 lmp.py:1917] 
DEBUG 01-15 16:09:00.964305.964305 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:00.964855.964855 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:00.964797.964797 lmp.py:1935]   Expert 12 |     19 | CPU
DEBUG 01-15 16:09:00.964870.964870 lmp.py:1935]   Expert 47 |     25 | CPU
DEBUG 01-15 16:09:00.964798.964798 lmp.py:1935]   Expert 38 |     31 | CPU
DEBUG 01-15 16:09:00.964487.964487 lmp.py:1935]   Expert 27 |     33 | CPU
DEBUG 01-15 16:09:00.964461.964461 lmp.py:1935]   Expert 16 |     36 | CPU
DEBUG 01-15 16:09:00.964912.964912 lmp.py:1935]   Expert 52 |     38 | CPU
DEBUG 01-15 16:09:00.964747.964747 lmp.py:1935]   Expert 63 |     42 | CPU
DEBUG 01-15 16:09:00.964960.964960 lmp.py:1935]   Expert  4 |     60 | CPU
DEBUG 01-15 16:09:00.964410.964410 lmp.py:1935]   Expert 43 |     61 | CPU
DEBUG 01-15 16:09:00.964815.964815 lmp.py:1935]   Expert 44 |     65 | CPU
DEBUG 01-15 16:09:00.964504.964504 lmp.py:1935]   Expert 61 |     65 | CPU
DEBUG 01-15 16:09:00.964001.964001 lmp.py:1935]   Expert 34 |     76 | CPU
DEBUG 01-15 16:09:00.964267.964267 lmp.py:1935]   Expert 53 |     83 | CPU
DEBUG 01-15 16:09:00.964771.964771 lmp.py:1935]   Expert 32 |     88 | CPU
DEBUG 01-15 16:09:00.964222.964222 lmp.py:1935]   Expert  0 |     89 | CPU
DEBUG 01-15 16:09:00.964057.964057 lmp.py:1935]   Expert 37 |     91 | CPU
DEBUG 01-15 16:09:00.964508.964508 lmp.py:1935]   Expert 13 |    102 | CPU
DEBUG 01-15 16:09:00.964244.964244 lmp.py:1935]   Expert 39 |    112 | CPU
DEBUG 01-15 16:09:00.964456.964456 lmp.py:1935]   Expert 21 |    116 | CPU
DEBUG 01-15 16:09:00.964576.964576 lmp.py:1935]   Expert 11 |    121 | CPU
DEBUG 01-15 16:09:00.965140.965140 lmp.py:1935]   Expert 20 |    127 | CPU
DEBUG 01-15 16:09:00.965452.965452 lmp.py:1935]   Expert  8 |    129 | CPU
DEBUG 01-15 16:09:00.965254.965254 lmp.py:1935]   Expert 60 |    133 | CPU
DEBUG 01-15 16:09:00.965089.965089 lmp.py:1935]   Expert 14 |    137 | CPU
DEBUG 01-15 16:09:00.965017.965017 lmp.py:1935]   Expert 57 |    138 | CPU
DEBUG 01-15 16:09:00.965759.965759 lmp.py:1935]   Expert 22 |    139 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965879.965879 lmp.py:1935]   Expert 45 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965522.965522 lmp.py:1935]   Expert  2 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965450.965450 lmp.py:1935]   Expert 23 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965854.965854 lmp.py:1935]   Expert 18 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965020.965020 lmp.py:1935]   Expert 17 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965948.965948 lmp.py:1935]   Expert  7 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965260.965260 lmp.py:1935]   Expert 58 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965426.965426 lmp.py:1935]   Expert 30 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965354.965354 lmp.py:1935]   Expert 42 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965282.965282 lmp.py:1935]   Expert 48 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965209.965209 lmp.py:1935]   Expert 49 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965806.965806 lmp.py:1935]   Expert 55 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965210.965210 lmp.py:1935]   Expert 62 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965046.965046 lmp.py:1935]   Expert 35 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965689.965689 lmp.py:1935]   Expert 51 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965616.965616 lmp.py:1935]   Expert 29 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965544.965544 lmp.py:1935]   Expert 25 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965472.965472 lmp.py:1935]   Expert  6 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965876.965876 lmp.py:1935]   Expert 36 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965473.965473 lmp.py:1935]   Expert  1 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965401.965401 lmp.py:1935]   Expert 31 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965759.965759 lmp.py:1935]   Expert 28 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965925.965925 lmp.py:1935]   Expert  5 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965045.965045 lmp.py:1935]   Expert 41 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965211.965211 lmp.py:1935]   Expert 54 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965900.965900 lmp.py:1935]   Expert  9 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965828.965828 lmp.py:1935]   Expert 19 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965901.965901 lmp.py:1935]   Expert 24 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965591.965591 lmp.py:1935]   Expert 50 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965280.965280 lmp.py:1935]   Expert 46 |    309 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965969.965969 lmp.py:1935]   Expert 59 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965420.965420 lmp.py:1935]   Expert 56 |    376 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965540.965540 lmp.py:1935]   Expert 26 |    406 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965706.965706 lmp.py:1935]   Expert 33 |    423 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965395.965395 lmp.py:1935]   Expert  3 |    589 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965846.965846 lmp.py:1935]   Expert 10 |    642 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965536.965536 lmp.py:1935]   Expert 15 |    646 | GPU2(cuda:2)
DEBUG 01-15 16:09:00.965225.965225 lmp.py:1935]   Expert 40 |    792 | GPU1(cuda:1)
DEBUG 01-15 16:09:00.965676.965676 lmp.py:1937] 
DEBUG 01-15 16:09:00.965676.965676 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:00.965842.965842 lmp.py:1938]   CPU:   2017 tokens
DEBUG 01-15 16:09:00.965008.965008 lmp.py:1942]   cuda:1:   5205 tokens (20 experts)
DEBUG 01-15 16:09:00.965697.965697 lmp.py:1942]   cuda:2:   5066 tokens (19 experts)
DEBUG 01-15 16:09:00.965910.965910 lmp.py:1943]   Total GPU:  10271 tokens
DEBUG 01-15 16:09:00.965645.965645 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:00.965645.965645 lmp.py:1944] 
DEBUG 01-15 16:09:00.965772.965772 cuda_h.py:19] end experts_map_get cost 0.0017418861389160156 seconds
DEBUG 01-15 16:09:00.965191.965191 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:00.966663.966663 lmp.py:1953] 
DEBUG 01-15 16:09:00.966663.966663 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:00.966929.966929 cuda_h.py:19] end cpu_experts_submit cost 5.7220458984375e-05 seconds
DEBUG 01-15 16:09:00.966626.966626 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:00.966330.966330 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:00.967168.967168 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.1.gate_proj.weight': 0, 'model.layers.12.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.55.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.55.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.55.up_proj.weight': 322961408, 'model.layers.12.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.12.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.12.mlp.experts.59.up_proj.weight': 340262912}, 2: {'model.layers.12.mlp.experts.2.gate_proj.weight': 0, 'model.layers.12.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(15058075648, 5767168, 0, 0), (15063842816, 5767168, 5767168, 0), (15052308480, 5767168, 11534336, 0), (15092678656, 5767168, 17301504, 0), (15098445824, 5767168, 23068672, 0), (15086911488, 5767168, 28835840, 0), (15144583168, 5767168, 34603008, 0), (15150350336, 5767168, 40370176, 0), (15138816000, 5767168, 46137344, 0), (15161884672, 5767168, 51904512, 0), (15167651840, 5767168, 57671680, 0), (15156117504, 5767168, 63438848, 0), (15334899712, 5767168, 69206016, 0), (15340666880, 5767168, 74973184, 0), (15329132544, 5767168, 80740352, 0), (15352201216, 5767168, 86507520, 0), (15357968384, 5767168, 92274688, 0), (15346434048, 5767168, 98041856, 0), (15421407232, 5767168, 103809024, 0), (15427174400, 5767168, 109576192, 0), (15415640064, 5767168, 115343360, 0), (15456010240, 5767168, 121110528, 0), (15461777408, 5767168, 126877696, 0), (15450243072, 5767168, 132644864, 0), (15490613248, 5767168, 138412032, 0), (15496380416, 5767168, 144179200, 0), (15484846080, 5767168, 149946368, 0), (15542517760, 5767168, 155713536, 0), (15548284928, 5767168, 161480704, 0), (15536750592, 5767168, 167247872, 0), (15577120768, 5767168, 173015040, 0), (15582887936, 5767168, 178782208, 0), (15571353600, 5767168, 184549376, 0), (15646326784, 5767168, 190316544, 0), (15652093952, 5767168, 196083712, 0), (15640559616, 5767168, 201850880, 0), (15732834304, 5767168, 207618048, 0), (15738601472, 5767168, 213385216, 0), (15727067136, 5767168, 219152384, 0), (15750135808, 5767168, 224919552, 0), (15755902976, 5767168, 230686720, 0), (15744368640, 5767168, 236453888, 0), (15819341824, 5767168, 242221056, 0), (15825108992, 5767168, 247988224, 0), (15813574656, 5767168, 253755392, 0), (15871246336, 5767168, 259522560, 0), (15877013504, 5767168, 265289728, 0), (15865479168, 5767168, 271056896, 0), (15905849344, 5767168, 276824064, 0), (15911616512, 5767168, 282591232, 0), (15900082176, 5767168, 288358400, 0), (15975055360, 5767168, 294125568, 0), (15980822528, 5767168, 299892736, 0), (15969288192, 5767168, 305659904, 0), (15992356864, 5767168, 311427072, 0), (15998124032, 5767168, 317194240, 0), (15986589696, 5767168, 322961408, 0), (16061562880, 5767168, 328728576, 0), (16067330048, 5767168, 334495744, 0), (16055795712, 5767168, 340262912, 0)], 2: [(15075377152, 5767168, 0, 0), (15081144320, 5767168, 5767168, 0), (15069609984, 5767168, 11534336, 0), (15127281664, 5767168, 17301504, 0), (15133048832, 5767168, 23068672, 0), (15121514496, 5767168, 28835840, 0), (15196487680, 5767168, 34603008, 0), (15202254848, 5767168, 40370176, 0), (15190720512, 5767168, 46137344, 0), (15213789184, 5767168, 51904512, 0), (15219556352, 5767168, 57671680, 0), (15208022016, 5767168, 63438848, 0), (15300296704, 5767168, 69206016, 0), (15306063872, 5767168, 74973184, 0), (15294529536, 5767168, 80740352, 0), (15369502720, 5767168, 86507520, 0), (15375269888, 5767168, 92274688, 0), (15363735552, 5767168, 98041856, 0), (15438708736, 5767168, 103809024, 0), (15444475904, 5767168, 109576192, 0), (15432941568, 5767168, 115343360, 0), (15473311744, 5767168, 121110528, 0), (15479078912, 5767168, 126877696, 0), (15467544576, 5767168, 132644864, 0), (15525216256, 5767168, 138412032, 0), (15530983424, 5767168, 144179200, 0), (15519449088, 5767168, 149946368, 0), (15559819264, 5767168, 155713536, 0), (15565586432, 5767168, 161480704, 0), (15554052096, 5767168, 167247872, 0), (15611723776, 5767168, 173015040, 0), (15617490944, 5767168, 178782208, 0), (15605956608, 5767168, 184549376, 0), (15663628288, 5767168, 190316544, 0), (15669395456, 5767168, 196083712, 0), (15657861120, 5767168, 201850880, 0), (15767437312, 5767168, 207618048, 0), (15773204480, 5767168, 213385216, 0), (15761670144, 5767168, 219152384, 0), (15836643328, 5767168, 224919552, 0), (15842410496, 5767168, 230686720, 0), (15830876160, 5767168, 236453888, 0), (15888547840, 5767168, 242221056, 0), (15894315008, 5767168, 247988224, 0), (15882780672, 5767168, 253755392, 0), (15923150848, 5767168, 259522560, 0), (15928918016, 5767168, 265289728, 0), (15917383680, 5767168, 271056896, 0), (16009658368, 5767168, 276824064, 0), (16015425536, 5767168, 282591232, 0), (16003891200, 5767168, 288358400, 0), (16044261376, 5767168, 294125568, 0), (16050028544, 5767168, 299892736, 0), (16038494208, 5767168, 305659904, 0), (16113467392, 5767168, 311427072, 0), (16119234560, 5767168, 317194240, 0), (16107700224, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6887aa160>, 2: <capsule object NULL at 0x74a6bc4fe130>}
DEBUG 01-15 16:09:00.967927.967927 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:00.967983.967983 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:00.967123.967123 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e62d2c2f-c090-4a3b-8cb8-e612a3ec6999
DEBUG 01-15 16:09:00.967118.967118 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:00.968553.968553 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:00.968708.968708 client.py:127] Model loaded
DEBUG 01-15 16:09:00.968010.968010 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.968352.968352 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:00.969140.969140 cuda_h.py:19] end move_flatidxs cost 0.0008900165557861328 seconds
DEBUG 01-15 16:09:00.969517.969517 cuda_h.py:19] end restore2model cost 0.0010209083557128906 seconds
DEBUG 01-15 16:09:00.969122.969122 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:00.969177.969177 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e62d2c2f-c090-4a3b-8cb8-e612a3ec6999
DEBUG 01-15 16:09:00.969360.969360 cuda_h.py:19] end sllm_worker_task cost 0.012658834457397461 seconds
DEBUG 01-15 16:09:00.970876.970876 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0040132999420166016 seconds
DEBUG 01-15 16:09:00.970445.970445 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:00.973233.973233 cuda_h.py:19] end restore2model cost 0.0033168792724609375 seconds
DEBUG 01-15 16:09:00.973904.973904 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007696628570556641 seconds
DEBUG 01-15 16:09:00.973673.973673 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:00.974664.974664 cuda_h.py:19] end gpu_sexperts cost 0.000278472900390625 seconds
DEBUG 01-15 16:09:00.974162.974162 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:00.974721.974721 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:09:00.974417.974417 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:00.974742.974742 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.975148.975148 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010039806365966797 seconds
DEBUG 01-15 16:09:00.975475.975475 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.975622.975622 cuda_h.py:19] end gpu_group_list cost 0.0002193450927734375 seconds
DEBUG 01-15 16:09:00.976844.976844 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:00.977862.977862 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010766983032226562 seconds
DEBUG 01-15 16:09:00.977918.977918 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:00.978853.978853 cuda_h.py:19] end gpu_group_list cost 0.00020194053649902344 seconds
DEBUG 01-15 16:09:00.978850.978850 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:00.978587.978587 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e62d2c2f-c090-4a3b-8cb8-e612a3ec6999
DEBUG 01-15 16:09:00.978805.978805 cuda_h.py:19] end group_tensors cost 0.009475946426391602 seconds
DEBUG 01-15 16:09:00.979524.979524 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:00.982324.982324 cuda_h.py:19] end group pad cost 0.0026803016662597656 seconds
DEBUG 01-15 16:09:00.982829.982829 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:01.002759.002759 cuda_h.py:19] end group_einsum cost 0.020368337631225586 seconds
DEBUG 01-15 16:09:01.003314.003314 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:01.005567.005567 client.py:127] Model loaded
DEBUG 01-15 16:09:01.006089.006089 cuda_h.py:19] end wait_experts_multi_device cost 0.02724742889404297 seconds
DEBUG 01-15 16:09:01.006020.006020 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.007086.007086 cuda_h.py:19] end get_outputs_cpu1 cost 0.003977537155151367 seconds
DEBUG 01-15 16:09:01.007126.007126 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0397646427154541 seconds
DEBUG 01-15 16:09:01.008439.008439 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0026984214782714844 seconds
DEBUG 01-15 16:09:01.009175.009175 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.011977.011977 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.011160.011160 cuda_h.py:19] end index_scatter cost 0.00015473365783691406 seconds
DEBUG 01-15 16:09:01.012389.012389 cuda_h.py:19] end cpuoutputsdeal cost 0.002933979034423828 seconds
DEBUG 01-15 16:09:01.012177.012177 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.012968.012968 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.012266.012266 cuda_h.py:19] end gpu_group_tensor cost 0.0003113746643066406 seconds
DEBUG 01-15 16:09:01.013918.013918 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.013136.013136 cuda_h.py:19] end gpu_group_tensor cost 0.0002913475036621094 seconds
DEBUG 01-15 16:09:01.013018.013018 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.014862.014862 cuda_h.py:19] end gpu_group_einsum cost 0.0011708736419677734 seconds
DEBUG 01-15 16:09:01.015070.015070 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.016029.016029 cuda_h.py:19] end gpu_group_einsum cost 0.0008332729339599609 seconds
DEBUG 01-15 16:09:01.016746.016746 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.016708.016708 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.017487.017487 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004394054412841797 seconds
DEBUG 01-15 16:09:01.017550.017550 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.017238.017238 cuda_h.py:19] end concat_expert_out cost 0.00011587142944335938 seconds
DEBUG 01-15 16:09:01.017919.017919 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.017734.017734 cuda_h.py:19] end index_scatter cost 0.000125885009765625 seconds
DEBUG 01-15 16:09:01.018406.018406 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00179290771484375 seconds
DEBUG 01-15 16:09:01.018168.018168 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.018796.018796 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.019115.019115 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003643035888671875 seconds
DEBUG 01-15 16:09:01.019979.019979 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.019720.019720 cuda_h.py:19] end concat_expert_out cost 0.00012183189392089844 seconds
DEBUG 01-15 16:09:01.019440.019440 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.019672.019672 cuda_h.py:19] end index_scatter cost 0.00011897087097167969 seconds
DEBUG 01-15 16:09:01.019987.019987 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012104511260986328 seconds
DEBUG 01-15 16:09:01.019072.019072 cuda_h.py:19] end gpu_experts_multi_device cost 0.0456395149230957 seconds
DEBUG 01-15 16:09:01.020515.020515 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.05675530433654785 seconds
DEBUG 01-15 16:09:01.020709.020709 cuda_h.py:19] end prefill_layer cost 0.06473040580749512 seconds
DEBUG 01-15 16:09:01.021071.021071 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:09:01.021259.021259 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.021638.021638 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:09:01.021541.021541 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:09:01.021405.021405 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:09:01.021516.021516 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:09:01.021240.021240 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 0.00016236305236816406 seconds
DEBUG 01-15 16:09:01.021805.021805 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.021030.021030 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.022730.022730 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.022807.022807 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.022673.022673 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.023636.023636 cuda_h.py:19] end allocate_cuda_memory cost 0.0005586147308349609 seconds
DEBUG 01-15 16:09:01.023093.023093 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.023282.023282 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.023028.023028 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.023025.023025 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d1a0f2bc-ee37-4801-8062-e64513b298ab
DEBUG 01-15 16:09:01.024847.024847 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.024920.024920 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.025737.025737 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d1a0f2bc-ee37-4801-8062-e64513b298ab
DEBUG 01-15 16:09:01.026029.026029 cuda_h.py:19] end load_into_gpu_async cost 0.0023355484008789062 seconds
DEBUG 01-15 16:09:01.026330.026330 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.026153.026153 cuda_h.py:19] end restore_tensors2 cost 0.00017786026000976562 seconds
DEBUG 01-15 16:09:01.026328.026328 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003809213638305664 seconds
INFO 01-15 16:09:01.026520.026520 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d1a0f2bc-ee37-4801-8062-e64513b298ab
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.029927.029927 cuda_h.py:19] end self_attn cost 0.005211830139160156 seconds
DEBUG 01-15 16:09:01.030968.030968 cuda_h.py:19] end iln_self_attn_paln cost 0.008536338806152344 seconds
DEBUG 01-15 16:09:01.030249.030249 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:09:01.030879.030879 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.031508.031508 cuda_h.py:19] end gate cost 0.0007364749908447266 seconds
DEBUG 01-15 16:09:01.031027.031027 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.031068.031068 lmp.py:1912] 
DEBUG 01-15 16:09:01.031068.031068 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.031699.031699 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.031077.031077 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.031211.031211 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.031722.031722 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.031564.031564 lmp.py:1917] 
DEBUG 01-15 16:09:01.031564.031564 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.031313.031313 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.031784.031784 lmp.py:1935]   Expert 19 |     23 | CPU
DEBUG 01-15 16:09:01.031056.031056 lmp.py:1935]   Expert 42 |     24 | CPU
DEBUG 01-15 16:09:01.031375.031375 lmp.py:1935]   Expert 30 |     27 | CPU
DEBUG 01-15 16:09:01.032455.032455 lmp.py:1935]   Expert 32 |     47 | CPU
DEBUG 01-15 16:09:01.032774.032774 lmp.py:1935]   Expert  6 |     58 | CPU
DEBUG 01-15 16:09:01.032093.032093 lmp.py:1935]   Expert  5 |     73 | CPU
DEBUG 01-15 16:09:01.032604.032604 lmp.py:1935]   Expert 53 |     73 | CPU
DEBUG 01-15 16:09:01.032399.032399 lmp.py:1935]   Expert  1 |     80 | CPU
DEBUG 01-15 16:09:01.032956.032956 lmp.py:1935]   Expert 13 |    118 | CPU
DEBUG 01-15 16:09:01.032990.032990 lmp.py:1935]   Expert  9 |    123 | CPU
DEBUG 01-15 16:09:01.032117.032117 lmp.py:1935]   Expert 63 |    127 | CPU
DEBUG 01-15 16:09:01.032959.032959 lmp.py:1935]   Expert 34 |    128 | CPU
DEBUG 01-15 16:09:01.032800.032800 lmp.py:1935]   Expert 58 |    130 | CPU
DEBUG 01-15 16:09:01.032404.032404 lmp.py:1935]   Expert 50 |    131 | CPU
DEBUG 01-15 16:09:01.032769.032769 lmp.py:1935]   Expert 11 |    135 | CPU
DEBUG 01-15 16:09:01.032372.032372 lmp.py:1935]   Expert 26 |    136 | CPU
DEBUG 01-15 16:09:01.032976.032976 lmp.py:1935]   Expert 31 |    136 | CPU
DEBUG 01-15 16:09:01.032056.032056 lmp.py:1935]   Expert 18 |    138 | CPU
DEBUG 01-15 16:09:01.032183.032183 lmp.py:1935]   Expert 59 |    140 | CPU
DEBUG 01-15 16:09:01.032422.032422 lmp.py:1935]   Expert 40 |    144 | CPU
DEBUG 01-15 16:09:01.032741.032741 lmp.py:1935]   Expert 12 |    148 | CPU
DEBUG 01-15 16:09:01.032060.032060 lmp.py:1935]   Expert  4 |    150 | CPU
DEBUG 01-15 16:09:01.032617.032617 lmp.py:1935]   Expert 48 |    152 | CPU
DEBUG 01-15 16:09:01.032412.032412 lmp.py:1935]   Expert 56 |    152 | CPU
DEBUG 01-15 16:09:01.032493.032493 lmp.py:1935]   Expert 20 |    153 | CPU
DEBUG 01-15 16:09:01.032719.032719 lmp.py:1935]   Expert  2 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.032991.032991 lmp.py:1935]   Expert 46 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.032025.032025 lmp.py:1935]   Expert 33 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.032297.032297 lmp.py:1935]   Expert 61 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.032331.032331 lmp.py:1935]   Expert 35 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.032365.032365 lmp.py:1935]   Expert 10 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.032161.032161 lmp.py:1935]   Expert 55 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.032956.032956 lmp.py:1935]   Expert 51 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.032467.032467 lmp.py:1935]   Expert 36 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.032978.032978 lmp.py:1935]   Expert  8 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.032012.032012 lmp.py:1935]   Expert 52 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.032523.032523 lmp.py:1935]   Expert 37 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.032795.032795 lmp.py:1935]   Expert 57 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.032829.032829 lmp.py:1935]   Expert  0 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.032909.032909 lmp.py:1935]   Expert 39 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.032989.032989 lmp.py:1935]   Expert 25 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.032308.032308 lmp.py:1935]   Expert 62 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.032388.032388 lmp.py:1935]   Expert 38 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033184.033184 lmp.py:1935]   Expert  7 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033979.033979 lmp.py:1935]   Expert  3 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033490.033490 lmp.py:1935]   Expert 24 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033763.033763 lmp.py:1935]   Expert 27 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033797.033797 lmp.py:1935]   Expert 28 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033354.033354 lmp.py:1935]   Expert 49 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033911.033911 lmp.py:1935]   Expert 21 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033468.033468 lmp.py:1935]   Expert 60 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033025.033025 lmp.py:1935]   Expert 16 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033821.033821 lmp.py:1935]   Expert 43 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033616.033616 lmp.py:1935]   Expert 23 |    275 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033650.033650 lmp.py:1935]   Expert 29 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033684.033684 lmp.py:1935]   Expert 47 |    293 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033718.033718 lmp.py:1935]   Expert 15 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033467.033467 lmp.py:1935]   Expert 22 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033263.033263 lmp.py:1935]   Expert 41 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033820.033820 lmp.py:1935]   Expert 44 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033139.033139 lmp.py:1935]   Expert 54 |    350 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033696.033696 lmp.py:1935]   Expert 14 |    375 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033014.033014 lmp.py:1935]   Expert 17 |    406 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.033571.033571 lmp.py:1935]   Expert 45 |    450 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.033221.033221 lmp.py:1937] 
DEBUG 01-15 16:09:01.033221.033221 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.033017.033017 lmp.py:1938]   CPU:   2746 tokens
DEBUG 01-15 16:09:01.033051.033051 lmp.py:1942]   cuda:1:   4848 tokens (20 experts)
DEBUG 01-15 16:09:01.033846.033846 lmp.py:1942]   cuda:2:   4694 tokens (19 experts)
DEBUG 01-15 16:09:01.033688.033688 lmp.py:1943]   Total GPU:   9542 tokens
DEBUG 01-15 16:09:01.033768.033768 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.033768.033768 lmp.py:1944] 
DEBUG 01-15 16:09:01.033048.033048 cuda_h.py:19] end experts_map_get cost 0.002362966537475586 seconds
DEBUG 01-15 16:09:01.033348.033348 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.033164.033164 lmp.py:1953] 
DEBUG 01-15 16:09:01.033164.033164 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.033451.033451 cuda_h.py:19] end cpu_experts_submit cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:09:01.033254.033254 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.033316.033316 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.034965.034965 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.3.gate_proj.weight': 0, 'model.layers.13.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.15.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.15.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.15.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.23.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.23.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.23.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.25.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.25.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.25.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.36.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.36.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.36.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.55.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.55.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.55.up_proj.weight': 322961408, 'model.layers.13.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.13.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.13.mlp.experts.61.up_proj.weight': 340262912}, 2: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(16199974912, 5767168, 0, 0), (16205742080, 5767168, 5767168, 0), (16194207744, 5767168, 11534336, 0), (16269180928, 5767168, 17301504, 0), (16274948096, 5767168, 23068672, 0), (16263413760, 5767168, 28835840, 0), (16407592960, 5767168, 34603008, 0), (16413360128, 5767168, 40370176, 0), (16401825792, 5767168, 46137344, 0), (16424894464, 5767168, 51904512, 0), (16430661632, 5767168, 57671680, 0), (16419127296, 5767168, 63438848, 0), (16546004992, 5767168, 69206016, 0), (16551772160, 5767168, 74973184, 0), (16540237824, 5767168, 80740352, 0), (16580608000, 5767168, 86507520, 0), (16586375168, 5767168, 92274688, 0), (16574840832, 5767168, 98041856, 0), (16615211008, 5767168, 103809024, 0), (16620978176, 5767168, 109576192, 0), (16609443840, 5767168, 115343360, 0), (16649814016, 5767168, 121110528, 0), (16655581184, 5767168, 126877696, 0), (16644046848, 5767168, 132644864, 0), (16719020032, 5767168, 138412032, 0), (16724787200, 5767168, 144179200, 0), (16713252864, 5767168, 149946368, 0), (16770924544, 5767168, 155713536, 0), (16776691712, 5767168, 161480704, 0), (16765157376, 5767168, 167247872, 0), (16788226048, 5767168, 173015040, 0), (16793993216, 5767168, 178782208, 0), (16782458880, 5767168, 184549376, 0), (16822829056, 5767168, 190316544, 0), (16828596224, 5767168, 196083712, 0), (16817061888, 5767168, 201850880, 0), (16857432064, 5767168, 207618048, 0), (16863199232, 5767168, 213385216, 0), (16851664896, 5767168, 219152384, 0), (16926638080, 5767168, 224919552, 0), (16932405248, 5767168, 230686720, 0), (16920870912, 5767168, 236453888, 0), (16943939584, 5767168, 242221056, 0), (16949706752, 5767168, 247988224, 0), (16938172416, 5767168, 253755392, 0), (16995844096, 5767168, 259522560, 0), (17001611264, 5767168, 265289728, 0), (16990076928, 5767168, 271056896, 0), (17047748608, 5767168, 276824064, 0), (17053515776, 5767168, 282591232, 0), (17041981440, 5767168, 288358400, 0), (17082351616, 5767168, 294125568, 0), (17088118784, 5767168, 299892736, 0), (17076584448, 5767168, 305659904, 0), (17099653120, 5767168, 311427072, 0), (17105420288, 5767168, 317194240, 0), (17093885952, 5767168, 322961408, 0), (17203462144, 5767168, 328728576, 0), (17209229312, 5767168, 334495744, 0), (17197694976, 5767168, 340262912, 0)], 2: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16182673408, 5767168, 17301504, 0), (16188440576, 5767168, 23068672, 0), (16176906240, 5767168, 28835840, 0), (16286482432, 5767168, 34603008, 0), (16292249600, 5767168, 40370176, 0), (16280715264, 5767168, 46137344, 0), (16321085440, 5767168, 51904512, 0), (16326852608, 5767168, 57671680, 0), (16315318272, 5767168, 63438848, 0), (16390291456, 5767168, 69206016, 0), (16396058624, 5767168, 74973184, 0), (16384524288, 5767168, 80740352, 0), (16442195968, 5767168, 86507520, 0), (16447963136, 5767168, 92274688, 0), (16436428800, 5767168, 98041856, 0), (16511401984, 5767168, 103809024, 0), (16517169152, 5767168, 109576192, 0), (16505634816, 5767168, 115343360, 0), (16528703488, 5767168, 121110528, 0), (16534470656, 5767168, 126877696, 0), (16522936320, 5767168, 132644864, 0), (16563306496, 5767168, 138412032, 0), (16569073664, 5767168, 144179200, 0), (16557539328, 5767168, 149946368, 0), (16632512512, 5767168, 155713536, 0), (16638279680, 5767168, 161480704, 0), (16626745344, 5767168, 167247872, 0), (16753623040, 5767168, 173015040, 0), (16759390208, 5767168, 178782208, 0), (16747855872, 5767168, 184549376, 0), (16805527552, 5767168, 190316544, 0), (16811294720, 5767168, 196083712, 0), (16799760384, 5767168, 201850880, 0), (16892035072, 5767168, 207618048, 0), (16897802240, 5767168, 213385216, 0), (16886267904, 5767168, 219152384, 0), (16909336576, 5767168, 224919552, 0), (16915103744, 5767168, 230686720, 0), (16903569408, 5767168, 236453888, 0), (16961241088, 5767168, 242221056, 0), (16967008256, 5767168, 247988224, 0), (16955473920, 5767168, 253755392, 0), (17030447104, 5767168, 259522560, 0), (17036214272, 5767168, 265289728, 0), (17024679936, 5767168, 271056896, 0), (17134256128, 5767168, 276824064, 0), (17140023296, 5767168, 282591232, 0), (17128488960, 5767168, 288358400, 0), (17186160640, 5767168, 294125568, 0), (17191927808, 5767168, 299892736, 0), (17180393472, 5767168, 305659904, 0), (17220763648, 5767168, 311427072, 0), (17226530816, 5767168, 317194240, 0), (17214996480, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a81469dec0>, 2: <capsule object NULL at 0x74a660701ec0>}
DEBUG 01-15 16:09:01.035144.035144 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.035737.035737 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.035891.035891 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b642b4b9-ca42-461a-94ef-7cf769b69e05
DEBUG 01-15 16:09:01.035000.035000 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.035225.035225 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:01.035347.035347 client.py:127] Model loaded
DEBUG 01-15 16:09:01.036683.036683 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.036882.036882 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.037789.037789 cuda_h.py:19] end restore2model cost 0.0011072158813476562 seconds
DEBUG 01-15 16:09:01.037357.037357 cuda_h.py:19] end move_flatidxs cost 0.0010018348693847656 seconds
DEBUG 01-15 16:09:01.037182.037182 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:01.037398.037398 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b642b4b9-ca42-461a-94ef-7cf769b69e05
DEBUG 01-15 16:09:01.037621.037621 cuda_h.py:19] end sllm_worker_task cost 0.015229940414428711 seconds
DEBUG 01-15 16:09:01.038032.038032 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004127979278564453 seconds
DEBUG 01-15 16:09:01.038324.038324 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.041493.041493 cuda_h.py:19] end restore2model cost 0.0030355453491210938 seconds
DEBUG 01-15 16:09:01.041575.041575 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007579326629638672 seconds
DEBUG 01-15 16:09:01.041370.041370 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.041268.041268 cuda_h.py:19] end gpu_sexperts cost 0.00027942657470703125 seconds
DEBUG 01-15 16:09:01.041098.041098 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.041642.041642 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.193450927734375e-05 seconds
DEBUG 01-15 16:09:01.041815.041815 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.041949.041949 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.042267.042267 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009768009185791016 seconds
DEBUG 01-15 16:09:01.043541.043541 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.043568.043568 cuda_h.py:19] end gpu_group_list cost 0.00020241737365722656 seconds
DEBUG 01-15 16:09:01.044451.044451 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.045302.045302 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010242462158203125 seconds
DEBUG 01-15 16:09:01.045722.045722 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.045458.045458 cuda_h.py:19] end gpu_group_list cost 0.00019693374633789062 seconds
DEBUG 01-15 16:09:01.046659.046659 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.046396.046396 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b642b4b9-ca42-461a-94ef-7cf769b69e05
DEBUG 01-15 16:09:01.051356.051356 cuda_h.py:19] end group_tensors cost 0.014020919799804688 seconds
DEBUG 01-15 16:09:01.052059.052059 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.055239.055239 cuda_h.py:19] end group pad cost 0.0031595230102539062 seconds
DEBUG 01-15 16:09:01.055765.055765 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:01.076766.076766 client.py:127] Model loaded
DEBUG 01-15 16:09:01.076165.076165 cuda_h.py:19] end wait_experts_multi_device cost 0.03019404411315918 seconds
DEBUG 01-15 16:09:01.076222.076222 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.082706.082706 cuda_h.py:19] end group_einsum cost 0.02642822265625 seconds
DEBUG 01-15 16:09:01.082871.082871 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.085928.085928 cuda_h.py:19] end get_outputs_cpu1 cost 0.0026526451110839844 seconds
DEBUG 01-15 16:09:01.085751.085751 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05002284049987793 seconds
DEBUG 01-15 16:09:01.086397.086397 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010058403015136719 seconds
DEBUG 01-15 16:09:01.087444.087444 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.089877.089877 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.089497.089497 cuda_h.py:19] end index_scatter cost 0.00015163421630859375 seconds
DEBUG 01-15 16:09:01.090356.090356 cuda_h.py:19] end cpuoutputsdeal cost 0.0029931068420410156 seconds
DEBUG 01-15 16:09:01.090063.090063 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.090060.090060 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.090782.090782 cuda_h.py:19] end gpu_group_tensor cost 0.00030684471130371094 seconds
DEBUG 01-15 16:09:01.090765.090765 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.091691.091691 cuda_h.py:19] end gpu_group_tensor cost 0.00028634071350097656 seconds
DEBUG 01-15 16:09:01.091388.091388 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.092412.092412 cuda_h.py:19] end gpu_group_einsum cost 0.00119781494140625 seconds
DEBUG 01-15 16:09:01.093620.093620 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.093383.093383 cuda_h.py:19] end gpu_group_einsum cost 0.0007343292236328125 seconds
DEBUG 01-15 16:09:01.094130.094130 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.094102.094102 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.094140.094140 cuda_h.py:19] end all_expert_outputs_slices cost 0.00028228759765625 seconds
DEBUG 01-15 16:09:01.094400.094400 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.094385.094385 cuda_h.py:19] end concat_expert_out cost 7.557868957519531e-05 seconds
DEBUG 01-15 16:09:01.094606.094606 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.095366.095366 cuda_h.py:19] end index_scatter cost 8.20159912109375e-05 seconds
DEBUG 01-15 16:09:01.095166.095166 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011827945709228516 seconds
DEBUG 01-15 16:09:01.095918.095918 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.095279.095279 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.095414.095414 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022673606872558594 seconds
DEBUG 01-15 16:09:01.095429.095429 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.096268.096268 cuda_h.py:19] end concat_expert_out cost 7.891654968261719e-05 seconds
DEBUG 01-15 16:09:01.096198.096198 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.096659.096659 cuda_h.py:19] end index_scatter cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:09:01.096449.096449 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007548332214355469 seconds
DEBUG 01-15 16:09:01.096149.096149 cuda_h.py:19] end gpu_experts_multi_device cost 0.05443310737609863 seconds
DEBUG 01-15 16:09:01.096696.096696 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.06612443923950195 seconds
DEBUG 01-15 16:09:01.097265.097265 cuda_h.py:19] end prefill_layer cost 0.07588338851928711 seconds
DEBUG 01-15 16:09:01.097216.097216 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:09:01.097800.097800 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.097861.097861 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:09:01.097777.097777 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:09:01.097368.097368 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:09:01.097219.097219 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 5.6743621826171875e-05 seconds
DEBUG 01-15 16:09:01.097856.097856 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 0.00011014938354492188 seconds
DEBUG 01-15 16:09:01.097619.097619 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.097981.097981 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.097755.097755 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.098991.098991 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.098792.098792 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.099054.099054 cuda_h.py:19] end allocate_cuda_memory cost 0.0005655288696289062 seconds
DEBUG 01-15 16:09:01.099092.099092 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.099175.099175 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.099961.099961 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.099500.099500 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 51b44d01-e6a1-406e-826e-957d94d0d4a6
DEBUG 01-15 16:09:01.099289.099289 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.100736.100736 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.101746.101746 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 51b44d01-e6a1-406e-826e-957d94d0d4a6
DEBUG 01-15 16:09:01.101541.101541 cuda_h.py:19] end load_into_gpu_async cost 0.002406597137451172 seconds
DEBUG 01-15 16:09:01.101749.101749 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.102996.102996 cuda_h.py:19] end restore_tensors2 cost 0.00017404556274414062 seconds
DEBUG 01-15 16:09:01.102656.102656 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0038907527923583984 seconds
INFO 01-15 16:09:01.102144.102144 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 51b44d01-e6a1-406e-826e-957d94d0d4a6
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.105406.105406 cuda_h.py:19] end self_attn cost 0.0048046112060546875 seconds
DEBUG 01-15 16:09:01.105159.105159 cuda_h.py:19] end iln_self_attn_paln cost 0.007966756820678711 seconds
DEBUG 01-15 16:09:01.105002.105002 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:09:01.105718.105718 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.106365.106365 cuda_h.py:19] end gate cost 0.0006854534149169922 seconds
DEBUG 01-15 16:09:01.106539.106539 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.106512.106512 lmp.py:1912] 
DEBUG 01-15 16:09:01.106512.106512 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.106890.106890 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.106547.106547 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.106528.106528 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.106078.106078 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.106152.106152 lmp.py:1917] 
DEBUG 01-15 16:09:01.106152.106152 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.107225.107225 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.107789.107789 lmp.py:1935]   Expert  7 |     30 | CPU
DEBUG 01-15 16:09:01.107909.107909 lmp.py:1935]   Expert 34 |     31 | CPU
DEBUG 01-15 16:09:01.107313.107313 lmp.py:1935]   Expert 13 |     42 | CPU
DEBUG 01-15 16:09:01.107003.107003 lmp.py:1935]   Expert 54 |     75 | CPU
DEBUG 01-15 16:09:01.107407.107407 lmp.py:1935]   Expert 18 |     85 | CPU
DEBUG 01-15 16:09:01.107765.107765 lmp.py:1935]   Expert 49 |     86 | CPU
DEBUG 01-15 16:09:01.107932.107932 lmp.py:1935]   Expert 39 |     88 | CPU
DEBUG 01-15 16:09:01.107528.107528 lmp.py:1935]   Expert 59 |    101 | CPU
DEBUG 01-15 16:09:01.107271.107271 lmp.py:1935]   Expert 21 |    106 | CPU
DEBUG 01-15 16:09:01.107437.107437 lmp.py:1935]   Expert  0 |    108 | CPU
DEBUG 01-15 16:09:01.107888.107888 lmp.py:1935]   Expert 16 |    108 | CPU
DEBUG 01-15 16:09:01.107577.107577 lmp.py:1935]   Expert 41 |    117 | CPU
DEBUG 01-15 16:09:01.107028.107028 lmp.py:1935]   Expert 15 |    121 | CPU
DEBUG 01-15 16:09:01.107956.107956 lmp.py:1935]   Expert 45 |    121 | CPU
DEBUG 01-15 16:09:01.107883.107883 lmp.py:1935]   Expert 22 |    122 | CPU
DEBUG 01-15 16:09:01.107334.107334 lmp.py:1935]   Expert 17 |    127 | CPU
DEBUG 01-15 16:09:01.107547.107547 lmp.py:1935]   Expert  8 |    135 | CPU
DEBUG 01-15 16:09:01.107997.107997 lmp.py:1935]   Expert 52 |    135 | CPU
DEBUG 01-15 16:09:01.107448.107448 lmp.py:1935]   Expert 61 |    136 | CPU
DEBUG 01-15 16:09:01.107330.107330 lmp.py:1935]   Expert 35 |    139 | CPU
DEBUG 01-15 16:09:01.107211.107211 lmp.py:1935]   Expert 12 |    142 | CPU
DEBUG 01-15 16:09:01.107192.107192 lmp.py:1935]   Expert 38 |    143 | CPU
DEBUG 01-15 16:09:01.107789.107789 lmp.py:1935]   Expert 48 |    148 | CPU
DEBUG 01-15 16:09:01.107716.107716 lmp.py:1935]   Expert 31 |    149 | CPU
DEBUG 01-15 16:09:01.107644.107644 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:09:01.107718.107718 lmp.py:1935]   Expert 53 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107268.107268 lmp.py:1935]   Expert 50 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107388.107388 lmp.py:1935]   Expert 40 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107746.107746 lmp.py:1935]   Expert 60 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107389.107389 lmp.py:1935]   Expert 27 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107032.107032 lmp.py:1935]   Expert 19 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107913.107913 lmp.py:1935]   Expert  4 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107748.107748 lmp.py:1935]   Expert 29 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107060.107060 lmp.py:1935]   Expert 30 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107895.107895 lmp.py:1935]   Expert 11 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107969.107969 lmp.py:1935]   Expert 20 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107373.107373 lmp.py:1935]   Expert 26 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107778.107778 lmp.py:1935]   Expert 57 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107421.107421 lmp.py:1935]   Expert  6 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107587.107587 lmp.py:1935]   Expert 43 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107753.107753 lmp.py:1935]   Expert 46 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107919.107919 lmp.py:1935]   Expert  2 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107324.107324 lmp.py:1935]   Expert 23 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107490.107490 lmp.py:1935]   Expert 33 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107656.107656 lmp.py:1935]   Expert 42 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107690.107690 lmp.py:1935]   Expert 55 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107810.107810 lmp.py:1935]   Expert 56 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107645.107645 lmp.py:1935]   Expert 32 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107626.107626 lmp.py:1935]   Expert  3 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107938.107938 lmp.py:1935]   Expert  9 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107819.107819 lmp.py:1935]   Expert 14 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107224.107224 lmp.py:1935]   Expert 28 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.107628.107628 lmp.py:1935]   Expert  1 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.107033.107033 lmp.py:1935]   Expert 51 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.108437.108437 lmp.py:1935]   Expert 44 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.108842.108842 lmp.py:1935]   Expert 58 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.108246.108246 lmp.py:1935]   Expert 63 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.108128.108128 lmp.py:1935]   Expert 37 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.108532.108532 lmp.py:1935]   Expert 47 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.108414.108414 lmp.py:1935]   Expert 24 |    309 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.108348.108348 lmp.py:1935]   Expert 62 |    312 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.108475.108475 lmp.py:1935]   Expert 10 |    313 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.108409.108409 lmp.py:1935]   Expert 25 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.108298.108298 lmp.py:1935]   Expert  5 |    365 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.108609.108609 lmp.py:1937] 
DEBUG 01-15 16:09:01.108609.108609 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.108352.108352 lmp.py:1938]   CPU:   2750 tokens
DEBUG 01-15 16:09:01.108048.108048 lmp.py:1942]   cuda:1:   4842 tokens (20 experts)
DEBUG 01-15 16:09:01.108506.108506 lmp.py:1942]   cuda:2:   4696 tokens (19 experts)
DEBUG 01-15 16:09:01.108771.108771 lmp.py:1943]   Total GPU:   9538 tokens
DEBUG 01-15 16:09:01.108275.108275 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.108275.108275 lmp.py:1944] 
DEBUG 01-15 16:09:01.108217.108217 cuda_h.py:19] end experts_map_get cost 0.0018038749694824219 seconds
DEBUG 01-15 16:09:01.108378.108378 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.108711.108711 lmp.py:1953] 
DEBUG 01-15 16:09:01.108711.108711 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.108759.108759 cuda_h.py:19] end cpu_experts_submit cost 6.985664367675781e-05 seconds
DEBUG 01-15 16:09:01.108694.108694 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.108775.108775 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.109936.109936 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.3.gate_proj.weight': 0, 'model.layers.14.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.40.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.40.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.40.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.51.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.51.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.51.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.14.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.14.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.14.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.14.mlp.experts.1.gate_proj.weight': 0, 'model.layers.14.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.57.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(17307271168, 5767168, 0, 0), (17313038336, 5767168, 5767168, 0), (17301504000, 5767168, 11534336, 0), (17341874176, 5767168, 17301504, 0), (17347641344, 5767168, 23068672, 0), (17336107008, 5767168, 28835840, 0), (17359175680, 5767168, 34603008, 0), (17364942848, 5767168, 40370176, 0), (17353408512, 5767168, 46137344, 0), (17445683200, 5767168, 51904512, 0), (17451450368, 5767168, 57671680, 0), (17439916032, 5767168, 63438848, 0), (17584095232, 5767168, 69206016, 0), (17589862400, 5767168, 74973184, 0), (17578328064, 5767168, 80740352, 0), (17653301248, 5767168, 86507520, 0), (17659068416, 5767168, 92274688, 0), (17647534080, 5767168, 98041856, 0), (17705205760, 5767168, 103809024, 0), (17710972928, 5767168, 109576192, 0), (17699438592, 5767168, 115343360, 0), (17739808768, 5767168, 121110528, 0), (17745575936, 5767168, 126877696, 0), (17734041600, 5767168, 132644864, 0), (17774411776, 5767168, 138412032, 0), (17780178944, 5767168, 144179200, 0), (17768644608, 5767168, 149946368, 0), (17947426816, 5767168, 155713536, 0), (17953193984, 5767168, 161480704, 0), (17941659648, 5767168, 167247872, 0), (17982029824, 5767168, 173015040, 0), (17987796992, 5767168, 178782208, 0), (17976262656, 5767168, 184549376, 0), (17999331328, 5767168, 190316544, 0), (18005098496, 5767168, 196083712, 0), (17993564160, 5767168, 201850880, 0), (18068537344, 5767168, 207618048, 0), (18074304512, 5767168, 213385216, 0), (18062770176, 5767168, 219152384, 0), (18137743360, 5767168, 224919552, 0), (18143510528, 5767168, 230686720, 0), (18131976192, 5767168, 236453888, 0), (18172346368, 5767168, 242221056, 0), (18178113536, 5767168, 247988224, 0), (18166579200, 5767168, 253755392, 0), (18224250880, 5767168, 259522560, 0), (18230018048, 5767168, 265289728, 0), (18218483712, 5767168, 271056896, 0), (18258853888, 5767168, 276824064, 0), (18264621056, 5767168, 282591232, 0), (18253086720, 5767168, 288358400, 0), (18293456896, 5767168, 294125568, 0), (18299224064, 5767168, 299892736, 0), (18287689728, 5767168, 305659904, 0), (18328059904, 5767168, 311427072, 0), (18333827072, 5767168, 317194240, 0), (18322292736, 5767168, 322961408, 0), (18345361408, 5767168, 328728576, 0), (18351128576, 5767168, 334495744, 0), (18339594240, 5767168, 340262912, 0)], 2: [(17272668160, 5767168, 0, 0), (17278435328, 5767168, 5767168, 0), (17266900992, 5767168, 11534336, 0), (17289969664, 5767168, 17301504, 0), (17295736832, 5767168, 23068672, 0), (17284202496, 5767168, 28835840, 0), (17324572672, 5767168, 34603008, 0), (17330339840, 5767168, 40370176, 0), (17318805504, 5767168, 46137344, 0), (17411080192, 5767168, 51904512, 0), (17416847360, 5767168, 57671680, 0), (17405313024, 5767168, 63438848, 0), (17428381696, 5767168, 69206016, 0), (17434148864, 5767168, 74973184, 0), (17422614528, 5767168, 80740352, 0), (17497587712, 5767168, 86507520, 0), (17503354880, 5767168, 92274688, 0), (17491820544, 5767168, 98041856, 0), (17601396736, 5767168, 103809024, 0), (17607163904, 5767168, 109576192, 0), (17595629568, 5767168, 115343360, 0), (17670602752, 5767168, 121110528, 0), (17676369920, 5767168, 126877696, 0), (17664835584, 5767168, 132644864, 0), (17687904256, 5767168, 138412032, 0), (17693671424, 5767168, 144179200, 0), (17682137088, 5767168, 149946368, 0), (17722507264, 5767168, 155713536, 0), (17728274432, 5767168, 161480704, 0), (17716740096, 5767168, 167247872, 0), (17757110272, 5767168, 173015040, 0), (17762877440, 5767168, 178782208, 0), (17751343104, 5767168, 184549376, 0), (17809014784, 5767168, 190316544, 0), (17814781952, 5767168, 196083712, 0), (17803247616, 5767168, 201850880, 0), (17826316288, 5767168, 207618048, 0), (17832083456, 5767168, 213385216, 0), (17820549120, 5767168, 219152384, 0), (17895522304, 5767168, 224919552, 0), (17901289472, 5767168, 230686720, 0), (17889755136, 5767168, 236453888, 0), (18016632832, 5767168, 242221056, 0), (18022400000, 5767168, 247988224, 0), (18010865664, 5767168, 253755392, 0), (18051235840, 5767168, 259522560, 0), (18057003008, 5767168, 265289728, 0), (18045468672, 5767168, 271056896, 0), (18120441856, 5767168, 276824064, 0), (18126209024, 5767168, 282591232, 0), (18114674688, 5767168, 288358400, 0), (18206949376, 5767168, 294125568, 0), (18212716544, 5767168, 299892736, 0), (18201182208, 5767168, 305659904, 0), (18241552384, 5767168, 311427072, 0), (18247319552, 5767168, 317194240, 0), (18235785216, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc4fe220>, 2: <capsule object NULL at 0x74a6bc4fdf80>}
DEBUG 01-15 16:09:01.109309.109309 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.110617.110617 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.110479.110479 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 307aa8bd-7d3f-4455-bd7f-34c4d351cc11
DEBUG 01-15 16:09:01.110542.110542 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.110604.110604 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:01.110520.110520 client.py:127] Model loaded
DEBUG 01-15 16:09:01.110578.110578 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.110570.110570 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.111683.111683 cuda_h.py:19] end move_flatidxs cost 0.0009207725524902344 seconds
DEBUG 01-15 16:09:01.112764.112764 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.112599.112599 cuda_h.py:19] end restore2model cost 0.0011467933654785156 seconds
INFO 01-15 16:09:01.112990.112990 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 307aa8bd-7d3f-4455-bd7f-34c4d351cc11
DEBUG 01-15 16:09:01.112763.112763 cuda_h.py:19] end sllm_worker_task cost 0.014168262481689453 seconds
DEBUG 01-15 16:09:01.113354.113354 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004379749298095703 seconds
DEBUG 01-15 16:09:01.113224.113224 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.116608.116608 cuda_h.py:19] end restore2model cost 0.0033218860626220703 seconds
DEBUG 01-15 16:09:01.116862.116862 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008177757263183594 seconds
DEBUG 01-15 16:09:01.116850.116850 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.117238.117238 cuda_h.py:19] end gpu_sexperts cost 0.0002865791320800781 seconds
DEBUG 01-15 16:09:01.117267.117267 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.117580.117580 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.6464462280273438e-05 seconds
DEBUG 01-15 16:09:01.117706.117706 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.117555.117555 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.117291.117291 cuda_h.py:19] end group_tensors cost 0.005186319351196289 seconds
DEBUG 01-15 16:09:01.117651.117651 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.118763.118763 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010266304016113281 seconds
DEBUG 01-15 16:09:01.118733.118733 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.118245.118245 cuda_h.py:19] end gpu_group_list cost 0.0002422332763671875 seconds
DEBUG 01-15 16:09:01.119528.119528 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.120761.120761 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011615753173828125 seconds
DEBUG 01-15 16:09:01.120222.120222 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.121660.121660 cuda_h.py:19] end gpu_group_list cost 0.00022339820861816406 seconds
DEBUG 01-15 16:09:01.121752.121752 cuda_h.py:19] end group pad cost 0.0033540725708007812 seconds
DEBUG 01-15 16:09:01.121655.121655 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:01.122137.122137 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.122893.122893 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 307aa8bd-7d3f-4455-bd7f-34c4d351cc11
DEBUG 01-15 16:09:01.147015.147015 cuda_h.py:19] end group_einsum cost 0.026256322860717773 seconds
DEBUG 01-15 16:09:01.147769.147769 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.151752.151752 cuda_h.py:19] end get_outputs_cpu1 cost 0.003171682357788086 seconds
DEBUG 01-15 16:09:01.152836.152836 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04143667221069336 seconds
INFO 01-15 16:09:01.153655.153655 client.py:127] Model loaded
DEBUG 01-15 16:09:01.153264.153264 cuda_h.py:19] end wait_experts_multi_device cost 0.030686140060424805 seconds
DEBUG 01-15 16:09:01.153663.153663 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.154626.154626 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0006256103515625 seconds
DEBUG 01-15 16:09:01.154059.154059 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.155639.155639 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.155440.155440 cuda_h.py:19] end index_scatter cost 0.00010585784912109375 seconds
DEBUG 01-15 16:09:01.156552.156552 cuda_h.py:19] end cpuoutputsdeal cost 0.001825571060180664 seconds
DEBUG 01-15 16:09:01.156609.156609 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.156776.156776 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.156442.156442 cuda_h.py:19] end gpu_group_tensor cost 0.00026488304138183594 seconds
DEBUG 01-15 16:09:01.156093.156093 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.156081.156081 cuda_h.py:19] end gpu_group_tensor cost 0.00018906593322753906 seconds
DEBUG 01-15 16:09:01.157907.157907 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.157075.157075 cuda_h.py:19] end gpu_group_einsum cost 0.0007898807525634766 seconds
DEBUG 01-15 16:09:01.158445.158445 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.159886.159886 cuda_h.py:19] end gpu_group_einsum cost 0.0009937286376953125 seconds
DEBUG 01-15 16:09:01.159073.159073 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.159865.159865 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.159311.159311 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002162456512451172 seconds
DEBUG 01-15 16:09:01.159928.159928 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.159587.159587 cuda_h.py:19] end concat_expert_out cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:09:01.160013.160013 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.160732.160732 cuda_h.py:19] end index_scatter cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:09:01.160927.160927 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009036064147949219 seconds
DEBUG 01-15 16:09:01.160646.160646 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.160496.160496 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.160992.160992 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015807151794433594 seconds
DEBUG 01-15 16:09:01.160841.160841 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.160910.160910 cuda_h.py:19] end concat_expert_out cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:09:01.160468.160468 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.161637.161637 cuda_h.py:19] end index_scatter cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:09:01.161207.161207 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005164146423339844 seconds
DEBUG 01-15 16:09:01.161045.161045 cuda_h.py:19] end gpu_experts_multi_device cost 0.043906450271606445 seconds
DEBUG 01-15 16:09:01.161485.161485 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.05552053451538086 seconds
DEBUG 01-15 16:09:01.161508.161508 cuda_h.py:19] end prefill_layer cost 0.06441617012023926 seconds
DEBUG 01-15 16:09:01.161689.161689 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:09:01.161346.161346 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.161002.161002 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:09:01.161089.161089 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:09:01.161322.161322 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:09:01.161033.161033 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 3.981590270996094e-05 seconds
DEBUG 01-15 16:09:01.162394.162394 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.162006.162006 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 0.0002205371856689453 seconds
DEBUG 01-15 16:09:01.162117.162117 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.162829.162829 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.162893.162893 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.162355.162355 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.163658.163658 cuda_h.py:19] end allocate_cuda_memory cost 0.00042319297790527344 seconds
DEBUG 01-15 16:09:01.163653.163653 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.163457.163457 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.163896.163896 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.163481.163481 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9ef8119d-420c-49cc-9154-0cf57f78dc48
DEBUG 01-15 16:09:01.163400.163400 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.164519.164519 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.165222.165222 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9ef8119d-420c-49cc-9154-0cf57f78dc48
DEBUG 01-15 16:09:01.165505.165505 cuda_h.py:19] end load_into_gpu_async cost 0.0024199485778808594 seconds
DEBUG 01-15 16:09:01.166130.166130 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.166891.166891 cuda_h.py:19] end restore_tensors2 cost 0.00012731552124023438 seconds
DEBUG 01-15 16:09:01.166541.166541 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003782510757446289 seconds
INFO 01-15 16:09:01.166015.166015 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9ef8119d-420c-49cc-9154-0cf57f78dc48
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.167856.167856 cuda_h.py:19] end self_attn cost 0.003614187240600586 seconds
DEBUG 01-15 16:09:01.168827.168827 cuda_h.py:19] end iln_self_attn_paln cost 0.005654811859130859 seconds
DEBUG 01-15 16:09:01.168756.168756 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:09:01.168433.168433 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.169330.169330 cuda_h.py:19] end gate cost 0.0006444454193115234 seconds
DEBUG 01-15 16:09:01.169352.169352 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.169575.169575 lmp.py:1912] 
DEBUG 01-15 16:09:01.169575.169575 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.169000.169000 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.169842.169842 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.169631.169631 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.169751.169751 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.169884.169884 lmp.py:1917] 
DEBUG 01-15 16:09:01.169884.169884 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.169765.169765 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.169654.169654 lmp.py:1935]   Expert 15 |     65 | CPU
DEBUG 01-15 16:09:01.169820.169820 lmp.py:1935]   Expert 41 |     69 | CPU
DEBUG 01-15 16:09:01.169509.169509 lmp.py:1935]   Expert  0 |     74 | CPU
DEBUG 01-15 16:09:01.169483.169483 lmp.py:1935]   Expert 63 |     77 | CPU
DEBUG 01-15 16:09:01.169457.169457 lmp.py:1935]   Expert 20 |     83 | CPU
DEBUG 01-15 16:09:01.169670.169670 lmp.py:1935]   Expert  7 |     92 | CPU
DEBUG 01-15 16:09:01.169359.169359 lmp.py:1935]   Expert 45 |     96 | CPU
DEBUG 01-15 16:09:01.169525.169525 lmp.py:1935]   Expert 28 |     99 | CPU
DEBUG 01-15 16:09:01.169930.169930 lmp.py:1935]   Expert 54 |    104 | CPU
DEBUG 01-15 16:09:01.169619.169619 lmp.py:1935]   Expert 12 |    109 | CPU
DEBUG 01-15 16:09:01.169308.169308 lmp.py:1935]   Expert 52 |    120 | CPU
DEBUG 01-15 16:09:01.169051.169051 lmp.py:1935]   Expert  5 |    121 | CPU
DEBUG 01-15 16:09:01.169124.169124 lmp.py:1935]   Expert 40 |    122 | CPU
DEBUG 01-15 16:09:01.169244.169244 lmp.py:1935]   Expert 59 |    123 | CPU
DEBUG 01-15 16:09:01.169364.169364 lmp.py:1935]   Expert  4 |    129 | CPU
DEBUG 01-15 16:09:01.169484.169484 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:09:01.169842.169842 lmp.py:1935]   Expert 62 |    134 | CPU
DEBUG 01-15 16:09:01.169962.169962 lmp.py:1935]   Expert 55 |    136 | CPU
DEBUG 01-15 16:09:01.169320.169320 lmp.py:1935]   Expert 13 |    137 | CPU
DEBUG 01-15 16:09:01.169632.169632 lmp.py:1935]   Expert 61 |    137 | CPU
DEBUG 01-15 16:09:01.170182.170182 lmp.py:1935]   Expert 21 |    138 | CPU
DEBUG 01-15 16:09:01.170256.170256 lmp.py:1935]   Expert 42 |    141 | CPU
DEBUG 01-15 16:09:01.170091.170091 lmp.py:1935]   Expert 10 |    146 | CPU
DEBUG 01-15 16:09:01.170164.170164 lmp.py:1935]   Expert 14 |    148 | CPU
DEBUG 01-15 16:09:01.170999.170999 lmp.py:1935]   Expert 22 |    149 | CPU
DEBUG 01-15 16:09:01.170026.170026 lmp.py:1935]   Expert 51 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170577.170577 lmp.py:1935]   Expert 32 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170650.170650 lmp.py:1935]   Expert 25 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170962.170962 lmp.py:1935]   Expert 47 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170274.170274 lmp.py:1935]   Expert  1 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170586.170586 lmp.py:1935]   Expert 26 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170421.170421 lmp.py:1935]   Expert 50 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170210.170210 lmp.py:1935]   Expert 53 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170760.170760 lmp.py:1935]   Expert 19 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170026.170026 lmp.py:1935]   Expert  6 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170814.170814 lmp.py:1935]   Expert  2 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170603.170603 lmp.py:1935]   Expert 35 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170677.170677 lmp.py:1935]   Expert 30 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170750.170750 lmp.py:1935]   Expert 11 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170108.170108 lmp.py:1935]   Expert 56 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170705.170705 lmp.py:1935]   Expert 57 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170540.170540 lmp.py:1935]   Expert 48 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170137.170137 lmp.py:1935]   Expert 24 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170733.170733 lmp.py:1935]   Expert 44 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170284.170284 lmp.py:1935]   Expert 16 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170834.170834 lmp.py:1935]   Expert 46 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170861.170861 lmp.py:1935]   Expert 39 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170411.170411 lmp.py:1935]   Expert 18 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170439.170439 lmp.py:1935]   Expert 29 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170274.170274 lmp.py:1935]   Expert 37 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170632.170632 lmp.py:1935]   Expert 31 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170990.170990 lmp.py:1935]   Expert 60 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170825.170825 lmp.py:1935]   Expert 36 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170422.170422 lmp.py:1935]   Expert  3 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170257.170257 lmp.py:1935]   Expert  9 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170330.170330 lmp.py:1935]   Expert 17 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170642.170642 lmp.py:1935]   Expert 38 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170431.170431 lmp.py:1935]   Expert 23 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170981.170981 lmp.py:1935]   Expert 27 |    349 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170578.170578 lmp.py:1935]   Expert 43 |    364 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170936.170936 lmp.py:1935]   Expert  8 |    395 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170771.170771 lmp.py:1935]   Expert 33 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170129.170129 lmp.py:1935]   Expert 58 |    445 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.170488.170488 lmp.py:1935]   Expert 49 |    540 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.170369.170369 lmp.py:1937] 
DEBUG 01-15 16:09:01.170369.170369 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.170250.170250 lmp.py:1938]   CPU:   2882 tokens
DEBUG 01-15 16:09:01.170324.170324 lmp.py:1942]   cuda:1:   4634 tokens (19 experts)
DEBUG 01-15 16:09:01.170636.170636 lmp.py:1942]   cuda:2:   4772 tokens (20 experts)
DEBUG 01-15 16:09:01.170471.170471 lmp.py:1943]   Total GPU:   9406 tokens
DEBUG 01-15 16:09:01.170068.170068 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.170068.170068 lmp.py:1944] 
DEBUG 01-15 16:09:01.170386.170386 cuda_h.py:19] end experts_map_get cost 0.001786947250366211 seconds
DEBUG 01-15 16:09:01.171574.171574 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.171184.171184 lmp.py:1953] 
DEBUG 01-15 16:09:01.171184.171184 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.171643.171643 cuda_h.py:19] end cpu_experts_submit cost 5.698204040527344e-05 seconds
DEBUG 01-15 16:09:01.171386.171386 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.171560.171560 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.172170.172170 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.6.gate_proj.weight': 0, 'model.layers.15.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.38.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.38.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.38.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.39.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.39.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.39.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.50.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.50.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.50.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.56.up_proj.weight': 322961408}, 2: {'model.layers.15.mlp.experts.1.gate_proj.weight': 0, 'model.layers.15.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.15.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.15.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.15.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(18466471936, 5767168, 0, 0), (18472239104, 5767168, 5767168, 0), (18460704768, 5767168, 11534336, 0), (18501074944, 5767168, 17301504, 0), (18506842112, 5767168, 23068672, 0), (18495307776, 5767168, 28835840, 0), (18552979456, 5767168, 34603008, 0), (18558746624, 5767168, 40370176, 0), (18547212288, 5767168, 46137344, 0), (18639486976, 5767168, 51904512, 0), (18645254144, 5767168, 57671680, 0), (18633719808, 5767168, 63438848, 0), (18656788480, 5767168, 69206016, 0), (18662555648, 5767168, 74973184, 0), (18651021312, 5767168, 80740352, 0), (18777899008, 5767168, 86507520, 0), (18783666176, 5767168, 92274688, 0), (18772131840, 5767168, 98041856, 0), (18812502016, 5767168, 103809024, 0), (18818269184, 5767168, 109576192, 0), (18806734848, 5767168, 115343360, 0), (18829803520, 5767168, 121110528, 0), (18835570688, 5767168, 126877696, 0), (18824036352, 5767168, 132644864, 0), (18864406528, 5767168, 138412032, 0), (18870173696, 5767168, 144179200, 0), (18858639360, 5767168, 149946368, 0), (18899009536, 5767168, 155713536, 0), (18904776704, 5767168, 161480704, 0), (18893242368, 5767168, 167247872, 0), (18916311040, 5767168, 173015040, 0), (18922078208, 5767168, 178782208, 0), (18910543872, 5767168, 184549376, 0), (18968215552, 5767168, 190316544, 0), (18973982720, 5767168, 196083712, 0), (18962448384, 5767168, 201850880, 0), (18985517056, 5767168, 207618048, 0), (18991284224, 5767168, 213385216, 0), (18979749888, 5767168, 219152384, 0), (19020120064, 5767168, 224919552, 0), (19025887232, 5767168, 230686720, 0), (19014352896, 5767168, 236453888, 0), (19037421568, 5767168, 242221056, 0), (19043188736, 5767168, 247988224, 0), (19031654400, 5767168, 253755392, 0), (19175833600, 5767168, 259522560, 0), (19181600768, 5767168, 265289728, 0), (19170066432, 5767168, 271056896, 0), (19210436608, 5767168, 276824064, 0), (19216203776, 5767168, 282591232, 0), (19204669440, 5767168, 288358400, 0), (19227738112, 5767168, 294125568, 0), (19233505280, 5767168, 299892736, 0), (19221970944, 5767168, 305659904, 0), (19331547136, 5767168, 311427072, 0), (19337314304, 5767168, 317194240, 0), (19325779968, 5767168, 322961408, 0)], 2: [(18379964416, 5767168, 0, 0), (18385731584, 5767168, 5767168, 0), (18374197248, 5767168, 11534336, 0), (18397265920, 5767168, 17301504, 0), (18403033088, 5767168, 23068672, 0), (18391498752, 5767168, 28835840, 0), (18414567424, 5767168, 34603008, 0), (18420334592, 5767168, 40370176, 0), (18408800256, 5767168, 46137344, 0), (18518376448, 5767168, 51904512, 0), (18524143616, 5767168, 57671680, 0), (18512609280, 5767168, 63438848, 0), (18674089984, 5767168, 69206016, 0), (18679857152, 5767168, 74973184, 0), (18668322816, 5767168, 80740352, 0), (18691391488, 5767168, 86507520, 0), (18697158656, 5767168, 92274688, 0), (18685624320, 5767168, 98041856, 0), (18760597504, 5767168, 103809024, 0), (18766364672, 5767168, 109576192, 0), (18754830336, 5767168, 115343360, 0), (18795200512, 5767168, 121110528, 0), (18800967680, 5767168, 126877696, 0), (18789433344, 5767168, 132644864, 0), (18881708032, 5767168, 138412032, 0), (18887475200, 5767168, 144179200, 0), (18875940864, 5767168, 149946368, 0), (18933612544, 5767168, 155713536, 0), (18939379712, 5767168, 161480704, 0), (18927845376, 5767168, 167247872, 0), (19002818560, 5767168, 173015040, 0), (19008585728, 5767168, 178782208, 0), (18997051392, 5767168, 184549376, 0), (19106627584, 5767168, 190316544, 0), (19112394752, 5767168, 196083712, 0), (19100860416, 5767168, 201850880, 0), (19123929088, 5767168, 207618048, 0), (19129696256, 5767168, 213385216, 0), (19118161920, 5767168, 219152384, 0), (19158532096, 5767168, 224919552, 0), (19164299264, 5767168, 230686720, 0), (19152764928, 5767168, 236453888, 0), (19193135104, 5767168, 242221056, 0), (19198902272, 5767168, 247988224, 0), (19187367936, 5767168, 253755392, 0), (19245039616, 5767168, 259522560, 0), (19250806784, 5767168, 265289728, 0), (19239272448, 5767168, 271056896, 0), (19279642624, 5767168, 276824064, 0), (19285409792, 5767168, 282591232, 0), (19273875456, 5767168, 288358400, 0), (19348848640, 5767168, 294125568, 0), (19354615808, 5767168, 299892736, 0), (19343081472, 5767168, 305659904, 0), (19366150144, 5767168, 311427072, 0), (19371917312, 5767168, 317194240, 0), (19360382976, 5767168, 322961408, 0), (19400753152, 5767168, 328728576, 0), (19406520320, 5767168, 334495744, 0), (19394985984, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b34eeec1e0>, 2: <capsule object NULL at 0x74a6bc4fe2e0>}
DEBUG 01-15 16:09:01.172219.172219 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.172884.172884 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.172455.172455 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5eb8cc21-3a2f-47a8-bcad-ad1c9c2f9fe5
DEBUG 01-15 16:09:01.173748.173748 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:01.173717.173717 client.py:127] Model loaded
DEBUG 01-15 16:09:01.173541.173541 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.173951.173951 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.173958.173958 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.173762.173762 cuda_h.py:19] end restore2model cost 0.0004417896270751953 seconds
DEBUG 01-15 16:09:01.173068.173068 cuda_h.py:19] end sllm_worker_task cost 0.011611461639404297 seconds
INFO 01-15 16:09:01.174169.174169 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5eb8cc21-3a2f-47a8-bcad-ad1c9c2f9fe5
DEBUG 01-15 16:09:01.174326.174326 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034618377685546875 seconds
DEBUG 01-15 16:09:01.174310.174310 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.174568.174568 cuda_h.py:19] end move_flatidxs cost 0.0009136199951171875 seconds
DEBUG 01-15 16:09:01.174339.174339 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.177730.177730 cuda_h.py:19] end restore2model cost 0.0030128955841064453 seconds
DEBUG 01-15 16:09:01.177857.177857 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006751537322998047 seconds
DEBUG 01-15 16:09:01.177912.177912 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.178578.178578 cuda_h.py:19] end gpu_sexperts cost 0.00028324127197265625 seconds
DEBUG 01-15 16:09:01.178792.178792 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.178191.178191 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.0265579223632812e-05 seconds
DEBUG 01-15 16:09:01.178887.178887 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.178782.178782 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.179927.179927 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009527206420898438 seconds
DEBUG 01-15 16:09:01.179300.179300 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.179944.179944 cuda_h.py:19] end gpu_group_list cost 0.00020051002502441406 seconds
DEBUG 01-15 16:09:01.180998.180998 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.181466.181466 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010552406311035156 seconds
DEBUG 01-15 16:09:01.181085.181085 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.181795.181795 cuda_h.py:19] end gpu_group_list cost 0.00021219253540039062 seconds
DEBUG 01-15 16:09:01.182428.182428 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.182595.182595 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5eb8cc21-3a2f-47a8-bcad-ad1c9c2f9fe5
DEBUG 01-15 16:09:01.182956.182956 cuda_h.py:19] end group_tensors cost 0.007272481918334961 seconds
DEBUG 01-15 16:09:01.183886.183886 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.186198.186198 cuda_h.py:19] end group pad cost 0.003509998321533203 seconds
DEBUG 01-15 16:09:01.187274.187274 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:01.213065.213065 client.py:127] Model loaded
DEBUG 01-15 16:09:01.213599.213599 cuda_h.py:19] end wait_experts_multi_device cost 0.03057074546813965 seconds
DEBUG 01-15 16:09:01.213554.213554 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.213211.213211 cuda_h.py:19] end group_einsum cost 0.026506900787353516 seconds
DEBUG 01-15 16:09:01.213335.213335 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.216759.216759 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029191970825195312 seconds
DEBUG 01-15 16:09:01.217934.217934 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.044091224670410156 seconds
DEBUG 01-15 16:09:01.218385.218385 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0049533843994140625 seconds
DEBUG 01-15 16:09:01.218888.218888 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.220733.220733 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.221637.221637 cuda_h.py:19] end index_scatter cost 0.0001513957977294922 seconds
DEBUG 01-15 16:09:01.221469.221469 cuda_h.py:19] end cpuoutputsdeal cost 0.002995729446411133 seconds
DEBUG 01-15 16:09:01.221177.221177 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.221868.221868 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.222921.222921 cuda_h.py:19] end gpu_group_tensor cost 0.00030803680419921875 seconds
DEBUG 01-15 16:09:01.222997.222997 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.222963.222963 cuda_h.py:19] end gpu_group_tensor cost 0.0002830028533935547 seconds
DEBUG 01-15 16:09:01.223930.223930 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.224895.224895 cuda_h.py:19] end gpu_group_einsum cost 0.0011932849884033203 seconds
DEBUG 01-15 16:09:01.224958.224958 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.225495.225495 cuda_h.py:19] end gpu_group_einsum cost 0.0010769367218017578 seconds
DEBUG 01-15 16:09:01.226270.226270 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.226632.226632 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.226999.226999 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022840499877929688 seconds
DEBUG 01-15 16:09:01.226662.226662 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.226460.226460 cuda_h.py:19] end concat_expert_out cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:09:01.226694.226694 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.226598.226598 cuda_h.py:19] end index_scatter cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:09:01.227111.227111 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009119510650634766 seconds
DEBUG 01-15 16:09:01.227300.227300 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.227010.227010 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.227070.227070 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015234947204589844 seconds
DEBUG 01-15 16:09:01.227110.227110 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.227179.227179 cuda_h.py:19] end concat_expert_out cost 5.602836608886719e-05 seconds
DEBUG 01-15 16:09:01.227354.227354 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.227999.227999 cuda_h.py:19] end index_scatter cost 5.650520324707031e-05 seconds
DEBUG 01-15 16:09:01.227378.227378 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005123615264892578 seconds
DEBUG 01-15 16:09:01.227308.227308 cuda_h.py:19] end gpu_experts_multi_device cost 0.049340009689331055 seconds
DEBUG 01-15 16:09:01.227694.227694 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.059432268142700195 seconds
DEBUG 01-15 16:09:01.228559.228559 cuda_h.py:19] end prefill_layer cost 0.0664527416229248 seconds
DEBUG 01-15 16:09:01.228754.228754 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:09:01.228080.228080 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.228452.228452 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:09:01.228777.228777 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:09:01.228771.228771 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:09:01.228767.228767 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 3.933906555175781e-05 seconds
DEBUG 01-15 16:09:01.228616.228616 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:09:01.228550.228550 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.228811.228811 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.228564.228564 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.229446.229446 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.229576.229576 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.229054.229054 cuda_h.py:19] end allocate_cuda_memory cost 0.0005412101745605469 seconds
DEBUG 01-15 16:09:01.230981.230981 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.230833.230833 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.230354.230354 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.230178.230178 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 624166c0-c671-4dc6-8820-cf5037c276bd
DEBUG 01-15 16:09:01.230073.230073 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.230978.230978 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.231866.231866 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 624166c0-c671-4dc6-8820-cf5037c276bd
DEBUG 01-15 16:09:01.232455.232455 cuda_h.py:19] end load_into_gpu_async cost 0.0019617080688476562 seconds
DEBUG 01-15 16:09:01.232424.232424 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.232719.232719 cuda_h.py:19] end restore_tensors2 cost 0.00019025802612304688 seconds
DEBUG 01-15 16:09:01.232921.232921 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0034208297729492188 seconds
INFO 01-15 16:09:01.232185.232185 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 624166c0-c671-4dc6-8820-cf5037c276bd
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.235025.235025 cuda_h.py:19] end self_attn cost 0.004256725311279297 seconds
DEBUG 01-15 16:09:01.235533.235533 cuda_h.py:19] end iln_self_attn_paln cost 0.00697779655456543 seconds
DEBUG 01-15 16:09:01.235177.235177 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:09:01.235841.235841 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.236341.236341 cuda_h.py:19] end gate cost 0.0006504058837890625 seconds
DEBUG 01-15 16:09:01.236839.236839 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.236056.236056 lmp.py:1912] 
DEBUG 01-15 16:09:01.236056.236056 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.236719.236719 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.236322.236322 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.236634.236634 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.236039.236039 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.236205.236205 lmp.py:1917] 
DEBUG 01-15 16:09:01.236205.236205 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.236278.236278 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.236643.236643 lmp.py:1935]   Expert 58 |     35 | CPU
DEBUG 01-15 16:09:01.237048.237048 lmp.py:1935]   Expert 47 |     58 | CPU
DEBUG 01-15 16:09:01.237499.237499 lmp.py:1935]   Expert 31 |     61 | CPU
DEBUG 01-15 16:09:01.237950.237950 lmp.py:1935]   Expert 49 |     61 | CPU
DEBUG 01-15 16:09:01.237639.237639 lmp.py:1935]   Expert  4 |     65 | CPU
DEBUG 01-15 16:09:01.237613.237613 lmp.py:1935]   Expert 38 |     69 | CPU
DEBUG 01-15 16:09:01.237587.237587 lmp.py:1935]   Expert 45 |     72 | CPU
DEBUG 01-15 16:09:01.237800.237800 lmp.py:1935]   Expert 43 |     82 | CPU
DEBUG 01-15 16:09:01.237250.237250 lmp.py:1935]   Expert 41 |     83 | CPU
DEBUG 01-15 16:09:01.237655.237655 lmp.py:1935]   Expert 33 |     95 | CPU
DEBUG 01-15 16:09:01.237344.237344 lmp.py:1935]   Expert 50 |    101 | CPU
DEBUG 01-15 16:09:01.237034.237034 lmp.py:1935]   Expert 57 |    103 | CPU
DEBUG 01-15 16:09:01.237200.237200 lmp.py:1935]   Expert 11 |    107 | CPU
DEBUG 01-15 16:09:01.237366.237366 lmp.py:1935]   Expert  2 |    112 | CPU
DEBUG 01-15 16:09:01.237055.237055 lmp.py:1935]   Expert 51 |    115 | CPU
DEBUG 01-15 16:09:01.237268.237268 lmp.py:1935]   Expert  0 |    121 | CPU
DEBUG 01-15 16:09:01.237480.237480 lmp.py:1935]   Expert 14 |    123 | CPU
DEBUG 01-15 16:09:01.237454.237454 lmp.py:1935]   Expert 54 |    128 | CPU
DEBUG 01-15 16:09:01.237190.237190 lmp.py:1935]   Expert 26 |    142 | CPU
DEBUG 01-15 16:09:01.237402.237402 lmp.py:1935]   Expert 56 |    142 | CPU
DEBUG 01-15 16:09:01.237138.237138 lmp.py:1935]   Expert 34 |    143 | CPU
DEBUG 01-15 16:09:01.237350.237350 lmp.py:1935]   Expert 27 |    153 | CPU
DEBUG 01-15 16:09:01.237324.237324 lmp.py:1935]   Expert 28 |    157 | CPU
DEBUG 01-15 16:09:01.237060.237060 lmp.py:1935]   Expert 55 |    158 | CPU
DEBUG 01-15 16:09:01.237749.237749 lmp.py:1935]   Expert 10 |    162 | CPU
DEBUG 01-15 16:09:01.237108.237108 lmp.py:1935]   Expert 25 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237181.237181 lmp.py:1935]   Expert  9 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237062.237062 lmp.py:1935]   Expert 13 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237467.237467 lmp.py:1935]   Expert 61 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237872.237872 lmp.py:1935]   Expert 48 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237561.237561 lmp.py:1935]   Expert  6 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237873.237873 lmp.py:1935]   Expert  7 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237661.237661 lmp.py:1935]   Expert 24 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237497.237497 lmp.py:1935]   Expert 46 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237808.237808 lmp.py:1935]   Expert 42 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237644.237644 lmp.py:1935]   Expert 18 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237955.237955 lmp.py:1935]   Expert 40 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237996.237996 lmp.py:1935]   Expert 29 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237546.237546 lmp.py:1935]   Expert 12 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237574.237574 lmp.py:1935]   Expert 63 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237362.237362 lmp.py:1935]   Expert 21 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237436.237436 lmp.py:1935]   Expert 22 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237748.237748 lmp.py:1935]   Expert 59 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237490.237490 lmp.py:1935]   Expert 32 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237564.237564 lmp.py:1935]   Expert 19 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237637.237637 lmp.py:1935]   Expert 36 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237234.237234 lmp.py:1935]   Expert  3 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237546.237546 lmp.py:1935]   Expert 37 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237381.237381 lmp.py:1935]   Expert 16 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237977.237977 lmp.py:1935]   Expert  1 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237051.237051 lmp.py:1935]   Expert 20 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237078.237078 lmp.py:1935]   Expert  5 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237390.237390 lmp.py:1935]   Expert  8 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237179.237179 lmp.py:1935]   Expert 30 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.237206.237206 lmp.py:1935]   Expert 15 |    272 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.237756.237756 lmp.py:1935]   Expert 62 |    273 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.238420.238420 lmp.py:1935]   Expert 39 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.238374.238374 lmp.py:1935]   Expert 35 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.238733.238733 lmp.py:1935]   Expert 17 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.238945.238945 lmp.py:1935]   Expert 60 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.238158.238158 lmp.py:1935]   Expert 52 |    356 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.238370.238370 lmp.py:1935]   Expert 23 |    362 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.238536.238536 lmp.py:1935]   Expert 44 |    378 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.238226.238226 lmp.py:1935]   Expert 53 |    438 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.238200.238200 lmp.py:1937] 
DEBUG 01-15 16:09:01.238200.238200 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.238604.238604 lmp.py:1938]   CPU:   2648 tokens
DEBUG 01-15 16:09:01.238009.238009 lmp.py:1942]   cuda:1:   4901 tokens (20 experts)
DEBUG 01-15 16:09:01.238652.238652 lmp.py:1942]   cuda:2:   4739 tokens (19 experts)
DEBUG 01-15 16:09:01.238063.238063 lmp.py:1943]   Total GPU:   9640 tokens
DEBUG 01-15 16:09:01.238799.238799 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.238799.238799 lmp.py:1944] 
DEBUG 01-15 16:09:01.238733.238733 cuda_h.py:19] end experts_map_get cost 0.0017538070678710938 seconds
DEBUG 01-15 16:09:01.238007.238007 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.238379.238379 lmp.py:1953] 
DEBUG 01-15 16:09:01.238379.238379 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.238407.238407 cuda_h.py:19] end cpu_experts_submit cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:09:01.238672.238672 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.238986.238986 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.239640.239640 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.3.gate_proj.weight': 0, 'model.layers.16.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.59.up_proj.weight': 322961408, 'model.layers.16.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.16.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.16.mlp.experts.61.up_proj.weight': 340262912}, 2: {'model.layers.16.mlp.experts.1.gate_proj.weight': 0, 'model.layers.16.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.60.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.60.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.60.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(19521863680, 5767168, 0, 0), (19527630848, 5767168, 5767168, 0), (19516096512, 5767168, 11534336, 0), (19573768192, 5767168, 17301504, 0), (19579535360, 5767168, 23068672, 0), (19568001024, 5767168, 28835840, 0), (19608371200, 5767168, 34603008, 0), (19614138368, 5767168, 40370176, 0), (19602604032, 5767168, 46137344, 0), (19625672704, 5767168, 51904512, 0), (19631439872, 5767168, 57671680, 0), (19619905536, 5767168, 63438848, 0), (19677577216, 5767168, 69206016, 0), (19683344384, 5767168, 74973184, 0), (19671810048, 5767168, 80740352, 0), (19729481728, 5767168, 86507520, 0), (19735248896, 5767168, 92274688, 0), (19723714560, 5767168, 98041856, 0), (19746783232, 5767168, 103809024, 0), (19752550400, 5767168, 109576192, 0), (19741016064, 5767168, 115343360, 0), (19764084736, 5767168, 121110528, 0), (19769851904, 5767168, 126877696, 0), (19758317568, 5767168, 132644864, 0), (19798687744, 5767168, 138412032, 0), (19804454912, 5767168, 144179200, 0), (19792920576, 5767168, 149946368, 0), (19815989248, 5767168, 155713536, 0), (19821756416, 5767168, 161480704, 0), (19810222080, 5767168, 167247872, 0), (19850592256, 5767168, 173015040, 0), (19856359424, 5767168, 178782208, 0), (19844825088, 5767168, 184549376, 0), (19902496768, 5767168, 190316544, 0), (19908263936, 5767168, 196083712, 0), (19896729600, 5767168, 201850880, 0), (20144717824, 5767168, 207618048, 0), (20150484992, 5767168, 213385216, 0), (20138950656, 5767168, 219152384, 0), (20162019328, 5767168, 224919552, 0), (20167786496, 5767168, 230686720, 0), (20156252160, 5767168, 236453888, 0), (20196622336, 5767168, 242221056, 0), (20202389504, 5767168, 247988224, 0), (20190855168, 5767168, 253755392, 0), (20265828352, 5767168, 259522560, 0), (20271595520, 5767168, 265289728, 0), (20260061184, 5767168, 271056896, 0), (20369637376, 5767168, 276824064, 0), (20375404544, 5767168, 282591232, 0), (20363870208, 5767168, 288358400, 0), (20386938880, 5767168, 294125568, 0), (20392706048, 5767168, 299892736, 0), (20381171712, 5767168, 305659904, 0), (20490747904, 5767168, 311427072, 0), (20496515072, 5767168, 317194240, 0), (20484980736, 5767168, 322961408, 0), (20525350912, 5767168, 328728576, 0), (20531118080, 5767168, 334495744, 0), (20519583744, 5767168, 340262912, 0)], 2: [(19487260672, 5767168, 0, 0), (19493027840, 5767168, 5767168, 0), (19481493504, 5767168, 11534336, 0), (19556466688, 5767168, 17301504, 0), (19562233856, 5767168, 23068672, 0), (19550699520, 5767168, 28835840, 0), (19591069696, 5767168, 34603008, 0), (19596836864, 5767168, 40370176, 0), (19585302528, 5767168, 46137344, 0), (19694878720, 5767168, 51904512, 0), (19700645888, 5767168, 57671680, 0), (19689111552, 5767168, 63438848, 0), (19781386240, 5767168, 69206016, 0), (19787153408, 5767168, 74973184, 0), (19775619072, 5767168, 80740352, 0), (19833290752, 5767168, 86507520, 0), (19839057920, 5767168, 92274688, 0), (19827523584, 5767168, 98041856, 0), (19867893760, 5767168, 103809024, 0), (19873660928, 5767168, 109576192, 0), (19862126592, 5767168, 115343360, 0), (19885195264, 5767168, 121110528, 0), (19890962432, 5767168, 126877696, 0), (19879428096, 5767168, 132644864, 0), (19971702784, 5767168, 138412032, 0), (19977469952, 5767168, 144179200, 0), (19965935616, 5767168, 149946368, 0), (19989004288, 5767168, 155713536, 0), (19994771456, 5767168, 161480704, 0), (19983237120, 5767168, 167247872, 0), (20023607296, 5767168, 173015040, 0), (20029374464, 5767168, 178782208, 0), (20017840128, 5767168, 184549376, 0), (20075511808, 5767168, 190316544, 0), (20081278976, 5767168, 196083712, 0), (20069744640, 5767168, 201850880, 0), (20092813312, 5767168, 207618048, 0), (20098580480, 5767168, 213385216, 0), (20087046144, 5767168, 219152384, 0), (20110114816, 5767168, 224919552, 0), (20115881984, 5767168, 230686720, 0), (20104347648, 5767168, 236453888, 0), (20231225344, 5767168, 242221056, 0), (20236992512, 5767168, 247988224, 0), (20225458176, 5767168, 253755392, 0), (20300431360, 5767168, 259522560, 0), (20306198528, 5767168, 265289728, 0), (20294664192, 5767168, 271056896, 0), (20508049408, 5767168, 276824064, 0), (20513816576, 5767168, 282591232, 0), (20502282240, 5767168, 288358400, 0), (20542652416, 5767168, 294125568, 0), (20548419584, 5767168, 299892736, 0), (20536885248, 5767168, 305659904, 0), (20559953920, 5767168, 311427072, 0), (20565721088, 5767168, 317194240, 0), (20554186752, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a9f372bdb0>, 2: <capsule object NULL at 0x74a6bc4fe070>}
DEBUG 01-15 16:09:01.239941.239941 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.239334.239334 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.239143.239143 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d1959fb4-c586-4adf-8b36-1e8bff1ec8fe
DEBUG 01-15 16:09:01.240138.240138 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.240496.240496 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:01.240949.240949 client.py:127] Model loaded
DEBUG 01-15 16:09:01.240263.240263 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.240073.240073 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.241805.241805 cuda_h.py:19] end move_flatidxs cost 0.0008862018585205078 seconds
DEBUG 01-15 16:09:01.241496.241496 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.241358.241358 cuda_h.py:19] end restore2model cost 0.001094818115234375 seconds
INFO 01-15 16:09:01.241013.241013 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d1959fb4-c586-4adf-8b36-1e8bff1ec8fe
DEBUG 01-15 16:09:01.242428.242428 cuda_h.py:19] end sllm_worker_task cost 0.012923717498779297 seconds
DEBUG 01-15 16:09:01.242818.242818 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004046440124511719 seconds
DEBUG 01-15 16:09:01.242925.242925 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.245949.245949 cuda_h.py:19] end restore2model cost 0.003069162368774414 seconds
DEBUG 01-15 16:09:01.245931.245931 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00750732421875 seconds
DEBUG 01-15 16:09:01.246462.246462 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.246281.246281 cuda_h.py:19] end gpu_sexperts cost 0.0002906322479248047 seconds
DEBUG 01-15 16:09:01.246111.246111 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.246371.246371 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.3603439331054688e-05 seconds
DEBUG 01-15 16:09:01.246041.246041 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.246651.246651 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.247990.247990 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009908676147460938 seconds
DEBUG 01-15 16:09:01.247317.247317 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.247265.247265 cuda_h.py:19] end gpu_group_list cost 0.00021338462829589844 seconds
DEBUG 01-15 16:09:01.248141.248141 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.249668.249668 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010292530059814453 seconds
DEBUG 01-15 16:09:01.249432.249432 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.250406.250406 cuda_h.py:19] end gpu_group_list cost 0.00019693374633789062 seconds
DEBUG 01-15 16:09:01.250634.250634 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.250371.250371 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d1959fb4-c586-4adf-8b36-1e8bff1ec8fe
DEBUG 01-15 16:09:01.251340.251340 cuda_h.py:19] end group_tensors cost 0.009934186935424805 seconds
DEBUG 01-15 16:09:01.252085.252085 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.255568.255568 cuda_h.py:19] end group pad cost 0.002689838409423828 seconds
DEBUG 01-15 16:09:01.255834.255834 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:01.282088.282088 client.py:127] Model loaded
DEBUG 01-15 16:09:01.283219.283219 cuda_h.py:19] end wait_experts_multi_device cost 0.0322413444519043 seconds
DEBUG 01-15 16:09:01.283618.283618 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.284546.284546 cuda_h.py:19] end group_einsum cost 0.02868795394897461 seconds
DEBUG 01-15 16:09:01.284026.284026 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.286789.286789 cuda_h.py:19] end get_outputs_cpu1 cost 0.002542734146118164 seconds
DEBUG 01-15 16:09:01.287962.287962 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04742264747619629 seconds
DEBUG 01-15 16:09:01.288826.288826 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0051343441009521484 seconds
DEBUG 01-15 16:09:01.288050.288050 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.289656.289656 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.289628.289628 cuda_h.py:19] end index_scatter cost 8.0108642578125e-05 seconds
DEBUG 01-15 16:09:01.289624.289624 cuda_h.py:19] end cpuoutputsdeal cost 0.0012960433959960938 seconds
DEBUG 01-15 16:09:01.289939.289939 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.289079.289079 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.290263.290263 cuda_h.py:19] end gpu_group_tensor cost 0.0001399517059326172 seconds
DEBUG 01-15 16:09:01.290264.290264 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.290336.290336 cuda_h.py:19] end gpu_group_tensor cost 0.00013184547424316406 seconds
DEBUG 01-15 16:09:01.290816.290816 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.291508.291508 cuda_h.py:19] end gpu_group_einsum cost 0.0010027885437011719 seconds
DEBUG 01-15 16:09:01.291237.291237 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.292514.292514 cuda_h.py:19] end gpu_group_einsum cost 0.0004911422729492188 seconds
DEBUG 01-15 16:09:01.292486.292486 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.292517.292517 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.292341.292341 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024390220642089844 seconds
DEBUG 01-15 16:09:01.292025.292025 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.292677.292677 cuda_h.py:19] end concat_expert_out cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:09:01.293719.293719 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.293862.293862 cuda_h.py:19] end index_scatter cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:09:01.293248.293248 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009233951568603516 seconds
DEBUG 01-15 16:09:01.293092.293092 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.293511.293511 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.293908.293908 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015592575073242188 seconds
DEBUG 01-15 16:09:01.293949.293949 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.293064.293064 cuda_h.py:19] end concat_expert_out cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:09:01.293570.293570 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.294877.294877 cuda_h.py:19] end index_scatter cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:09:01.294209.294209 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005064010620117188 seconds
DEBUG 01-15 16:09:01.294179.294179 cuda_h.py:19] end gpu_experts_multi_device cost 0.04762148857116699 seconds
DEBUG 01-15 16:09:01.294089.294089 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.05843853950500488 seconds
DEBUG 01-15 16:09:01.294622.294622 cuda_h.py:19] end prefill_layer cost 0.06620240211486816 seconds
DEBUG 01-15 16:09:01.294266.294266 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:09:01.294446.294446 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.294103.294103 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:09:01.294428.294428 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:09:01.294376.294376 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:09:01.294895.294895 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 3.886222839355469e-05 seconds
DEBUG 01-15 16:09:01.294505.294505 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 6.985664367675781e-05 seconds
DEBUG 01-15 16:09:01.294201.294201 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.295754.295754 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.295712.295712 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.295179.295179 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.295879.295879 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.296094.296094 cuda_h.py:19] end allocate_cuda_memory cost 0.0005526542663574219 seconds
DEBUG 01-15 16:09:01.296467.296467 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.296484.296484 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.296415.296415 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.296690.296690 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e1b89d00-4b5d-4825-a49a-d46f34746340
DEBUG 01-15 16:09:01.297419.297419 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.297628.297628 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.299507.299507 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e1b89d00-4b5d-4825-a49a-d46f34746340
DEBUG 01-15 16:09:01.299321.299321 cuda_h.py:19] end load_into_gpu_async cost 0.002650737762451172 seconds
DEBUG 01-15 16:09:01.299337.299337 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.299790.299790 cuda_h.py:19] end restore_tensors2 cost 0.00016951560974121094 seconds
DEBUG 01-15 16:09:01.299701.299701 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0042057037353515625 seconds
INFO 01-15 16:09:01.299057.299057 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e1b89d00-4b5d-4825-a49a-d46f34746340
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.301884.301884 cuda_h.py:19] end self_attn cost 0.003957033157348633 seconds
DEBUG 01-15 16:09:01.301737.301737 cuda_h.py:19] end iln_self_attn_paln cost 0.0068509578704833984 seconds
DEBUG 01-15 16:09:01.301334.301334 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:09:01.301521.301521 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.302781.302781 cuda_h.py:19] end gate cost 0.0006101131439208984 seconds
DEBUG 01-15 16:09:01.302001.302001 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.303887.303887 lmp.py:1912] 
DEBUG 01-15 16:09:01.303887.303887 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.303550.303550 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.303008.303008 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.303558.303558 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.303678.303678 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.303606.303606 lmp.py:1917] 
DEBUG 01-15 16:09:01.303606.303606 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.303202.303202 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.303806.303806 lmp.py:1935]   Expert  4 |      9 | CPU
DEBUG 01-15 16:09:01.303972.303972 lmp.py:1935]   Expert 28 |     29 | CPU
DEBUG 01-15 16:09:01.303946.303946 lmp.py:1935]   Expert  7 |     46 | CPU
DEBUG 01-15 16:09:01.303159.303159 lmp.py:1935]   Expert 53 |     56 | CPU
DEBUG 01-15 16:09:01.303371.303371 lmp.py:1935]   Expert 52 |     69 | CPU
DEBUG 01-15 16:09:01.303868.303868 lmp.py:1935]   Expert 43 |     70 | CPU
DEBUG 01-15 16:09:01.303604.303604 lmp.py:1935]   Expert 49 |     84 | CPU
DEBUG 01-15 16:09:01.303101.303101 lmp.py:1935]   Expert 12 |     90 | CPU
DEBUG 01-15 16:09:01.303314.303314 lmp.py:1935]   Expert 47 |    101 | CPU
DEBUG 01-15 16:09:01.303241.303241 lmp.py:1935]   Expert 33 |    106 | CPU
DEBUG 01-15 16:09:01.303407.303407 lmp.py:1935]   Expert 24 |    107 | CPU
DEBUG 01-15 16:09:01.303097.303097 lmp.py:1935]   Expert 50 |    107 | CPU
DEBUG 01-15 16:09:01.303024.303024 lmp.py:1935]   Expert  2 |    111 | CPU
DEBUG 01-15 16:09:01.303191.303191 lmp.py:1935]   Expert 15 |    111 | CPU
DEBUG 01-15 16:09:01.303165.303165 lmp.py:1935]   Expert 60 |    112 | CPU
DEBUG 01-15 16:09:01.303662.303662 lmp.py:1935]   Expert 39 |    115 | CPU
DEBUG 01-15 16:09:01.303397.303397 lmp.py:1935]   Expert 36 |    117 | CPU
DEBUG 01-15 16:09:01.303133.303133 lmp.py:1935]   Expert  6 |    124 | CPU
DEBUG 01-15 16:09:01.303869.303869 lmp.py:1935]   Expert 25 |    124 | CPU
DEBUG 01-15 16:09:01.303843.303843 lmp.py:1935]   Expert 61 |    128 | CPU
DEBUG 01-15 16:09:01.303340.303340 lmp.py:1935]   Expert 59 |    132 | CPU
DEBUG 01-15 16:09:01.303175.303175 lmp.py:1935]   Expert  3 |    142 | CPU
DEBUG 01-15 16:09:01.303772.303772 lmp.py:1935]   Expert 27 |    143 | CPU
DEBUG 01-15 16:09:01.303130.303130 lmp.py:1935]   Expert 58 |    148 | CPU
DEBUG 01-15 16:09:01.303488.303488 lmp.py:1935]   Expert  8 |    149 | CPU
DEBUG 01-15 16:09:01.303184.303184 lmp.py:1935]   Expert 30 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303403.303403 lmp.py:1935]   Expert 31 |    153 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303736.303736 lmp.py:1935]   Expert 10 |    156 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303763.303763 lmp.py:1935]   Expert 38 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303075.303075 lmp.py:1935]   Expert 40 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303148.303148 lmp.py:1935]   Expert 57 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303699.303699 lmp.py:1935]   Expert 41 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303295.303295 lmp.py:1935]   Expert 14 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303607.303607 lmp.py:1935]   Expert 37 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303919.303919 lmp.py:1935]   Expert 32 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303469.303469 lmp.py:1935]   Expert 54 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303973.303973 lmp.py:1935]   Expert 46 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303001.303001 lmp.py:1935]   Expert 19 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303028.303028 lmp.py:1935]   Expert 42 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303055.303055 lmp.py:1935]   Expert 11 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303367.303367 lmp.py:1935]   Expert 34 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303679.303679 lmp.py:1935]   Expert 22 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303514.303514 lmp.py:1935]   Expert 18 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303349.303349 lmp.py:1935]   Expert 26 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.303422.303422 lmp.py:1935]   Expert  0 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.303496.303496 lmp.py:1935]   Expert 56 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.304808.304808 lmp.py:1935]   Expert 44 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304358.304358 lmp.py:1935]   Expert  1 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.304385.304385 lmp.py:1935]   Expert 51 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304651.304651 lmp.py:1935]   Expert 20 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.304439.304439 lmp.py:1935]   Expert 29 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304705.304705 lmp.py:1935]   Expert 48 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.304779.304779 lmp.py:1935]   Expert 45 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304852.304852 lmp.py:1935]   Expert 21 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.304926.304926 lmp.py:1935]   Expert 35 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304761.304761 lmp.py:1935]   Expert 16 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.304834.304834 lmp.py:1935]   Expert 55 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304908.304908 lmp.py:1935]   Expert  5 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.304981.304981 lmp.py:1935]   Expert 23 |    369 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304293.304293 lmp.py:1935]   Expert 13 |    384 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.304843.304843 lmp.py:1935]   Expert 17 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304109.304109 lmp.py:1935]   Expert  9 |    460 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304136.304136 lmp.py:1935]   Expert 63 |    463 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.304402.304402 lmp.py:1935]   Expert 62 |   1182 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.304521.304521 lmp.py:1937] 
DEBUG 01-15 16:09:01.304521.304521 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.304356.304356 lmp.py:1938]   CPU:   2530 tokens
DEBUG 01-15 16:09:01.304344.304344 lmp.py:1942]   cuda:1:   4917 tokens (19 experts)
DEBUG 01-15 16:09:01.304868.304868 lmp.py:1942]   cuda:2:   4841 tokens (20 experts)
DEBUG 01-15 16:09:01.304465.304465 lmp.py:1943]   Total GPU:   9758 tokens
DEBUG 01-15 16:09:01.304300.304300 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.304300.304300 lmp.py:1944] 
DEBUG 01-15 16:09:01.304573.304573 cuda_h.py:19] end experts_map_get cost 0.0017955303192138672 seconds
DEBUG 01-15 16:09:01.304946.304946 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.304748.304748 lmp.py:1953] 
DEBUG 01-15 16:09:01.304748.304748 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.304730.304730 cuda_h.py:19] end cpu_experts_submit cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:09:01.304280.304280 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.304501.304501 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.305601.305601 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.1.gate_proj.weight': 0, 'model.layers.17.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.30.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.30.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.30.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.32.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.32.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.32.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.38.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.38.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.38.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.41.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.41.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.41.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.9.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.9.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.9.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.17.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.17.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.17.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.18.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.18.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.18.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.23.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.23.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.23.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.29.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.29.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.29.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.31.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.31.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.31.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.34.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.34.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.34.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.17.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.17.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.17.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(20594556928, 5767168, 0, 0), (20600324096, 5767168, 5767168, 0), (20588789760, 5767168, 11534336, 0), (20663762944, 5767168, 17301504, 0), (20669530112, 5767168, 23068672, 0), (20657995776, 5767168, 28835840, 0), (20750270464, 5767168, 34603008, 0), (20756037632, 5767168, 40370176, 0), (20744503296, 5767168, 46137344, 0), (20767571968, 5767168, 51904512, 0), (20773339136, 5767168, 57671680, 0), (20761804800, 5767168, 63438848, 0), (20802174976, 5767168, 69206016, 0), (20807942144, 5767168, 74973184, 0), (20796407808, 5767168, 80740352, 0), (20819476480, 5767168, 86507520, 0), (20825243648, 5767168, 92274688, 0), (20813709312, 5767168, 98041856, 0), (20854079488, 5767168, 103809024, 0), (20859846656, 5767168, 109576192, 0), (20848312320, 5767168, 115343360, 0), (20905984000, 5767168, 121110528, 0), (20911751168, 5767168, 126877696, 0), (20900216832, 5767168, 132644864, 0), (20923285504, 5767168, 138412032, 0), (20929052672, 5767168, 144179200, 0), (20917518336, 5767168, 149946368, 0), (20940587008, 5767168, 155713536, 0), (20946354176, 5767168, 161480704, 0), (20934819840, 5767168, 167247872, 0), (20957888512, 5767168, 173015040, 0), (20963655680, 5767168, 178782208, 0), (20952121344, 5767168, 184549376, 0), (21027094528, 5767168, 190316544, 0), (21032861696, 5767168, 196083712, 0), (21021327360, 5767168, 201850880, 0), (21096300544, 5767168, 207618048, 0), (21102067712, 5767168, 213385216, 0), (21090533376, 5767168, 219152384, 0), (21130903552, 5767168, 224919552, 0), (21136670720, 5767168, 230686720, 0), (21125136384, 5767168, 236453888, 0), (21234712576, 5767168, 242221056, 0), (21240479744, 5767168, 247988224, 0), (21228945408, 5767168, 253755392, 0), (21286617088, 5767168, 259522560, 0), (21292384256, 5767168, 265289728, 0), (21280849920, 5767168, 271056896, 0), (21407727616, 5767168, 276824064, 0), (21413494784, 5767168, 282591232, 0), (21401960448, 5767168, 288358400, 0), (21546139648, 5767168, 294125568, 0), (21551906816, 5767168, 299892736, 0), (21540372480, 5767168, 305659904, 0), (21649948672, 5767168, 311427072, 0), (21655715840, 5767168, 317194240, 0), (21644181504, 5767168, 322961408, 0)], 2: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (20732968960, 5767168, 17301504, 0), (20738736128, 5767168, 23068672, 0), (20727201792, 5767168, 28835840, 0), (20871380992, 5767168, 34603008, 0), (20877148160, 5767168, 40370176, 0), (20865613824, 5767168, 46137344, 0), (20888682496, 5767168, 51904512, 0), (20894449664, 5767168, 57671680, 0), (20882915328, 5767168, 63438848, 0), (20975190016, 5767168, 69206016, 0), (20980957184, 5767168, 74973184, 0), (20969422848, 5767168, 80740352, 0), (21078999040, 5767168, 86507520, 0), (21084766208, 5767168, 92274688, 0), (21073231872, 5767168, 98041856, 0), (21113602048, 5767168, 103809024, 0), (21119369216, 5767168, 109576192, 0), (21107834880, 5767168, 115343360, 0), (21165506560, 5767168, 121110528, 0), (21171273728, 5767168, 126877696, 0), (21159739392, 5767168, 132644864, 0), (21182808064, 5767168, 138412032, 0), (21188575232, 5767168, 144179200, 0), (21177040896, 5767168, 149946368, 0), (21217411072, 5767168, 155713536, 0), (21223178240, 5767168, 161480704, 0), (21211643904, 5767168, 167247872, 0), (21269315584, 5767168, 173015040, 0), (21275082752, 5767168, 178782208, 0), (21263548416, 5767168, 184549376, 0), (21303918592, 5767168, 190316544, 0), (21309685760, 5767168, 196083712, 0), (21298151424, 5767168, 201850880, 0), (21338521600, 5767168, 207618048, 0), (21344288768, 5767168, 213385216, 0), (21332754432, 5767168, 219152384, 0), (21355823104, 5767168, 224919552, 0), (21361590272, 5767168, 230686720, 0), (21350055936, 5767168, 236453888, 0), (21373124608, 5767168, 242221056, 0), (21378891776, 5767168, 247988224, 0), (21367357440, 5767168, 253755392, 0), (21459632128, 5767168, 259522560, 0), (21465399296, 5767168, 265289728, 0), (21453864960, 5767168, 271056896, 0), (21511536640, 5767168, 276824064, 0), (21517303808, 5767168, 282591232, 0), (21505769472, 5767168, 288358400, 0), (21528838144, 5767168, 294125568, 0), (21534605312, 5767168, 299892736, 0), (21523070976, 5767168, 305659904, 0), (21563441152, 5767168, 311427072, 0), (21569208320, 5767168, 317194240, 0), (21557673984, 5767168, 322961408, 0), (21667250176, 5767168, 328728576, 0), (21673017344, 5767168, 334495744, 0), (21661483008, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc4fe310>, 2: <capsule object NULL at 0x74a6bc4fe370>}
DEBUG 01-15 16:09:01.305188.305188 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.305396.305396 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.305967.305967 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 81ee9b30-c92a-49a1-ab59-e6227cf95852
DEBUG 01-15 16:09:01.306193.306193 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:01.307399.307399 client.py:127] Model loaded
DEBUG 01-15 16:09:01.307651.307651 cuda_h.py:10] start restore2model
INFO 01-15 16:09:01.307993.307993 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 81ee9b30-c92a-49a1-ab59-e6227cf95852
DEBUG 01-15 16:09:01.307947.307947 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.308415.308415 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0033416748046875 seconds
DEBUG 01-15 16:09:01.308359.308359 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.308604.308604 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.309765.309765 cuda_h.py:19] end restore2model cost 0.0010454654693603516 seconds
DEBUG 01-15 16:09:01.309568.309568 cuda_h.py:19] end move_flatidxs cost 0.0010075569152832031 seconds
DEBUG 01-15 16:09:01.309556.309556 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.309457.309457 cuda_h.py:19] end sllm_worker_task cost 0.014274120330810547 seconds
DEBUG 01-15 16:09:01.312976.312976 cuda_h.py:19] end restore2model cost 0.0044057369232177734 seconds
DEBUG 01-15 16:09:01.312073.312073 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008216381072998047 seconds
DEBUG 01-15 16:09:01.312299.312299 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.313673.313673 cuda_h.py:19] end gpu_sexperts cost 0.0002772808074951172 seconds
DEBUG 01-15 16:09:01.313933.313933 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.313624.313624 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.3603439331054688e-05 seconds
DEBUG 01-15 16:09:01.313035.313035 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.313645.313645 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.314532.314532 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009396076202392578 seconds
DEBUG 01-15 16:09:01.314998.314998 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.314774.314774 cuda_h.py:19] end gpu_group_list cost 0.00019288063049316406 seconds
DEBUG 01-15 16:09:01.315145.315145 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.316818.316818 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001069784164428711 seconds
DEBUG 01-15 16:09:01.316251.316251 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.316286.316286 cuda_h.py:19] end gpu_group_list cost 0.00020599365234375 seconds
DEBUG 01-15 16:09:01.317898.317898 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.317172.317172 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 81ee9b30-c92a-49a1-ab59-e6227cf95852
DEBUG 01-15 16:09:01.320769.320769 cuda_h.py:19] end group_tensors cost 0.010261058807373047 seconds
DEBUG 01-15 16:09:01.320753.320753 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.324216.324216 cuda_h.py:19] end group pad cost 0.0036902427673339844 seconds
DEBUG 01-15 16:09:01.324099.324099 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:01.346876.346876 client.py:127] Model loaded
DEBUG 01-15 16:09:01.346148.346148 cuda_h.py:19] end wait_experts_multi_device cost 0.028934001922607422 seconds
DEBUG 01-15 16:09:01.346103.346103 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.352296.352296 cuda_h.py:19] end group_einsum cost 0.027136564254760742 seconds
DEBUG 01-15 16:09:01.352732.352732 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.354214.354214 cuda_h.py:19] end get_outputs_cpu1 cost 0.002657175064086914 seconds
DEBUG 01-15 16:09:01.355251.355251 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04766488075256348 seconds
DEBUG 01-15 16:09:01.356169.356169 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01000213623046875 seconds
DEBUG 01-15 16:09:01.356554.356554 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.359879.359879 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.359811.359811 cuda_h.py:19] end index_scatter cost 0.00017690658569335938 seconds
DEBUG 01-15 16:09:01.360796.360796 cuda_h.py:19] end cpuoutputsdeal cost 0.0029883384704589844 seconds
DEBUG 01-15 16:09:01.360650.360650 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.360057.360057 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.360620.360620 cuda_h.py:19] end gpu_group_tensor cost 0.0003287792205810547 seconds
DEBUG 01-15 16:09:01.360796.360796 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.361530.361530 cuda_h.py:19] end gpu_group_tensor cost 0.0002875328063964844 seconds
DEBUG 01-15 16:09:01.361981.361981 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.364024.364024 cuda_h.py:19] end gpu_group_einsum cost 0.0027353763580322266 seconds
DEBUG 01-15 16:09:01.364767.364767 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.366612.366612 cuda_h.py:19] end gpu_group_einsum cost 0.0014927387237548828 seconds
DEBUG 01-15 16:09:01.366100.366100 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.366713.366713 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.367768.367768 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021123886108398438 seconds
DEBUG 01-15 16:09:01.367147.367147 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.367389.367389 cuda_h.py:19] end concat_expert_out cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:09:01.367444.367444 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.367355.367355 cuda_h.py:19] end index_scatter cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:09:01.367073.367073 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009450912475585938 seconds
DEBUG 01-15 16:09:01.367355.367355 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.367013.367013 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.368496.368496 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014925003051757812 seconds
DEBUG 01-15 16:09:01.368344.368344 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.368844.368844 cuda_h.py:19] end concat_expert_out cost 5.7697296142578125e-05 seconds
DEBUG 01-15 16:09:01.368594.368594 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.368617.368617 cuda_h.py:19] end index_scatter cost 5.507469177246094e-05 seconds
DEBUG 01-15 16:09:01.368665.368665 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005083084106445312 seconds
DEBUG 01-15 16:09:01.368497.368497 cuda_h.py:19] end gpu_experts_multi_device cost 0.055091142654418945 seconds
DEBUG 01-15 16:09:01.368466.368466 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.06660699844360352 seconds
DEBUG 01-15 16:09:01.368421.368421 cuda_h.py:19] end prefill_layer cost 0.07417678833007812 seconds
DEBUG 01-15 16:09:01.369065.369065 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:09:01.369430.369430 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.369557.369557 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:09:01.369160.369160 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:09:01.369148.369148 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:09:01.369275.369275 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 3.4809112548828125e-05 seconds
DEBUG 01-15 16:09:01.369502.369502 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:09:01.369860.369860 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.369061.369061 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.369285.369285 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.369864.369864 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.369062.369062 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.370630.370630 cuda_h.py:19] end allocate_cuda_memory cost 0.0005908012390136719 seconds
DEBUG 01-15 16:09:01.370622.370622 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.370693.370693 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.371446.371446 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.371746.371746 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 59513af9-5c11-4064-8441-1bfeb9a31cd9
DEBUG 01-15 16:09:01.371311.371311 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.371594.371594 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.373790.373790 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 59513af9-5c11-4064-8441-1bfeb9a31cd9
DEBUG 01-15 16:09:01.373365.373365 cuda_h.py:19] end load_into_gpu_async cost 0.002283811569213867 seconds
DEBUG 01-15 16:09:01.373686.373686 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.373794.373794 cuda_h.py:19] end restore_tensors2 cost 0.00017523765563964844 seconds
DEBUG 01-15 16:09:01.373367.373367 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003828287124633789 seconds
INFO 01-15 16:09:01.373883.373883 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 59513af9-5c11-4064-8441-1bfeb9a31cd9
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.375409.375409 cuda_h.py:19] end self_attn cost 0.003843069076538086 seconds
DEBUG 01-15 16:09:01.376552.376552 cuda_h.py:19] end iln_self_attn_paln cost 0.0067996978759765625 seconds
DEBUG 01-15 16:09:01.376673.376673 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:09:01.376144.376144 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.376690.376690 cuda_h.py:19] end gate cost 0.0006504058837890625 seconds
DEBUG 01-15 16:09:01.376712.376712 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.377141.377141 lmp.py:1912] 
DEBUG 01-15 16:09:01.377141.377141 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.377758.377758 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.377269.377269 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.377296.377296 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.377939.377939 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.377913.377913 lmp.py:1917] 
DEBUG 01-15 16:09:01.377913.377913 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.377794.377794 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.377206.377206 lmp.py:1935]   Expert 32 |     33 | CPU
DEBUG 01-15 16:09:01.377372.377372 lmp.py:1935]   Expert 30 |     51 | CPU
DEBUG 01-15 16:09:01.377300.377300 lmp.py:1935]   Expert  5 |     53 | CPU
DEBUG 01-15 16:09:01.377088.377088 lmp.py:1935]   Expert 46 |     74 | CPU
DEBUG 01-15 16:09:01.377115.377115 lmp.py:1935]   Expert 40 |     89 | CPU
DEBUG 01-15 16:09:01.377712.377712 lmp.py:1935]   Expert  8 |     93 | CPU
DEBUG 01-15 16:09:01.377547.377547 lmp.py:1935]   Expert 12 |    100 | CPU
DEBUG 01-15 16:09:01.377859.377859 lmp.py:1935]   Expert 17 |    107 | CPU
DEBUG 01-15 16:09:01.377694.377694 lmp.py:1935]   Expert 60 |    109 | CPU
DEBUG 01-15 16:09:01.377529.377529 lmp.py:1935]   Expert  3 |    113 | CPU
DEBUG 01-15 16:09:01.377126.377126 lmp.py:1935]   Expert 27 |    114 | CPU
DEBUG 01-15 16:09:01.377438.377438 lmp.py:1935]   Expert 58 |    118 | CPU
DEBUG 01-15 16:09:01.377034.377034 lmp.py:1935]   Expert 28 |    120 | CPU
DEBUG 01-15 16:09:01.377360.377360 lmp.py:1935]   Expert 29 |    120 | CPU
DEBUG 01-15 16:09:01.377718.377718 lmp.py:1935]   Expert 21 |    121 | CPU
DEBUG 01-15 16:09:01.377076.377076 lmp.py:1935]   Expert 25 |    129 | CPU
DEBUG 01-15 16:09:01.377435.377435 lmp.py:1935]   Expert 41 |    131 | CPU
DEBUG 01-15 16:09:01.377554.377554 lmp.py:1935]   Expert 35 |    132 | CPU
DEBUG 01-15 16:09:01.377151.377151 lmp.py:1935]   Expert 19 |    137 | CPU
DEBUG 01-15 16:09:01.377509.377509 lmp.py:1935]   Expert  0 |    144 | CPU
DEBUG 01-15 16:09:01.377344.377344 lmp.py:1935]   Expert  6 |    146 | CPU
DEBUG 01-15 16:09:01.377464.377464 lmp.py:1935]   Expert 52 |    149 | CPU
DEBUG 01-15 16:09:01.377822.377822 lmp.py:1935]   Expert 54 |    149 | CPU
DEBUG 01-15 16:09:01.377704.377704 lmp.py:1935]   Expert 56 |    149 | CPU
DEBUG 01-15 16:09:01.377347.377347 lmp.py:1935]   Expert 37 |    150 | CPU
DEBUG 01-15 16:09:01.377612.377612 lmp.py:1935]   Expert 53 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.377878.377878 lmp.py:1935]   Expert 48 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.377097.377097 lmp.py:1935]   Expert 63 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.377124.377124 lmp.py:1935]   Expert 36 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.377913.377913 lmp.py:1935]   Expert 59 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.377225.377225 lmp.py:1935]   Expert  9 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.377537.377537 lmp.py:1935]   Expert  1 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378610.378610 lmp.py:1935]   Expert 39 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378922.378922 lmp.py:1935]   Expert 20 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378996.378996 lmp.py:1935]   Expert 61 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378546.378546 lmp.py:1935]   Expert  7 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378143.378143 lmp.py:1935]   Expert 42 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378693.378693 lmp.py:1935]   Expert 43 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378720.378720 lmp.py:1935]   Expert 11 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378271.378271 lmp.py:1935]   Expert 47 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378821.378821 lmp.py:1935]   Expert 34 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378133.378133 lmp.py:1935]   Expert 55 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378445.378445 lmp.py:1935]   Expert 13 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378518.378518 lmp.py:1935]   Expert 57 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378068.378068 lmp.py:1935]   Expert 16 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378096.378096 lmp.py:1935]   Expert 18 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378600.378600 lmp.py:1935]   Expert 15 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378673.378673 lmp.py:1935]   Expert  4 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378985.378985 lmp.py:1935]   Expert 50 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378820.378820 lmp.py:1935]   Expert 22 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378655.378655 lmp.py:1935]   Expert 31 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378490.378490 lmp.py:1935]   Expert 45 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378915.378915 lmp.py:1935]   Expert 33 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378081.378081 lmp.py:1935]   Expert 51 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378440.378440 lmp.py:1935]   Expert 49 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378129.378129 lmp.py:1935]   Expert 38 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378341.378341 lmp.py:1935]   Expert 26 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378031.378031 lmp.py:1935]   Expert 10 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378243.378243 lmp.py:1935]   Expert 44 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378217.378217 lmp.py:1935]   Expert  2 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378430.378430 lmp.py:1935]   Expert 24 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378404.378404 lmp.py:1935]   Expert 14 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378378.378378 lmp.py:1935]   Expert 23 |    402 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.378352.378352 lmp.py:1935]   Expert 62 |    676 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.378134.378134 lmp.py:1937] 
DEBUG 01-15 16:09:01.378134.378134 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.378585.378585 lmp.py:1938]   CPU:   2831 tokens
DEBUG 01-15 16:09:01.378035.378035 lmp.py:1942]   cuda:1:   4728 tokens (19 experts)
DEBUG 01-15 16:09:01.378248.378248 lmp.py:1942]   cuda:2:   4729 tokens (20 experts)
DEBUG 01-15 16:09:01.378745.378745 lmp.py:1943]   Total GPU:   9457 tokens
DEBUG 01-15 16:09:01.378527.378527 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.378527.378527 lmp.py:1944] 
DEBUG 01-15 16:09:01.378985.378985 cuda_h.py:19] end experts_map_get cost 0.0017845630645751953 seconds
DEBUG 01-15 16:09:01.378212.378212 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.378584.378584 lmp.py:1953] 
DEBUG 01-15 16:09:01.378584.378584 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.378089.378089 cuda_h.py:19] end cpu_experts_submit cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:09:01.378831.378831 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.378906.378906 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.380492.380492 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.15.gate_proj.weight': 0, 'model.layers.18.mlp.experts.15.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.15.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.16.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.16.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.16.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.20.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.20.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.20.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.24.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.24.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.24.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.26.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.26.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.26.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.31.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.31.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.31.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.33.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.33.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.33.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.36.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.36.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.36.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.39.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.39.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.39.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.42.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.42.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.42.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.43.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.43.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.43.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.44.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.44.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.44.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.18.mlp.experts.1.gate_proj.weight': 0, 'model.layers.18.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.23.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.23.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.23.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.38.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.38.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.38.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.18.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.18.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.18.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(21944074240, 5767168, 0, 0), (21949841408, 5767168, 5767168, 0), (21938307072, 5767168, 11534336, 0), (21961375744, 5767168, 17301504, 0), (21967142912, 5767168, 23068672, 0), (21955608576, 5767168, 28835840, 0), (22030581760, 5767168, 34603008, 0), (22036348928, 5767168, 40370176, 0), (22024814592, 5767168, 46137344, 0), (22099787776, 5767168, 51904512, 0), (22105554944, 5767168, 57671680, 0), (22094020608, 5767168, 63438848, 0), (22134390784, 5767168, 69206016, 0), (22140157952, 5767168, 74973184, 0), (22128623616, 5767168, 80740352, 0), (22220898304, 5767168, 86507520, 0), (22226665472, 5767168, 92274688, 0), (22215131136, 5767168, 98041856, 0), (22255501312, 5767168, 103809024, 0), (22261268480, 5767168, 109576192, 0), (22249734144, 5767168, 115343360, 0), (22307405824, 5767168, 121110528, 0), (22313172992, 5767168, 126877696, 0), (22301638656, 5767168, 132644864, 0), (22359310336, 5767168, 138412032, 0), (22365077504, 5767168, 144179200, 0), (22353543168, 5767168, 149946368, 0), (22411214848, 5767168, 155713536, 0), (22416982016, 5767168, 161480704, 0), (22405447680, 5767168, 167247872, 0), (22428516352, 5767168, 173015040, 0), (22434283520, 5767168, 178782208, 0), (22422749184, 5767168, 184549376, 0), (22445817856, 5767168, 190316544, 0), (22451585024, 5767168, 196083712, 0), (22440050688, 5767168, 201850880, 0), (22497722368, 5767168, 207618048, 0), (22503489536, 5767168, 213385216, 0), (22491955200, 5767168, 219152384, 0), (22515023872, 5767168, 224919552, 0), (22520791040, 5767168, 230686720, 0), (22509256704, 5767168, 236453888, 0), (22532325376, 5767168, 242221056, 0), (22538092544, 5767168, 247988224, 0), (22526558208, 5767168, 253755392, 0), (22549626880, 5767168, 259522560, 0), (22555394048, 5767168, 265289728, 0), (22543859712, 5767168, 271056896, 0), (22670737408, 5767168, 276824064, 0), (22676504576, 5767168, 282591232, 0), (22664970240, 5767168, 288358400, 0), (22705340416, 5767168, 294125568, 0), (22711107584, 5767168, 299892736, 0), (22699573248, 5767168, 305659904, 0), (22757244928, 5767168, 311427072, 0), (22763012096, 5767168, 317194240, 0), (22751477760, 5767168, 322961408, 0)], 2: [(21701853184, 5767168, 0, 0), (21707620352, 5767168, 5767168, 0), (21696086016, 5767168, 11534336, 0), (21719154688, 5767168, 17301504, 0), (21724921856, 5767168, 23068672, 0), (21713387520, 5767168, 28835840, 0), (21753757696, 5767168, 34603008, 0), (21759524864, 5767168, 40370176, 0), (21747990528, 5767168, 46137344, 0), (21805662208, 5767168, 51904512, 0), (21811429376, 5767168, 57671680, 0), (21799895040, 5767168, 63438848, 0), (21840265216, 5767168, 69206016, 0), (21846032384, 5767168, 74973184, 0), (21834498048, 5767168, 80740352, 0), (21857566720, 5767168, 86507520, 0), (21863333888, 5767168, 92274688, 0), (21851799552, 5767168, 98041856, 0), (21874868224, 5767168, 103809024, 0), (21880635392, 5767168, 109576192, 0), (21869101056, 5767168, 115343360, 0), (21909471232, 5767168, 121110528, 0), (21915238400, 5767168, 126877696, 0), (21903704064, 5767168, 132644864, 0), (21926772736, 5767168, 138412032, 0), (21932539904, 5767168, 144179200, 0), (21921005568, 5767168, 149946368, 0), (21995978752, 5767168, 155713536, 0), (22001745920, 5767168, 161480704, 0), (21990211584, 5767168, 167247872, 0), (22065184768, 5767168, 173015040, 0), (22070951936, 5767168, 178782208, 0), (22059417600, 5767168, 184549376, 0), (22082486272, 5767168, 190316544, 0), (22088253440, 5767168, 196083712, 0), (22076719104, 5767168, 201850880, 0), (22272802816, 5767168, 207618048, 0), (22278569984, 5767168, 213385216, 0), (22267035648, 5767168, 219152384, 0), (22342008832, 5767168, 224919552, 0), (22347776000, 5767168, 230686720, 0), (22336241664, 5767168, 236453888, 0), (22463119360, 5767168, 242221056, 0), (22468886528, 5767168, 247988224, 0), (22457352192, 5767168, 253755392, 0), (22566928384, 5767168, 259522560, 0), (22572695552, 5767168, 265289728, 0), (22561161216, 5767168, 271056896, 0), (22601531392, 5767168, 276824064, 0), (22607298560, 5767168, 282591232, 0), (22595764224, 5767168, 288358400, 0), (22636134400, 5767168, 294125568, 0), (22641901568, 5767168, 299892736, 0), (22630367232, 5767168, 305659904, 0), (22739943424, 5767168, 311427072, 0), (22745710592, 5767168, 317194240, 0), (22734176256, 5767168, 322961408, 0), (22774546432, 5767168, 328728576, 0), (22780313600, 5767168, 334495744, 0), (22768779264, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b34ee59ec0>, 2: <capsule object NULL at 0x74a6bc4fe2b0>}
DEBUG 01-15 16:09:01.380166.380166 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.380307.380307 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.380448.380448 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0b2bb2b6-e96f-476f-85be-33e900fee040
DEBUG 01-15 16:09:01.380429.380429 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:01.381397.381397 client.py:127] Model loaded
DEBUG 01-15 16:09:01.381111.381111 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.382295.382295 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.382232.382232 cuda_h.py:19] end restore2model cost 0.0011501312255859375 seconds
DEBUG 01-15 16:09:01.382466.382466 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:01.383840.383840 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0b2bb2b6-e96f-476f-85be-33e900fee040
DEBUG 01-15 16:09:01.383375.383375 cuda_h.py:19] end sllm_worker_task cost 0.013501644134521484 seconds
DEBUG 01-15 16:09:01.383255.383255 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00484466552734375 seconds
DEBUG 01-15 16:09:01.384621.384621 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.384403.384403 cuda_h.py:19] end move_flatidxs cost 0.0009436607360839844 seconds
DEBUG 01-15 16:09:01.384067.384067 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.387339.387339 cuda_h.py:19] end restore2model cost 0.0030181407928466797 seconds
DEBUG 01-15 16:09:01.387705.387705 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008277654647827148 seconds
DEBUG 01-15 16:09:01.387547.387547 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.387459.387459 cuda_h.py:19] end gpu_sexperts cost 0.00028896331787109375 seconds
DEBUG 01-15 16:09:01.387003.387003 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.387786.387786 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2649765014648438e-05 seconds
DEBUG 01-15 16:09:01.387767.387767 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.387424.387424 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.388900.388900 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009515285491943359 seconds
DEBUG 01-15 16:09:01.388373.388373 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.388010.388010 cuda_h.py:19] end gpu_group_list cost 0.00019431114196777344 seconds
DEBUG 01-15 16:09:01.389747.389747 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.389964.389964 cuda_h.py:19] end group_tensors cost 0.005555868148803711 seconds
DEBUG 01-15 16:09:01.390341.390341 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.391591.391591 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001226186752319336 seconds
DEBUG 01-15 16:09:01.391417.391417 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.391322.391322 cuda_h.py:19] end gpu_group_list cost 0.00031566619873046875 seconds
DEBUG 01-15 16:09:01.392098.392098 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.392623.392623 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0b2bb2b6-e96f-476f-85be-33e900fee040
DEBUG 01-15 16:09:01.393123.393123 cuda_h.py:19] end group pad cost 0.003328561782836914 seconds
DEBUG 01-15 16:09:01.393873.393873 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:01.422078.422078 cuda_h.py:19] end group_einsum cost 0.027960538864135742 seconds
DEBUG 01-15 16:09:01.422911.422911 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:01.423914.423914 client.py:127] Model loaded
DEBUG 01-15 16:09:01.423959.423959 cuda_h.py:19] end wait_experts_multi_device cost 0.030400991439819336 seconds
DEBUG 01-15 16:09:01.423497.423497 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.425551.425551 cuda_h.py:19] end get_outputs_cpu1 cost 0.003039121627807617 seconds
DEBUG 01-15 16:09:01.426930.426930 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04369473457336426 seconds
DEBUG 01-15 16:09:01.426939.426939 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.003615856170654297 seconds
DEBUG 01-15 16:09:01.427998.427998 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.429563.429563 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.429871.429871 cuda_h.py:19] end index_scatter cost 0.0001494884490966797 seconds
DEBUG 01-15 16:09:01.430531.430531 cuda_h.py:19] end cpuoutputsdeal cost 0.002961874008178711 seconds
DEBUG 01-15 16:09:01.430802.430802 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.430540.430540 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.430639.430639 cuda_h.py:19] end gpu_group_tensor cost 0.00030517578125 seconds
DEBUG 01-15 16:09:01.430569.430569 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.431396.431396 cuda_h.py:19] end gpu_group_tensor cost 0.00028443336486816406 seconds
DEBUG 01-15 16:09:01.431463.431463 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.432884.432884 cuda_h.py:19] end gpu_group_einsum cost 0.0010046958923339844 seconds
DEBUG 01-15 16:09:01.432509.432509 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.434348.434348 cuda_h.py:19] end gpu_group_einsum cost 0.0010268688201904297 seconds
DEBUG 01-15 16:09:01.434328.434328 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.434915.434915 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.434644.434644 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017952919006347656 seconds
DEBUG 01-15 16:09:01.434500.434500 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.434874.434874 cuda_h.py:19] end concat_expert_out cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:09:01.434293.434293 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.434661.434661 cuda_h.py:19] end index_scatter cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:09:01.435072.435072 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008080005645751953 seconds
DEBUG 01-15 16:09:01.435162.435162 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.435959.435959 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.435991.435991 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012254714965820312 seconds
DEBUG 01-15 16:09:01.435886.435886 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.435994.435994 cuda_h.py:19] end concat_expert_out cost 5.340576171875e-05 seconds
DEBUG 01-15 16:09:01.435261.435261 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.435661.435661 cuda_h.py:19] end index_scatter cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:09:01.435894.435894 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004699230194091797 seconds
DEBUG 01-15 16:09:01.435480.435480 cuda_h.py:19] end gpu_experts_multi_device cost 0.04815101623535156 seconds
DEBUG 01-15 16:09:01.435528.435528 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.05973219871520996 seconds
DEBUG 01-15 16:09:01.436668.436668 cuda_h.py:19] end prefill_layer cost 0.06721305847167969 seconds
DEBUG 01-15 16:09:01.436950.436950 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:09:01.436222.436222 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.436587.436587 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:09:01.436191.436191 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:09:01.436893.436893 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:09:01.436207.436207 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 3.147125244140625e-05 seconds
DEBUG 01-15 16:09:01.436625.436625 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 6.103515625e-05 seconds
DEBUG 01-15 16:09:01.436221.436221 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.436370.436370 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.436937.436937 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.437065.437065 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.437071.437071 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.437248.437248 cuda_h.py:19] end allocate_cuda_memory cost 0.0005850791931152344 seconds
DEBUG 01-15 16:09:01.438466.438466 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.438053.438053 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.438370.438370 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.438945.438945 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b304588e-3cd7-4257-8716-c46b31d0a314
DEBUG 01-15 16:09:01.438399.438399 cuda_h.py:10] start self_attn
DEBUG 01-15 16:09:01.438492.438492 client.py:106] call stub.LoadModelAsync
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
INFO 01-15 16:09:01.441726.441726 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b304588e-3cd7-4257-8716-c46b31d0a314
DEBUG 01-15 16:09:01.441341.441341 cuda_h.py:19] end load_into_gpu_async cost 0.0030524730682373047 seconds
DEBUG 01-15 16:09:01.441212.441212 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.441871.441871 cuda_h.py:19] end restore_tensors2 cost 0.00016546249389648438 seconds
DEBUG 01-15 16:09:01.441855.441855 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0046672821044921875 seconds
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
INFO 01-15 16:09:01.442466.442466 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b304588e-3cd7-4257-8716-c46b31d0a314
DEBUG 01-15 16:09:01.442251.442251 cuda_h.py:19] end self_attn cost 0.0035614967346191406 seconds
DEBUG 01-15 16:09:01.442076.442076 cuda_h.py:19] end iln_self_attn_paln cost 0.006323337554931641 seconds
DEBUG 01-15 16:09:01.443151.443151 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:09:01.443483.443483 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.443539.443539 cuda_h.py:19] end gate cost 0.0006365776062011719 seconds
DEBUG 01-15 16:09:01.443567.443567 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.444824.444824 lmp.py:1912] 
DEBUG 01-15 16:09:01.444824.444824 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.444295.444295 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.444183.444183 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.444495.444495 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.444184.444184 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.444920.444920 lmp.py:1917] 
DEBUG 01-15 16:09:01.444920.444920 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.444324.444324 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.444497.444497 lmp.py:1935]   Expert 44 |     40 | CPU
DEBUG 01-15 16:09:01.444425.444425 lmp.py:1935]   Expert  1 |     47 | CPU
DEBUG 01-15 16:09:01.444161.444161 lmp.py:1935]   Expert 60 |     60 | CPU
DEBUG 01-15 16:09:01.444896.444896 lmp.py:1935]   Expert 28 |     71 | CPU
DEBUG 01-15 16:09:01.444632.444632 lmp.py:1935]   Expert 48 |     78 | CPU
DEBUG 01-15 16:09:01.444129.444129 lmp.py:1935]   Expert 27 |     85 | CPU
DEBUG 01-15 16:09:01.444103.444103 lmp.py:1935]   Expert  0 |    101 | CPU
DEBUG 01-15 16:09:01.444554.444554 lmp.py:1935]   Expert 62 |    107 | CPU
DEBUG 01-15 16:09:01.444588.444588 lmp.py:1935]   Expert 22 |    111 | CPU
DEBUG 01-15 16:09:01.444774.444774 lmp.py:1935]   Expert 42 |    113 | CPU
DEBUG 01-15 16:09:01.444225.444225 lmp.py:1935]   Expert 30 |    114 | CPU
DEBUG 01-15 16:09:01.444199.444199 lmp.py:1935]   Expert 59 |    119 | CPU
DEBUG 01-15 16:09:01.444173.444173 lmp.py:1935]   Expert 58 |    124 | CPU
DEBUG 01-15 16:09:01.444909.444909 lmp.py:1935]   Expert  8 |    125 | CPU
DEBUG 01-15 16:09:01.444122.444122 lmp.py:1935]   Expert 16 |    128 | CPU
DEBUG 01-15 16:09:01.444096.444096 lmp.py:1935]   Expert 12 |    130 | CPU
DEBUG 01-15 16:09:01.444500.444500 lmp.py:1935]   Expert 50 |    136 | CPU
DEBUG 01-15 16:09:01.444951.444951 lmp.py:1935]   Expert  5 |    144 | CPU
DEBUG 01-15 16:09:01.444978.444978 lmp.py:1935]   Expert 56 |    144 | CPU
DEBUG 01-15 16:09:01.444052.444052 lmp.py:1935]   Expert 57 |    150 | CPU
DEBUG 01-15 16:09:01.444887.444887 lmp.py:1935]   Expert 55 |    151 | CPU
DEBUG 01-15 16:09:01.444245.444245 lmp.py:1935]   Expert 15 |    152 | CPU
DEBUG 01-15 16:09:01.444842.444842 lmp.py:1935]   Expert 26 |    154 | CPU
DEBUG 01-15 16:09:01.444438.444438 lmp.py:1935]   Expert 32 |    158 | CPU
DEBUG 01-15 16:09:01.444750.444750 lmp.py:1935]   Expert 47 |    159 | CPU
DEBUG 01-15 16:09:01.444969.444969 lmp.py:1935]   Expert 34 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.444427.444427 lmp.py:1935]   Expert 24 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.444693.444693 lmp.py:1935]   Expert 52 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.444197.444197 lmp.py:1935]   Expert  2 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.444462.444462 lmp.py:1935]   Expert 40 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.444251.444251 lmp.py:1935]   Expert 18 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.444040.444040 lmp.py:1935]   Expert  6 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.444544.444544 lmp.py:1935]   Expert 13 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.444048.444048 lmp.py:1935]   Expert 41 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.444598.444598 lmp.py:1935]   Expert  3 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.444387.444387 lmp.py:1935]   Expert 54 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.444937.444937 lmp.py:1935]   Expert 19 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.444011.444011 lmp.py:1935]   Expert 37 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.444323.444323 lmp.py:1935]   Expert 20 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.444396.444396 lmp.py:1935]   Expert 46 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445708.445708 lmp.py:1935]   Expert 25 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445735.445735 lmp.py:1935]   Expert 51 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445524.445524 lmp.py:1935]   Expert 17 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445836.445836 lmp.py:1935]   Expert 43 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445909.445909 lmp.py:1935]   Expert 11 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445221.445221 lmp.py:1935]   Expert 31 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445771.445771 lmp.py:1935]   Expert 35 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445083.445083 lmp.py:1935]   Expert 23 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445634.445634 lmp.py:1935]   Expert 49 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445946.445946 lmp.py:1935]   Expert 39 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445450.445450 lmp.py:1935]   Expert 53 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445000.445000 lmp.py:1935]   Expert 10 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445312.445312 lmp.py:1935]   Expert 33 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445862.445862 lmp.py:1935]   Expert 36 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445174.445174 lmp.py:1935]   Expert 38 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445724.445724 lmp.py:1935]   Expert  4 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445036.445036 lmp.py:1935]   Expert 21 |    334 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445587.445587 lmp.py:1935]   Expert 14 |    347 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445852.445852 lmp.py:1935]   Expert 63 |    369 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445879.445879 lmp.py:1935]   Expert 45 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445668.445668 lmp.py:1935]   Expert  9 |    389 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445503.445503 lmp.py:1935]   Expert 61 |    391 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445815.445815 lmp.py:1935]   Expert 29 |    489 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.445472.445472 lmp.py:1935]   Expert  7 |    515 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.445545.445545 lmp.py:1937] 
DEBUG 01-15 16:09:01.445545.445545 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.445811.445811 lmp.py:1938]   CPU:   2901 tokens
DEBUG 01-15 16:09:01.445599.445599 lmp.py:1942]   cuda:1:   4615 tokens (19 experts)
DEBUG 01-15 16:09:01.445673.445673 lmp.py:1942]   cuda:2:   4772 tokens (20 experts)
DEBUG 01-15 16:09:01.445508.445508 lmp.py:1943]   Total GPU:   9387 tokens
DEBUG 01-15 16:09:01.445389.445389 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.445389.445389 lmp.py:1944] 
DEBUG 01-15 16:09:01.445675.445675 cuda_h.py:19] end experts_map_get cost 0.0018067359924316406 seconds
DEBUG 01-15 16:09:01.445717.445717 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.445043.445043 lmp.py:1953] 
DEBUG 01-15 16:09:01.445043.445043 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.445740.445740 cuda_h.py:19] end cpu_experts_submit cost 5.6743621826171875e-05 seconds
DEBUG 01-15 16:09:01.445244.445244 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.445749.445749 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.447966.447966 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.3.gate_proj.weight': 0, 'model.layers.19.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.35.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.35.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.35.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.36.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.36.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.36.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.19.mlp.experts.2.gate_proj.weight': 0, 'model.layers.19.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.31.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.31.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.31.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.33.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.33.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.33.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.34.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.34.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.34.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.54.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.54.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.54.up_proj.weight': 322961408, 'model.layers.19.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.19.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.19.mlp.experts.61.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(22843752448, 5767168, 0, 0), (22849519616, 5767168, 5767168, 0), (22837985280, 5767168, 11534336, 0), (22912958464, 5767168, 17301504, 0), (22918725632, 5767168, 23068672, 0), (22907191296, 5767168, 28835840, 0), (22947561472, 5767168, 34603008, 0), (22953328640, 5767168, 40370176, 0), (22941794304, 5767168, 46137344, 0), (22964862976, 5767168, 51904512, 0), (22970630144, 5767168, 57671680, 0), (22959095808, 5767168, 63438848, 0), (23016767488, 5767168, 69206016, 0), (23022534656, 5767168, 74973184, 0), (23011000320, 5767168, 80740352, 0), (23155179520, 5767168, 86507520, 0), (23160946688, 5767168, 92274688, 0), (23149412352, 5767168, 98041856, 0), (23189782528, 5767168, 103809024, 0), (23195549696, 5767168, 109576192, 0), (23184015360, 5767168, 115343360, 0), (23397400576, 5767168, 121110528, 0), (23403167744, 5767168, 126877696, 0), (23391633408, 5767168, 132644864, 0), (23414702080, 5767168, 138412032, 0), (23420469248, 5767168, 144179200, 0), (23408934912, 5767168, 149946368, 0), (23432003584, 5767168, 155713536, 0), (23437770752, 5767168, 161480704, 0), (23426236416, 5767168, 167247872, 0), (23449305088, 5767168, 173015040, 0), (23455072256, 5767168, 178782208, 0), (23443537920, 5767168, 184549376, 0), (23466606592, 5767168, 190316544, 0), (23472373760, 5767168, 196083712, 0), (23460839424, 5767168, 201850880, 0), (23483908096, 5767168, 207618048, 0), (23489675264, 5767168, 213385216, 0), (23478140928, 5767168, 219152384, 0), (23501209600, 5767168, 224919552, 0), (23506976768, 5767168, 230686720, 0), (23495442432, 5767168, 236453888, 0), (23535812608, 5767168, 242221056, 0), (23541579776, 5767168, 247988224, 0), (23530045440, 5767168, 253755392, 0), (23587717120, 5767168, 259522560, 0), (23593484288, 5767168, 265289728, 0), (23581949952, 5767168, 271056896, 0), (23674224640, 5767168, 276824064, 0), (23679991808, 5767168, 282591232, 0), (23668457472, 5767168, 288358400, 0), (23691526144, 5767168, 294125568, 0), (23697293312, 5767168, 299892736, 0), (23685758976, 5767168, 305659904, 0), (23881842688, 5767168, 311427072, 0), (23887609856, 5767168, 317194240, 0), (23876075520, 5767168, 322961408, 0)], 2: [(22826450944, 5767168, 0, 0), (22832218112, 5767168, 5767168, 0), (22820683776, 5767168, 11534336, 0), (22861053952, 5767168, 17301504, 0), (22866821120, 5767168, 23068672, 0), (22855286784, 5767168, 28835840, 0), (22895656960, 5767168, 34603008, 0), (22901424128, 5767168, 40370176, 0), (22889889792, 5767168, 46137344, 0), (22982164480, 5767168, 51904512, 0), (22987931648, 5767168, 57671680, 0), (22976397312, 5767168, 63438848, 0), (23034068992, 5767168, 69206016, 0), (23039836160, 5767168, 74973184, 0), (23028301824, 5767168, 80740352, 0), (23085973504, 5767168, 86507520, 0), (23091740672, 5767168, 92274688, 0), (23080206336, 5767168, 98041856, 0), (23103275008, 5767168, 103809024, 0), (23109042176, 5767168, 109576192, 0), (23097507840, 5767168, 115343360, 0), (23120576512, 5767168, 121110528, 0), (23126343680, 5767168, 126877696, 0), (23114809344, 5767168, 132644864, 0), (23137878016, 5767168, 138412032, 0), (23143645184, 5767168, 144179200, 0), (23132110848, 5767168, 149946368, 0), (23207084032, 5767168, 155713536, 0), (23212851200, 5767168, 161480704, 0), (23201316864, 5767168, 167247872, 0), (23224385536, 5767168, 173015040, 0), (23230152704, 5767168, 178782208, 0), (23218618368, 5767168, 184549376, 0), (23293591552, 5767168, 190316544, 0), (23299358720, 5767168, 196083712, 0), (23287824384, 5767168, 201850880, 0), (23328194560, 5767168, 207618048, 0), (23333961728, 5767168, 213385216, 0), (23322427392, 5767168, 219152384, 0), (23362797568, 5767168, 224919552, 0), (23368564736, 5767168, 230686720, 0), (23357030400, 5767168, 236453888, 0), (23380099072, 5767168, 242221056, 0), (23385866240, 5767168, 247988224, 0), (23374331904, 5767168, 253755392, 0), (23570415616, 5767168, 259522560, 0), (23576182784, 5767168, 265289728, 0), (23564648448, 5767168, 271056896, 0), (23639621632, 5767168, 276824064, 0), (23645388800, 5767168, 282591232, 0), (23633854464, 5767168, 288358400, 0), (23708827648, 5767168, 294125568, 0), (23714594816, 5767168, 299892736, 0), (23703060480, 5767168, 305659904, 0), (23726129152, 5767168, 311427072, 0), (23731896320, 5767168, 317194240, 0), (23720361984, 5767168, 322961408, 0), (23847239680, 5767168, 328728576, 0), (23853006848, 5767168, 334495744, 0), (23841472512, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc4fdcb0>, 2: <capsule object NULL at 0x74a6bc4fdf20>}
DEBUG 01-15 16:09:01.447507.447507 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.447681.447681 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.448537.448537 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2ff11458-460f-47b7-a5c7-904878a4889b
DEBUG 01-15 16:09:01.448671.448671 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:01.449502.449502 client.py:127] Model loaded
DEBUG 01-15 16:09:01.449539.449539 cuda_h.py:10] start restore2model
INFO 01-15 16:09:01.449139.449139 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2ff11458-460f-47b7-a5c7-904878a4889b
DEBUG 01-15 16:09:01.450955.450955 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.450459.450459 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004338979721069336 seconds
DEBUG 01-15 16:09:01.450185.450185 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.450847.450847 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.451962.451962 cuda_h.py:19] end move_flatidxs cost 0.0011229515075683594 seconds
DEBUG 01-15 16:09:01.451341.451341 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.451014.451014 cuda_h.py:19] end restore2model cost 0.0015702247619628906 seconds
DEBUG 01-15 16:09:01.452854.452854 cuda_h.py:19] end sllm_worker_task cost 0.015073776245117188 seconds
DEBUG 01-15 16:09:01.455205.455205 cuda_h.py:19] end restore2model cost 0.0047037601470947266 seconds
DEBUG 01-15 16:09:01.455116.455116 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.009397268295288086 seconds
DEBUG 01-15 16:09:01.455342.455342 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.455571.455571 cuda_h.py:19] end gpu_sexperts cost 0.0002779960632324219 seconds
DEBUG 01-15 16:09:01.455831.455831 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.455568.455568 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.3365020751953125e-05 seconds
DEBUG 01-15 16:09:01.455787.455787 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.455251.455251 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.456271.456271 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009665489196777344 seconds
DEBUG 01-15 16:09:01.456890.456890 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.456480.456480 cuda_h.py:19] end gpu_group_list cost 0.0001952648162841797 seconds
DEBUG 01-15 16:09:01.457561.457561 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.458277.458277 cuda_h.py:19] end group_tensors cost 0.006325721740722656 seconds
DEBUG 01-15 16:09:01.458664.458664 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010340213775634766 seconds
DEBUG 01-15 16:09:01.458812.458812 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.459919.459919 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.459204.459204 cuda_h.py:19] end gpu_group_list cost 0.00020623207092285156 seconds
DEBUG 01-15 16:09:01.460747.460747 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.460696.460696 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2ff11458-460f-47b7-a5c7-904878a4889b
DEBUG 01-15 16:09:01.462333.462333 cuda_h.py:19] end group pad cost 0.003193378448486328 seconds
DEBUG 01-15 16:09:01.462891.462891 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:01.490974.490974 client.py:127] Model loaded
DEBUG 01-15 16:09:01.490820.490820 cuda_h.py:19] end group_einsum cost 0.028026580810546875 seconds
DEBUG 01-15 16:09:01.490866.490866 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.490409.490409 cuda_h.py:19] end wait_experts_multi_device cost 0.030397415161132812 seconds
DEBUG 01-15 16:09:01.490292.490292 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.493874.493874 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029757022857666016 seconds
DEBUG 01-15 16:09:01.494410.494410 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04450082778930664 seconds
DEBUG 01-15 16:09:01.495293.495293 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.004685878753662109 seconds
DEBUG 01-15 16:09:01.495544.495544 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.497203.497203 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.498147.498147 cuda_h.py:19] end index_scatter cost 0.00015497207641601562 seconds
DEBUG 01-15 16:09:01.498078.498078 cuda_h.py:19] end cpuoutputsdeal cost 0.0029871463775634766 seconds
DEBUG 01-15 16:09:01.498786.498786 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.499001.499001 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.499054.499054 cuda_h.py:19] end gpu_group_tensor cost 0.0003082752227783203 seconds
DEBUG 01-15 16:09:01.499891.499891 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.499572.499572 cuda_h.py:19] end gpu_group_tensor cost 0.00028252601623535156 seconds
DEBUG 01-15 16:09:01.500924.500924 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.501253.501253 cuda_h.py:19] end gpu_group_einsum cost 0.0010089874267578125 seconds
DEBUG 01-15 16:09:01.501215.501215 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.502258.502258 cuda_h.py:19] end gpu_group_einsum cost 0.0009644031524658203 seconds
DEBUG 01-15 16:09:01.502074.502074 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.503095.503095 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.503485.503485 cuda_h.py:19] end all_expert_outputs_slices cost 0.00045609474182128906 seconds
DEBUG 01-15 16:09:01.503786.503786 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.503766.503766 cuda_h.py:19] end concat_expert_out cost 0.00011944770812988281 seconds
DEBUG 01-15 16:09:01.504916.504916 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.504301.504301 cuda_h.py:19] end index_scatter cost 0.00012564659118652344 seconds
DEBUG 01-15 16:09:01.504325.504325 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001825094223022461 seconds
DEBUG 01-15 16:09:01.504325.504325 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.505045.505045 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.505291.505291 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003452301025390625 seconds
DEBUG 01-15 16:09:01.505678.505678 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.505857.505857 cuda_h.py:19] end concat_expert_out cost 0.00011992454528808594 seconds
DEBUG 01-15 16:09:01.505239.505239 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.506061.506061 cuda_h.py:19] end index_scatter cost 0.0001163482666015625 seconds
DEBUG 01-15 16:09:01.506521.506521 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012102127075195312 seconds
DEBUG 01-15 16:09:01.506706.506706 cuda_h.py:19] end gpu_experts_multi_device cost 0.050749778747558594 seconds
DEBUG 01-15 16:09:01.506149.506149 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.06357812881469727 seconds
DEBUG 01-15 16:09:01.507129.507129 cuda_h.py:19] end prefill_layer cost 0.07095575332641602 seconds
DEBUG 01-15 16:09:01.507770.507770 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:09:01.507958.507958 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.507099.507099 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:09:01.507625.507625 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:09:01.507919.507919 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:09:01.508268.508268 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:09:01.508662.508662 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 0.0001652240753173828 seconds
DEBUG 01-15 16:09:01.508319.508319 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.508895.508895 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.508221.508221 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.508596.508596 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.509197.509197 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.509657.509657 cuda_h.py:19] end allocate_cuda_memory cost 0.0005776882171630859 seconds
DEBUG 01-15 16:09:01.509782.509782 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.510004.510004 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.510743.510743 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.510567.510567 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b9bdc0b1-4dbb-4d00-8f29-80b140b2de54
DEBUG 01-15 16:09:01.510680.510680 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.510050.510050 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.512550.512550 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b9bdc0b1-4dbb-4d00-8f29-80b140b2de54
DEBUG 01-15 16:09:01.512100.512100 cuda_h.py:19] end load_into_gpu_async cost 0.0022764205932617188 seconds
DEBUG 01-15 16:09:01.512738.512738 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.517360.517360 cuda_h.py:19] end restore_tensors2 cost 0.00012826919555664062 seconds
DEBUG 01-15 16:09:01.517748.517748 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.008405447006225586 seconds
INFO 01-15 16:09:01.517884.517884 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b9bdc0b1-4dbb-4d00-8f29-80b140b2de54
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
INFO 01-15 16:09:01.519083.519083 client.py:127] Model loaded
DEBUG 01-15 16:09:01.519820.519820 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.520420.520420 cuda_h.py:19] end restore2model cost 0.0004794597625732422 seconds
DEBUG 01-15 16:09:01.520939.520939 cuda_h.py:19] end sllm_worker_task cost 0.011672496795654297 seconds
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.521973.521973 cuda_h.py:19] end self_attn cost 0.010245800018310547 seconds
DEBUG 01-15 16:09:01.521200.521200 cuda_h.py:19] end iln_self_attn_paln cost 0.013571977615356445 seconds
DEBUG 01-15 16:09:01.521559.521559 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:09:01.521753.521753 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.522613.522613 cuda_h.py:19] end gate cost 0.0007350444793701172 seconds
DEBUG 01-15 16:09:01.522257.522257 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.523164.523164 lmp.py:1912] 
DEBUG 01-15 16:09:01.523164.523164 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.523403.523403 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.523106.523106 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.523756.523756 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.523783.523783 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.523857.523857 lmp.py:1917] 
DEBUG 01-15 16:09:01.523857.523857 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.523407.523407 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.523156.523156 lmp.py:1935]   Expert 54 |     22 | CPU
DEBUG 01-15 16:09:01.523137.523137 lmp.py:1935]   Expert  3 |     33 | CPU
DEBUG 01-15 16:09:01.523879.523879 lmp.py:1935]   Expert  8 |     40 | CPU
DEBUG 01-15 16:09:01.523191.523191 lmp.py:1935]   Expert 28 |     43 | CPU
DEBUG 01-15 16:09:01.523788.523788 lmp.py:1935]   Expert 63 |     55 | CPU
DEBUG 01-15 16:09:01.523623.523623 lmp.py:1935]   Expert 43 |     56 | CPU
DEBUG 01-15 16:09:01.523220.523220 lmp.py:1935]   Expert 36 |     71 | CPU
DEBUG 01-15 16:09:01.523578.523578 lmp.py:1935]   Expert 38 |     78 | CPU
DEBUG 01-15 16:09:01.523174.523174 lmp.py:1935]   Expert  6 |     80 | CPU
DEBUG 01-15 16:09:01.523771.523771 lmp.py:1935]   Expert 39 |     93 | CPU
DEBUG 01-15 16:09:01.523129.523129 lmp.py:1935]   Expert 57 |     97 | CPU
DEBUG 01-15 16:09:01.523395.523395 lmp.py:1935]   Expert 41 |    104 | CPU
DEBUG 01-15 16:09:01.523184.523184 lmp.py:1935]   Expert 12 |    110 | CPU
DEBUG 01-15 16:09:01.523780.523780 lmp.py:1935]   Expert 52 |    111 | CPU
DEBUG 01-15 16:09:01.523615.523615 lmp.py:1935]   Expert 19 |    120 | CPU
DEBUG 01-15 16:09:01.523974.523974 lmp.py:1935]   Expert 47 |    128 | CPU
DEBUG 01-15 16:09:01.523332.523332 lmp.py:1935]   Expert 13 |    132 | CPU
DEBUG 01-15 16:09:01.523452.523452 lmp.py:1935]   Expert 22 |    142 | CPU
DEBUG 01-15 16:09:01.523810.523810 lmp.py:1935]   Expert 46 |    145 | CPU
DEBUG 01-15 16:09:01.523930.523930 lmp.py:1935]   Expert 50 |    155 | CPU
DEBUG 01-15 16:09:01.523765.523765 lmp.py:1935]   Expert 40 |    163 | CPU
DEBUG 01-15 16:09:01.523600.523600 lmp.py:1935]   Expert 24 |    165 | CPU
DEBUG 01-15 16:09:01.523435.523435 lmp.py:1935]   Expert 20 |    166 | CPU
DEBUG 01-15 16:09:01.523555.523555 lmp.py:1935]   Expert 23 |    168 | CPU
DEBUG 01-15 16:09:01.523913.523913 lmp.py:1935]   Expert 55 |    169 | CPU
DEBUG 01-15 16:09:01.523940.523940 lmp.py:1935]   Expert 53 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.523967.523967 lmp.py:1935]   Expert 61 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.523756.523756 lmp.py:1935]   Expert 37 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.523545.523545 lmp.py:1935]   Expert  2 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524718.524718 lmp.py:1935]   Expert 21 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524699.524699 lmp.py:1935]   Expert 42 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524249.524249 lmp.py:1935]   Expert 49 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524561.524561 lmp.py:1935]   Expert 33 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524634.524634 lmp.py:1935]   Expert 18 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524185.524185 lmp.py:1935]   Expert 32 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524735.524735 lmp.py:1935]   Expert 16 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524808.524808 lmp.py:1935]   Expert 30 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524359.524359 lmp.py:1935]   Expert  0 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524671.524671 lmp.py:1935]   Expert  5 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524982.524982 lmp.py:1935]   Expert 14 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524486.524486 lmp.py:1935]   Expert  7 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524514.524514 lmp.py:1935]   Expert 34 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524302.524302 lmp.py:1935]   Expert 31 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524376.524376 lmp.py:1935]   Expert 60 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524688.524688 lmp.py:1935]   Expert 59 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524523.524523 lmp.py:1935]   Expert 62 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524596.524596 lmp.py:1935]   Expert  9 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524431.524431 lmp.py:1935]   Expert 17 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524757.524757 lmp.py:1935]   Expert 10 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524546.524546 lmp.py:1935]   Expert 29 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524096.524096 lmp.py:1935]   Expert  4 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524123.524123 lmp.py:1935]   Expert 15 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524912.524912 lmp.py:1935]   Expert 58 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524462.524462 lmp.py:1935]   Expert 26 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524536.524536 lmp.py:1935]   Expert 51 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524848.524848 lmp.py:1935]   Expert 11 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524067.524067 lmp.py:1935]   Expert 44 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524094.524094 lmp.py:1935]   Expert 56 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524406.524406 lmp.py:1935]   Expert 27 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524718.524718 lmp.py:1935]   Expert  1 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524268.524268 lmp.py:1935]   Expert 45 |    367 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524818.524818 lmp.py:1935]   Expert 25 |    461 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524369.524369 lmp.py:1935]   Expert 35 |    515 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.524681.524681 lmp.py:1935]   Expert 48 |    645 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.524800.524800 lmp.py:1937] 
DEBUG 01-15 16:09:01.524800.524800 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.524636.524636 lmp.py:1938]   CPU:   2646 tokens
DEBUG 01-15 16:09:01.524947.524947 lmp.py:1942]   cuda:1:   4736 tokens (19 experts)
DEBUG 01-15 16:09:01.524021.524021 lmp.py:1942]   cuda:2:   4906 tokens (20 experts)
DEBUG 01-15 16:09:01.524141.524141 lmp.py:1943]   Total GPU:   9642 tokens
DEBUG 01-15 16:09:01.524499.524499 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.524499.524499 lmp.py:1944] 
DEBUG 01-15 16:09:01.524487.524487 cuda_h.py:19] end experts_map_get cost 0.0020685195922851562 seconds
DEBUG 01-15 16:09:01.524019.524019 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.524682.524682 lmp.py:1953] 
DEBUG 01-15 16:09:01.524682.524682 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.524664.524664 cuda_h.py:19] end cpu_experts_submit cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:09:01.525745.525745 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.525323.525323 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.525343.525343 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.20.mlp.experts.1.gate_proj.weight': 0, 'model.layers.20.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.31.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.31.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.31.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.33.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.33.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.33.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.35.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.35.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.35.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.59.up_proj.weight': 322961408, 'model.layers.20.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.20.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.20.mlp.experts.61.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (23933747200, 5767168, 17301504, 0), (23939514368, 5767168, 23068672, 0), (23927980032, 5767168, 28835840, 0), (23968350208, 5767168, 34603008, 0), (23974117376, 5767168, 40370176, 0), (23962583040, 5767168, 46137344, 0), (24072159232, 5767168, 51904512, 0), (24077926400, 5767168, 57671680, 0), (24066392064, 5767168, 63438848, 0), (24175968256, 5767168, 69206016, 0), (24181735424, 5767168, 74973184, 0), (24170201088, 5767168, 80740352, 0), (24193269760, 5767168, 86507520, 0), (24199036928, 5767168, 92274688, 0), (24187502592, 5767168, 98041856, 0), (24210571264, 5767168, 103809024, 0), (24216338432, 5767168, 109576192, 0), (24204804096, 5767168, 115343360, 0), (24366284800, 5767168, 121110528, 0), (24372051968, 5767168, 126877696, 0), (24360517632, 5767168, 132644864, 0), (24452792320, 5767168, 138412032, 0), (24458559488, 5767168, 144179200, 0), (24447025152, 5767168, 149946368, 0), (24487395328, 5767168, 155713536, 0), (24493162496, 5767168, 161480704, 0), (24481628160, 5767168, 167247872, 0), (24539299840, 5767168, 173015040, 0), (24545067008, 5767168, 178782208, 0), (24533532672, 5767168, 184549376, 0), (24625807360, 5767168, 190316544, 0), (24631574528, 5767168, 196083712, 0), (24620040192, 5767168, 201850880, 0), (24677711872, 5767168, 207618048, 0), (24683479040, 5767168, 213385216, 0), (24671944704, 5767168, 219152384, 0), (24729616384, 5767168, 224919552, 0), (24735383552, 5767168, 230686720, 0), (24723849216, 5767168, 236453888, 0), (24781520896, 5767168, 242221056, 0), (24787288064, 5767168, 247988224, 0), (24775753728, 5767168, 253755392, 0), (24868028416, 5767168, 259522560, 0), (24873795584, 5767168, 265289728, 0), (24862261248, 5767168, 271056896, 0), (24902631424, 5767168, 276824064, 0), (24908398592, 5767168, 282591232, 0), (24896864256, 5767168, 288358400, 0), (24937234432, 5767168, 294125568, 0), (24943001600, 5767168, 299892736, 0), (24931467264, 5767168, 305659904, 0), (24971837440, 5767168, 311427072, 0), (24977604608, 5767168, 317194240, 0), (24966070272, 5767168, 322961408, 0)], 2: [(23916445696, 5767168, 0, 0), (23922212864, 5767168, 5767168, 0), (23910678528, 5767168, 11534336, 0), (23985651712, 5767168, 17301504, 0), (23991418880, 5767168, 23068672, 0), (23979884544, 5767168, 28835840, 0), (24020254720, 5767168, 34603008, 0), (24026021888, 5767168, 40370176, 0), (24014487552, 5767168, 46137344, 0), (24054857728, 5767168, 51904512, 0), (24060624896, 5767168, 57671680, 0), (24049090560, 5767168, 63438848, 0), (24089460736, 5767168, 69206016, 0), (24095227904, 5767168, 74973184, 0), (24083693568, 5767168, 80740352, 0), (24141365248, 5767168, 86507520, 0), (24147132416, 5767168, 92274688, 0), (24135598080, 5767168, 98041856, 0), (24158666752, 5767168, 103809024, 0), (24164433920, 5767168, 109576192, 0), (24152899584, 5767168, 115343360, 0), (24262475776, 5767168, 121110528, 0), (24268242944, 5767168, 126877696, 0), (24256708608, 5767168, 132644864, 0), (24331681792, 5767168, 138412032, 0), (24337448960, 5767168, 144179200, 0), (24325914624, 5767168, 149946368, 0), (24348983296, 5767168, 155713536, 0), (24354750464, 5767168, 161480704, 0), (24343216128, 5767168, 167247872, 0), (24400887808, 5767168, 173015040, 0), (24406654976, 5767168, 178782208, 0), (24395120640, 5767168, 184549376, 0), (24418189312, 5767168, 190316544, 0), (24423956480, 5767168, 196083712, 0), (24412422144, 5767168, 201850880, 0), (24435490816, 5767168, 207618048, 0), (24441257984, 5767168, 213385216, 0), (24429723648, 5767168, 219152384, 0), (24470093824, 5767168, 224919552, 0), (24475860992, 5767168, 230686720, 0), (24464326656, 5767168, 236453888, 0), (24504696832, 5767168, 242221056, 0), (24510464000, 5767168, 247988224, 0), (24498929664, 5767168, 253755392, 0), (24660410368, 5767168, 259522560, 0), (24666177536, 5767168, 265289728, 0), (24654643200, 5767168, 271056896, 0), (24746917888, 5767168, 276824064, 0), (24752685056, 5767168, 282591232, 0), (24741150720, 5767168, 288358400, 0), (24816123904, 5767168, 294125568, 0), (24821891072, 5767168, 299892736, 0), (24810356736, 5767168, 305659904, 0), (24919932928, 5767168, 311427072, 0), (24925700096, 5767168, 317194240, 0), (24914165760, 5767168, 322961408, 0), (24954535936, 5767168, 328728576, 0), (24960303104, 5767168, 334495744, 0), (24948768768, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74aa041f1ec0>, 2: <capsule object NULL at 0x74a814571470>}
DEBUG 01-15 16:09:01.526864.526864 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.526632.526632 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.526569.526569 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e2d2490d-993b-4f9f-a111-bcebd4871323
DEBUG 01-15 16:09:01.526247.526247 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.527538.527538 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.527622.527622 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.528372.528372 cuda_h.py:19] end move_flatidxs cost 0.0008838176727294922 seconds
DEBUG 01-15 16:09:01.528712.528712 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:01.529688.529688 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e2d2490d-993b-4f9f-a111-bcebd4871323
DEBUG 01-15 16:09:01.530581.530581 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005262136459350586 seconds
DEBUG 01-15 16:09:01.530448.530448 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.537814.537814 cuda_h.py:19] end group_tensors cost 0.009328842163085938 seconds
DEBUG 01-15 16:09:01.538450.538450 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.540683.540683 cuda_h.py:19] end restore2model cost 0.00927877426147461 seconds
DEBUG 01-15 16:09:01.540014.540014 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.015118598937988281 seconds
DEBUG 01-15 16:09:01.540453.540453 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.541233.541233 cuda_h.py:19] end gpu_sexperts cost 0.0008294582366943359 seconds
DEBUG 01-15 16:09:01.541436.541436 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.541428.541428 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 4.315376281738281e-05 seconds
DEBUG 01-15 16:09:01.541761.541761 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.541307.541307 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.542521.542521 cuda_h.py:19] end group pad cost 0.0038437843322753906 seconds
DEBUG 01-15 16:09:01.542624.542624 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:01.543139.543139 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015740394592285156 seconds
DEBUG 01-15 16:09:01.543466.543466 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.544277.544277 cuda_h.py:19] end gpu_group_list cost 0.0004379749298095703 seconds
DEBUG 01-15 16:09:01.546510.546510 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.549899.549899 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0025157928466796875 seconds
DEBUG 01-15 16:09:01.549167.549167 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.550100.550100 cuda_h.py:19] end gpu_group_list cost 0.0005154609680175781 seconds
DEBUG 01-15 16:09:01.551552.551552 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.551796.551796 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e2d2490d-993b-4f9f-a111-bcebd4871323
INFO 01-15 16:09:01.569771.569771 client.py:127] Model loaded
DEBUG 01-15 16:09:01.569769.569769 cuda_h.py:19] end wait_experts_multi_device cost 0.017578840255737305 seconds
DEBUG 01-15 16:09:01.569830.569830 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.575944.575944 cuda_h.py:19] end group_einsum cost 0.03301262855529785 seconds
DEBUG 01-15 16:09:01.576285.576285 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.579642.579642 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036852359771728516 seconds
DEBUG 01-15 16:09:01.581067.581067 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012141942977905273 seconds
DEBUG 01-15 16:09:01.581688.581688 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.582523.582523 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05538153648376465 seconds
DEBUG 01-15 16:09:01.582837.582837 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.583757.583757 cuda_h.py:19] end index_scatter cost 9.274482727050781e-05 seconds
DEBUG 01-15 16:09:01.583674.583674 cuda_h.py:19] end cpuoutputsdeal cost 0.001523733139038086 seconds
DEBUG 01-15 16:09:01.583710.583710 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.583003.583003 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.583036.583036 cuda_h.py:19] end gpu_group_tensor cost 0.000164031982421875 seconds
DEBUG 01-15 16:09:01.583713.583713 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.583772.583772 cuda_h.py:19] end gpu_group_tensor cost 0.0001506805419921875 seconds
DEBUG 01-15 16:09:01.584512.584512 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.584686.584686 cuda_h.py:19] end gpu_group_einsum cost 0.0005991458892822266 seconds
DEBUG 01-15 16:09:01.584300.584300 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.586406.586406 cuda_h.py:19] end gpu_group_einsum cost 0.0011026859283447266 seconds
DEBUG 01-15 16:09:01.586375.586375 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.586406.586406 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.586534.586534 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022745132446289062 seconds
DEBUG 01-15 16:09:01.586674.586674 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.586539.586539 cuda_h.py:19] end concat_expert_out cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:09:01.587899.587899 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.587805.587805 cuda_h.py:19] end index_scatter cost 0.00010347366333007812 seconds
DEBUG 01-15 16:09:01.587537.587537 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010101795196533203 seconds
DEBUG 01-15 16:09:01.587196.587196 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.587853.587853 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.587098.587098 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014972686767578125 seconds
DEBUG 01-15 16:09:01.587946.587946 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.587062.587062 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:09:01.587474.587474 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.588497.588497 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:01.588591.588591 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005013942718505859 seconds
DEBUG 01-15 16:09:01.588574.588574 cuda_h.py:19] end gpu_experts_multi_device cost 0.04657125473022461 seconds
DEBUG 01-15 16:09:01.588107.588107 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.06634664535522461 seconds
DEBUG 01-15 16:09:01.588979.588979 cuda_h.py:19] end prefill_layer cost 0.08106040954589844 seconds
DEBUG 01-15 16:09:01.588299.588299 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:09:01.588194.588194 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.588566.588566 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:09:01.588176.588176 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:09:01.588217.588217 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:09:01.589073.589073 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 4.315376281738281e-05 seconds
DEBUG 01-15 16:09:01.589313.589313 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 8.153915405273438e-05 seconds
DEBUG 01-15 16:09:01.589870.589870 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.589608.589608 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.589512.589512 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.589352.589352 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.589807.589807 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.590925.590925 cuda_h.py:19] end allocate_cuda_memory cost 0.0004954338073730469 seconds
DEBUG 01-15 16:09:01.590480.590480 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.590841.590841 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.590222.590222 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.590576.590576 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b17d84bc-4e23-481d-9200-efb105e7d940
DEBUG 01-15 16:09:01.590119.590119 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.590512.590512 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.592529.592529 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b17d84bc-4e23-481d-9200-efb105e7d940
DEBUG 01-15 16:09:01.592073.592073 cuda_h.py:19] end load_into_gpu_async cost 0.0023605823516845703 seconds
DEBUG 01-15 16:09:01.592171.592171 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.593395.593395 cuda_h.py:19] end restore_tensors2 cost 0.00021576881408691406 seconds
DEBUG 01-15 16:09:01.593527.593527 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003926753997802734 seconds
INFO 01-15 16:09:01.593760.593760 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b17d84bc-4e23-481d-9200-efb105e7d940
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.595128.595128 cuda_h.py:19] end self_attn cost 0.004155874252319336 seconds
DEBUG 01-15 16:09:01.595853.595853 cuda_h.py:19] end iln_self_attn_paln cost 0.006379842758178711 seconds
DEBUG 01-15 16:09:01.595252.595252 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:09:01.595677.595677 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.596897.596897 cuda_h.py:19] end gate cost 0.0006203651428222656 seconds
DEBUG 01-15 16:09:01.596535.596535 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.596713.596713 lmp.py:1912] 
DEBUG 01-15 16:09:01.596713.596713 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.596906.596906 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.596225.596225 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.596206.596206 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.596279.596279 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.596684.596684 lmp.py:1917] 
DEBUG 01-15 16:09:01.596684.596684 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.596757.596757 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.596791.596791 lmp.py:1935]   Expert 44 |     29 | CPU
DEBUG 01-15 16:09:01.596103.596103 lmp.py:1935]   Expert  9 |     35 | CPU
DEBUG 01-15 16:09:01.596746.596746 lmp.py:1935]   Expert 11 |     36 | CPU
DEBUG 01-15 16:09:01.596866.596866 lmp.py:1935]   Expert 56 |     58 | CPU
DEBUG 01-15 16:09:01.596271.596271 lmp.py:1935]   Expert 54 |     81 | CPU
DEBUG 01-15 16:09:01.596152.596152 lmp.py:1935]   Expert 62 |     90 | CPU
DEBUG 01-15 16:09:01.596556.596556 lmp.py:1935]   Expert  7 |     91 | CPU
DEBUG 01-15 16:09:01.596676.596676 lmp.py:1935]   Expert 47 |     95 | CPU
DEBUG 01-15 16:09:01.596558.596558 lmp.py:1935]   Expert 51 |    102 | CPU
DEBUG 01-15 16:09:01.596201.596201 lmp.py:1935]   Expert 60 |    105 | CPU
DEBUG 01-15 16:09:01.597844.597844 lmp.py:1935]   Expert 52 |    107 | CPU
DEBUG 01-15 16:09:01.597487.597487 lmp.py:1935]   Expert 22 |    109 | CPU
DEBUG 01-15 16:09:01.597891.597891 lmp.py:1935]   Expert 41 |    109 | CPU
DEBUG 01-15 16:09:01.597296.597296 lmp.py:1935]   Expert 53 |    111 | CPU
DEBUG 01-15 16:09:01.597700.597700 lmp.py:1935]   Expert 48 |    126 | CPU
DEBUG 01-15 16:09:01.597343.597343 lmp.py:1935]   Expert  8 |    127 | CPU
DEBUG 01-15 16:09:01.597225.597225 lmp.py:1935]   Expert  1 |    128 | CPU
DEBUG 01-15 16:09:01.597583.597583 lmp.py:1935]   Expert  6 |    128 | CPU
DEBUG 01-15 16:09:01.597226.597226 lmp.py:1935]   Expert 32 |    128 | CPU
DEBUG 01-15 16:09:01.597869.597869 lmp.py:1935]   Expert  2 |    132 | CPU
DEBUG 01-15 16:09:01.597273.597273 lmp.py:1935]   Expert 35 |    137 | CPU
DEBUG 01-15 16:09:01.597155.597155 lmp.py:1935]   Expert 27 |    138 | CPU
DEBUG 01-15 16:09:01.597798.597798 lmp.py:1935]   Expert 23 |    139 | CPU
DEBUG 01-15 16:09:01.597441.597441 lmp.py:1935]   Expert 59 |    143 | CPU
DEBUG 01-15 16:09:01.597845.597845 lmp.py:1935]   Expert 39 |    145 | CPU
DEBUG 01-15 16:09:01.597872.597872 lmp.py:1935]   Expert 26 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597615.597615 lmp.py:1935]   Expert 50 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597642.597642 lmp.py:1935]   Expert 14 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597477.597477 lmp.py:1935]   Expert 46 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597187.597187 lmp.py:1935]   Expert 38 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597260.597260 lmp.py:1935]   Expert 24 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597426.597426 lmp.py:1935]   Expert  4 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597116.597116 lmp.py:1935]   Expert 34 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597520.597520 lmp.py:1935]   Expert  0 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597686.597686 lmp.py:1935]   Expert 49 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597852.597852 lmp.py:1935]   Expert 40 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597019.597019 lmp.py:1935]   Expert  5 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597708.597708 lmp.py:1935]   Expert 63 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597159.597159 lmp.py:1935]   Expert 19 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597325.597325 lmp.py:1935]   Expert 13 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597299.597299 lmp.py:1935]   Expert 29 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597750.597750 lmp.py:1935]   Expert 43 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597439.597439 lmp.py:1935]   Expert 61 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597128.597128 lmp.py:1935]   Expert 57 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597341.597341 lmp.py:1935]   Expert 33 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597030.597030 lmp.py:1935]   Expert 31 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597958.597958 lmp.py:1935]   Expert 16 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597363.597363 lmp.py:1935]   Expert 20 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597529.597529 lmp.py:1935]   Expert  3 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597218.597218 lmp.py:1935]   Expert 37 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597669.597669 lmp.py:1935]   Expert 15 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597120.597120 lmp.py:1935]   Expert 36 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597047.597047 lmp.py:1935]   Expert 18 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597737.597737 lmp.py:1935]   Expert 12 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597188.597188 lmp.py:1935]   Expert 17 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597639.597639 lmp.py:1935]   Expert 28 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597328.597328 lmp.py:1935]   Expert 55 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597779.597779 lmp.py:1935]   Expert 30 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597468.597468 lmp.py:1935]   Expert 25 |    321 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597157.597157 lmp.py:1935]   Expert 58 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597754.597754 lmp.py:1935]   Expert 10 |    360 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597503.597503 lmp.py:1935]   Expert 21 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597928.597928 lmp.py:1935]   Expert 45 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.597048.597048 lmp.py:1935]   Expert 42 |    644 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.597260.597260 lmp.py:1937] 
DEBUG 01-15 16:09:01.597260.597260 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.597950.597950 lmp.py:1938]   CPU:   2629 tokens
DEBUG 01-15 16:09:01.598354.598354 lmp.py:1942]   cuda:1:   4829 tokens (19 experts)
DEBUG 01-15 16:09:01.598282.598282 lmp.py:1942]   cuda:2:   4830 tokens (20 experts)
DEBUG 01-15 16:09:01.598256.598256 lmp.py:1943]   Total GPU:   9659 tokens
DEBUG 01-15 16:09:01.598992.598992 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.598992.598992 lmp.py:1944] 
DEBUG 01-15 16:09:01.598741.598741 cuda_h.py:19] end experts_map_get cost 0.0017726421356201172 seconds
DEBUG 01-15 16:09:01.598498.598498 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.598585.598585 lmp.py:1953] 
DEBUG 01-15 16:09:01.598585.598585 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.598660.598660 cuda_h.py:19] end cpu_experts_submit cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:09:01.598310.598310 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.598523.598523 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.600110.600110 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.3.gate_proj.weight': 0, 'model.layers.21.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.21.mlp.experts.0.gate_proj.weight': 0, 'model.layers.21.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.12.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.12.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.12.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.13.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.13.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.13.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.38.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.38.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.38.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.40.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.40.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.40.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.45.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.45.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.45.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.21.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.21.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.21.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(25058344960, 5767168, 0, 0), (25064112128, 5767168, 5767168, 0), (25052577792, 5767168, 11534336, 0), (25075646464, 5767168, 17301504, 0), (25081413632, 5767168, 23068672, 0), (25069879296, 5767168, 28835840, 0), (25092947968, 5767168, 34603008, 0), (25098715136, 5767168, 40370176, 0), (25087180800, 5767168, 46137344, 0), (25179455488, 5767168, 51904512, 0), (25185222656, 5767168, 57671680, 0), (25173688320, 5767168, 63438848, 0), (25265963008, 5767168, 69206016, 0), (25271730176, 5767168, 74973184, 0), (25260195840, 5767168, 80740352, 0), (25283264512, 5767168, 86507520, 0), (25289031680, 5767168, 92274688, 0), (25277497344, 5767168, 98041856, 0), (25300566016, 5767168, 103809024, 0), (25306333184, 5767168, 109576192, 0), (25294798848, 5767168, 115343360, 0), (25317867520, 5767168, 121110528, 0), (25323634688, 5767168, 126877696, 0), (25312100352, 5767168, 132644864, 0), (25335169024, 5767168, 138412032, 0), (25340936192, 5767168, 144179200, 0), (25329401856, 5767168, 149946368, 0), (25421676544, 5767168, 155713536, 0), (25427443712, 5767168, 161480704, 0), (25415909376, 5767168, 167247872, 0), (25438978048, 5767168, 173015040, 0), (25444745216, 5767168, 178782208, 0), (25433210880, 5767168, 184549376, 0), (25508184064, 5767168, 190316544, 0), (25513951232, 5767168, 196083712, 0), (25502416896, 5767168, 201850880, 0), (25577390080, 5767168, 207618048, 0), (25583157248, 5767168, 213385216, 0), (25571622912, 5767168, 219152384, 0), (25733103616, 5767168, 224919552, 0), (25738870784, 5767168, 230686720, 0), (25727336448, 5767168, 236453888, 0), (25802309632, 5767168, 242221056, 0), (25808076800, 5767168, 247988224, 0), (25796542464, 5767168, 253755392, 0), (25854214144, 5767168, 259522560, 0), (25859981312, 5767168, 265289728, 0), (25848446976, 5767168, 271056896, 0), (25871515648, 5767168, 276824064, 0), (25877282816, 5767168, 282591232, 0), (25865748480, 5767168, 288358400, 0), (25958023168, 5767168, 294125568, 0), (25963790336, 5767168, 299892736, 0), (25952256000, 5767168, 305659904, 0), (26061832192, 5767168, 311427072, 0), (26067599360, 5767168, 317194240, 0), (26056065024, 5767168, 322961408, 0)], 2: [(25006440448, 5767168, 0, 0), (25012207616, 5767168, 5767168, 0), (25000673280, 5767168, 11534336, 0), (25214058496, 5767168, 17301504, 0), (25219825664, 5767168, 23068672, 0), (25208291328, 5767168, 28835840, 0), (25231360000, 5767168, 34603008, 0), (25237127168, 5767168, 40370176, 0), (25225592832, 5767168, 46137344, 0), (25248661504, 5767168, 51904512, 0), (25254428672, 5767168, 57671680, 0), (25242894336, 5767168, 63438848, 0), (25352470528, 5767168, 69206016, 0), (25358237696, 5767168, 74973184, 0), (25346703360, 5767168, 80740352, 0), (25369772032, 5767168, 86507520, 0), (25375539200, 5767168, 92274688, 0), (25364004864, 5767168, 98041856, 0), (25456279552, 5767168, 103809024, 0), (25462046720, 5767168, 109576192, 0), (25450512384, 5767168, 115343360, 0), (25490882560, 5767168, 121110528, 0), (25496649728, 5767168, 126877696, 0), (25485115392, 5767168, 132644864, 0), (25525485568, 5767168, 138412032, 0), (25531252736, 5767168, 144179200, 0), (25519718400, 5767168, 149946368, 0), (25542787072, 5767168, 155713536, 0), (25548554240, 5767168, 161480704, 0), (25537019904, 5767168, 167247872, 0), (25594691584, 5767168, 173015040, 0), (25600458752, 5767168, 178782208, 0), (25588924416, 5767168, 184549376, 0), (25629294592, 5767168, 190316544, 0), (25635061760, 5767168, 196083712, 0), (25623527424, 5767168, 201850880, 0), (25646596096, 5767168, 207618048, 0), (25652363264, 5767168, 213385216, 0), (25640828928, 5767168, 219152384, 0), (25663897600, 5767168, 224919552, 0), (25669664768, 5767168, 230686720, 0), (25658130432, 5767168, 236453888, 0), (25698500608, 5767168, 242221056, 0), (25704267776, 5767168, 247988224, 0), (25692733440, 5767168, 253755392, 0), (25750405120, 5767168, 259522560, 0), (25756172288, 5767168, 265289728, 0), (25744637952, 5767168, 271056896, 0), (25785008128, 5767168, 276824064, 0), (25790775296, 5767168, 282591232, 0), (25779240960, 5767168, 288358400, 0), (25992626176, 5767168, 294125568, 0), (25998393344, 5767168, 299892736, 0), (25986859008, 5767168, 305659904, 0), (26009927680, 5767168, 311427072, 0), (26015694848, 5767168, 317194240, 0), (26004160512, 5767168, 322961408, 0), (26096435200, 5767168, 328728576, 0), (26102202368, 5767168, 334495744, 0), (26090668032, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814571a40>, 2: <capsule object NULL at 0x74a6bc4fe520>}
DEBUG 01-15 16:09:01.600607.600607 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.600125.600125 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.600643.600643 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1c4cc2fa-e416-424a-9efc-7489810f3cc6
DEBUG 01-15 16:09:01.600319.600319 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:01.601084.601084 client.py:127] Model loaded
DEBUG 01-15 16:09:01.601063.601063 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.602693.602693 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.602986.602986 cuda_h.py:19] end restore2model cost 0.0009634494781494141 seconds
DEBUG 01-15 16:09:01.602811.602811 cuda_h.py:19] end sllm_worker_task cost 0.013127565383911133 seconds
INFO 01-15 16:09:01.602362.602362 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1c4cc2fa-e416-424a-9efc-7489810f3cc6
DEBUG 01-15 16:09:01.603399.603399 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.603464.603464 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00491023063659668 seconds
DEBUG 01-15 16:09:01.603003.603003 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.604036.604036 cuda_h.py:19] end move_flatidxs cost 0.0013232231140136719 seconds
DEBUG 01-15 16:09:01.604177.604177 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.606514.606514 cuda_h.py:19] end restore2model cost 0.0029764175415039062 seconds
DEBUG 01-15 16:09:01.606735.606735 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008121728897094727 seconds
DEBUG 01-15 16:09:01.606292.606292 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.606461.606461 cuda_h.py:19] end gpu_sexperts cost 0.00026988983154296875 seconds
DEBUG 01-15 16:09:01.606098.606098 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.606729.606729 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:09:01.606948.606948 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.606128.606128 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.607630.607630 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009377002716064453 seconds
DEBUG 01-15 16:09:01.607188.607188 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.608640.608640 cuda_h.py:19] end gpu_group_list cost 0.0001990795135498047 seconds
DEBUG 01-15 16:09:01.608846.608846 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.610605.610605 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010547637939453125 seconds
DEBUG 01-15 16:09:01.610840.610840 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.610503.610503 cuda_h.py:19] end gpu_group_list cost 0.00021457672119140625 seconds
DEBUG 01-15 16:09:01.611454.611454 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.611760.611760 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1c4cc2fa-e416-424a-9efc-7489810f3cc6
DEBUG 01-15 16:09:01.617546.617546 cuda_h.py:19] end group_tensors cost 0.012932538986206055 seconds
DEBUG 01-15 16:09:01.618392.618392 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.623311.623311 cuda_h.py:19] end group pad cost 0.005200862884521484 seconds
DEBUG 01-15 16:09:01.624365.624365 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:01.637903.637903 client.py:127] Model loaded
DEBUG 01-15 16:09:01.638005.638005 cuda_h.py:19] end wait_experts_multi_device cost 0.026763200759887695 seconds
DEBUG 01-15 16:09:01.638678.638678 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.649147.649147 cuda_h.py:19] end group_einsum cost 0.02577352523803711 seconds
DEBUG 01-15 16:09:01.650636.650636 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.652730.652730 cuda_h.py:19] end get_outputs_cpu1 cost 0.0027494430541992188 seconds
DEBUG 01-15 16:09:01.653138.653138 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05135798454284668 seconds
DEBUG 01-15 16:09:01.654272.654272 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.015991926193237305 seconds
DEBUG 01-15 16:09:01.654045.654045 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.655879.655879 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.656667.656667 cuda_h.py:19] end index_scatter cost 9.608268737792969e-05 seconds
DEBUG 01-15 16:09:01.656691.656691 cuda_h.py:19] end cpuoutputsdeal cost 0.0016558170318603516 seconds
DEBUG 01-15 16:09:01.656676.656676 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.656942.656942 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.656042.656042 cuda_h.py:19] end gpu_group_tensor cost 0.00017333030700683594 seconds
DEBUG 01-15 16:09:01.656534.656534 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.656613.656613 cuda_h.py:19] end gpu_group_tensor cost 0.00016117095947265625 seconds
DEBUG 01-15 16:09:01.657221.657221 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.657825.657825 cuda_h.py:19] end gpu_group_einsum cost 0.0005977153778076172 seconds
DEBUG 01-15 16:09:01.657777.657777 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.658400.658400 cuda_h.py:19] end gpu_group_einsum cost 0.0005390644073486328 seconds
DEBUG 01-15 16:09:01.658007.658007 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.658084.658084 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.659643.659643 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002219676971435547 seconds
DEBUG 01-15 16:09:01.659697.659697 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.659323.659323 cuda_h.py:19] end concat_expert_out cost 7.319450378417969e-05 seconds
DEBUG 01-15 16:09:01.659425.659425 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.659429.659429 cuda_h.py:19] end index_scatter cost 6.4849853515625e-05 seconds
DEBUG 01-15 16:09:01.659422.659422 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012197494506835938 seconds
DEBUG 01-15 16:09:01.660173.660173 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.660885.660885 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.660522.660522 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018906593322753906 seconds
DEBUG 01-15 16:09:01.660139.660139 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.660957.660957 cuda_h.py:19] end concat_expert_out cost 7.62939453125e-05 seconds
DEBUG 01-15 16:09:01.660913.660913 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.660016.660016 cuda_h.py:19] end index_scatter cost 7.677078247070312e-05 seconds
DEBUG 01-15 16:09:01.660302.660302 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006449222564697266 seconds
DEBUG 01-15 16:09:01.661498.661498 cuda_h.py:19] end gpu_experts_multi_device cost 0.05417585372924805 seconds
DEBUG 01-15 16:09:01.661461.661461 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.06553864479064941 seconds
DEBUG 01-15 16:09:01.661928.661928 cuda_h.py:19] end prefill_layer cost 0.07273364067077637 seconds
DEBUG 01-15 16:09:01.661037.661037 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:09:01.661409.661409 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.661257.661257 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:09:01.661583.661583 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:09:01.661100.661100 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:09:01.661904.661904 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:09:01.661706.661706 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:09:01.661118.661118 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.662631.662631 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.662290.662290 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.662440.662440 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.662391.662391 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.662791.662791 cuda_h.py:19] end allocate_cuda_memory cost 0.00041031837463378906 seconds
DEBUG 01-15 16:09:01.662722.662722 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.663910.663910 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.663641.663641 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.663431.663431 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8f973dbf-44a8-4328-82ad-d125797cf8b9
DEBUG 01-15 16:09:01.663945.663945 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.663104.663104 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.665088.665088 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8f973dbf-44a8-4328-82ad-d125797cf8b9
DEBUG 01-15 16:09:01.665371.665371 cuda_h.py:19] end load_into_gpu_async cost 0.002088785171508789 seconds
DEBUG 01-15 16:09:01.665109.665109 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.665658.665658 cuda_h.py:19] end restore_tensors2 cost 0.0001285076141357422 seconds
DEBUG 01-15 16:09:01.665635.665635 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0031540393829345703 seconds
INFO 01-15 16:09:01.665997.665997 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8f973dbf-44a8-4328-82ad-d125797cf8b9
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.667102.667102 cuda_h.py:19] end self_attn cost 0.0036115646362304688 seconds
DEBUG 01-15 16:09:01.667694.667694 cuda_h.py:19] end iln_self_attn_paln cost 0.0055158138275146484 seconds
DEBUG 01-15 16:09:01.667855.667855 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:09:01.667565.667565 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.668634.668634 cuda_h.py:19] end gate cost 0.0006499290466308594 seconds
DEBUG 01-15 16:09:01.668417.668417 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.668309.668309 lmp.py:1912] 
DEBUG 01-15 16:09:01.668309.668309 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.668972.668972 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.668384.668384 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.668457.668457 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.668385.668385 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.668598.668598 lmp.py:1917] 
DEBUG 01-15 16:09:01.668598.668598 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.668764.668764 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.668890.668890 lmp.py:1935]   Expert 25 |     14 | CPU
DEBUG 01-15 16:09:01.668295.668295 lmp.py:1935]   Expert 48 |     32 | CPU
DEBUG 01-15 16:09:01.668984.668984 lmp.py:1935]   Expert 45 |     36 | CPU
DEBUG 01-15 16:09:01.668673.668673 lmp.py:1935]   Expert  9 |     63 | CPU
DEBUG 01-15 16:09:01.668886.668886 lmp.py:1935]   Expert  0 |     83 | CPU
DEBUG 01-15 16:09:01.668098.668098 lmp.py:1935]   Expert 43 |     85 | CPU
DEBUG 01-15 16:09:01.668026.668026 lmp.py:1935]   Expert 20 |     87 | CPU
DEBUG 01-15 16:09:01.668477.668477 lmp.py:1935]   Expert 47 |     88 | CPU
DEBUG 01-15 16:09:01.668405.668405 lmp.py:1935]   Expert 54 |     88 | CPU
DEBUG 01-15 16:09:01.668094.668094 lmp.py:1935]   Expert 57 |     91 | CPU
DEBUG 01-15 16:09:01.668068.668068 lmp.py:1935]   Expert  6 |     95 | CPU
DEBUG 01-15 16:09:01.668280.668280 lmp.py:1935]   Expert 36 |     98 | CPU
DEBUG 01-15 16:09:01.669016.669016 lmp.py:1935]   Expert 15 |    103 | CPU
DEBUG 01-15 16:09:01.669990.669990 lmp.py:1935]   Expert 62 |    104 | CPU
DEBUG 01-15 16:09:01.669964.669964 lmp.py:1935]   Expert  1 |    106 | CPU
DEBUG 01-15 16:09:01.669700.669700 lmp.py:1935]   Expert 13 |    106 | CPU
DEBUG 01-15 16:09:01.669435.669435 lmp.py:1935]   Expert 50 |    107 | CPU
DEBUG 01-15 16:09:01.669171.669171 lmp.py:1935]   Expert 61 |    107 | CPU
DEBUG 01-15 16:09:01.669622.669622 lmp.py:1935]   Expert 38 |    112 | CPU
DEBUG 01-15 16:09:01.669311.669311 lmp.py:1935]   Expert 37 |    113 | CPU
DEBUG 01-15 16:09:01.669047.669047 lmp.py:1935]   Expert 14 |    121 | CPU
DEBUG 01-15 16:09:01.669690.669690 lmp.py:1935]   Expert 46 |    122 | CPU
DEBUG 01-15 16:09:01.669810.669810 lmp.py:1935]   Expert 21 |    130 | CPU
DEBUG 01-15 16:09:01.669453.669453 lmp.py:1935]   Expert  7 |    138 | CPU
DEBUG 01-15 16:09:01.669380.669380 lmp.py:1935]   Expert 28 |    139 | CPU
DEBUG 01-15 16:09:01.669692.669692 lmp.py:1935]   Expert 52 |    142 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669766.669766 lmp.py:1935]   Expert 44 |    143 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669601.669601 lmp.py:1935]   Expert 10 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669959.669959 lmp.py:1935]   Expert 24 |    153 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669033.669033 lmp.py:1935]   Expert 42 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669152.669152 lmp.py:1935]   Expert 11 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669511.669511 lmp.py:1935]   Expert  2 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669869.669869 lmp.py:1935]   Expert 35 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669227.669227 lmp.py:1935]   Expert 26 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669108.669108 lmp.py:1935]   Expert 31 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669957.669957 lmp.py:1935]   Expert  3 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669315.669315 lmp.py:1935]   Expert 19 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669674.669674 lmp.py:1935]   Expert 32 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669793.669793 lmp.py:1935]   Expert 12 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669959.669959 lmp.py:1935]   Expert 56 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669887.669887 lmp.py:1935]   Expert 60 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669815.669815 lmp.py:1935]   Expert 40 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669504.669504 lmp.py:1935]   Expert 41 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669717.669717 lmp.py:1935]   Expert 53 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669644.669644 lmp.py:1935]   Expert 23 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669572.669572 lmp.py:1935]   Expert  8 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669513.669513 lmp.py:1935]   Expert 16 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669441.669441 lmp.py:1935]   Expert 58 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669369.669369 lmp.py:1935]   Expert 51 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669581.669581 lmp.py:1935]   Expert 59 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669794.669794 lmp.py:1935]   Expert  4 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669245.669245 lmp.py:1935]   Expert 55 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669219.669219 lmp.py:1935]   Expert 49 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669908.669908 lmp.py:1935]   Expert 29 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669120.669120 lmp.py:1935]   Expert 18 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669333.669333 lmp.py:1935]   Expert 34 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669784.669784 lmp.py:1935]   Expert 63 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669950.669950 lmp.py:1935]   Expert 27 |    355 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669878.669878 lmp.py:1935]   Expert 39 |    377 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669567.669567 lmp.py:1935]   Expert 17 |    391 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669303.669303 lmp.py:1935]   Expert 22 |    432 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669515.669515 lmp.py:1935]   Expert 33 |    449 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669728.669728 lmp.py:1935]   Expert 30 |    457 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.669940.669940 lmp.py:1935]   Expert  5 |    711 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.669199.669199 lmp.py:1937] 
DEBUG 01-15 16:09:01.669199.669199 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.669173.669173 lmp.py:1938]   CPU:   2368 tokens
DEBUG 01-15 16:09:01.669816.669816 lmp.py:1942]   cuda:1:   4918 tokens (19 experts)
DEBUG 01-15 16:09:01.669982.669982 lmp.py:1942]   cuda:2:   5002 tokens (20 experts)
DEBUG 01-15 16:09:01.669956.669956 lmp.py:1943]   Total GPU:   9920 tokens
DEBUG 01-15 16:09:01.669976.669976 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.669976.669976 lmp.py:1944] 
DEBUG 01-15 16:09:01.670672.670672 cuda_h.py:19] end experts_map_get cost 0.0016696453094482422 seconds
DEBUG 01-15 16:09:01.670284.670284 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.670894.670894 lmp.py:1953] 
DEBUG 01-15 16:09:01.670894.670894 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.670538.670538 cuda_h.py:19] end cpu_experts_submit cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:01.670016.670016 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.670661.670661 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.672860.672860 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.4.gate_proj.weight': 0, 'model.layers.22.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.22.mlp.experts.2.gate_proj.weight': 0, 'model.layers.22.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.26.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.26.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.26.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.22.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.22.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.22.mlp.experts.59.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(26182942720, 5767168, 0, 0), (26188709888, 5767168, 5767168, 0), (26177175552, 5767168, 11534336, 0), (26200244224, 5767168, 17301504, 0), (26206011392, 5767168, 23068672, 0), (26194477056, 5767168, 28835840, 0), (26252148736, 5767168, 34603008, 0), (26257915904, 5767168, 40370176, 0), (26246381568, 5767168, 46137344, 0), (26304053248, 5767168, 51904512, 0), (26309820416, 5767168, 57671680, 0), (26298286080, 5767168, 63438848, 0), (26321354752, 5767168, 69206016, 0), (26327121920, 5767168, 74973184, 0), (26315587584, 5767168, 80740352, 0), (26425163776, 5767168, 86507520, 0), (26430930944, 5767168, 92274688, 0), (26419396608, 5767168, 98041856, 0), (26442465280, 5767168, 103809024, 0), (26448232448, 5767168, 109576192, 0), (26436698112, 5767168, 115343360, 0), (26494369792, 5767168, 121110528, 0), (26500136960, 5767168, 126877696, 0), (26488602624, 5767168, 132644864, 0), (26511671296, 5767168, 138412032, 0), (26517438464, 5767168, 144179200, 0), (26505904128, 5767168, 149946368, 0), (26528972800, 5767168, 155713536, 0), (26534739968, 5767168, 161480704, 0), (26523205632, 5767168, 167247872, 0), (26650083328, 5767168, 173015040, 0), (26655850496, 5767168, 178782208, 0), (26644316160, 5767168, 184549376, 0), (26719289344, 5767168, 190316544, 0), (26725056512, 5767168, 196083712, 0), (26713522176, 5767168, 201850880, 0), (26788495360, 5767168, 207618048, 0), (26794262528, 5767168, 213385216, 0), (26782728192, 5767168, 219152384, 0), (26823098368, 5767168, 224919552, 0), (26828865536, 5767168, 230686720, 0), (26817331200, 5767168, 236453888, 0), (26875002880, 5767168, 242221056, 0), (26880770048, 5767168, 247988224, 0), (26869235712, 5767168, 253755392, 0), (26961510400, 5767168, 259522560, 0), (26967277568, 5767168, 265289728, 0), (26955743232, 5767168, 271056896, 0), (26996113408, 5767168, 276824064, 0), (27001880576, 5767168, 282591232, 0), (26990346240, 5767168, 288358400, 0), (27151826944, 5767168, 294125568, 0), (27157594112, 5767168, 299892736, 0), (27146059776, 5767168, 305659904, 0), (27203731456, 5767168, 311427072, 0), (27209498624, 5767168, 317194240, 0), (27197964288, 5767168, 322961408, 0)], 2: [(26148339712, 5767168, 0, 0), (26154106880, 5767168, 5767168, 0), (26142572544, 5767168, 11534336, 0), (26165641216, 5767168, 17301504, 0), (26171408384, 5767168, 23068672, 0), (26159874048, 5767168, 28835840, 0), (26286751744, 5767168, 34603008, 0), (26292518912, 5767168, 40370176, 0), (26280984576, 5767168, 46137344, 0), (26390560768, 5767168, 51904512, 0), (26396327936, 5767168, 57671680, 0), (26384793600, 5767168, 63438848, 0), (26407862272, 5767168, 69206016, 0), (26413629440, 5767168, 74973184, 0), (26402095104, 5767168, 80740352, 0), (26563575808, 5767168, 86507520, 0), (26569342976, 5767168, 92274688, 0), (26557808640, 5767168, 98041856, 0), (26580877312, 5767168, 103809024, 0), (26586644480, 5767168, 109576192, 0), (26575110144, 5767168, 115343360, 0), (26615480320, 5767168, 121110528, 0), (26621247488, 5767168, 126877696, 0), (26609713152, 5767168, 132644864, 0), (26632781824, 5767168, 138412032, 0), (26638548992, 5767168, 144179200, 0), (26627014656, 5767168, 149946368, 0), (26667384832, 5767168, 155713536, 0), (26673152000, 5767168, 161480704, 0), (26661617664, 5767168, 167247872, 0), (26684686336, 5767168, 173015040, 0), (26690453504, 5767168, 178782208, 0), (26678919168, 5767168, 184549376, 0), (26701987840, 5767168, 190316544, 0), (26707755008, 5767168, 196083712, 0), (26696220672, 5767168, 201850880, 0), (26805796864, 5767168, 207618048, 0), (26811564032, 5767168, 213385216, 0), (26800029696, 5767168, 219152384, 0), (26840399872, 5767168, 224919552, 0), (26846167040, 5767168, 230686720, 0), (26834632704, 5767168, 236453888, 0), (27013414912, 5767168, 242221056, 0), (27019182080, 5767168, 247988224, 0), (27007647744, 5767168, 253755392, 0), (27030716416, 5767168, 259522560, 0), (27036483584, 5767168, 265289728, 0), (27024949248, 5767168, 271056896, 0), (27065319424, 5767168, 276824064, 0), (27071086592, 5767168, 282591232, 0), (27059552256, 5767168, 288358400, 0), (27082620928, 5767168, 294125568, 0), (27088388096, 5767168, 299892736, 0), (27076853760, 5767168, 305659904, 0), (27117223936, 5767168, 311427072, 0), (27122991104, 5767168, 317194240, 0), (27111456768, 5767168, 322961408, 0), (27134525440, 5767168, 328728576, 0), (27140292608, 5767168, 334495744, 0), (27128758272, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74aa04639ec0>, 2: <capsule object NULL at 0x74a6bc4fe3d0>}
DEBUG 01-15 16:09:01.672725.672725 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.672111.672111 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.672351.672351 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bdeecb22-7ad5-424f-817c-1318d88b5543
DEBUG 01-15 16:09:01.672981.672981 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:01.673243.673243 client.py:127] Model loaded
DEBUG 01-15 16:09:01.673143.673143 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.673692.673692 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.673537.673537 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.674771.674771 cuda_h.py:19] end restore2model cost 0.0008444786071777344 seconds
DEBUG 01-15 16:09:01.674859.674859 cuda_h.py:19] end sllm_worker_task cost 0.012118339538574219 seconds
INFO 01-15 16:09:01.674191.674191 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bdeecb22-7ad5-424f-817c-1318d88b5543
DEBUG 01-15 16:09:01.674488.674488 cuda_h.py:19] end move_flatidxs cost 0.0008502006530761719 seconds
DEBUG 01-15 16:09:01.674933.674933 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.675491.675491 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0047953128814697266 seconds
DEBUG 01-15 16:09:01.675315.675315 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.678423.678423 cuda_h.py:19] end restore2model cost 0.0029942989349365234 seconds
DEBUG 01-15 16:09:01.678498.678498 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008022546768188477 seconds
DEBUG 01-15 16:09:01.678837.678837 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.678033.678033 cuda_h.py:19] end gpu_sexperts cost 0.00028443336486816406 seconds
DEBUG 01-15 16:09:01.678385.678385 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.678731.678731 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:09:01.678189.678189 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.678084.678084 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.679468.679468 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009529590606689453 seconds
DEBUG 01-15 16:09:01.679702.679702 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.680683.680683 cuda_h.py:19] end gpu_group_list cost 0.0002028942108154297 seconds
DEBUG 01-15 16:09:01.680696.680696 cuda_h.py:19] end group_tensors cost 0.005123615264892578 seconds
DEBUG 01-15 16:09:01.680727.680727 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.680155.680155 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.682491.682491 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016012191772460938 seconds
DEBUG 01-15 16:09:01.682534.682534 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.683155.683155 cuda_h.py:19] end gpu_group_list cost 0.0003139972686767578 seconds
DEBUG 01-15 16:09:01.683320.683320 cuda_h.py:19] end group pad cost 0.0028390884399414062 seconds
DEBUG 01-15 16:09:01.683587.683587 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:01.684543.684543 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.684134.684134 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bdeecb22-7ad5-424f-817c-1318d88b5543
DEBUG 01-15 16:09:01.710246.710246 cuda_h.py:19] end group_einsum cost 0.02690410614013672 seconds
DEBUG 01-15 16:09:01.710146.710146 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.713920.713920 cuda_h.py:19] end get_outputs_cpu1 cost 0.0026886463165283203 seconds
DEBUG 01-15 16:09:01.714943.714943 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04084515571594238 seconds
INFO 01-15 16:09:01.715253.715253 client.py:127] Model loaded
DEBUG 01-15 16:09:01.715946.715946 cuda_h.py:19] end wait_experts_multi_device cost 0.030434131622314453 seconds
DEBUG 01-15 16:09:01.715662.715662 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.715713.715713 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0004911422729492188 seconds
DEBUG 01-15 16:09:01.715510.715510 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.716159.716159 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.716979.716979 cuda_h.py:19] end index_scatter cost 7.510185241699219e-05 seconds
DEBUG 01-15 16:09:01.717822.717822 cuda_h.py:19] end cpuoutputsdeal cost 0.0013871192932128906 seconds
DEBUG 01-15 16:09:01.717831.717831 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.717164.717164 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.717885.717885 cuda_h.py:19] end gpu_group_tensor cost 0.00015020370483398438 seconds
DEBUG 01-15 16:09:01.717979.717979 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.717879.717879 cuda_h.py:19] end gpu_group_tensor cost 0.00014281272888183594 seconds
DEBUG 01-15 16:09:01.717359.717359 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.718296.718296 cuda_h.py:19] end gpu_group_einsum cost 0.0004334449768066406 seconds
DEBUG 01-15 16:09:01.718975.718975 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.718575.718575 cuda_h.py:19] end gpu_group_einsum cost 0.00046181678771972656 seconds
DEBUG 01-15 16:09:01.719036.719036 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.719431.719431 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.719996.719996 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023102760314941406 seconds
DEBUG 01-15 16:09:01.719428.719428 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.719749.719749 cuda_h.py:19] end concat_expert_out cost 6.222724914550781e-05 seconds
DEBUG 01-15 16:09:01.719923.719923 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.719893.719893 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:01.719543.719543 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008487701416015625 seconds
DEBUG 01-15 16:09:01.720904.720904 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.720608.720608 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.720137.720137 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001494884490966797 seconds
DEBUG 01-15 16:09:01.720747.720747 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.720969.720969 cuda_h.py:19] end concat_expert_out cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:09:01.720217.720217 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.720332.720332 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:01.720995.720995 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005233287811279297 seconds
DEBUG 01-15 16:09:01.720952.720952 cuda_h.py:19] end gpu_experts_multi_device cost 0.04193735122680664 seconds
DEBUG 01-15 16:09:01.720053.720053 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.05317234992980957 seconds
DEBUG 01-15 16:09:01.721080.721080 cuda_h.py:19] end prefill_layer cost 0.059381723403930664 seconds
DEBUG 01-15 16:09:01.721976.721976 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:09:01.721918.721918 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.721051.721051 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:09:01.721708.721708 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:09:01.721033.721033 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:09:01.721890.721890 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 4.172325134277344e-05 seconds
DEBUG 01-15 16:09:01.721407.721407 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:09:01.721534.721534 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.721199.721199 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.721468.721468 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.721575.721575 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.721553.721553 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.722443.722443 cuda_h.py:19] end allocate_cuda_memory cost 0.00023245811462402344 seconds
DEBUG 01-15 16:09:01.722882.722882 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.722453.722453 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.722461.722461 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.722733.722733 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c4f5fa6f-58bb-4c33-bf20-ae55750cffca
DEBUG 01-15 16:09:01.722134.722134 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.722593.722593 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.723661.723661 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c4f5fa6f-58bb-4c33-bf20-ae55750cffca
DEBUG 01-15 16:09:01.723921.723921 cuda_h.py:19] end load_into_gpu_async cost 0.0013279914855957031 seconds
DEBUG 01-15 16:09:01.723479.723479 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.723992.723992 cuda_h.py:19] end restore_tensors2 cost 7.104873657226562e-05 seconds
DEBUG 01-15 16:09:01.723840.723840 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0018851757049560547 seconds
INFO 01-15 16:09:01.723445.723445 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c4f5fa6f-58bb-4c33-bf20-ae55750cffca
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.726473.726473 cuda_h.py:19] end self_attn cost 0.0036683082580566406 seconds
DEBUG 01-15 16:09:01.726523.726523 cuda_h.py:19] end iln_self_attn_paln cost 0.005190372467041016 seconds
DEBUG 01-15 16:09:01.726068.726068 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:09:01.726262.726262 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.727683.727683 cuda_h.py:19] end gate cost 0.0006940364837646484 seconds
DEBUG 01-15 16:09:01.727135.727135 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.727755.727755 lmp.py:1912] 
DEBUG 01-15 16:09:01.727755.727755 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.727465.727465 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.727592.727592 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.727380.727380 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.727785.727785 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.727997.727997 lmp.py:1917] 
DEBUG 01-15 16:09:01.727997.727997 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.728164.728164 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.728767.728767 lmp.py:1935]   Expert  5 |     14 | CPU
DEBUG 01-15 16:09:01.728648.728648 lmp.py:1935]   Expert 56 |     32 | CPU
DEBUG 01-15 16:09:01.728338.728338 lmp.py:1935]   Expert 16 |     86 | CPU
DEBUG 01-15 16:09:01.728233.728233 lmp.py:1935]   Expert 27 |     88 | CPU
DEBUG 01-15 16:09:01.728637.728637 lmp.py:1935]   Expert 17 |     89 | CPU
DEBUG 01-15 16:09:01.728711.728711 lmp.py:1935]   Expert 40 |     93 | CPU
DEBUG 01-15 16:09:01.728354.728354 lmp.py:1935]   Expert 63 |    103 | CPU
DEBUG 01-15 16:09:01.728043.728043 lmp.py:1935]   Expert 49 |    105 | CPU
DEBUG 01-15 16:09:01.728732.728732 lmp.py:1935]   Expert 51 |    105 | CPU
DEBUG 01-15 16:09:01.728898.728898 lmp.py:1935]   Expert 53 |    105 | CPU
DEBUG 01-15 16:09:01.728349.728349 lmp.py:1935]   Expert 28 |    110 | CPU
DEBUG 01-15 16:09:01.728515.728515 lmp.py:1935]   Expert  7 |    113 | CPU
DEBUG 01-15 16:09:01.728205.728205 lmp.py:1935]   Expert 47 |    121 | CPU
DEBUG 01-15 16:09:01.728894.728894 lmp.py:1935]   Expert 38 |    124 | CPU
DEBUG 01-15 16:09:01.728583.728583 lmp.py:1935]   Expert 37 |    125 | CPU
DEBUG 01-15 16:09:01.728273.728273 lmp.py:1935]   Expert 58 |    126 | CPU
DEBUG 01-15 16:09:01.728962.728962 lmp.py:1935]   Expert 62 |    127 | CPU
DEBUG 01-15 16:09:01.728843.728843 lmp.py:1935]   Expert 11 |    129 | CPU
DEBUG 01-15 16:09:01.728486.728486 lmp.py:1935]   Expert 57 |    139 | CPU
DEBUG 01-15 16:09:01.728652.728652 lmp.py:1935]   Expert  1 |    144 | CPU
DEBUG 01-15 16:09:01.728819.728819 lmp.py:1935]   Expert 39 |    146 | CPU
DEBUG 01-15 16:09:01.728667.728667 lmp.py:1935]   Expert 14 |    148 | CPU
DEBUG 01-15 16:09:01.728833.728833 lmp.py:1935]   Expert 52 |    151 | CPU
DEBUG 01-15 16:09:01.728523.728523 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:09:01.728450.728450 lmp.py:1935]   Expert 25 |    159 | CPU
DEBUG 01-15 16:09:01.728524.728524 lmp.py:1935]   Expert 33 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728359.728359 lmp.py:1935]   Expert 21 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728479.728479 lmp.py:1935]   Expert  6 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728122.728122 lmp.py:1935]   Expert 60 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728003.728003 lmp.py:1935]   Expert 45 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728077.728077 lmp.py:1935]   Expert 19 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728958.728958 lmp.py:1935]   Expert 44 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728601.728601 lmp.py:1935]   Expert  4 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728244.728244 lmp.py:1935]   Expert 12 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728887.728887 lmp.py:1935]   Expert 55 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728007.728007 lmp.py:1935]   Expert 31 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728934.728934 lmp.py:1935]   Expert  3 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728339.728339 lmp.py:1935]   Expert 30 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728220.728220 lmp.py:1935]   Expert 36 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728340.728340 lmp.py:1935]   Expert  9 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728460.728460 lmp.py:1935]   Expert  0 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728103.728103 lmp.py:1935]   Expert 34 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728984.728984 lmp.py:1935]   Expert 22 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728389.728389 lmp.py:1935]   Expert 41 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728793.728793 lmp.py:1935]   Expert 54 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728436.728436 lmp.py:1935]   Expert 26 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728603.728603 lmp.py:1935]   Expert 43 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728007.728007 lmp.py:1935]   Expert 59 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728650.728650 lmp.py:1935]   Expert 18 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728247.728247 lmp.py:1935]   Expert 20 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728843.728843 lmp.py:1935]   Expert 13 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728486.728486 lmp.py:1935]   Expert 50 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728891.728891 lmp.py:1935]   Expert 15 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.728296.728296 lmp.py:1935]   Expert 24 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.728462.728462 lmp.py:1935]   Expert 42 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.729866.729866 lmp.py:1935]   Expert 29 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.729509.729509 lmp.py:1935]   Expert 61 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.729152.729152 lmp.py:1935]   Expert 35 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.729272.729272 lmp.py:1935]   Expert 32 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.729869.729869 lmp.py:1935]   Expert  2 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.729227.729227 lmp.py:1935]   Expert  8 |    338 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.729108.729108 lmp.py:1935]   Expert 10 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.729466.729466 lmp.py:1935]   Expert 46 |    424 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.729109.729109 lmp.py:1935]   Expert 48 |    448 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.729322.729322 lmp.py:1937] 
DEBUG 01-15 16:09:01.729322.729322 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.729726.729726 lmp.py:1938]   CPU:   2837 tokens
DEBUG 01-15 16:09:01.729846.729846 lmp.py:1942]   cuda:1:   4647 tokens (19 experts)
DEBUG 01-15 16:09:01.729728.729728 lmp.py:1942]   cuda:2:   4804 tokens (20 experts)
DEBUG 01-15 16:09:01.729655.729655 lmp.py:1943]   Total GPU:   9451 tokens
DEBUG 01-15 16:09:01.729060.729060 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.729060.729060 lmp.py:1944] 
DEBUG 01-15 16:09:01.729710.729710 cuda_h.py:19] end experts_map_get cost 0.0016939640045166016 seconds
DEBUG 01-15 16:09:01.729990.729990 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.729077.729077 lmp.py:1953] 
DEBUG 01-15 16:09:01.729077.729077 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.729668.729668 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:01.729887.729887 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.729770.729770 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.730946.730946 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.3.gate_proj.weight': 0, 'model.layers.23.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.60.up_proj.weight': 322961408}, 2: {'model.layers.23.mlp.experts.0.gate_proj.weight': 0, 'model.layers.23.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.31.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.31.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.31.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.33.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.33.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.33.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.45.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.45.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.45.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.46.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.46.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.46.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.50.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.50.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.50.up_proj.weight': 322961408, 'model.layers.23.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.23.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.23.mlp.experts.61.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(27272937472, 5767168, 0, 0), (27278704640, 5767168, 5767168, 0), (27267170304, 5767168, 11534336, 0), (27290238976, 5767168, 17301504, 0), (27296006144, 5767168, 23068672, 0), (27284471808, 5767168, 28835840, 0), (27324841984, 5767168, 34603008, 0), (27330609152, 5767168, 40370176, 0), (27319074816, 5767168, 46137344, 0), (27359444992, 5767168, 51904512, 0), (27365212160, 5767168, 57671680, 0), (27353677824, 5767168, 63438848, 0), (27445952512, 5767168, 69206016, 0), (27451719680, 5767168, 74973184, 0), (27440185344, 5767168, 80740352, 0), (27532460032, 5767168, 86507520, 0), (27538227200, 5767168, 92274688, 0), (27526692864, 5767168, 98041856, 0), (27601666048, 5767168, 103809024, 0), (27607433216, 5767168, 109576192, 0), (27595898880, 5767168, 115343360, 0), (27636269056, 5767168, 121110528, 0), (27642036224, 5767168, 126877696, 0), (27630501888, 5767168, 132644864, 0), (27774681088, 5767168, 138412032, 0), (27780448256, 5767168, 144179200, 0), (27768913920, 5767168, 149946368, 0), (27809284096, 5767168, 155713536, 0), (27815051264, 5767168, 161480704, 0), (27803516928, 5767168, 167247872, 0), (27826585600, 5767168, 173015040, 0), (27832352768, 5767168, 178782208, 0), (27820818432, 5767168, 184549376, 0), (27843887104, 5767168, 190316544, 0), (27849654272, 5767168, 196083712, 0), (27838119936, 5767168, 201850880, 0), (27947696128, 5767168, 207618048, 0), (27953463296, 5767168, 213385216, 0), (27941928960, 5767168, 219152384, 0), (27982299136, 5767168, 224919552, 0), (27988066304, 5767168, 230686720, 0), (27976531968, 5767168, 236453888, 0), (28051505152, 5767168, 242221056, 0), (28057272320, 5767168, 247988224, 0), (28045737984, 5767168, 253755392, 0), (28155314176, 5767168, 259522560, 0), (28161081344, 5767168, 265289728, 0), (28149547008, 5767168, 271056896, 0), (28172615680, 5767168, 276824064, 0), (28178382848, 5767168, 282591232, 0), (28166848512, 5767168, 288358400, 0), (28241821696, 5767168, 294125568, 0), (28247588864, 5767168, 299892736, 0), (28236054528, 5767168, 305659904, 0), (28259123200, 5767168, 311427072, 0), (28264890368, 5767168, 317194240, 0), (28253356032, 5767168, 322961408, 0)], 2: [(27221032960, 5767168, 0, 0), (27226800128, 5767168, 5767168, 0), (27215265792, 5767168, 11534336, 0), (27255635968, 5767168, 17301504, 0), (27261403136, 5767168, 23068672, 0), (27249868800, 5767168, 28835840, 0), (27376746496, 5767168, 34603008, 0), (27382513664, 5767168, 40370176, 0), (27370979328, 5767168, 46137344, 0), (27394048000, 5767168, 51904512, 0), (27399815168, 5767168, 57671680, 0), (27388280832, 5767168, 63438848, 0), (27428651008, 5767168, 69206016, 0), (27434418176, 5767168, 74973184, 0), (27422883840, 5767168, 80740352, 0), (27480555520, 5767168, 86507520, 0), (27486322688, 5767168, 92274688, 0), (27474788352, 5767168, 98041856, 0), (27549761536, 5767168, 103809024, 0), (27555528704, 5767168, 109576192, 0), (27543994368, 5767168, 115343360, 0), (27567063040, 5767168, 121110528, 0), (27572830208, 5767168, 126877696, 0), (27561295872, 5767168, 132644864, 0), (27584364544, 5767168, 138412032, 0), (27590131712, 5767168, 144179200, 0), (27578597376, 5767168, 149946368, 0), (27670872064, 5767168, 155713536, 0), (27676639232, 5767168, 161480704, 0), (27665104896, 5767168, 167247872, 0), (27722776576, 5767168, 173015040, 0), (27728543744, 5767168, 178782208, 0), (27717009408, 5767168, 184549376, 0), (27740078080, 5767168, 190316544, 0), (27745845248, 5767168, 196083712, 0), (27734310912, 5767168, 201850880, 0), (27757379584, 5767168, 207618048, 0), (27763146752, 5767168, 213385216, 0), (27751612416, 5767168, 219152384, 0), (27791982592, 5767168, 224919552, 0), (27797749760, 5767168, 230686720, 0), (27786215424, 5767168, 236453888, 0), (27930394624, 5767168, 242221056, 0), (27936161792, 5767168, 247988224, 0), (27924627456, 5767168, 253755392, 0), (27964997632, 5767168, 259522560, 0), (27970764800, 5767168, 265289728, 0), (27959230464, 5767168, 271056896, 0), (27999600640, 5767168, 276824064, 0), (28005367808, 5767168, 282591232, 0), (27993833472, 5767168, 288358400, 0), (28016902144, 5767168, 294125568, 0), (28022669312, 5767168, 299892736, 0), (28011134976, 5767168, 305659904, 0), (28086108160, 5767168, 311427072, 0), (28091875328, 5767168, 317194240, 0), (28080340992, 5767168, 322961408, 0), (28276424704, 5767168, 328728576, 0), (28282191872, 5767168, 334495744, 0), (28270657536, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8144b0690>, 2: <capsule object NULL at 0x74a6bc4fe280>}
INFO 01-15 16:09:01.730004.730004 client.py:127] Model loaded
DEBUG 01-15 16:09:01.730954.730954 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.730871.730871 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.730462.730462 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.731129.731129 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.730985.730985 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.731277.731277 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 40702080-9add-4298-b493-be21aae4d607
DEBUG 01-15 16:09:01.731298.731298 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.731056.731056 cuda_h.py:19] end restore2model cost 0.00048661231994628906 seconds
DEBUG 01-15 16:09:01.731076.731076 cuda_h.py:19] end sllm_worker_task cost 0.010031700134277344 seconds
DEBUG 01-15 16:09:01.732152.732152 cuda_h.py:19] end move_flatidxs cost 0.0008366107940673828 seconds
DEBUG 01-15 16:09:01.732405.732405 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:01.732276.732276 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 40702080-9add-4298-b493-be21aae4d607
DEBUG 01-15 16:09:01.733502.733502 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0038454532623291016 seconds
DEBUG 01-15 16:09:01.733611.733611 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.736607.736607 cuda_h.py:19] end restore2model cost 0.0030183792114257812 seconds
DEBUG 01-15 16:09:01.736610.736610 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007110595703125 seconds
DEBUG 01-15 16:09:01.736074.736074 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.736681.736681 cuda_h.py:19] end gpu_sexperts cost 0.00027441978454589844 seconds
DEBUG 01-15 16:09:01.736272.736272 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.737856.737856 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:09:01.737029.737029 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.737162.737162 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.738148.738148 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000942230224609375 seconds
DEBUG 01-15 16:09:01.738952.738952 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.738755.738755 cuda_h.py:19] end gpu_group_list cost 0.00021195411682128906 seconds
DEBUG 01-15 16:09:01.739107.739107 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.740952.740952 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001055002212524414 seconds
DEBUG 01-15 16:09:01.740855.740855 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.740440.740440 cuda_h.py:19] end gpu_group_list cost 0.00022530555725097656 seconds
DEBUG 01-15 16:09:01.741828.741828 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.741803.741803 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 40702080-9add-4298-b493-be21aae4d607
DEBUG 01-15 16:09:01.741105.741105 cuda_h.py:19] end group_tensors cost 0.00939631462097168 seconds
DEBUG 01-15 16:09:01.742638.742638 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.745249.745249 cuda_h.py:19] end group pad cost 0.0031714439392089844 seconds
DEBUG 01-15 16:09:01.745563.745563 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:01.772818.772818 client.py:127] Model loaded
DEBUG 01-15 16:09:01.772405.772405 cuda_h.py:19] end wait_experts_multi_device cost 0.03083491325378418 seconds
DEBUG 01-15 16:09:01.772407.772407 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.772962.772962 cuda_h.py:19] end group_einsum cost 0.02668309211730957 seconds
DEBUG 01-15 16:09:01.772199.772199 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.775891.775891 cuda_h.py:19] end get_outputs_cpu1 cost 0.002799510955810547 seconds
DEBUG 01-15 16:09:01.776705.776705 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.045546531677246094 seconds
DEBUG 01-15 16:09:01.777050.777050 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005097150802612305 seconds
DEBUG 01-15 16:09:01.777593.777593 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.779488.779488 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.780750.780750 cuda_h.py:19] end index_scatter cost 0.00015044212341308594 seconds
DEBUG 01-15 16:09:01.780160.780160 cuda_h.py:19] end cpuoutputsdeal cost 0.002969026565551758 seconds
DEBUG 01-15 16:09:01.780113.780113 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.780281.780281 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.781487.781487 cuda_h.py:19] end gpu_group_tensor cost 0.0003142356872558594 seconds
DEBUG 01-15 16:09:01.781371.781371 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.781389.781389 cuda_h.py:19] end gpu_group_tensor cost 0.0002884864807128906 seconds
DEBUG 01-15 16:09:01.782457.782457 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.783071.783071 cuda_h.py:19] end gpu_group_einsum cost 0.0010457038879394531 seconds
DEBUG 01-15 16:09:01.783635.783635 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.784973.784973 cuda_h.py:19] end gpu_group_einsum cost 0.0007176399230957031 seconds
DEBUG 01-15 16:09:01.784967.784967 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.784072.784072 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.785332.785332 cuda_h.py:19] end all_expert_outputs_slices cost 0.00036072731018066406 seconds
DEBUG 01-15 16:09:01.785844.785844 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.785438.785438 cuda_h.py:19] end concat_expert_out cost 9.34600830078125e-05 seconds
DEBUG 01-15 16:09:01.785548.785548 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.785964.785964 cuda_h.py:19] end index_scatter cost 9.775161743164062e-05 seconds
DEBUG 01-15 16:09:01.785024.785024 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0014276504516601562 seconds
DEBUG 01-15 16:09:01.786672.786672 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.786252.786252 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.786839.786839 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002663135528564453 seconds
DEBUG 01-15 16:09:01.786390.786390 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.786548.786548 cuda_h.py:19] end concat_expert_out cost 9.202957153320312e-05 seconds
DEBUG 01-15 16:09:01.786743.786743 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.787484.787484 cuda_h.py:19] end index_scatter cost 9.131431579589844e-05 seconds
DEBUG 01-15 16:09:01.787672.787672 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009121894836425781 seconds
DEBUG 01-15 16:09:01.787040.787040 cuda_h.py:19] end gpu_experts_multi_device cost 0.05015420913696289 seconds
DEBUG 01-15 16:09:01.787376.787376 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.06057882308959961 seconds
DEBUG 01-15 16:09:01.788001.788001 cuda_h.py:19] end prefill_layer cost 0.06673383712768555 seconds
DEBUG 01-15 16:09:01.788171.788171 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:09:01.788623.788623 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.788320.788320 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:09:01.788441.788441 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:09:01.788808.788808 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:09:01.788130.788130 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:09:01.788119.788119 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 0.0001289844512939453 seconds
DEBUG 01-15 16:09:01.788279.788279 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.788813.788813 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.788646.788646 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.788775.788775 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.789705.789705 cuda_h.py:19] end allocate_cuda_memory cost 0.0004076957702636719 seconds
DEBUG 01-15 16:09:01.789676.789676 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.789419.789419 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.789468.789468 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.789675.789675 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1f52c857-f75d-4700-9305-854f3135da7c
DEBUG 01-15 16:09:01.789911.789911 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.790098.790098 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.790807.790807 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.791580.791580 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1f52c857-f75d-4700-9305-854f3135da7c
DEBUG 01-15 16:09:01.791068.791068 cuda_h.py:19] end load_into_gpu_async cost 0.0019474029541015625 seconds
DEBUG 01-15 16:09:01.791897.791897 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.791260.791260 cuda_h.py:19] end restore_tensors2 cost 0.00011730194091796875 seconds
DEBUG 01-15 16:09:01.791096.791096 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0029053688049316406 seconds
INFO 01-15 16:09:01.791728.791728 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1f52c857-f75d-4700-9305-854f3135da7c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.795618.795618 cuda_h.py:19] end self_attn cost 0.004737138748168945 seconds
DEBUG 01-15 16:09:01.795850.795850 cuda_h.py:19] end iln_self_attn_paln cost 0.00728297233581543 seconds
DEBUG 01-15 16:09:01.796639.796639 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:09:01.796356.796356 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.796016.796016 cuda_h.py:19] end gate cost 0.0006954669952392578 seconds
DEBUG 01-15 16:09:01.796952.796952 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.797131.797131 lmp.py:1912] 
DEBUG 01-15 16:09:01.797131.797131 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.797325.797325 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.797266.797266 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.797869.797869 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.797373.797373 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.797685.797685 lmp.py:1917] 
DEBUG 01-15 16:09:01.797685.797685 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.797666.797666 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.797846.797846 lmp.py:1935]   Expert 36 |     22 | CPU
DEBUG 01-15 16:09:01.797873.797873 lmp.py:1935]   Expert 35 |     31 | CPU
DEBUG 01-15 16:09:01.797662.797662 lmp.py:1935]   Expert 25 |     45 | CPU
DEBUG 01-15 16:09:01.797358.797358 lmp.py:1935]   Expert 46 |     45 | CPU
DEBUG 01-15 16:09:01.797577.797577 lmp.py:1935]   Expert 51 |     51 | CPU
DEBUG 01-15 16:09:01.797512.797512 lmp.py:1935]   Expert 16 |     59 | CPU
DEBUG 01-15 16:09:01.797731.797731 lmp.py:1935]   Expert  0 |     64 | CPU
DEBUG 01-15 16:09:01.797473.797473 lmp.py:1935]   Expert 30 |     65 | CPU
DEBUG 01-15 16:09:01.797454.797454 lmp.py:1935]   Expert 43 |     70 | CPU
DEBUG 01-15 16:09:01.797197.797197 lmp.py:1935]   Expert 47 |     70 | CPU
DEBUG 01-15 16:09:01.797177.797177 lmp.py:1935]   Expert 55 |     71 | CPU
DEBUG 01-15 16:09:01.797397.797397 lmp.py:1935]   Expert 44 |     72 | CPU
DEBUG 01-15 16:09:01.797377.797377 lmp.py:1935]   Expert 39 |     76 | CPU
DEBUG 01-15 16:09:01.797643.797643 lmp.py:1935]   Expert 42 |     77 | CPU
DEBUG 01-15 16:09:01.797624.797624 lmp.py:1935]   Expert  2 |     82 | CPU
DEBUG 01-15 16:09:01.797843.797843 lmp.py:1935]   Expert  4 |    105 | CPU
DEBUG 01-15 16:09:01.797586.797586 lmp.py:1935]   Expert 33 |    120 | CPU
DEBUG 01-15 16:09:01.797613.797613 lmp.py:1935]   Expert 48 |    121 | CPU
DEBUG 01-15 16:09:01.797117.797117 lmp.py:1935]   Expert  6 |    122 | CPU
DEBUG 01-15 16:09:01.797157.797157 lmp.py:1935]   Expert 13 |    125 | CPU
DEBUG 01-15 16:09:01.797377.797377 lmp.py:1935]   Expert 24 |    126 | CPU
DEBUG 01-15 16:09:01.797596.797596 lmp.py:1935]   Expert 61 |    126 | CPU
DEBUG 01-15 16:09:01.797054.797054 lmp.py:1935]   Expert 56 |    128 | CPU
DEBUG 01-15 16:09:01.797558.797558 lmp.py:1935]   Expert 29 |    133 | CPU
DEBUG 01-15 16:09:01.797062.797062 lmp.py:1935]   Expert 15 |    134 | CPU
DEBUG 01-15 16:09:01.797711.797711 lmp.py:1935]   Expert  9 |    141 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.797884.797884 lmp.py:1935]   Expert 38 |    141 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798342.798342 lmp.py:1935]   Expert 54 |    144 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798038.798038 lmp.py:1935]   Expert 20 |    147 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798449.798449 lmp.py:1935]   Expert 59 |    148 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798020.798020 lmp.py:1935]   Expert  7 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798524.798524 lmp.py:1935]   Expert 62 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798551.798551 lmp.py:1935]   Expert 45 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798340.798340 lmp.py:1935]   Expert 19 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798367.798367 lmp.py:1935]   Expert 34 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798156.798156 lmp.py:1935]   Expert 57 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798183.798183 lmp.py:1935]   Expert 31 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798164.798164 lmp.py:1935]   Expert 50 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798429.798429 lmp.py:1935]   Expert 10 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798457.798457 lmp.py:1935]   Expert 23 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798722.798722 lmp.py:1935]   Expert 60 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798749.798749 lmp.py:1935]   Expert  8 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798015.798015 lmp.py:1935]   Expert 18 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798042.798042 lmp.py:1935]   Expert 22 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798592.798592 lmp.py:1935]   Expert 53 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798381.798381 lmp.py:1935]   Expert 37 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798408.798408 lmp.py:1935]   Expert 52 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798151.798151 lmp.py:1935]   Expert  5 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798370.798370 lmp.py:1935]   Expert 17 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798397.798397 lmp.py:1935]   Expert 11 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798948.798948 lmp.py:1935]   Expert  1 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798498.798498 lmp.py:1935]   Expert 41 |    280 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798287.798287 lmp.py:1935]   Expert 49 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798837.798837 lmp.py:1935]   Expert 28 |    287 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798056.798056 lmp.py:1935]   Expert 26 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798752.798752 lmp.py:1935]   Expert 58 |    293 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798687.798687 lmp.py:1935]   Expert 32 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798144.798144 lmp.py:1935]   Expert 40 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798648.798648 lmp.py:1935]   Expert 14 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798629.798629 lmp.py:1935]   Expert 12 |    328 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798133.798133 lmp.py:1935]   Expert 63 |    334 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798068.798068 lmp.py:1935]   Expert 21 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798810.798810 lmp.py:1935]   Expert 27 |    668 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.798983.798983 lmp.py:1935]   Expert  3 |   1014 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.798533.798533 lmp.py:1937] 
DEBUG 01-15 16:09:01.798533.798533 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.798514.798514 lmp.py:1938]   CPU:   2140 tokens
DEBUG 01-15 16:09:01.798687.798687 lmp.py:1942]   cuda:1:   5065 tokens (19 experts)
DEBUG 01-15 16:09:01.798622.798622 lmp.py:1942]   cuda:2:   5083 tokens (20 experts)
DEBUG 01-15 16:09:01.798887.798887 lmp.py:1943]   Total GPU:  10148 tokens
DEBUG 01-15 16:09:01.798676.798676 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.798676.798676 lmp.py:1944] 
DEBUG 01-15 16:09:01.798902.798902 cuda_h.py:19] end experts_map_get cost 0.0020368099212646484 seconds
INFO 01-15 16:09:01.799753.799753 client.py:127] Model loaded
DEBUG 01-15 16:09:01.799889.799889 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.799646.799646 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.799237.799237 lmp.py:1953] 
DEBUG 01-15 16:09:01.799237.799237 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.799411.799411 cuda_h.py:19] end cpu_experts_submit cost 0.00023603439331054688 seconds
DEBUG 01-15 16:09:01.799544.799544 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.799308.799308 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.800412.800412 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.1.gate_proj.weight': 0, 'model.layers.24.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.32.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.32.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.32.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.24.mlp.experts.5.gate_proj.weight': 0, 'model.layers.24.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.24.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.24.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.24.mlp.experts.62.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(28345630720, 5767168, 0, 0), (28351397888, 5767168, 5767168, 0), (28339863552, 5767168, 11534336, 0), (28380233728, 5767168, 17301504, 0), (28386000896, 5767168, 23068672, 0), (28374466560, 5767168, 28835840, 0), (28449439744, 5767168, 34603008, 0), (28455206912, 5767168, 40370176, 0), (28443672576, 5767168, 46137344, 0), (28484042752, 5767168, 51904512, 0), (28489809920, 5767168, 57671680, 0), (28478275584, 5767168, 63438848, 0), (28501344256, 5767168, 69206016, 0), (28507111424, 5767168, 74973184, 0), (28495577088, 5767168, 80740352, 0), (28570550272, 5767168, 86507520, 0), (28576317440, 5767168, 92274688, 0), (28564783104, 5767168, 98041856, 0), (28622454784, 5767168, 103809024, 0), (28628221952, 5767168, 109576192, 0), (28616687616, 5767168, 115343360, 0), (28639756288, 5767168, 121110528, 0), (28645523456, 5767168, 126877696, 0), (28633989120, 5767168, 132644864, 0), (28657057792, 5767168, 138412032, 0), (28662824960, 5767168, 144179200, 0), (28651290624, 5767168, 149946368, 0), (28674359296, 5767168, 155713536, 0), (28680126464, 5767168, 161480704, 0), (28668592128, 5767168, 167247872, 0), (28726263808, 5767168, 173015040, 0), (28732030976, 5767168, 178782208, 0), (28720496640, 5767168, 184549376, 0), (28778168320, 5767168, 190316544, 0), (28783935488, 5767168, 196083712, 0), (28772401152, 5767168, 201850880, 0), (28881977344, 5767168, 207618048, 0), (28887744512, 5767168, 213385216, 0), (28876210176, 5767168, 219152384, 0), (28968484864, 5767168, 224919552, 0), (28974252032, 5767168, 230686720, 0), (28962717696, 5767168, 236453888, 0), (29037690880, 5767168, 242221056, 0), (29043458048, 5767168, 247988224, 0), (29031923712, 5767168, 253755392, 0), (29106896896, 5767168, 259522560, 0), (29112664064, 5767168, 265289728, 0), (29101129728, 5767168, 271056896, 0), (29245308928, 5767168, 276824064, 0), (29251076096, 5767168, 282591232, 0), (29239541760, 5767168, 288358400, 0), (29314514944, 5767168, 294125568, 0), (29320282112, 5767168, 299892736, 0), (29308747776, 5767168, 305659904, 0), (29418323968, 5767168, 311427072, 0), (29424091136, 5767168, 317194240, 0), (29412556800, 5767168, 322961408, 0)], 2: [(28414836736, 5767168, 0, 0), (28420603904, 5767168, 5767168, 0), (28409069568, 5767168, 11534336, 0), (28466741248, 5767168, 17301504, 0), (28472508416, 5767168, 23068672, 0), (28460974080, 5767168, 28835840, 0), (28518645760, 5767168, 34603008, 0), (28524412928, 5767168, 40370176, 0), (28512878592, 5767168, 46137344, 0), (28535947264, 5767168, 51904512, 0), (28541714432, 5767168, 57671680, 0), (28530180096, 5767168, 63438848, 0), (28691660800, 5767168, 69206016, 0), (28697427968, 5767168, 74973184, 0), (28685893632, 5767168, 80740352, 0), (28708962304, 5767168, 86507520, 0), (28714729472, 5767168, 92274688, 0), (28703195136, 5767168, 98041856, 0), (28795469824, 5767168, 103809024, 0), (28801236992, 5767168, 109576192, 0), (28789702656, 5767168, 115343360, 0), (28812771328, 5767168, 121110528, 0), (28818538496, 5767168, 126877696, 0), (28807004160, 5767168, 132644864, 0), (28864675840, 5767168, 138412032, 0), (28870443008, 5767168, 144179200, 0), (28858908672, 5767168, 149946368, 0), (28916580352, 5767168, 155713536, 0), (28922347520, 5767168, 161480704, 0), (28910813184, 5767168, 167247872, 0), (28985786368, 5767168, 173015040, 0), (28991553536, 5767168, 178782208, 0), (28980019200, 5767168, 184549376, 0), (29020389376, 5767168, 190316544, 0), (29026156544, 5767168, 196083712, 0), (29014622208, 5767168, 201850880, 0), (29176102912, 5767168, 207618048, 0), (29181870080, 5767168, 213385216, 0), (29170335744, 5767168, 219152384, 0), (29193404416, 5767168, 224919552, 0), (29199171584, 5767168, 230686720, 0), (29187637248, 5767168, 236453888, 0), (29228007424, 5767168, 242221056, 0), (29233774592, 5767168, 247988224, 0), (29222240256, 5767168, 253755392, 0), (29262610432, 5767168, 259522560, 0), (29268377600, 5767168, 265289728, 0), (29256843264, 5767168, 271056896, 0), (29331816448, 5767168, 276824064, 0), (29337583616, 5767168, 282591232, 0), (29326049280, 5767168, 288358400, 0), (29349117952, 5767168, 294125568, 0), (29354885120, 5767168, 299892736, 0), (29343350784, 5767168, 305659904, 0), (29366419456, 5767168, 311427072, 0), (29372186624, 5767168, 317194240, 0), (29360652288, 5767168, 322961408, 0), (29401022464, 5767168, 328728576, 0), (29406789632, 5767168, 334495744, 0), (29395255296, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc4fe640>, 2: <capsule object NULL at 0x74a6bc4fe6a0>}
DEBUG 01-15 16:09:01.800421.800421 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.800940.800940 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.800511.800511 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 562f6003-dcf0-424b-b753-b0e7c8dea914
DEBUG 01-15 16:09:01.801533.801533 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.801800.801800 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.801459.801459 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.801177.801177 cuda_h.py:19] end restore2model cost 0.0026793479919433594 seconds
DEBUG 01-15 16:09:01.802076.802076 cuda_h.py:19] end sllm_worker_task cost 0.01324915885925293 seconds
INFO 01-15 16:09:01.802532.802532 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 562f6003-dcf0-424b-b753-b0e7c8dea914
DEBUG 01-15 16:09:01.802913.802913 cuda_h.py:19] end move_flatidxs cost 0.0008351802825927734 seconds
DEBUG 01-15 16:09:01.802643.802643 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.802087.802087 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003240823745727539 seconds
DEBUG 01-15 16:09:01.802063.802063 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.808262.808262 cuda_h.py:19] end restore2model cost 0.005289554595947266 seconds
DEBUG 01-15 16:09:01.808533.808533 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008953571319580078 seconds
DEBUG 01-15 16:09:01.808428.808428 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.808361.808361 cuda_h.py:19] end group_tensors cost 0.005581855773925781 seconds
DEBUG 01-15 16:09:01.808555.808555 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.809990.809990 cuda_h.py:19] end gpu_sexperts cost 0.0005221366882324219 seconds
DEBUG 01-15 16:09:01.809629.809629 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.809698.809698 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:09:01.809262.809262 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.809846.809846 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.810557.810557 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016078948974609375 seconds
DEBUG 01-15 16:09:01.811686.811686 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.811705.811705 cuda_h.py:19] end gpu_group_list cost 0.00032258033752441406 seconds
DEBUG 01-15 16:09:01.811651.811651 cuda_h.py:19] end group pad cost 0.002827167510986328 seconds
DEBUG 01-15 16:09:01.811156.811156 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:01.814632.814632 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.818328.818328 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0026433467864990234 seconds
DEBUG 01-15 16:09:01.818557.818557 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.819926.819926 cuda_h.py:19] end gpu_group_list cost 0.000782012939453125 seconds
DEBUG 01-15 16:09:01.823098.823098 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.823908.823908 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 562f6003-dcf0-424b-b753-b0e7c8dea914
DEBUG 01-15 16:09:01.831106.831106 cuda_h.py:19] end group_einsum cost 0.019172191619873047 seconds
DEBUG 01-15 16:09:01.831516.831516 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.835098.835098 cuda_h.py:19] end get_outputs_cpu1 cost 0.004266023635864258 seconds
DEBUG 01-15 16:09:01.836062.836062 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03526139259338379 seconds
INFO 01-15 16:09:01.838869.838869 client.py:127] Model loaded
DEBUG 01-15 16:09:01.838184.838184 cuda_h.py:19] end wait_experts_multi_device cost 0.01462554931640625 seconds
DEBUG 01-15 16:09:01.838047.838047 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.838977.838977 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0006468296051025391 seconds
DEBUG 01-15 16:09:01.838681.838681 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.840055.840055 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.840048.840048 cuda_h.py:19] end index_scatter cost 0.00010657310485839844 seconds
DEBUG 01-15 16:09:01.840986.840986 cuda_h.py:19] end cpuoutputsdeal cost 0.0015549659729003906 seconds
DEBUG 01-15 16:09:01.840294.840294 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.840341.840341 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.840812.840812 cuda_h.py:19] end gpu_group_tensor cost 0.0001747608184814453 seconds
DEBUG 01-15 16:09:01.840098.840098 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.841660.841660 cuda_h.py:19] end gpu_group_tensor cost 0.0001385211944580078 seconds
DEBUG 01-15 16:09:01.841895.841895 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.842793.842793 cuda_h.py:19] end gpu_group_einsum cost 0.001065969467163086 seconds
DEBUG 01-15 16:09:01.842891.842891 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.843129.843129 cuda_h.py:19] end gpu_group_einsum cost 0.0005402565002441406 seconds
DEBUG 01-15 16:09:01.843074.843074 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.843535.843535 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.843937.843937 cuda_h.py:19] end all_expert_outputs_slices cost 0.00028133392333984375 seconds
DEBUG 01-15 16:09:01.843521.843521 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.843445.843445 cuda_h.py:19] end concat_expert_out cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:01.843811.843811 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.843549.843549 cuda_h.py:19] end index_scatter cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:09:01.844565.844565 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009725093841552734 seconds
DEBUG 01-15 16:09:01.844171.844171 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.844988.844988 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.844300.844300 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001614093780517578 seconds
DEBUG 01-15 16:09:01.844440.844440 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.844939.844939 cuda_h.py:19] end concat_expert_out cost 5.650520324707031e-05 seconds
DEBUG 01-15 16:09:01.844829.844829 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.844567.844567 cuda_h.py:19] end index_scatter cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:09:01.844614.844614 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005354881286621094 seconds
DEBUG 01-15 16:09:01.844816.844816 cuda_h.py:19] end gpu_experts_multi_device cost 0.035704851150512695 seconds
DEBUG 01-15 16:09:01.845163.845163 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.04901933670043945 seconds
DEBUG 01-15 16:09:01.845139.845139 cuda_h.py:19] end prefill_layer cost 0.05727648735046387 seconds
DEBUG 01-15 16:09:01.845664.845664 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:09:01.845083.845083 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.845454.845454 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:09:01.845396.845396 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:09:01.845483.845483 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:09:01.845955.845955 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 3.9577484130859375e-05 seconds
DEBUG 01-15 16:09:01.845572.845572 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:09:01.845460.845460 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.845668.845668 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.846698.846698 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.846211.846211 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.846234.846234 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.846667.846667 cuda_h.py:19] end allocate_cuda_memory cost 0.00023698806762695312 seconds
DEBUG 01-15 16:09:01.846995.846995 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.846248.846248 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.846807.846807 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.846616.846616 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a41ac375-010e-4e7b-bb5d-db64a606de6c
DEBUG 01-15 16:09:01.846096.846096 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.847219.847219 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.848012.848012 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a41ac375-010e-4e7b-bb5d-db64a606de6c
DEBUG 01-15 16:09:01.848531.848531 cuda_h.py:19] end load_into_gpu_async cost 0.001506805419921875 seconds
DEBUG 01-15 16:09:01.848956.848956 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.848940.848940 cuda_h.py:19] end restore_tensors2 cost 9.465217590332031e-05 seconds
DEBUG 01-15 16:09:01.848471.848471 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021872520446777344 seconds
INFO 01-15 16:09:01.848408.848408 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a41ac375-010e-4e7b-bb5d-db64a606de6c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.850659.850659 cuda_h.py:19] end self_attn cost 0.0033805370330810547 seconds
DEBUG 01-15 16:09:01.850749.850749 cuda_h.py:19] end iln_self_attn_paln cost 0.0050258636474609375 seconds
DEBUG 01-15 16:09:01.850247.850247 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:09:01.850957.850957 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.851570.851570 cuda_h.py:19] end gate cost 0.0006649494171142578 seconds
DEBUG 01-15 16:09:01.851876.851876 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.852589.852589 lmp.py:1912] 
DEBUG 01-15 16:09:01.852589.852589 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.852219.852219 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.852300.852300 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.852896.852896 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.852824.852824 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.852798.852798 lmp.py:1917] 
DEBUG 01-15 16:09:01.852798.852798 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.852010.852010 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.852375.852375 lmp.py:1935]   Expert 13 |     27 | CPU
DEBUG 01-15 16:09:01.852018.852018 lmp.py:1935]   Expert 44 |     40 | CPU
DEBUG 01-15 16:09:01.852185.852185 lmp.py:1935]   Expert  9 |     42 | CPU
DEBUG 01-15 16:09:01.852397.852397 lmp.py:1935]   Expert 25 |     42 | CPU
DEBUG 01-15 16:09:01.852371.852371 lmp.py:1935]   Expert 16 |     48 | CPU
DEBUG 01-15 16:09:01.852345.852345 lmp.py:1935]   Expert 38 |     49 | CPU
DEBUG 01-15 16:09:01.852558.852558 lmp.py:1935]   Expert  2 |     53 | CPU
DEBUG 01-15 16:09:01.852055.852055 lmp.py:1935]   Expert 22 |     53 | CPU
DEBUG 01-15 16:09:01.852506.852506 lmp.py:1935]   Expert 33 |     59 | CPU
DEBUG 01-15 16:09:01.852480.852480 lmp.py:1935]   Expert 42 |     62 | CPU
DEBUG 01-15 16:09:01.852169.852169 lmp.py:1935]   Expert  5 |     63 | CPU
DEBUG 01-15 16:09:01.852772.852772 lmp.py:1935]   Expert 23 |     78 | CPU
DEBUG 01-15 16:09:01.852721.852721 lmp.py:1935]   Expert 24 |     81 | CPU
DEBUG 01-15 16:09:01.852456.852456 lmp.py:1935]   Expert 10 |     86 | CPU
DEBUG 01-15 16:09:01.852430.852430 lmp.py:1935]   Expert 59 |    102 | CPU
DEBUG 01-15 16:09:01.852404.852404 lmp.py:1935]   Expert 21 |    104 | CPU
DEBUG 01-15 16:09:01.852617.852617 lmp.py:1935]   Expert 55 |    114 | CPU
DEBUG 01-15 16:09:01.852352.852352 lmp.py:1935]   Expert 46 |    115 | CPU
DEBUG 01-15 16:09:01.852995.852995 lmp.py:1935]   Expert 45 |    119 | CPU
DEBUG 01-15 16:09:01.852638.852638 lmp.py:1935]   Expert 61 |    122 | CPU
DEBUG 01-15 16:09:01.852804.852804 lmp.py:1935]   Expert 31 |    128 | CPU
DEBUG 01-15 16:09:01.852017.852017 lmp.py:1935]   Expert 51 |    139 | CPU
DEBUG 01-15 16:09:01.852468.852468 lmp.py:1935]   Expert  6 |    142 | CPU
DEBUG 01-15 16:09:01.852203.852203 lmp.py:1935]   Expert 36 |    142 | CPU
DEBUG 01-15 16:09:01.852416.852416 lmp.py:1935]   Expert  8 |    147 | CPU
DEBUG 01-15 16:09:01.852774.852774 lmp.py:1935]   Expert 43 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852655.852655 lmp.py:1935]   Expert  3 |    151 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.852060.852060 lmp.py:1935]   Expert  0 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852418.852418 lmp.py:1935]   Expert 26 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.852061.852061 lmp.py:1935]   Expert 18 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852466.852466 lmp.py:1935]   Expert 48 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852394.852394 lmp.py:1935]   Expert 41 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.852798.852798 lmp.py:1935]   Expert 12 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.852964.852964 lmp.py:1935]   Expert  7 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852654.852654 lmp.py:1935]   Expert 20 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852820.852820 lmp.py:1935]   Expert 56 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.852747.852747 lmp.py:1935]   Expert 28 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852437.852437 lmp.py:1935]   Expert 34 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.852556.852556 lmp.py:1935]   Expert  1 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852199.852199 lmp.py:1935]   Expert 27 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.852366.852366 lmp.py:1935]   Expert 47 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.852293.852293 lmp.py:1935]   Expert 32 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852221.852221 lmp.py:1935]   Expert 11 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852149.852149 lmp.py:1935]   Expert 40 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.852315.852315 lmp.py:1935]   Expert 49 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.852243.852243 lmp.py:1935]   Expert 53 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853409.853409 lmp.py:1935]   Expert 63 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.853052.853052 lmp.py:1935]   Expert 15 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.853695.853695 lmp.py:1935]   Expert 29 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853622.853622 lmp.py:1935]   Expert 50 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853550.853550 lmp.py:1935]   Expert 30 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.853001.853001 lmp.py:1935]   Expert  4 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853690.853690 lmp.py:1935]   Expert 35 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.853380.853380 lmp.py:1935]   Expert 14 |    275 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853307.853307 lmp.py:1935]   Expert 37 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853142.853142 lmp.py:1935]   Expert 52 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.853262.853262 lmp.py:1935]   Expert 17 |    363 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.853667.853667 lmp.py:1935]   Expert 54 |    378 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853071.853071 lmp.py:1935]   Expert 39 |    387 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853191.853191 lmp.py:1935]   Expert 57 |    412 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.853311.853311 lmp.py:1935]   Expert 62 |    456 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853669.853669 lmp.py:1935]   Expert 60 |    458 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.853312.853312 lmp.py:1935]   Expert 19 |    545 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.853955.853955 lmp.py:1935]   Expert 58 |    571 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.853406.853406 lmp.py:1937] 
DEBUG 01-15 16:09:01.853406.853406 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.853811.853811 lmp.py:1938]   CPU:   2157 tokens
DEBUG 01-15 16:09:01.853930.853930 lmp.py:1942]   cuda:1:   4993 tokens (19 experts)
DEBUG 01-15 16:09:01.853573.853573 lmp.py:1942]   cuda:2:   5138 tokens (20 experts)
DEBUG 01-15 16:09:01.853739.853739 lmp.py:1943]   Total GPU:  10131 tokens
DEBUG 01-15 16:09:01.853906.853906 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.853906.853906 lmp.py:1944] 
DEBUG 01-15 16:09:01.853747.853747 cuda_h.py:19] end experts_map_get cost 0.0016684532165527344 seconds
DEBUG 01-15 16:09:01.853743.853743 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.853022.853022 lmp.py:1953] 
DEBUG 01-15 16:09:01.853022.853022 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.853720.853720 cuda_h.py:19] end cpu_experts_submit cost 5.6743621826171875e-05 seconds
DEBUG 01-15 16:09:01.853177.853177 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.853014.853014 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.855602.855602 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.3.gate_proj.weight': 0, 'model.layers.25.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.26.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.26.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.26.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.27.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.27.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.27.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.34.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.34.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.34.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.37.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.37.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.37.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.39.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.39.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.39.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.25.mlp.experts.0.gate_proj.weight': 0, 'model.layers.25.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.25.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.25.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.25.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(29487529984, 5767168, 0, 0), (29493297152, 5767168, 5767168, 0), (29481762816, 5767168, 11534336, 0), (29504831488, 5767168, 17301504, 0), (29510598656, 5767168, 23068672, 0), (29499064320, 5767168, 28835840, 0), (29643243520, 5767168, 34603008, 0), (29649010688, 5767168, 40370176, 0), (29637476352, 5767168, 46137344, 0), (29677846528, 5767168, 51904512, 0), (29683613696, 5767168, 57671680, 0), (29672079360, 5767168, 63438848, 0), (29885464576, 5767168, 69206016, 0), (29891231744, 5767168, 74973184, 0), (29879697408, 5767168, 80740352, 0), (29902766080, 5767168, 86507520, 0), (29908533248, 5767168, 92274688, 0), (29896998912, 5767168, 98041856, 0), (29937369088, 5767168, 103809024, 0), (29943136256, 5767168, 109576192, 0), (29931601920, 5767168, 115343360, 0), (30023876608, 5767168, 121110528, 0), (30029643776, 5767168, 126877696, 0), (30018109440, 5767168, 132644864, 0), (30075781120, 5767168, 138412032, 0), (30081548288, 5767168, 144179200, 0), (30070013952, 5767168, 149946368, 0), (30110384128, 5767168, 155713536, 0), (30116151296, 5767168, 161480704, 0), (30104616960, 5767168, 167247872, 0), (30127685632, 5767168, 173015040, 0), (30133452800, 5767168, 178782208, 0), (30121918464, 5767168, 184549376, 0), (30144987136, 5767168, 190316544, 0), (30150754304, 5767168, 196083712, 0), (30139219968, 5767168, 201850880, 0), (30248796160, 5767168, 207618048, 0), (30254563328, 5767168, 213385216, 0), (30243028992, 5767168, 219152384, 0), (30300700672, 5767168, 224919552, 0), (30306467840, 5767168, 230686720, 0), (30294933504, 5767168, 236453888, 0), (30352605184, 5767168, 242221056, 0), (30358372352, 5767168, 247988224, 0), (30346838016, 5767168, 253755392, 0), (30369906688, 5767168, 259522560, 0), (30375673856, 5767168, 265289728, 0), (30364139520, 5767168, 271056896, 0), (30404509696, 5767168, 276824064, 0), (30410276864, 5767168, 282591232, 0), (30398742528, 5767168, 288358400, 0), (30439112704, 5767168, 294125568, 0), (30444879872, 5767168, 299892736, 0), (30433345536, 5767168, 305659904, 0), (30508318720, 5767168, 311427072, 0), (30514085888, 5767168, 317194240, 0), (30502551552, 5767168, 322961408, 0)], 2: [(29435625472, 5767168, 0, 0), (29441392640, 5767168, 5767168, 0), (29429858304, 5767168, 11534336, 0), (29452926976, 5767168, 17301504, 0), (29458694144, 5767168, 23068672, 0), (29447159808, 5767168, 28835840, 0), (29556736000, 5767168, 34603008, 0), (29562503168, 5767168, 40370176, 0), (29550968832, 5767168, 46137344, 0), (29625942016, 5767168, 51904512, 0), (29631709184, 5767168, 57671680, 0), (29620174848, 5767168, 63438848, 0), (29695148032, 5767168, 69206016, 0), (29700915200, 5767168, 74973184, 0), (29689380864, 5767168, 80740352, 0), (29729751040, 5767168, 86507520, 0), (29735518208, 5767168, 92274688, 0), (29723983872, 5767168, 98041856, 0), (29747052544, 5767168, 103809024, 0), (29752819712, 5767168, 109576192, 0), (29741285376, 5767168, 115343360, 0), (29764354048, 5767168, 121110528, 0), (29770121216, 5767168, 126877696, 0), (29758586880, 5767168, 132644864, 0), (29781655552, 5767168, 138412032, 0), (29787422720, 5767168, 144179200, 0), (29775888384, 5767168, 149946368, 0), (29920067584, 5767168, 155713536, 0), (29925834752, 5767168, 161480704, 0), (29914300416, 5767168, 167247872, 0), (29954670592, 5767168, 173015040, 0), (29960437760, 5767168, 178782208, 0), (29948903424, 5767168, 184549376, 0), (29989273600, 5767168, 190316544, 0), (29995040768, 5767168, 196083712, 0), (29983506432, 5767168, 201850880, 0), (30041178112, 5767168, 207618048, 0), (30046945280, 5767168, 213385216, 0), (30035410944, 5767168, 219152384, 0), (30179590144, 5767168, 224919552, 0), (30185357312, 5767168, 230686720, 0), (30173822976, 5767168, 236453888, 0), (30266097664, 5767168, 242221056, 0), (30271864832, 5767168, 247988224, 0), (30260330496, 5767168, 253755392, 0), (30283399168, 5767168, 259522560, 0), (30289166336, 5767168, 265289728, 0), (30277632000, 5767168, 271056896, 0), (30335303680, 5767168, 276824064, 0), (30341070848, 5767168, 282591232, 0), (30329536512, 5767168, 288358400, 0), (30421811200, 5767168, 294125568, 0), (30427578368, 5767168, 299892736, 0), (30416044032, 5767168, 305659904, 0), (30473715712, 5767168, 311427072, 0), (30479482880, 5767168, 317194240, 0), (30467948544, 5767168, 322961408, 0), (30525620224, 5767168, 328728576, 0), (30531387392, 5767168, 334495744, 0), (30519853056, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74aa0434dec0>, 2: <capsule object NULL at 0x74a6bc4fe0d0>}
DEBUG 01-15 16:09:01.855637.855637 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.855387.855387 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.855998.855998 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d69e3304-0894-4318-b36a-52884879e097
DEBUG 01-15 16:09:01.856543.856543 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.856512.856512 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:01.856672.856672 client.py:127] Model loaded
DEBUG 01-15 16:09:01.856404.856404 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.856232.856232 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.856359.856359 cuda_h.py:19] end restore2model cost 0.0003859996795654297 seconds
DEBUG 01-15 16:09:01.856705.856705 cuda_h.py:19] end sllm_worker_task cost 0.010594606399536133 seconds
INFO 01-15 16:09:01.857505.857505 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d69e3304-0894-4318-b36a-52884879e097
DEBUG 01-15 16:09:01.857508.857508 cuda_h.py:19] end move_flatidxs cost 0.00086212158203125 seconds
DEBUG 01-15 16:09:01.857444.857444 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:01.857512.857512 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003987550735473633 seconds
DEBUG 01-15 16:09:01.857012.857012 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.860070.860070 cuda_h.py:19] end restore2model cost 0.0030977725982666016 seconds
DEBUG 01-15 16:09:01.860841.860841 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007348299026489258 seconds
DEBUG 01-15 16:09:01.861352.861352 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.861720.861720 cuda_h.py:19] end gpu_sexperts cost 0.0002720355987548828 seconds
DEBUG 01-15 16:09:01.861788.861788 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.861041.861041 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:09:01.861168.861168 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.861347.861347 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.862030.862030 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009624958038330078 seconds
DEBUG 01-15 16:09:01.862741.862741 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.862636.862636 cuda_h.py:19] end gpu_group_list cost 0.0002090930938720703 seconds
DEBUG 01-15 16:09:01.863056.863056 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.864115.864115 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011010169982910156 seconds
DEBUG 01-15 16:09:01.864548.864548 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.865556.865556 cuda_h.py:19] end gpu_group_list cost 0.00022101402282714844 seconds
DEBUG 01-15 16:09:01.865057.865057 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.865417.865417 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d69e3304-0894-4318-b36a-52884879e097
DEBUG 01-15 16:09:01.868892.868892 cuda_h.py:19] end group_tensors cost 0.010574102401733398 seconds
DEBUG 01-15 16:09:01.868822.868822 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.871839.871839 cuda_h.py:19] end group pad cost 0.0028405189514160156 seconds
DEBUG 01-15 16:09:01.871914.871914 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:01.895312.895312 client.py:127] Model loaded
DEBUG 01-15 16:09:01.895427.895427 cuda_h.py:19] end wait_experts_multi_device cost 0.029699325561523438 seconds
DEBUG 01-15 16:09:01.895843.895843 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.900365.900365 cuda_h.py:19] end group_einsum cost 0.02822136878967285 seconds
DEBUG 01-15 16:09:01.900145.900145 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.902365.902365 cuda_h.py:19] end get_outputs_cpu1 cost 0.0025649070739746094 seconds
DEBUG 01-15 16:09:01.903646.903646 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04764199256896973 seconds
DEBUG 01-15 16:09:01.904630.904630 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008755207061767578 seconds
DEBUG 01-15 16:09:01.904164.904164 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.905204.905204 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.905832.905832 cuda_h.py:19] end index_scatter cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:09:01.906928.906928 cuda_h.py:19] end cpuoutputsdeal cost 0.0014376640319824219 seconds
DEBUG 01-15 16:09:01.906222.906222 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.906555.906555 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.906938.906938 cuda_h.py:19] end gpu_group_tensor cost 0.00014662742614746094 seconds
DEBUG 01-15 16:09:01.906794.906794 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.906779.906779 cuda_h.py:19] end gpu_group_tensor cost 0.00013518333435058594 seconds
DEBUG 01-15 16:09:01.906869.906869 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.907640.907640 cuda_h.py:19] end gpu_group_einsum cost 0.0006241798400878906 seconds
DEBUG 01-15 16:09:01.907638.907638 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.908908.908908 cuda_h.py:19] end gpu_group_einsum cost 0.0004897117614746094 seconds
DEBUG 01-15 16:09:01.908720.908720 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.908797.908797 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.908689.908689 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025725364685058594 seconds
DEBUG 01-15 16:09:01.908518.908518 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.908965.908965 cuda_h.py:19] end concat_expert_out cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:01.909092.909092 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.909923.909923 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:01.909189.909189 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008864402770996094 seconds
DEBUG 01-15 16:09:01.909212.909212 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.909201.909201 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.909306.909306 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015020370483398438 seconds
DEBUG 01-15 16:09:01.909301.909301 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.909277.909277 cuda_h.py:19] end concat_expert_out cost 5.698204040527344e-05 seconds
DEBUG 01-15 16:09:01.909928.909928 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.909328.909328 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:09:01.909184.909184 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005044937133789062 seconds
DEBUG 01-15 16:09:01.910762.910762 cuda_h.py:19] end gpu_experts_multi_device cost 0.04859042167663574 seconds
DEBUG 01-15 16:09:01.910103.910103 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.05913376808166504 seconds
DEBUG 01-15 16:09:01.910573.910573 cuda_h.py:19] end prefill_layer cost 0.0648660659790039 seconds
DEBUG 01-15 16:09:01.910641.910641 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:09:01.910583.910583 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.910286.910286 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:09:01.910465.910465 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:09:01.910791.910791 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:09:01.910171.910171 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 4.220008850097656e-05 seconds
DEBUG 01-15 16:09:01.910734.910734 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 7.414817810058594e-05 seconds
DEBUG 01-15 16:09:01.910623.910623 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.910373.910373 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:01.910933.910933 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:01.911074.911074 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:01.911053.911053 cuda_h.py:19] end allocate_cuda_memory cost 0.0003261566162109375 seconds
DEBUG 01-15 16:09:01.911872.911872 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.911191.911191 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:01.911447.911447 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.911522.911522 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.911569.911569 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e606f4e7-d41a-48d3-97e4-2fbd93ff6377
DEBUG 01-15 16:09:01.911335.911335 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.912332.912332 cuda_h.py:10] start self_attn
INFO 01-15 16:09:01.913857.913857 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e606f4e7-d41a-48d3-97e4-2fbd93ff6377
DEBUG 01-15 16:09:01.913237.913237 cuda_h.py:19] end load_into_gpu_async cost 0.001636505126953125 seconds
DEBUG 01-15 16:09:01.913900.913900 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:01.913348.913348 cuda_h.py:19] end restore_tensors2 cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:09:01.913402.913402 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0025141239166259766 seconds
INFO 01-15 16:09:01.913352.913352 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e606f4e7-d41a-48d3-97e4-2fbd93ff6377
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.915297.915297 cuda_h.py:19] end self_attn cost 0.003237009048461914 seconds
DEBUG 01-15 16:09:01.915533.915533 cuda_h.py:19] end iln_self_attn_paln cost 0.004992246627807617 seconds
DEBUG 01-15 16:09:01.915985.915985 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:09:01.915132.915132 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.916713.916713 cuda_h.py:19] end gate cost 0.0007042884826660156 seconds
DEBUG 01-15 16:09:01.916218.916218 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.917113.917113 lmp.py:1912] 
DEBUG 01-15 16:09:01.917113.917113 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.917021.917021 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.917247.917247 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.917990.917990 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.917348.917348 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.917991.917991 lmp.py:1917] 
DEBUG 01-15 16:09:01.917991.917991 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.917826.917826 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.917622.917622 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:09:01.917457.917457 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:09:01.917292.917292 lmp.py:1935]   Expert 11 |     27 | CPU
DEBUG 01-15 16:09:01.917127.917127 lmp.py:1935]   Expert  7 |     39 | CPU
DEBUG 01-15 16:09:01.917770.917770 lmp.py:1935]   Expert 62 |     42 | CPU
DEBUG 01-15 16:09:01.917174.917174 lmp.py:1935]   Expert 51 |     44 | CPU
DEBUG 01-15 16:09:01.917056.917056 lmp.py:1935]   Expert  3 |     47 | CPU
DEBUG 01-15 16:09:01.917699.917699 lmp.py:1935]   Expert 30 |     51 | CPU
DEBUG 01-15 16:09:01.917342.917342 lmp.py:1935]   Expert 29 |     55 | CPU
DEBUG 01-15 16:09:01.917746.917746 lmp.py:1935]   Expert 17 |     56 | CPU
DEBUG 01-15 16:09:01.917674.917674 lmp.py:1935]   Expert  6 |     58 | CPU
DEBUG 01-15 16:09:01.917317.917317 lmp.py:1935]   Expert  9 |     65 | CPU
DEBUG 01-15 16:09:01.917033.917033 lmp.py:1935]   Expert 63 |     74 | CPU
DEBUG 01-15 16:09:01.917438.917438 lmp.py:1935]   Expert 38 |     76 | CPU
DEBUG 01-15 16:09:01.917081.917081 lmp.py:1935]   Expert 55 |     83 | CPU
DEBUG 01-15 16:09:01.917247.917247 lmp.py:1935]   Expert 59 |     85 | CPU
DEBUG 01-15 16:09:01.917175.917175 lmp.py:1935]   Expert 48 |     89 | CPU
DEBUG 01-15 16:09:01.917102.917102 lmp.py:1935]   Expert  8 |     95 | CPU
DEBUG 01-15 16:09:01.917553.917553 lmp.py:1935]   Expert 19 |     95 | CPU
DEBUG 01-15 16:09:01.917243.917243 lmp.py:1935]   Expert 22 |    101 | CPU
DEBUG 01-15 16:09:01.917694.917694 lmp.py:1935]   Expert 49 |    104 | CPU
DEBUG 01-15 16:09:01.917621.917621 lmp.py:1935]   Expert 24 |    109 | CPU
DEBUG 01-15 16:09:01.917787.917787 lmp.py:1935]   Expert 36 |    113 | CPU
DEBUG 01-15 16:09:01.917192.917192 lmp.py:1935]   Expert 34 |    116 | CPU
DEBUG 01-15 16:09:01.917358.917358 lmp.py:1935]   Expert 42 |    117 | CPU
DEBUG 01-15 16:09:01.917670.917670 lmp.py:1935]   Expert 50 |    121 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.917982.917982 lmp.py:1935]   Expert 39 |    125 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.917863.917863 lmp.py:1935]   Expert  4 |    133 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.917745.917745 lmp.py:1935]   Expert 37 |    141 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.917864.917864 lmp.py:1935]   Expert 41 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.917223.917223 lmp.py:1935]   Expert 15 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.917104.917104 lmp.py:1935]   Expert 23 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.917701.917701 lmp.py:1935]   Expert 56 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.917549.917549 lmp.py:1935]   Expert 60 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.917431.917431 lmp.py:1935]   Expert 16 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.917835.917835 lmp.py:1935]   Expert 44 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.917763.917763 lmp.py:1935]   Expert  1 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.917406.917406 lmp.py:1935]   Expert 21 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918334.918334 lmp.py:1935]   Expert 43 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918738.918738 lmp.py:1935]   Expert 47 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918143.918143 lmp.py:1935]   Expert 53 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918024.918024 lmp.py:1935]   Expert 12 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918906.918906 lmp.py:1935]   Expert 33 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918549.918549 lmp.py:1935]   Expert 13 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918953.918953 lmp.py:1935]   Expert 32 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918119.918119 lmp.py:1935]   Expert 28 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918239.918239 lmp.py:1935]   Expert  0 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918644.918644 lmp.py:1935]   Expert 31 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918571.918571 lmp.py:1935]   Expert 26 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918738.918738 lmp.py:1935]   Expert 54 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918142.918142 lmp.py:1935]   Expert 10 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918785.918785 lmp.py:1935]   Expert 18 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918190.918190 lmp.py:1935]   Expert 57 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918071.918071 lmp.py:1935]   Expert  2 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918429.918429 lmp.py:1935]   Expert 58 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918834.918834 lmp.py:1935]   Expert 40 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918715.918715 lmp.py:1935]   Expert 45 |    362 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918405.918405 lmp.py:1935]   Expert 25 |    365 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918571.918571 lmp.py:1935]   Expert  5 |    441 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918975.918975 lmp.py:1935]   Expert 35 |    467 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918618.918618 lmp.py:1935]   Expert 27 |    486 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918784.918784 lmp.py:1935]   Expert 46 |    552 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918904.918904 lmp.py:1935]   Expert 52 |    599 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.918786.918786 lmp.py:1935]   Expert 14 |    890 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.918998.918998 lmp.py:1937] 
DEBUG 01-15 16:09:01.918998.918998 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.918164.918164 lmp.py:1938]   CPU:   1763 tokens
DEBUG 01-15 16:09:01.918569.918569 lmp.py:1942]   cuda:1:   5246 tokens (19 experts)
DEBUG 01-15 16:09:01.918735.918735 lmp.py:1942]   cuda:2:   5279 tokens (20 experts)
DEBUG 01-15 16:09:01.918947.918947 lmp.py:1943]   Total GPU:  10525 tokens
DEBUG 01-15 16:09:01.918637.918637 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.918637.918637 lmp.py:1944] 
DEBUG 01-15 16:09:01.918571.918571 cuda_h.py:19] end experts_map_get cost 0.0018086433410644531 seconds
DEBUG 01-15 16:09:01.918136.918136 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.918985.918985 lmp.py:1953] 
DEBUG 01-15 16:09:01.918985.918985 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.918722.918722 cuda_h.py:19] end cpu_experts_submit cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:01.918749.918749 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.918201.918201 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.920068.920068 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.58.up_proj.weight': 322961408}, 2: {'model.layers.26.mlp.experts.2.gate_proj.weight': 0, 'model.layers.26.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.54.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.54.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.54.up_proj.weight': 322961408, 'model.layers.26.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.26.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.26.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (30560223232, 5767168, 17301504, 0), (30565990400, 5767168, 23068672, 0), (30554456064, 5767168, 28835840, 0), (30629429248, 5767168, 34603008, 0), (30635196416, 5767168, 40370176, 0), (30623662080, 5767168, 46137344, 0), (30715936768, 5767168, 51904512, 0), (30721703936, 5767168, 57671680, 0), (30710169600, 5767168, 63438848, 0), (30785142784, 5767168, 69206016, 0), (30790909952, 5767168, 74973184, 0), (30779375616, 5767168, 80740352, 0), (30802444288, 5767168, 86507520, 0), (30808211456, 5767168, 92274688, 0), (30796677120, 5767168, 98041856, 0), (30819745792, 5767168, 103809024, 0), (30825512960, 5767168, 109576192, 0), (30813978624, 5767168, 115343360, 0), (30992760832, 5767168, 121110528, 0), (30998528000, 5767168, 126877696, 0), (30986993664, 5767168, 132644864, 0), (31010062336, 5767168, 138412032, 0), (31015829504, 5767168, 144179200, 0), (31004295168, 5767168, 149946368, 0), (31096569856, 5767168, 155713536, 0), (31102337024, 5767168, 161480704, 0), (31090802688, 5767168, 167247872, 0), (31113871360, 5767168, 173015040, 0), (31119638528, 5767168, 178782208, 0), (31108104192, 5767168, 184549376, 0), (31183077376, 5767168, 190316544, 0), (31188844544, 5767168, 196083712, 0), (31177310208, 5767168, 201850880, 0), (31217680384, 5767168, 207618048, 0), (31223447552, 5767168, 213385216, 0), (31211913216, 5767168, 219152384, 0), (31286886400, 5767168, 224919552, 0), (31292653568, 5767168, 230686720, 0), (31281119232, 5767168, 236453888, 0), (31321489408, 5767168, 242221056, 0), (31327256576, 5767168, 247988224, 0), (31315722240, 5767168, 253755392, 0), (31459901440, 5767168, 259522560, 0), (31465668608, 5767168, 265289728, 0), (31454134272, 5767168, 271056896, 0), (31511805952, 5767168, 276824064, 0), (31517573120, 5767168, 282591232, 0), (31506038784, 5767168, 288358400, 0), (31529107456, 5767168, 294125568, 0), (31534874624, 5767168, 299892736, 0), (31523340288, 5767168, 305659904, 0), (31546408960, 5767168, 311427072, 0), (31552176128, 5767168, 317194240, 0), (31540641792, 5767168, 322961408, 0)], 2: [(30577524736, 5767168, 0, 0), (30583291904, 5767168, 5767168, 0), (30571757568, 5767168, 11534336, 0), (30612127744, 5767168, 17301504, 0), (30617894912, 5767168, 23068672, 0), (30606360576, 5767168, 28835840, 0), (30750539776, 5767168, 34603008, 0), (30756306944, 5767168, 40370176, 0), (30744772608, 5767168, 46137344, 0), (30767841280, 5767168, 51904512, 0), (30773608448, 5767168, 57671680, 0), (30762074112, 5767168, 63438848, 0), (30854348800, 5767168, 69206016, 0), (30860115968, 5767168, 74973184, 0), (30848581632, 5767168, 80740352, 0), (30906253312, 5767168, 86507520, 0), (30912020480, 5767168, 92274688, 0), (30900486144, 5767168, 98041856, 0), (30940856320, 5767168, 103809024, 0), (30946623488, 5767168, 109576192, 0), (30935089152, 5767168, 115343360, 0), (30975459328, 5767168, 121110528, 0), (30981226496, 5767168, 126877696, 0), (30969692160, 5767168, 132644864, 0), (31027363840, 5767168, 138412032, 0), (31033131008, 5767168, 144179200, 0), (31021596672, 5767168, 149946368, 0), (31079268352, 5767168, 155713536, 0), (31085035520, 5767168, 161480704, 0), (31073501184, 5767168, 167247872, 0), (31148474368, 5767168, 173015040, 0), (31154241536, 5767168, 178782208, 0), (31142707200, 5767168, 184549376, 0), (31234981888, 5767168, 190316544, 0), (31240749056, 5767168, 196083712, 0), (31229214720, 5767168, 201850880, 0), (31252283392, 5767168, 207618048, 0), (31258050560, 5767168, 213385216, 0), (31246516224, 5767168, 219152384, 0), (31304187904, 5767168, 224919552, 0), (31309955072, 5767168, 230686720, 0), (31298420736, 5767168, 236453888, 0), (31338790912, 5767168, 242221056, 0), (31344558080, 5767168, 247988224, 0), (31333023744, 5767168, 253755392, 0), (31356092416, 5767168, 259522560, 0), (31361859584, 5767168, 265289728, 0), (31350325248, 5767168, 271056896, 0), (31407996928, 5767168, 276824064, 0), (31413764096, 5767168, 282591232, 0), (31402229760, 5767168, 288358400, 0), (31442599936, 5767168, 294125568, 0), (31448367104, 5767168, 299892736, 0), (31436832768, 5767168, 305659904, 0), (31477202944, 5767168, 311427072, 0), (31482970112, 5767168, 317194240, 0), (31471435776, 5767168, 322961408, 0), (31581011968, 5767168, 328728576, 0), (31586779136, 5767168, 334495744, 0), (31575244800, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b34fffdec0>, 2: <capsule object NULL at 0x74a6bc4fe160>}
DEBUG 01-15 16:09:01.920942.920942 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.921686.921686 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.921296.921296 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 02a9f7a2-635f-4ad0-8926-0834921707b0
DEBUG 01-15 16:09:01.921170.921170 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.921873.921873 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.921984.921984 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:01.921226.921226 client.py:127] Model loaded
DEBUG 01-15 16:09:01.921242.921242 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.922370.922370 cuda_h.py:19] end restore2model cost 0.0006034374237060547 seconds
DEBUG 01-15 16:09:01.922133.922133 cuda_h.py:19] end move_flatidxs cost 0.0008323192596435547 seconds
DEBUG 01-15 16:09:01.922339.922339 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:01.922022.922022 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 02a9f7a2-635f-4ad0-8926-0834921707b0
DEBUG 01-15 16:09:01.922839.922839 cuda_h.py:19] end sllm_worker_task cost 0.01145172119140625 seconds
DEBUG 01-15 16:09:01.922278.922278 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004057645797729492 seconds
DEBUG 01-15 16:09:01.923554.923554 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.926730.926730 cuda_h.py:19] end restore2model cost 0.0030438899993896484 seconds
DEBUG 01-15 16:09:01.926719.926719 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007400035858154297 seconds
DEBUG 01-15 16:09:01.926250.926250 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.926910.926910 cuda_h.py:19] end gpu_sexperts cost 0.0002753734588623047 seconds
DEBUG 01-15 16:09:01.926547.926547 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:01.926370.926370 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:09:01.926066.926066 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.926007.926007 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.927345.927345 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009551048278808594 seconds
DEBUG 01-15 16:09:01.927387.927387 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.927799.927799 cuda_h.py:19] end gpu_group_list cost 0.00020432472229003906 seconds
DEBUG 01-15 16:09:01.928979.928979 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.929347.929347 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010526180267333984 seconds
DEBUG 01-15 16:09:01.929774.929774 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.930960.930960 cuda_h.py:19] end gpu_group_list cost 0.0002129077911376953 seconds
DEBUG 01-15 16:09:01.930203.930203 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.930986.930986 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 02a9f7a2-635f-4ad0-8926-0834921707b0
DEBUG 01-15 16:09:01.931114.931114 cuda_h.py:19] end group_tensors cost 0.009240865707397461 seconds
DEBUG 01-15 16:09:01.932088.932088 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.935147.935147 cuda_h.py:19] end group pad cost 0.0026903152465820312 seconds
DEBUG 01-15 16:09:01.935983.935983 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:01.954985.954985 cuda_h.py:19] end group_einsum cost 0.01915144920349121 seconds
DEBUG 01-15 16:09:01.954594.954594 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:01.956088.956088 cuda_h.py:19] end get_outputs_cpu1 cost 0.002235889434814453 seconds
DEBUG 01-15 16:09:01.957390.957390 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.036577701568603516 seconds
INFO 01-15 16:09:01.958358.958358 client.py:127] Model loaded
DEBUG 01-15 16:09:01.958435.958435 cuda_h.py:19] end wait_experts_multi_device cost 0.027636051177978516 seconds
DEBUG 01-15 16:09:01.958674.958674 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:01.959938.959938 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005071163177490234 seconds
DEBUG 01-15 16:09:01.959966.959966 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:01.960163.960163 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.960585.960585 cuda_h.py:19] end index_scatter cost 7.319450378417969e-05 seconds
DEBUG 01-15 16:09:01.960356.960356 cuda_h.py:19] end cpuoutputsdeal cost 0.0013382434844970703 seconds
DEBUG 01-15 16:09:01.960273.960273 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:01.960082.960082 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.960850.960850 cuda_h.py:19] end gpu_group_tensor cost 0.000148773193359375 seconds
DEBUG 01-15 16:09:01.960659.960659 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:01.961446.961446 cuda_h.py:19] end gpu_group_tensor cost 0.0001304149627685547 seconds
DEBUG 01-15 16:09:01.961866.961866 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.961940.961940 cuda_h.py:19] end gpu_group_einsum cost 0.000568389892578125 seconds
DEBUG 01-15 16:09:01.961792.961792 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:01.962009.962009 cuda_h.py:19] end gpu_group_einsum cost 0.0004868507385253906 seconds
DEBUG 01-15 16:09:01.962470.962470 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.962964.962964 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.963835.963835 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024318695068359375 seconds
DEBUG 01-15 16:09:01.963651.963651 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.963164.963164 cuda_h.py:19] end concat_expert_out cost 6.008148193359375e-05 seconds
DEBUG 01-15 16:09:01.963053.963053 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.963977.963977 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:09:01.963395.963395 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008754730224609375 seconds
DEBUG 01-15 16:09:01.963226.963226 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:01.963261.963261 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:01.963506.963506 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014853477478027344 seconds
DEBUG 01-15 16:09:01.963884.963884 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:01.964477.964477 cuda_h.py:19] end concat_expert_out cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:09:01.964081.964081 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:01.964144.964144 cuda_h.py:19] end index_scatter cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:09:01.964761.964761 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004990100860595703 seconds
DEBUG 01-15 16:09:01.964717.964717 cuda_h.py:19] end gpu_experts_multi_device cost 0.03760576248168945 seconds
DEBUG 01-15 16:09:01.964673.964673 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.04840207099914551 seconds
DEBUG 01-15 16:09:01.964336.964336 cuda_h.py:19] end prefill_layer cost 0.05410480499267578 seconds
DEBUG 01-15 16:09:01.964192.964192 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:09:01.964849.964849 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:01.964505.964505 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:09:01.964924.964924 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:01.964125.964125 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:01.965301.965301 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:01.968665.968665 cuda_h.py:19] end self_attn cost 0.002706289291381836 seconds
DEBUG 01-15 16:09:01.968801.968801 cuda_h.py:19] end iln_self_attn_paln cost 0.0034236907958984375 seconds
DEBUG 01-15 16:09:01.968949.968949 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:09:01.968096.968096 cuda_h.py:10] start gate
DEBUG 01-15 16:09:01.969325.969325 cuda_h.py:19] end gate cost 0.0006558895111083984 seconds
DEBUG 01-15 16:09:01.969923.969923 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:01.969287.969287 lmp.py:1912] 
DEBUG 01-15 16:09:01.969287.969287 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:01.969858.969858 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:01.969322.969322 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:01.969164.969164 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:01.969622.969622 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:01.969887.969887 lmp.py:1917] 
DEBUG 01-15 16:09:01.969887.969887 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:01.969584.969584 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:01.969194.969194 lmp.py:1935]   Expert 18 |     65 | CPU
DEBUG 01-15 16:09:01.969890.969890 lmp.py:1935]   Expert 54 |     71 | CPU
DEBUG 01-15 16:09:01.969632.969632 lmp.py:1935]   Expert 47 |     74 | CPU
DEBUG 01-15 16:09:01.969136.969136 lmp.py:1935]   Expert 23 |     76 | CPU
DEBUG 01-15 16:09:01.969071.969071 lmp.py:1935]   Expert 48 |     79 | CPU
DEBUG 01-15 16:09:01.969005.969005 lmp.py:1935]   Expert 44 |     84 | CPU
DEBUG 01-15 16:09:01.969748.969748 lmp.py:1935]   Expert 45 |     84 | CPU
DEBUG 01-15 16:09:01.969729.969729 lmp.py:1935]   Expert 20 |     91 | CPU
DEBUG 01-15 16:09:01.969994.969994 lmp.py:1935]   Expert 31 |     99 | CPU
DEBUG 01-15 16:09:01.969498.969498 lmp.py:1935]   Expert 36 |    104 | CPU
DEBUG 01-15 16:09:01.970764.970764 lmp.py:1935]   Expert 61 |    108 | CPU
DEBUG 01-15 16:09:01.970506.970506 lmp.py:1935]   Expert 33 |    119 | CPU
DEBUG 01-15 16:09:01.970010.970010 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:09:01.970276.970276 lmp.py:1935]   Expert 24 |    121 | CPU
DEBUG 01-15 16:09:01.970733.970733 lmp.py:1935]   Expert 42 |    121 | CPU
DEBUG 01-15 16:09:01.970191.970191 lmp.py:1935]   Expert 11 |    122 | CPU
DEBUG 01-15 16:09:01.970695.970695 lmp.py:1935]   Expert 43 |    123 | CPU
DEBUG 01-15 16:09:01.970199.970199 lmp.py:1935]   Expert 49 |    127 | CPU
DEBUG 01-15 16:09:01.970465.970465 lmp.py:1935]   Expert 56 |    129 | CPU
DEBUG 01-15 16:09:01.970969.970969 lmp.py:1935]   Expert  6 |    138 | CPU
DEBUG 01-15 16:09:01.970996.970996 lmp.py:1935]   Expert 51 |    144 | CPU
DEBUG 01-15 16:09:01.970500.970500 lmp.py:1935]   Expert  0 |    150 | CPU
DEBUG 01-15 16:09:01.970289.970289 lmp.py:1935]   Expert 17 |    152 | CPU
DEBUG 01-15 16:09:01.970554.970554 lmp.py:1935]   Expert 12 |    154 | CPU
DEBUG 01-15 16:09:01.970773.970773 lmp.py:1935]   Expert 40 |    155 | CPU
DEBUG 01-15 16:09:01.970377.970377 lmp.py:1935]   Expert  5 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970550.970550 lmp.py:1935]   Expert 55 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970769.970769 lmp.py:1935]   Expert 57 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970988.970988 lmp.py:1935]   Expert 59 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970208.970208 lmp.py:1935]   Expert 26 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970188.970188 lmp.py:1935]   Expert 38 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970884.970884 lmp.py:1935]   Expert 46 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970581.970581 lmp.py:1935]   Expert 13 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970800.970800 lmp.py:1935]   Expert 35 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970019.970019 lmp.py:1935]   Expert 58 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970715.970715 lmp.py:1935]   Expert 30 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970934.970934 lmp.py:1935]   Expert  7 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970392.970392 lmp.py:1935]   Expert 50 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970373.970373 lmp.py:1935]   Expert 16 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970546.970546 lmp.py:1935]   Expert 15 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970480.970480 lmp.py:1935]   Expert 32 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970938.970938 lmp.py:1935]   Expert 14 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970919.970919 lmp.py:1935]   Expert  1 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970900.970900 lmp.py:1935]   Expert  3 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970880.970880 lmp.py:1935]   Expert  4 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970861.970861 lmp.py:1935]   Expert 34 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970081.970081 lmp.py:1935]   Expert 39 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970730.970730 lmp.py:1935]   Expert 28 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970142.970142 lmp.py:1935]   Expert 52 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970553.970553 lmp.py:1935]   Expert 25 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970203.970203 lmp.py:1935]   Expert 22 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970614.970614 lmp.py:1935]   Expert  2 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970264.970264 lmp.py:1935]   Expert 41 |    280 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970437.970437 lmp.py:1935]   Expert 21 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.970087.970087 lmp.py:1935]   Expert 60 |    284 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.970736.970736 lmp.py:1935]   Expert 63 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.971671.971671 lmp.py:1935]   Expert 29 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.971890.971890 lmp.py:1935]   Expert 62 |    298 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.971871.971871 lmp.py:1935]   Expert 27 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.971090.971090 lmp.py:1935]   Expert  8 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.971548.971548 lmp.py:1935]   Expert 37 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.971721.971721 lmp.py:1935]   Expert 53 |    337 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.971371.971371 lmp.py:1935]   Expert 19 |    442 | GPU2(cuda:2)
DEBUG 01-15 16:09:01.971590.971590 lmp.py:1935]   Expert  9 |    616 | GPU1(cuda:1)
DEBUG 01-15 16:09:01.971617.971617 lmp.py:1937] 
DEBUG 01-15 16:09:01.971617.971617 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:01.971121.971121 lmp.py:1938]   CPU:   2811 tokens
DEBUG 01-15 16:09:01.971340.971340 lmp.py:1942]   cuda:1:   4684 tokens (19 experts)
DEBUG 01-15 16:09:01.971513.971513 lmp.py:1942]   cuda:2:   4793 tokens (20 experts)
DEBUG 01-15 16:09:01.971971.971971 lmp.py:1943]   Total GPU:   9477 tokens
DEBUG 01-15 16:09:01.971713.971713 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:01.971713.971713 lmp.py:1944] 
DEBUG 01-15 16:09:01.971085.971085 cuda_h.py:19] end experts_map_get cost 0.0020551681518554688 seconds
DEBUG 01-15 16:09:01.971928.971928 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:01.971492.971492 lmp.py:1953] 
DEBUG 01-15 16:09:01.971492.971492 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:01.971375.971375 cuda_h.py:19] end cpu_experts_submit cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:01.971309.971309 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:01.971669.971669 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:01.972646.972646 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.1.gate_proj.weight': 0, 'model.layers.27.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.38.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.38.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.38.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.39.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.39.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.39.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.60.up_proj.weight': 322961408}, 2: {'model.layers.27.mlp.experts.2.gate_proj.weight': 0, 'model.layers.27.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.27.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.27.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.27.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(31667519488, 5767168, 0, 0), (31673286656, 5767168, 5767168, 0), (31661752320, 5767168, 11534336, 0), (31719424000, 5767168, 17301504, 0), (31725191168, 5767168, 23068672, 0), (31713656832, 5767168, 28835840, 0), (31771328512, 5767168, 34603008, 0), (31777095680, 5767168, 40370176, 0), (31765561344, 5767168, 46137344, 0), (31788630016, 5767168, 51904512, 0), (31794397184, 5767168, 57671680, 0), (31782862848, 5767168, 63438848, 0), (31805931520, 5767168, 69206016, 0), (31811698688, 5767168, 74973184, 0), (31800164352, 5767168, 80740352, 0), (31875137536, 5767168, 86507520, 0), (31880904704, 5767168, 92274688, 0), (31869370368, 5767168, 98041856, 0), (31927042048, 5767168, 103809024, 0), (31932809216, 5767168, 109576192, 0), (31921274880, 5767168, 115343360, 0), (32030851072, 5767168, 121110528, 0), (32036618240, 5767168, 126877696, 0), (32025083904, 5767168, 132644864, 0), (32117358592, 5767168, 138412032, 0), (32123125760, 5767168, 144179200, 0), (32111591424, 5767168, 149946368, 0), (32151961600, 5767168, 155713536, 0), (32157728768, 5767168, 161480704, 0), (32146194432, 5767168, 167247872, 0), (32203866112, 5767168, 173015040, 0), (32209633280, 5767168, 178782208, 0), (32198098944, 5767168, 184549376, 0), (32255770624, 5767168, 190316544, 0), (32261537792, 5767168, 196083712, 0), (32250003456, 5767168, 201850880, 0), (32307675136, 5767168, 207618048, 0), (32313442304, 5767168, 213385216, 0), (32301907968, 5767168, 219152384, 0), (32324976640, 5767168, 224919552, 0), (32330743808, 5767168, 230686720, 0), (32319209472, 5767168, 236453888, 0), (32359579648, 5767168, 242221056, 0), (32365346816, 5767168, 247988224, 0), (32353812480, 5767168, 253755392, 0), (32549896192, 5767168, 259522560, 0), (32555663360, 5767168, 265289728, 0), (32544129024, 5767168, 271056896, 0), (32601800704, 5767168, 276824064, 0), (32607567872, 5767168, 282591232, 0), (32596033536, 5767168, 288358400, 0), (32671006720, 5767168, 294125568, 0), (32676773888, 5767168, 299892736, 0), (32665239552, 5767168, 305659904, 0), (32688308224, 5767168, 311427072, 0), (32694075392, 5767168, 317194240, 0), (32682541056, 5767168, 322961408, 0)], 2: [(31684820992, 5767168, 0, 0), (31690588160, 5767168, 5767168, 0), (31679053824, 5767168, 11534336, 0), (31702122496, 5767168, 17301504, 0), (31707889664, 5767168, 23068672, 0), (31696355328, 5767168, 28835840, 0), (31736725504, 5767168, 34603008, 0), (31742492672, 5767168, 40370176, 0), (31730958336, 5767168, 46137344, 0), (31892439040, 5767168, 51904512, 0), (31898206208, 5767168, 57671680, 0), (31886671872, 5767168, 63438848, 0), (31909740544, 5767168, 69206016, 0), (31915507712, 5767168, 74973184, 0), (31903973376, 5767168, 80740352, 0), (31978946560, 5767168, 86507520, 0), (31984713728, 5767168, 92274688, 0), (31973179392, 5767168, 98041856, 0), (32013549568, 5767168, 103809024, 0), (32019316736, 5767168, 109576192, 0), (32007782400, 5767168, 115343360, 0), (32082755584, 5767168, 121110528, 0), (32088522752, 5767168, 126877696, 0), (32076988416, 5767168, 132644864, 0), (32100057088, 5767168, 138412032, 0), (32105824256, 5767168, 144179200, 0), (32094289920, 5767168, 149946368, 0), (32134660096, 5767168, 155713536, 0), (32140427264, 5767168, 161480704, 0), (32128892928, 5767168, 167247872, 0), (32169263104, 5767168, 173015040, 0), (32175030272, 5767168, 178782208, 0), (32163495936, 5767168, 184549376, 0), (32238469120, 5767168, 190316544, 0), (32244236288, 5767168, 196083712, 0), (32232701952, 5767168, 201850880, 0), (32290373632, 5767168, 207618048, 0), (32296140800, 5767168, 213385216, 0), (32284606464, 5767168, 219152384, 0), (32446087168, 5767168, 224919552, 0), (32451854336, 5767168, 230686720, 0), (32440320000, 5767168, 236453888, 0), (32515293184, 5767168, 242221056, 0), (32521060352, 5767168, 247988224, 0), (32509526016, 5767168, 253755392, 0), (32567197696, 5767168, 259522560, 0), (32572964864, 5767168, 265289728, 0), (32561430528, 5767168, 271056896, 0), (32636403712, 5767168, 276824064, 0), (32642170880, 5767168, 282591232, 0), (32630636544, 5767168, 288358400, 0), (32653705216, 5767168, 294125568, 0), (32659472384, 5767168, 299892736, 0), (32647938048, 5767168, 305659904, 0), (32722911232, 5767168, 311427072, 0), (32728678400, 5767168, 317194240, 0), (32717144064, 5767168, 322961408, 0), (32740212736, 5767168, 328728576, 0), (32745979904, 5767168, 334495744, 0), (32734445568, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8140d1ec0>, 2: <capsule object NULL at 0x74aa107da160>}
DEBUG 01-15 16:09:01.972434.972434 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:01.972515.972515 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:01.972079.972079 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e4b78851-c8d5-49f9-9b56-0d5ad00c1350
DEBUG 01-15 16:09:01.972701.972701 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:01.972829.972829 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:01.973601.973601 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:01.973657.973657 cuda_h.py:19] end move_flatidxs cost 0.0008335113525390625 seconds
DEBUG 01-15 16:09:01.973910.973910 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:01.975610.975610 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e4b78851-c8d5-49f9-9b56-0d5ad00c1350
DEBUG 01-15 16:09:01.975427.975427 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004426717758178711 seconds
DEBUG 01-15 16:09:01.976920.976920 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:01.979985.979985 cuda_h.py:19] end restore2model cost 0.0031032562255859375 seconds
DEBUG 01-15 16:09:01.979544.979544 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007769107818603516 seconds
DEBUG 01-15 16:09:01.979836.979836 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:01.979251.979251 cuda_h.py:19] end gpu_sexperts cost 0.0002720355987548828 seconds
DEBUG 01-15 16:09:01.979126.979126 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:01.979267.979267 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.979619.979619 cuda_h.py:19] end group_tensors cost 0.005120754241943359 seconds
DEBUG 01-15 16:09:01.979148.979148 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:01.981040.981040 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015094280242919922 seconds
DEBUG 01-15 16:09:01.981461.981461 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.981559.981559 cuda_h.py:19] end gpu_group_list cost 0.0003123283386230469 seconds
DEBUG 01-15 16:09:01.982368.982368 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:01.983532.983532 cuda_h.py:19] end group pad cost 0.0031130313873291016 seconds
DEBUG 01-15 16:09:01.983084.983084 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:01.991183.991183 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.007873058319091797 seconds
DEBUG 01-15 16:09:01.991552.991552 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:01.992059.992059 cuda_h.py:19] end gpu_group_list cost 0.0004258155822753906 seconds
DEBUG 01-15 16:09:01.993207.993207 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:01.993264.993264 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e4b78851-c8d5-49f9-9b56-0d5ad00c1350
DEBUG 01-15 16:09:02.011581.011581 cuda_h.py:19] end group_einsum cost 0.028385162353515625 seconds
DEBUG 01-15 16:09:02.011488.011488 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:02.014643.014643 cuda_h.py:19] end get_outputs_cpu1 cost 0.003003835678100586 seconds
DEBUG 01-15 16:09:02.015740.015740 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04283404350280762 seconds
INFO 01-15 16:09:02.016090.016090 client.py:127] Model loaded
DEBUG 01-15 16:09:02.016602.016602 cuda_h.py:19] end wait_experts_multi_device cost 0.02335333824157715 seconds
DEBUG 01-15 16:09:02.016418.016418 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:02.017493.017493 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0004413127899169922 seconds
DEBUG 01-15 16:09:02.017853.017853 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:02.018927.018927 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:02.018772.018772 cuda_h.py:19] end index_scatter cost 6.937980651855469e-05 seconds
DEBUG 01-15 16:09:02.018814.018814 cuda_h.py:19] end cpuoutputsdeal cost 0.0011646747589111328 seconds
DEBUG 01-15 16:09:02.018339.018339 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:02.018857.018857 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:02.018498.018498 cuda_h.py:19] end gpu_group_tensor cost 0.0001289844512939453 seconds
DEBUG 01-15 16:09:02.018923.018923 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:02.019730.019730 cuda_h.py:19] end gpu_group_tensor cost 0.00013303756713867188 seconds
DEBUG 01-15 16:09:02.019740.019740 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:02.019367.019367 cuda_h.py:19] end gpu_group_einsum cost 0.0006520748138427734 seconds
DEBUG 01-15 16:09:02.019835.019835 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:02.020548.020548 cuda_h.py:19] end gpu_group_einsum cost 0.0004658699035644531 seconds
DEBUG 01-15 16:09:02.020956.020956 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:02.020920.020920 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:02.021259.021259 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002040863037109375 seconds
DEBUG 01-15 16:09:02.021168.021168 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:02.021072.021072 cuda_h.py:19] end concat_expert_out cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:09:02.021313.021313 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:02.021356.021356 cuda_h.py:19] end index_scatter cost 6.127357482910156e-05 seconds
DEBUG 01-15 16:09:02.021040.021040 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008833408355712891 seconds
DEBUG 01-15 16:09:02.021713.021713 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:02.021046.021046 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:02.021510.021510 cuda_h.py:19] end all_expert_outputs_slices cost 0.000156402587890625 seconds
DEBUG 01-15 16:09:02.022034.022034 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:02.022024.022024 cuda_h.py:19] end concat_expert_out cost 6.389617919921875e-05 seconds
DEBUG 01-15 16:09:02.022881.022881 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:02.022255.022255 cuda_h.py:19] end index_scatter cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:09:02.022217.022217 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005736351013183594 seconds
DEBUG 01-15 16:09:02.022180.022180 cuda_h.py:19] end gpu_experts_multi_device cost 0.042708396911621094 seconds
DEBUG 01-15 16:09:02.022348.022348 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.054015159606933594 seconds
DEBUG 01-15 16:09:02.022773.022773 cuda_h.py:19] end prefill_layer cost 0.058020591735839844 seconds
DEBUG 01-15 16:09:02.022901.022901 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:09:02.022471.022471 cuda_h.py:19] end prefill cost 1.7875573635101318 seconds
DEBUG 01-15 16:09:04.246790.246790 cuda_h.py:10] start generate_input_ids
generate input ids cost 0.09285569190979004 s
DEBUG 01-15 16:09:04.601305.601305 cuda_h.py:19] end generate_input_ids cost 0.3547849655151367 seconds
DEBUG 01-15 16:09:04.602833.602833 cuda_h.py:10] start init_cache
DEBUG 01-15 16:09:04.602917.602917 cuda_h.py:19] end init_cache cost 8.7738037109375e-05 seconds
DEBUG 01-15 16:09:07.039809.039809 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:09:07.039427.039427 cuda_h.py:19] end init_meta_layer cost 1.239776611328125e-05 seconds
DEBUG 01-15 16:09:07.039654.039654 cuda_h.py:10] start init_weights
DEBUG 01-15 16:09:07.039284.039284 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:07.039617.039617 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:07.040954.040954 cuda_h.py:19] end allocate_cuda_memory cost 0.0009579658508300781 seconds
DEBUG 01-15 16:09:07.041758.041758 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:07.041944.041944 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:07.041251.041251 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:07.041430.041430 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f5a59b95-01d6-48b9-9109-775e3eaf9ac9
DEBUG 01-15 16:09:07.041474.041474 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:07.042754.042754 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f5a59b95-01d6-48b9-9109-775e3eaf9ac9
DEBUG 01-15 16:09:07.042928.042928 cuda_h.py:19] end load_into_gpu_async cost 0.0014977455139160156 seconds
DEBUG 01-15 16:09:07.042532.042532 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:07.042025.042025 cuda_h.py:19] end restore_tensors2 cost 5.817413330078125e-05 seconds
DEBUG 01-15 16:09:07.042920.042920 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002728700637817383 seconds
DEBUG 01-15 16:09:07.042046.042046 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:07.042152.042152 cuda_h.py:19] end restore2model cost 0.0001595020294189453 seconds
INFO 01-15 16:09:07.042769.042769 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f5a59b95-01d6-48b9-9109-775e3eaf9ac9
INFO 01-15 16:09:07.121867.121867 client.py:127] Model loaded
DEBUG 01-15 16:09:07.121124.121124 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:09:07.121579.121579 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:07.121179.121179 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:07.122421.122421 cuda_h.py:19] end allocate_cuda_memory cost 0.0003981590270996094 seconds
DEBUG 01-15 16:09:07.122472.122472 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:07.122342.122342 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:07.122848.122848 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:07.122036.122036 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d8a444dc-8db7-477f-b4ed-fe4f74973e53
DEBUG 01-15 16:09:07.122313.122313 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:07.124214.124214 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d8a444dc-8db7-477f-b4ed-fe4f74973e53
DEBUG 01-15 16:09:07.124255.124255 cuda_h.py:19] end load_into_gpu_async cost 0.0021386146545410156 seconds
DEBUG 01-15 16:09:07.124132.124132 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:07.124405.124405 cuda_h.py:19] end restore_tensors2 cost 0.00015735626220703125 seconds
DEBUG 01-15 16:09:07.124097.124097 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003320455551147461 seconds
INFO 01-15 16:09:07.125152.125152 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d8a444dc-8db7-477f-b4ed-fe4f74973e53
INFO 01-15 16:09:07.140863.140863 client.py:127] Model loaded
DEBUG 01-15 16:09:07.140828.140828 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:07.141290.141290 cuda_h.py:19] end restore2model cost 0.0008323192596435547 seconds
DEBUG 01-15 16:09:07.141466.141466 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.019821882247924805 seconds
DEBUG 01-15 16:09:07.141582.141582 cuda_h.py:19] end init_weights cost 0.10145115852355957 seconds
DEBUG 01-15 16:09:07.141101.141101 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:09:07.936663.936663 cuda_h.py:19] end copy_emodel cost 0.7951483726501465 seconds
DEBUG 01-15 16:09:07.937262.937262 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:09:07.937175.937175 cuda_h.py:19] end init_inputs_tokens cost 0.0003097057342529297 seconds
DEBUG 01-15 16:09:07.938958.938958 cuda_h.py:10] start prefill
DEBUG 01-15 16:09:07.938674.938674 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:07.938615.938615 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:09:07.938596.938596 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:09:07.938869.938869 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:09:07.938772.938772 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 4.3392181396484375e-05 seconds
DEBUG 01-15 16:09:07.938064.938064 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 8.940696716308594e-05 seconds
DEBUG 01-15 16:09:07.938736.938736 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:07.938426.938426 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:07.938067.938067 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:07.938826.938826 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:07.938464.938464 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:07.939838.939838 cuda_h.py:19] end allocate_cuda_memory cost 0.00037550926208496094 seconds
DEBUG 01-15 16:09:07.939537.939537 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:07.939743.939743 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:07.939898.939898 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:07.939615.939615 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 826fd192-de7d-472a-83ef-06423c89ba1a
DEBUG 01-15 16:09:07.939208.939208 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:07.939889.939889 cuda_h.py:10] start self_attn
INFO 01-15 16:09:07.941074.941074 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 826fd192-de7d-472a-83ef-06423c89ba1a
DEBUG 01-15 16:09:07.941381.941381 cuda_h.py:19] end load_into_gpu_async cost 0.0018410682678222656 seconds
DEBUG 01-15 16:09:07.941429.941429 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:07.941923.941923 cuda_h.py:19] end restore_tensors2 cost 8.320808410644531e-05 seconds
DEBUG 01-15 16:09:07.941216.941216 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002690553665161133 seconds
INFO 01-15 16:09:07.941132.941132 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 826fd192-de7d-472a-83ef-06423c89ba1a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:07.944711.944711 cuda_h.py:19] end self_attn cost 0.004678249359130859 seconds
DEBUG 01-15 16:09:07.945814.945814 cuda_h.py:19] end iln_self_attn_paln cost 0.006427288055419922 seconds
DEBUG 01-15 16:09:07.945975.945975 cuda_h.py:10] start dense_mlp
INFO 01-15 16:09:07.948661.948661 client.py:127] Model loaded
DEBUG 01-15 16:09:07.948610.948610 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:07.949300.949300 cuda_h.py:19] end restore2model cost 0.0005617141723632812 seconds
DEBUG 01-15 16:09:07.949627.949627 cuda_h.py:19] end sllm_worker_task cost 0.010783195495605469 seconds
DEBUG 01-15 16:09:07.949464.949464 cuda_h.py:19] end dense_mlp cost 0.004290103912353516 seconds
DEBUG 01-15 16:09:07.949653.949653 cuda_h.py:19] end prefill_layer cost 0.011404037475585938 seconds
DEBUG 01-15 16:09:07.949893.949893 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:09:07.949542.949542 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:07.949099.949099 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:09:07.949703.949703 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:09:07.949545.949545 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:09:07.949559.949559 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 2.1696090698242188e-05 seconds
DEBUG 01-15 16:09:07.949978.949978 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:09:07.949482.949482 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:07.949457.949457 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:07.949730.949730 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:07.949289.949289 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:07.949060.949060 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:07.950293.950293 cuda_h.py:19] end allocate_cuda_memory cost 0.00016927719116210938 seconds
DEBUG 01-15 16:09:07.950773.950773 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:07.950431.950431 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:07.950705.950705 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:07.950726.950726 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ce2cce05-bdf7-4312-bc04-6f508aa55db4
DEBUG 01-15 16:09:07.950776.950776 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:07.950165.950165 cuda_h.py:10] start self_attn
INFO 01-15 16:09:07.951731.951731 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ce2cce05-bdf7-4312-bc04-6f508aa55db4
DEBUG 01-15 16:09:07.951330.951330 cuda_h.py:19] end load_into_gpu_async cost 0.0013267993927001953 seconds
DEBUG 01-15 16:09:07.951199.951199 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:07.951236.951236 cuda_h.py:19] end restore_tensors2 cost 7.891654968261719e-05 seconds
DEBUG 01-15 16:09:07.951848.951848 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0020079612731933594 seconds
INFO 01-15 16:09:07.952042.952042 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ce2cce05-bdf7-4312-bc04-6f508aa55db4
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:07.953859.953859 cuda_h.py:19] end self_attn cost 0.003038644790649414 seconds
DEBUG 01-15 16:09:07.954577.954577 cuda_h.py:19] end iln_self_attn_paln cost 0.00439453125 seconds
DEBUG 01-15 16:09:07.954506.954506 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:09:07.954931.954931 cuda_h.py:10] start gate
DEBUG 01-15 16:09:07.955104.955104 cuda_h.py:19] end gate cost 0.0007958412170410156 seconds
DEBUG 01-15 16:09:07.955318.955318 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:07.955342.955342 lmp.py:1912] 
DEBUG 01-15 16:09:07.955342.955342 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:07.955867.955867 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:07.955424.955424 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:07.955166.955166 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:07.955955.955955 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:07.955075.955075 lmp.py:1917] 
DEBUG 01-15 16:09:07.955075.955075 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:07.955194.955194 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:07.955182.955182 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:09:07.955540.955540 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:09:07.955422.955422 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:09:07.955588.955588 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:09:07.955992.955992 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:09:07.955159.955159 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:09:07.955994.955994 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:09:07.955352.955352 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:09:07.955710.955710 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:09:07.955307.955307 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:09:07.955427.955427 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:09:07.955593.955593 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:09:07.955759.955759 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:09:07.955925.955925 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:09:07.955091.955091 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:09:07.955780.955780 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:09:07.955947.955947 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:09:07.955636.955636 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:09:07.955040.955040 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:09:07.955968.955968 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:09:07.955326.955326 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:09:07.955685.955685 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:09:07.956804.956804 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:09:07.956646.956646 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:09:07.956289.956289 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:09:07.956647.956647 lmp.py:1935]   Expert 34 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956006.956006 lmp.py:1935]   Expert 35 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956649.956649 lmp.py:1935]   Expert 36 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956815.956815 lmp.py:1935]   Expert 11 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956743.956743 lmp.py:1935]   Expert 17 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956909.956909 lmp.py:1935]   Expert 59 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956598.956598 lmp.py:1935]   Expert 10 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956287.956287 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956977.956977 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956620.956620 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956263.956263 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956098.956098 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956933.956933 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956814.956814 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956219.956219 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956908.956908 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956597.956597 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956525.956525 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956214.956214 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956156.956156 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956560.956560 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956726.956726 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956892.956892 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956820.956820 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956225.956225 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956344.956344 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956987.956987 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956869.956869 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956035.956035 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956201.956201 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956367.956367 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956295.956295 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956984.956984 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956435.956435 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956124.956124 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956052.956052 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956980.956980 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956669.956669 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:09:07.956882.956882 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:09:07.956856.956856 lmp.py:1937] 
DEBUG 01-15 16:09:07.956856.956856 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:07.956783.956783 lmp.py:1938]   CPU:   2872 tokens
DEBUG 01-15 16:09:07.956380.956380 lmp.py:1942]   cuda:1:   4628 tokens (19 experts)
DEBUG 01-15 16:09:07.956261.956261 lmp.py:1942]   cuda:2:   4788 tokens (20 experts)
DEBUG 01-15 16:09:07.956428.956428 lmp.py:1943]   Total GPU:   9416 tokens
DEBUG 01-15 16:09:07.956594.956594 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:07.956594.956594 lmp.py:1944] 
DEBUG 01-15 16:09:07.956005.956005 cuda_h.py:19] end experts_map_get cost 0.001706838607788086 seconds
DEBUG 01-15 16:09:07.956014.956014 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:07.956532.956532 lmp.py:1953] 
DEBUG 01-15 16:09:07.956532.956532 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:07.957421.957421 cuda_h.py:19] end cpu_experts_submit cost 5.7697296142578125e-05 seconds
DEBUG 01-15 16:09:07.957029.957029 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:07.957256.957256 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:07.958439.958439 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.39.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.39.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.39.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.1.mlp.experts.2.gate_proj.weight': 0, 'model.layers.1.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.46.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.46.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.46.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.1.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.1.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.1.mlp.experts.59.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3016228864, 5767168, 34603008, 0), (3021996032, 5767168, 40370176, 0), (3010461696, 5767168, 46137344, 0), (3033530368, 5767168, 51904512, 0), (3039297536, 5767168, 57671680, 0), (3027763200, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3154640896, 5767168, 86507520, 0), (3160408064, 5767168, 92274688, 0), (3148873728, 5767168, 98041856, 0), (3189243904, 5767168, 103809024, 0), (3195011072, 5767168, 109576192, 0), (3183476736, 5767168, 115343360, 0), (3206545408, 5767168, 121110528, 0), (3212312576, 5767168, 126877696, 0), (3200778240, 5767168, 132644864, 0), (3275751424, 5767168, 138412032, 0), (3281518592, 5767168, 144179200, 0), (3269984256, 5767168, 149946368, 0), (3310354432, 5767168, 155713536, 0), (3316121600, 5767168, 161480704, 0), (3304587264, 5767168, 167247872, 0), (3362258944, 5767168, 173015040, 0), (3368026112, 5767168, 178782208, 0), (3356491776, 5767168, 184549376, 0), (3379560448, 5767168, 190316544, 0), (3385327616, 5767168, 196083712, 0), (3373793280, 5767168, 201850880, 0), (3466067968, 5767168, 207618048, 0), (3471835136, 5767168, 213385216, 0), (3460300800, 5767168, 219152384, 0), (3535273984, 5767168, 224919552, 0), (3541041152, 5767168, 230686720, 0), (3529506816, 5767168, 236453888, 0), (3690987520, 5767168, 242221056, 0), (3696754688, 5767168, 247988224, 0), (3685220352, 5767168, 253755392, 0), (3725590528, 5767168, 259522560, 0), (3731357696, 5767168, 265289728, 0), (3719823360, 5767168, 271056896, 0), (3846701056, 5767168, 276824064, 0), (3852468224, 5767168, 282591232, 0), (3840933888, 5767168, 288358400, 0), (3915907072, 5767168, 294125568, 0), (3921674240, 5767168, 299892736, 0), (3910139904, 5767168, 305659904, 0), (3950510080, 5767168, 311427072, 0), (3956277248, 5767168, 317194240, 0), (3944742912, 5767168, 322961408, 0)], 2: [(2895118336, 5767168, 0, 0), (2900885504, 5767168, 5767168, 0), (2889351168, 5767168, 11534336, 0), (2929721344, 5767168, 17301504, 0), (2935488512, 5767168, 23068672, 0), (2923954176, 5767168, 28835840, 0), (2947022848, 5767168, 34603008, 0), (2952790016, 5767168, 40370176, 0), (2941255680, 5767168, 46137344, 0), (2998927360, 5767168, 51904512, 0), (3004694528, 5767168, 57671680, 0), (2993160192, 5767168, 63438848, 0), (3050831872, 5767168, 69206016, 0), (3056599040, 5767168, 74973184, 0), (3045064704, 5767168, 80740352, 0), (3102736384, 5767168, 86507520, 0), (3108503552, 5767168, 92274688, 0), (3096969216, 5767168, 98041856, 0), (3120037888, 5767168, 103809024, 0), (3125805056, 5767168, 109576192, 0), (3114270720, 5767168, 115343360, 0), (3137339392, 5767168, 121110528, 0), (3143106560, 5767168, 126877696, 0), (3131572224, 5767168, 132644864, 0), (3223846912, 5767168, 138412032, 0), (3229614080, 5767168, 144179200, 0), (3218079744, 5767168, 149946368, 0), (3258449920, 5767168, 155713536, 0), (3264217088, 5767168, 161480704, 0), (3252682752, 5767168, 167247872, 0), (3431464960, 5767168, 173015040, 0), (3437232128, 5767168, 178782208, 0), (3425697792, 5767168, 184549376, 0), (3448766464, 5767168, 190316544, 0), (3454533632, 5767168, 196083712, 0), (3442999296, 5767168, 201850880, 0), (3483369472, 5767168, 207618048, 0), (3489136640, 5767168, 213385216, 0), (3477602304, 5767168, 219152384, 0), (3552575488, 5767168, 224919552, 0), (3558342656, 5767168, 230686720, 0), (3546808320, 5767168, 236453888, 0), (3587178496, 5767168, 242221056, 0), (3592945664, 5767168, 247988224, 0), (3581411328, 5767168, 253755392, 0), (3639083008, 5767168, 259522560, 0), (3644850176, 5767168, 265289728, 0), (3633315840, 5767168, 271056896, 0), (3656384512, 5767168, 276824064, 0), (3662151680, 5767168, 282591232, 0), (3650617344, 5767168, 288358400, 0), (3777495040, 5767168, 294125568, 0), (3783262208, 5767168, 299892736, 0), (3771727872, 5767168, 305659904, 0), (3829399552, 5767168, 311427072, 0), (3835166720, 5767168, 317194240, 0), (3823632384, 5767168, 322961408, 0), (3881304064, 5767168, 328728576, 0), (3887071232, 5767168, 334495744, 0), (3875536896, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8143f4780>, 2: <capsule object NULL at 0x74a8143f7540>}
DEBUG 01-15 16:09:07.958234.958234 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:07.958679.958679 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:07.958574.958574 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6871d8ea-fe81-44df-ab26-776caba0a47f
DEBUG 01-15 16:09:07.958028.958028 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:07.959525.959525 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:07.959903.959903 client.py:127] Model loaded
DEBUG 01-15 16:09:07.959224.959224 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:07.959295.959295 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:07.960146.960146 cuda_h.py:19] end restore2model cost 0.0007488727569580078 seconds
DEBUG 01-15 16:09:07.960168.960168 cuda_h.py:19] end sllm_worker_task cost 0.010442495346069336 seconds
DEBUG 01-15 16:09:07.960721.960721 cuda_h.py:19] end move_flatidxs cost 0.001110076904296875 seconds
DEBUG 01-15 16:09:07.960461.960461 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:07.960105.960105 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6871d8ea-fe81-44df-ab26-776caba0a47f
DEBUG 01-15 16:09:07.961161.961161 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036513805389404297 seconds
DEBUG 01-15 16:09:07.961091.961091 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:07.964505.964505 cuda_h.py:19] end restore2model cost 0.003216981887817383 seconds
DEBUG 01-15 16:09:07.964938.964938 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007135152816772461 seconds
DEBUG 01-15 16:09:07.964210.964210 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:07.965247.965247 cuda_h.py:19] end gpu_sexperts cost 0.00027680397033691406 seconds
DEBUG 01-15 16:09:07.965030.965030 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:07.965138.965138 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:09:07.965834.965834 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:07.965067.965067 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:07.966921.966921 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009484291076660156 seconds
DEBUG 01-15 16:09:07.966102.966102 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:07.966129.966129 cuda_h.py:19] end gpu_group_list cost 0.00020170211791992188 seconds
DEBUG 01-15 16:09:07.967973.967973 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:07.969467.969467 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016350746154785156 seconds
DEBUG 01-15 16:09:07.969795.969795 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:07.970465.970465 cuda_h.py:19] end gpu_group_list cost 0.0002186298370361328 seconds
DEBUG 01-15 16:09:07.970205.970205 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:07.970849.970849 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6871d8ea-fe81-44df-ab26-776caba0a47f
DEBUG 01-15 16:09:07.970031.970031 cuda_h.py:19] end group_tensors cost 0.009809494018554688 seconds
DEBUG 01-15 16:09:07.971528.971528 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:07.982516.982516 cuda_h.py:19] end group pad cost 0.010841846466064453 seconds
DEBUG 01-15 16:09:07.982889.982889 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:07.995529.995529 client.py:127] Model loaded
DEBUG 01-15 16:09:07.996537.996537 cuda_h.py:19] end wait_experts_multi_device cost 0.025321245193481445 seconds
DEBUG 01-15 16:09:07.996112.996112 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.010107.010107 cuda_h.py:19] end group_einsum cost 0.02797389030456543 seconds
DEBUG 01-15 16:09:08.010795.010795 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.015853.015853 cuda_h.py:19] end get_outputs_cpu1 cost 0.004815340042114258 seconds
DEBUG 01-15 16:09:08.016037.016037 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05783510208129883 seconds
DEBUG 01-15 16:09:08.017053.017053 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.020648717880249023 seconds
DEBUG 01-15 16:09:08.017295.017295 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.018215.018215 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.018473.018473 cuda_h.py:19] end index_scatter cost 0.00011038780212402344 seconds
DEBUG 01-15 16:09:08.019530.019530 cuda_h.py:19] end cpuoutputsdeal cost 0.0014328956604003906 seconds
DEBUG 01-15 16:09:08.019638.019638 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.019878.019878 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.019973.019973 cuda_h.py:19] end gpu_group_tensor cost 0.0006287097930908203 seconds
DEBUG 01-15 16:09:08.019624.019624 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.020627.020627 cuda_h.py:19] end gpu_group_tensor cost 0.0006284713745117188 seconds
DEBUG 01-15 16:09:08.020141.020141 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.021204.021204 cuda_h.py:19] end gpu_group_einsum cost 0.0008368492126464844 seconds
DEBUG 01-15 16:09:08.021089.021089 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.022995.022995 cuda_h.py:19] end gpu_group_einsum cost 0.001108407974243164 seconds
DEBUG 01-15 16:09:08.022205.022205 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.023517.023517 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.023234.023234 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001964569091796875 seconds
DEBUG 01-15 16:09:08.023089.023089 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.023819.023819 cuda_h.py:19] end concat_expert_out cost 0.00018715858459472656 seconds
DEBUG 01-15 16:09:08.023841.023841 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.023607.023607 cuda_h.py:19] end index_scatter cost 6.628036499023438e-05 seconds
DEBUG 01-15 16:09:08.024317.024317 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011136531829833984 seconds
DEBUG 01-15 16:09:08.024962.024962 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.024852.024852 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.024450.024450 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023436546325683594 seconds
DEBUG 01-15 16:09:08.024067.024067 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.024402.024402 cuda_h.py:19] end concat_expert_out cost 6.008148193359375e-05 seconds
DEBUG 01-15 16:09:08.024198.024198 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.024552.024552 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:08.024215.024215 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006084442138671875 seconds
DEBUG 01-15 16:09:08.024079.024079 cuda_h.py:19] end gpu_experts_multi_device cost 0.05970883369445801 seconds
DEBUG 01-15 16:09:08.024419.024419 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.07076406478881836 seconds
DEBUG 01-15 16:09:08.025254.025254 cuda_h.py:19] end prefill_layer cost 0.07580447196960449 seconds
DEBUG 01-15 16:09:08.025687.025687 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:09:08.025343.025343 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.025477.025477 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:09:08.025895.025895 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:09:08.025459.025459 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:09:08.025262.025262 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:09:08.025780.025780 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:09:08.025575.025575 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.025823.025823 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.025720.025720 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.025279.025279 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.026857.026857 cuda_h.py:19] end allocate_cuda_memory cost 0.0002105236053466797 seconds
DEBUG 01-15 16:09:08.026852.026852 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.026377.026377 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.026445.026445 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.026108.026108 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3f2a50b7-c685-44b8-a891-058441f7b8c6
DEBUG 01-15 16:09:08.026681.026681 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.026905.026905 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.026459.026459 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.027411.027411 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3f2a50b7-c685-44b8-a891-058441f7b8c6
DEBUG 01-15 16:09:08.027036.027036 cuda_h.py:19] end load_into_gpu_async cost 0.0016777515411376953 seconds
DEBUG 01-15 16:09:08.028414.028414 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.028332.028332 cuda_h.py:19] end restore_tensors2 cost 8.392333984375e-05 seconds
DEBUG 01-15 16:09:08.028810.028810 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022437572479248047 seconds
INFO 01-15 16:09:08.028243.028243 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3f2a50b7-c685-44b8-a891-058441f7b8c6
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.030546.030546 cuda_h.py:19] end self_attn cost 0.0035157203674316406 seconds
DEBUG 01-15 16:09:08.030543.030543 cuda_h.py:19] end iln_self_attn_paln cost 0.005140066146850586 seconds
DEBUG 01-15 16:09:08.030704.030704 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:09:08.030414.030414 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.031575.031575 cuda_h.py:19] end gate cost 0.0006489753723144531 seconds
DEBUG 01-15 16:09:08.031690.031690 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.032032.032032 lmp.py:1912] 
DEBUG 01-15 16:09:08.032032.032032 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.032503.032503 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.032868.032868 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.032418.032418 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.032823.032823 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.032797.032797 lmp.py:1917] 
DEBUG 01-15 16:09:08.032797.032797 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.032963.032963 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.032613.032613 lmp.py:1935]   Expert 58 |     51 | CPU
DEBUG 01-15 16:09:08.032733.032733 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:09:08.032375.032375 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:09:08.032303.032303 lmp.py:1935]   Expert 17 |     84 | CPU
DEBUG 01-15 16:09:08.032516.032516 lmp.py:1935]   Expert 24 |     86 | CPU
DEBUG 01-15 16:09:08.032967.032967 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:09:08.032941.032941 lmp.py:1935]   Expert 28 |    104 | CPU
DEBUG 01-15 16:09:08.032915.032915 lmp.py:1935]   Expert 34 |    116 | CPU
DEBUG 01-15 16:09:08.032889.032889 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:09:08.032863.032863 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:09:08.032075.032075 lmp.py:1935]   Expert  9 |    130 | CPU
DEBUG 01-15 16:09:08.032526.032526 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:09:08.032500.032500 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:09:08.032905.032905 lmp.py:1935]   Expert 23 |    136 | CPU
DEBUG 01-15 16:09:08.032177.032177 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:09:08.032582.032582 lmp.py:1935]   Expert 30 |    144 | CPU
DEBUG 01-15 16:09:08.032556.032556 lmp.py:1935]   Expert 45 |    145 | CPU
DEBUG 01-15 16:09:08.032007.032007 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:09:08.032981.032981 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:09:08.032955.032955 lmp.py:1935]   Expert  1 |    152 | CPU
DEBUG 01-15 16:09:08.032690.032690 lmp.py:1935]   Expert 36 |    156 | CPU
DEBUG 01-15 16:09:08.032664.032664 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:09:08.032400.032400 lmp.py:1935]   Expert 29 |    161 | CPU
DEBUG 01-15 16:09:08.032374.032374 lmp.py:1935]   Expert 25 |    163 | CPU
DEBUG 01-15 16:09:08.032063.032063 lmp.py:1935]   Expert 54 |    166 | CPU
DEBUG 01-15 16:09:08.032614.032614 lmp.py:1935]   Expert  6 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032733.032733 lmp.py:1935]   Expert 49 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032138.032138 lmp.py:1935]   Expert 48 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032543.032543 lmp.py:1935]   Expert 12 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032947.032947 lmp.py:1935]   Expert 35 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032021.032021 lmp.py:1935]   Expert 37 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032140.032140 lmp.py:1935]   Expert 60 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032499.032499 lmp.py:1935]   Expert 13 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032618.032618 lmp.py:1935]   Expert 53 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032977.032977 lmp.py:1935]   Expert 33 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032335.032335 lmp.py:1935]   Expert 10 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032216.032216 lmp.py:1935]   Expert 16 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032098.032098 lmp.py:1935]   Expert 21 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032217.032217 lmp.py:1935]   Expert 40 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032337.032337 lmp.py:1935]   Expert 43 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032742.032742 lmp.py:1935]   Expert 38 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032623.032623 lmp.py:1935]   Expert  5 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032505.032505 lmp.py:1935]   Expert 44 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032148.032148 lmp.py:1935]   Expert 52 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032698.032698 lmp.py:1935]   Expert 50 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032533.032533 lmp.py:1935]   Expert 41 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032130.032130 lmp.py:1935]   Expert 19 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.032249.032249 lmp.py:1935]   Expert  4 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.032369.032369 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.033489.033489 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.033370.033370 lmp.py:1935]   Expert 31 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.033729.033729 lmp.py:1935]   Expert 56 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.033849.033849 lmp.py:1935]   Expert 20 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.033491.033491 lmp.py:1935]   Expert 39 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.033373.033373 lmp.py:1935]   Expert 22 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.033731.033731 lmp.py:1935]   Expert  2 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.033566.033566 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.033448.033448 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.033329.033329 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.033210.033210 lmp.py:1935]   Expert 18 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.033853.033853 lmp.py:1935]   Expert 14 |    318 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.033212.033212 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.033855.033855 lmp.py:1935]   Expert 11 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.033498.033498 lmp.py:1935]   Expert 61 |    460 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.033948.033948 lmp.py:1937] 
DEBUG 01-15 16:09:08.033948.033948 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.033068.033068 lmp.py:1938]   CPU:   3107 tokens
DEBUG 01-15 16:09:08.033665.033665 lmp.py:1942]   cuda:1:   4507 tokens (19 experts)
DEBUG 01-15 16:09:08.033023.033023 lmp.py:1942]   cuda:2:   4674 tokens (20 experts)
DEBUG 01-15 16:09:08.033951.033951 lmp.py:1943]   Total GPU:   9181 tokens
DEBUG 01-15 16:09:08.033879.033879 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.033879.033879 lmp.py:1944] 
DEBUG 01-15 16:09:08.033972.033972 cuda_h.py:19] end experts_map_get cost 0.0016782283782958984 seconds
DEBUG 01-15 16:09:08.033220.033220 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.033691.033691 lmp.py:1953] 
DEBUG 01-15 16:09:08.033691.033691 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.033587.033587 cuda_h.py:19] end cpu_experts_submit cost 6.365776062011719e-05 seconds
DEBUG 01-15 16:09:08.033112.033112 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.033041.033041 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.034747.034747 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.4.gate_proj.weight': 0, 'model.layers.2.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.12.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.12.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.12.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.14.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.14.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.14.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.21.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.21.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.21.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.22.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.22.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.22.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.31.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.31.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.31.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.33.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.33.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.33.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.38.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.38.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.38.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.39.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.39.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.39.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.41.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.41.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.41.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.59.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.59.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.59.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.60.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.60.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.60.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.2.mlp.experts.2.gate_proj.weight': 0, 'model.layers.2.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.50.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.50.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.50.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.55.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.55.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.55.up_proj.weight': 322961408, 'model.layers.2.mlp.experts.56.gate_proj.weight': 328728576, 'model.layers.2.mlp.experts.56.down_proj.weight': 334495744, 'model.layers.2.mlp.experts.56.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(4037017600, 5767168, 0, 0), (4042784768, 5767168, 5767168, 0), (4031250432, 5767168, 11534336, 0), (4175429632, 5767168, 17301504, 0), (4181196800, 5767168, 23068672, 0), (4169662464, 5767168, 28835840, 0), (4210032640, 5767168, 34603008, 0), (4215799808, 5767168, 40370176, 0), (4204265472, 5767168, 46137344, 0), (4331143168, 5767168, 51904512, 0), (4336910336, 5767168, 57671680, 0), (4325376000, 5767168, 63438848, 0), (4348444672, 5767168, 69206016, 0), (4354211840, 5767168, 74973184, 0), (4342677504, 5767168, 80740352, 0), (4504158208, 5767168, 86507520, 0), (4509925376, 5767168, 92274688, 0), (4498391040, 5767168, 98041856, 0), (4538761216, 5767168, 103809024, 0), (4544528384, 5767168, 109576192, 0), (4532994048, 5767168, 115343360, 0), (4625268736, 5767168, 121110528, 0), (4631035904, 5767168, 126877696, 0), (4619501568, 5767168, 132644864, 0), (4642570240, 5767168, 138412032, 0), (4648337408, 5767168, 144179200, 0), (4636803072, 5767168, 149946368, 0), (4677173248, 5767168, 155713536, 0), (4682940416, 5767168, 161480704, 0), (4671406080, 5767168, 167247872, 0), (4694474752, 5767168, 173015040, 0), (4700241920, 5767168, 178782208, 0), (4688707584, 5767168, 184549376, 0), (4711776256, 5767168, 190316544, 0), (4717543424, 5767168, 196083712, 0), (4706009088, 5767168, 201850880, 0), (4815585280, 5767168, 207618048, 0), (4821352448, 5767168, 213385216, 0), (4809818112, 5767168, 219152384, 0), (4867489792, 5767168, 224919552, 0), (4873256960, 5767168, 230686720, 0), (4861722624, 5767168, 236453888, 0), (4884791296, 5767168, 242221056, 0), (4890558464, 5767168, 247988224, 0), (4879024128, 5767168, 253755392, 0), (4988600320, 5767168, 259522560, 0), (4994367488, 5767168, 265289728, 0), (4982833152, 5767168, 271056896, 0), (5005901824, 5767168, 276824064, 0), (5011668992, 5767168, 282591232, 0), (5000134656, 5767168, 288358400, 0), (5023203328, 5767168, 294125568, 0), (5028970496, 5767168, 299892736, 0), (5017436160, 5767168, 305659904, 0), (5057806336, 5767168, 311427072, 0), (5063573504, 5767168, 317194240, 0), (5052039168, 5767168, 322961408, 0)], 2: [(4002414592, 5767168, 0, 0), (4008181760, 5767168, 5767168, 0), (3996647424, 5767168, 11534336, 0), (4054319104, 5767168, 17301504, 0), (4060086272, 5767168, 23068672, 0), (4048551936, 5767168, 28835840, 0), (4071620608, 5767168, 34603008, 0), (4077387776, 5767168, 40370176, 0), (4065853440, 5767168, 46137344, 0), (4140826624, 5767168, 51904512, 0), (4146593792, 5767168, 57671680, 0), (4135059456, 5767168, 63438848, 0), (4158128128, 5767168, 69206016, 0), (4163895296, 5767168, 74973184, 0), (4152360960, 5767168, 80740352, 0), (4192731136, 5767168, 86507520, 0), (4198498304, 5767168, 92274688, 0), (4186963968, 5767168, 98041856, 0), (4244635648, 5767168, 103809024, 0), (4250402816, 5767168, 109576192, 0), (4238868480, 5767168, 115343360, 0), (4279238656, 5767168, 121110528, 0), (4285005824, 5767168, 126877696, 0), (4273471488, 5767168, 132644864, 0), (4296540160, 5767168, 138412032, 0), (4302307328, 5767168, 144179200, 0), (4290772992, 5767168, 149946368, 0), (4313841664, 5767168, 155713536, 0), (4319608832, 5767168, 161480704, 0), (4308074496, 5767168, 167247872, 0), (4573364224, 5767168, 173015040, 0), (4579131392, 5767168, 178782208, 0), (4567597056, 5767168, 184549376, 0), (4607967232, 5767168, 190316544, 0), (4613734400, 5767168, 196083712, 0), (4602200064, 5767168, 201850880, 0), (4659871744, 5767168, 207618048, 0), (4665638912, 5767168, 213385216, 0), (4654104576, 5767168, 219152384, 0), (4729077760, 5767168, 224919552, 0), (4734844928, 5767168, 230686720, 0), (4723310592, 5767168, 236453888, 0), (4763680768, 5767168, 242221056, 0), (4769447936, 5767168, 247988224, 0), (4757913600, 5767168, 253755392, 0), (4780982272, 5767168, 259522560, 0), (4786749440, 5767168, 265289728, 0), (4775215104, 5767168, 271056896, 0), (4798283776, 5767168, 276824064, 0), (4804050944, 5767168, 282591232, 0), (4792516608, 5767168, 288358400, 0), (4832886784, 5767168, 294125568, 0), (4838653952, 5767168, 299892736, 0), (4827119616, 5767168, 305659904, 0), (4919394304, 5767168, 311427072, 0), (4925161472, 5767168, 317194240, 0), (4913627136, 5767168, 322961408, 0), (4936695808, 5767168, 328728576, 0), (4942462976, 5767168, 334495744, 0), (4930928640, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a81433b9f0>, 2: <capsule object NULL at 0x74a8143f7c90>}
DEBUG 01-15 16:09:08.034162.034162 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.034799.034799 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.034694.034694 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5d9662a9-bd97-4a39-8867-f574b28af962
DEBUG 01-15 16:09:08.035695.035695 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.035987.035987 client.py:127] Model loaded
DEBUG 01-15 16:09:08.035055.035055 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.036094.036094 cuda_h.py:19] end restore2model cost 0.000347137451171875 seconds
DEBUG 01-15 16:09:08.036241.036241 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:08.036786.036786 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5d9662a9-bd97-4a39-8867-f574b28af962
DEBUG 01-15 16:09:08.036346.036346 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.036344.036344 cuda_h.py:19] end sllm_worker_task cost 0.010334014892578125 seconds
DEBUG 01-15 16:09:08.036997.036997 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030241012573242188 seconds
DEBUG 01-15 16:09:08.036247.036247 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.037324.037324 cuda_h.py:19] end move_flatidxs cost 0.0008594989776611328 seconds
DEBUG 01-15 16:09:08.037075.037075 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:08.040516.040516 cuda_h.py:19] end restore2model cost 0.003239870071411133 seconds
DEBUG 01-15 16:09:08.040082.040082 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006592273712158203 seconds
DEBUG 01-15 16:09:08.040116.040116 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.040696.040696 cuda_h.py:19] end gpu_sexperts cost 0.0002913475036621094 seconds
DEBUG 01-15 16:09:08.040002.040002 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.040586.040586 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:09:08.040759.040759 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.040654.040654 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.041620.041620 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009279251098632812 seconds
DEBUG 01-15 16:09:08.041748.041748 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.041571.041571 cuda_h.py:19] end gpu_group_list cost 0.0002276897430419922 seconds
DEBUG 01-15 16:09:08.042102.042102 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.043053.043053 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010645389556884766 seconds
DEBUG 01-15 16:09:08.044645.044645 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.044674.044674 cuda_h.py:19] end gpu_group_list cost 0.00022172927856445312 seconds
DEBUG 01-15 16:09:08.044155.044155 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.045892.045892 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5d9662a9-bd97-4a39-8867-f574b28af962
DEBUG 01-15 16:09:08.044548.044548 cuda_h.py:19] end group_tensors cost 0.006964206695556641 seconds
DEBUG 01-15 16:09:08.045419.045419 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.049796.049796 cuda_h.py:19] end group pad cost 0.0038657188415527344 seconds
DEBUG 01-15 16:09:08.049970.049970 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:08.076254.076254 client.py:127] Model loaded
DEBUG 01-15 16:09:08.076313.076313 cuda_h.py:19] end wait_experts_multi_device cost 0.031348228454589844 seconds
DEBUG 01-15 16:09:08.076467.076467 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.077221.077221 cuda_h.py:19] end group_einsum cost 0.027979612350463867 seconds
DEBUG 01-15 16:09:08.077418.077418 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.082206.082206 cuda_h.py:19] end get_outputs_cpu1 cost 0.005268573760986328 seconds
DEBUG 01-15 16:09:08.083852.083852 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04760885238647461 seconds
DEBUG 01-15 16:09:08.084374.084374 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008202552795410156 seconds
DEBUG 01-15 16:09:08.084023.084023 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.087395.087395 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.087590.087590 cuda_h.py:19] end index_scatter cost 0.00032591819763183594 seconds
DEBUG 01-15 16:09:08.088276.088276 cuda_h.py:19] end cpuoutputsdeal cost 0.003119230270385742 seconds
DEBUG 01-15 16:09:08.088597.088597 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.088982.088982 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.088276.088276 cuda_h.py:19] end gpu_group_tensor cost 0.00020003318786621094 seconds
DEBUG 01-15 16:09:08.088436.088436 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.088859.088859 cuda_h.py:19] end gpu_group_tensor cost 0.00014090538024902344 seconds
DEBUG 01-15 16:09:08.088154.088154 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.089612.089612 cuda_h.py:19] end gpu_group_einsum cost 0.0009510517120361328 seconds
DEBUG 01-15 16:09:08.090975.090975 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.090565.090565 cuda_h.py:19] end gpu_group_einsum cost 0.0005333423614501953 seconds
DEBUG 01-15 16:09:08.090417.090417 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.090679.090679 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.091802.091802 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002532005310058594 seconds
DEBUG 01-15 16:09:08.091439.091439 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.091231.091231 cuda_h.py:19] end concat_expert_out cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:09:08.091034.091034 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.091534.091534 cuda_h.py:19] end index_scatter cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:09:08.091867.091867 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009064674377441406 seconds
DEBUG 01-15 16:09:08.091327.091327 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.091369.091369 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.092474.092474 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015234947204589844 seconds
DEBUG 01-15 16:09:08.092038.092038 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.092683.092683 cuda_h.py:19] end concat_expert_out cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:09:08.092864.092864 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.092695.092695 cuda_h.py:19] end index_scatter cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:09:08.092617.092617 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005290508270263672 seconds
DEBUG 01-15 16:09:08.092434.092434 cuda_h.py:19] end gpu_experts_multi_device cost 0.051773786544799805 seconds
DEBUG 01-15 16:09:08.092066.092066 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.0616145133972168 seconds
DEBUG 01-15 16:09:08.093049.093049 cuda_h.py:19] end prefill_layer cost 0.06751799583435059 seconds
DEBUG 01-15 16:09:08.093482.093482 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:09:08.093615.093615 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.093272.093272 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:09:08.093928.093928 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:09:08.093446.093446 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:09:08.093726.093726 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 3.933906555175781e-05 seconds
DEBUG 01-15 16:09:08.093674.093674 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:09:08.093278.093278 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.093108.093108 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.093502.093502 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.093902.093902 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.093361.093361 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.093263.093263 cuda_h.py:19] end allocate_cuda_memory cost 0.00020647048950195312 seconds
DEBUG 01-15 16:09:08.093524.093524 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.094625.094625 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.094170.094170 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.094118.094118 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 66795e62-1407-4b23-be46-b91e3fa489e3
DEBUG 01-15 16:09:08.094929.094929 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.094714.094714 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.095658.095658 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 66795e62-1407-4b23-be46-b91e3fa489e3
DEBUG 01-15 16:09:08.095853.095853 cuda_h.py:19] end load_into_gpu_async cost 0.001466512680053711 seconds
DEBUG 01-15 16:09:08.095231.095231 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.095911.095911 cuda_h.py:19] end restore_tensors2 cost 8.273124694824219e-05 seconds
DEBUG 01-15 16:09:08.095435.095435 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0020456314086914062 seconds
INFO 01-15 16:09:08.095120.095120 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 66795e62-1407-4b23-be46-b91e3fa489e3
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.098266.098266 cuda_h.py:19] end self_attn cost 0.0037412643432617188 seconds
DEBUG 01-15 16:09:08.098011.098011 cuda_h.py:19] end iln_self_attn_paln cost 0.005247831344604492 seconds
DEBUG 01-15 16:09:08.098410.098410 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:09:08.098596.098596 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.099397.099397 cuda_h.py:19] end gate cost 0.000732421875 seconds
DEBUG 01-15 16:09:08.099849.099849 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.099767.099767 lmp.py:1912] 
DEBUG 01-15 16:09:08.099767.099767 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.099576.099576 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.099941.099941 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.099730.099730 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.099135.099135 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.099347.099347 lmp.py:1917] 
DEBUG 01-15 16:09:08.099347.099347 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.099036.099036 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.100163.100163 lmp.py:1935]   Expert  1 |     51 | CPU
DEBUG 01-15 16:09:08.100329.100329 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:09:08.100542.100542 lmp.py:1935]   Expert  7 |     76 | CPU
DEBUG 01-15 16:09:08.100992.100992 lmp.py:1935]   Expert 48 |     82 | CPU
DEBUG 01-15 16:09:08.100728.100728 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:09:08.100464.100464 lmp.py:1935]   Expert 30 |    109 | CPU
DEBUG 01-15 16:09:08.100391.100391 lmp.py:1935]   Expert 61 |    116 | CPU
DEBUG 01-15 16:09:08.100842.100842 lmp.py:1935]   Expert 32 |    118 | CPU
DEBUG 01-15 16:09:08.100677.100677 lmp.py:1935]   Expert 45 |    118 | CPU
DEBUG 01-15 16:09:08.100843.100843 lmp.py:1935]   Expert 18 |    119 | CPU
DEBUG 01-15 16:09:08.100533.100533 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:09:08.100222.100222 lmp.py:1935]   Expert 39 |    135 | CPU
DEBUG 01-15 16:09:08.100911.100911 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:09:08.100362.100362 lmp.py:1935]   Expert 36 |    138 | CPU
DEBUG 01-15 16:09:08.100052.100052 lmp.py:1935]   Expert 11 |    140 | CPU
DEBUG 01-15 16:09:08.100218.100218 lmp.py:1935]   Expert  5 |    141 | CPU
DEBUG 01-15 16:09:08.100907.100907 lmp.py:1935]   Expert 59 |    142 | CPU
DEBUG 01-15 16:09:08.100312.100312 lmp.py:1935]   Expert  6 |    144 | CPU
DEBUG 01-15 16:09:08.100478.100478 lmp.py:1935]   Expert 51 |    144 | CPU
DEBUG 01-15 16:09:08.100405.100405 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:09:08.100095.100095 lmp.py:1935]   Expert 23 |    156 | CPU
DEBUG 01-15 16:09:08.100546.100546 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:09:08.100441.100441 lmp.py:1935]   Expert  9 |    158 | CPU
DEBUG 01-15 16:09:08.100845.100845 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:09:08.100773.100773 lmp.py:1935]   Expert 56 |    167 | CPU
DEBUG 01-15 16:09:08.100608.100608 lmp.py:1935]   Expert 40 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100681.100681 lmp.py:1935]   Expert 52 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100516.100516 lmp.py:1935]   Expert 16 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100875.100875 lmp.py:1935]   Expert 35 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100471.100471 lmp.py:1935]   Expert  4 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100353.100353 lmp.py:1935]   Expert 42 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100711.100711 lmp.py:1935]   Expert 13 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100592.100592 lmp.py:1935]   Expert 37 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100235.100235 lmp.py:1935]   Expert 38 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100117.100117 lmp.py:1935]   Expert 17 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100237.100237 lmp.py:1935]   Expert 62 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100118.100118 lmp.py:1935]   Expert 21 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100238.100238 lmp.py:1935]   Expert  3 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100119.100119 lmp.py:1935]   Expert 44 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100477.100477 lmp.py:1935]   Expert 60 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100836.100836 lmp.py:1935]   Expert 28 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100955.100955 lmp.py:1935]   Expert 58 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100360.100360 lmp.py:1935]   Expert 10 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100765.100765 lmp.py:1935]   Expert 47 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100646.100646 lmp.py:1935]   Expert 53 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100050.100050 lmp.py:1935]   Expert 55 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100932.100932 lmp.py:1935]   Expert 20 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100336.100336 lmp.py:1935]   Expert 33 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100979.100979 lmp.py:1935]   Expert 57 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100622.100622 lmp.py:1935]   Expert 31 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100126.100126 lmp.py:1935]   Expert 46 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100008.100008 lmp.py:1935]   Expert  8 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100651.100651 lmp.py:1935]   Expert 19 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100340.100340 lmp.py:1935]   Expert 24 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.100791.100791 lmp.py:1935]   Expert 14 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.100003.100003 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.101693.101693 lmp.py:1935]   Expert 12 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.101144.101144 lmp.py:1935]   Expert 29 |    275 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.101833.101833 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.101522.101522 lmp.py:1935]   Expert  0 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.101973.101973 lmp.py:1935]   Expert 43 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.101901.101901 lmp.py:1935]   Expert 54 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.101590.101590 lmp.py:1935]   Expert 41 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.101233.101233 lmp.py:1935]   Expert 25 |    413 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.101445.101445 lmp.py:1937] 
DEBUG 01-15 16:09:08.101445.101445 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.101658.101658 lmp.py:1938]   CPU:   3161 tokens
DEBUG 01-15 16:09:08.101539.101539 lmp.py:1942]   cuda:1:   4642 tokens (20 experts)
DEBUG 01-15 16:09:08.101229.101229 lmp.py:1942]   cuda:2:   4485 tokens (19 experts)
DEBUG 01-15 16:09:08.101441.101441 lmp.py:1943]   Total GPU:   9127 tokens
DEBUG 01-15 16:09:08.101415.101415 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.101415.101415 lmp.py:1944] 
DEBUG 01-15 16:09:08.101873.101873 cuda_h.py:19] end experts_map_get cost 0.0016748905181884766 seconds
DEBUG 01-15 16:09:08.101054.101054 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.101048.101048 lmp.py:1953] 
DEBUG 01-15 16:09:08.101048.101048 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.101884.101884 cuda_h.py:19] end cpu_experts_submit cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:09:08.101634.101634 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.101708.101708 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.102749.102749 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.14.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.14.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.14.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.16.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.16.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.16.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.17.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.17.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.17.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.47.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.47.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.47.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.3.mlp.experts.58.gate_proj.weight': 328728576, 'model.layers.3.mlp.experts.58.down_proj.weight': 334495744, 'model.layers.3.mlp.experts.58.up_proj.weight': 340262912}, 2: {'model.layers.3.mlp.experts.3.gate_proj.weight': 0, 'model.layers.3.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.46.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.46.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.46.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.60.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.60.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.60.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5317328896, 5767168, 17301504, 0), (5323096064, 5767168, 23068672, 0), (5311561728, 5767168, 28835840, 0), (5351931904, 5767168, 34603008, 0), (5357699072, 5767168, 40370176, 0), (5346164736, 5767168, 46137344, 0), (5369233408, 5767168, 51904512, 0), (5375000576, 5767168, 57671680, 0), (5363466240, 5767168, 63438848, 0), (5403836416, 5767168, 69206016, 0), (5409603584, 5767168, 74973184, 0), (5398069248, 5767168, 80740352, 0), (5438439424, 5767168, 86507520, 0), (5444206592, 5767168, 92274688, 0), (5432672256, 5767168, 98041856, 0), (5507645440, 5767168, 103809024, 0), (5513412608, 5767168, 109576192, 0), (5501878272, 5767168, 115343360, 0), (5576851456, 5767168, 121110528, 0), (5582618624, 5767168, 126877696, 0), (5571084288, 5767168, 132644864, 0), (5611454464, 5767168, 138412032, 0), (5617221632, 5767168, 144179200, 0), (5605687296, 5767168, 149946368, 0), (5646057472, 5767168, 155713536, 0), (5651824640, 5767168, 161480704, 0), (5640290304, 5767168, 167247872, 0), (5680660480, 5767168, 173015040, 0), (5686427648, 5767168, 178782208, 0), (5674893312, 5767168, 184549376, 0), (5732564992, 5767168, 190316544, 0), (5738332160, 5767168, 196083712, 0), (5726797824, 5767168, 201850880, 0), (5767168000, 5767168, 207618048, 0), (5772935168, 5767168, 213385216, 0), (5761400832, 5767168, 219152384, 0), (5801771008, 5767168, 224919552, 0), (5807538176, 5767168, 230686720, 0), (5796003840, 5767168, 236453888, 0), (5819072512, 5767168, 242221056, 0), (5824839680, 5767168, 247988224, 0), (5813305344, 5767168, 253755392, 0), (5836374016, 5767168, 259522560, 0), (5842141184, 5767168, 265289728, 0), (5830606848, 5767168, 271056896, 0), (5888278528, 5767168, 276824064, 0), (5894045696, 5767168, 282591232, 0), (5882511360, 5767168, 288358400, 0), (6026690560, 5767168, 294125568, 0), (6032457728, 5767168, 299892736, 0), (6020923392, 5767168, 305659904, 0), (6061293568, 5767168, 311427072, 0), (6067060736, 5767168, 317194240, 0), (6055526400, 5767168, 322961408, 0), (6078595072, 5767168, 328728576, 0), (6084362240, 5767168, 334495744, 0), (6072827904, 5767168, 340262912, 0)], 2: [(5127012352, 5767168, 0, 0), (5132779520, 5767168, 5767168, 0), (5121245184, 5767168, 11534336, 0), (5144313856, 5767168, 17301504, 0), (5150081024, 5767168, 23068672, 0), (5138546688, 5767168, 28835840, 0), (5213519872, 5767168, 34603008, 0), (5219287040, 5767168, 40370176, 0), (5207752704, 5767168, 46137344, 0), (5248122880, 5767168, 51904512, 0), (5253890048, 5767168, 57671680, 0), (5242355712, 5767168, 63438848, 0), (5282725888, 5767168, 69206016, 0), (5288493056, 5767168, 74973184, 0), (5276958720, 5767168, 80740352, 0), (5300027392, 5767168, 86507520, 0), (5305794560, 5767168, 92274688, 0), (5294260224, 5767168, 98041856, 0), (5421137920, 5767168, 103809024, 0), (5426905088, 5767168, 109576192, 0), (5415370752, 5767168, 115343360, 0), (5455740928, 5767168, 121110528, 0), (5461508096, 5767168, 126877696, 0), (5449973760, 5767168, 132644864, 0), (5490343936, 5767168, 138412032, 0), (5496111104, 5767168, 144179200, 0), (5484576768, 5767168, 149946368, 0), (5559549952, 5767168, 155713536, 0), (5565317120, 5767168, 161480704, 0), (5553782784, 5767168, 167247872, 0), (5715263488, 5767168, 173015040, 0), (5721030656, 5767168, 178782208, 0), (5709496320, 5767168, 184549376, 0), (5784469504, 5767168, 190316544, 0), (5790236672, 5767168, 196083712, 0), (5778702336, 5767168, 201850880, 0), (5870977024, 5767168, 207618048, 0), (5876744192, 5767168, 213385216, 0), (5865209856, 5767168, 219152384, 0), (5974786048, 5767168, 224919552, 0), (5980553216, 5767168, 230686720, 0), (5969018880, 5767168, 236453888, 0), (5992087552, 5767168, 242221056, 0), (5997854720, 5767168, 247988224, 0), (5986320384, 5767168, 253755392, 0), (6009389056, 5767168, 259522560, 0), (6015156224, 5767168, 265289728, 0), (6003621888, 5767168, 271056896, 0), (6113198080, 5767168, 276824064, 0), (6118965248, 5767168, 282591232, 0), (6107430912, 5767168, 288358400, 0), (6147801088, 5767168, 294125568, 0), (6153568256, 5767168, 299892736, 0), (6142033920, 5767168, 305659904, 0), (6165102592, 5767168, 311427072, 0), (6170869760, 5767168, 317194240, 0), (6159335424, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814083d80>, 2: <capsule object NULL at 0x74a81433bf30>}
DEBUG 01-15 16:09:08.102637.102637 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:09:08.102977.102977 client.py:127] Model loaded
DEBUG 01-15 16:09:08.102602.102602 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.102711.102711 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1ba5338a-04f4-4026-8879-1965317cdc0a
DEBUG 01-15 16:09:08.102553.102553 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.102181.102181 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.102001.102001 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.103880.103880 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.103390.103390 cuda_h.py:19] end restore2model cost 0.00036787986755371094 seconds
DEBUG 01-15 16:09:08.103167.103167 cuda_h.py:19] end sllm_worker_task cost 0.009893655776977539 seconds
INFO 01-15 16:09:08.103876.103876 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1ba5338a-04f4-4026-8879-1965317cdc0a
DEBUG 01-15 16:09:08.104242.104242 cuda_h.py:19] end move_flatidxs cost 0.0008363723754882812 seconds
DEBUG 01-15 16:09:08.104687.104687 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:08.104635.104635 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002930879592895508 seconds
DEBUG 01-15 16:09:08.104679.104679 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.107125.107125 cuda_h.py:19] end restore2model cost 0.0032074451446533203 seconds
DEBUG 01-15 16:09:08.107074.107074 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006411552429199219 seconds
DEBUG 01-15 16:09:08.107631.107631 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.108615.108615 cuda_h.py:19] end gpu_sexperts cost 0.0002727508544921875 seconds
DEBUG 01-15 16:09:08.108683.108683 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.108844.108844 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:09:08.108255.108255 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.108719.108719 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.109357.109357 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010008811950683594 seconds
DEBUG 01-15 16:09:08.109445.109445 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.109446.109446 cuda_h.py:19] end gpu_group_list cost 0.00021839141845703125 seconds
DEBUG 01-15 16:09:08.109350.109350 cuda_h.py:19] end group_tensors cost 0.005332469940185547 seconds
DEBUG 01-15 16:09:08.110618.110618 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.110251.110251 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.112342.112342 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016248226165771484 seconds
DEBUG 01-15 16:09:08.112802.112802 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.112383.112383 cuda_h.py:19] end gpu_group_list cost 0.00031638145446777344 seconds
DEBUG 01-15 16:09:08.113906.113906 cuda_h.py:19] end group pad cost 0.003215789794921875 seconds
DEBUG 01-15 16:09:08.113173.113173 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:08.113759.113759 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.113218.113218 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1ba5338a-04f4-4026-8879-1965317cdc0a
INFO 01-15 16:09:08.144662.144662 client.py:127] Model loaded
DEBUG 01-15 16:09:08.144766.144766 cuda_h.py:19] end wait_experts_multi_device cost 0.03079819679260254 seconds
DEBUG 01-15 16:09:08.144960.144960 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.144623.144623 cuda_h.py:19] end group_einsum cost 0.031148195266723633 seconds
DEBUG 01-15 16:09:08.144528.144528 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.148169.148169 cuda_h.py:19] end get_outputs_cpu1 cost 0.003259897232055664 seconds
DEBUG 01-15 16:09:08.149889.149889 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04616284370422363 seconds
DEBUG 01-15 16:09:08.150449.150449 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005220174789428711 seconds
DEBUG 01-15 16:09:08.150350.150350 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.152908.152908 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.152387.152387 cuda_h.py:19] end index_scatter cost 0.00029206275939941406 seconds
DEBUG 01-15 16:09:08.153554.153554 cuda_h.py:19] end cpuoutputsdeal cost 0.003049612045288086 seconds
DEBUG 01-15 16:09:08.153392.153392 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.153823.153823 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.155641.155641 cuda_h.py:19] end gpu_group_tensor cost 0.0014395713806152344 seconds
DEBUG 01-15 16:09:08.155868.155868 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.155411.155411 cuda_h.py:19] end gpu_group_tensor cost 0.00015163421630859375 seconds
DEBUG 01-15 16:09:08.155898.155898 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.155294.155294 cuda_h.py:19] end gpu_group_einsum cost 0.00048661231994628906 seconds
DEBUG 01-15 16:09:08.156563.156563 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.156349.156349 cuda_h.py:19] end gpu_group_einsum cost 0.0004899501800537109 seconds
DEBUG 01-15 16:09:08.156533.156533 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.156464.156464 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.157600.157600 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002613067626953125 seconds
DEBUG 01-15 16:09:08.157191.157191 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.157691.157691 cuda_h.py:19] end concat_expert_out cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:08.157541.157541 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.157372.157372 cuda_h.py:19] end index_scatter cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:09:08.157267.157267 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009074211120605469 seconds
DEBUG 01-15 16:09:08.157396.157396 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.157796.157796 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.158438.158438 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015854835510253906 seconds
DEBUG 01-15 16:09:08.158479.158479 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.158833.158833 cuda_h.py:19] end concat_expert_out cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:09:08.158722.158722 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.158759.158759 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:08.158522.158522 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005323886871337891 seconds
DEBUG 01-15 16:09:08.158954.158954 cuda_h.py:19] end gpu_experts_multi_device cost 0.050142526626586914 seconds
DEBUG 01-15 16:09:08.158633.158633 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.05982613563537598 seconds
DEBUG 01-15 16:09:08.159947.159947 cuda_h.py:19] end prefill_layer cost 0.06583809852600098 seconds
DEBUG 01-15 16:09:08.159803.159803 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:09:08.159937.159937 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.159832.159832 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:09:08.159773.159773 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:09:08.159098.159098 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:09:08.159492.159492 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:09:08.159870.159870 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 8.749961853027344e-05 seconds
DEBUG 01-15 16:09:08.159427.159427 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.159701.159701 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.159863.159863 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.159514.159514 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.159701.159701 cuda_h.py:19] end allocate_cuda_memory cost 0.00020575523376464844 seconds
DEBUG 01-15 16:09:08.159466.159466 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.159925.159925 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.160120.160120 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.160573.160573 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.160567.160567 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8b3e236a-85bf-4005-a8d9-d923b8c761ce
DEBUG 01-15 16:09:08.160425.160425 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.160959.160959 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.161441.161441 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8b3e236a-85bf-4005-a8d9-d923b8c761ce
DEBUG 01-15 16:09:08.161562.161562 cuda_h.py:19] end load_into_gpu_async cost 0.0015211105346679688 seconds
DEBUG 01-15 16:09:08.161073.161073 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.161871.161871 cuda_h.py:19] end restore_tensors2 cost 7.009506225585938e-05 seconds
DEBUG 01-15 16:09:08.161104.161104 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002216339111328125 seconds
INFO 01-15 16:09:08.161370.161370 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8b3e236a-85bf-4005-a8d9-d923b8c761ce
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.163874.163874 cuda_h.py:19] end self_attn cost 0.002920389175415039 seconds
DEBUG 01-15 16:09:08.163109.163109 cuda_h.py:19] end iln_self_attn_paln cost 0.004507303237915039 seconds
DEBUG 01-15 16:09:08.163031.163031 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:09:08.163218.163218 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.164207.164207 cuda_h.py:19] end gate cost 0.0006253719329833984 seconds
DEBUG 01-15 16:09:08.164467.164467 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.164147.164147 lmp.py:1912] 
DEBUG 01-15 16:09:08.164147.164147 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.165618.165618 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.165506.165506 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.165295.165295 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.165958.165958 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.165316.165316 lmp.py:1917] 
DEBUG 01-15 16:09:08.165316.165316 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.165436.165436 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.165516.165516 lmp.py:1935]   Expert 14 |     65 | CPU
DEBUG 01-15 16:09:08.165398.165398 lmp.py:1935]   Expert 57 |     72 | CPU
DEBUG 01-15 16:09:08.165564.165564 lmp.py:1935]   Expert 13 |     76 | CPU
DEBUG 01-15 16:09:08.165730.165730 lmp.py:1935]   Expert 26 |     80 | CPU
DEBUG 01-15 16:09:08.165373.165373 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:09:08.165255.165255 lmp.py:1935]   Expert 54 |     91 | CPU
DEBUG 01-15 16:09:08.165136.165136 lmp.py:1935]   Expert 11 |     94 | CPU
DEBUG 01-15 16:09:08.165540.165540 lmp.py:1935]   Expert 45 |     95 | CPU
DEBUG 01-15 16:09:08.165230.165230 lmp.py:1935]   Expert 58 |    102 | CPU
DEBUG 01-15 16:09:08.165158.165158 lmp.py:1935]   Expert 30 |    108 | CPU
DEBUG 01-15 16:09:08.165324.165324 lmp.py:1935]   Expert 51 |    108 | CPU
DEBUG 01-15 16:09:08.165490.165490 lmp.py:1935]   Expert 36 |    111 | CPU
DEBUG 01-15 16:09:08.165417.165417 lmp.py:1935]   Expert 10 |    114 | CPU
DEBUG 01-15 16:09:08.165868.165868 lmp.py:1935]   Expert 32 |    114 | CPU
DEBUG 01-15 16:09:08.165558.165558 lmp.py:1935]   Expert 20 |    128 | CPU
DEBUG 01-15 16:09:08.165247.165247 lmp.py:1935]   Expert  8 |    134 | CPU
DEBUG 01-15 16:09:08.165175.165175 lmp.py:1935]   Expert  4 |    137 | CPU
DEBUG 01-15 16:09:08.165864.165864 lmp.py:1935]   Expert 63 |    139 | CPU
DEBUG 01-15 16:09:08.165553.165553 lmp.py:1935]   Expert 53 |    140 | CPU
DEBUG 01-15 16:09:08.165958.165958 lmp.py:1935]   Expert 61 |    143 | CPU
DEBUG 01-15 16:09:08.165124.165124 lmp.py:1935]   Expert 34 |    144 | CPU
DEBUG 01-15 16:09:08.165052.165052 lmp.py:1935]   Expert 47 |    147 | CPU
DEBUG 01-15 16:09:08.165456.165456 lmp.py:1935]   Expert 16 |    148 | CPU
DEBUG 01-15 16:09:08.165907.165907 lmp.py:1935]   Expert 28 |    158 | CPU
DEBUG 01-15 16:09:08.165596.165596 lmp.py:1935]   Expert 42 |    159 | CPU
DEBUG 01-15 16:09:08.165670.165670 lmp.py:1935]   Expert 60 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165551.165551 lmp.py:1935]   Expert 17 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165685.165685 lmp.py:1935]   Expert 29 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165328.165328 lmp.py:1935]   Expert 44 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165971.165971 lmp.py:1935]   Expert 27 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165852.165852 lmp.py:1935]   Expert  7 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165541.165541 lmp.py:1935]   Expert 41 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165708.165708 lmp.py:1935]   Expert 48 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165874.165874 lmp.py:1935]   Expert 56 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165040.165040 lmp.py:1935]   Expert  3 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165206.165206 lmp.py:1935]   Expert  9 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165372.165372 lmp.py:1935]   Expert  2 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165538.165538 lmp.py:1935]   Expert 15 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165704.165704 lmp.py:1935]   Expert 24 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165870.165870 lmp.py:1935]   Expert  0 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165752.165752 lmp.py:1935]   Expert 18 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165633.165633 lmp.py:1935]   Expert 55 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165799.165799 lmp.py:1935]   Expert 23 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165727.165727 lmp.py:1935]   Expert 38 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165655.165655 lmp.py:1935]   Expert 40 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165344.165344 lmp.py:1935]   Expert 22 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.165510.165510 lmp.py:1935]   Expert  6 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.165438.165438 lmp.py:1935]   Expert 37 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.166604.166604 lmp.py:1935]   Expert 46 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.166532.166532 lmp.py:1935]   Expert 19 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.166936.166936 lmp.py:1935]   Expert 39 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.166579.166579 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.166222.166222 lmp.py:1935]   Expert 50 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.166388.166388 lmp.py:1935]   Expert 12 |    263 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.166316.166316 lmp.py:1935]   Expert 62 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.166244.166244 lmp.py:1935]   Expert 21 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.166410.166410 lmp.py:1935]   Expert 35 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.166576.166576 lmp.py:1935]   Expert 49 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.166504.166504 lmp.py:1935]   Expert 52 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.166193.166193 lmp.py:1935]   Expert 33 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.166075.166075 lmp.py:1935]   Expert  1 |    348 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.166718.166718 lmp.py:1935]   Expert  5 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.166884.166884 lmp.py:1935]   Expert 43 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.166335.166335 lmp.py:1935]   Expert 59 |    585 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.166309.166309 lmp.py:1937] 
DEBUG 01-15 16:09:08.166309.166309 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.166998.166998 lmp.py:1938]   CPU:   2898 tokens
DEBUG 01-15 16:09:08.166164.166164 lmp.py:1942]   cuda:1:   4639 tokens (19 experts)
DEBUG 01-15 16:09:08.166092.166092 lmp.py:1942]   cuda:2:   4751 tokens (20 experts)
DEBUG 01-15 16:09:08.166304.166304 lmp.py:1943]   Total GPU:   9390 tokens
DEBUG 01-15 16:09:08.166517.166517 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.166517.166517 lmp.py:1944] 
DEBUG 01-15 16:09:08.166690.166690 cuda_h.py:19] end experts_map_get cost 0.0016803741455078125 seconds
DEBUG 01-15 16:09:08.166871.166871 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.166481.166481 lmp.py:1953] 
DEBUG 01-15 16:09:08.166481.166481 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.166933.166933 cuda_h.py:19] end cpu_experts_submit cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:09:08.166696.166696 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.166625.166625 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.168378.168378 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.38.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.38.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.38.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.27.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.27.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.27.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.4.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.4.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.4.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6217007104, 5767168, 17301504, 0), (6222774272, 5767168, 23068672, 0), (6211239936, 5767168, 28835840, 0), (6286213120, 5767168, 34603008, 0), (6291980288, 5767168, 40370176, 0), (6280445952, 5767168, 46137344, 0), (6303514624, 5767168, 51904512, 0), (6309281792, 5767168, 57671680, 0), (6297747456, 5767168, 63438848, 0), (6338117632, 5767168, 69206016, 0), (6343884800, 5767168, 74973184, 0), (6332350464, 5767168, 80740352, 0), (6476529664, 5767168, 86507520, 0), (6482296832, 5767168, 92274688, 0), (6470762496, 5767168, 98041856, 0), (6493831168, 5767168, 103809024, 0), (6499598336, 5767168, 109576192, 0), (6488064000, 5767168, 115343360, 0), (6597640192, 5767168, 121110528, 0), (6603407360, 5767168, 126877696, 0), (6591873024, 5767168, 132644864, 0), (6787956736, 5767168, 138412032, 0), (6793723904, 5767168, 144179200, 0), (6782189568, 5767168, 149946368, 0), (6839861248, 5767168, 155713536, 0), (6845628416, 5767168, 161480704, 0), (6834094080, 5767168, 167247872, 0), (6857162752, 5767168, 173015040, 0), (6862929920, 5767168, 178782208, 0), (6851395584, 5767168, 184549376, 0), (6874464256, 5767168, 190316544, 0), (6880231424, 5767168, 196083712, 0), (6868697088, 5767168, 201850880, 0), (6943670272, 5767168, 207618048, 0), (6949437440, 5767168, 213385216, 0), (6937903104, 5767168, 219152384, 0), (6978273280, 5767168, 224919552, 0), (6984040448, 5767168, 230686720, 0), (6972506112, 5767168, 236453888, 0), (7012876288, 5767168, 242221056, 0), (7018643456, 5767168, 247988224, 0), (7007109120, 5767168, 253755392, 0), (7047479296, 5767168, 259522560, 0), (7053246464, 5767168, 265289728, 0), (7041712128, 5767168, 271056896, 0), (7082082304, 5767168, 276824064, 0), (7087849472, 5767168, 282591232, 0), (7076315136, 5767168, 288358400, 0), (7203192832, 5767168, 294125568, 0), (7208960000, 5767168, 299892736, 0), (7197425664, 5767168, 305659904, 0), (7255097344, 5767168, 311427072, 0), (7260864512, 5767168, 317194240, 0), (7249330176, 5767168, 322961408, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6234308608, 5767168, 17301504, 0), (6240075776, 5767168, 23068672, 0), (6228541440, 5767168, 28835840, 0), (6268911616, 5767168, 34603008, 0), (6274678784, 5767168, 40370176, 0), (6263144448, 5767168, 46137344, 0), (6390022144, 5767168, 51904512, 0), (6395789312, 5767168, 57671680, 0), (6384254976, 5767168, 63438848, 0), (6441926656, 5767168, 69206016, 0), (6447693824, 5767168, 74973184, 0), (6436159488, 5767168, 80740352, 0), (6511132672, 5767168, 86507520, 0), (6516899840, 5767168, 92274688, 0), (6505365504, 5767168, 98041856, 0), (6545735680, 5767168, 103809024, 0), (6551502848, 5767168, 109576192, 0), (6539968512, 5767168, 115343360, 0), (6563037184, 5767168, 121110528, 0), (6568804352, 5767168, 126877696, 0), (6557270016, 5767168, 132644864, 0), (6580338688, 5767168, 138412032, 0), (6586105856, 5767168, 144179200, 0), (6574571520, 5767168, 149946368, 0), (6614941696, 5767168, 155713536, 0), (6620708864, 5767168, 161480704, 0), (6609174528, 5767168, 167247872, 0), (6649544704, 5767168, 173015040, 0), (6655311872, 5767168, 178782208, 0), (6643777536, 5767168, 184549376, 0), (6684147712, 5767168, 190316544, 0), (6689914880, 5767168, 196083712, 0), (6678380544, 5767168, 201850880, 0), (6753353728, 5767168, 207618048, 0), (6759120896, 5767168, 213385216, 0), (6747586560, 5767168, 219152384, 0), (6822559744, 5767168, 224919552, 0), (6828326912, 5767168, 230686720, 0), (6816792576, 5767168, 236453888, 0), (6891765760, 5767168, 242221056, 0), (6897532928, 5767168, 247988224, 0), (6885998592, 5767168, 253755392, 0), (6926368768, 5767168, 259522560, 0), (6932135936, 5767168, 265289728, 0), (6920601600, 5767168, 271056896, 0), (7030177792, 5767168, 276824064, 0), (7035944960, 5767168, 282591232, 0), (7024410624, 5767168, 288358400, 0), (7133986816, 5767168, 294125568, 0), (7139753984, 5767168, 299892736, 0), (7128219648, 5767168, 305659904, 0), (7151288320, 5767168, 311427072, 0), (7157055488, 5767168, 317194240, 0), (7145521152, 5767168, 322961408, 0), (7220494336, 5767168, 328728576, 0), (7226261504, 5767168, 334495744, 0), (7214727168, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc75bab0>, 2: <capsule object NULL at 0x74a8143f7660>}
DEBUG 01-15 16:09:08.168835.168835 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.169294.169294 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.169427.169427 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 86c16b0e-3ecb-4445-a498-afe9c29c0767
DEBUG 01-15 16:09:08.169441.169441 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.169911.169911 client.py:127] Model loaded
DEBUG 01-15 16:09:08.169694.169694 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.169318.169318 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.169548.169548 cuda_h.py:19] end restore2model cost 0.0003516674041748047 seconds
DEBUG 01-15 16:09:08.169602.169602 cuda_h.py:19] end sllm_worker_task cost 0.010419845581054688 seconds
DEBUG 01-15 16:09:08.169271.169271 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.170263.170263 cuda_h.py:19] end move_flatidxs cost 0.0008628368377685547 seconds
DEBUG 01-15 16:09:08.170927.170927 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:08.170543.170543 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 86c16b0e-3ecb-4445-a498-afe9c29c0767
DEBUG 01-15 16:09:08.171496.171496 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004822254180908203 seconds
DEBUG 01-15 16:09:08.171625.171625 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.174819.174819 cuda_h.py:19] end restore2model cost 0.003197193145751953 seconds
DEBUG 01-15 16:09:08.174053.174053 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008282899856567383 seconds
DEBUG 01-15 16:09:08.174279.174279 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.175932.175932 cuda_h.py:19] end gpu_sexperts cost 0.0002741813659667969 seconds
DEBUG 01-15 16:09:08.175331.175331 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.175074.175074 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.8848648071289062e-05 seconds
DEBUG 01-15 16:09:08.175440.175440 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.175573.175573 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.176698.176698 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009400844573974609 seconds
DEBUG 01-15 16:09:08.176436.176436 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.176206.176206 cuda_h.py:19] end gpu_group_list cost 0.0002181529998779297 seconds
DEBUG 01-15 16:09:08.177234.177234 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.177143.177143 cuda_h.py:19] end group_tensors cost 0.006394624710083008 seconds
DEBUG 01-15 16:09:08.178845.178845 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.178409.178409 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012247562408447266 seconds
DEBUG 01-15 16:09:08.178499.178499 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.179082.179082 cuda_h.py:19] end gpu_group_list cost 0.0003502368927001953 seconds
DEBUG 01-15 16:09:08.180387.180387 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.180065.180065 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 86c16b0e-3ecb-4445-a498-afe9c29c0767
DEBUG 01-15 16:09:08.181639.181639 cuda_h.py:19] end group pad cost 0.003474712371826172 seconds
DEBUG 01-15 16:09:08.181906.181906 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:08.210599.210599 client.py:127] Model loaded
DEBUG 01-15 16:09:08.211399.211399 cuda_h.py:19] end wait_experts_multi_device cost 0.03068065643310547 seconds
DEBUG 01-15 16:09:08.211227.211227 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.213230.213230 cuda_h.py:19] end group_einsum cost 0.03198432922363281 seconds
DEBUG 01-15 16:09:08.214576.214576 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.218193.218193 cuda_h.py:19] end get_outputs_cpu1 cost 0.003729581832885742 seconds
DEBUG 01-15 16:09:08.219111.219111 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.049724578857421875 seconds
DEBUG 01-15 16:09:08.220509.220509 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008682489395141602 seconds
DEBUG 01-15 16:09:08.220688.220688 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.221877.221877 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.221030.221030 cuda_h.py:19] end index_scatter cost 7.62939453125e-05 seconds
DEBUG 01-15 16:09:08.222965.222965 cuda_h.py:19] end cpuoutputsdeal cost 0.0015120506286621094 seconds
DEBUG 01-15 16:09:08.222596.222596 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.222843.222843 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.222967.222967 cuda_h.py:19] end gpu_group_tensor cost 0.00013256072998046875 seconds
DEBUG 01-15 16:09:08.222492.222492 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.222810.222810 cuda_h.py:19] end gpu_group_tensor cost 0.0003781318664550781 seconds
DEBUG 01-15 16:09:08.222251.222251 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.223927.223927 cuda_h.py:19] end gpu_group_einsum cost 0.0005478858947753906 seconds
DEBUG 01-15 16:09:08.223249.223249 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.224801.224801 cuda_h.py:19] end gpu_group_einsum cost 0.00038433074951171875 seconds
DEBUG 01-15 16:09:08.224440.224440 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.224039.224039 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.224716.224716 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001819133758544922 seconds
DEBUG 01-15 16:09:08.224571.224571 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.224103.224103 cuda_h.py:19] end concat_expert_out cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:09:08.224701.224701 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.224777.224777 cuda_h.py:19] end index_scatter cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:08.224102.224102 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007262229919433594 seconds
DEBUG 01-15 16:09:08.225780.225780 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.225663.225663 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.225244.225244 cuda_h.py:19] end all_expert_outputs_slices cost 0.00011968612670898438 seconds
DEBUG 01-15 16:09:08.225139.225139 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.225717.225717 cuda_h.py:19] end concat_expert_out cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:08.225646.225646 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.225848.225848 cuda_h.py:19] end index_scatter cost 4.696846008300781e-05 seconds
DEBUG 01-15 16:09:08.225319.225319 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00043487548828125 seconds
DEBUG 01-15 16:09:08.225447.225447 cuda_h.py:19] end gpu_experts_multi_device cost 0.05022883415222168 seconds
DEBUG 01-15 16:09:08.225065.225065 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.06168842315673828 seconds
DEBUG 01-15 16:09:08.225494.225494 cuda_h.py:19] end prefill_layer cost 0.06684756278991699 seconds
DEBUG 01-15 16:09:08.226436.226436 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:09:08.226040.226040 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.226882.226882 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:09:08.226247.226247 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:09:08.226519.226519 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:09:08.226693.226693 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 3.409385681152344e-05 seconds
DEBUG 01-15 16:09:08.226396.226396 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:09:08.226662.226662 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.226571.226571 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.226865.226865 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.226795.226795 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.226777.226777 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.226688.226688 cuda_h.py:19] end allocate_cuda_memory cost 0.00024700164794921875 seconds
DEBUG 01-15 16:09:08.226710.226710 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.226334.226334 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.227356.227356 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.227873.227873 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0040cad0-1082-4b43-8f70-4bef5d770709
DEBUG 01-15 16:09:08.227923.227923 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.227270.227270 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.228614.228614 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0040cad0-1082-4b43-8f70-4bef5d770709
DEBUG 01-15 16:09:08.229140.229140 cuda_h.py:19] end load_into_gpu_async cost 0.0021028518676757812 seconds
DEBUG 01-15 16:09:08.229373.229373 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.229383.229383 cuda_h.py:19] end restore_tensors2 cost 8.273124694824219e-05 seconds
DEBUG 01-15 16:09:08.229822.229822 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002724170684814453 seconds
INFO 01-15 16:09:08.229241.229241 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0040cad0-1082-4b43-8f70-4bef5d770709
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.230514.230514 cuda_h.py:19] end self_attn cost 0.0028905868530273438 seconds
DEBUG 01-15 16:09:08.230676.230676 cuda_h.py:19] end iln_self_attn_paln cost 0.004369974136352539 seconds
DEBUG 01-15 16:09:08.230791.230791 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:09:08.230408.230408 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.231992.231992 cuda_h.py:19] end gate cost 0.0006070137023925781 seconds
DEBUG 01-15 16:09:08.231775.231775 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.231548.231548 lmp.py:1912] 
DEBUG 01-15 16:09:08.231548.231548 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.231132.231132 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.231358.231358 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.231147.231147 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.231551.231551 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.231002.231002 lmp.py:1917] 
DEBUG 01-15 16:09:08.231002.231002 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.231328.231328 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.231931.231931 lmp.py:1935]   Expert 34 |     25 | CPU
DEBUG 01-15 16:09:08.231574.231574 lmp.py:1935]   Expert 45 |     66 | CPU
DEBUG 01-15 16:09:08.231263.231263 lmp.py:1935]   Expert 22 |     74 | CPU
DEBUG 01-15 16:09:08.231622.231622 lmp.py:1935]   Expert 57 |     77 | CPU
DEBUG 01-15 16:09:08.232311.232311 lmp.py:1935]   Expert 17 |     95 | CPU
DEBUG 01-15 16:09:08.232523.232523 lmp.py:1935]   Expert 15 |     99 | CPU
DEBUG 01-15 16:09:08.232974.232974 lmp.py:1935]   Expert  4 |    100 | CPU
DEBUG 01-15 16:09:08.232948.232948 lmp.py:1935]   Expert 28 |    107 | CPU
DEBUG 01-15 16:09:08.232922.232922 lmp.py:1935]   Expert 32 |    112 | CPU
DEBUG 01-15 16:09:08.232135.232135 lmp.py:1935]   Expert 60 |    113 | CPU
DEBUG 01-15 16:09:08.232586.232586 lmp.py:1935]   Expert 36 |    124 | CPU
DEBUG 01-15 16:09:08.232560.232560 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:09:08.232534.232534 lmp.py:1935]   Expert 12 |    127 | CPU
DEBUG 01-15 16:09:08.232269.232269 lmp.py:1935]   Expert 14 |    128 | CPU
DEBUG 01-15 16:09:08.232436.232436 lmp.py:1935]   Expert 52 |    130 | CPU
DEBUG 01-15 16:09:08.232840.232840 lmp.py:1935]   Expert 25 |    131 | CPU
DEBUG 01-15 16:09:08.232291.232291 lmp.py:1935]   Expert  8 |    134 | CPU
DEBUG 01-15 16:09:08.232265.232265 lmp.py:1935]   Expert  2 |    139 | CPU
DEBUG 01-15 16:09:08.232477.232477 lmp.py:1935]   Expert 35 |    142 | CPU
DEBUG 01-15 16:09:08.232690.232690 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:09:08.232426.232426 lmp.py:1935]   Expert 30 |    152 | CPU
DEBUG 01-15 16:09:08.232400.232400 lmp.py:1935]   Expert 23 |    154 | CPU
DEBUG 01-15 16:09:08.232897.232897 lmp.py:1935]   Expert  0 |    157 | CPU
DEBUG 01-15 16:09:08.232871.232871 lmp.py:1935]   Expert 39 |    157 | CPU
DEBUG 01-15 16:09:08.232845.232845 lmp.py:1935]   Expert 61 |    157 | CPU
DEBUG 01-15 16:09:08.232978.232978 lmp.py:1935]   Expert  3 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232575.232575 lmp.py:1935]   Expert 13 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232689.232689 lmp.py:1935]   Expert 42 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232193.232193 lmp.py:1935]   Expert 44 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232598.232598 lmp.py:1935]   Expert 31 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232002.232002 lmp.py:1935]   Expert  9 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232168.232168 lmp.py:1935]   Expert 41 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232096.232096 lmp.py:1935]   Expert 46 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232262.232262 lmp.py:1935]   Expert 43 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232621.232621 lmp.py:1935]   Expert 18 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232740.232740 lmp.py:1935]   Expert 26 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232906.232906 lmp.py:1935]   Expert 27 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232834.232834 lmp.py:1935]   Expert 49 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232762.232762 lmp.py:1935]   Expert 50 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232451.232451 lmp.py:1935]   Expert 62 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232379.232379 lmp.py:1935]   Expert 51 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232591.232591 lmp.py:1935]   Expert 11 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232281.232281 lmp.py:1935]   Expert 47 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232970.232970 lmp.py:1935]   Expert 19 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232851.232851 lmp.py:1935]   Expert 63 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232733.232733 lmp.py:1935]   Expert 20 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232661.232661 lmp.py:1935]   Expert 55 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232350.232350 lmp.py:1935]   Expert 56 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232039.232039 lmp.py:1935]   Expert 38 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232967.232967 lmp.py:1935]   Expert 48 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232133.232133 lmp.py:1935]   Expert  1 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232584.232584 lmp.py:1935]   Expert 10 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232273.232273 lmp.py:1935]   Expert 54 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232870.232870 lmp.py:1935]   Expert  7 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232705.232705 lmp.py:1935]   Expert 21 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232586.232586 lmp.py:1935]   Expert 33 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232468.232468 lmp.py:1935]   Expert 29 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232111.232111 lmp.py:1935]   Expert 40 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.232230.232230 lmp.py:1935]   Expert 24 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.232873.232873 lmp.py:1935]   Expert 59 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.233755.233755 lmp.py:1935]   Expert 37 |    331 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.233636.233636 lmp.py:1935]   Expert 58 |    366 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.233518.233518 lmp.py:1935]   Expert  6 |    389 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.233413.233413 lmp.py:1935]   Expert 53 |    854 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.233532.233532 lmp.py:1937] 
DEBUG 01-15 16:09:08.233532.233532 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.233937.233937 lmp.py:1938]   CPU:   2972 tokens
DEBUG 01-15 16:09:08.233341.233341 lmp.py:1942]   cuda:1:   4663 tokens (19 experts)
DEBUG 01-15 16:09:08.233269.233269 lmp.py:1942]   cuda:2:   4653 tokens (20 experts)
DEBUG 01-15 16:09:08.233482.233482 lmp.py:1943]   Total GPU:   9316 tokens
DEBUG 01-15 16:09:08.233979.233979 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.233979.233979 lmp.py:1944] 
DEBUG 01-15 16:09:08.233635.233635 cuda_h.py:19] end experts_map_get cost 0.001722574234008789 seconds
DEBUG 01-15 16:09:08.233201.233201 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.233718.233718 lmp.py:1953] 
DEBUG 01-15 16:09:08.233718.233718 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.233839.233839 cuda_h.py:19] end cpu_experts_submit cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:08.233866.233866 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.233126.233126 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.235444.235444 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.3.gate_proj.weight': 0, 'model.layers.5.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.59.up_proj.weight': 322961408}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.21.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.21.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.21.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.24.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.24.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.24.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.26.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.26.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.26.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.29.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.29.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.29.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.37.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.37.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.37.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.38.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.38.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.38.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.41.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.41.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.41.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.42.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.42.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.42.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.44.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.44.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.44.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.46.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.46.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.46.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.5.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.5.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.5.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(7341604864, 5767168, 0, 0), (7347372032, 5767168, 5767168, 0), (7335837696, 5767168, 11534336, 0), (7410810880, 5767168, 17301504, 0), (7416578048, 5767168, 23068672, 0), (7405043712, 5767168, 28835840, 0), (7445413888, 5767168, 34603008, 0), (7451181056, 5767168, 40370176, 0), (7439646720, 5767168, 46137344, 0), (7462715392, 5767168, 51904512, 0), (7468482560, 5767168, 57671680, 0), (7456948224, 5767168, 63438848, 0), (7480016896, 5767168, 69206016, 0), (7485784064, 5767168, 74973184, 0), (7474249728, 5767168, 80740352, 0), (7514619904, 5767168, 86507520, 0), (7520387072, 5767168, 92274688, 0), (7508852736, 5767168, 98041856, 0), (7601127424, 5767168, 103809024, 0), (7606894592, 5767168, 109576192, 0), (7595360256, 5767168, 115343360, 0), (7618428928, 5767168, 121110528, 0), (7624196096, 5767168, 126877696, 0), (7612661760, 5767168, 132644864, 0), (7635730432, 5767168, 138412032, 0), (7641497600, 5767168, 144179200, 0), (7629963264, 5767168, 149946368, 0), (7756840960, 5767168, 155713536, 0), (7762608128, 5767168, 161480704, 0), (7751073792, 5767168, 167247872, 0), (7826046976, 5767168, 173015040, 0), (7831814144, 5767168, 178782208, 0), (7820279808, 5767168, 184549376, 0), (7860649984, 5767168, 190316544, 0), (7866417152, 5767168, 196083712, 0), (7854882816, 5767168, 201850880, 0), (7981760512, 5767168, 207618048, 0), (7987527680, 5767168, 213385216, 0), (7975993344, 5767168, 219152384, 0), (8033665024, 5767168, 224919552, 0), (8039432192, 5767168, 230686720, 0), (8027897856, 5767168, 236453888, 0), (8120172544, 5767168, 242221056, 0), (8125939712, 5767168, 247988224, 0), (8114405376, 5767168, 253755392, 0), (8154775552, 5767168, 259522560, 0), (8160542720, 5767168, 265289728, 0), (8149008384, 5767168, 271056896, 0), (8206680064, 5767168, 276824064, 0), (8212447232, 5767168, 282591232, 0), (8200912896, 5767168, 288358400, 0), (8258584576, 5767168, 294125568, 0), (8264351744, 5767168, 299892736, 0), (8252817408, 5767168, 305659904, 0), (8310489088, 5767168, 311427072, 0), (8316256256, 5767168, 317194240, 0), (8304721920, 5767168, 322961408, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (7393509376, 5767168, 17301504, 0), (7399276544, 5767168, 23068672, 0), (7387742208, 5767168, 28835840, 0), (7653031936, 5767168, 34603008, 0), (7658799104, 5767168, 40370176, 0), (7647264768, 5767168, 46137344, 0), (7704936448, 5767168, 51904512, 0), (7710703616, 5767168, 57671680, 0), (7699169280, 5767168, 63438848, 0), (7739539456, 5767168, 69206016, 0), (7745306624, 5767168, 74973184, 0), (7733772288, 5767168, 80740352, 0), (7791443968, 5767168, 86507520, 0), (7797211136, 5767168, 92274688, 0), (7785676800, 5767168, 98041856, 0), (7929856000, 5767168, 103809024, 0), (7935623168, 5767168, 109576192, 0), (7924088832, 5767168, 115343360, 0), (7947157504, 5767168, 121110528, 0), (7952924672, 5767168, 126877696, 0), (7941390336, 5767168, 132644864, 0), (7999062016, 5767168, 138412032, 0), (8004829184, 5767168, 144179200, 0), (7993294848, 5767168, 149946368, 0), (8016363520, 5767168, 155713536, 0), (8022130688, 5767168, 161480704, 0), (8010596352, 5767168, 167247872, 0), (8050966528, 5767168, 173015040, 0), (8056733696, 5767168, 178782208, 0), (8045199360, 5767168, 184549376, 0), (8085569536, 5767168, 190316544, 0), (8091336704, 5767168, 196083712, 0), (8079802368, 5767168, 201850880, 0), (8102871040, 5767168, 207618048, 0), (8108638208, 5767168, 213385216, 0), (8097103872, 5767168, 219152384, 0), (8137474048, 5767168, 224919552, 0), (8143241216, 5767168, 230686720, 0), (8131706880, 5767168, 236453888, 0), (8172077056, 5767168, 242221056, 0), (8177844224, 5767168, 247988224, 0), (8166309888, 5767168, 253755392, 0), (8223981568, 5767168, 259522560, 0), (8229748736, 5767168, 265289728, 0), (8218214400, 5767168, 271056896, 0), (8241283072, 5767168, 276824064, 0), (8247050240, 5767168, 282591232, 0), (8235515904, 5767168, 288358400, 0), (8293187584, 5767168, 294125568, 0), (8298954752, 5767168, 299892736, 0), (8287420416, 5767168, 305659904, 0), (8362393600, 5767168, 311427072, 0), (8368160768, 5767168, 317194240, 0), (8356626432, 5767168, 322961408, 0), (8379695104, 5767168, 328728576, 0), (8385462272, 5767168, 334495744, 0), (8373927936, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc5efd20>, 2: <capsule object NULL at 0x74a6806122e0>}
DEBUG 01-15 16:09:08.235845.235845 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.235529.235529 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.235709.235709 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2641ad41-afca-4dff-9639-94c6d99aa4cc
DEBUG 01-15 16:09:08.235755.235755 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.236564.236564 client.py:127] Model loaded
DEBUG 01-15 16:09:08.236202.236202 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.236611.236611 cuda_h.py:19] end restore2model cost 0.0003402233123779297 seconds
DEBUG 01-15 16:09:08.236758.236758 cuda_h.py:19] end sllm_worker_task cost 0.01037454605102539 seconds
DEBUG 01-15 16:09:08.237497.237497 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.237972.237972 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:08.238691.238691 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2641ad41-afca-4dff-9639-94c6d99aa4cc
DEBUG 01-15 16:09:08.238059.238059 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0050814151763916016 seconds
DEBUG 01-15 16:09:08.238089.238089 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.239456.239456 cuda_h.py:19] end move_flatidxs cost 0.001129150390625 seconds
DEBUG 01-15 16:09:08.239047.239047 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:08.241666.241666 cuda_h.py:19] end restore2model cost 0.003163576126098633 seconds
DEBUG 01-15 16:09:08.241801.241801 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008497238159179688 seconds
DEBUG 01-15 16:09:08.241358.241358 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.242765.242765 cuda_h.py:19] end gpu_sexperts cost 0.00026988983154296875 seconds
DEBUG 01-15 16:09:08.242210.242210 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.242410.242410 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4781951904296875e-05 seconds
DEBUG 01-15 16:09:08.242630.242630 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.242571.242571 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.243491.243491 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009291172027587891 seconds
DEBUG 01-15 16:09:08.243618.243618 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.243335.243335 cuda_h.py:19] end gpu_group_list cost 0.00021910667419433594 seconds
DEBUG 01-15 16:09:08.244932.244932 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.245858.245858 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010793209075927734 seconds
DEBUG 01-15 16:09:08.245642.245642 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.245611.245611 cuda_h.py:19] end gpu_group_list cost 0.0002281665802001953 seconds
DEBUG 01-15 16:09:08.246522.246522 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.246167.246167 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2641ad41-afca-4dff-9639-94c6d99aa4cc
DEBUG 01-15 16:09:08.247790.247790 cuda_h.py:19] end group_tensors cost 0.007916927337646484 seconds
DEBUG 01-15 16:09:08.248643.248643 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.251645.251645 cuda_h.py:19] end group pad cost 0.0031740665435791016 seconds
DEBUG 01-15 16:09:08.251296.251296 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:08.275963.275963 client.py:127] Model loaded
DEBUG 01-15 16:09:08.275637.275637 cuda_h.py:19] end wait_experts_multi_device cost 0.029052734375 seconds
DEBUG 01-15 16:09:08.275830.275830 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.281278.281278 cuda_h.py:19] end group_einsum cost 0.030009746551513672 seconds
DEBUG 01-15 16:09:08.281097.281097 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.284233.284233 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029859542846679688 seconds
DEBUG 01-15 16:09:08.285759.285759 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04808521270751953 seconds
DEBUG 01-15 16:09:08.286048.286048 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010572195053100586 seconds
DEBUG 01-15 16:09:08.286710.286710 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.288322.288322 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.289929.289929 cuda_h.py:19] end index_scatter cost 0.0001590251922607422 seconds
DEBUG 01-15 16:09:08.289537.289537 cuda_h.py:19] end cpuoutputsdeal cost 0.003034353256225586 seconds
DEBUG 01-15 16:09:08.289245.289245 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.290890.290890 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.290493.290493 cuda_h.py:19] end gpu_group_tensor cost 0.0003178119659423828 seconds
DEBUG 01-15 16:09:08.290146.290146 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.290754.290754 cuda_h.py:19] end gpu_group_tensor cost 0.00029659271240234375 seconds
DEBUG 01-15 16:09:08.291795.291795 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.292846.292846 cuda_h.py:19] end gpu_group_einsum cost 0.0010013580322265625 seconds
DEBUG 01-15 16:09:08.292750.292750 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.293194.293194 cuda_h.py:19] end gpu_group_einsum cost 0.0005335807800292969 seconds
DEBUG 01-15 16:09:08.293259.293259 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.293674.293674 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.293565.293565 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025653839111328125 seconds
DEBUG 01-15 16:09:08.293911.293911 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.293662.293662 cuda_h.py:19] end concat_expert_out cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:09:08.293419.293419 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.294754.294754 cuda_h.py:19] end index_scatter cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:09:08.294505.294505 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009634494781494141 seconds
DEBUG 01-15 16:09:08.294071.294071 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.294736.294736 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.294881.294881 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001461505889892578 seconds
DEBUG 01-15 16:09:08.294206.294206 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.294368.294368 cuda_h.py:19] end concat_expert_out cost 5.459785461425781e-05 seconds
DEBUG 01-15 16:09:08.294781.294781 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.294996.294996 cuda_h.py:19] end index_scatter cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:09:08.295089.295089 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005021095275878906 seconds
DEBUG 01-15 16:09:08.295073.295073 cuda_h.py:19] end gpu_experts_multi_device cost 0.052762508392333984 seconds
DEBUG 01-15 16:09:08.295797.295797 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.06444072723388672 seconds
DEBUG 01-15 16:09:08.295596.295596 cuda_h.py:19] end prefill_layer cost 0.06958889961242676 seconds
DEBUG 01-15 16:09:08.295022.295022 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:09:08.295109.295109 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.295527.295527 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:09:08.295091.295091 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:09:08.295324.295324 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:09:08.295128.295128 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 3.7670135498046875e-05 seconds
DEBUG 01-15 16:09:08.295029.295029 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:09:08.296778.296778 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.296040.296040 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.296824.296824 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.296615.296615 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.296174.296174 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.296916.296916 cuda_h.py:19] end allocate_cuda_memory cost 0.0003674030303955078 seconds
DEBUG 01-15 16:09:08.296746.296746 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.296761.296761 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.296141.296141 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.296519.296519 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f24b88fc-0dd1-47da-833f-b25b3678a83b
DEBUG 01-15 16:09:08.297285.297285 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.297981.297981 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.298120.298120 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f24b88fc-0dd1-47da-833f-b25b3678a83b
DEBUG 01-15 16:09:08.298764.298764 cuda_h.py:19] end load_into_gpu_async cost 0.0014777183532714844 seconds
DEBUG 01-15 16:09:08.298037.298037 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.298265.298265 cuda_h.py:19] end restore_tensors2 cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:09:08.298736.298736 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002197265625 seconds
INFO 01-15 16:09:08.298195.298195 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f24b88fc-0dd1-47da-833f-b25b3678a83b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.300639.300639 cuda_h.py:19] end self_attn cost 0.0031511783599853516 seconds
DEBUG 01-15 16:09:08.300252.300252 cuda_h.py:19] end iln_self_attn_paln cost 0.00480961799621582 seconds
DEBUG 01-15 16:09:08.300750.300750 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:09:08.300937.300937 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.301847.301847 cuda_h.py:19] end gate cost 0.0006375312805175781 seconds
DEBUG 01-15 16:09:08.301643.301643 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.301846.301846 lmp.py:1912] 
DEBUG 01-15 16:09:08.301846.301846 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.302808.302808 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.302412.302412 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.302962.302962 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.302651.302651 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.302625.302625 lmp.py:1917] 
DEBUG 01-15 16:09:08.302625.302625 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.302314.302314 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.302679.302679 lmp.py:1935]   Expert  1 |     45 | CPU
DEBUG 01-15 16:09:08.302846.302846 lmp.py:1935]   Expert  7 |     60 | CPU
DEBUG 01-15 16:09:08.302058.302058 lmp.py:1935]   Expert 37 |     70 | CPU
DEBUG 01-15 16:09:08.302747.302747 lmp.py:1935]   Expert 54 |     76 | CPU
DEBUG 01-15 16:09:08.302914.302914 lmp.py:1935]   Expert 17 |     77 | CPU
DEBUG 01-15 16:09:08.302603.302603 lmp.py:1935]   Expert 18 |     84 | CPU
DEBUG 01-15 16:09:08.302292.302292 lmp.py:1935]   Expert  9 |     92 | CPU
DEBUG 01-15 16:09:08.302220.302220 lmp.py:1935]   Expert 13 |     94 | CPU
DEBUG 01-15 16:09:08.302148.302148 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:09:08.302360.302360 lmp.py:1935]   Expert 22 |    103 | CPU
DEBUG 01-15 16:09:08.302049.302049 lmp.py:1935]   Expert  0 |    107 | CPU
DEBUG 01-15 16:09:08.302262.302262 lmp.py:1935]   Expert 26 |    116 | CPU
DEBUG 01-15 16:09:08.302997.302997 lmp.py:1935]   Expert 16 |    119 | CPU
DEBUG 01-15 16:09:08.302495.302495 lmp.py:1935]   Expert 10 |    122 | CPU
DEBUG 01-15 16:09:08.302230.302230 lmp.py:1935]   Expert 63 |    129 | CPU
DEBUG 01-15 16:09:08.302966.302966 lmp.py:1935]   Expert 59 |    131 | CPU
DEBUG 01-15 16:09:08.302940.302940 lmp.py:1935]   Expert 62 |    140 | CPU
DEBUG 01-15 16:09:08.302676.302676 lmp.py:1935]   Expert 43 |    142 | CPU
DEBUG 01-15 16:09:08.302411.302411 lmp.py:1935]   Expert 28 |    145 | CPU
DEBUG 01-15 16:09:08.302908.302908 lmp.py:1935]   Expert 33 |    147 | CPU
DEBUG 01-15 16:09:08.302882.302882 lmp.py:1935]   Expert 29 |    148 | CPU
DEBUG 01-15 16:09:08.302095.302095 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:09:08.302784.302784 lmp.py:1935]   Expert 51 |    163 | CPU
DEBUG 01-15 16:09:08.302997.302997 lmp.py:1935]   Expert 55 |    164 | CPU
DEBUG 01-15 16:09:08.302686.302686 lmp.py:1935]   Expert  3 |    166 | CPU
DEBUG 01-15 16:09:08.302283.302283 lmp.py:1935]   Expert 45 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302402.302402 lmp.py:1935]   Expert 11 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302569.302569 lmp.py:1935]   Expert 23 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302496.302496 lmp.py:1935]   Expert 32 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302186.302186 lmp.py:1935]   Expert 53 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302113.302113 lmp.py:1935]   Expert 40 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302279.302279 lmp.py:1935]   Expert 34 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302730.302730 lmp.py:1935]   Expert 14 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302420.302420 lmp.py:1935]   Expert 41 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302632.302632 lmp.py:1935]   Expert 52 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302845.302845 lmp.py:1935]   Expert 42 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302819.302819 lmp.py:1935]   Expert 21 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302031.302031 lmp.py:1935]   Expert 57 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302720.302720 lmp.py:1935]   Expert 30 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302410.302410 lmp.py:1935]   Expert 15 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302576.302576 lmp.py:1935]   Expert 35 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302742.302742 lmp.py:1935]   Expert 12 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302385.302385 lmp.py:1935]   Expert  4 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302551.302551 lmp.py:1935]   Expert 46 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302479.302479 lmp.py:1935]   Expert 50 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302168.302168 lmp.py:1935]   Expert 24 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302619.302619 lmp.py:1935]   Expert 19 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302593.302593 lmp.py:1935]   Expert 44 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302805.302805 lmp.py:1935]   Expert  8 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.302256.302256 lmp.py:1935]   Expert 49 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302946.302946 lmp.py:1935]   Expert 38 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.302158.302158 lmp.py:1935]   Expert 47 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.303609.303609 lmp.py:1935]   Expert  6 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.303537.303537 lmp.py:1935]   Expert 31 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.303941.303941 lmp.py:1935]   Expert 61 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.303869.303869 lmp.py:1935]   Expert 39 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.303797.303797 lmp.py:1935]   Expert  5 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.303155.303155 lmp.py:1935]   Expert 36 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.303513.303513 lmp.py:1935]   Expert 27 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.303679.303679 lmp.py:1935]   Expert 60 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.303084.303084 lmp.py:1935]   Expert 20 |    340 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.303250.303250 lmp.py:1935]   Expert 48 |    369 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.303893.303893 lmp.py:1935]   Expert 25 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.303059.303059 lmp.py:1935]   Expert 56 |    556 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.303510.303510 lmp.py:1937] 
DEBUG 01-15 16:09:08.303510.303510 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.303915.303915 lmp.py:1938]   CPU:   2898 tokens
DEBUG 01-15 16:09:08.303796.303796 lmp.py:1942]   cuda:1:   4624 tokens (19 experts)
DEBUG 01-15 16:09:08.303393.303393 lmp.py:1942]   cuda:2:   4766 tokens (20 experts)
DEBUG 01-15 16:09:08.303559.303559 lmp.py:1943]   Total GPU:   9390 tokens
DEBUG 01-15 16:09:08.303486.303486 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.303486.303486 lmp.py:1944] 
DEBUG 01-15 16:09:08.303136.303136 cuda_h.py:19] end experts_map_get cost 0.0016336441040039062 seconds
DEBUG 01-15 16:09:08.303509.303509 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.303312.303312 lmp.py:1953] 
DEBUG 01-15 16:09:08.303312.303312 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.303002.303002 cuda_h.py:19] end cpu_experts_submit cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:08.303765.303765 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.303171.303171 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.305192.305192 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.4.gate_proj.weight': 0, 'model.layers.6.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.14.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.14.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.14.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.6.mlp.experts.5.gate_proj.weight': 0, 'model.layers.6.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.47.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.47.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.47.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.48.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.48.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.48.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.6.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.6.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.6.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(8466202624, 5767168, 0, 0), (8471969792, 5767168, 5767168, 0), (8460435456, 5767168, 11534336, 0), (8500805632, 5767168, 17301504, 0), (8506572800, 5767168, 23068672, 0), (8495038464, 5767168, 28835840, 0), (8639217664, 5767168, 34603008, 0), (8644984832, 5767168, 40370176, 0), (8633450496, 5767168, 46137344, 0), (8725725184, 5767168, 51904512, 0), (8731492352, 5767168, 57671680, 0), (8719958016, 5767168, 63438848, 0), (8743026688, 5767168, 69206016, 0), (8748793856, 5767168, 74973184, 0), (8737259520, 5767168, 80740352, 0), (8760328192, 5767168, 86507520, 0), (8766095360, 5767168, 92274688, 0), (8754561024, 5767168, 98041856, 0), (8794931200, 5767168, 103809024, 0), (8800698368, 5767168, 109576192, 0), (8789164032, 5767168, 115343360, 0), (8864137216, 5767168, 121110528, 0), (8869904384, 5767168, 126877696, 0), (8858370048, 5767168, 132644864, 0), (8916041728, 5767168, 138412032, 0), (8921808896, 5767168, 144179200, 0), (8910274560, 5767168, 149946368, 0), (9002549248, 5767168, 155713536, 0), (9008316416, 5767168, 161480704, 0), (8996782080, 5767168, 167247872, 0), (9019850752, 5767168, 173015040, 0), (9025617920, 5767168, 178782208, 0), (9014083584, 5767168, 184549376, 0), (9054453760, 5767168, 190316544, 0), (9060220928, 5767168, 196083712, 0), (9048686592, 5767168, 201850880, 0), (9089056768, 5767168, 207618048, 0), (9094823936, 5767168, 213385216, 0), (9083289600, 5767168, 219152384, 0), (9244770304, 5767168, 224919552, 0), (9250537472, 5767168, 230686720, 0), (9239003136, 5767168, 236453888, 0), (9262071808, 5767168, 242221056, 0), (9267838976, 5767168, 247988224, 0), (9256304640, 5767168, 253755392, 0), (9296674816, 5767168, 259522560, 0), (9302441984, 5767168, 265289728, 0), (9290907648, 5767168, 271056896, 0), (9313976320, 5767168, 276824064, 0), (9319743488, 5767168, 282591232, 0), (9308209152, 5767168, 288358400, 0), (9365880832, 5767168, 294125568, 0), (9371648000, 5767168, 299892736, 0), (9360113664, 5767168, 305659904, 0), (9452388352, 5767168, 311427072, 0), (9458155520, 5767168, 317194240, 0), (9446621184, 5767168, 322961408, 0)], 2: [(8483504128, 5767168, 0, 0), (8489271296, 5767168, 5767168, 0), (8477736960, 5767168, 11534336, 0), (8535408640, 5767168, 17301504, 0), (8541175808, 5767168, 23068672, 0), (8529641472, 5767168, 28835840, 0), (8587313152, 5767168, 34603008, 0), (8593080320, 5767168, 40370176, 0), (8581545984, 5767168, 46137344, 0), (8604614656, 5767168, 51904512, 0), (8610381824, 5767168, 57671680, 0), (8598847488, 5767168, 63438848, 0), (8656519168, 5767168, 69206016, 0), (8662286336, 5767168, 74973184, 0), (8650752000, 5767168, 80740352, 0), (8812232704, 5767168, 86507520, 0), (8817999872, 5767168, 92274688, 0), (8806465536, 5767168, 98041856, 0), (8829534208, 5767168, 103809024, 0), (8835301376, 5767168, 109576192, 0), (8823767040, 5767168, 115343360, 0), (8933343232, 5767168, 121110528, 0), (8939110400, 5767168, 126877696, 0), (8927576064, 5767168, 132644864, 0), (8950644736, 5767168, 138412032, 0), (8956411904, 5767168, 144179200, 0), (8944877568, 5767168, 149946368, 0), (8985247744, 5767168, 155713536, 0), (8991014912, 5767168, 161480704, 0), (8979480576, 5767168, 167247872, 0), (9071755264, 5767168, 173015040, 0), (9077522432, 5767168, 178782208, 0), (9065988096, 5767168, 184549376, 0), (9106358272, 5767168, 190316544, 0), (9112125440, 5767168, 196083712, 0), (9100591104, 5767168, 201850880, 0), (9123659776, 5767168, 207618048, 0), (9129426944, 5767168, 213385216, 0), (9117892608, 5767168, 219152384, 0), (9158262784, 5767168, 224919552, 0), (9164029952, 5767168, 230686720, 0), (9152495616, 5767168, 236453888, 0), (9175564288, 5767168, 242221056, 0), (9181331456, 5767168, 247988224, 0), (9169797120, 5767168, 253755392, 0), (9192865792, 5767168, 259522560, 0), (9198632960, 5767168, 265289728, 0), (9187098624, 5767168, 271056896, 0), (9210167296, 5767168, 276824064, 0), (9215934464, 5767168, 282591232, 0), (9204400128, 5767168, 288358400, 0), (9227468800, 5767168, 294125568, 0), (9233235968, 5767168, 299892736, 0), (9221701632, 5767168, 305659904, 0), (9383182336, 5767168, 311427072, 0), (9388949504, 5767168, 317194240, 0), (9377415168, 5767168, 322961408, 0), (9435086848, 5767168, 328728576, 0), (9440854016, 5767168, 334495744, 0), (9429319680, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8143f7570>, 2: <capsule object NULL at 0x74a6bc4fccf0>}
DEBUG 01-15 16:09:08.305695.305695 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.305485.305485 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.305095.305095 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 39a279ac-1c16-482d-92c8-d1379e620a8b
DEBUG 01-15 16:09:08.305433.305433 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.305747.305747 client.py:127] Model loaded
DEBUG 01-15 16:09:08.306338.306338 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.306996.306996 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.306310.306310 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.306460.306460 cuda_h.py:19] end restore2model cost 0.0003788471221923828 seconds
DEBUG 01-15 16:09:08.306468.306468 cuda_h.py:19] end sllm_worker_task cost 0.01025843620300293 seconds
DEBUG 01-15 16:09:08.307348.307348 cuda_h.py:19] end move_flatidxs cost 0.0008661746978759766 seconds
DEBUG 01-15 16:09:08.307760.307760 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:08.307293.307293 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 39a279ac-1c16-482d-92c8-d1379e620a8b
DEBUG 01-15 16:09:08.308450.308450 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004562854766845703 seconds
DEBUG 01-15 16:09:08.308910.308910 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.311360.311360 cuda_h.py:19] end restore2model cost 0.0031058788299560547 seconds
DEBUG 01-15 16:09:08.311270.311270 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007933855056762695 seconds
DEBUG 01-15 16:09:08.311827.311827 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.311764.311764 cuda_h.py:19] end gpu_sexperts cost 0.00027370452880859375 seconds
DEBUG 01-15 16:09:08.311355.311355 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.311840.311840 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.3589859008789062e-05 seconds
DEBUG 01-15 16:09:08.311536.311536 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.311524.311524 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.312511.312511 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009770393371582031 seconds
DEBUG 01-15 16:09:08.313044.313044 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.313151.313151 cuda_h.py:19] end gpu_group_list cost 0.0002257823944091797 seconds
DEBUG 01-15 16:09:08.312623.312623 cuda_h.py:19] end group_tensors cost 0.005328655242919922 seconds
DEBUG 01-15 16:09:08.313788.313788 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.314637.314637 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.315638.315638 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001524209976196289 seconds
DEBUG 01-15 16:09:08.316841.316841 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.316312.316312 cuda_h.py:19] end gpu_group_list cost 0.00037479400634765625 seconds
DEBUG 01-15 16:09:08.316662.316662 cuda_h.py:19] end group pad cost 0.003283977508544922 seconds
DEBUG 01-15 16:09:08.316319.316319 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:08.318006.318006 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.318063.318063 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 39a279ac-1c16-482d-92c8-d1379e620a8b
INFO 01-15 16:09:08.348103.348103 client.py:127] Model loaded
DEBUG 01-15 16:09:08.348591.348591 cuda_h.py:19] end wait_experts_multi_device cost 0.030018091201782227 seconds
DEBUG 01-15 16:09:08.348931.348931 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.351589.351589 cuda_h.py:19] end group_einsum cost 0.034063100814819336 seconds
DEBUG 01-15 16:09:08.351494.351494 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.354671.354671 cuda_h.py:19] end get_outputs_cpu1 cost 0.0028448104858398438 seconds
DEBUG 01-15 16:09:08.354249.354249 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04886817932128906 seconds
DEBUG 01-15 16:09:08.355251.355251 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006892681121826172 seconds
DEBUG 01-15 16:09:08.355296.355296 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.357155.357155 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.358534.358534 cuda_h.py:19] end index_scatter cost 0.0002925395965576172 seconds
DEBUG 01-15 16:09:08.358403.358403 cuda_h.py:19] end cpuoutputsdeal cost 0.0027077198028564453 seconds
DEBUG 01-15 16:09:08.358094.358094 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.358188.358188 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.359632.359632 cuda_h.py:19] end gpu_group_tensor cost 0.00015425682067871094 seconds
DEBUG 01-15 16:09:08.359017.359017 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.359360.359360 cuda_h.py:19] end gpu_group_tensor cost 0.00011754035949707031 seconds
DEBUG 01-15 16:09:08.359648.359648 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.359926.359926 cuda_h.py:19] end gpu_group_einsum cost 0.0005414485931396484 seconds
DEBUG 01-15 16:09:08.360063.360063 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.360253.360253 cuda_h.py:19] end gpu_group_einsum cost 0.00046539306640625 seconds
DEBUG 01-15 16:09:08.360992.360992 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.360606.360606 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.361522.361522 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020742416381835938 seconds
DEBUG 01-15 16:09:08.361477.361477 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.361659.361659 cuda_h.py:19] end concat_expert_out cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:09:08.361648.361648 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.361233.361233 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:09:08.361671.361671 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008323192596435547 seconds
DEBUG 01-15 16:09:08.361555.361555 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.361789.361789 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.361608.361608 cuda_h.py:19] end all_expert_outputs_slices cost 0.00011992454528808594 seconds
DEBUG 01-15 16:09:08.361642.361642 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.362128.362128 cuda_h.py:19] end concat_expert_out cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:09:08.362157.362157 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.362550.362550 cuda_h.py:19] end index_scatter cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:09:08.362544.362544 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00045013427734375 seconds
DEBUG 01-15 16:09:08.362586.362586 cuda_h.py:19] end gpu_experts_multi_device cost 0.050331830978393555 seconds
DEBUG 01-15 16:09:08.362350.362350 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.061403512954711914 seconds
DEBUG 01-15 16:09:08.362000.362000 cuda_h.py:19] end prefill_layer cost 0.06691789627075195 seconds
DEBUG 01-15 16:09:08.362757.362757 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:09:08.362076.362076 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.362394.362394 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:09:08.362951.362951 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:09:08.362416.362416 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:09:08.362206.362206 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 3.1948089599609375e-05 seconds
DEBUG 01-15 16:09:08.363989.363989 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.363103.363103 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 0.0001316070556640625 seconds
DEBUG 01-15 16:09:08.363020.363020 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.363750.363750 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.363567.363567 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.363672.363672 cuda_h.py:19] end allocate_cuda_memory cost 0.0002703666687011719 seconds
DEBUG 01-15 16:09:08.363722.363722 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.363319.363319 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.363886.363886 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.363828.363828 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.363492.363492 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ed0e8f81-2b36-4f0f-8e17-446d5de7b8e0
DEBUG 01-15 16:09:08.364780.364780 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.364471.364471 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.365001.365001 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ed0e8f81-2b36-4f0f-8e17-446d5de7b8e0
DEBUG 01-15 16:09:08.365811.365811 cuda_h.py:19] end load_into_gpu_async cost 0.001735687255859375 seconds
DEBUG 01-15 16:09:08.365614.365614 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.365816.365816 cuda_h.py:19] end restore_tensors2 cost 8.225440979003906e-05 seconds
DEBUG 01-15 16:09:08.365341.365341 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0025632381439208984 seconds
INFO 01-15 16:09:08.365038.365038 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ed0e8f81-2b36-4f0f-8e17-446d5de7b8e0
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.367311.367311 cuda_h.py:19] end self_attn cost 0.002864837646484375 seconds
DEBUG 01-15 16:09:08.367473.367473 cuda_h.py:19] end iln_self_attn_paln cost 0.0042684078216552734 seconds
DEBUG 01-15 16:09:08.367395.367395 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:09:08.367820.367820 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.368001.368001 cuda_h.py:19] end gate cost 0.0006272792816162109 seconds
DEBUG 01-15 16:09:08.368308.368308 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.368392.368392 lmp.py:1912] 
DEBUG 01-15 16:09:08.368392.368392 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.368294.368294 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.368944.368944 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.368255.368255 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.368422.368422 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.368111.368111 lmp.py:1917] 
DEBUG 01-15 16:09:08.368111.368111 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.368515.368515 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.368165.368165 lmp.py:1935]   Expert 50 |     42 | CPU
DEBUG 01-15 16:09:08.368808.368808 lmp.py:1935]   Expert  3 |     53 | CPU
DEBUG 01-15 16:09:08.368736.368736 lmp.py:1935]   Expert 46 |     55 | CPU
DEBUG 01-15 16:09:08.368379.368379 lmp.py:1935]   Expert  1 |     76 | CPU
DEBUG 01-15 16:09:08.368307.368307 lmp.py:1935]   Expert  4 |     87 | CPU
DEBUG 01-15 16:09:08.368234.368234 lmp.py:1935]   Expert 29 |     88 | CPU
DEBUG 01-15 16:09:08.368924.368924 lmp.py:1935]   Expert 15 |     96 | CPU
DEBUG 01-15 16:09:08.368136.368136 lmp.py:1935]   Expert 40 |     96 | CPU
DEBUG 01-15 16:09:08.369064.369064 lmp.py:1935]   Expert  8 |    109 | CPU
DEBUG 01-15 16:09:08.369515.369515 lmp.py:1935]   Expert 41 |    113 | CPU
DEBUG 01-15 16:09:08.369489.369489 lmp.py:1935]   Expert 28 |    114 | CPU
DEBUG 01-15 16:09:08.369986.369986 lmp.py:1935]   Expert 16 |    126 | CPU
DEBUG 01-15 16:09:08.369960.369960 lmp.py:1935]   Expert 27 |    128 | CPU
DEBUG 01-15 16:09:08.369457.369457 lmp.py:1935]   Expert 48 |    129 | CPU
DEBUG 01-15 16:09:08.369193.369193 lmp.py:1935]   Expert  6 |    130 | CPU
DEBUG 01-15 16:09:08.369690.369690 lmp.py:1935]   Expert 13 |    132 | CPU
DEBUG 01-15 16:09:08.369426.369426 lmp.py:1935]   Expert 54 |    132 | CPU
DEBUG 01-15 16:09:08.369923.369923 lmp.py:1935]   Expert  7 |    136 | CPU
DEBUG 01-15 16:09:08.369420.369420 lmp.py:1935]   Expert 51 |    138 | CPU
DEBUG 01-15 16:09:08.369063.369063 lmp.py:1935]   Expert 39 |    139 | CPU
DEBUG 01-15 16:09:08.369752.369752 lmp.py:1935]   Expert 18 |    140 | CPU
DEBUG 01-15 16:09:08.369680.369680 lmp.py:1935]   Expert 60 |    141 | CPU
DEBUG 01-15 16:09:08.369369.369369 lmp.py:1935]   Expert 14 |    146 | CPU
DEBUG 01-15 16:09:08.369059.369059 lmp.py:1935]   Expert 52 |    147 | CPU
DEBUG 01-15 16:09:08.369225.369225 lmp.py:1935]   Expert 20 |    148 | CPU
DEBUG 01-15 16:09:08.369298.369298 lmp.py:1935]   Expert 43 |    148 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369372.369372 lmp.py:1935]   Expert 56 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369730.369730 lmp.py:1935]   Expert 55 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369611.369611 lmp.py:1935]   Expert 36 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369731.369731 lmp.py:1935]   Expert 10 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369374.369374 lmp.py:1935]   Expert 11 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369779.369779 lmp.py:1935]   Expert 45 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369422.369422 lmp.py:1935]   Expert  5 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369826.369826 lmp.py:1935]   Expert 62 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369231.369231 lmp.py:1935]   Expert 57 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369635.369635 lmp.py:1935]   Expert 33 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369801.369801 lmp.py:1935]   Expert 44 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369444.369444 lmp.py:1935]   Expert 25 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369849.369849 lmp.py:1935]   Expert 58 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369207.369207 lmp.py:1935]   Expert 53 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369327.369327 lmp.py:1935]   Expert 32 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369685.369685 lmp.py:1935]   Expert  2 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369567.369567 lmp.py:1935]   Expert 35 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369925.369925 lmp.py:1935]   Expert 31 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369329.369329 lmp.py:1935]   Expert 63 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369734.369734 lmp.py:1935]   Expert 21 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369900.369900 lmp.py:1935]   Expert 49 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369305.369305 lmp.py:1935]   Expert 17 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369709.369709 lmp.py:1935]   Expert 42 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369114.369114 lmp.py:1935]   Expert 34 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369280.369280 lmp.py:1935]   Expert 37 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369446.369446 lmp.py:1935]   Expert 59 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369328.369328 lmp.py:1935]   Expert 22 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369447.369447 lmp.py:1935]   Expert  0 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369090.369090 lmp.py:1935]   Expert 19 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369687.369687 lmp.py:1935]   Expert 24 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369568.369568 lmp.py:1935]   Expert 61 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369688.369688 lmp.py:1935]   Expert 30 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369093.369093 lmp.py:1935]   Expert 47 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369736.369736 lmp.py:1935]   Expert 38 |    367 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369140.369140 lmp.py:1935]   Expert 26 |    369 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369783.369783 lmp.py:1935]   Expert 12 |    425 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369188.369188 lmp.py:1935]   Expert  9 |    683 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.369592.369592 lmp.py:1935]   Expert 23 |    700 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.369043.369043 lmp.py:1937] 
DEBUG 01-15 16:09:08.369043.369043 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.369448.369448 lmp.py:1938]   CPU:   2841 tokens
DEBUG 01-15 16:09:08.370329.370329 lmp.py:1942]   cuda:1:   4797 tokens (20 experts)
DEBUG 01-15 16:09:08.370449.370449 lmp.py:1942]   cuda:2:   4650 tokens (19 experts)
DEBUG 01-15 16:09:08.370092.370092 lmp.py:1943]   Total GPU:   9447 tokens
DEBUG 01-15 16:09:08.370735.370735 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.370735.370735 lmp.py:1944] 
DEBUG 01-15 16:09:08.370338.370338 cuda_h.py:19] end experts_map_get cost 0.0016674995422363281 seconds
DEBUG 01-15 16:09:08.370281.370281 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.370891.370891 lmp.py:1953] 
DEBUG 01-15 16:09:08.370891.370891 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.370721.370721 cuda_h.py:19] end cpu_experts_submit cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:09:08.370509.370509 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.370677.370677 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.371288.371288 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.22.gate_proj.weight': 0, 'model.layers.7.mlp.experts.22.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.22.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.23.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.23.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.23.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.24.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.24.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.24.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.26.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.26.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.26.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.32.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.32.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.32.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.33.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.33.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.33.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.35.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.35.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.35.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.36.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.36.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.36.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.37.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.37.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.37.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.38.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.38.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.38.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.45.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.45.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.45.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.55.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.55.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.55.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.57.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.7.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.7.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.7.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.31.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.31.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.31.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.34.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.34.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.34.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.47.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.47.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.47.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.59.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(9884925952, 5767168, 0, 0), (9890693120, 5767168, 5767168, 0), (9879158784, 5767168, 11534336, 0), (9902227456, 5767168, 17301504, 0), (9907994624, 5767168, 23068672, 0), (9896460288, 5767168, 28835840, 0), (9919528960, 5767168, 34603008, 0), (9925296128, 5767168, 40370176, 0), (9913761792, 5767168, 46137344, 0), (9954131968, 5767168, 51904512, 0), (9959899136, 5767168, 57671680, 0), (9948364800, 5767168, 63438848, 0), (10057940992, 5767168, 69206016, 0), (10063708160, 5767168, 74973184, 0), (10052173824, 5767168, 80740352, 0), (10075242496, 5767168, 86507520, 0), (10081009664, 5767168, 92274688, 0), (10069475328, 5767168, 98041856, 0), (10109845504, 5767168, 103809024, 0), (10115612672, 5767168, 109576192, 0), (10104078336, 5767168, 115343360, 0), (10127147008, 5767168, 121110528, 0), (10132914176, 5767168, 126877696, 0), (10121379840, 5767168, 132644864, 0), (10144448512, 5767168, 138412032, 0), (10150215680, 5767168, 144179200, 0), (10138681344, 5767168, 149946368, 0), (10161750016, 5767168, 155713536, 0), (10167517184, 5767168, 161480704, 0), (10155982848, 5767168, 167247872, 0), (10230956032, 5767168, 173015040, 0), (10236723200, 5767168, 178782208, 0), (10225188864, 5767168, 184549376, 0), (10282860544, 5767168, 190316544, 0), (10288627712, 5767168, 196083712, 0), (10277093376, 5767168, 201850880, 0), (10352066560, 5767168, 207618048, 0), (10357833728, 5767168, 213385216, 0), (10346299392, 5767168, 219152384, 0), (10455875584, 5767168, 224919552, 0), (10461642752, 5767168, 230686720, 0), (10450108416, 5767168, 236453888, 0), (10473177088, 5767168, 242221056, 0), (10478944256, 5767168, 247988224, 0), (10467409920, 5767168, 253755392, 0), (10490478592, 5767168, 259522560, 0), (10496245760, 5767168, 265289728, 0), (10484711424, 5767168, 271056896, 0), (10507780096, 5767168, 276824064, 0), (10513547264, 5767168, 282591232, 0), (10502012928, 5767168, 288358400, 0), (10559684608, 5767168, 294125568, 0), (10565451776, 5767168, 299892736, 0), (10553917440, 5767168, 305659904, 0), (10576986112, 5767168, 311427072, 0), (10582753280, 5767168, 317194240, 0), (10571218944, 5767168, 322961408, 0), (10594287616, 5767168, 328728576, 0), (10600054784, 5767168, 334495744, 0), (10588520448, 5767168, 340262912, 0)], 2: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (9538895872, 5767168, 17301504, 0), (9544663040, 5767168, 23068672, 0), (9533128704, 5767168, 28835840, 0), (9590800384, 5767168, 34603008, 0), (9596567552, 5767168, 40370176, 0), (9585033216, 5767168, 46137344, 0), (9660006400, 5767168, 51904512, 0), (9665773568, 5767168, 57671680, 0), (9654239232, 5767168, 63438848, 0), (9677307904, 5767168, 69206016, 0), (9683075072, 5767168, 74973184, 0), (9671540736, 5767168, 80740352, 0), (9694609408, 5767168, 86507520, 0), (9700376576, 5767168, 92274688, 0), (9688842240, 5767168, 98041856, 0), (9711910912, 5767168, 103809024, 0), (9717678080, 5767168, 109576192, 0), (9706143744, 5767168, 115343360, 0), (9798418432, 5767168, 121110528, 0), (9804185600, 5767168, 126877696, 0), (9792651264, 5767168, 132644864, 0), (9833021440, 5767168, 138412032, 0), (9838788608, 5767168, 144179200, 0), (9827254272, 5767168, 149946368, 0), (9867624448, 5767168, 155713536, 0), (9873391616, 5767168, 161480704, 0), (9861857280, 5767168, 167247872, 0), (9936830464, 5767168, 173015040, 0), (9942597632, 5767168, 178782208, 0), (9931063296, 5767168, 184549376, 0), (10023337984, 5767168, 190316544, 0), (10029105152, 5767168, 196083712, 0), (10017570816, 5767168, 201850880, 0), (10040639488, 5767168, 207618048, 0), (10046406656, 5767168, 213385216, 0), (10034872320, 5767168, 219152384, 0), (10092544000, 5767168, 224919552, 0), (10098311168, 5767168, 230686720, 0), (10086776832, 5767168, 236453888, 0), (10248257536, 5767168, 242221056, 0), (10254024704, 5767168, 247988224, 0), (10242490368, 5767168, 253755392, 0), (10265559040, 5767168, 259522560, 0), (10271326208, 5767168, 265289728, 0), (10259791872, 5767168, 271056896, 0), (10317463552, 5767168, 276824064, 0), (10323230720, 5767168, 282591232, 0), (10311696384, 5767168, 288358400, 0), (10421272576, 5767168, 294125568, 0), (10427039744, 5767168, 299892736, 0), (10415505408, 5767168, 305659904, 0), (10525081600, 5767168, 311427072, 0), (10530848768, 5767168, 317194240, 0), (10519314432, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8143f78d0>, 2: <capsule object NULL at 0x74a6807aa340>}
DEBUG 01-15 16:09:08.372669.372669 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.372114.372114 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.372102.372102 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4c90e743-5e53-47c1-b410-afb351c9cf10
DEBUG 01-15 16:09:08.372235.372235 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.373299.373299 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:08.373992.373992 client.py:127] Model loaded
DEBUG 01-15 16:09:08.373391.373391 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.373137.373137 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.373085.373085 cuda_h.py:19] end restore2model cost 0.00034117698669433594 seconds
DEBUG 01-15 16:09:08.373663.373663 cuda_h.py:19] end sllm_worker_task cost 0.01070261001586914 seconds
DEBUG 01-15 16:09:08.374637.374637 cuda_h.py:19] end move_flatidxs cost 0.0008344650268554688 seconds
DEBUG 01-15 16:09:08.374751.374751 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:08.374011.374011 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4c90e743-5e53-47c1-b410-afb351c9cf10
DEBUG 01-15 16:09:08.375413.375413 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004759073257446289 seconds
DEBUG 01-15 16:09:08.375450.375450 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.378307.378307 cuda_h.py:19] end restore2model cost 0.003229856491088867 seconds
DEBUG 01-15 16:09:08.378919.378919 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008251190185546875 seconds
DEBUG 01-15 16:09:08.378999.378999 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.378818.378818 cuda_h.py:19] end gpu_sexperts cost 0.0002918243408203125 seconds
DEBUG 01-15 16:09:08.378793.378793 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.378424.378424 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:09:08.378928.378928 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.379107.379107 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.379850.379850 cuda_h.py:19] end group_tensors cost 0.004704713821411133 seconds
DEBUG 01-15 16:09:08.380975.380975 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.381177.381177 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0024399757385253906 seconds
DEBUG 01-15 16:09:08.381690.381690 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.381569.381569 cuda_h.py:19] end gpu_group_list cost 0.0002963542938232422 seconds
DEBUG 01-15 16:09:08.383140.383140 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.384859.384859 cuda_h.py:19] end group pad cost 0.004597902297973633 seconds
DEBUG 01-15 16:09:08.384795.384795 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:08.384439.384439 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015249252319335938 seconds
DEBUG 01-15 16:09:08.384860.384860 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.385326.385326 cuda_h.py:19] end gpu_group_list cost 0.000759124755859375 seconds
DEBUG 01-15 16:09:08.391159.391159 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.392873.392873 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4c90e743-5e53-47c1-b410-afb351c9cf10
DEBUG 01-15 16:09:08.412898.412898 cuda_h.py:19] end group_einsum cost 0.02791142463684082 seconds
DEBUG 01-15 16:09:08.412445.412445 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:08.413565.413565 client.py:127] Model loaded
DEBUG 01-15 16:09:08.413033.413033 cuda_h.py:19] end wait_experts_multi_device cost 0.021154403686523438 seconds
DEBUG 01-15 16:09:08.413326.413326 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.415645.415645 cuda_h.py:19] end get_outputs_cpu1 cost 0.0027616024017333984 seconds
DEBUG 01-15 16:09:08.416890.416890 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.043294668197631836 seconds
DEBUG 01-15 16:09:08.417305.417305 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0042188167572021484 seconds
DEBUG 01-15 16:09:08.417907.417907 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.419603.419603 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.420561.420561 cuda_h.py:19] end index_scatter cost 0.0001583099365234375 seconds
DEBUG 01-15 16:09:08.420787.420787 cuda_h.py:19] end cpuoutputsdeal cost 0.0030357837677001953 seconds
DEBUG 01-15 16:09:08.421184.421184 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.421665.421665 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.421527.421527 cuda_h.py:19] end gpu_group_tensor cost 0.0003311634063720703 seconds
DEBUG 01-15 16:09:08.421444.421444 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.422299.422299 cuda_h.py:19] end gpu_group_tensor cost 0.00032782554626464844 seconds
DEBUG 01-15 16:09:08.422466.422466 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.423140.423140 cuda_h.py:19] end gpu_group_einsum cost 0.0010161399841308594 seconds
DEBUG 01-15 16:09:08.423539.423539 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.425374.425374 cuda_h.py:19] end gpu_group_einsum cost 0.002069234848022461 seconds
DEBUG 01-15 16:09:08.426163.426163 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.426525.426525 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.426786.426786 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021886825561523438 seconds
DEBUG 01-15 16:09:08.426164.426164 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.426393.426393 cuda_h.py:19] end concat_expert_out cost 6.270408630371094e-05 seconds
DEBUG 01-15 16:09:08.426819.426819 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.426246.426246 cuda_h.py:19] end index_scatter cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:09:08.427562.427562 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009696483612060547 seconds
DEBUG 01-15 16:09:08.427487.427487 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.427343.427343 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.427555.427555 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016021728515625 seconds
DEBUG 01-15 16:09:08.427357.427357 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.427857.427857 cuda_h.py:19] end concat_expert_out cost 5.698204040527344e-05 seconds
DEBUG 01-15 16:09:08.427316.427316 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.427623.427623 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:08.427240.427240 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005185604095458984 seconds
DEBUG 01-15 16:09:08.427846.427846 cuda_h.py:19] end gpu_experts_multi_device cost 0.04896998405456543 seconds
DEBUG 01-15 16:09:08.428094.428094 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.06038689613342285 seconds
DEBUG 01-15 16:09:08.428092.428092 cuda_h.py:19] end prefill_layer cost 0.06571149826049805 seconds
DEBUG 01-15 16:09:08.428770.428770 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:09:08.428141.428141 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.428513.428513 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:09:08.428124.428124 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:09:08.428833.428833 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:09:08.428021.428021 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 4.00543212890625e-05 seconds
DEBUG 01-15 16:09:08.428598.428598 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 7.414817810058594e-05 seconds
DEBUG 01-15 16:09:08.428905.428905 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.428012.428012 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.429207.429207 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.429965.429965 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.429888.429888 cuda_h.py:19] end allocate_cuda_memory cost 0.00022029876708984375 seconds
DEBUG 01-15 16:09:08.429600.429600 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.429290.429290 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.429234.429234 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.429487.429487 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.429428.429428 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a08d5bd3-3931-4d7e-b386-1b59f0edb455
DEBUG 01-15 16:09:08.429551.429551 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.430912.430912 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.431253.431253 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a08d5bd3-3931-4d7e-b386-1b59f0edb455
DEBUG 01-15 16:09:08.431897.431897 cuda_h.py:19] end load_into_gpu_async cost 0.0014023780822753906 seconds
DEBUG 01-15 16:09:08.431215.431215 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.431967.431967 cuda_h.py:19] end restore_tensors2 cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:09:08.431293.431293 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002089977264404297 seconds
INFO 01-15 16:09:08.431944.431944 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a08d5bd3-3931-4d7e-b386-1b59f0edb455
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.433394.433394 cuda_h.py:19] end self_attn cost 0.003278493881225586 seconds
DEBUG 01-15 16:09:08.433531.433531 cuda_h.py:19] end iln_self_attn_paln cost 0.004677772521972656 seconds
DEBUG 01-15 16:09:08.433168.433168 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:09:08.433116.433116 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.434218.434218 cuda_h.py:19] end gate cost 0.0006380081176757812 seconds
DEBUG 01-15 16:09:08.434048.434048 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.434012.434012 lmp.py:1912] 
DEBUG 01-15 16:09:08.434012.434012 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.434437.434437 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.434564.434564 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.434637.434637 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.435042.435042 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.435208.435208 lmp.py:1917] 
DEBUG 01-15 16:09:08.435208.435208 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.435851.435851 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.435977.435977 lmp.py:1935]   Expert 38 |     12 | CPU
DEBUG 01-15 16:09:08.435144.435144 lmp.py:1935]   Expert 39 |     60 | CPU
DEBUG 01-15 16:09:08.435071.435071 lmp.py:1935]   Expert  7 |     72 | CPU
DEBUG 01-15 16:09:08.435522.435522 lmp.py:1935]   Expert 30 |     73 | CPU
DEBUG 01-15 16:09:08.435258.435258 lmp.py:1935]   Expert 14 |     93 | CPU
DEBUG 01-15 16:09:08.435470.435470 lmp.py:1935]   Expert 24 |     93 | CPU
DEBUG 01-15 16:09:08.435444.435444 lmp.py:1935]   Expert 27 |     94 | CPU
DEBUG 01-15 16:09:08.435657.435657 lmp.py:1935]   Expert 36 |     96 | CPU
DEBUG 01-15 16:09:08.435823.435823 lmp.py:1935]   Expert 17 |     99 | CPU
DEBUG 01-15 16:09:08.435512.435512 lmp.py:1935]   Expert 40 |    100 | CPU
DEBUG 01-15 16:09:08.435202.435202 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:09:08.435652.435652 lmp.py:1935]   Expert 32 |    107 | CPU
DEBUG 01-15 16:09:08.435103.435103 lmp.py:1935]   Expert 18 |    108 | CPU
DEBUG 01-15 16:09:08.435793.435793 lmp.py:1935]   Expert 48 |    110 | CPU
DEBUG 01-15 16:09:08.435767.435767 lmp.py:1935]   Expert 12 |    115 | CPU
DEBUG 01-15 16:09:08.435741.435741 lmp.py:1935]   Expert  1 |    116 | CPU
DEBUG 01-15 16:09:08.435476.435476 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:09:08.435212.435212 lmp.py:1935]   Expert 59 |    130 | CPU
DEBUG 01-15 16:09:08.435186.435186 lmp.py:1935]   Expert 42 |    136 | CPU
DEBUG 01-15 16:09:08.435160.435160 lmp.py:1935]   Expert  0 |    140 | CPU
DEBUG 01-15 16:09:08.435372.435372 lmp.py:1935]   Expert 22 |    146 | CPU
DEBUG 01-15 16:09:08.435300.435300 lmp.py:1935]   Expert 53 |    146 | CPU
DEBUG 01-15 16:09:08.435228.435228 lmp.py:1935]   Expert 51 |    149 | CPU
DEBUG 01-15 16:09:08.435394.435394 lmp.py:1935]   Expert  8 |    161 | CPU
DEBUG 01-15 16:09:08.435083.435083 lmp.py:1935]   Expert 44 |    167 | CPU
DEBUG 01-15 16:09:08.435918.435918 lmp.py:1935]   Expert 60 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435038.435038 lmp.py:1935]   Expert 15 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435681.435681 lmp.py:1935]   Expert 29 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435609.435609 lmp.py:1935]   Expert 54 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435013.435013 lmp.py:1935]   Expert 34 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435941.435941 lmp.py:1935]   Expert 35 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435630.435630 lmp.py:1935]   Expert 33 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435558.435558 lmp.py:1935]   Expert 47 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435724.435724 lmp.py:1935]   Expert  9 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435175.435175 lmp.py:1935]   Expert 19 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435103.435103 lmp.py:1935]   Expert  3 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435031.435031 lmp.py:1935]   Expert 21 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435581.435581 lmp.py:1935]   Expert 46 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435131.435131 lmp.py:1935]   Expert 56 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435966.435966 lmp.py:1935]   Expert 45 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435325.435325 lmp.py:1935]   Expert 20 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435398.435398 lmp.py:1935]   Expert 49 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435279.435279 lmp.py:1935]   Expert 28 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435161.435161 lmp.py:1935]   Expert 57 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435565.435565 lmp.py:1935]   Expert  2 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435970.435970 lmp.py:1935]   Expert  4 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435375.435375 lmp.py:1935]   Expert 13 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435018.435018 lmp.py:1935]   Expert 43 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435661.435661 lmp.py:1935]   Expert 10 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435303.435303 lmp.py:1935]   Expert 50 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435946.435946 lmp.py:1935]   Expert 41 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435113.435113 lmp.py:1935]   Expert 26 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435471.435471 lmp.py:1935]   Expert 63 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.435591.435591 lmp.py:1935]   Expert 37 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.435472.435472 lmp.py:1935]   Expert 31 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.436830.436830 lmp.py:1935]   Expert 61 |    273 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.436950.436950 lmp.py:1935]   Expert 52 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.436547.436547 lmp.py:1935]   Expert 58 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.436951.436951 lmp.py:1935]   Expert 62 |    324 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.436594.436594 lmp.py:1935]   Expert 55 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.436237.436237 lmp.py:1935]   Expert 11 |    380 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.436880.436880 lmp.py:1935]   Expert 23 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.436523.436523 lmp.py:1935]   Expert 25 |    408 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.436166.436166 lmp.py:1935]   Expert  5 |    517 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.436617.436617 lmp.py:1937] 
DEBUG 01-15 16:09:08.436617.436617 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.436022.436022 lmp.py:1938]   CPU:   2756 tokens
DEBUG 01-15 16:09:08.436665.436665 lmp.py:1942]   cuda:1:   4692 tokens (19 experts)
DEBUG 01-15 16:09:08.436261.436261 lmp.py:1942]   cuda:2:   4840 tokens (20 experts)
DEBUG 01-15 16:09:08.436666.436666 lmp.py:1943]   Total GPU:   9532 tokens
DEBUG 01-15 16:09:08.436832.436832 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.436832.436832 lmp.py:1944] 
DEBUG 01-15 16:09:08.436674.436674 cuda_h.py:19] end experts_map_get cost 0.0016617774963378906 seconds
DEBUG 01-15 16:09:08.436901.436901 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.436227.436227 lmp.py:1953] 
DEBUG 01-15 16:09:08.436227.436227 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.436454.436454 cuda_h.py:19] end cpu_experts_submit cost 6.127357482910156e-05 seconds
DEBUG 01-15 16:09:08.436719.436719 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.436531.436531 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.437361.437361 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.437891.437891 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.8.mlp.experts.4.gate_proj.weight': 0, 'model.layers.8.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.13.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.13.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.13.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.15.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.15.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.15.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.45.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.45.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.45.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.8.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.8.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.8.mlp.experts.61.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (10663493632, 5767168, 17301504, 0), (10669260800, 5767168, 23068672, 0), (10657726464, 5767168, 28835840, 0), (10698096640, 5767168, 34603008, 0), (10703863808, 5767168, 40370176, 0), (10692329472, 5767168, 46137344, 0), (10767302656, 5767168, 51904512, 0), (10773069824, 5767168, 57671680, 0), (10761535488, 5767168, 63438848, 0), (10784604160, 5767168, 69206016, 0), (10790371328, 5767168, 74973184, 0), (10778836992, 5767168, 80740352, 0), (10801905664, 5767168, 86507520, 0), (10807672832, 5767168, 92274688, 0), (10796138496, 5767168, 98041856, 0), (10957619200, 5767168, 103809024, 0), (10963386368, 5767168, 109576192, 0), (10951852032, 5767168, 115343360, 0), (11096031232, 5767168, 121110528, 0), (11101798400, 5767168, 126877696, 0), (11090264064, 5767168, 132644864, 0), (11113332736, 5767168, 138412032, 0), (11119099904, 5767168, 144179200, 0), (11107565568, 5767168, 149946368, 0), (11147935744, 5767168, 155713536, 0), (11153702912, 5767168, 161480704, 0), (11142168576, 5767168, 167247872, 0), (11217141760, 5767168, 173015040, 0), (11222908928, 5767168, 178782208, 0), (11211374592, 5767168, 184549376, 0), (11320950784, 5767168, 190316544, 0), (11326717952, 5767168, 196083712, 0), (11315183616, 5767168, 201850880, 0), (11355553792, 5767168, 207618048, 0), (11361320960, 5767168, 213385216, 0), (11349786624, 5767168, 219152384, 0), (11407458304, 5767168, 224919552, 0), (11413225472, 5767168, 230686720, 0), (11401691136, 5767168, 236453888, 0), (11424759808, 5767168, 242221056, 0), (11430526976, 5767168, 247988224, 0), (11418992640, 5767168, 253755392, 0), (11511267328, 5767168, 259522560, 0), (11517034496, 5767168, 265289728, 0), (11505500160, 5767168, 271056896, 0), (11545870336, 5767168, 276824064, 0), (11551637504, 5767168, 282591232, 0), (11540103168, 5767168, 288358400, 0), (11684282368, 5767168, 294125568, 0), (11690049536, 5767168, 299892736, 0), (11678515200, 5767168, 305659904, 0), (11701583872, 5767168, 311427072, 0), (11707351040, 5767168, 317194240, 0), (11695816704, 5767168, 322961408, 0)], 2: [(10680795136, 5767168, 0, 0), (10686562304, 5767168, 5767168, 0), (10675027968, 5767168, 11534336, 0), (10836508672, 5767168, 17301504, 0), (10842275840, 5767168, 23068672, 0), (10830741504, 5767168, 28835840, 0), (10871111680, 5767168, 34603008, 0), (10876878848, 5767168, 40370176, 0), (10865344512, 5767168, 46137344, 0), (10940317696, 5767168, 51904512, 0), (10946084864, 5767168, 57671680, 0), (10934550528, 5767168, 63438848, 0), (10974920704, 5767168, 69206016, 0), (10980687872, 5767168, 74973184, 0), (10969153536, 5767168, 80740352, 0), (11009523712, 5767168, 86507520, 0), (11015290880, 5767168, 92274688, 0), (11003756544, 5767168, 98041856, 0), (11044126720, 5767168, 103809024, 0), (11049893888, 5767168, 109576192, 0), (11038359552, 5767168, 115343360, 0), (11061428224, 5767168, 121110528, 0), (11067195392, 5767168, 126877696, 0), (11055661056, 5767168, 132644864, 0), (11182538752, 5767168, 138412032, 0), (11188305920, 5767168, 144179200, 0), (11176771584, 5767168, 149946368, 0), (11199840256, 5767168, 155713536, 0), (11205607424, 5767168, 161480704, 0), (11194073088, 5767168, 167247872, 0), (11251744768, 5767168, 173015040, 0), (11257511936, 5767168, 178782208, 0), (11245977600, 5767168, 184549376, 0), (11390156800, 5767168, 190316544, 0), (11395923968, 5767168, 196083712, 0), (11384389632, 5767168, 201850880, 0), (11459362816, 5767168, 207618048, 0), (11465129984, 5767168, 213385216, 0), (11453595648, 5767168, 219152384, 0), (11476664320, 5767168, 224919552, 0), (11482431488, 5767168, 230686720, 0), (11470897152, 5767168, 236453888, 0), (11563171840, 5767168, 242221056, 0), (11568939008, 5767168, 247988224, 0), (11557404672, 5767168, 253755392, 0), (11580473344, 5767168, 259522560, 0), (11586240512, 5767168, 265289728, 0), (11574706176, 5767168, 271056896, 0), (11597774848, 5767168, 276824064, 0), (11603542016, 5767168, 282591232, 0), (11592007680, 5767168, 288358400, 0), (11615076352, 5767168, 294125568, 0), (11620843520, 5767168, 299892736, 0), (11609309184, 5767168, 305659904, 0), (11649679360, 5767168, 311427072, 0), (11655446528, 5767168, 317194240, 0), (11643912192, 5767168, 322961408, 0), (11666980864, 5767168, 328728576, 0), (11672748032, 5767168, 334495744, 0), (11661213696, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc75bf30>, 2: <capsule object NULL at 0x74a6807aa070>}
DEBUG 01-15 16:09:08.437644.437644 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.437282.437282 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.437269.437269 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 10d898eb-bf11-4a74-bacc-625ba9a9e138
DEBUG 01-15 16:09:08.437782.437782 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.438302.438302 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.438983.438983 client.py:127] Model loaded
DEBUG 01-15 16:09:08.438143.438143 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.438699.438699 cuda_h.py:19] end restore2model cost 0.0003440380096435547 seconds
DEBUG 01-15 16:09:08.438561.438561 cuda_h.py:19] end sllm_worker_task cost 0.009610891342163086 seconds
DEBUG 01-15 16:09:08.438806.438806 cuda_h.py:19] end move_flatidxs cost 0.0008347034454345703 seconds
DEBUG 01-15 16:09:08.438489.438489 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:08.439742.439742 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 10d898eb-bf11-4a74-bacc-625ba9a9e138
DEBUG 01-15 16:09:08.439865.439865 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003194093704223633 seconds
DEBUG 01-15 16:09:08.440364.440364 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.443690.443690 cuda_h.py:19] end restore2model cost 0.003154754638671875 seconds
DEBUG 01-15 16:09:08.443155.443155 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006604194641113281 seconds
DEBUG 01-15 16:09:08.443971.443971 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.443823.443823 cuda_h.py:19] end gpu_sexperts cost 0.0002799034118652344 seconds
DEBUG 01-15 16:09:08.443222.443222 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.443375.443375 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:09:08.443310.443310 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.443966.443966 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.444165.444165 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000957489013671875 seconds
DEBUG 01-15 16:09:08.444869.444869 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.445665.445665 cuda_h.py:19] end gpu_group_list cost 0.0002067089080810547 seconds
DEBUG 01-15 16:09:08.445832.445832 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.447420.447420 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010652542114257812 seconds
DEBUG 01-15 16:09:08.447972.447972 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.447524.447524 cuda_h.py:19] end gpu_group_list cost 0.0002219676971435547 seconds
DEBUG 01-15 16:09:08.448535.448535 cuda_h.py:10] start wait_experts_multi_device
DEBUG 01-15 16:09:08.447609.447609 cuda_h.py:19] end group_tensors cost 0.008688211441040039 seconds
INFO 01-15 16:09:08.448232.448232 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 10d898eb-bf11-4a74-bacc-625ba9a9e138
DEBUG 01-15 16:09:08.448639.448639 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.451912.451912 cuda_h.py:19] end group pad cost 0.0027484893798828125 seconds
DEBUG 01-15 16:09:08.451417.451417 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:08.479845.479845 client.py:127] Model loaded
DEBUG 01-15 16:09:08.479509.479509 cuda_h.py:19] end wait_experts_multi_device cost 0.03155350685119629 seconds
DEBUG 01-15 16:09:08.479531.479531 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.482171.482171 cuda_h.py:19] end group_einsum cost 0.03094625473022461 seconds
DEBUG 01-15 16:09:08.482507.482507 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.485368.485368 cuda_h.py:19] end get_outputs_cpu1 cost 0.002721548080444336 seconds
DEBUG 01-15 16:09:08.485906.485906 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.048220157623291016 seconds
DEBUG 01-15 16:09:08.486748.486748 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006970405578613281 seconds
DEBUG 01-15 16:09:08.487914.487914 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.489943.489943 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.489086.489086 cuda_h.py:19] end index_scatter cost 0.00015616416931152344 seconds
DEBUG 01-15 16:09:08.490542.490542 cuda_h.py:19] end cpuoutputsdeal cost 0.0030317306518554688 seconds
DEBUG 01-15 16:09:08.490150.490150 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.490888.490888 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.490948.490948 cuda_h.py:19] end gpu_group_tensor cost 0.00031185150146484375 seconds
DEBUG 01-15 16:09:08.491070.491070 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.491156.491156 cuda_h.py:19] end gpu_group_tensor cost 0.0004892349243164062 seconds
DEBUG 01-15 16:09:08.491203.491203 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.492013.492013 cuda_h.py:19] end gpu_group_einsum cost 0.0006079673767089844 seconds
DEBUG 01-15 16:09:08.492111.492111 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.493738.493738 cuda_h.py:19] end gpu_group_einsum cost 0.0004673004150390625 seconds
DEBUG 01-15 16:09:08.493477.493477 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.493110.493110 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.493026.493026 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020837783813476562 seconds
DEBUG 01-15 16:09:08.493696.493696 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.493547.493547 cuda_h.py:19] end concat_expert_out cost 6.461143493652344e-05 seconds
DEBUG 01-15 16:09:08.493251.493251 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.493287.493287 cuda_h.py:19] end index_scatter cost 6.628036499023438e-05 seconds
DEBUG 01-15 16:09:08.494725.494725 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008325576782226562 seconds
DEBUG 01-15 16:09:08.494132.494132 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.494114.494114 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.494795.494795 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012350082397460938 seconds
DEBUG 01-15 16:09:08.494590.494590 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.494315.494315 cuda_h.py:19] end concat_expert_out cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:09:08.494436.494436 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.494306.494306 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:08.494824.494824 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004405975341796875 seconds
DEBUG 01-15 16:09:08.494455.494455 cuda_h.py:19] end gpu_experts_multi_device cost 0.05099821090698242 seconds
DEBUG 01-15 16:09:08.494219.494219 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.061034202575683594 seconds
DEBUG 01-15 16:09:08.495637.495637 cuda_h.py:19] end prefill_layer cost 0.06659078598022461 seconds
DEBUG 01-15 16:09:08.495732.495732 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:09:08.495812.495812 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.495416.495416 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:09:08.495735.495735 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:09:08.495676.495676 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:09:08.495618.495618 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 3.5762786865234375e-05 seconds
DEBUG 01-15 16:09:08.495196.495196 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 7.700920104980469e-05 seconds
DEBUG 01-15 16:09:08.495753.495753 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.495397.495397 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.495836.495836 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.495043.495043 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.496929.496929 cuda_h.py:19] end allocate_cuda_memory cost 0.00033855438232421875 seconds
DEBUG 01-15 16:09:08.496216.496216 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.496694.496694 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.496817.496817 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.496222.496222 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.496636.496636 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4b3e4355-2e4b-4a11-a40c-5a0305c207c4
DEBUG 01-15 16:09:08.496209.496209 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.496182.496182 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.497465.497465 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4b3e4355-2e4b-4a11-a40c-5a0305c207c4
DEBUG 01-15 16:09:08.497726.497726 cuda_h.py:19] end load_into_gpu_async cost 0.0017406940460205078 seconds
DEBUG 01-15 16:09:08.498688.498688 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.498777.498777 cuda_h.py:19] end restore_tensors2 cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:09:08.498448.498448 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024232864379882812 seconds
INFO 01-15 16:09:08.498874.498874 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4b3e4355-2e4b-4a11-a40c-5a0305c207c4
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.500212.500212 cuda_h.py:19] end self_attn cost 0.0036149024963378906 seconds
DEBUG 01-15 16:09:08.500427.500427 cuda_h.py:19] end iln_self_attn_paln cost 0.0052754878997802734 seconds
DEBUG 01-15 16:09:08.500634.500634 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:09:08.500344.500344 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.501616.501616 cuda_h.py:19] end gate cost 0.0007643699645996094 seconds
DEBUG 01-15 16:09:08.501399.501399 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.502072.502072 lmp.py:1912] 
DEBUG 01-15 16:09:08.502072.502072 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.502066.502066 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.502716.502716 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.502743.502743 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.502625.502625 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.502791.502791 lmp.py:1917] 
DEBUG 01-15 16:09:08.502791.502791 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.502672.502672 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.502560.502560 lmp.py:1935]   Expert 24 |     38 | CPU
DEBUG 01-15 16:09:08.502965.502965 lmp.py:1935]   Expert  2 |     46 | CPU
DEBUG 01-15 16:09:08.502654.502654 lmp.py:1935]   Expert 26 |     63 | CPU
DEBUG 01-15 16:09:08.502628.502628 lmp.py:1935]   Expert 32 |     66 | CPU
DEBUG 01-15 16:09:08.502364.502364 lmp.py:1935]   Expert 19 |     69 | CPU
DEBUG 01-15 16:09:08.502338.502338 lmp.py:1935]   Expert 50 |     70 | CPU
DEBUG 01-15 16:09:08.502504.502504 lmp.py:1935]   Expert 15 |     80 | CPU
DEBUG 01-15 16:09:08.502432.502432 lmp.py:1935]   Expert  7 |     82 | CPU
DEBUG 01-15 16:09:08.502598.502598 lmp.py:1935]   Expert 28 |     82 | CPU
DEBUG 01-15 16:09:08.502287.502287 lmp.py:1935]   Expert  4 |     83 | CPU
DEBUG 01-15 16:09:08.502215.502215 lmp.py:1935]   Expert 60 |     83 | CPU
DEBUG 01-15 16:09:08.502904.502904 lmp.py:1935]   Expert 59 |     90 | CPU
DEBUG 01-15 16:09:08.502878.502878 lmp.py:1935]   Expert 23 |     96 | CPU
DEBUG 01-15 16:09:08.502614.502614 lmp.py:1935]   Expert 49 |     97 | CPU
DEBUG 01-15 16:09:08.502826.502826 lmp.py:1935]   Expert  5 |    105 | CPU
DEBUG 01-15 16:09:08.502039.502039 lmp.py:1935]   Expert 12 |    105 | CPU
DEBUG 01-15 16:09:08.502251.502251 lmp.py:1935]   Expert 10 |    111 | CPU
DEBUG 01-15 16:09:08.502464.502464 lmp.py:1935]   Expert 27 |    111 | CPU
DEBUG 01-15 16:09:08.502961.502961 lmp.py:1935]   Expert 41 |    120 | CPU
DEBUG 01-15 16:09:08.502697.502697 lmp.py:1935]   Expert  3 |    126 | CPU
DEBUG 01-15 16:09:08.502624.502624 lmp.py:1935]   Expert 25 |    127 | CPU
DEBUG 01-15 16:09:08.502950.502950 lmp.py:1935]   Expert 40 |    130 | CPU
DEBUG 01-15 16:09:08.502169.502169 lmp.py:1935]   Expert 20 |    131 | CPU
DEBUG 01-15 16:09:08.502865.502865 lmp.py:1935]   Expert 16 |    132 | CPU
DEBUG 01-15 16:09:08.502607.502607 lmp.py:1935]   Expert 13 |    133 | CPU
DEBUG 01-15 16:09:08.502780.502780 lmp.py:1935]   Expert 37 |    144 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.502477.502477 lmp.py:1935]   Expert 17 |    145 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.502457.502457 lmp.py:1935]   Expert 35 |    147 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.502438.502438 lmp.py:1935]   Expert 47 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.502419.502419 lmp.py:1935]   Expert 22 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.502400.502400 lmp.py:1935]   Expert 53 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.502142.502142 lmp.py:1935]   Expert 39 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.502315.502315 lmp.py:1935]   Expert 38 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.502250.502250 lmp.py:1935]   Expert 44 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.502469.502469 lmp.py:1935]   Expert 36 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.502927.502927 lmp.py:1935]   Expert 52 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.502623.502623 lmp.py:1935]   Expert 58 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503365.503365 lmp.py:1935]   Expert 18 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503631.503631 lmp.py:1935]   Expert 62 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503135.503135 lmp.py:1935]   Expert 11 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503116.503116 lmp.py:1935]   Expert 48 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503620.503620 lmp.py:1935]   Expert 30 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503885.503885 lmp.py:1935]   Expert 14 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503628.503628 lmp.py:1935]   Expert  1 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503847.503847 lmp.py:1935]   Expert 42 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503828.503828 lmp.py:1935]   Expert 45 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503570.503570 lmp.py:1935]   Expert 31 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503312.503312 lmp.py:1935]   Expert  6 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503816.503816 lmp.py:1935]   Expert 51 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503320.503320 lmp.py:1935]   Expert 29 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503586.503586 lmp.py:1935]   Expert 34 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503852.503852 lmp.py:1935]   Expert 33 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503879.503879 lmp.py:1935]   Expert 57 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503144.503144 lmp.py:1935]   Expert 61 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503172.503172 lmp.py:1935]   Expert 43 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503676.503676 lmp.py:1935]   Expert  0 |    321 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503941.503941 lmp.py:1935]   Expert 46 |    348 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503922.503922 lmp.py:1935]   Expert  8 |    378 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503903.503903 lmp.py:1935]   Expert  9 |    390 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503122.503122 lmp.py:1935]   Expert 56 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503103.503103 lmp.py:1935]   Expert 54 |    398 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503322.503322 lmp.py:1935]   Expert 63 |    406 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503065.503065 lmp.py:1935]   Expert 55 |    424 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.503569.503569 lmp.py:1935]   Expert 21 |    489 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.503404.503404 lmp.py:1937] 
DEBUG 01-15 16:09:08.503404.503404 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.503623.503623 lmp.py:1938]   CPU:   2376 tokens
DEBUG 01-15 16:09:08.503888.503888 lmp.py:1942]   cuda:1:   5028 tokens (20 experts)
DEBUG 01-15 16:09:08.503916.503916 lmp.py:1942]   cuda:2:   4884 tokens (19 experts)
DEBUG 01-15 16:09:08.503466.503466 lmp.py:1943]   Total GPU:   9912 tokens
DEBUG 01-15 16:09:08.503970.503970 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.503970.503970 lmp.py:1944] 
DEBUG 01-15 16:09:08.503673.503673 cuda_h.py:19] end experts_map_get cost 0.00182342529296875 seconds
DEBUG 01-15 16:09:08.503198.503198 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.503054.503054 lmp.py:1953] 
DEBUG 01-15 16:09:08.503054.503054 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.503506.503506 cuda_h.py:19] end cpu_experts_submit cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:08.503818.503818 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.503416.503416 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.504061.504061 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.9.gate_proj.weight': 0, 'model.layers.9.mlp.experts.9.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.9.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.14.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.14.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.14.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.18.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.18.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.18.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.21.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.21.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.21.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.29.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.29.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.29.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.30.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.30.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.30.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.31.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.31.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.31.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.34.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.34.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.34.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.9.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.9.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.9.mlp.experts.62.up_proj.weight': 340262912}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.33.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.33.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.33.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.36.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.36.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.36.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.42.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.42.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.42.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.44.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.44.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.44.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.48.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.48.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.48.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.51.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.51.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.51.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.53.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.53.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.53.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(11874598912, 5767168, 0, 0), (11880366080, 5767168, 5767168, 0), (11868831744, 5767168, 11534336, 0), (11961106432, 5767168, 17301504, 0), (11966873600, 5767168, 23068672, 0), (11955339264, 5767168, 28835840, 0), (12030312448, 5767168, 34603008, 0), (12036079616, 5767168, 40370176, 0), (12024545280, 5767168, 46137344, 0), (12082216960, 5767168, 51904512, 0), (12087984128, 5767168, 57671680, 0), (12076449792, 5767168, 63438848, 0), (12220628992, 5767168, 69206016, 0), (12226396160, 5767168, 74973184, 0), (12214861824, 5767168, 80740352, 0), (12237930496, 5767168, 86507520, 0), (12243697664, 5767168, 92274688, 0), (12232163328, 5767168, 98041856, 0), (12255232000, 5767168, 103809024, 0), (12260999168, 5767168, 109576192, 0), (12249464832, 5767168, 115343360, 0), (12307136512, 5767168, 121110528, 0), (12312903680, 5767168, 126877696, 0), (12301369344, 5767168, 132644864, 0), (12324438016, 5767168, 138412032, 0), (12330205184, 5767168, 144179200, 0), (12318670848, 5767168, 149946368, 0), (12359041024, 5767168, 155713536, 0), (12364808192, 5767168, 161480704, 0), (12353273856, 5767168, 167247872, 0), (12376342528, 5767168, 173015040, 0), (12382109696, 5767168, 178782208, 0), (12370575360, 5767168, 184549376, 0), (12393644032, 5767168, 190316544, 0), (12399411200, 5767168, 196083712, 0), (12387876864, 5767168, 201850880, 0), (12462850048, 5767168, 207618048, 0), (12468617216, 5767168, 213385216, 0), (12457082880, 5767168, 219152384, 0), (12497453056, 5767168, 224919552, 0), (12503220224, 5767168, 230686720, 0), (12491685888, 5767168, 236453888, 0), (12514754560, 5767168, 242221056, 0), (12520521728, 5767168, 247988224, 0), (12508987392, 5767168, 253755392, 0), (12532056064, 5767168, 259522560, 0), (12537823232, 5767168, 265289728, 0), (12526288896, 5767168, 271056896, 0), (12618563584, 5767168, 276824064, 0), (12624330752, 5767168, 282591232, 0), (12612796416, 5767168, 288358400, 0), (12653166592, 5767168, 294125568, 0), (12658933760, 5767168, 299892736, 0), (12647399424, 5767168, 305659904, 0), (12705071104, 5767168, 311427072, 0), (12710838272, 5767168, 317194240, 0), (12699303936, 5767168, 322961408, 0), (12791578624, 5767168, 328728576, 0), (12797345792, 5767168, 334495744, 0), (12785811456, 5767168, 340262912, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (11736186880, 5767168, 17301504, 0), (11741954048, 5767168, 23068672, 0), (11730419712, 5767168, 28835840, 0), (11822694400, 5767168, 34603008, 0), (11828461568, 5767168, 40370176, 0), (11816927232, 5767168, 46137344, 0), (11857297408, 5767168, 51904512, 0), (11863064576, 5767168, 57671680, 0), (11851530240, 5767168, 63438848, 0), (11909201920, 5767168, 69206016, 0), (11914969088, 5767168, 74973184, 0), (11903434752, 5767168, 80740352, 0), (12013010944, 5767168, 86507520, 0), (12018778112, 5767168, 92274688, 0), (12007243776, 5767168, 98041856, 0), (12099518464, 5767168, 103809024, 0), (12105285632, 5767168, 109576192, 0), (12093751296, 5767168, 115343360, 0), (12289835008, 5767168, 121110528, 0), (12295602176, 5767168, 126877696, 0), (12284067840, 5767168, 132644864, 0), (12341739520, 5767168, 138412032, 0), (12347506688, 5767168, 144179200, 0), (12335972352, 5767168, 149946368, 0), (12445548544, 5767168, 155713536, 0), (12451315712, 5767168, 161480704, 0), (12439781376, 5767168, 167247872, 0), (12480151552, 5767168, 173015040, 0), (12485918720, 5767168, 178782208, 0), (12474384384, 5767168, 184549376, 0), (12549357568, 5767168, 190316544, 0), (12555124736, 5767168, 196083712, 0), (12543590400, 5767168, 201850880, 0), (12601262080, 5767168, 207618048, 0), (12607029248, 5767168, 213385216, 0), (12595494912, 5767168, 219152384, 0), (12635865088, 5767168, 224919552, 0), (12641632256, 5767168, 230686720, 0), (12630097920, 5767168, 236453888, 0), (12670468096, 5767168, 242221056, 0), (12676235264, 5767168, 247988224, 0), (12664700928, 5767168, 253755392, 0), (12687769600, 5767168, 259522560, 0), (12693536768, 5767168, 265289728, 0), (12682002432, 5767168, 271056896, 0), (12722372608, 5767168, 276824064, 0), (12728139776, 5767168, 282591232, 0), (12716605440, 5767168, 288358400, 0), (12774277120, 5767168, 294125568, 0), (12780044288, 5767168, 299892736, 0), (12768509952, 5767168, 305659904, 0), (12808880128, 5767168, 311427072, 0), (12814647296, 5767168, 317194240, 0), (12803112960, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8143f7780>, 2: <capsule object NULL at 0x74a6807aa1c0>}
DEBUG 01-15 16:09:08.504292.504292 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.505659.505659 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.505176.505176 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a7617bab-a063-48ca-96d1-c2069a1bac7c
DEBUG 01-15 16:09:08.505130.505130 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.505513.505513 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:08.505692.505692 client.py:127] Model loaded
DEBUG 01-15 16:09:08.505138.505138 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.505622.505622 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.505707.505707 cuda_h.py:19] end restore2model cost 0.0003540515899658203 seconds
DEBUG 01-15 16:09:08.505331.505331 cuda_h.py:19] end sllm_worker_task cost 0.010235071182250977 seconds
DEBUG 01-15 16:09:08.506546.506546 cuda_h.py:19] end move_flatidxs cost 0.0008318424224853516 seconds
DEBUG 01-15 16:09:08.506991.506991 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:08.506258.506258 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a7617bab-a063-48ca-96d1-c2069a1bac7c
DEBUG 01-15 16:09:08.511890.511890 cuda_h.py:19] end group_tensors cost 0.0044515132904052734 seconds
DEBUG 01-15 16:09:08.511958.511958 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.515189.515189 cuda_h.py:19] end group pad cost 0.003699779510498047 seconds
DEBUG 01-15 16:09:08.515715.515715 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:08.531085.531085 cuda_h.py:19] end group_einsum cost 0.01570606231689453 seconds
DEBUG 01-15 16:09:08.531216.531216 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.533296.533296 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.029794692993164062 seconds
DEBUG 01-15 16:09:08.533488.533488 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.534783.534783 cuda_h.py:19] end get_outputs_cpu1 cost 0.0030319690704345703 seconds
DEBUG 01-15 16:09:08.535272.535272 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.029744625091552734 seconds
DEBUG 01-15 16:09:08.537330.537330 cuda_h.py:19] end restore2model cost 0.0033495426177978516 seconds
DEBUG 01-15 16:09:08.537518.537518 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.033535003662109375 seconds
DEBUG 01-15 16:09:08.537605.537605 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.537454.537454 cuda_h.py:19] end gpu_sexperts cost 0.00041794776916503906 seconds
DEBUG 01-15 16:09:08.537191.537191 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.538120.538120 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2172927856445312e-05 seconds
DEBUG 01-15 16:09:08.538008.538008 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.538387.538387 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.539914.539914 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010938644409179688 seconds
DEBUG 01-15 16:09:08.539970.539970 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.539700.539700 cuda_h.py:19] end gpu_group_list cost 0.0002219676971435547 seconds
DEBUG 01-15 16:09:08.540235.540235 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.541484.541484 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001058816909790039 seconds
DEBUG 01-15 16:09:08.541554.541554 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.541171.541171 cuda_h.py:19] end gpu_group_list cost 0.00021076202392578125 seconds
DEBUG 01-15 16:09:08.542466.542466 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.542117.542117 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a7617bab-a063-48ca-96d1-c2069a1bac7c
INFO 01-15 16:09:08.543971.543971 client.py:127] Model loaded
DEBUG 01-15 16:09:08.543562.543562 cuda_h.py:19] end wait_experts_multi_device cost 0.0010342597961425781 seconds
DEBUG 01-15 16:09:08.543358.543358 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.544198.544198 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0003437995910644531 seconds
DEBUG 01-15 16:09:08.544160.544160 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.544311.544311 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.545726.545726 cuda_h.py:19] end index_scatter cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:09:08.545985.545985 cuda_h.py:19] end cpuoutputsdeal cost 0.0010652542114257812 seconds
DEBUG 01-15 16:09:08.545967.545967 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.545815.545815 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.545290.545290 cuda_h.py:19] end gpu_group_tensor cost 0.00011229515075683594 seconds
DEBUG 01-15 16:09:08.545238.545238 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.545554.545554 cuda_h.py:19] end gpu_group_tensor cost 9.846687316894531e-05 seconds
DEBUG 01-15 16:09:08.545272.545272 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.546040.546040 cuda_h.py:19] end gpu_group_einsum cost 0.0003509521484375 seconds
DEBUG 01-15 16:09:08.546791.546791 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.546205.546205 cuda_h.py:19] end gpu_group_einsum cost 0.0004334449768066406 seconds
DEBUG 01-15 16:09:08.546414.546414 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.546027.546027 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.547577.547577 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018668174743652344 seconds
DEBUG 01-15 16:09:08.547863.547863 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.547979.547979 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:09:08.547359.547359 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.547798.547798 cuda_h.py:19] end index_scatter cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:09:08.547480.547480 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000732421875 seconds
DEBUG 01-15 16:09:08.547741.547741 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.547915.547915 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.547119.547119 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012421607971191406 seconds
DEBUG 01-15 16:09:08.547630.547630 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.547586.547586 cuda_h.py:19] end concat_expert_out cost 4.7206878662109375e-05 seconds
DEBUG 01-15 16:09:08.548230.548230 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.548008.548008 cuda_h.py:19] end index_scatter cost 4.696846008300781e-05 seconds
DEBUG 01-15 16:09:08.548671.548671 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00043964385986328125 seconds
DEBUG 01-15 16:09:08.548521.548521 cuda_h.py:19] end gpu_experts_multi_device cost 0.010152101516723633 seconds
DEBUG 01-15 16:09:08.548808.548808 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.04729056358337402 seconds
DEBUG 01-15 16:09:08.548216.548216 cuda_h.py:19] end prefill_layer cost 0.05314779281616211 seconds
DEBUG 01-15 16:09:08.548959.548959 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:09:08.548039.548039 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.548120.548120 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:09:08.548200.548200 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:09:08.548525.548525 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:09:08.548700.548700 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 3.075599670410156e-05 seconds
DEBUG 01-15 16:09:08.548323.548323 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:09:08.548656.548656 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.548531.548531 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.548388.548388 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.549760.549760 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.549909.549909 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.549091.549091 cuda_h.py:19] end allocate_cuda_memory cost 0.0002315044403076172 seconds
DEBUG 01-15 16:09:08.549808.549808 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.549286.549286 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.549984.549984 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.549415.549415 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 179d072f-a122-47f0-bdd8-3f57254ddcaf
DEBUG 01-15 16:09:08.549709.549709 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.550350.550350 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.550974.550974 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 179d072f-a122-47f0-bdd8-3f57254ddcaf
DEBUG 01-15 16:09:08.550154.550154 cuda_h.py:19] end load_into_gpu_async cost 0.0012018680572509766 seconds
DEBUG 01-15 16:09:08.550692.550692 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.550106.550106 cuda_h.py:19] end restore_tensors2 cost 6.580352783203125e-05 seconds
DEBUG 01-15 16:09:08.550154.550154 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0017859935760498047 seconds
INFO 01-15 16:09:08.551368.551368 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 179d072f-a122-47f0-bdd8-3f57254ddcaf
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.553399.553399 cuda_h.py:19] end self_attn cost 0.003000020980834961 seconds
DEBUG 01-15 16:09:08.553362.553362 cuda_h.py:19] end iln_self_attn_paln cost 0.004456520080566406 seconds
DEBUG 01-15 16:09:08.553331.553331 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:09:08.553425.553425 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.554029.554029 cuda_h.py:19] end gate cost 0.0006158351898193359 seconds
DEBUG 01-15 16:09:08.554859.554859 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.554120.554120 lmp.py:1912] 
DEBUG 01-15 16:09:08.554120.554120 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.554022.554022 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.554195.554195 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.554507.554507 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.554435.554435 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.554839.554839 lmp.py:1917] 
DEBUG 01-15 16:09:08.554839.554839 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.554781.554781 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.554583.554583 lmp.py:1935]   Expert 43 |     16 | CPU
DEBUG 01-15 16:09:08.554041.554041 lmp.py:1935]   Expert 27 |     31 | CPU
DEBUG 01-15 16:09:08.554783.554783 lmp.py:1935]   Expert 26 |     52 | CPU
DEBUG 01-15 16:09:08.554049.554049 lmp.py:1935]   Expert 34 |     53 | CPU
DEBUG 01-15 16:09:08.554076.554076 lmp.py:1935]   Expert 56 |     55 | CPU
DEBUG 01-15 16:09:08.554341.554341 lmp.py:1935]   Expert  3 |     58 | CPU
DEBUG 01-15 16:09:08.554845.554845 lmp.py:1935]   Expert  4 |     65 | CPU
DEBUG 01-15 16:09:08.554826.554826 lmp.py:1935]   Expert 61 |     80 | CPU
DEBUG 01-15 16:09:08.554330.554330 lmp.py:1935]   Expert 14 |     94 | CPU
DEBUG 01-15 16:09:08.554834.554834 lmp.py:1935]   Expert 38 |     98 | CPU
DEBUG 01-15 16:09:08.554577.554577 lmp.py:1935]   Expert  2 |    110 | CPU
DEBUG 01-15 16:09:08.554842.554842 lmp.py:1935]   Expert 22 |    121 | CPU
DEBUG 01-15 16:09:08.554393.554393 lmp.py:1935]   Expert 17 |    123 | CPU
DEBUG 01-15 16:09:08.554943.554943 lmp.py:1935]   Expert 47 |    128 | CPU
DEBUG 01-15 16:09:08.554493.554493 lmp.py:1935]   Expert 37 |    129 | CPU
DEBUG 01-15 16:09:08.554043.554043 lmp.py:1935]   Expert 55 |    133 | CPU
DEBUG 01-15 16:09:08.554071.554071 lmp.py:1935]   Expert 54 |    135 | CPU
DEBUG 01-15 16:09:08.554383.554383 lmp.py:1935]   Expert 28 |    137 | CPU
DEBUG 01-15 16:09:08.554171.554171 lmp.py:1935]   Expert  7 |    144 | CPU
DEBUG 01-15 16:09:08.554675.554675 lmp.py:1935]   Expert 15 |    146 | CPU
DEBUG 01-15 16:09:08.555179.555179 lmp.py:1935]   Expert 48 |    146 | CPU
DEBUG 01-15 16:09:08.555683.555683 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:09:08.555187.555187 lmp.py:1935]   Expert 45 |    147 | CPU
DEBUG 01-15 16:09:08.555214.555214 lmp.py:1935]   Expert 51 |    147 | CPU
DEBUG 01-15 16:09:08.555242.555242 lmp.py:1935]   Expert 60 |    151 | CPU
DEBUG 01-15 16:09:08.555461.555461 lmp.py:1935]   Expert 63 |    153 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555203.555203 lmp.py:1935]   Expert 12 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555707.555707 lmp.py:1935]   Expert 19 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555973.555973 lmp.py:1935]   Expert  6 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555477.555477 lmp.py:1935]   Expert 57 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555219.555219 lmp.py:1935]   Expert 52 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555485.555485 lmp.py:1935]   Expert 18 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555704.555704 lmp.py:1935]   Expert 50 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555923.555923 lmp.py:1935]   Expert 44 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555143.555143 lmp.py:1935]   Expert 13 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555177.555177 lmp.py:1935]   Expert 31 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555734.555734 lmp.py:1935]   Expert 30 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555138.555138 lmp.py:1935]   Expert 23 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555589.555589 lmp.py:1935]   Expert 59 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555278.555278 lmp.py:1935]   Expert 39 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555729.555729 lmp.py:1935]   Expert 53 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555703.555703 lmp.py:1935]   Expert 20 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555154.555154 lmp.py:1935]   Expert 21 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555367.555367 lmp.py:1935]   Expert 29 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555341.555341 lmp.py:1935]   Expert 36 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555553.555553 lmp.py:1935]   Expert 16 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555766.555766 lmp.py:1935]   Expert 25 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555740.555740 lmp.py:1935]   Expert 41 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555952.555952 lmp.py:1935]   Expert 49 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555118.555118 lmp.py:1935]   Expert 32 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555046.555046 lmp.py:1935]   Expert 46 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555735.555735 lmp.py:1935]   Expert  8 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555425.555425 lmp.py:1935]   Expert 42 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555352.555352 lmp.py:1935]   Expert 10 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555042.555042 lmp.py:1935]   Expert 62 |    263 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555493.555493 lmp.py:1935]   Expert 35 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555228.555228 lmp.py:1935]   Expert 33 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555441.555441 lmp.py:1935]   Expert  9 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555176.555176 lmp.py:1935]   Expert 58 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555389.555389 lmp.py:1935]   Expert 40 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555124.555124 lmp.py:1935]   Expert 11 |    429 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555098.555098 lmp.py:1935]   Expert  0 |    430 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555549.555549 lmp.py:1935]   Expert 24 |    563 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.555762.555762 lmp.py:1935]   Expert  1 |    644 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.555020.555020 lmp.py:1937] 
DEBUG 01-15 16:09:08.555020.555020 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.555187.555187 lmp.py:1938]   CPU:   2646 tokens
DEBUG 01-15 16:09:08.555591.555591 lmp.py:1942]   cuda:1:   4896 tokens (20 experts)
DEBUG 01-15 16:09:08.555042.555042 lmp.py:1942]   cuda:2:   4746 tokens (19 experts)
DEBUG 01-15 16:09:08.555016.555016 lmp.py:1943]   Total GPU:   9642 tokens
DEBUG 01-15 16:09:08.555990.555990 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.555990.555990 lmp.py:1944] 
DEBUG 01-15 16:09:08.555925.555925 cuda_h.py:19] end experts_map_get cost 0.0017406940460205078 seconds
DEBUG 01-15 16:09:08.555721.555721 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.556616.556616 lmp.py:1953] 
DEBUG 01-15 16:09:08.556616.556616 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.556777.556777 cuda_h.py:19] end cpu_experts_submit cost 4.7206878662109375e-05 seconds
DEBUG 01-15 16:09:08.556493.556493 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.556515.556515 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.557712.557712 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.1.gate_proj.weight': 0, 'model.layers.10.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.32.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.32.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.32.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.46.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.46.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.46.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.59.up_proj.weight': 322961408, 'model.layers.10.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.10.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.10.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.23.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.23.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.23.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.36.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.36.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.36.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(12843483136, 5767168, 0, 0), (12849250304, 5767168, 5767168, 0), (12837715968, 5767168, 11534336, 0), (12929990656, 5767168, 17301504, 0), (12935757824, 5767168, 23068672, 0), (12924223488, 5767168, 28835840, 0), (12981895168, 5767168, 34603008, 0), (12987662336, 5767168, 40370176, 0), (12976128000, 5767168, 46137344, 0), (13016498176, 5767168, 51904512, 0), (13022265344, 5767168, 57671680, 0), (13010731008, 5767168, 63438848, 0), (13033799680, 5767168, 69206016, 0), (13039566848, 5767168, 74973184, 0), (13028032512, 5767168, 80740352, 0), (13051101184, 5767168, 86507520, 0), (13056868352, 5767168, 92274688, 0), (13045334016, 5767168, 98041856, 0), (13103005696, 5767168, 103809024, 0), (13108772864, 5767168, 109576192, 0), (13097238528, 5767168, 115343360, 0), (13137608704, 5767168, 121110528, 0), (13143375872, 5767168, 126877696, 0), (13131841536, 5767168, 132644864, 0), (13172211712, 5767168, 138412032, 0), (13177978880, 5767168, 144179200, 0), (13166444544, 5767168, 149946368, 0), (13189513216, 5767168, 155713536, 0), (13195280384, 5767168, 161480704, 0), (13183746048, 5767168, 167247872, 0), (13258719232, 5767168, 173015040, 0), (13264486400, 5767168, 178782208, 0), (13252952064, 5767168, 184549376, 0), (13345226752, 5767168, 190316544, 0), (13350993920, 5767168, 196083712, 0), (13339459584, 5767168, 201850880, 0), (13379829760, 5767168, 207618048, 0), (13385596928, 5767168, 213385216, 0), (13374062592, 5767168, 219152384, 0), (13431734272, 5767168, 224919552, 0), (13437501440, 5767168, 230686720, 0), (13425967104, 5767168, 236453888, 0), (13552844800, 5767168, 242221056, 0), (13558611968, 5767168, 247988224, 0), (13547077632, 5767168, 253755392, 0), (13587447808, 5767168, 259522560, 0), (13593214976, 5767168, 265289728, 0), (13581680640, 5767168, 271056896, 0), (13622050816, 5767168, 276824064, 0), (13627817984, 5767168, 282591232, 0), (13616283648, 5767168, 288358400, 0), (13829668864, 5767168, 294125568, 0), (13835436032, 5767168, 299892736, 0), (13823901696, 5767168, 305659904, 0), (13846970368, 5767168, 311427072, 0), (13852737536, 5767168, 317194240, 0), (13841203200, 5767168, 322961408, 0), (13916176384, 5767168, 328728576, 0), (13921943552, 5767168, 334495744, 0), (13910409216, 5767168, 340262912, 0)], 2: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (12964593664, 5767168, 17301504, 0), (12970360832, 5767168, 23068672, 0), (12958826496, 5767168, 28835840, 0), (12999196672, 5767168, 34603008, 0), (13004963840, 5767168, 40370176, 0), (12993429504, 5767168, 46137344, 0), (13154910208, 5767168, 51904512, 0), (13160677376, 5767168, 57671680, 0), (13149143040, 5767168, 63438848, 0), (13224116224, 5767168, 69206016, 0), (13229883392, 5767168, 74973184, 0), (13218349056, 5767168, 80740352, 0), (13241417728, 5767168, 86507520, 0), (13247184896, 5767168, 92274688, 0), (13235650560, 5767168, 98041856, 0), (13327925248, 5767168, 103809024, 0), (13333692416, 5767168, 109576192, 0), (13322158080, 5767168, 115343360, 0), (13362528256, 5767168, 121110528, 0), (13368295424, 5767168, 126877696, 0), (13356761088, 5767168, 132644864, 0), (13397131264, 5767168, 138412032, 0), (13402898432, 5767168, 144179200, 0), (13391364096, 5767168, 149946368, 0), (13449035776, 5767168, 155713536, 0), (13454802944, 5767168, 161480704, 0), (13443268608, 5767168, 167247872, 0), (13500940288, 5767168, 173015040, 0), (13506707456, 5767168, 178782208, 0), (13495173120, 5767168, 184549376, 0), (13518241792, 5767168, 190316544, 0), (13524008960, 5767168, 196083712, 0), (13512474624, 5767168, 201850880, 0), (13535543296, 5767168, 207618048, 0), (13541310464, 5767168, 213385216, 0), (13529776128, 5767168, 219152384, 0), (13673955328, 5767168, 224919552, 0), (13679722496, 5767168, 230686720, 0), (13668188160, 5767168, 236453888, 0), (13691256832, 5767168, 242221056, 0), (13697024000, 5767168, 247988224, 0), (13685489664, 5767168, 253755392, 0), (13725859840, 5767168, 259522560, 0), (13731627008, 5767168, 265289728, 0), (13720092672, 5767168, 271056896, 0), (13743161344, 5767168, 276824064, 0), (13748928512, 5767168, 282591232, 0), (13737394176, 5767168, 288358400, 0), (13812367360, 5767168, 294125568, 0), (13818134528, 5767168, 299892736, 0), (13806600192, 5767168, 305659904, 0), (13898874880, 5767168, 311427072, 0), (13904642048, 5767168, 317194240, 0), (13893107712, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc337ab0>, 2: <capsule object NULL at 0x74a8143f79f0>}
DEBUG 01-15 16:09:08.557292.557292 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.558373.558373 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.558606.558606 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, eaa541d3-fa00-407c-b9ea-db91d50bc14c
DEBUG 01-15 16:09:08.558825.558825 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.558309.558309 client.py:127] Model loaded
DEBUG 01-15 16:09:08.558377.558377 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.559235.559235 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.559066.559066 cuda_h.py:19] end restore2model cost 0.0003688335418701172 seconds
DEBUG 01-15 16:09:08.559551.559551 cuda_h.py:19] end sllm_worker_task cost 0.01017308235168457 seconds
DEBUG 01-15 16:09:08.559761.559761 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:08.559625.559625 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, eaa541d3-fa00-407c-b9ea-db91d50bc14c
DEBUG 01-15 16:09:08.560363.560363 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003930568695068359 seconds
DEBUG 01-15 16:09:08.560717.560717 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.560493.560493 cuda_h.py:19] end move_flatidxs cost 0.0008318424224853516 seconds
DEBUG 01-15 16:09:08.560746.560746 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:08.563188.563188 cuda_h.py:19] end restore2model cost 0.003157377243041992 seconds
DEBUG 01-15 16:09:08.563885.563885 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00732421875 seconds
DEBUG 01-15 16:09:08.563965.563965 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.563551.563551 cuda_h.py:19] end gpu_sexperts cost 0.0002617835998535156 seconds
DEBUG 01-15 16:09:08.563235.563235 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.563488.563488 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:09:08.563469.563469 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.563265.563265 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.564425.564425 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010008811950683594 seconds
DEBUG 01-15 16:09:08.564175.564175 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.565038.565038 cuda_h.py:19] end gpu_group_list cost 0.00022077560424804688 seconds
DEBUG 01-15 16:09:08.566357.566357 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.567195.567195 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001013040542602539 seconds
DEBUG 01-15 16:09:08.567535.567535 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.567735.567735 cuda_h.py:19] end gpu_group_list cost 0.00022149085998535156 seconds
DEBUG 01-15 16:09:08.568441.568441 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.568039.568039 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, eaa541d3-fa00-407c-b9ea-db91d50bc14c
DEBUG 01-15 16:09:08.569542.569542 cuda_h.py:19] end group_tensors cost 0.008906364440917969 seconds
DEBUG 01-15 16:09:08.569441.569441 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.573903.573903 cuda_h.py:19] end group pad cost 0.003448963165283203 seconds
DEBUG 01-15 16:09:08.573362.573362 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:08.600652.600652 client.py:127] Model loaded
DEBUG 01-15 16:09:08.600366.600366 cuda_h.py:19] end wait_experts_multi_device cost 0.03231525421142578 seconds
DEBUG 01-15 16:09:08.600739.600739 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.601172.601172 cuda_h.py:19] end group_einsum cost 0.02829265594482422 seconds
DEBUG 01-15 16:09:08.602833.602833 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.604705.604705 cuda_h.py:19] end get_outputs_cpu1 cost 0.0026488304138183594 seconds
DEBUG 01-15 16:09:08.605777.605777 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04618668556213379 seconds
DEBUG 01-15 16:09:08.605706.605706 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005238056182861328 seconds
DEBUG 01-15 16:09:08.606638.606638 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.607074.607074 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.608745.608745 cuda_h.py:19] end index_scatter cost 0.00011920928955078125 seconds
DEBUG 01-15 16:09:08.608840.608840 cuda_h.py:19] end cpuoutputsdeal cost 0.002382040023803711 seconds
DEBUG 01-15 16:09:08.608062.608062 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.608204.608204 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.609188.609188 cuda_h.py:19] end gpu_group_tensor cost 0.000240325927734375 seconds
DEBUG 01-15 16:09:08.609190.609190 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.609743.609743 cuda_h.py:19] end gpu_group_tensor cost 0.000225067138671875 seconds
DEBUG 01-15 16:09:08.609219.609219 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.610697.610697 cuda_h.py:19] end gpu_group_einsum cost 0.0009276866912841797 seconds
DEBUG 01-15 16:09:08.610691.610691 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.611559.611559 cuda_h.py:19] end gpu_group_einsum cost 0.0007207393646240234 seconds
DEBUG 01-15 16:09:08.611784.611784 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.612950.612950 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.612144.612144 cuda_h.py:19] end all_expert_outputs_slices cost 0.00037550926208496094 seconds
DEBUG 01-15 16:09:08.612954.612954 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.612086.612086 cuda_h.py:19] end concat_expert_out cost 9.775161743164062e-05 seconds
DEBUG 01-15 16:09:08.612110.612110 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.613924.613924 cuda_h.py:19] end index_scatter cost 0.00010275840759277344 seconds
DEBUG 01-15 16:09:08.613897.613897 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0014858245849609375 seconds
DEBUG 01-15 16:09:08.613619.613619 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.613252.613252 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.614787.614787 cuda_h.py:19] end all_expert_outputs_slices cost 0.00029087066650390625 seconds
DEBUG 01-15 16:09:08.614822.614822 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.614145.614145 cuda_h.py:19] end concat_expert_out cost 0.00010132789611816406 seconds
DEBUG 01-15 16:09:08.614878.614878 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.614870.614870 cuda_h.py:19] end index_scatter cost 0.00010013580322265625 seconds
DEBUG 01-15 16:09:08.614065.614065 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009913444519042969 seconds
DEBUG 01-15 16:09:08.614963.614963 cuda_h.py:19] end gpu_experts_multi_device cost 0.05093264579772949 seconds
DEBUG 01-15 16:09:08.614929.614929 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.06153416633605957 seconds
DEBUG 01-15 16:09:08.615854.615854 cuda_h.py:19] end prefill_layer cost 0.06682801246643066 seconds
DEBUG 01-15 16:09:08.615452.615452 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:09:08.615347.615347 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.615010.615010 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:09:08.615813.615813 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:09:08.615046.615046 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:09:08.615710.615710 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 3.695487976074219e-05 seconds
DEBUG 01-15 16:09:08.615771.615771 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 8.392333984375e-05 seconds
DEBUG 01-15 16:09:08.615097.615097 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.615032.615032 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.615630.615630 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.616420.616420 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.616794.616794 cuda_h.py:19] end allocate_cuda_memory cost 0.0002357959747314453 seconds
DEBUG 01-15 16:09:08.616525.616525 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.616991.616991 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.616257.616257 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.616828.616828 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 03ecd90d-c1eb-4483-ac9a-13416a8c1e0c
DEBUG 01-15 16:09:08.616255.616255 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.616394.616394 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.617021.617021 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.617899.617899 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 03ecd90d-c1eb-4483-ac9a-13416a8c1e0c
DEBUG 01-15 16:09:08.617424.617424 cuda_h.py:19] end load_into_gpu_async cost 0.0014789104461669922 seconds
DEBUG 01-15 16:09:08.617094.617094 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.618158.618158 cuda_h.py:19] end restore_tensors2 cost 7.915496826171875e-05 seconds
DEBUG 01-15 16:09:08.618550.618550 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002117633819580078 seconds
INFO 01-15 16:09:08.618029.618029 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 03ecd90d-c1eb-4483-ac9a-13416a8c1e0c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.620428.620428 cuda_h.py:19] end self_attn cost 0.003572225570678711 seconds
DEBUG 01-15 16:09:08.621519.621519 cuda_h.py:19] end iln_self_attn_paln cost 0.00525975227355957 seconds
DEBUG 01-15 16:09:08.621355.621355 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:09:08.621787.621787 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.621130.621130 cuda_h.py:19] end gate cost 0.0007050037384033203 seconds
DEBUG 01-15 16:09:08.622542.622542 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.622125.622125 lmp.py:1912] 
DEBUG 01-15 16:09:08.622125.622125 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.622749.622749 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.622836.622836 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.622108.622108 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.622758.622758 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.622454.622454 lmp.py:1917] 
DEBUG 01-15 16:09:08.622454.622454 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.622866.622866 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.622237.622237 lmp.py:1935]   Expert 39 |     16 | CPU
DEBUG 01-15 16:09:08.622457.622457 lmp.py:1935]   Expert 13 |     18 | CPU
DEBUG 01-15 16:09:08.622199.622199 lmp.py:1935]   Expert 49 |     37 | CPU
DEBUG 01-15 16:09:08.622942.622942 lmp.py:1935]   Expert 35 |     54 | CPU
DEBUG 01-15 16:09:08.622684.622684 lmp.py:1935]   Expert 19 |     62 | CPU
DEBUG 01-15 16:09:08.622903.622903 lmp.py:1935]   Expert  9 |     72 | CPU
DEBUG 01-15 16:09:08.622599.622599 lmp.py:1935]   Expert 32 |     74 | CPU
DEBUG 01-15 16:09:08.622819.622819 lmp.py:1935]   Expert 26 |     75 | CPU
DEBUG 01-15 16:09:08.622038.622038 lmp.py:1935]   Expert 41 |     77 | CPU
DEBUG 01-15 16:09:08.622317.622317 lmp.py:1935]   Expert 33 |     82 | CPU
DEBUG 01-15 16:09:08.622119.622119 lmp.py:1935]   Expert 23 |     87 | CPU
DEBUG 01-15 16:09:08.622822.622822 lmp.py:1935]   Expert 46 |     88 | CPU
DEBUG 01-15 16:09:08.622333.622333 lmp.py:1935]   Expert 18 |     90 | CPU
DEBUG 01-15 16:09:08.622082.622082 lmp.py:1935]   Expert 31 |     92 | CPU
DEBUG 01-15 16:09:08.622831.622831 lmp.py:1935]   Expert 38 |     98 | CPU
DEBUG 01-15 16:09:08.622587.622587 lmp.py:1935]   Expert  3 |    102 | CPU
DEBUG 01-15 16:09:08.622191.622191 lmp.py:1935]   Expert 17 |    103 | CPU
DEBUG 01-15 16:09:08.623172.623172 lmp.py:1935]   Expert  6 |    107 | CPU
DEBUG 01-15 16:09:08.623868.623868 lmp.py:1935]   Expert 20 |    118 | CPU
DEBUG 01-15 16:09:08.623372.623372 lmp.py:1935]   Expert 40 |    128 | CPU
DEBUG 01-15 16:09:08.623160.623160 lmp.py:1935]   Expert 61 |    130 | CPU
DEBUG 01-15 16:09:08.623188.623188 lmp.py:1935]   Expert 15 |    131 | CPU
DEBUG 01-15 16:09:08.623215.623215 lmp.py:1935]   Expert 62 |    133 | CPU
DEBUG 01-15 16:09:08.623480.623480 lmp.py:1935]   Expert 43 |    135 | CPU
DEBUG 01-15 16:09:08.623031.623031 lmp.py:1935]   Expert 44 |    136 | CPU
DEBUG 01-15 16:09:08.623680.623680 lmp.py:1935]   Expert 16 |    137 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623615.623615 lmp.py:1935]   Expert 63 |    138 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623265.623265 lmp.py:1935]   Expert 50 |    139 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623199.623199 lmp.py:1935]   Expert 59 |    140 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623372.623372 lmp.py:1935]   Expert 42 |    141 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623783.623783 lmp.py:1935]   Expert  2 |    149 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623241.623241 lmp.py:1935]   Expert 36 |    149 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623937.623937 lmp.py:1935]   Expert 10 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623918.623918 lmp.py:1935]   Expert  5 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623137.623137 lmp.py:1935]   Expert 34 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623595.623595 lmp.py:1935]   Expert 27 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623576.623576 lmp.py:1935]   Expert 52 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623318.623318 lmp.py:1935]   Expert 45 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623299.623299 lmp.py:1935]   Expert 60 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623234.623234 lmp.py:1935]   Expert 48 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623122.623122 lmp.py:1935]   Expert 51 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623295.623295 lmp.py:1935]   Expert 56 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623183.623183 lmp.py:1935]   Expert 24 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623641.623641 lmp.py:1935]   Expert  7 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623860.623860 lmp.py:1935]   Expert 53 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623079.623079 lmp.py:1935]   Expert  8 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623537.623537 lmp.py:1935]   Expert 57 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623756.623756 lmp.py:1935]   Expert 47 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623737.623737 lmp.py:1935]   Expert 21 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623956.623956 lmp.py:1935]   Expert 29 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623937.623937 lmp.py:1935]   Expert  0 |    284 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623156.623156 lmp.py:1935]   Expert  4 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623091.623091 lmp.py:1935]   Expert 14 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623264.623264 lmp.py:1935]   Expert 22 |    314 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623675.623675 lmp.py:1935]   Expert 58 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623609.623609 lmp.py:1935]   Expert 55 |    319 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623067.623067 lmp.py:1935]   Expert 37 |    321 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623048.623048 lmp.py:1935]   Expert  1 |    323 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623267.623267 lmp.py:1935]   Expert 54 |    333 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623725.623725 lmp.py:1935]   Expert 28 |    364 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.623706.623706 lmp.py:1935]   Expert 12 |    378 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623925.623925 lmp.py:1935]   Expert 25 |    397 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.623383.623383 lmp.py:1935]   Expert 11 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.624317.624317 lmp.py:1935]   Expert 30 |    837 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.624159.624159 lmp.py:1937] 
DEBUG 01-15 16:09:08.624159.624159 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.624584.624584 lmp.py:1938]   CPU:   2245 tokens
DEBUG 01-15 16:09:08.624241.624241 lmp.py:1942]   cuda:1:   5021 tokens (19 experts)
DEBUG 01-15 16:09:08.624705.624705 lmp.py:1942]   cuda:2:   5022 tokens (20 experts)
DEBUG 01-15 16:09:08.624977.624977 lmp.py:1943]   Total GPU:  10043 tokens
DEBUG 01-15 16:09:08.624250.624250 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.624250.624250 lmp.py:1944] 
DEBUG 01-15 16:09:08.624059.624059 cuda_h.py:19] end experts_map_get cost 0.0021321773529052734 seconds
DEBUG 01-15 16:09:08.624313.624313 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.624851.624851 lmp.py:1953] 
DEBUG 01-15 16:09:08.624851.624851 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.624615.624615 cuda_h.py:19] end cpu_experts_submit cost 7.510185241699219e-05 seconds
DEBUG 01-15 16:09:08.624795.624795 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.624221.624221 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.625286.625286 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.1.gate_proj.weight': 0, 'model.layers.11.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.52.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.52.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.52.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.55.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.55.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.55.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.57.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.54.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.54.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.54.up_proj.weight': 322961408, 'model.layers.11.mlp.experts.58.gate_proj.weight': 328728576, 'model.layers.11.mlp.experts.58.down_proj.weight': 334495744, 'model.layers.11.mlp.experts.58.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(13950779392, 5767168, 0, 0), (13956546560, 5767168, 5767168, 0), (13945012224, 5767168, 11534336, 0), (14019985408, 5767168, 17301504, 0), (14025752576, 5767168, 23068672, 0), (14014218240, 5767168, 28835840, 0), (14054588416, 5767168, 34603008, 0), (14060355584, 5767168, 40370176, 0), (14048821248, 5767168, 46137344, 0), (14106492928, 5767168, 51904512, 0), (14112260096, 5767168, 57671680, 0), (14100725760, 5767168, 63438848, 0), (14175698944, 5767168, 69206016, 0), (14181466112, 5767168, 74973184, 0), (14169931776, 5767168, 80740352, 0), (14296809472, 5767168, 86507520, 0), (14302576640, 5767168, 92274688, 0), (14291042304, 5767168, 98041856, 0), (14314110976, 5767168, 103809024, 0), (14319878144, 5767168, 109576192, 0), (14308343808, 5767168, 115343360, 0), (14417920000, 5767168, 121110528, 0), (14423687168, 5767168, 126877696, 0), (14412152832, 5767168, 132644864, 0), (14435221504, 5767168, 138412032, 0), (14440988672, 5767168, 144179200, 0), (14429454336, 5767168, 149946368, 0), (14452523008, 5767168, 155713536, 0), (14458290176, 5767168, 161480704, 0), (14446755840, 5767168, 167247872, 0), (14660141056, 5767168, 173015040, 0), (14665908224, 5767168, 178782208, 0), (14654373888, 5767168, 184549376, 0), (14815854592, 5767168, 190316544, 0), (14821621760, 5767168, 196083712, 0), (14810087424, 5767168, 201850880, 0), (14833156096, 5767168, 207618048, 0), (14838923264, 5767168, 213385216, 0), (14827388928, 5767168, 219152384, 0), (14885060608, 5767168, 224919552, 0), (14890827776, 5767168, 230686720, 0), (14879293440, 5767168, 236453888, 0), (14902362112, 5767168, 242221056, 0), (14908129280, 5767168, 247988224, 0), (14896594944, 5767168, 253755392, 0), (14919663616, 5767168, 259522560, 0), (14925430784, 5767168, 265289728, 0), (14913896448, 5767168, 271056896, 0), (14954266624, 5767168, 276824064, 0), (14960033792, 5767168, 282591232, 0), (14948499456, 5767168, 288358400, 0), (14971568128, 5767168, 294125568, 0), (14977335296, 5767168, 299892736, 0), (14965800960, 5767168, 305659904, 0), (15023472640, 5767168, 311427072, 0), (15029239808, 5767168, 317194240, 0), (15017705472, 5767168, 322961408, 0)], 2: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (13968080896, 5767168, 17301504, 0), (13973848064, 5767168, 23068672, 0), (13962313728, 5767168, 28835840, 0), (14002683904, 5767168, 34603008, 0), (14008451072, 5767168, 40370176, 0), (13996916736, 5767168, 46137344, 0), (14071889920, 5767168, 51904512, 0), (14077657088, 5767168, 57671680, 0), (14066122752, 5767168, 63438848, 0), (14123794432, 5767168, 69206016, 0), (14129561600, 5767168, 74973184, 0), (14118027264, 5767168, 80740352, 0), (14141095936, 5767168, 86507520, 0), (14146863104, 5767168, 92274688, 0), (14135328768, 5767168, 98041856, 0), (14210301952, 5767168, 103809024, 0), (14216069120, 5767168, 109576192, 0), (14204534784, 5767168, 115343360, 0), (14348713984, 5767168, 121110528, 0), (14354481152, 5767168, 126877696, 0), (14342946816, 5767168, 132644864, 0), (14366015488, 5767168, 138412032, 0), (14371782656, 5767168, 144179200, 0), (14360248320, 5767168, 149946368, 0), (14400618496, 5767168, 155713536, 0), (14406385664, 5767168, 161480704, 0), (14394851328, 5767168, 167247872, 0), (14521729024, 5767168, 173015040, 0), (14527496192, 5767168, 178782208, 0), (14515961856, 5767168, 184549376, 0), (14556332032, 5767168, 190316544, 0), (14562099200, 5767168, 196083712, 0), (14550564864, 5767168, 201850880, 0), (14573633536, 5767168, 207618048, 0), (14579400704, 5767168, 213385216, 0), (14567866368, 5767168, 219152384, 0), (14712045568, 5767168, 224919552, 0), (14717812736, 5767168, 230686720, 0), (14706278400, 5767168, 236453888, 0), (14746648576, 5767168, 242221056, 0), (14752415744, 5767168, 247988224, 0), (14740881408, 5767168, 253755392, 0), (14763950080, 5767168, 259522560, 0), (14769717248, 5767168, 265289728, 0), (14758182912, 5767168, 271056896, 0), (14798553088, 5767168, 276824064, 0), (14804320256, 5767168, 282591232, 0), (14792785920, 5767168, 288358400, 0), (14850457600, 5767168, 294125568, 0), (14856224768, 5767168, 299892736, 0), (14844690432, 5767168, 305659904, 0), (14867759104, 5767168, 311427072, 0), (14873526272, 5767168, 317194240, 0), (14861991936, 5767168, 322961408, 0), (14936965120, 5767168, 328728576, 0), (14942732288, 5767168, 334495744, 0), (14931197952, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6807aa160>, 2: <capsule object NULL at 0x74a6807aa1f0>}
DEBUG 01-15 16:09:08.625817.625817 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.625859.625859 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.625807.625807 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9adc00a8-69a0-4361-b763-faf6469ec8f2
DEBUG 01-15 16:09:08.625437.625437 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.626590.626590 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:08.626263.626263 client.py:127] Model loaded
DEBUG 01-15 16:09:08.626417.626417 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.626686.626686 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.626873.626873 cuda_h.py:19] end restore2model cost 0.0003407001495361328 seconds
DEBUG 01-15 16:09:08.626689.626689 cuda_h.py:19] end sllm_worker_task cost 0.01065826416015625 seconds
INFO 01-15 16:09:08.627446.627446 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9adc00a8-69a0-4361-b763-faf6469ec8f2
DEBUG 01-15 16:09:08.627517.627517 cuda_h.py:19] end move_flatidxs cost 0.0008349418640136719 seconds
DEBUG 01-15 16:09:08.627532.627532 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:08.627291.627291 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0032367706298828125 seconds
DEBUG 01-15 16:09:08.627183.627183 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.631595.631595 cuda_h.py:19] end restore2model cost 0.0031747817993164062 seconds
DEBUG 01-15 16:09:08.631823.631823 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0067365169525146484 seconds
DEBUG 01-15 16:09:08.631831.631831 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.631861.631861 cuda_h.py:19] end gpu_sexperts cost 0.0002682209014892578 seconds
DEBUG 01-15 16:09:08.631021.631021 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.631274.631274 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5020370483398438e-05 seconds
DEBUG 01-15 16:09:08.631971.631971 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.631912.631912 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.632753.632753 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009765625 seconds
DEBUG 01-15 16:09:08.632661.632661 cuda_h.py:19] end group_tensors cost 0.004798173904418945 seconds
DEBUG 01-15 16:09:08.632325.632325 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.632129.632129 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.632605.632605 cuda_h.py:19] end gpu_group_list cost 0.00021147727966308594 seconds
DEBUG 01-15 16:09:08.633628.633628 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.635073.635073 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001112222671508789 seconds
DEBUG 01-15 16:09:08.635824.635824 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.635383.635383 cuda_h.py:19] end gpu_group_list cost 0.00023984909057617188 seconds
DEBUG 01-15 16:09:08.636787.636787 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.636213.636213 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9adc00a8-69a0-4361-b763-faf6469ec8f2
DEBUG 01-15 16:09:08.636422.636422 cuda_h.py:19] end group pad cost 0.003955841064453125 seconds
DEBUG 01-15 16:09:08.636212.636212 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:08.657161.657161 cuda_h.py:19] end group_einsum cost 0.020336389541625977 seconds
DEBUG 01-15 16:09:08.657226.657226 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.660942.660942 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029268264770507812 seconds
DEBUG 01-15 16:09:08.661034.661034 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03495287895202637 seconds
INFO 01-15 16:09:08.662517.662517 client.py:127] Model loaded
DEBUG 01-15 16:09:08.662687.662687 cuda_h.py:19] end wait_experts_multi_device cost 0.026391983032226562 seconds
DEBUG 01-15 16:09:08.662324.662324 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.663462.663462 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005052089691162109 seconds
DEBUG 01-15 16:09:08.663557.663557 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.664012.664012 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.664469.664469 cuda_h.py:19] end index_scatter cost 9.489059448242188e-05 seconds
DEBUG 01-15 16:09:08.665167.665167 cuda_h.py:19] end cpuoutputsdeal cost 0.0017690658569335938 seconds
DEBUG 01-15 16:09:08.665720.665720 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.665020.665020 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.665418.665418 cuda_h.py:19] end gpu_group_tensor cost 0.00017905235290527344 seconds
DEBUG 01-15 16:09:08.665506.665506 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.665350.665350 cuda_h.py:19] end gpu_group_tensor cost 0.00021314620971679688 seconds
DEBUG 01-15 16:09:08.666474.666474 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.666419.666419 cuda_h.py:19] end gpu_group_einsum cost 0.0007979869842529297 seconds
DEBUG 01-15 16:09:08.667015.667015 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.667059.667059 cuda_h.py:19] end gpu_group_einsum cost 0.0006458759307861328 seconds
DEBUG 01-15 16:09:08.668880.668880 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.668773.668773 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.668993.668993 cuda_h.py:19] end all_expert_outputs_slices cost 0.00036072731018066406 seconds
DEBUG 01-15 16:09:08.668796.668796 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.668332.668332 cuda_h.py:19] end concat_expert_out cost 0.00012731552124023438 seconds
DEBUG 01-15 16:09:08.669489.669489 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.669328.669328 cuda_h.py:19] end index_scatter cost 9.489059448242188e-05 seconds
DEBUG 01-15 16:09:08.669924.669924 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0014193058013916016 seconds
DEBUG 01-15 16:09:08.669002.669002 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.669529.669529 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.670666.670666 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002853870391845703 seconds
DEBUG 01-15 16:09:08.670940.670940 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.670150.670150 cuda_h.py:19] end concat_expert_out cost 9.703636169433594e-05 seconds
DEBUG 01-15 16:09:08.670584.670584 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.670325.670325 cuda_h.py:19] end index_scatter cost 9.226799011230469e-05 seconds
DEBUG 01-15 16:09:08.670512.670512 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009393692016601562 seconds
DEBUG 01-15 16:09:08.670689.670689 cuda_h.py:19] end gpu_experts_multi_device cost 0.0392301082611084 seconds
DEBUG 01-15 16:09:08.671675.671675 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.04984235763549805 seconds
DEBUG 01-15 16:09:08.671601.671601 cuda_h.py:19] end prefill_layer cost 0.05600547790527344 seconds
DEBUG 01-15 16:09:08.671724.671724 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:09:08.671222.671222 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.671105.671105 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:09:08.671749.671749 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:09:08.671254.671254 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:09:08.672629.672629 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:09:08.672453.672453 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 0.00016355514526367188 seconds
DEBUG 01-15 16:09:08.672528.672528 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.672842.672842 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.672793.672793 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.672591.672591 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.672849.672849 cuda_h.py:19] end allocate_cuda_memory cost 0.0003266334533691406 seconds
DEBUG 01-15 16:09:08.673032.673032 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.673280.673280 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.673502.673502 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.673967.673967 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 89da7a15-545f-41ab-b9fa-ef115c6786e7
DEBUG 01-15 16:09:08.673284.673284 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.673769.673769 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.674332.674332 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.674919.674919 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 89da7a15-545f-41ab-b9fa-ef115c6786e7
DEBUG 01-15 16:09:08.674420.674420 cuda_h.py:19] end load_into_gpu_async cost 0.0016112327575683594 seconds
DEBUG 01-15 16:09:08.674177.674177 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.675727.675727 cuda_h.py:19] end restore_tensors2 cost 0.0001316070556640625 seconds
DEBUG 01-15 16:09:08.675597.675597 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0025949478149414062 seconds
INFO 01-15 16:09:08.675868.675868 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 89da7a15-545f-41ab-b9fa-ef115c6786e7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.678043.678043 cuda_h.py:19] end self_attn cost 0.004118442535400391 seconds
DEBUG 01-15 16:09:08.678299.678299 cuda_h.py:19] end iln_self_attn_paln cost 0.00655055046081543 seconds
DEBUG 01-15 16:09:08.678698.678698 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:09:08.678407.678407 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.679376.679376 cuda_h.py:19] end gate cost 0.0006110668182373047 seconds
DEBUG 01-15 16:09:08.679729.679729 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.679938.679938 lmp.py:1912] 
DEBUG 01-15 16:09:08.679938.679938 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.679847.679847 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.680027.680027 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.680630.680630 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.680611.680611 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.680877.680877 lmp.py:1917] 
DEBUG 01-15 16:09:08.680877.680877 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.680096.680096 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.680183.680183 lmp.py:1935]   Expert 12 |     20 | CPU
DEBUG 01-15 16:09:08.680641.680641 lmp.py:1935]   Expert 47 |     22 | CPU
DEBUG 01-15 16:09:08.680383.680383 lmp.py:1935]   Expert 38 |     30 | CPU
DEBUG 01-15 16:09:08.680125.680125 lmp.py:1935]   Expert 27 |     35 | CPU
DEBUG 01-15 16:09:08.680583.680583 lmp.py:1935]   Expert 16 |     37 | CPU
DEBUG 01-15 16:09:08.680325.680325 lmp.py:1935]   Expert 52 |     39 | CPU
DEBUG 01-15 16:09:08.680306.680306 lmp.py:1935]   Expert 63 |     46 | CPU
DEBUG 01-15 16:09:08.680049.680049 lmp.py:1935]   Expert  4 |     58 | CPU
DEBUG 01-15 16:09:08.680997.680997 lmp.py:1935]   Expert 44 |     60 | CPU
DEBUG 01-15 16:09:08.680070.680070 lmp.py:1935]   Expert 43 |     62 | CPU
DEBUG 01-15 16:09:08.680283.680283 lmp.py:1935]   Expert 61 |     62 | CPU
DEBUG 01-15 16:09:08.680257.680257 lmp.py:1935]   Expert 34 |     76 | CPU
DEBUG 01-15 16:09:08.680708.680708 lmp.py:1935]   Expert 53 |     83 | CPU
DEBUG 01-15 16:09:08.680881.680881 lmp.py:1935]   Expert  0 |     89 | CPU
DEBUG 01-15 16:09:08.680524.680524 lmp.py:1935]   Expert 32 |     89 | CPU
DEBUG 01-15 16:09:08.680498.680498 lmp.py:1935]   Expert 37 |     91 | CPU
DEBUG 01-15 16:09:08.680233.680233 lmp.py:1935]   Expert 13 |    103 | CPU
DEBUG 01-15 16:09:08.680161.680161 lmp.py:1935]   Expert 39 |    115 | CPU
DEBUG 01-15 16:09:08.680850.680850 lmp.py:1935]   Expert 21 |    116 | CPU
DEBUG 01-15 16:09:08.680540.680540 lmp.py:1935]   Expert 11 |    122 | CPU
DEBUG 01-15 16:09:08.680229.680229 lmp.py:1935]   Expert  8 |    127 | CPU
DEBUG 01-15 16:09:08.680680.680680 lmp.py:1935]   Expert 20 |    128 | CPU
DEBUG 01-15 16:09:08.680131.680131 lmp.py:1935]   Expert 60 |    133 | CPU
DEBUG 01-15 16:09:08.680343.680343 lmp.py:1935]   Expert 14 |    137 | CPU
DEBUG 01-15 16:09:08.680602.680602 lmp.py:1935]   Expert 57 |    138 | CPU
DEBUG 01-15 16:09:08.680006.680006 lmp.py:1935]   Expert 22 |    139 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680888.680888 lmp.py:1935]   Expert 45 |    151 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680054.680054 lmp.py:1935]   Expert 18 |    156 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680982.680982 lmp.py:1935]   Expert  2 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680433.680433 lmp.py:1935]   Expert 23 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680360.680360 lmp.py:1935]   Expert  7 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680811.680811 lmp.py:1935]   Expert 17 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680501.680501 lmp.py:1935]   Expert 58 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680428.680428 lmp.py:1935]   Expert 30 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680118.680118 lmp.py:1935]   Expert 42 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680807.680807 lmp.py:1935]   Expert 48 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680450.680450 lmp.py:1935]   Expert 49 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680854.680854 lmp.py:1935]   Expert 55 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680259.680259 lmp.py:1935]   Expert 62 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680663.680663 lmp.py:1935]   Expert 35 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680830.680830 lmp.py:1935]   Expert 51 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680234.680234 lmp.py:1935]   Expert 29 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680162.680162 lmp.py:1935]   Expert  6 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680613.680613 lmp.py:1935]   Expert 36 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680064.680064 lmp.py:1935]   Expert 25 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680991.680991 lmp.py:1935]   Expert  1 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680158.680158 lmp.py:1935]   Expert 31 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.680847.680847 lmp.py:1935]   Expert 28 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680536.680536 lmp.py:1935]   Expert 54 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.680225.680225 lmp.py:1935]   Expert  5 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.681153.681153 lmp.py:1935]   Expert 41 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.681319.681319 lmp.py:1935]   Expert 19 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.681009.681009 lmp.py:1935]   Expert  9 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.681698.681698 lmp.py:1935]   Expert 24 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.681387.681387 lmp.py:1935]   Expert 50 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.681269.681269 lmp.py:1935]   Expert 46 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.681673.681673 lmp.py:1935]   Expert 59 |    312 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.681316.681316 lmp.py:1935]   Expert 56 |    376 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.681721.681721 lmp.py:1935]   Expert 26 |    405 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.681125.681125 lmp.py:1935]   Expert 33 |    423 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.681530.681530 lmp.py:1935]   Expert  3 |    586 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.681219.681219 lmp.py:1935]   Expert 10 |    645 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.681908.681908 lmp.py:1935]   Expert 15 |    645 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.681359.681359 lmp.py:1935]   Expert 40 |    794 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.681095.681095 lmp.py:1937] 
DEBUG 01-15 16:09:08.681095.681095 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.681784.681784 lmp.py:1938]   CPU:   2018 tokens
DEBUG 01-15 16:09:08.681950.681950 lmp.py:1942]   cuda:1:   5204 tokens (20 experts)
DEBUG 01-15 16:09:08.681163.681163 lmp.py:1942]   cuda:2:   5066 tokens (19 experts)
DEBUG 01-15 16:09:08.681375.681375 lmp.py:1943]   Total GPU:  10270 tokens
DEBUG 01-15 16:09:08.681872.681872 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.681872.681872 lmp.py:1944] 
DEBUG 01-15 16:09:08.681721.681721 cuda_h.py:19] end experts_map_get cost 0.0017201900482177734 seconds
DEBUG 01-15 16:09:08.681578.681578 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.681956.681956 lmp.py:1953] 
DEBUG 01-15 16:09:08.681956.681956 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.681508.681508 cuda_h.py:19] end cpu_experts_submit cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:09:08.681264.681264 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.681630.681630 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.682134.682134 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.3.gate_proj.weight': 0, 'model.layers.12.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.17.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.17.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.17.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.22.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.22.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.22.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.45.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.45.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.45.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.48.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.48.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.48.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.12.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.12.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.12.mlp.experts.59.up_proj.weight': 340262912}, 2: {'model.layers.12.mlp.experts.1.gate_proj.weight': 0, 'model.layers.12.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.36.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.36.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.36.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.42.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.42.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.42.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.46.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.46.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.46.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(15092678656, 5767168, 0, 0), (15098445824, 5767168, 5767168, 0), (15086911488, 5767168, 11534336, 0), (15127281664, 5767168, 17301504, 0), (15133048832, 5767168, 23068672, 0), (15121514496, 5767168, 28835840, 0), (15144583168, 5767168, 34603008, 0), (15150350336, 5767168, 40370176, 0), (15138816000, 5767168, 46137344, 0), (15334899712, 5767168, 51904512, 0), (15340666880, 5767168, 57671680, 0), (15329132544, 5767168, 63438848, 0), (15421407232, 5767168, 69206016, 0), (15427174400, 5767168, 74973184, 0), (15415640064, 5767168, 80740352, 0), (15438708736, 5767168, 86507520, 0), (15444475904, 5767168, 92274688, 0), (15432941568, 5767168, 98041856, 0), (15456010240, 5767168, 103809024, 0), (15461777408, 5767168, 109576192, 0), (15450243072, 5767168, 115343360, 0), (15473311744, 5767168, 121110528, 0), (15479078912, 5767168, 126877696, 0), (15467544576, 5767168, 132644864, 0), (15490613248, 5767168, 138412032, 0), (15496380416, 5767168, 144179200, 0), (15484846080, 5767168, 149946368, 0), (15525216256, 5767168, 155713536, 0), (15530983424, 5767168, 161480704, 0), (15519449088, 5767168, 167247872, 0), (15732834304, 5767168, 173015040, 0), (15738601472, 5767168, 178782208, 0), (15727067136, 5767168, 184549376, 0), (15819341824, 5767168, 190316544, 0), (15825108992, 5767168, 196083712, 0), (15813574656, 5767168, 201850880, 0), (15871246336, 5767168, 207618048, 0), (15877013504, 5767168, 213385216, 0), (15865479168, 5767168, 219152384, 0), (15888547840, 5767168, 224919552, 0), (15894315008, 5767168, 230686720, 0), (15882780672, 5767168, 236453888, 0), (15905849344, 5767168, 242221056, 0), (15911616512, 5767168, 247988224, 0), (15900082176, 5767168, 253755392, 0), (15923150848, 5767168, 259522560, 0), (15928918016, 5767168, 265289728, 0), (15917383680, 5767168, 271056896, 0), (15975055360, 5767168, 276824064, 0), (15980822528, 5767168, 282591232, 0), (15969288192, 5767168, 288358400, 0), (15992356864, 5767168, 294125568, 0), (15998124032, 5767168, 299892736, 0), (15986589696, 5767168, 305659904, 0), (16044261376, 5767168, 311427072, 0), (16050028544, 5767168, 317194240, 0), (16038494208, 5767168, 322961408, 0), (16061562880, 5767168, 328728576, 0), (16067330048, 5767168, 334495744, 0), (16055795712, 5767168, 340262912, 0)], 2: [(15058075648, 5767168, 0, 0), (15063842816, 5767168, 5767168, 0), (15052308480, 5767168, 11534336, 0), (15075377152, 5767168, 17301504, 0), (15081144320, 5767168, 23068672, 0), (15069609984, 5767168, 28835840, 0), (15161884672, 5767168, 34603008, 0), (15167651840, 5767168, 40370176, 0), (15156117504, 5767168, 46137344, 0), (15196487680, 5767168, 51904512, 0), (15202254848, 5767168, 57671680, 0), (15190720512, 5767168, 63438848, 0), (15213789184, 5767168, 69206016, 0), (15219556352, 5767168, 74973184, 0), (15208022016, 5767168, 80740352, 0), (15300296704, 5767168, 86507520, 0), (15306063872, 5767168, 92274688, 0), (15294529536, 5767168, 98041856, 0), (15352201216, 5767168, 103809024, 0), (15357968384, 5767168, 109576192, 0), (15346434048, 5767168, 115343360, 0), (15369502720, 5767168, 121110528, 0), (15375269888, 5767168, 126877696, 0), (15363735552, 5767168, 132644864, 0), (15542517760, 5767168, 138412032, 0), (15548284928, 5767168, 144179200, 0), (15536750592, 5767168, 149946368, 0), (15559819264, 5767168, 155713536, 0), (15565586432, 5767168, 161480704, 0), (15554052096, 5767168, 167247872, 0), (15577120768, 5767168, 173015040, 0), (15582887936, 5767168, 178782208, 0), (15571353600, 5767168, 184549376, 0), (15611723776, 5767168, 190316544, 0), (15617490944, 5767168, 196083712, 0), (15605956608, 5767168, 201850880, 0), (15646326784, 5767168, 207618048, 0), (15652093952, 5767168, 213385216, 0), (15640559616, 5767168, 219152384, 0), (15663628288, 5767168, 224919552, 0), (15669395456, 5767168, 230686720, 0), (15657861120, 5767168, 236453888, 0), (15750135808, 5767168, 242221056, 0), (15755902976, 5767168, 247988224, 0), (15744368640, 5767168, 253755392, 0), (15767437312, 5767168, 259522560, 0), (15773204480, 5767168, 265289728, 0), (15761670144, 5767168, 271056896, 0), (15836643328, 5767168, 276824064, 0), (15842410496, 5767168, 282591232, 0), (15830876160, 5767168, 288358400, 0), (16009658368, 5767168, 294125568, 0), (16015425536, 5767168, 299892736, 0), (16003891200, 5767168, 305659904, 0), (16113467392, 5767168, 311427072, 0), (16119234560, 5767168, 317194240, 0), (16107700224, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a688117c60>, 2: <capsule object NULL at 0x74a6882a22e0>}
DEBUG 01-15 16:09:08.682045.682045 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.682974.682974 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.682061.682061 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7a0b3e69-13b2-413f-91ba-fbb4f6add1ef
DEBUG 01-15 16:09:08.682638.682638 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.683114.683114 client.py:127] Model loaded
DEBUG 01-15 16:09:08.683275.683275 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.683339.683339 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.683123.683123 cuda_h.py:19] end restore2model cost 0.0003809928894042969 seconds
DEBUG 01-15 16:09:08.683562.683562 cuda_h.py:19] end sllm_worker_task cost 0.011113405227661133 seconds
DEBUG 01-15 16:09:08.683819.683819 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:08.683641.683641 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7a0b3e69-13b2-413f-91ba-fbb4f6add1ef
DEBUG 01-15 16:09:08.684433.684433 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002771615982055664 seconds
DEBUG 01-15 16:09:08.684515.684515 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.684941.684941 cuda_h.py:19] end move_flatidxs cost 0.0008382797241210938 seconds
DEBUG 01-15 16:09:08.684671.684671 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:08.687180.687180 cuda_h.py:19] end restore2model cost 0.0030202865600585938 seconds
DEBUG 01-15 16:09:08.687646.687646 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0060503482818603516 seconds
DEBUG 01-15 16:09:08.687395.687395 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.687809.687809 cuda_h.py:19] end gpu_sexperts cost 0.0002753734588623047 seconds
DEBUG 01-15 16:09:08.688745.688745 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.688329.688329 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4781951904296875e-05 seconds
DEBUG 01-15 16:09:08.688741.688741 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.688967.688967 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.689597.689597 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009953975677490234 seconds
DEBUG 01-15 16:09:08.689209.689209 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.689734.689734 cuda_h.py:19] end gpu_group_list cost 0.00021791458129882812 seconds
DEBUG 01-15 16:09:08.690060.690060 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.691435.691435 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010592937469482422 seconds
DEBUG 01-15 16:09:08.691186.691186 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.691533.691533 cuda_h.py:19] end gpu_group_list cost 0.00021028518676757812 seconds
DEBUG 01-15 16:09:08.692986.692986 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.692961.692961 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7a0b3e69-13b2-413f-91ba-fbb4f6add1ef
DEBUG 01-15 16:09:08.694240.694240 cuda_h.py:19] end group_tensors cost 0.010210275650024414 seconds
DEBUG 01-15 16:09:08.695578.695578 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.698225.698225 cuda_h.py:19] end group pad cost 0.003050565719604492 seconds
DEBUG 01-15 16:09:08.698207.698207 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:08.717850.717850 cuda_h.py:19] end group_einsum cost 0.018700361251831055 seconds
DEBUG 01-15 16:09:08.717260.717260 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:08.720176.720176 client.py:127] Model loaded
DEBUG 01-15 16:09:08.721148.721148 cuda_h.py:19] end wait_experts_multi_device cost 0.028467893600463867 seconds
DEBUG 01-15 16:09:08.721479.721479 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.722939.722939 cuda_h.py:19] end get_outputs_cpu1 cost 0.00419163703918457 seconds
DEBUG 01-15 16:09:08.722972.722972 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039197444915771484 seconds
DEBUG 01-15 16:09:08.723625.723625 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0017504692077636719 seconds
DEBUG 01-15 16:09:08.723276.723276 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.724779.724779 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.724492.724492 cuda_h.py:19] end index_scatter cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:09:08.724152.724152 cuda_h.py:19] end cpuoutputsdeal cost 0.0013790130615234375 seconds
DEBUG 01-15 16:09:08.724737.724737 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.724831.724831 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.724122.724122 cuda_h.py:19] end gpu_group_tensor cost 0.00014781951904296875 seconds
DEBUG 01-15 16:09:08.724692.724692 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.724553.724553 cuda_h.py:19] end gpu_group_tensor cost 0.00013589859008789062 seconds
DEBUG 01-15 16:09:08.725172.725172 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.725491.725491 cuda_h.py:19] end gpu_group_einsum cost 0.0005702972412109375 seconds
DEBUG 01-15 16:09:08.725727.725727 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.726956.726956 cuda_h.py:19] end gpu_group_einsum cost 0.0004184246063232422 seconds
DEBUG 01-15 16:09:08.726906.726906 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.726393.726393 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.726599.726599 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018405914306640625 seconds
DEBUG 01-15 16:09:08.726594.726594 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.726318.726318 cuda_h.py:19] end concat_expert_out cost 4.7206878662109375e-05 seconds
DEBUG 01-15 16:09:08.726969.726969 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.727654.727654 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:08.727563.727563 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007908344268798828 seconds
DEBUG 01-15 16:09:08.727732.727732 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.727005.727005 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.727349.727349 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015211105346679688 seconds
DEBUG 01-15 16:09:08.727483.727483 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.727638.727638 cuda_h.py:19] end concat_expert_out cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:08.727335.727335 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.727146.727146 cuda_h.py:19] end index_scatter cost 5.602836608886719e-05 seconds
DEBUG 01-15 16:09:08.727485.727485 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005207061767578125 seconds
DEBUG 01-15 16:09:08.728402.728402 cuda_h.py:19] end gpu_experts_multi_device cost 0.03989672660827637 seconds
DEBUG 01-15 16:09:08.728650.728650 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.04916262626647949 seconds
DEBUG 01-15 16:09:08.728705.728705 cuda_h.py:19] end prefill_layer cost 0.05669212341308594 seconds
DEBUG 01-15 16:09:08.728972.728972 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:09:08.728151.728151 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.728569.728569 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:09:08.728246.728246 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:09:08.728956.728956 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:09:08.728283.728283 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 3.719329833984375e-05 seconds
DEBUG 01-15 16:09:08.728608.728608 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:09:08.728304.728304 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.728684.728684 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.729987.729987 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.729539.729539 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.729008.729008 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.729893.729893 cuda_h.py:19] end allocate_cuda_memory cost 0.00027298927307128906 seconds
DEBUG 01-15 16:09:08.729640.729640 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.729913.729913 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.729459.729459 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.729812.729812 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 61db1085-9f80-475e-867a-73c6baa37f18
DEBUG 01-15 16:09:08.730750.730750 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.730494.730494 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.731822.731822 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 61db1085-9f80-475e-867a-73c6baa37f18
DEBUG 01-15 16:09:08.731462.731462 cuda_h.py:19] end load_into_gpu_async cost 0.001561880111694336 seconds
DEBUG 01-15 16:09:08.731914.731914 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.731125.731125 cuda_h.py:19] end restore_tensors2 cost 0.00010752677917480469 seconds
DEBUG 01-15 16:09:08.731108.731108 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024199485778808594 seconds
INFO 01-15 16:09:08.731793.731793 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 61db1085-9f80-475e-867a-73c6baa37f18
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.733472.733472 cuda_h.py:19] end self_attn cost 0.0034248828887939453 seconds
DEBUG 01-15 16:09:08.734244.734244 cuda_h.py:19] end iln_self_attn_paln cost 0.005294084548950195 seconds
DEBUG 01-15 16:09:08.734743.734743 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:09:08.734903.734903 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.735901.735901 cuda_h.py:19] end gate cost 0.0006968975067138672 seconds
DEBUG 01-15 16:09:08.735168.735168 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.735288.735288 lmp.py:1912] 
DEBUG 01-15 16:09:08.735288.735288 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.735925.735925 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.735820.735820 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.735900.735900 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.735835.735835 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.735577.735577 lmp.py:1917] 
DEBUG 01-15 16:09:08.735577.735577 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.735273.735273 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.735122.735122 lmp.py:1935]   Expert 19 |     23 | CPU
DEBUG 01-15 16:09:08.735580.735580 lmp.py:1935]   Expert 42 |     25 | CPU
DEBUG 01-15 16:09:08.735322.735322 lmp.py:1935]   Expert 30 |     27 | CPU
DEBUG 01-15 16:09:08.735064.735064 lmp.py:1935]   Expert 32 |     42 | CPU
DEBUG 01-15 16:09:08.735807.735807 lmp.py:1935]   Expert  6 |     57 | CPU
DEBUG 01-15 16:09:08.735311.735311 lmp.py:1935]   Expert  5 |     74 | CPU
DEBUG 01-15 16:09:08.735676.735676 lmp.py:1935]   Expert 53 |     74 | CPU
DEBUG 01-15 16:09:08.735372.735372 lmp.py:1935]   Expert  1 |     77 | CPU
DEBUG 01-15 16:09:08.735830.735830 lmp.py:1935]   Expert 13 |    120 | CPU
DEBUG 01-15 16:09:08.735877.735877 lmp.py:1935]   Expert  9 |    122 | CPU
DEBUG 01-15 16:09:08.735487.735487 lmp.py:1935]   Expert 63 |    125 | CPU
DEBUG 01-15 16:09:08.735184.735184 lmp.py:1935]   Expert 34 |    131 | CPU
DEBUG 01-15 16:09:08.735926.735926 lmp.py:1935]   Expert 58 |    131 | CPU
DEBUG 01-15 16:09:08.735430.735430 lmp.py:1935]   Expert 50 |    132 | CPU
DEBUG 01-15 16:09:08.735696.735696 lmp.py:1935]   Expert 11 |    137 | CPU
DEBUG 01-15 16:09:08.735961.735961 lmp.py:1935]   Expert 26 |    137 | CPU
DEBUG 01-15 16:09:08.736988.736988 lmp.py:1935]   Expert 18 |    139 | CPU
DEBUG 01-15 16:09:08.736492.736492 lmp.py:1935]   Expert 31 |    140 | CPU
DEBUG 01-15 16:09:08.736519.736519 lmp.py:1935]   Expert 59 |    140 | CPU
DEBUG 01-15 16:09:08.736785.736785 lmp.py:1935]   Expert 40 |    146 | CPU
DEBUG 01-15 16:09:08.736289.736289 lmp.py:1935]   Expert  4 |    148 | CPU
DEBUG 01-15 16:09:08.736031.736031 lmp.py:1935]   Expert 12 |    149 | CPU
DEBUG 01-15 16:09:08.736489.736489 lmp.py:1935]   Expert 46 |    150 | CPU
DEBUG 01-15 16:09:08.736609.736609 lmp.py:1935]   Expert  2 |    151 | CPU
DEBUG 01-15 16:09:08.736298.736298 lmp.py:1935]   Expert 48 |    151 | CPU
DEBUG 01-15 16:09:08.736133.736133 lmp.py:1935]   Expert 20 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736730.736730 lmp.py:1935]   Expert 61 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736565.736565 lmp.py:1935]   Expert 33 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736446.736446 lmp.py:1935]   Expert 56 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736851.736851 lmp.py:1935]   Expert 35 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736255.736255 lmp.py:1935]   Expert 10 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736898.736898 lmp.py:1935]   Expert 51 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736303.736303 lmp.py:1935]   Expert 55 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736469.736469 lmp.py:1935]   Expert 36 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736397.736397 lmp.py:1935]   Expert  8 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736325.736325 lmp.py:1935]   Expert 52 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736743.736743 lmp.py:1935]   Expert 37 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736816.736816 lmp.py:1935]   Expert  0 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736698.736698 lmp.py:1935]   Expert 57 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736579.736579 lmp.py:1935]   Expert 39 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736460.736460 lmp.py:1935]   Expert 25 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736057.736057 lmp.py:1935]   Expert 62 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736177.736177 lmp.py:1935]   Expert 38 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736820.736820 lmp.py:1935]   Expert  7 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736986.736986 lmp.py:1935]   Expert  3 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736152.736152 lmp.py:1935]   Expert 24 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736325.736325 lmp.py:1935]   Expert 27 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736922.736922 lmp.py:1935]   Expert 28 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736280.736280 lmp.py:1935]   Expert 60 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736400.736400 lmp.py:1935]   Expert 49 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736281.736281 lmp.py:1935]   Expert 21 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736924.736924 lmp.py:1935]   Expert 16 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736442.736442 lmp.py:1935]   Expert 43 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736323.736323 lmp.py:1935]   Expert 23 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736204.736204 lmp.py:1935]   Expert 29 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736040.736040 lmp.py:1935]   Expert 15 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736159.736159 lmp.py:1935]   Expert 22 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736564.736564 lmp.py:1935]   Expert 47 |    293 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736207.736207 lmp.py:1935]   Expert 41 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736611.736611 lmp.py:1935]   Expert 44 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736016.736016 lmp.py:1935]   Expert 54 |    352 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736421.736421 lmp.py:1935]   Expert 14 |    375 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736825.736825 lmp.py:1935]   Expert 17 |    408 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.736706.736706 lmp.py:1935]   Expert 45 |    453 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.736919.736919 lmp.py:1937] 
DEBUG 01-15 16:09:08.736919.736919 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.736085.736085 lmp.py:1938]   CPU:   2748 tokens
DEBUG 01-15 16:09:08.736728.736728 lmp.py:1942]   cuda:1:   4846 tokens (20 experts)
DEBUG 01-15 16:09:08.736894.736894 lmp.py:1942]   cuda:2:   4694 tokens (19 experts)
DEBUG 01-15 16:09:08.736060.736060 lmp.py:1943]   Total GPU:   9540 tokens
DEBUG 01-15 16:09:08.737180.737180 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.737180.737180 lmp.py:1944] 
DEBUG 01-15 16:09:08.737737.737737 cuda_h.py:19] end experts_map_get cost 0.0019326210021972656 seconds
DEBUG 01-15 16:09:08.737494.737494 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.737774.737774 lmp.py:1953] 
DEBUG 01-15 16:09:08.737774.737774 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.737272.737272 cuda_h.py:19] end cpu_experts_submit cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:08.737538.737538 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.737089.737089 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.738436.738436 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.21.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.21.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.21.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.22.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.22.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.22.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.23.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.23.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.23.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.38.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.38.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.38.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.41.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.41.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.41.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.45.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.45.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.45.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.51.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.51.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.51.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.54.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.54.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.54.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.13.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.13.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.13.mlp.experts.62.up_proj.weight': 340262912}, 2: {'model.layers.13.mlp.experts.3.gate_proj.weight': 0, 'model.layers.13.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.39.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.39.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.39.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.47.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.47.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.47.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.60.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16286482432, 5767168, 17301504, 0), (16292249600, 5767168, 23068672, 0), (16280715264, 5767168, 28835840, 0), (16511401984, 5767168, 34603008, 0), (16517169152, 5767168, 40370176, 0), (16505634816, 5767168, 46137344, 0), (16528703488, 5767168, 51904512, 0), (16534470656, 5767168, 57671680, 0), (16522936320, 5767168, 63438848, 0), (16546004992, 5767168, 69206016, 0), (16551772160, 5767168, 74973184, 0), (16540237824, 5767168, 80740352, 0), (16563306496, 5767168, 86507520, 0), (16569073664, 5767168, 92274688, 0), (16557539328, 5767168, 98041856, 0), (16615211008, 5767168, 103809024, 0), (16620978176, 5767168, 109576192, 0), (16609443840, 5767168, 115343360, 0), (16649814016, 5767168, 121110528, 0), (16655581184, 5767168, 126877696, 0), (16644046848, 5767168, 132644864, 0), (16719020032, 5767168, 138412032, 0), (16724787200, 5767168, 144179200, 0), (16713252864, 5767168, 149946368, 0), (16805527552, 5767168, 155713536, 0), (16811294720, 5767168, 161480704, 0), (16799760384, 5767168, 167247872, 0), (16857432064, 5767168, 173015040, 0), (16863199232, 5767168, 178782208, 0), (16851664896, 5767168, 184549376, 0), (16926638080, 5767168, 190316544, 0), (16932405248, 5767168, 196083712, 0), (16920870912, 5767168, 201850880, 0), (16995844096, 5767168, 207618048, 0), (17001611264, 5767168, 213385216, 0), (16990076928, 5767168, 219152384, 0), (17030447104, 5767168, 224919552, 0), (17036214272, 5767168, 230686720, 0), (17024679936, 5767168, 236453888, 0), (17082351616, 5767168, 242221056, 0), (17088118784, 5767168, 247988224, 0), (17076584448, 5767168, 253755392, 0), (17099653120, 5767168, 259522560, 0), (17105420288, 5767168, 265289728, 0), (17093885952, 5767168, 271056896, 0), (17116954624, 5767168, 276824064, 0), (17122721792, 5767168, 282591232, 0), (17111187456, 5767168, 288358400, 0), (17134256128, 5767168, 294125568, 0), (17140023296, 5767168, 299892736, 0), (17128488960, 5767168, 305659904, 0), (17203462144, 5767168, 311427072, 0), (17209229312, 5767168, 317194240, 0), (17197694976, 5767168, 322961408, 0), (17220763648, 5767168, 328728576, 0), (17226530816, 5767168, 334495744, 0), (17214996480, 5767168, 340262912, 0)], 2: [(16199974912, 5767168, 0, 0), (16205742080, 5767168, 5767168, 0), (16194207744, 5767168, 11534336, 0), (16269180928, 5767168, 17301504, 0), (16274948096, 5767168, 23068672, 0), (16263413760, 5767168, 28835840, 0), (16321085440, 5767168, 34603008, 0), (16326852608, 5767168, 40370176, 0), (16315318272, 5767168, 46137344, 0), (16390291456, 5767168, 51904512, 0), (16396058624, 5767168, 57671680, 0), (16384524288, 5767168, 63438848, 0), (16407592960, 5767168, 69206016, 0), (16413360128, 5767168, 74973184, 0), (16401825792, 5767168, 80740352, 0), (16424894464, 5767168, 86507520, 0), (16430661632, 5767168, 92274688, 0), (16419127296, 5767168, 98041856, 0), (16442195968, 5767168, 103809024, 0), (16447963136, 5767168, 109576192, 0), (16436428800, 5767168, 115343360, 0), (16494100480, 5767168, 121110528, 0), (16499867648, 5767168, 126877696, 0), (16488333312, 5767168, 132644864, 0), (16580608000, 5767168, 138412032, 0), (16586375168, 5767168, 144179200, 0), (16574840832, 5767168, 149946368, 0), (16632512512, 5767168, 155713536, 0), (16638279680, 5767168, 161480704, 0), (16626745344, 5767168, 167247872, 0), (16753623040, 5767168, 173015040, 0), (16759390208, 5767168, 178782208, 0), (16747855872, 5767168, 184549376, 0), (16770924544, 5767168, 190316544, 0), (16776691712, 5767168, 196083712, 0), (16765157376, 5767168, 201850880, 0), (16788226048, 5767168, 207618048, 0), (16793993216, 5767168, 213385216, 0), (16782458880, 5767168, 219152384, 0), (16822829056, 5767168, 224919552, 0), (16828596224, 5767168, 230686720, 0), (16817061888, 5767168, 236453888, 0), (16892035072, 5767168, 242221056, 0), (16897802240, 5767168, 247988224, 0), (16886267904, 5767168, 253755392, 0), (16909336576, 5767168, 259522560, 0), (16915103744, 5767168, 265289728, 0), (16903569408, 5767168, 271056896, 0), (16961241088, 5767168, 276824064, 0), (16967008256, 5767168, 282591232, 0), (16955473920, 5767168, 288358400, 0), (17047748608, 5767168, 294125568, 0), (17053515776, 5767168, 299892736, 0), (17041981440, 5767168, 305659904, 0), (17186160640, 5767168, 311427072, 0), (17191927808, 5767168, 317194240, 0), (17180393472, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6883d3f30>, 2: <capsule object NULL at 0x74a6bc27be10>}
DEBUG 01-15 16:09:08.738064.738064 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.738940.738940 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.738643.738643 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 77d4389c-efc4-4520-971e-a407c3d8af0a
DEBUG 01-15 16:09:08.738458.738458 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.739842.739842 client.py:127] Model loaded
DEBUG 01-15 16:09:08.739969.739969 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.739430.739430 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.739353.739353 cuda_h.py:19] end restore2model cost 0.0003528594970703125 seconds
DEBUG 01-15 16:09:08.739838.739838 cuda_h.py:19] end sllm_worker_task cost 0.010737419128417969 seconds
DEBUG 01-15 16:09:08.739618.739618 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:08.740229.740229 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 77d4389c-efc4-4520-971e-a407c3d8af0a
DEBUG 01-15 16:09:08.740141.740141 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0033049583435058594 seconds
DEBUG 01-15 16:09:08.740509.740509 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.740449.740449 cuda_h.py:19] end move_flatidxs cost 0.000835418701171875 seconds
DEBUG 01-15 16:09:08.740703.740703 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:08.743808.743808 cuda_h.py:19] end restore2model cost 0.003171205520629883 seconds
DEBUG 01-15 16:09:08.743738.743738 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006733417510986328 seconds
DEBUG 01-15 16:09:08.744203.744203 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.744571.744571 cuda_h.py:19] end gpu_sexperts cost 0.0002758502960205078 seconds
DEBUG 01-15 16:09:08.744208.744208 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.744514.744514 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.3126602172851562e-05 seconds
DEBUG 01-15 16:09:08.744687.744687 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.744105.744105 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.745868.745868 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009870529174804688 seconds
DEBUG 01-15 16:09:08.745618.745618 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.745448.745448 cuda_h.py:19] end gpu_group_list cost 0.00023317337036132812 seconds
DEBUG 01-15 16:09:08.746834.746834 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.747169.747169 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010263919830322266 seconds
DEBUG 01-15 16:09:08.747416.747416 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.748716.748716 cuda_h.py:19] end gpu_group_list cost 0.00021886825561523438 seconds
DEBUG 01-15 16:09:08.748362.748362 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.748529.748529 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 77d4389c-efc4-4520-971e-a407c3d8af0a
DEBUG 01-15 16:09:08.749703.749703 cuda_h.py:19] end group_tensors cost 0.008671045303344727 seconds
DEBUG 01-15 16:09:08.750404.750404 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.753123.753123 cuda_h.py:19] end group pad cost 0.003038644790649414 seconds
DEBUG 01-15 16:09:08.753350.753350 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:08.781899.781899 cuda_h.py:19] end group_einsum cost 0.027774572372436523 seconds
DEBUG 01-15 16:09:08.781063.781063 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:08.782033.782033 client.py:127] Model loaded
DEBUG 01-15 16:09:08.782773.782773 cuda_h.py:19] end wait_experts_multi_device cost 0.03338313102722168 seconds
DEBUG 01-15 16:09:08.782682.782682 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.784766.784766 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029015541076660156 seconds
DEBUG 01-15 16:09:08.785601.785601 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04539084434509277 seconds
DEBUG 01-15 16:09:08.785254.785254 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0031888484954833984 seconds
DEBUG 01-15 16:09:08.785633.785633 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.786395.786395 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.786876.786876 cuda_h.py:19] end index_scatter cost 7.176399230957031e-05 seconds
DEBUG 01-15 16:09:08.786653.786653 cuda_h.py:19] end cpuoutputsdeal cost 0.0013475418090820312 seconds
DEBUG 01-15 16:09:08.787086.787086 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.787226.787226 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.787418.787418 cuda_h.py:19] end gpu_group_tensor cost 0.00014662742614746094 seconds
DEBUG 01-15 16:09:08.787796.787796 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.787106.787106 cuda_h.py:19] end gpu_group_tensor cost 0.00012969970703125 seconds
DEBUG 01-15 16:09:08.787858.787858 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.788872.788872 cuda_h.py:19] end gpu_group_einsum cost 0.0005588531494140625 seconds
DEBUG 01-15 16:09:08.788870.788870 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.788752.788752 cuda_h.py:19] end gpu_group_einsum cost 0.000591278076171875 seconds
DEBUG 01-15 16:09:08.789220.789220 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.789191.789191 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.789571.789571 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002338886260986328 seconds
DEBUG 01-15 16:09:08.789387.789387 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.789662.789662 cuda_h.py:19] end concat_expert_out cost 6.008148193359375e-05 seconds
DEBUG 01-15 16:09:08.789313.789313 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.789442.789442 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:09:08.789231.789231 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008661746978759766 seconds
DEBUG 01-15 16:09:08.790592.790592 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.790819.790819 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.790216.790216 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015592575073242188 seconds
DEBUG 01-15 16:09:08.790542.790542 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.790988.790988 cuda_h.py:19] end concat_expert_out cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:09:08.790824.790824 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.790218.790218 cuda_h.py:19] end index_scatter cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:09:08.790073.790073 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004963874816894531 seconds
DEBUG 01-15 16:09:08.790506.790506 cuda_h.py:19] end gpu_experts_multi_device cost 0.046193599700927734 seconds
DEBUG 01-15 16:09:08.790985.790985 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.05646800994873047 seconds
DEBUG 01-15 16:09:08.791026.791026 cuda_h.py:19] end prefill_layer cost 0.06248164176940918 seconds
DEBUG 01-15 16:09:08.791524.791524 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:09:08.791989.791989 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.791930.791930 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:09:08.791540.791540 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:09:08.791343.791343 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:09:08.791762.791762 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 3.695487976074219e-05 seconds
DEBUG 01-15 16:09:08.791777.791777 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 8.487701416015625e-05 seconds
DEBUG 01-15 16:09:08.791380.791380 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.791562.791562 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.791927.791927 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.791022.791022 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.792075.792075 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.792133.792133 cuda_h.py:19] end allocate_cuda_memory cost 0.0004220008850097656 seconds
DEBUG 01-15 16:09:08.792311.792311 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.792029.792029 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.792100.792100 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.793215.793215 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 60fc5841-8612-4fb2-9929-1c1b1a4e8842
DEBUG 01-15 16:09:08.793454.793454 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.793825.793825 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.794531.794531 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 60fc5841-8612-4fb2-9929-1c1b1a4e8842
DEBUG 01-15 16:09:08.794668.794668 cuda_h.py:19] end load_into_gpu_async cost 0.0020492076873779297 seconds
DEBUG 01-15 16:09:08.794160.794160 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.795274.795274 cuda_h.py:19] end restore_tensors2 cost 0.00014066696166992188 seconds
DEBUG 01-15 16:09:08.795681.795681 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0032951831817626953 seconds
INFO 01-15 16:09:08.795407.795407 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 60fc5841-8612-4fb2-9929-1c1b1a4e8842
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.797056.797056 cuda_h.py:19] end self_attn cost 0.004060029983520508 seconds
DEBUG 01-15 16:09:08.797650.797650 cuda_h.py:19] end iln_self_attn_paln cost 0.006434917449951172 seconds
DEBUG 01-15 16:09:08.797433.797433 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:09:08.798719.798719 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.798353.798353 cuda_h.py:19] end gate cost 0.0006761550903320312 seconds
DEBUG 01-15 16:09:08.798851.798851 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.799313.799313 lmp.py:1912] 
DEBUG 01-15 16:09:08.799313.799313 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.799930.799930 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.799725.799725 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.799991.799991 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.799826.799826 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.799184.799184 lmp.py:1917] 
DEBUG 01-15 16:09:08.799184.799184 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.799702.799702 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.799020.799020 lmp.py:1935]   Expert 34 |     27 | CPU
DEBUG 01-15 16:09:08.799094.799094 lmp.py:1935]   Expert  7 |     32 | CPU
DEBUG 01-15 16:09:08.799498.799498 lmp.py:1935]   Expert 13 |     41 | CPU
DEBUG 01-15 16:09:08.799141.799141 lmp.py:1935]   Expert 54 |     77 | CPU
DEBUG 01-15 16:09:08.799069.799069 lmp.py:1935]   Expert 18 |     85 | CPU
DEBUG 01-15 16:09:08.799997.799997 lmp.py:1935]   Expert 39 |     86 | CPU
DEBUG 01-15 16:09:08.799925.799925 lmp.py:1935]   Expert 49 |     87 | CPU
DEBUG 01-15 16:09:08.799806.799806 lmp.py:1935]   Expert 59 |    102 | CPU
DEBUG 01-15 16:09:08.799495.799495 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:09:08.799900.799900 lmp.py:1935]   Expert 21 |    107 | CPU
DEBUG 01-15 16:09:08.799828.799828 lmp.py:1935]   Expert  0 |    109 | CPU
DEBUG 01-15 16:09:08.799663.799663 lmp.py:1935]   Expert 41 |    118 | CPU
DEBUG 01-15 16:09:08.799021.799021 lmp.py:1935]   Expert 15 |    120 | CPU
DEBUG 01-15 16:09:08.799379.799379 lmp.py:1935]   Expert 22 |    121 | CPU
DEBUG 01-15 16:09:08.799022.799022 lmp.py:1935]   Expert 45 |    121 | CPU
DEBUG 01-15 16:09:08.799003.799003 lmp.py:1935]   Expert 17 |    125 | CPU
DEBUG 01-15 16:09:08.799381.799381 lmp.py:1935]   Expert 61 |    134 | CPU
DEBUG 01-15 16:09:08.799548.799548 lmp.py:1935]   Expert  8 |    135 | CPU
DEBUG 01-15 16:09:08.799714.799714 lmp.py:1935]   Expert 52 |    136 | CPU
DEBUG 01-15 16:09:08.799118.799118 lmp.py:1935]   Expert 35 |    138 | CPU
DEBUG 01-15 16:09:08.799284.799284 lmp.py:1935]   Expert 38 |    139 | CPU
DEBUG 01-15 16:09:08.799451.799451 lmp.py:1935]   Expert 12 |    145 | CPU
DEBUG 01-15 16:09:08.799094.799094 lmp.py:1935]   Expert 48 |    147 | CPU
DEBUG 01-15 16:09:08.799690.799690 lmp.py:1935]   Expert 31 |    151 | CPU
DEBUG 01-15 16:09:08.799525.799525 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:09:08.799076.799076 lmp.py:1935]   Expert 53 |    156 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.799864.799864 lmp.py:1935]   Expert 50 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.799461.799461 lmp.py:1935]   Expert 60 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.799342.799342 lmp.py:1935]   Expert 40 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.799462.799462 lmp.py:1935]   Expert 27 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.799344.799344 lmp.py:1935]   Expert 19 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.799463.799463 lmp.py:1935]   Expert  4 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.799822.799822 lmp.py:1935]   Expert 29 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.799180.799180 lmp.py:1935]   Expert 30 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.799061.799061 lmp.py:1935]   Expert 11 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.799658.799658 lmp.py:1935]   Expert 26 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.799877.799877 lmp.py:1935]   Expert 20 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.799666.799666 lmp.py:1935]   Expert 57 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.799216.799216 lmp.py:1935]   Expert  6 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800767.800767 lmp.py:1935]   Expert 46 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800078.800078 lmp.py:1935]   Expert 43 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800675.800675 lmp.py:1935]   Expert 23 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800318.800318 lmp.py:1935]   Expert  2 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800915.800915 lmp.py:1935]   Expert 33 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800796.800796 lmp.py:1935]   Expert 42 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800916.800916 lmp.py:1935]   Expert 55 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800036.800036 lmp.py:1935]   Expert 32 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800632.800632 lmp.py:1935]   Expert 56 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800275.800275 lmp.py:1935]   Expert  9 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800395.800395 lmp.py:1935]   Expert  3 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800515.800515 lmp.py:1935]   Expert 14 |    263 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800165.800165 lmp.py:1935]   Expert 28 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800046.800046 lmp.py:1935]   Expert 51 |    275 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800120.800120 lmp.py:1935]   Expert  1 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800001.800001 lmp.py:1935]   Expert 58 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800598.800598 lmp.py:1935]   Expert 44 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800717.800717 lmp.py:1935]   Expert 37 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800314.800314 lmp.py:1935]   Expert 47 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800957.800957 lmp.py:1935]   Expert 63 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800315.800315 lmp.py:1935]   Expert 62 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800349.800349 lmp.py:1935]   Expert 24 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800707.800707 lmp.py:1935]   Expert 10 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800781.800781 lmp.py:1935]   Expert 25 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.800378.800378 lmp.py:1935]   Expert  5 |    367 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.800259.800259 lmp.py:1937] 
DEBUG 01-15 16:09:08.800259.800259 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.800147.800147 lmp.py:1938]   CPU:   2742 tokens
DEBUG 01-15 16:09:08.800221.800221 lmp.py:1942]   cuda:1:   4847 tokens (20 experts)
DEBUG 01-15 16:09:08.800009.800009 lmp.py:1942]   cuda:2:   4699 tokens (19 experts)
DEBUG 01-15 16:09:08.800414.800414 lmp.py:1943]   Total GPU:   9546 tokens
DEBUG 01-15 16:09:08.800534.800534 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.800534.800534 lmp.py:1944] 
DEBUG 01-15 16:09:08.800899.800899 cuda_h.py:19] end experts_map_get cost 0.0017964839935302734 seconds
DEBUG 01-15 16:09:08.800033.800033 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.800882.800882 lmp.py:1953] 
DEBUG 01-15 16:09:08.800882.800882 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.800380.800380 cuda_h.py:19] end cpu_experts_submit cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:08.800315.800315 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.800297.800297 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.801881.801881 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.2.gate_proj.weight': 0, 'model.layers.14.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.14.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.14.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.14.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.14.mlp.experts.1.gate_proj.weight': 0, 'model.layers.14.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(17289969664, 5767168, 0, 0), (17295736832, 5767168, 5767168, 0), (17284202496, 5767168, 11534336, 0), (17307271168, 5767168, 17301504, 0), (17313038336, 5767168, 23068672, 0), (17301504000, 5767168, 28835840, 0), (17324572672, 5767168, 34603008, 0), (17330339840, 5767168, 40370176, 0), (17318805504, 5767168, 46137344, 0), (17341874176, 5767168, 51904512, 0), (17347641344, 5767168, 57671680, 0), (17336107008, 5767168, 63438848, 0), (17411080192, 5767168, 69206016, 0), (17416847360, 5767168, 74973184, 0), (17405313024, 5767168, 80740352, 0), (17601396736, 5767168, 86507520, 0), (17607163904, 5767168, 92274688, 0), (17595629568, 5767168, 98041856, 0), (17670602752, 5767168, 103809024, 0), (17676369920, 5767168, 109576192, 0), (17664835584, 5767168, 115343360, 0), (17705205760, 5767168, 121110528, 0), (17710972928, 5767168, 126877696, 0), (17699438592, 5767168, 132644864, 0), (17757110272, 5767168, 138412032, 0), (17762877440, 5767168, 144179200, 0), (17751343104, 5767168, 149946368, 0), (17895522304, 5767168, 155713536, 0), (17901289472, 5767168, 161480704, 0), (17889755136, 5767168, 167247872, 0), (17947426816, 5767168, 173015040, 0), (17953193984, 5767168, 178782208, 0), (17941659648, 5767168, 184549376, 0), (17982029824, 5767168, 190316544, 0), (17987796992, 5767168, 196083712, 0), (17976262656, 5767168, 201850880, 0), (17999331328, 5767168, 207618048, 0), (18005098496, 5767168, 213385216, 0), (17993564160, 5767168, 219152384, 0), (18051235840, 5767168, 224919552, 0), (18057003008, 5767168, 230686720, 0), (18045468672, 5767168, 236453888, 0), (18137743360, 5767168, 242221056, 0), (18143510528, 5767168, 247988224, 0), (18131976192, 5767168, 253755392, 0), (18172346368, 5767168, 259522560, 0), (18178113536, 5767168, 265289728, 0), (18166579200, 5767168, 271056896, 0), (18224250880, 5767168, 276824064, 0), (18230018048, 5767168, 282591232, 0), (18218483712, 5767168, 288358400, 0), (18258853888, 5767168, 294125568, 0), (18264621056, 5767168, 299892736, 0), (18253086720, 5767168, 305659904, 0), (18293456896, 5767168, 311427072, 0), (18299224064, 5767168, 317194240, 0), (18287689728, 5767168, 322961408, 0), (18345361408, 5767168, 328728576, 0), (18351128576, 5767168, 334495744, 0), (18339594240, 5767168, 340262912, 0)], 2: [(17272668160, 5767168, 0, 0), (17278435328, 5767168, 5767168, 0), (17266900992, 5767168, 11534336, 0), (17359175680, 5767168, 17301504, 0), (17364942848, 5767168, 23068672, 0), (17353408512, 5767168, 28835840, 0), (17428381696, 5767168, 34603008, 0), (17434148864, 5767168, 40370176, 0), (17422614528, 5767168, 46137344, 0), (17445683200, 5767168, 51904512, 0), (17451450368, 5767168, 57671680, 0), (17439916032, 5767168, 63438848, 0), (17497587712, 5767168, 69206016, 0), (17503354880, 5767168, 74973184, 0), (17491820544, 5767168, 80740352, 0), (17584095232, 5767168, 86507520, 0), (17589862400, 5767168, 92274688, 0), (17578328064, 5767168, 98041856, 0), (17653301248, 5767168, 103809024, 0), (17659068416, 5767168, 109576192, 0), (17647534080, 5767168, 115343360, 0), (17687904256, 5767168, 121110528, 0), (17693671424, 5767168, 126877696, 0), (17682137088, 5767168, 132644864, 0), (17722507264, 5767168, 138412032, 0), (17728274432, 5767168, 144179200, 0), (17716740096, 5767168, 149946368, 0), (17739808768, 5767168, 155713536, 0), (17745575936, 5767168, 161480704, 0), (17734041600, 5767168, 167247872, 0), (17774411776, 5767168, 173015040, 0), (17780178944, 5767168, 178782208, 0), (17768644608, 5767168, 184549376, 0), (17809014784, 5767168, 190316544, 0), (17814781952, 5767168, 196083712, 0), (17803247616, 5767168, 201850880, 0), (17826316288, 5767168, 207618048, 0), (17832083456, 5767168, 213385216, 0), (17820549120, 5767168, 219152384, 0), (18016632832, 5767168, 224919552, 0), (18022400000, 5767168, 230686720, 0), (18010865664, 5767168, 236453888, 0), (18068537344, 5767168, 242221056, 0), (18074304512, 5767168, 247988224, 0), (18062770176, 5767168, 253755392, 0), (18120441856, 5767168, 259522560, 0), (18126209024, 5767168, 265289728, 0), (18114674688, 5767168, 271056896, 0), (18206949376, 5767168, 276824064, 0), (18212716544, 5767168, 282591232, 0), (18201182208, 5767168, 288358400, 0), (18241552384, 5767168, 294125568, 0), (18247319552, 5767168, 299892736, 0), (18235785216, 5767168, 305659904, 0), (18328059904, 5767168, 311427072, 0), (18333827072, 5767168, 317194240, 0), (18322292736, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6883d3b10>, 2: <capsule object NULL at 0x74a6807a9dd0>}
DEBUG 01-15 16:09:08.801755.801755 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.801128.801128 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.801738.801738 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6ea8bf3a-336d-444d-9834-b750a7820144
DEBUG 01-15 16:09:08.802699.802699 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.802801.802801 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:08.802852.802852 client.py:127] Model loaded
DEBUG 01-15 16:09:08.802260.802260 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.802684.802684 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.803468.803468 cuda_h.py:19] end move_flatidxs cost 0.0008327960968017578 seconds
DEBUG 01-15 16:09:08.803437.803437 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:08.803581.803581 cuda_h.py:19] end restore2model cost 0.0009915828704833984 seconds
INFO 01-15 16:09:08.803327.803327 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6ea8bf3a-336d-444d-9834-b750a7820144
DEBUG 01-15 16:09:08.803648.803648 cuda_h.py:19] end sllm_worker_task cost 0.01198720932006836 seconds
DEBUG 01-15 16:09:08.804844.804844 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034575462341308594 seconds
DEBUG 01-15 16:09:08.804665.804665 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.807917.807917 cuda_h.py:19] end restore2model cost 0.003134012222290039 seconds
DEBUG 01-15 16:09:08.807091.807091 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006938934326171875 seconds
DEBUG 01-15 16:09:08.807430.807430 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.808460.808460 cuda_h.py:19] end gpu_sexperts cost 0.0002727508544921875 seconds
DEBUG 01-15 16:09:08.808098.808098 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.808967.808967 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:09:08.808947.808947 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.808889.808889 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.807634.807634 cuda_h.py:19] end group_tensors cost 0.004461050033569336 seconds
DEBUG 01-15 16:09:08.808320.808320 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.809539.809539 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011775493621826172 seconds
DEBUG 01-15 16:09:08.809906.809906 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.809136.809136 cuda_h.py:19] end gpu_group_list cost 0.0003094673156738281 seconds
DEBUG 01-15 16:09:08.811677.811677 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.811685.811685 cuda_h.py:19] end group pad cost 0.0031304359436035156 seconds
DEBUG 01-15 16:09:08.811237.811237 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:08.815957.815957 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.00395965576171875 seconds
DEBUG 01-15 16:09:08.816610.816610 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.817989.817989 cuda_h.py:19] end gpu_group_list cost 0.0009984970092773438 seconds
DEBUG 01-15 16:09:08.821940.821940 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.821579.821579 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6ea8bf3a-336d-444d-9834-b750a7820144
DEBUG 01-15 16:09:08.839429.839429 cuda_h.py:19] end group_einsum cost 0.027231931686401367 seconds
DEBUG 01-15 16:09:08.839957.839957 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.842254.842254 cuda_h.py:19] end get_outputs_cpu1 cost 0.0028743743896484375 seconds
DEBUG 01-15 16:09:08.842751.842751 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04058551788330078 seconds
INFO 01-15 16:09:08.847154.847154 client.py:127] Model loaded
DEBUG 01-15 16:09:08.847317.847317 cuda_h.py:19] end wait_experts_multi_device cost 0.02560710906982422 seconds
DEBUG 01-15 16:09:08.847318.847318 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.847420.847420 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00045752525329589844 seconds
DEBUG 01-15 16:09:08.847548.847548 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.848072.848072 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.849515.849515 cuda_h.py:19] end index_scatter cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:09:08.849537.849537 cuda_h.py:19] end cpuoutputsdeal cost 0.0013659000396728516 seconds
DEBUG 01-15 16:09:08.849122.849122 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.849454.849454 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.849150.849150 cuda_h.py:19] end gpu_group_tensor cost 0.00016641616821289062 seconds
DEBUG 01-15 16:09:08.849528.849528 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.849269.849269 cuda_h.py:19] end gpu_group_tensor cost 0.00013113021850585938 seconds
DEBUG 01-15 16:09:08.849497.849497 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.850301.850301 cuda_h.py:19] end gpu_group_einsum cost 0.0006117820739746094 seconds
DEBUG 01-15 16:09:08.850491.850491 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.851678.851678 cuda_h.py:19] end gpu_group_einsum cost 0.00039649009704589844 seconds
DEBUG 01-15 16:09:08.851166.851166 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.851056.851056 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.851302.851302 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018024444580078125 seconds
DEBUG 01-15 16:09:08.851581.851581 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.851067.851067 cuda_h.py:19] end concat_expert_out cost 4.696846008300781e-05 seconds
DEBUG 01-15 16:09:08.851334.851334 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.851158.851158 cuda_h.py:19] end index_scatter cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:08.852616.852616 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007419586181640625 seconds
DEBUG 01-15 16:09:08.852169.852169 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.852681.852681 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.852271.852271 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001647472381591797 seconds
DEBUG 01-15 16:09:08.852457.852457 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.852811.852811 cuda_h.py:19] end concat_expert_out cost 5.507469177246094e-05 seconds
DEBUG 01-15 16:09:08.852316.852316 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.852332.852332 cuda_h.py:19] end index_scatter cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:09:08.852711.852711 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005407333374023438 seconds
DEBUG 01-15 16:09:08.852667.852667 cuda_h.py:19] end gpu_experts_multi_device cost 0.044609785079956055 seconds
DEBUG 01-15 16:09:08.852199.852199 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.05488705635070801 seconds
DEBUG 01-15 16:09:08.853300.853300 cuda_h.py:19] end prefill_layer cost 0.062065839767456055 seconds
DEBUG 01-15 16:09:08.853990.853990 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:09:08.853170.853170 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.853065.853065 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:09:08.853437.853437 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:09:08.853193.853193 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:09:08.853758.853758 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 3.719329833984375e-05 seconds
DEBUG 01-15 16:09:08.853322.853322 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 6.914138793945312e-05 seconds
DEBUG 01-15 16:09:08.853018.853018 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.853319.853319 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.853060.853060 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.853685.853685 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.854058.854058 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.854326.854326 cuda_h.py:19] end allocate_cuda_memory cost 0.00039196014404296875 seconds
DEBUG 01-15 16:09:08.854206.854206 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.854077.854077 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.855678.855678 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.855561.855561 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d194359d-c173-445b-af0f-82b8a7ce9013
DEBUG 01-15 16:09:08.855913.855913 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.855510.855510 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.856805.856805 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d194359d-c173-445b-af0f-82b8a7ce9013
DEBUG 01-15 16:09:08.856300.856300 cuda_h.py:19] end load_into_gpu_async cost 0.0017049312591552734 seconds
DEBUG 01-15 16:09:08.856681.856681 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.857537.857537 cuda_h.py:19] end restore_tensors2 cost 0.00015091896057128906 seconds
DEBUG 01-15 16:09:08.857302.857302 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0030074119567871094 seconds
INFO 01-15 16:09:08.857631.857631 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d194359d-c173-445b-af0f-82b8a7ce9013
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.859545.859545 cuda_h.py:19] end self_attn cost 0.0037496089935302734 seconds
DEBUG 01-15 16:09:08.859661.859661 cuda_h.py:19] end iln_self_attn_paln cost 0.006130695343017578 seconds
DEBUG 01-15 16:09:08.859868.859868 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:09:08.859147.859147 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.860634.860634 cuda_h.py:19] end gate cost 0.0006420612335205078 seconds
DEBUG 01-15 16:09:08.860509.860509 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.860374.860374 lmp.py:1912] 
DEBUG 01-15 16:09:08.860374.860374 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.860468.860468 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.860125.860125 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.860582.860582 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.860610.860610 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.861968.861968 lmp.py:1917] 
DEBUG 01-15 16:09:08.861968.861968 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.861041.861041 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.861598.861598 lmp.py:1935]   Expert 15 |     66 | CPU
DEBUG 01-15 16:09:08.861434.861434 lmp.py:1935]   Expert 41 |     69 | CPU
DEBUG 01-15 16:09:08.861553.861553 lmp.py:1935]   Expert 63 |     76 | CPU
DEBUG 01-15 16:09:08.861150.861150 lmp.py:1935]   Expert  0 |     77 | CPU
DEBUG 01-15 16:09:08.861031.861031 lmp.py:1935]   Expert 20 |     82 | CPU
DEBUG 01-15 16:09:08.861390.861390 lmp.py:1935]   Expert 45 |     88 | CPU
DEBUG 01-15 16:09:08.861178.861178 lmp.py:1935]   Expert  7 |     95 | CPU
DEBUG 01-15 16:09:08.861298.861298 lmp.py:1935]   Expert 28 |     96 | CPU
DEBUG 01-15 16:09:08.861656.861656 lmp.py:1935]   Expert 54 |    106 | CPU
DEBUG 01-15 16:09:08.861207.861207 lmp.py:1935]   Expert 12 |    109 | CPU
DEBUG 01-15 16:09:08.861241.861241 lmp.py:1935]   Expert 52 |    121 | CPU
DEBUG 01-15 16:09:08.861168.861168 lmp.py:1935]   Expert 40 |    122 | CPU
DEBUG 01-15 16:09:08.861142.861142 lmp.py:1935]   Expert 59 |    123 | CPU
DEBUG 01-15 16:09:08.861878.861878 lmp.py:1935]   Expert  5 |    125 | CPU
DEBUG 01-15 16:09:08.861852.861852 lmp.py:1935]   Expert  4 |    133 | CPU
DEBUG 01-15 16:09:08.861588.861588 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:09:08.861562.861562 lmp.py:1935]   Expert 62 |    133 | CPU
DEBUG 01-15 16:09:08.861297.861297 lmp.py:1935]   Expert 61 |    137 | CPU
DEBUG 01-15 16:09:08.861795.861795 lmp.py:1935]   Expert 55 |    138 | CPU
DEBUG 01-15 16:09:08.861530.861530 lmp.py:1935]   Expert 13 |    139 | CPU
DEBUG 01-15 16:09:08.861504.861504 lmp.py:1935]   Expert 21 |    139 | CPU
DEBUG 01-15 16:09:08.861478.861478 lmp.py:1935]   Expert 42 |    140 | CPU
DEBUG 01-15 16:09:08.861168.861168 lmp.py:1935]   Expert 14 |    145 | CPU
DEBUG 01-15 16:09:08.861572.861572 lmp.py:1935]   Expert 22 |    146 | CPU
DEBUG 01-15 16:09:08.861977.861977 lmp.py:1935]   Expert 10 |    150 | CPU
DEBUG 01-15 16:09:08.861050.861050 lmp.py:1935]   Expert 51 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861885.861885 lmp.py:1935]   Expert 32 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861290.861290 lmp.py:1935]   Expert 25 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861456.861456 lmp.py:1935]   Expert 47 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861145.861145 lmp.py:1935]   Expert  1 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861073.861073 lmp.py:1935]   Expert 26 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861762.861762 lmp.py:1935]   Expert 53 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861227.861227 lmp.py:1935]   Expert 19 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861678.861678 lmp.py:1935]   Expert 50 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861129.861129 lmp.py:1935]   Expert  6 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861772.861772 lmp.py:1935]   Expert 35 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861461.861461 lmp.py:1935]   Expert  2 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861865.861865 lmp.py:1935]   Expert 11 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861555.861555 lmp.py:1935]   Expert 30 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861006.861006 lmp.py:1935]   Expert 56 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861695.861695 lmp.py:1935]   Expert 57 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861146.861146 lmp.py:1935]   Expert 48 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861597.861597 lmp.py:1935]   Expert 16 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861809.861809 lmp.py:1935]   Expert 24 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861260.861260 lmp.py:1935]   Expert 44 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861472.861472 lmp.py:1935]   Expert 46 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861446.861446 lmp.py:1935]   Expert 39 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861659.861659 lmp.py:1935]   Expert 18 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861871.861871 lmp.py:1935]   Expert 29 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861845.861845 lmp.py:1935]   Expert 37 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861535.861535 lmp.py:1935]   Expert 31 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861747.861747 lmp.py:1935]   Expert 36 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861390.861390 lmp.py:1935]   Expert 60 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861080.861080 lmp.py:1935]   Expert  3 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.861722.861722 lmp.py:1935]   Expert 38 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.861511.861511 lmp.py:1935]   Expert  9 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.862962.862962 lmp.py:1935]   Expert 17 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.862413.862413 lmp.py:1935]   Expert 23 |    273 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.862625.862625 lmp.py:1935]   Expert 27 |    346 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.862599.862599 lmp.py:1935]   Expert 43 |    362 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.862812.862812 lmp.py:1935]   Expert 33 |    393 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.862548.862548 lmp.py:1935]   Expert  8 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.862760.862760 lmp.py:1935]   Expert 58 |    444 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.862734.862734 lmp.py:1935]   Expert 49 |    547 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.862993.862993 lmp.py:1937] 
DEBUG 01-15 16:09:08.862993.862993 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.862205.862205 lmp.py:1938]   CPU:   2888 tokens
DEBUG 01-15 16:09:08.862087.862087 lmp.py:1942]   cuda:1:   4629 tokens (19 experts)
DEBUG 01-15 16:09:08.862776.862776 lmp.py:1942]   cuda:2:   4771 tokens (20 experts)
DEBUG 01-15 16:09:08.862465.862465 lmp.py:1943]   Total GPU:   9400 tokens
DEBUG 01-15 16:09:08.862201.862201 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.862201.862201 lmp.py:1944] 
DEBUG 01-15 16:09:08.862612.862612 cuda_h.py:19] end experts_map_get cost 0.0016629695892333984 seconds
DEBUG 01-15 16:09:08.862647.862647 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.862735.862735 lmp.py:1953] 
DEBUG 01-15 16:09:08.862735.862735 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.862802.862802 cuda_h.py:19] end cpu_experts_submit cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:09:08.862188.862188 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.862622.862622 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.863659.863659 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.6.gate_proj.weight': 0, 'model.layers.15.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.11.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.11.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.11.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.17.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.17.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.17.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.24.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.24.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.24.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.27.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.27.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.27.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.29.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.29.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.29.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.30.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.30.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.30.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.57.up_proj.weight': 322961408}, 2: {'model.layers.15.mlp.experts.1.gate_proj.weight': 0, 'model.layers.15.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.26.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.26.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.26.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.15.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.15.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.15.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(18466471936, 5767168, 0, 0), (18472239104, 5767168, 5767168, 0), (18460704768, 5767168, 11534336, 0), (18552979456, 5767168, 17301504, 0), (18558746624, 5767168, 23068672, 0), (18547212288, 5767168, 28835840, 0), (18656788480, 5767168, 34603008, 0), (18662555648, 5767168, 40370176, 0), (18651021312, 5767168, 46137344, 0), (18777899008, 5767168, 51904512, 0), (18783666176, 5767168, 57671680, 0), (18772131840, 5767168, 63438848, 0), (18829803520, 5767168, 69206016, 0), (18835570688, 5767168, 74973184, 0), (18824036352, 5767168, 80740352, 0), (18864406528, 5767168, 86507520, 0), (18870173696, 5767168, 92274688, 0), (18858639360, 5767168, 98041856, 0), (18881708032, 5767168, 103809024, 0), (18887475200, 5767168, 109576192, 0), (18875940864, 5767168, 115343360, 0), (18899009536, 5767168, 121110528, 0), (18904776704, 5767168, 126877696, 0), (18893242368, 5767168, 132644864, 0), (18916311040, 5767168, 138412032, 0), (18922078208, 5767168, 144179200, 0), (18910543872, 5767168, 149946368, 0), (18933612544, 5767168, 155713536, 0), (18939379712, 5767168, 161480704, 0), (18927845376, 5767168, 167247872, 0), (18985517056, 5767168, 173015040, 0), (18991284224, 5767168, 178782208, 0), (18979749888, 5767168, 184549376, 0), (19020120064, 5767168, 190316544, 0), (19025887232, 5767168, 196083712, 0), (19014352896, 5767168, 201850880, 0), (19037421568, 5767168, 207618048, 0), (19043188736, 5767168, 213385216, 0), (19031654400, 5767168, 219152384, 0), (19123929088, 5767168, 224919552, 0), (19129696256, 5767168, 230686720, 0), (19118161920, 5767168, 236453888, 0), (19175833600, 5767168, 242221056, 0), (19181600768, 5767168, 247988224, 0), (19170066432, 5767168, 253755392, 0), (19210436608, 5767168, 259522560, 0), (19216203776, 5767168, 265289728, 0), (19204669440, 5767168, 271056896, 0), (19227738112, 5767168, 276824064, 0), (19233505280, 5767168, 282591232, 0), (19221970944, 5767168, 288358400, 0), (19279642624, 5767168, 294125568, 0), (19285409792, 5767168, 299892736, 0), (19273875456, 5767168, 305659904, 0), (19348848640, 5767168, 311427072, 0), (19354615808, 5767168, 317194240, 0), (19343081472, 5767168, 322961408, 0)], 2: [(18379964416, 5767168, 0, 0), (18385731584, 5767168, 5767168, 0), (18374197248, 5767168, 11534336, 0), (18397265920, 5767168, 17301504, 0), (18403033088, 5767168, 23068672, 0), (18391498752, 5767168, 28835840, 0), (18414567424, 5767168, 34603008, 0), (18420334592, 5767168, 40370176, 0), (18408800256, 5767168, 46137344, 0), (18501074944, 5767168, 51904512, 0), (18506842112, 5767168, 57671680, 0), (18495307776, 5767168, 63438848, 0), (18518376448, 5767168, 69206016, 0), (18524143616, 5767168, 74973184, 0), (18512609280, 5767168, 80740352, 0), (18639486976, 5767168, 86507520, 0), (18645254144, 5767168, 92274688, 0), (18633719808, 5767168, 98041856, 0), (18674089984, 5767168, 103809024, 0), (18679857152, 5767168, 109576192, 0), (18668322816, 5767168, 115343360, 0), (18691391488, 5767168, 121110528, 0), (18697158656, 5767168, 126877696, 0), (18685624320, 5767168, 132644864, 0), (18760597504, 5767168, 138412032, 0), (18766364672, 5767168, 144179200, 0), (18754830336, 5767168, 149946368, 0), (18795200512, 5767168, 155713536, 0), (18800967680, 5767168, 161480704, 0), (18789433344, 5767168, 167247872, 0), (18812502016, 5767168, 173015040, 0), (18818269184, 5767168, 178782208, 0), (18806734848, 5767168, 184549376, 0), (18968215552, 5767168, 190316544, 0), (18973982720, 5767168, 196083712, 0), (18962448384, 5767168, 201850880, 0), (19002818560, 5767168, 207618048, 0), (19008585728, 5767168, 213385216, 0), (18997051392, 5767168, 219152384, 0), (19106627584, 5767168, 224919552, 0), (19112394752, 5767168, 230686720, 0), (19100860416, 5767168, 236453888, 0), (19158532096, 5767168, 242221056, 0), (19164299264, 5767168, 247988224, 0), (19152764928, 5767168, 253755392, 0), (19193135104, 5767168, 259522560, 0), (19198902272, 5767168, 265289728, 0), (19187367936, 5767168, 271056896, 0), (19245039616, 5767168, 276824064, 0), (19250806784, 5767168, 282591232, 0), (19239272448, 5767168, 288358400, 0), (19331547136, 5767168, 294125568, 0), (19337314304, 5767168, 299892736, 0), (19325779968, 5767168, 305659904, 0), (19366150144, 5767168, 311427072, 0), (19371917312, 5767168, 317194240, 0), (19360382976, 5767168, 322961408, 0), (19400753152, 5767168, 328728576, 0), (19406520320, 5767168, 334495744, 0), (19394985984, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a68848bd20>, 2: <capsule object NULL at 0x74a6807aa5e0>}
DEBUG 01-15 16:09:08.863157.863157 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.863507.863507 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.863860.863860 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.863086.863086 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5cd8c6b6-8ed0-46f5-8374-f7f4f24327cc
DEBUG 01-15 16:09:08.864540.864540 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.864748.864748 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.864477.864477 client.py:127] Model loaded
DEBUG 01-15 16:09:08.864242.864242 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.865970.865970 cuda_h.py:19] end move_flatidxs cost 0.0008714199066162109 seconds
DEBUG 01-15 16:09:08.865243.865243 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:08.865084.865084 cuda_h.py:19] end restore2model cost 0.0005600452423095703 seconds
DEBUG 01-15 16:09:08.865841.865841 cuda_h.py:19] end sllm_worker_task cost 0.011198282241821289 seconds
INFO 01-15 16:09:08.865599.865599 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5cd8c6b6-8ed0-46f5-8374-f7f4f24327cc
DEBUG 01-15 16:09:08.865814.865814 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0029113292694091797 seconds
DEBUG 01-15 16:09:08.865691.865691 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.868493.868493 cuda_h.py:19] end restore2model cost 0.0031554698944091797 seconds
DEBUG 01-15 16:09:08.869098.869098 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006318092346191406 seconds
DEBUG 01-15 16:09:08.869132.869132 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.869585.869585 cuda_h.py:19] end gpu_sexperts cost 0.0002689361572265625 seconds
DEBUG 01-15 16:09:08.869984.869984 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.869661.869661 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.3828277587890625e-05 seconds
DEBUG 01-15 16:09:08.869357.869357 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.869014.869014 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.870079.870079 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009305477142333984 seconds
DEBUG 01-15 16:09:08.870684.870684 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.870003.870003 cuda_h.py:19] end gpu_group_list cost 0.0002071857452392578 seconds
DEBUG 01-15 16:09:08.870943.870943 cuda_h.py:19] end group_tensors cost 0.005419015884399414 seconds
DEBUG 01-15 16:09:08.871498.871498 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.871172.871172 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.873549.873549 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016338825225830078 seconds
DEBUG 01-15 16:09:08.873969.873969 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.873485.873485 cuda_h.py:19] end gpu_group_list cost 0.0003399848937988281 seconds
DEBUG 01-15 16:09:08.873647.873647 cuda_h.py:19] end group pad cost 0.0026159286499023438 seconds
DEBUG 01-15 16:09:08.874099.874099 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:08.875957.875957 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.876394.876394 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5cd8c6b6-8ed0-46f5-8374-f7f4f24327cc
DEBUG 01-15 16:09:08.902129.902129 cuda_h.py:19] end group_einsum cost 0.028705358505249023 seconds
DEBUG 01-15 16:09:08.902868.902868 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.905183.905183 cuda_h.py:19] end get_outputs_cpu1 cost 0.0030264854431152344 seconds
DEBUG 01-15 16:09:08.906573.906573 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.042803287506103516 seconds
INFO 01-15 16:09:08.909124.909124 client.py:127] Model loaded
DEBUG 01-15 16:09:08.909240.909240 cuda_h.py:19] end wait_experts_multi_device cost 0.03323030471801758 seconds
DEBUG 01-15 16:09:08.909526.909526 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.910642.910642 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0004684925079345703 seconds
DEBUG 01-15 16:09:08.910678.910678 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.911093.911093 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.911667.911667 cuda_h.py:19] end index_scatter cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:09:08.911384.911384 cuda_h.py:19] end cpuoutputsdeal cost 0.0012831687927246094 seconds
DEBUG 01-15 16:09:08.911009.911009 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.911388.911388 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.911387.911387 cuda_h.py:19] end gpu_group_tensor cost 0.00014591217041015625 seconds
DEBUG 01-15 16:09:08.911766.911766 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.911314.911314 cuda_h.py:19] end gpu_group_tensor cost 0.0001308917999267578 seconds
DEBUG 01-15 16:09:08.911781.911781 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.912942.912942 cuda_h.py:19] end gpu_group_einsum cost 0.0005936622619628906 seconds
DEBUG 01-15 16:09:08.912238.912238 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.913356.913356 cuda_h.py:19] end gpu_group_einsum cost 0.00047898292541503906 seconds
DEBUG 01-15 16:09:08.913539.913539 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.913795.913795 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.913672.913672 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002491474151611328 seconds
DEBUG 01-15 16:09:08.913627.913627 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.914372.914372 cuda_h.py:19] end concat_expert_out cost 5.8650970458984375e-05 seconds
DEBUG 01-15 16:09:08.914592.914592 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.914608.914608 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:08.914960.914960 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008540153503417969 seconds
DEBUG 01-15 16:09:08.914798.914798 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.914256.914256 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.914401.914401 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001468658447265625 seconds
DEBUG 01-15 16:09:08.914250.914250 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.914405.914405 cuda_h.py:19] end concat_expert_out cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:08.914433.914433 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.914257.914257 cuda_h.py:19] end index_scatter cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:08.914636.914636 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004782676696777344 seconds
DEBUG 01-15 16:09:08.915453.915453 cuda_h.py:19] end gpu_experts_multi_device cost 0.04555320739746094 seconds
DEBUG 01-15 16:09:08.915932.915932 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.05528664588928223 seconds
DEBUG 01-15 16:09:08.915410.915410 cuda_h.py:19] end prefill_layer cost 0.062117815017700195 seconds
DEBUG 01-15 16:09:08.915577.915577 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:09:08.915042.915042 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.915698.915698 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:09:08.915163.915163 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:09:08.915250.915250 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:09:08.915908.915908 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 3.647804260253906e-05 seconds
DEBUG 01-15 16:09:08.915948.915948 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:09:08.915837.915837 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.915521.915521 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.916078.916078 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.916988.916988 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.916605.916605 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.916525.916525 cuda_h.py:19] end allocate_cuda_memory cost 0.0004413127899169922 seconds
DEBUG 01-15 16:09:08.917457.917457 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.917806.917806 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.917121.917121 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.917628.917628 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4d1a2662-01f7-4ae1-ae7b-2e870384b092
DEBUG 01-15 16:09:08.917602.917602 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.917662.917662 cuda_h.py:10] start self_attn
INFO 01-15 16:09:08.919617.919617 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4d1a2662-01f7-4ae1-ae7b-2e870384b092
DEBUG 01-15 16:09:08.919106.919106 cuda_h.py:19] end load_into_gpu_async cost 0.0024297237396240234 seconds
DEBUG 01-15 16:09:08.919989.919989 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.920269.920269 cuda_h.py:19] end restore_tensors2 cost 0.0001499652862548828 seconds
DEBUG 01-15 16:09:08.920863.920863 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0037946701049804688 seconds
INFO 01-15 16:09:08.920344.920344 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4d1a2662-01f7-4ae1-ae7b-2e870384b092
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.921050.921050 cuda_h.py:19] end self_attn cost 0.0035686492919921875 seconds
DEBUG 01-15 16:09:08.921119.921119 cuda_h.py:19] end iln_self_attn_paln cost 0.0060405731201171875 seconds
DEBUG 01-15 16:09:08.921803.921803 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:09:08.921274.921274 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.922627.922627 cuda_h.py:19] end gate cost 0.0006139278411865234 seconds
DEBUG 01-15 16:09:08.922549.922549 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.922546.922546 lmp.py:1912] 
DEBUG 01-15 16:09:08.922546.922546 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.923971.923971 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.923860.923860 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.923887.923887 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.923066.923066 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.923186.923186 lmp.py:1917] 
DEBUG 01-15 16:09:08.923186.923186 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.923306.923306 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.923148.923148 lmp.py:1935]   Expert 58 |     35 | CPU
DEBUG 01-15 16:09:08.923268.923268 lmp.py:1935]   Expert 47 |     56 | CPU
DEBUG 01-15 16:09:08.923911.923911 lmp.py:1935]   Expert 31 |     59 | CPU
DEBUG 01-15 16:09:08.923315.923315 lmp.py:1935]   Expert 49 |     61 | CPU
DEBUG 01-15 16:09:08.923912.923912 lmp.py:1935]   Expert  4 |     64 | CPU
DEBUG 01-15 16:09:08.923032.923032 lmp.py:1935]   Expert 38 |     68 | CPU
DEBUG 01-15 16:09:08.923198.923198 lmp.py:1935]   Expert 45 |     72 | CPU
DEBUG 01-15 16:09:08.923364.923364 lmp.py:1935]   Expert 41 |     83 | CPU
DEBUG 01-15 16:09:08.923530.923530 lmp.py:1935]   Expert 43 |     83 | CPU
DEBUG 01-15 16:09:08.923173.923173 lmp.py:1935]   Expert 33 |     94 | CPU
DEBUG 01-15 16:09:08.923624.923624 lmp.py:1935]   Expert 50 |    103 | CPU
DEBUG 01-15 16:09:08.923790.923790 lmp.py:1935]   Expert 57 |    104 | CPU
DEBUG 01-15 16:09:08.923479.923479 lmp.py:1935]   Expert 11 |    110 | CPU
DEBUG 01-15 16:09:08.923646.923646 lmp.py:1935]   Expert  2 |    112 | CPU
DEBUG 01-15 16:09:08.923481.923481 lmp.py:1935]   Expert 51 |    116 | CPU
DEBUG 01-15 16:09:08.923362.923362 lmp.py:1935]   Expert  0 |    122 | CPU
DEBUG 01-15 16:09:08.923767.923767 lmp.py:1935]   Expert 14 |    125 | CPU
DEBUG 01-15 16:09:08.923933.923933 lmp.py:1935]   Expert 54 |    129 | CPU
DEBUG 01-15 16:09:08.923860.923860 lmp.py:1935]   Expert 56 |    138 | CPU
DEBUG 01-15 16:09:08.923788.923788 lmp.py:1935]   Expert 26 |    140 | CPU
DEBUG 01-15 16:09:08.923239.923239 lmp.py:1935]   Expert 34 |    140 | CPU
DEBUG 01-15 16:09:08.923167.923167 lmp.py:1935]   Expert 27 |    151 | CPU
DEBUG 01-15 16:09:08.923856.923856 lmp.py:1935]   Expert 28 |    157 | CPU
DEBUG 01-15 16:09:08.923261.923261 lmp.py:1935]   Expert 55 |    158 | CPU
DEBUG 01-15 16:09:08.923427.923427 lmp.py:1935]   Expert 10 |    162 | CPU
DEBUG 01-15 16:09:08.923977.923977 lmp.py:1935]   Expert 25 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923858.923858 lmp.py:1935]   Expert  9 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923217.923217 lmp.py:1935]   Expert 13 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923337.923337 lmp.py:1935]   Expert 61 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923695.923695 lmp.py:1935]   Expert 48 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923338.923338 lmp.py:1935]   Expert  6 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923696.923696 lmp.py:1935]   Expert  7 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923577.923577 lmp.py:1935]   Expert 24 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923459.923459 lmp.py:1935]   Expert 46 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923340.923340 lmp.py:1935]   Expert 42 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923698.923698 lmp.py:1935]   Expert 18 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923818.923818 lmp.py:1935]   Expert 40 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923938.923938 lmp.py:1935]   Expert 12 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923343.923343 lmp.py:1935]   Expert 63 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923224.923224 lmp.py:1935]   Expert 29 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923628.923628 lmp.py:1935]   Expert 59 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923510.923510 lmp.py:1935]   Expert 21 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923630.923630 lmp.py:1935]   Expert 22 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923511.923511 lmp.py:1935]   Expert 32 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923916.923916 lmp.py:1935]   Expert 19 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923797.923797 lmp.py:1935]   Expert 36 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923678.923678 lmp.py:1935]   Expert  3 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.923514.923514 lmp.py:1935]   Expert  1 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.923395.923395 lmp.py:1935]   Expert 37 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.924276.924276 lmp.py:1935]   Expert 16 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.924681.924681 lmp.py:1935]   Expert 20 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.924085.924085 lmp.py:1935]   Expert  5 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.924490.924490 lmp.py:1935]   Expert  8 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.924133.924133 lmp.py:1935]   Expert 30 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.924299.924299 lmp.py:1935]   Expert 62 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.924180.924180 lmp.py:1935]   Expert 15 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.924539.924539 lmp.py:1935]   Expert 39 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.924135.924135 lmp.py:1935]   Expert 35 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.924931.924931 lmp.py:1935]   Expert 17 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.924196.924196 lmp.py:1935]   Expert 60 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.924601.924601 lmp.py:1935]   Expert 52 |    353 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.924482.924482 lmp.py:1935]   Expert 23 |    366 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.924887.924887 lmp.py:1935]   Expert 44 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.924530.924530 lmp.py:1935]   Expert 53 |    436 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.924219.924219 lmp.py:1937] 
DEBUG 01-15 16:09:08.924219.924219 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.924101.924101 lmp.py:1938]   CPU:   2642 tokens
DEBUG 01-15 16:09:08.924174.924174 lmp.py:1942]   cuda:1:   4904 tokens (20 experts)
DEBUG 01-15 16:09:08.924771.924771 lmp.py:1942]   cuda:2:   4742 tokens (19 experts)
DEBUG 01-15 16:09:08.924698.924698 lmp.py:1943]   Total GPU:   9646 tokens
DEBUG 01-15 16:09:08.924149.924149 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.924149.924149 lmp.py:1944] 
DEBUG 01-15 16:09:08.924336.924336 cuda_h.py:19] end experts_map_get cost 0.0017108917236328125 seconds
DEBUG 01-15 16:09:08.924014.924014 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.924816.924816 lmp.py:1953] 
DEBUG 01-15 16:09:08.924816.924816 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.924792.924792 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:08.924103.924103 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.924509.924509 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:08.927729.927729 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.3.gate_proj.weight': 0, 'model.layers.16.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.30.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.30.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.30.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.32.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.32.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.32.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.37.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.37.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.37.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.39.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.39.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.39.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.42.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.42.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.42.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.48.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.48.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.48.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.52.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.52.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.52.up_proj.weight': 322961408, 'model.layers.16.mlp.experts.53.gate_proj.weight': 328728576, 'model.layers.16.mlp.experts.53.down_proj.weight': 334495744, 'model.layers.16.mlp.experts.53.up_proj.weight': 340262912}, 2: {'model.layers.16.mlp.experts.1.gate_proj.weight': 0, 'model.layers.16.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.60.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.60.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.60.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.61.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.61.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.61.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(19521863680, 5767168, 0, 0), (19527630848, 5767168, 5767168, 0), (19516096512, 5767168, 11534336, 0), (19556466688, 5767168, 17301504, 0), (19562233856, 5767168, 23068672, 0), (19550699520, 5767168, 28835840, 0), (19573768192, 5767168, 34603008, 0), (19579535360, 5767168, 40370176, 0), (19568001024, 5767168, 46137344, 0), (19625672704, 5767168, 51904512, 0), (19631439872, 5767168, 57671680, 0), (19619905536, 5767168, 63438848, 0), (19677577216, 5767168, 69206016, 0), (19683344384, 5767168, 74973184, 0), (19671810048, 5767168, 80740352, 0), (19764084736, 5767168, 86507520, 0), (19769851904, 5767168, 92274688, 0), (19758317568, 5767168, 98041856, 0), (19798687744, 5767168, 103809024, 0), (19804454912, 5767168, 109576192, 0), (19792920576, 5767168, 115343360, 0), (19815989248, 5767168, 121110528, 0), (19821756416, 5767168, 126877696, 0), (19810222080, 5767168, 132644864, 0), (19833290752, 5767168, 138412032, 0), (19839057920, 5767168, 144179200, 0), (19827523584, 5767168, 149946368, 0), (19885195264, 5767168, 155713536, 0), (19890962432, 5767168, 161480704, 0), (19879428096, 5767168, 167247872, 0), (19902496768, 5767168, 173015040, 0), (19908263936, 5767168, 178782208, 0), (19896729600, 5767168, 184549376, 0), (19971702784, 5767168, 190316544, 0), (19977469952, 5767168, 196083712, 0), (19965935616, 5767168, 201850880, 0), (19989004288, 5767168, 207618048, 0), (19994771456, 5767168, 213385216, 0), (19983237120, 5767168, 219152384, 0), (20023607296, 5767168, 224919552, 0), (20029374464, 5767168, 230686720, 0), (20017840128, 5767168, 236453888, 0), (20110114816, 5767168, 242221056, 0), (20115881984, 5767168, 247988224, 0), (20104347648, 5767168, 253755392, 0), (20144717824, 5767168, 259522560, 0), (20150484992, 5767168, 265289728, 0), (20138950656, 5767168, 271056896, 0), (20196622336, 5767168, 276824064, 0), (20202389504, 5767168, 282591232, 0), (20190855168, 5767168, 288358400, 0), (20300431360, 5767168, 294125568, 0), (20306198528, 5767168, 299892736, 0), (20294664192, 5767168, 305659904, 0), (20369637376, 5767168, 311427072, 0), (20375404544, 5767168, 317194240, 0), (20363870208, 5767168, 322961408, 0), (20386938880, 5767168, 328728576, 0), (20392706048, 5767168, 334495744, 0), (20381171712, 5767168, 340262912, 0)], 2: [(19487260672, 5767168, 0, 0), (19493027840, 5767168, 5767168, 0), (19481493504, 5767168, 11534336, 0), (19591069696, 5767168, 17301504, 0), (19596836864, 5767168, 23068672, 0), (19585302528, 5767168, 28835840, 0), (19608371200, 5767168, 34603008, 0), (19614138368, 5767168, 40370176, 0), (19602604032, 5767168, 46137344, 0), (19694878720, 5767168, 51904512, 0), (19700645888, 5767168, 57671680, 0), (19689111552, 5767168, 63438848, 0), (19729481728, 5767168, 69206016, 0), (19735248896, 5767168, 74973184, 0), (19723714560, 5767168, 80740352, 0), (19746783232, 5767168, 86507520, 0), (19752550400, 5767168, 92274688, 0), (19741016064, 5767168, 98041856, 0), (19781386240, 5767168, 103809024, 0), (19787153408, 5767168, 109576192, 0), (19775619072, 5767168, 115343360, 0), (19850592256, 5767168, 121110528, 0), (19856359424, 5767168, 126877696, 0), (19844825088, 5767168, 132644864, 0), (19867893760, 5767168, 138412032, 0), (19873660928, 5767168, 144179200, 0), (19862126592, 5767168, 149946368, 0), (20075511808, 5767168, 155713536, 0), (20081278976, 5767168, 161480704, 0), (20069744640, 5767168, 167247872, 0), (20092813312, 5767168, 173015040, 0), (20098580480, 5767168, 178782208, 0), (20087046144, 5767168, 184549376, 0), (20162019328, 5767168, 190316544, 0), (20167786496, 5767168, 196083712, 0), (20156252160, 5767168, 201850880, 0), (20231225344, 5767168, 207618048, 0), (20236992512, 5767168, 213385216, 0), (20225458176, 5767168, 219152384, 0), (20265828352, 5767168, 224919552, 0), (20271595520, 5767168, 230686720, 0), (20260061184, 5767168, 236453888, 0), (20490747904, 5767168, 242221056, 0), (20496515072, 5767168, 247988224, 0), (20484980736, 5767168, 253755392, 0), (20508049408, 5767168, 259522560, 0), (20513816576, 5767168, 265289728, 0), (20502282240, 5767168, 271056896, 0), (20525350912, 5767168, 276824064, 0), (20531118080, 5767168, 282591232, 0), (20519583744, 5767168, 288358400, 0), (20542652416, 5767168, 294125568, 0), (20548419584, 5767168, 299892736, 0), (20536885248, 5767168, 305659904, 0), (20559953920, 5767168, 311427072, 0), (20565721088, 5767168, 317194240, 0), (20554186752, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6807aa430>, 2: <capsule object NULL at 0x74a6807aa3a0>}
DEBUG 01-15 16:09:08.927494.927494 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.927476.927476 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.927133.927133 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 00623425-9b54-40d9-be8e-625976287ae7
DEBUG 01-15 16:09:08.927650.927650 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:08.927250.927250 client.py:127] Model loaded
DEBUG 01-15 16:09:08.928803.928803 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.928081.928081 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:08.928563.928563 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:08.929092.929092 cuda_h.py:19] end restore2model cost 0.0009949207305908203 seconds
DEBUG 01-15 16:09:08.929155.929155 cuda_h.py:19] end sllm_worker_task cost 0.013131141662597656 seconds
DEBUG 01-15 16:09:08.929647.929647 cuda_h.py:19] end move_flatidxs cost 0.0008647441864013672 seconds
DEBUG 01-15 16:09:08.929106.929106 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:08.930359.930359 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 00623425-9b54-40d9-be8e-625976287ae7
DEBUG 01-15 16:09:08.930958.930958 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00584721565246582 seconds
DEBUG 01-15 16:09:08.930027.930027 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:08.933873.933873 cuda_h.py:19] end restore2model cost 0.003081798553466797 seconds
DEBUG 01-15 16:09:08.933524.933524 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.009171724319458008 seconds
DEBUG 01-15 16:09:08.933340.933340 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:08.934747.934747 cuda_h.py:19] end gpu_sexperts cost 0.00026988983154296875 seconds
DEBUG 01-15 16:09:08.934193.934193 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:08.934869.934869 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.430511474609375e-05 seconds
DEBUG 01-15 16:09:08.934850.934850 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:08.934745.934745 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.935071.935071 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009801387786865234 seconds
DEBUG 01-15 16:09:08.935344.935344 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.935677.935677 cuda_h.py:19] end gpu_group_list cost 0.000217437744140625 seconds
DEBUG 01-15 16:09:08.936407.936407 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:08.937562.937562 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010025501251220703 seconds
DEBUG 01-15 16:09:08.937366.937366 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:08.937282.937282 cuda_h.py:19] end gpu_group_list cost 0.00022459030151367188 seconds
DEBUG 01-15 16:09:08.938178.938178 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:08.938246.938246 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 00623425-9b54-40d9-be8e-625976287ae7
DEBUG 01-15 16:09:08.941404.941404 cuda_h.py:19] end group_tensors cost 0.01200723648071289 seconds
DEBUG 01-15 16:09:08.942522.942522 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:08.945364.945364 cuda_h.py:19] end group pad cost 0.002574920654296875 seconds
DEBUG 01-15 16:09:08.945240.945240 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:08.972459.972459 client.py:127] Model loaded
DEBUG 01-15 16:09:08.973560.973560 cuda_h.py:19] end wait_experts_multi_device cost 0.0346226692199707 seconds
DEBUG 01-15 16:09:08.973094.973094 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:08.975050.975050 cuda_h.py:19] end group_einsum cost 0.030002355575561523 seconds
DEBUG 01-15 16:09:08.975459.975459 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:08.978299.978299 cuda_h.py:19] end get_outputs_cpu1 cost 0.0027115345001220703 seconds
DEBUG 01-15 16:09:08.978043.978043 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05001997947692871 seconds
DEBUG 01-15 16:09:08.980146.980146 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006781578063964844 seconds
DEBUG 01-15 16:09:08.980094.980094 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:08.982035.982035 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.982549.982549 cuda_h.py:19] end index_scatter cost 0.00015473365783691406 seconds
DEBUG 01-15 16:09:08.983235.983235 cuda_h.py:19] end cpuoutputsdeal cost 0.0029134750366210938 seconds
DEBUG 01-15 16:09:08.983082.983082 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:08.983773.983773 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.984257.984257 cuda_h.py:19] end gpu_group_tensor cost 0.0003094673156738281 seconds
DEBUG 01-15 16:09:08.984425.984425 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:08.984094.984094 cuda_h.py:19] end gpu_group_tensor cost 0.0003070831298828125 seconds
DEBUG 01-15 16:09:08.984976.984976 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.986013.986013 cuda_h.py:19] end gpu_group_einsum cost 0.00138092041015625 seconds
DEBUG 01-15 16:09:08.986041.986041 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:08.987967.987967 cuda_h.py:19] end gpu_group_einsum cost 0.00048804283142089844 seconds
DEBUG 01-15 16:09:08.987030.987030 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.987173.987173 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.987624.987624 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018858909606933594 seconds
DEBUG 01-15 16:09:08.987857.987857 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.987794.987794 cuda_h.py:19] end concat_expert_out cost 5.936622619628906e-05 seconds
DEBUG 01-15 16:09:08.987121.987121 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.987310.987310 cuda_h.py:19] end index_scatter cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:09:08.988404.988404 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008237361907958984 seconds
DEBUG 01-15 16:09:08.988626.988626 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:08.988807.988807 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:08.988078.988078 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001697540283203125 seconds
DEBUG 01-15 16:09:08.988980.988980 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:08.988678.988678 cuda_h.py:19] end concat_expert_out cost 6.365776062011719e-05 seconds
DEBUG 01-15 16:09:08.988998.988998 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:08.988591.988591 cuda_h.py:19] end index_scatter cost 5.340576171875e-05 seconds
DEBUG 01-15 16:09:08.988539.988539 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005319118499755859 seconds
DEBUG 01-15 16:09:08.988853.988853 cuda_h.py:19] end gpu_experts_multi_device cost 0.05464959144592285 seconds
DEBUG 01-15 16:09:08.988670.988670 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.06700825691223145 seconds
DEBUG 01-15 16:09:08.989846.989846 cuda_h.py:19] end prefill_layer cost 0.07383966445922852 seconds
DEBUG 01-15 16:09:08.989875.989875 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:09:08.989054.989054 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:08.989949.989949 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:09:08.989606.989606 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:09:08.989455.989455 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:09:08.989212.989212 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 4.029273986816406e-05 seconds
DEBUG 01-15 16:09:08.989299.989299 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:09:08.989187.989187 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:08.989184.989184 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:08.989572.989572 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:08.990300.990300 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:08.990934.990934 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:08.990479.990479 cuda_h.py:19] end allocate_cuda_memory cost 0.000518798828125 seconds
DEBUG 01-15 16:09:08.991161.991161 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:08.991847.991847 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:08.991044.991044 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:08.991173.991173 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 67cd1307-218d-4517-a6d8-1a92c493424a
DEBUG 01-15 16:09:08.991127.991127 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:08.991819.991819 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
INFO 01-15 16:09:08.993837.993837 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 67cd1307-218d-4517-a6d8-1a92c493424a
DEBUG 01-15 16:09:08.994332.994332 cuda_h.py:19] end load_into_gpu_async cost 0.002814054489135742 seconds
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
DEBUG 01-15 16:09:08.994958.994958 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:08.994927.994927 cuda_h.py:19] end restore_tensors2 cost 0.00014734268188476562 seconds
DEBUG 01-15 16:09:08.994546.994546 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.00428462028503418 seconds
INFO 01-15 16:09:08.994187.994187 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 67cd1307-218d-4517-a6d8-1a92c493424a
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:08.995119.995119 cuda_h.py:19] end self_attn cost 0.003421306610107422 seconds
DEBUG 01-15 16:09:08.995321.995321 cuda_h.py:19] end iln_self_attn_paln cost 0.005929708480834961 seconds
DEBUG 01-15 16:09:08.995243.995243 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:09:08.995383.995383 cuda_h.py:10] start gate
DEBUG 01-15 16:09:08.996618.996618 cuda_h.py:19] end gate cost 0.0006659030914306641 seconds
DEBUG 01-15 16:09:08.996494.996494 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:08.996743.996743 lmp.py:1912] 
DEBUG 01-15 16:09:08.996743.996743 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:08.996499.996499 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:08.996911.996911 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:08.996223.996223 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:08.996150.996150 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:08.997124.997124 lmp.py:1917] 
DEBUG 01-15 16:09:08.997124.997124 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:08.997290.997290 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:08.997132.997132 lmp.py:1935]   Expert  4 |      9 | CPU
DEBUG 01-15 16:09:08.997775.997775 lmp.py:1935]   Expert 28 |     27 | CPU
DEBUG 01-15 16:09:08.997465.997465 lmp.py:1935]   Expert  7 |     46 | CPU
DEBUG 01-15 16:09:08.997915.997915 lmp.py:1935]   Expert 53 |     59 | CPU
DEBUG 01-15 16:09:08.997605.997605 lmp.py:1935]   Expert 52 |     67 | CPU
DEBUG 01-15 16:09:08.997532.997532 lmp.py:1935]   Expert 43 |     74 | CPU
DEBUG 01-15 16:09:08.997460.997460 lmp.py:1935]   Expert 49 |     86 | CPU
DEBUG 01-15 16:09:08.997911.997911 lmp.py:1935]   Expert 12 |     89 | CPU
DEBUG 01-15 16:09:08.997124.997124 lmp.py:1935]   Expert 47 |    103 | CPU
DEBUG 01-15 16:09:08.997482.997482 lmp.py:1935]   Expert 24 |    106 | CPU
DEBUG 01-15 16:09:08.997125.997125 lmp.py:1935]   Expert 33 |    107 | CPU
DEBUG 01-15 16:09:08.997768.997768 lmp.py:1935]   Expert  2 |    110 | CPU
DEBUG 01-15 16:09:08.997934.997934 lmp.py:1935]   Expert 50 |    111 | CPU
DEBUG 01-15 16:09:08.997815.997815 lmp.py:1935]   Expert 15 |    112 | CPU
DEBUG 01-15 16:09:08.997412.997412 lmp.py:1935]   Expert 39 |    114 | CPU
DEBUG 01-15 16:09:08.997532.997532 lmp.py:1935]   Expert 60 |    114 | CPU
DEBUG 01-15 16:09:08.997175.997175 lmp.py:1935]   Expert 36 |    119 | CPU
DEBUG 01-15 16:09:08.997818.997818 lmp.py:1935]   Expert 25 |    126 | CPU
DEBUG 01-15 16:09:08.997222.997222 lmp.py:1935]   Expert  6 |    128 | CPU
DEBUG 01-15 16:09:08.997865.997865 lmp.py:1935]   Expert 61 |    129 | CPU
DEBUG 01-15 16:09:08.997031.997031 lmp.py:1935]   Expert 59 |    138 | CPU
DEBUG 01-15 16:09:08.997913.997913 lmp.py:1935]   Expert  3 |    141 | CPU
DEBUG 01-15 16:09:08.997317.997317 lmp.py:1935]   Expert 27 |    142 | CPU
DEBUG 01-15 16:09:08.997960.997960 lmp.py:1935]   Expert 58 |    144 | CPU
DEBUG 01-15 16:09:08.997318.997318 lmp.py:1935]   Expert 30 |    149 | CPU
DEBUG 01-15 16:09:08.997584.997584 lmp.py:1935]   Expert  8 |    150 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997658.997658 lmp.py:1935]   Expert 31 |    150 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997016.997016 lmp.py:1935]   Expert 10 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997089.997089 lmp.py:1935]   Expert 40 |    156 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997686.997686 lmp.py:1935]   Expert 38 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997283.997283 lmp.py:1935]   Expert 41 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997879.997879 lmp.py:1935]   Expert 57 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997237.997237 lmp.py:1935]   Expert 14 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997311.997311 lmp.py:1935]   Expert 46 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997146.997146 lmp.py:1935]   Expert 37 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997458.997458 lmp.py:1935]   Expert 54 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997339.997339 lmp.py:1935]   Expert 32 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997698.997698 lmp.py:1935]   Expert 42 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997533.997533 lmp.py:1935]   Expert 19 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997891.997891 lmp.py:1935]   Expert 11 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997726.997726 lmp.py:1935]   Expert 34 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997038.997038 lmp.py:1935]   Expert 22 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997158.997158 lmp.py:1935]   Expert 18 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997277.997277 lmp.py:1935]   Expert 26 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997636.997636 lmp.py:1935]   Expert  0 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997994.997994 lmp.py:1935]   Expert 56 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997875.997875 lmp.py:1935]   Expert  1 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997472.997472 lmp.py:1935]   Expert 44 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997069.997069 lmp.py:1935]   Expert 51 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997665.997665 lmp.py:1935]   Expert 20 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997262.997262 lmp.py:1935]   Expert 29 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.997097.997097 lmp.py:1935]   Expert 48 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.997217.997217 lmp.py:1935]   Expert 45 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.998575.998575 lmp.py:1935]   Expert 21 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.998695.998695 lmp.py:1935]   Expert 35 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.998576.998576 lmp.py:1935]   Expert 55 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.998458.998458 lmp.py:1935]   Expert 16 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.998577.998577 lmp.py:1935]   Expert  5 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.998459.998459 lmp.py:1935]   Expert 23 |    373 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.998532.998532 lmp.py:1935]   Expert 13 |    381 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.998367.998367 lmp.py:1935]   Expert 17 |    435 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.998726.998726 lmp.py:1935]   Expert  9 |    457 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.998607.998607 lmp.py:1935]   Expert 63 |    460 | GPU2(cuda:2)
DEBUG 01-15 16:09:08.998204.998204 lmp.py:1935]   Expert 62 |   1181 | GPU1(cuda:1)
DEBUG 01-15 16:09:08.998893.998893 lmp.py:1937] 
DEBUG 01-15 16:09:08.998893.998893 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:08.998013.998013 lmp.py:1938]   CPU:   2550 tokens
DEBUG 01-15 16:09:08.998371.998371 lmp.py:1942]   cuda:1:   4905 tokens (19 experts)
DEBUG 01-15 16:09:08.998206.998206 lmp.py:1942]   cuda:2:   4833 tokens (20 experts)
DEBUG 01-15 16:09:08.998849.998849 lmp.py:1943]   Total GPU:   9738 tokens
DEBUG 01-15 16:09:08.998492.998492 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:08.998492.998492 lmp.py:1944] 
DEBUG 01-15 16:09:08.998380.998380 cuda_h.py:19] end experts_map_get cost 0.0017137527465820312 seconds
DEBUG 01-15 16:09:08.998846.998846 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:08.998648.998648 lmp.py:1953] 
DEBUG 01-15 16:09:08.998648.998648 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:08.998623.998623 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:08.998889.998889 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:08.998394.998394 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.001296.001296 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.5.gate_proj.weight': 0, 'model.layers.17.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.41.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.41.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.41.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.44.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.44.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.44.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.46.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.46.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.46.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.54.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.54.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.54.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.37.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.37.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.37.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.38.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.38.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.38.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.40.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.40.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.40.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.45.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.45.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.45.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.51.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.51.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.51.up_proj.weight': 322961408, 'model.layers.17.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.17.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.17.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(20663762944, 5767168, 0, 0), (20669530112, 5767168, 5767168, 0), (20657995776, 5767168, 11534336, 0), (20750270464, 5767168, 17301504, 0), (20756037632, 5767168, 23068672, 0), (20744503296, 5767168, 28835840, 0), (20767571968, 5767168, 34603008, 0), (20773339136, 5767168, 40370176, 0), (20761804800, 5767168, 46137344, 0), (20802174976, 5767168, 51904512, 0), (20807942144, 5767168, 57671680, 0), (20796407808, 5767168, 63438848, 0), (20888682496, 5767168, 69206016, 0), (20894449664, 5767168, 74973184, 0), (20882915328, 5767168, 80740352, 0), (20923285504, 5767168, 86507520, 0), (20929052672, 5767168, 92274688, 0), (20917518336, 5767168, 98041856, 0), (20940587008, 5767168, 103809024, 0), (20946354176, 5767168, 109576192, 0), (20934819840, 5767168, 115343360, 0), (20957888512, 5767168, 121110528, 0), (20963655680, 5767168, 126877696, 0), (20952121344, 5767168, 132644864, 0), (21113602048, 5767168, 138412032, 0), (21119369216, 5767168, 144179200, 0), (21107834880, 5767168, 149946368, 0), (21286617088, 5767168, 155713536, 0), (21292384256, 5767168, 161480704, 0), (21280849920, 5767168, 167247872, 0), (21303918592, 5767168, 173015040, 0), (21309685760, 5767168, 178782208, 0), (21298151424, 5767168, 184549376, 0), (21338521600, 5767168, 190316544, 0), (21344288768, 5767168, 196083712, 0), (21332754432, 5767168, 201850880, 0), (21373124608, 5767168, 207618048, 0), (21378891776, 5767168, 213385216, 0), (21367357440, 5767168, 219152384, 0), (21407727616, 5767168, 224919552, 0), (21413494784, 5767168, 230686720, 0), (21401960448, 5767168, 236453888, 0), (21511536640, 5767168, 242221056, 0), (21517303808, 5767168, 247988224, 0), (21505769472, 5767168, 253755392, 0), (21528838144, 5767168, 259522560, 0), (21534605312, 5767168, 265289728, 0), (21523070976, 5767168, 271056896, 0), (21546139648, 5767168, 276824064, 0), (21551906816, 5767168, 282591232, 0), (21540372480, 5767168, 288358400, 0), (21563441152, 5767168, 294125568, 0), (21569208320, 5767168, 299892736, 0), (21557673984, 5767168, 305659904, 0), (21649948672, 5767168, 311427072, 0), (21655715840, 5767168, 317194240, 0), (21644181504, 5767168, 322961408, 0)], 2: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (20594556928, 5767168, 17301504, 0), (20600324096, 5767168, 23068672, 0), (20588789760, 5767168, 28835840, 0), (20715667456, 5767168, 34603008, 0), (20721434624, 5767168, 40370176, 0), (20709900288, 5767168, 46137344, 0), (20732968960, 5767168, 51904512, 0), (20738736128, 5767168, 57671680, 0), (20727201792, 5767168, 63438848, 0), (20819476480, 5767168, 69206016, 0), (20825243648, 5767168, 74973184, 0), (20813709312, 5767168, 80740352, 0), (20854079488, 5767168, 86507520, 0), (20859846656, 5767168, 92274688, 0), (20848312320, 5767168, 98041856, 0), (20871380992, 5767168, 103809024, 0), (20877148160, 5767168, 109576192, 0), (20865613824, 5767168, 115343360, 0), (20905984000, 5767168, 121110528, 0), (20911751168, 5767168, 126877696, 0), (20900216832, 5767168, 132644864, 0), (20975190016, 5767168, 138412032, 0), (20980957184, 5767168, 144179200, 0), (20969422848, 5767168, 149946368, 0), (21027094528, 5767168, 155713536, 0), (21032861696, 5767168, 161480704, 0), (21021327360, 5767168, 167247872, 0), (21078999040, 5767168, 173015040, 0), (21084766208, 5767168, 178782208, 0), (21073231872, 5767168, 184549376, 0), (21130903552, 5767168, 190316544, 0), (21136670720, 5767168, 196083712, 0), (21125136384, 5767168, 201850880, 0), (21165506560, 5767168, 207618048, 0), (21171273728, 5767168, 213385216, 0), (21159739392, 5767168, 219152384, 0), (21182808064, 5767168, 224919552, 0), (21188575232, 5767168, 230686720, 0), (21177040896, 5767168, 236453888, 0), (21217411072, 5767168, 242221056, 0), (21223178240, 5767168, 247988224, 0), (21211643904, 5767168, 253755392, 0), (21234712576, 5767168, 259522560, 0), (21240479744, 5767168, 265289728, 0), (21228945408, 5767168, 271056896, 0), (21269315584, 5767168, 276824064, 0), (21275082752, 5767168, 282591232, 0), (21263548416, 5767168, 288358400, 0), (21355823104, 5767168, 294125568, 0), (21361590272, 5767168, 299892736, 0), (21350055936, 5767168, 305659904, 0), (21459632128, 5767168, 311427072, 0), (21465399296, 5767168, 317194240, 0), (21453864960, 5767168, 322961408, 0), (21667250176, 5767168, 328728576, 0), (21673017344, 5767168, 334495744, 0), (21661483008, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6807aa460>, 2: <capsule object NULL at 0x74a6807aa250>}
DEBUG 01-15 16:09:09.001776.001776 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.001712.001712 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.001037.001037 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d8c9faf0-4554-48f3-90e2-7b79b06e31d8
DEBUG 01-15 16:09:09.001389.001389 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:09.002448.002448 client.py:127] Model loaded
DEBUG 01-15 16:09:09.002750.002750 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.002437.002437 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.003604.003604 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:09.003459.003459 cuda_h.py:19] end restore2model cost 0.0009260177612304688 seconds
DEBUG 01-15 16:09:09.003079.003079 cuda_h.py:19] end sllm_worker_task cost 0.01319432258605957 seconds
DEBUG 01-15 16:09:09.004353.004353 cuda_h.py:19] end move_flatidxs cost 0.0009329319000244141 seconds
INFO 01-15 16:09:09.004155.004155 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d8c9faf0-4554-48f3-90e2-7b79b06e31d8
DEBUG 01-15 16:09:09.004527.004527 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:09.004357.004357 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0059795379638671875 seconds
DEBUG 01-15 16:09:09.004625.004625 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.007816.007816 cuda_h.py:19] end restore2model cost 0.003087282180786133 seconds
DEBUG 01-15 16:09:09.007897.007897 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00932455062866211 seconds
DEBUG 01-15 16:09:09.007236.007236 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.008597.008597 cuda_h.py:19] end gpu_sexperts cost 0.00027060508728027344 seconds
DEBUG 01-15 16:09:09.008804.008804 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.008627.008627 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:09:09.008515.008515 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.008456.008456 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.009701.009701 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009586811065673828 seconds
DEBUG 01-15 16:09:09.009068.009068 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.009665.009665 cuda_h.py:19] end gpu_group_list cost 0.00020194053649902344 seconds
DEBUG 01-15 16:09:09.010744.010744 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.011530.011530 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010478496551513672 seconds
DEBUG 01-15 16:09:09.011016.011016 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.011812.011812 cuda_h.py:19] end gpu_group_list cost 0.00020599365234375 seconds
DEBUG 01-15 16:09:09.012219.012219 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.012525.012525 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d8c9faf0-4554-48f3-90e2-7b79b06e31d8
DEBUG 01-15 16:09:09.014881.014881 cuda_h.py:19] end group_tensors cost 0.01053762435913086 seconds
DEBUG 01-15 16:09:09.015789.015789 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.018542.018542 cuda_h.py:19] end group pad cost 0.002646923065185547 seconds
DEBUG 01-15 16:09:09.018941.018941 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:09.046961.046961 cuda_h.py:19] end group_einsum cost 0.02802300453186035 seconds
DEBUG 01-15 16:09:09.046601.046601 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:09.047137.047137 client.py:127] Model loaded
DEBUG 01-15 16:09:09.047248.047248 cuda_h.py:19] end wait_experts_multi_device cost 0.03461122512817383 seconds
DEBUG 01-15 16:09:09.047924.047924 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.048136.048136 cuda_h.py:19] end get_outputs_cpu1 cost 0.002486705780029297 seconds
DEBUG 01-15 16:09:09.049407.049407 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.047290802001953125 seconds
DEBUG 01-15 16:09:09.050405.050405 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.003206014633178711 seconds
DEBUG 01-15 16:09:09.050806.050806 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.051474.051474 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.051360.051360 cuda_h.py:19] end index_scatter cost 8.177757263183594e-05 seconds
DEBUG 01-15 16:09:09.051184.051184 cuda_h.py:19] end cpuoutputsdeal cost 0.0013544559478759766 seconds
DEBUG 01-15 16:09:09.052267.052267 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.052646.052646 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.052122.052122 cuda_h.py:19] end gpu_group_tensor cost 0.00014638900756835938 seconds
DEBUG 01-15 16:09:09.052600.052600 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.052521.052521 cuda_h.py:19] end gpu_group_tensor cost 0.00019311904907226562 seconds
DEBUG 01-15 16:09:09.052631.052631 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.054357.054357 cuda_h.py:19] end gpu_group_einsum cost 0.0018384456634521484 seconds
DEBUG 01-15 16:09:09.054218.054218 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.055660.055660 cuda_h.py:19] end gpu_group_einsum cost 0.0004756450653076172 seconds
DEBUG 01-15 16:09:09.055115.055115 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.055549.055549 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.055035.055035 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002512931823730469 seconds
DEBUG 01-15 16:09:09.055030.055030 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.055828.055828 cuda_h.py:19] end concat_expert_out cost 6.437301635742188e-05 seconds
DEBUG 01-15 16:09:09.056254.056254 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.056118.056118 cuda_h.py:19] end index_scatter cost 7.891654968261719e-05 seconds
DEBUG 01-15 16:09:09.056154.056154 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009100437164306641 seconds
DEBUG 01-15 16:09:09.056827.056827 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.056246.056246 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.056320.056320 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002009868621826172 seconds
DEBUG 01-15 16:09:09.056976.056976 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.056099.056099 cuda_h.py:19] end concat_expert_out cost 5.9604644775390625e-05 seconds
DEBUG 01-15 16:09:09.057803.057803 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.057409.057409 cuda_h.py:19] end index_scatter cost 6.389617919921875e-05 seconds
DEBUG 01-15 16:09:09.057502.057502 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005667209625244141 seconds
DEBUG 01-15 16:09:09.057803.057803 cuda_h.py:19] end gpu_experts_multi_device cost 0.04892563819885254 seconds
DEBUG 01-15 16:09:09.057720.057720 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.06148695945739746 seconds
DEBUG 01-15 16:09:09.057132.057132 cuda_h.py:19] end prefill_layer cost 0.06814146041870117 seconds
DEBUG 01-15 16:09:09.057445.057445 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:09:09.057764.057764 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.057798.057798 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:09:09.057785.057785 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:09:09.057396.057396 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:09:09.057285.057285 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 3.409385681152344e-05 seconds
DEBUG 01-15 16:09:09.058392.058392 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:09:09.058850.058850 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.058416.058416 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.058797.058797 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:09.058810.058810 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:09.058067.058067 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:09.059813.059813 cuda_h.py:19] end allocate_cuda_memory cost 0.000385284423828125 seconds
DEBUG 01-15 16:09:09.059249.059249 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:09.059014.059014 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.059316.059316 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.059353.059353 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 29915d73-1ca4-41e2-a4b8-48537fa72a4d
DEBUG 01-15 16:09:09.059645.059645 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.059012.059012 cuda_h.py:10] start self_attn
INFO 01-15 16:09:09.061741.061741 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 29915d73-1ca4-41e2-a4b8-48537fa72a4d
DEBUG 01-15 16:09:09.061991.061991 cuda_h.py:19] end load_into_gpu_async cost 0.0024726390838623047 seconds
DEBUG 01-15 16:09:09.061352.061352 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:09.062380.062380 cuda_h.py:19] end restore_tensors2 cost 0.00015020370483398438 seconds
DEBUG 01-15 16:09:09.062237.062237 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0037491321563720703 seconds
INFO 01-15 16:09:09.062573.062573 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 29915d73-1ca4-41e2-a4b8-48537fa72a4d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.064031.064031 cuda_h.py:19] end self_attn cost 0.003921985626220703 seconds
DEBUG 01-15 16:09:09.064851.064851 cuda_h.py:19] end iln_self_attn_paln cost 0.006316661834716797 seconds
DEBUG 01-15 16:09:09.064257.064257 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:09:09.064019.064019 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.065297.065297 cuda_h.py:19] end gate cost 0.000728607177734375 seconds
DEBUG 01-15 16:09:09.065987.065987 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.065143.065143 lmp.py:1912] 
DEBUG 01-15 16:09:09.065143.065143 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.065330.065330 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.065172.065172 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.065676.065676 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.065272.065272 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.065677.065677 lmp.py:1917] 
DEBUG 01-15 16:09:09.065677.065677 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.065558.065558 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.065115.065115 lmp.py:1935]   Expert 32 |     34 | CPU
DEBUG 01-15 16:09:09.065473.065473 lmp.py:1935]   Expert 30 |     50 | CPU
DEBUG 01-15 16:09:09.065977.065977 lmp.py:1935]   Expert  5 |     52 | CPU
DEBUG 01-15 16:09:09.066005.066005 lmp.py:1935]   Expert 46 |     74 | CPU
DEBUG 01-15 16:09:09.066317.066317 lmp.py:1935]   Expert  8 |     87 | CPU
DEBUG 01-15 16:09:09.066867.066867 lmp.py:1935]   Expert 40 |     88 | CPU
DEBUG 01-15 16:09:09.066463.066463 lmp.py:1935]   Expert 12 |    100 | CPU
DEBUG 01-15 16:09:09.066967.066967 lmp.py:1935]   Expert 17 |    106 | CPU
DEBUG 01-15 16:09:09.066518.066518 lmp.py:1935]   Expert 27 |    107 | CPU
DEBUG 01-15 16:09:09.066068.066068 lmp.py:1935]   Expert 58 |    115 | CPU
DEBUG 01-15 16:09:09.066380.066380 lmp.py:1935]   Expert 60 |    115 | CPU
DEBUG 01-15 16:09:09.066169.066169 lmp.py:1935]   Expert  3 |    116 | CPU
DEBUG 01-15 16:09:09.066481.066481 lmp.py:1935]   Expert 21 |    118 | CPU
DEBUG 01-15 16:09:09.066362.066362 lmp.py:1935]   Expert 28 |    121 | CPU
DEBUG 01-15 16:09:09.066767.066767 lmp.py:1935]   Expert 29 |    122 | CPU
DEBUG 01-15 16:09:09.066476.066476 lmp.py:1935]   Expert 25 |    125 | CPU
DEBUG 01-15 16:09:09.066265.066265 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:09:09.066815.066815 lmp.py:1935]   Expert 35 |    133 | CPU
DEBUG 01-15 16:09:09.066889.066889 lmp.py:1935]   Expert 19 |    134 | CPU
DEBUG 01-15 16:09:09.066962.066962 lmp.py:1935]   Expert  0 |    142 | CPU
DEBUG 01-15 16:09:09.066036.066036 lmp.py:1935]   Expert  6 |    146 | CPU
DEBUG 01-15 16:09:09.066109.066109 lmp.py:1935]   Expert 52 |    146 | CPU
DEBUG 01-15 16:09:09.066421.066421 lmp.py:1935]   Expert 56 |    149 | CPU
DEBUG 01-15 16:09:09.066733.066733 lmp.py:1935]   Expert 54 |    150 | CPU
DEBUG 01-15 16:09:09.066807.066807 lmp.py:1935]   Expert 37 |    151 | CPU
DEBUG 01-15 16:09:09.066787.066787 lmp.py:1935]   Expert 53 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066484.066484 lmp.py:1935]   Expert 63 |    156 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066941.066941 lmp.py:1935]   Expert 48 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066684.066684 lmp.py:1935]   Expert 36 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066426.066426 lmp.py:1935]   Expert 59 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066930.066930 lmp.py:1935]   Expert  9 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066196.066196 lmp.py:1935]   Expert  1 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066223.066223 lmp.py:1935]   Expert 39 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066488.066488 lmp.py:1935]   Expert 20 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066992.066992 lmp.py:1935]   Expert 61 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066735.066735 lmp.py:1935]   Expert 42 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066762.066762 lmp.py:1935]   Expert 43 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066266.066266 lmp.py:1935]   Expert  7 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066293.066293 lmp.py:1935]   Expert 11 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066559.066559 lmp.py:1935]   Expert 34 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066871.066871 lmp.py:1935]   Expert 47 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066659.066659 lmp.py:1935]   Expert 55 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066163.066163 lmp.py:1935]   Expert 13 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066906.066906 lmp.py:1935]   Expert 16 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066993.066993 lmp.py:1935]   Expert 57 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066735.066735 lmp.py:1935]   Expert 18 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066762.066762 lmp.py:1935]   Expert 15 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066790.066790 lmp.py:1935]   Expert  4 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066863.066863 lmp.py:1935]   Expert 33 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066129.066129 lmp.py:1935]   Expert 22 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.066156.066156 lmp.py:1935]   Expert 45 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.066137.066137 lmp.py:1935]   Expert 31 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.067117.067117 lmp.py:1935]   Expert 50 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.067383.067383 lmp.py:1935]   Expert 51 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.067933.067933 lmp.py:1935]   Expert 49 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.067960.067960 lmp.py:1935]   Expert 38 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.067511.067511 lmp.py:1935]   Expert 26 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.067538.067538 lmp.py:1935]   Expert 10 |    284 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.067565.067565 lmp.py:1935]   Expert 44 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.067831.067831 lmp.py:1935]   Expert  2 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.067335.067335 lmp.py:1935]   Expert 24 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.067077.067077 lmp.py:1935]   Expert 14 |    313 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.067343.067343 lmp.py:1935]   Expert 23 |    403 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.067893.067893 lmp.py:1935]   Expert 62 |    676 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.067490.067490 lmp.py:1937] 
DEBUG 01-15 16:09:09.067490.067490 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.067517.067517 lmp.py:1938]   CPU:   2811 tokens
DEBUG 01-15 16:09:09.067544.067544 lmp.py:1942]   cuda:1:   4740 tokens (19 experts)
DEBUG 01-15 16:09:09.067571.067571 lmp.py:1942]   cuda:2:   4737 tokens (20 experts)
DEBUG 01-15 16:09:09.067645.067645 lmp.py:1943]   Total GPU:   9477 tokens
DEBUG 01-15 16:09:09.067433.067433 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.067433.067433 lmp.py:1944] 
DEBUG 01-15 16:09:09.067898.067898 cuda_h.py:19] end experts_map_get cost 0.0020453929901123047 seconds
DEBUG 01-15 16:09:09.067755.067755 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.067703.067703 lmp.py:1953] 
DEBUG 01-15 16:09:09.067703.067703 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.067824.067824 cuda_h.py:19] end cpu_experts_submit cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:09.067804.067804 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.067786.067786 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.069866.069866 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.1.gate_proj.weight': 0, 'model.layers.18.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.9.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.9.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.9.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.44.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.44.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.44.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.18.mlp.experts.2.gate_proj.weight': 0, 'model.layers.18.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.18.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.18.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.18.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(21701853184, 5767168, 0, 0), (21707620352, 5767168, 5767168, 0), (21696086016, 5767168, 11534336, 0), (21840265216, 5767168, 17301504, 0), (21846032384, 5767168, 23068672, 0), (21834498048, 5767168, 28835840, 0), (21874868224, 5767168, 34603008, 0), (21880635392, 5767168, 40370176, 0), (21869101056, 5767168, 46137344, 0), (21944074240, 5767168, 51904512, 0), (21949841408, 5767168, 57671680, 0), (21938307072, 5767168, 63438848, 0), (22030581760, 5767168, 69206016, 0), (22036348928, 5767168, 74973184, 0), (22024814592, 5767168, 80740352, 0), (22099787776, 5767168, 86507520, 0), (22105554944, 5767168, 92274688, 0), (22094020608, 5767168, 98041856, 0), (22134390784, 5767168, 103809024, 0), (22140157952, 5767168, 109576192, 0), (22128623616, 5767168, 115343360, 0), (22220898304, 5767168, 121110528, 0), (22226665472, 5767168, 126877696, 0), (22215131136, 5767168, 132644864, 0), (22255501312, 5767168, 138412032, 0), (22261268480, 5767168, 144179200, 0), (22249734144, 5767168, 149946368, 0), (22272802816, 5767168, 155713536, 0), (22278569984, 5767168, 161480704, 0), (22267035648, 5767168, 167247872, 0), (22411214848, 5767168, 173015040, 0), (22416982016, 5767168, 178782208, 0), (22405447680, 5767168, 184549376, 0), (22445817856, 5767168, 190316544, 0), (22451585024, 5767168, 196083712, 0), (22440050688, 5767168, 201850880, 0), (22463119360, 5767168, 207618048, 0), (22468886528, 5767168, 213385216, 0), (22457352192, 5767168, 219152384, 0), (22515023872, 5767168, 224919552, 0), (22520791040, 5767168, 230686720, 0), (22509256704, 5767168, 236453888, 0), (22532325376, 5767168, 242221056, 0), (22538092544, 5767168, 247988224, 0), (22526558208, 5767168, 253755392, 0), (22601531392, 5767168, 259522560, 0), (22607298560, 5767168, 265289728, 0), (22595764224, 5767168, 271056896, 0), (22636134400, 5767168, 276824064, 0), (22641901568, 5767168, 282591232, 0), (22630367232, 5767168, 288358400, 0), (22670737408, 5767168, 294125568, 0), (22676504576, 5767168, 299892736, 0), (22664970240, 5767168, 305659904, 0), (22757244928, 5767168, 311427072, 0), (22763012096, 5767168, 317194240, 0), (22751477760, 5767168, 322961408, 0)], 2: [(21719154688, 5767168, 0, 0), (21724921856, 5767168, 5767168, 0), (21713387520, 5767168, 11534336, 0), (21753757696, 5767168, 17301504, 0), (21759524864, 5767168, 23068672, 0), (21747990528, 5767168, 28835840, 0), (21805662208, 5767168, 34603008, 0), (21811429376, 5767168, 40370176, 0), (21799895040, 5767168, 46137344, 0), (21857566720, 5767168, 51904512, 0), (21863333888, 5767168, 57671680, 0), (21851799552, 5767168, 63438848, 0), (21909471232, 5767168, 69206016, 0), (21915238400, 5767168, 74973184, 0), (21903704064, 5767168, 80740352, 0), (21926772736, 5767168, 86507520, 0), (21932539904, 5767168, 92274688, 0), (21921005568, 5767168, 98041856, 0), (21961375744, 5767168, 103809024, 0), (21967142912, 5767168, 109576192, 0), (21955608576, 5767168, 115343360, 0), (21995978752, 5767168, 121110528, 0), (22001745920, 5767168, 126877696, 0), (21990211584, 5767168, 132644864, 0), (22065184768, 5767168, 138412032, 0), (22070951936, 5767168, 144179200, 0), (22059417600, 5767168, 149946368, 0), (22082486272, 5767168, 155713536, 0), (22088253440, 5767168, 161480704, 0), (22076719104, 5767168, 167247872, 0), (22307405824, 5767168, 173015040, 0), (22313172992, 5767168, 178782208, 0), (22301638656, 5767168, 184549376, 0), (22342008832, 5767168, 190316544, 0), (22347776000, 5767168, 196083712, 0), (22336241664, 5767168, 201850880, 0), (22359310336, 5767168, 207618048, 0), (22365077504, 5767168, 213385216, 0), (22353543168, 5767168, 219152384, 0), (22428516352, 5767168, 224919552, 0), (22434283520, 5767168, 230686720, 0), (22422749184, 5767168, 236453888, 0), (22497722368, 5767168, 242221056, 0), (22503489536, 5767168, 247988224, 0), (22491955200, 5767168, 253755392, 0), (22549626880, 5767168, 259522560, 0), (22555394048, 5767168, 265289728, 0), (22543859712, 5767168, 271056896, 0), (22566928384, 5767168, 276824064, 0), (22572695552, 5767168, 282591232, 0), (22561161216, 5767168, 288358400, 0), (22705340416, 5767168, 294125568, 0), (22711107584, 5767168, 299892736, 0), (22699573248, 5767168, 305659904, 0), (22739943424, 5767168, 311427072, 0), (22745710592, 5767168, 317194240, 0), (22734176256, 5767168, 322961408, 0), (22774546432, 5767168, 328728576, 0), (22780313600, 5767168, 334495744, 0), (22768779264, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a81475b780>, 2: <capsule object NULL at 0x74a6807aa0a0>}
DEBUG 01-15 16:09:09.069833.069833 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.069391.069391 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.069717.069717 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7fcadbb7-3ad0-4f3c-86af-8f0210f11037
DEBUG 01-15 16:09:09.069425.069425 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:09.070363.070363 client.py:127] Model loaded
DEBUG 01-15 16:09:09.070149.070149 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.070111.070111 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.070919.070919 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:09.071588.071588 cuda_h.py:19] end restore2model cost 0.0009691715240478516 seconds
DEBUG 01-15 16:09:09.071076.071076 cuda_h.py:19] end sllm_worker_task cost 0.012883901596069336 seconds
DEBUG 01-15 16:09:09.071421.071421 cuda_h.py:19] end move_flatidxs cost 0.000873565673828125 seconds
DEBUG 01-15 16:09:09.071960.071960 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:09.071468.071468 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7fcadbb7-3ad0-4f3c-86af-8f0210f11037
DEBUG 01-15 16:09:09.072420.072420 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004854917526245117 seconds
DEBUG 01-15 16:09:09.072635.072635 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.076573.076573 cuda_h.py:19] end restore2model cost 0.0036351680755615234 seconds
DEBUG 01-15 16:09:09.076601.076601 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008745431900024414 seconds
DEBUG 01-15 16:09:09.076325.076325 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.076191.076191 cuda_h.py:19] end gpu_sexperts cost 0.0003170967102050781 seconds
DEBUG 01-15 16:09:09.076736.076736 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.076943.076943 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7404556274414062e-05 seconds
DEBUG 01-15 16:09:09.076784.076784 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.076348.076348 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.078093.078093 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012543201446533203 seconds
DEBUG 01-15 16:09:09.078295.078295 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.078561.078561 cuda_h.py:19] end gpu_group_list cost 0.00020170211791992188 seconds
DEBUG 01-15 16:09:09.079688.079688 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.079974.079974 cuda_h.py:19] end group_tensors cost 0.007460117340087891 seconds
DEBUG 01-15 16:09:09.080774.080774 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.081989.081989 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.002192258834838867 seconds
DEBUG 01-15 16:09:09.081754.081754 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.082626.082626 cuda_h.py:19] end gpu_group_list cost 0.000286102294921875 seconds
DEBUG 01-15 16:09:09.083360.083360 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.083522.083522 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7fcadbb7-3ad0-4f3c-86af-8f0210f11037
DEBUG 01-15 16:09:09.085070.085070 cuda_h.py:19] end group pad cost 0.005024909973144531 seconds
DEBUG 01-15 16:09:09.085006.085006 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:09.105746.105746 client.py:127] Model loaded
DEBUG 01-15 16:09:09.106933.106933 cuda_h.py:19] end wait_experts_multi_device cost 0.022538185119628906 seconds
DEBUG 01-15 16:09:09.106062.106062 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.112743.112743 cuda_h.py:19] end group_einsum cost 0.026688098907470703 seconds
DEBUG 01-15 16:09:09.112060.112060 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:09.117928.117928 cuda_h.py:19] end get_outputs_cpu1 cost 0.004654645919799805 seconds
DEBUG 01-15 16:09:09.118090.118090 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.047539710998535156 seconds
DEBUG 01-15 16:09:09.118206.118206 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01248025894165039 seconds
DEBUG 01-15 16:09:09.119186.119186 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.121528.121528 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.121226.121226 cuda_h.py:19] end index_scatter cost 0.00015091896057128906 seconds
DEBUG 01-15 16:09:09.122720.122720 cuda_h.py:19] end cpuoutputsdeal cost 0.0029959678649902344 seconds
DEBUG 01-15 16:09:09.122944.122944 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.122921.122921 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.122874.122874 cuda_h.py:19] end gpu_group_tensor cost 0.0003075599670410156 seconds
DEBUG 01-15 16:09:09.122327.122327 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.123916.123916 cuda_h.py:19] end gpu_group_tensor cost 0.00028514862060546875 seconds
DEBUG 01-15 16:09:09.123314.123314 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.124550.124550 cuda_h.py:19] end gpu_group_einsum cost 0.0010066032409667969 seconds
DEBUG 01-15 16:09:09.124598.124598 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.125369.125369 cuda_h.py:19] end gpu_group_einsum cost 0.0009410381317138672 seconds
DEBUG 01-15 16:09:09.126132.126132 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.126676.126676 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.126351.126351 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004699230194091797 seconds
DEBUG 01-15 16:09:09.127605.127605 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.127870.127870 cuda_h.py:19] end concat_expert_out cost 0.00011992454528808594 seconds
DEBUG 01-15 16:09:09.127405.127405 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.127120.127120 cuda_h.py:19] end index_scatter cost 0.00012445449829101562 seconds
DEBUG 01-15 16:09:09.128515.128515 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0018193721771240234 seconds
DEBUG 01-15 16:09:09.128038.128038 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.128262.128262 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.128224.128224 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003440380096435547 seconds
DEBUG 01-15 16:09:09.128756.128756 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.129928.129928 cuda_h.py:19] end concat_expert_out cost 0.00012087821960449219 seconds
DEBUG 01-15 16:09:09.129648.129648 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.129927.129927 cuda_h.py:19] end index_scatter cost 0.00011777877807617188 seconds
DEBUG 01-15 16:09:09.129479.129479 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001195669174194336 seconds
DEBUG 01-15 16:09:09.129088.129088 cuda_h.py:19] end gpu_experts_multi_device cost 0.052927255630493164 seconds
DEBUG 01-15 16:09:09.129147.129147 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.06549906730651855 seconds
DEBUG 01-15 16:09:09.130863.130863 cuda_h.py:19] end prefill_layer cost 0.0729210376739502 seconds
DEBUG 01-15 16:09:09.130363.130363 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:09:09.130074.130074 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.131977.131977 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:09:09.131357.131357 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:09:09.131982.131982 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:09:09.131278.131278 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:09:09.131144.131144 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:09.131401.131401 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 0.00037407875061035156 seconds
DEBUG 01-15 16:09:09.131257.131257 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:09.132115.132115 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.132598.132598 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:09.132838.132838 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.133237.133237 cuda_h.py:19] end allocate_cuda_memory cost 0.0005517005920410156 seconds
DEBUG 01-15 16:09:09.133903.133903 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:09.133615.133615 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.133414.133414 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.133761.133761 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e47dded4-720d-41e8-bdf8-908e3761284a
DEBUG 01-15 16:09:09.133015.133015 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.134192.134192 cuda_h.py:10] start self_attn
INFO 01-15 16:09:09.135665.135665 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e47dded4-720d-41e8-bdf8-908e3761284a
DEBUG 01-15 16:09:09.135856.135856 cuda_h.py:19] end load_into_gpu_async cost 0.002455472946166992 seconds
DEBUG 01-15 16:09:09.136562.136562 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:09.136947.136947 cuda_h.py:19] end restore_tensors2 cost 0.00016832351684570312 seconds
DEBUG 01-15 16:09:09.136548.136548 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004301786422729492 seconds
INFO 01-15 16:09:09.136235.136235 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e47dded4-720d-41e8-bdf8-908e3761284a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.138825.138825 cuda_h.py:19] end self_attn cost 0.00434565544128418 seconds
DEBUG 01-15 16:09:09.139276.139276 cuda_h.py:19] end iln_self_attn_paln cost 0.006888866424560547 seconds
DEBUG 01-15 16:09:09.139609.139609 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:09:09.139240.139240 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.140548.140548 cuda_h.py:19] end gate cost 0.0008485317230224609 seconds
DEBUG 01-15 16:09:09.140543.140543 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.140844.140844 lmp.py:1912] 
DEBUG 01-15 16:09:09.140844.140844 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.140474.140474 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.140422.140422 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.140556.140556 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.140159.140159 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.140571.140571 lmp.py:1917] 
DEBUG 01-15 16:09:09.140571.140571 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.140936.140936 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.140645.140645 lmp.py:1935]   Expert 44 |     40 | CPU
DEBUG 01-15 16:09:09.140487.140487 lmp.py:1935]   Expert  1 |     49 | CPU
DEBUG 01-15 16:09:09.140614.140614 lmp.py:1935]   Expert 60 |     59 | CPU
DEBUG 01-15 16:09:09.140025.140025 lmp.py:1935]   Expert 28 |     71 | CPU
DEBUG 01-15 16:09:09.141629.141629 lmp.py:1935]   Expert 48 |     77 | CPU
DEBUG 01-15 16:09:09.141802.141802 lmp.py:1935]   Expert 27 |     89 | CPU
DEBUG 01-15 16:09:09.141690.141690 lmp.py:1935]   Expert  0 |     99 | CPU
DEBUG 01-15 16:09:09.141863.141863 lmp.py:1935]   Expert 62 |    106 | CPU
DEBUG 01-15 16:09:09.141274.141274 lmp.py:1935]   Expert 42 |    109 | CPU
DEBUG 01-15 16:09:09.141685.141685 lmp.py:1935]   Expert 22 |    112 | CPU
DEBUG 01-15 16:09:09.141858.141858 lmp.py:1935]   Expert 30 |    112 | CPU
DEBUG 01-15 16:09:09.141462.141462 lmp.py:1935]   Expert 59 |    114 | CPU
DEBUG 01-15 16:09:09.141873.141873 lmp.py:1935]   Expert 58 |    121 | CPU
DEBUG 01-15 16:09:09.141046.141046 lmp.py:1935]   Expert  8 |    125 | CPU
DEBUG 01-15 16:09:09.141510.141510 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:09:09.141783.141783 lmp.py:1935]   Expert 12 |    126 | CPU
DEBUG 01-15 16:09:09.141770.141770 lmp.py:1935]   Expert 50 |    133 | CPU
DEBUG 01-15 16:09:09.141473.141473 lmp.py:1935]   Expert 56 |    143 | CPU
DEBUG 01-15 16:09:09.141461.141461 lmp.py:1935]   Expert  5 |    144 | CPU
DEBUG 01-15 16:09:09.141495.141495 lmp.py:1935]   Expert 15 |    151 | CPU
DEBUG 01-15 16:09:09.141290.141290 lmp.py:1935]   Expert 55 |    151 | CPU
DEBUG 01-15 16:09:09.141324.141324 lmp.py:1935]   Expert 26 |    153 | CPU
DEBUG 01-15 16:09:09.141358.141358 lmp.py:1935]   Expert 32 |    156 | CPU
DEBUG 01-15 16:09:09.141392.141392 lmp.py:1935]   Expert 57 |    156 | CPU
DEBUG 01-15 16:09:09.141380.141380 lmp.py:1935]   Expert 47 |    159 | CPU
DEBUG 01-15 16:09:09.141321.141321 lmp.py:1935]   Expert 34 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141262.141262 lmp.py:1935]   Expert 24 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141965.141965 lmp.py:1935]   Expert 52 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.141191.141191 lmp.py:1935]   Expert  2 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.141133.141133 lmp.py:1935]   Expert 13 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141836.141836 lmp.py:1935]   Expert 40 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141062.141062 lmp.py:1935]   Expert 18 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.141764.141764 lmp.py:1935]   Expert  6 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141706.141706 lmp.py:1935]   Expert 41 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.141601.141601 lmp.py:1935]   Expert 54 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141410.141410 lmp.py:1935]   Expert  3 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.141173.141173 lmp.py:1935]   Expert 19 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.141961.141961 lmp.py:1935]   Expert 37 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141273.141273 lmp.py:1935]   Expert 20 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.141585.141585 lmp.py:1935]   Expert 46 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141897.141897 lmp.py:1935]   Expert 25 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.141640.141640 lmp.py:1935]   Expert 51 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141620.141620 lmp.py:1935]   Expert 17 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.141694.141694 lmp.py:1935]   Expert 43 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.141529.141529 lmp.py:1935]   Expert 11 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142602.142602 lmp.py:1935]   Expert 31 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142437.142437 lmp.py:1935]   Expert 35 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142511.142511 lmp.py:1935]   Expert 23 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142869.142869 lmp.py:1935]   Expert 49 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142704.142704 lmp.py:1935]   Expert 39 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142016.142016 lmp.py:1935]   Expert 10 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142282.142282 lmp.py:1935]   Expert 53 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142355.142355 lmp.py:1935]   Expert 33 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142429.142429 lmp.py:1935]   Expert 36 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142025.142025 lmp.py:1935]   Expert 38 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142576.142576 lmp.py:1935]   Expert  4 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142172.142172 lmp.py:1935]   Expert 21 |    332 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142769.142769 lmp.py:1935]   Expert 14 |    348 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142604.142604 lmp.py:1935]   Expert 63 |    368 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142439.142439 lmp.py:1935]   Expert 45 |    377 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142513.142513 lmp.py:1935]   Expert 61 |    392 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142063.142063 lmp.py:1935]   Expert  9 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142375.142375 lmp.py:1935]   Expert 29 |    488 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.142733.142733 lmp.py:1935]   Expert  7 |    518 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.142376.142376 lmp.py:1937] 
DEBUG 01-15 16:09:09.142376.142376 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.142211.142211 lmp.py:1938]   CPU:   2880 tokens
DEBUG 01-15 16:09:09.142523.142523 lmp.py:1942]   cuda:1:   4784 tokens (20 experts)
DEBUG 01-15 16:09:09.142120.142120 lmp.py:1942]   cuda:2:   4624 tokens (19 experts)
DEBUG 01-15 16:09:09.142001.142001 lmp.py:1943]   Total GPU:   9408 tokens
DEBUG 01-15 16:09:09.142882.142882 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.142882.142882 lmp.py:1944] 
DEBUG 01-15 16:09:09.142678.142678 cuda_h.py:19] end experts_map_get cost 0.00220489501953125 seconds
DEBUG 01-15 16:09:09.142528.142528 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.142807.142807 lmp.py:1953] 
DEBUG 01-15 16:09:09.142807.142807 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.142511.142511 cuda_h.py:19] end cpu_experts_submit cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:09:09.142061.142061 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.142825.142825 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.143134.143134 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.6.gate_proj.weight': 0, 'model.layers.19.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.34.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.34.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.34.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.36.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.36.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.36.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.19.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.19.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.19.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.19.mlp.experts.2.gate_proj.weight': 0, 'model.layers.19.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.31.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.31.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.31.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.33.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.33.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.33.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.53.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.53.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.53.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(22895656960, 5767168, 0, 0), (22901424128, 5767168, 5767168, 0), (22889889792, 5767168, 11534336, 0), (22912958464, 5767168, 17301504, 0), (22918725632, 5767168, 23068672, 0), (22907191296, 5767168, 28835840, 0), (22964862976, 5767168, 34603008, 0), (22970630144, 5767168, 40370176, 0), (22959095808, 5767168, 46137344, 0), (23016767488, 5767168, 51904512, 0), (23022534656, 5767168, 57671680, 0), (23011000320, 5767168, 63438848, 0), (23155179520, 5767168, 69206016, 0), (23160946688, 5767168, 74973184, 0), (23149412352, 5767168, 80740352, 0), (23189782528, 5767168, 86507520, 0), (23195549696, 5767168, 92274688, 0), (23184015360, 5767168, 98041856, 0), (23207084032, 5767168, 103809024, 0), (23212851200, 5767168, 109576192, 0), (23201316864, 5767168, 115343360, 0), (23380099072, 5767168, 121110528, 0), (23385866240, 5767168, 126877696, 0), (23374331904, 5767168, 132644864, 0), (23397400576, 5767168, 138412032, 0), (23403167744, 5767168, 144179200, 0), (23391633408, 5767168, 149946368, 0), (23414702080, 5767168, 155713536, 0), (23420469248, 5767168, 161480704, 0), (23408934912, 5767168, 167247872, 0), (23432003584, 5767168, 173015040, 0), (23437770752, 5767168, 178782208, 0), (23426236416, 5767168, 184549376, 0), (23449305088, 5767168, 190316544, 0), (23455072256, 5767168, 196083712, 0), (23443537920, 5767168, 201850880, 0), (23466606592, 5767168, 207618048, 0), (23472373760, 5767168, 213385216, 0), (23460839424, 5767168, 219152384, 0), (23483908096, 5767168, 224919552, 0), (23489675264, 5767168, 230686720, 0), (23478140928, 5767168, 236453888, 0), (23535812608, 5767168, 242221056, 0), (23541579776, 5767168, 247988224, 0), (23530045440, 5767168, 253755392, 0), (23587717120, 5767168, 259522560, 0), (23593484288, 5767168, 265289728, 0), (23581949952, 5767168, 271056896, 0), (23674224640, 5767168, 276824064, 0), (23679991808, 5767168, 282591232, 0), (23668457472, 5767168, 288358400, 0), (23726129152, 5767168, 294125568, 0), (23731896320, 5767168, 299892736, 0), (23720361984, 5767168, 305659904, 0), (23847239680, 5767168, 311427072, 0), (23853006848, 5767168, 317194240, 0), (23841472512, 5767168, 322961408, 0), (23881842688, 5767168, 328728576, 0), (23887609856, 5767168, 334495744, 0), (23876075520, 5767168, 340262912, 0)], 2: [(22826450944, 5767168, 0, 0), (22832218112, 5767168, 5767168, 0), (22820683776, 5767168, 11534336, 0), (22843752448, 5767168, 17301504, 0), (22849519616, 5767168, 23068672, 0), (22837985280, 5767168, 28835840, 0), (22861053952, 5767168, 34603008, 0), (22866821120, 5767168, 40370176, 0), (22855286784, 5767168, 46137344, 0), (22947561472, 5767168, 51904512, 0), (22953328640, 5767168, 57671680, 0), (22941794304, 5767168, 63438848, 0), (22982164480, 5767168, 69206016, 0), (22987931648, 5767168, 74973184, 0), (22976397312, 5767168, 80740352, 0), (23034068992, 5767168, 86507520, 0), (23039836160, 5767168, 92274688, 0), (23028301824, 5767168, 98041856, 0), (23085973504, 5767168, 103809024, 0), (23091740672, 5767168, 109576192, 0), (23080206336, 5767168, 115343360, 0), (23103275008, 5767168, 121110528, 0), (23109042176, 5767168, 126877696, 0), (23097507840, 5767168, 132644864, 0), (23120576512, 5767168, 138412032, 0), (23126343680, 5767168, 144179200, 0), (23114809344, 5767168, 149946368, 0), (23137878016, 5767168, 155713536, 0), (23143645184, 5767168, 161480704, 0), (23132110848, 5767168, 167247872, 0), (23224385536, 5767168, 173015040, 0), (23230152704, 5767168, 178782208, 0), (23218618368, 5767168, 184549376, 0), (23293591552, 5767168, 190316544, 0), (23299358720, 5767168, 196083712, 0), (23287824384, 5767168, 201850880, 0), (23328194560, 5767168, 207618048, 0), (23333961728, 5767168, 213385216, 0), (23322427392, 5767168, 219152384, 0), (23362797568, 5767168, 224919552, 0), (23368564736, 5767168, 230686720, 0), (23357030400, 5767168, 236453888, 0), (23501209600, 5767168, 242221056, 0), (23506976768, 5767168, 247988224, 0), (23495442432, 5767168, 253755392, 0), (23570415616, 5767168, 259522560, 0), (23576182784, 5767168, 265289728, 0), (23564648448, 5767168, 271056896, 0), (23639621632, 5767168, 276824064, 0), (23645388800, 5767168, 282591232, 0), (23633854464, 5767168, 288358400, 0), (23691526144, 5767168, 294125568, 0), (23697293312, 5767168, 299892736, 0), (23685758976, 5767168, 305659904, 0), (23708827648, 5767168, 311427072, 0), (23714594816, 5767168, 317194240, 0), (23703060480, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6805df360>, 2: <capsule object NULL at 0x74a6807aa3d0>}
DEBUG 01-15 16:09:09.144661.144661 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:09:09.144971.144971 client.py:127] Model loaded
DEBUG 01-15 16:09:09.144109.144109 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.144406.144406 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.144254.144254 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 833eb62f-38e2-442e-bb63-c2017498e16c
DEBUG 01-15 16:09:09.144423.144423 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.145213.145213 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:09.144530.144530 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.146160.146160 cuda_h.py:19] end move_flatidxs cost 0.0008993148803710938 seconds
DEBUG 01-15 16:09:09.146718.146718 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:09.146375.146375 cuda_h.py:19] end restore2model cost 0.0010464191436767578 seconds
DEBUG 01-15 16:09:09.146221.146221 cuda_h.py:19] end sllm_worker_task cost 0.014578819274902344 seconds
INFO 01-15 16:09:09.147772.147772 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 833eb62f-38e2-442e-bb63-c2017498e16c
DEBUG 01-15 16:09:09.148287.148287 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005609035491943359 seconds
DEBUG 01-15 16:09:09.148537.148537 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.151351.151351 cuda_h.py:19] end group_tensors cost 0.0057888031005859375 seconds
DEBUG 01-15 16:09:09.152490.152490 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.153497.153497 cuda_h.py:19] end restore2model cost 0.005068778991699219 seconds
DEBUG 01-15 16:09:09.153938.153938 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.011114835739135742 seconds
DEBUG 01-15 16:09:09.153754.153754 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.154111.154111 cuda_h.py:19] end gpu_sexperts cost 0.0007405281066894531 seconds
DEBUG 01-15 16:09:09.154074.154074 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.154163.154163 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1219253540039062e-05 seconds
DEBUG 01-15 16:09:09.154356.154356 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.154847.154847 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.156736.156736 cuda_h.py:19] end group pad cost 0.0035355091094970703 seconds
DEBUG 01-15 16:09:09.156526.156526 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:09.160194.160194 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.005465984344482422 seconds
DEBUG 01-15 16:09:09.161616.161616 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.163602.163602 cuda_h.py:19] end gpu_group_list cost 0.0010428428649902344 seconds
DEBUG 01-15 16:09:09.166972.166972 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.175966.175966 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.009023427963256836 seconds
DEBUG 01-15 16:09:09.176808.176808 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.176313.176313 cuda_h.py:19] end gpu_group_list cost 0.0003924369812011719 seconds
DEBUG 01-15 16:09:09.178811.178811 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.178904.178904 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 833eb62f-38e2-442e-bb63-c2017498e16c
INFO 01-15 16:09:09.182717.182717 client.py:127] Model loaded
DEBUG 01-15 16:09:09.182486.182486 cuda_h.py:19] end wait_experts_multi_device cost 0.004118204116821289 seconds
DEBUG 01-15 16:09:09.182998.182998 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.184651.184651 cuda_h.py:19] end group_einsum cost 0.027553558349609375 seconds
DEBUG 01-15 16:09:09.184582.184582 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:09.187109.187109 cuda_h.py:19] end get_outputs_cpu1 cost 0.0028197765350341797 seconds
DEBUG 01-15 16:09:09.187753.187753 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04310870170593262 seconds
DEBUG 01-15 16:09:09.188981.188981 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005566596984863281 seconds
DEBUG 01-15 16:09:09.188304.188304 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.190084.190084 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.190462.190462 cuda_h.py:19] end index_scatter cost 0.00011181831359863281 seconds
DEBUG 01-15 16:09:09.190241.190241 cuda_h.py:19] end cpuoutputsdeal cost 0.0018241405487060547 seconds
DEBUG 01-15 16:09:09.190006.190006 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.190935.190935 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.190089.190089 cuda_h.py:19] end gpu_group_tensor cost 0.00020432472229003906 seconds
DEBUG 01-15 16:09:09.191833.191833 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.191430.191430 cuda_h.py:19] end gpu_group_tensor cost 0.0001838207244873047 seconds
DEBUG 01-15 16:09:09.191250.191250 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.192039.192039 cuda_h.py:19] end gpu_group_einsum cost 0.0007584095001220703 seconds
DEBUG 01-15 16:09:09.192939.192939 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.193864.193864 cuda_h.py:19] end gpu_group_einsum cost 0.0008287429809570312 seconds
DEBUG 01-15 16:09:09.193826.193826 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.193135.193135 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.193798.193798 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020265579223632812 seconds
DEBUG 01-15 16:09:09.193223.193223 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.194843.194843 cuda_h.py:19] end concat_expert_out cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:09:09.194514.194514 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.194471.194471 cuda_h.py:19] end index_scatter cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:09:09.194130.194130 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009050369262695312 seconds
DEBUG 01-15 16:09:09.194895.194895 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.194129.194129 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.194387.194387 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015807151794433594 seconds
DEBUG 01-15 16:09:09.194951.194951 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.195073.195073 cuda_h.py:19] end concat_expert_out cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:09:09.195870.195870 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.195131.195131 cuda_h.py:19] end index_scatter cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:09:09.195702.195702 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000522613525390625 seconds
DEBUG 01-15 16:09:09.195506.195506 cuda_h.py:19] end gpu_experts_multi_device cost 0.04045581817626953 seconds
DEBUG 01-15 16:09:09.195377.195377 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.05611276626586914 seconds
DEBUG 01-15 16:09:09.195341.195341 cuda_h.py:19] end prefill_layer cost 0.0648794174194336 seconds
DEBUG 01-15 16:09:09.195761.195761 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:09:09.196656.196656 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.196551.196551 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:09:09.196684.196684 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:09:09.196248.196248 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:09:09.196549.196549 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:09.196689.196689 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 8.821487426757812e-05 seconds
DEBUG 01-15 16:09:09.196292.196292 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.196281.196281 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:09.196940.196940 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:09.196075.196075 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:09.196692.196692 cuda_h.py:19] end allocate_cuda_memory cost 0.0003802776336669922 seconds
DEBUG 01-15 16:09:09.197338.197338 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.197373.197373 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:09.197920.197920 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.197426.197426 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.197897.197897 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3a521b4e-b685-4bd9-a831-cc64c617ed3b
DEBUG 01-15 16:09:09.197139.197139 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.197369.197369 cuda_h.py:10] start self_attn
INFO 01-15 16:09:09.199511.199511 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3a521b4e-b685-4bd9-a831-cc64c617ed3b
DEBUG 01-15 16:09:09.199354.199354 cuda_h.py:19] end load_into_gpu_async cost 0.001865386962890625 seconds
DEBUG 01-15 16:09:09.199202.199202 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:09.199650.199650 cuda_h.py:19] end restore_tensors2 cost 8.821487426757812e-05 seconds
DEBUG 01-15 16:09:09.199982.199982 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002803802490234375 seconds
INFO 01-15 16:09:09.199064.199064 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3a521b4e-b685-4bd9-a831-cc64c617ed3b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.201184.201184 cuda_h.py:19] end self_attn cost 0.0035157203674316406 seconds
DEBUG 01-15 16:09:09.201870.201870 cuda_h.py:19] end iln_self_attn_paln cost 0.005343437194824219 seconds
DEBUG 01-15 16:09:09.201051.201051 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:09:09.201688.201688 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.202373.202373 cuda_h.py:19] end gate cost 0.0006465911865234375 seconds
DEBUG 01-15 16:09:09.202487.202487 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.202320.202320 lmp.py:1912] 
DEBUG 01-15 16:09:09.202320.202320 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.202698.202698 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.202779.202779 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.202567.202567 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.202926.202926 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.202376.202376 lmp.py:1917] 
DEBUG 01-15 16:09:09.202376.202376 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.202258.202258 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.202100.202100 lmp.py:1935]   Expert 54 |     21 | CPU
DEBUG 01-15 16:09:09.202650.202650 lmp.py:1935]   Expert  3 |     35 | CPU
DEBUG 01-15 16:09:09.202008.202008 lmp.py:1935]   Expert  8 |     40 | CPU
DEBUG 01-15 16:09:09.203651.203651 lmp.py:1935]   Expert 28 |     43 | CPU
DEBUG 01-15 16:09:09.203817.203817 lmp.py:1935]   Expert 43 |     55 | CPU
DEBUG 01-15 16:09:09.203222.203222 lmp.py:1935]   Expert 63 |     55 | CPU
DEBUG 01-15 16:09:09.203626.203626 lmp.py:1935]   Expert 36 |     74 | CPU
DEBUG 01-15 16:09:09.203793.203793 lmp.py:1935]   Expert  6 |     79 | CPU
DEBUG 01-15 16:09:09.203959.203959 lmp.py:1935]   Expert 38 |     79 | CPU
DEBUG 01-15 16:09:09.203079.203079 lmp.py:1935]   Expert 39 |     96 | CPU
DEBUG 01-15 16:09:09.203722.203722 lmp.py:1935]   Expert 57 |     96 | CPU
DEBUG 01-15 16:09:09.203126.203126 lmp.py:1935]   Expert 41 |    102 | CPU
DEBUG 01-15 16:09:09.203815.203815 lmp.py:1935]   Expert 12 |    108 | CPU
DEBUG 01-15 16:09:09.203982.203982 lmp.py:1935]   Expert 52 |    116 | CPU
DEBUG 01-15 16:09:09.203909.203909 lmp.py:1935]   Expert 19 |    119 | CPU
DEBUG 01-15 16:09:09.203837.203837 lmp.py:1935]   Expert 47 |    124 | CPU
DEBUG 01-15 16:09:09.203526.203526 lmp.py:1935]   Expert 13 |    135 | CPU
DEBUG 01-15 16:09:09.203692.203692 lmp.py:1935]   Expert 22 |    138 | CPU
DEBUG 01-15 16:09:09.203620.203620 lmp.py:1935]   Expert 50 |    142 | CPU
DEBUG 01-15 16:09:09.203025.203025 lmp.py:1935]   Expert 46 |    148 | CPU
DEBUG 01-15 16:09:09.203668.203668 lmp.py:1935]   Expert 24 |    162 | CPU
DEBUG 01-15 16:09:09.203834.203834 lmp.py:1935]   Expert 40 |    164 | CPU
DEBUG 01-15 16:09:09.203523.203523 lmp.py:1935]   Expert 20 |    167 | CPU
DEBUG 01-15 16:09:09.203212.203212 lmp.py:1935]   Expert 55 |    167 | CPU
DEBUG 01-15 16:09:09.203902.203902 lmp.py:1935]   Expert 23 |    169 | CPU
DEBUG 01-15 16:09:09.203975.203975 lmp.py:1935]   Expert 37 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203095.203095 lmp.py:1935]   Expert 53 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203215.203215 lmp.py:1935]   Expert  2 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203096.203096 lmp.py:1935]   Expert 61 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203931.203931 lmp.py:1935]   Expert 21 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203005.203005 lmp.py:1935]   Expert 42 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203601.203601 lmp.py:1935]   Expert 49 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203483.203483 lmp.py:1935]   Expert 33 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203126.203126 lmp.py:1935]   Expert 18 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203530.203530 lmp.py:1935]   Expert 32 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203173.203173 lmp.py:1935]   Expert  0 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203578.203578 lmp.py:1935]   Expert 30 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203221.203221 lmp.py:1935]   Expert  5 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203341.203341 lmp.py:1935]   Expert 16 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203460.203460 lmp.py:1935]   Expert 14 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203865.203865 lmp.py:1935]   Expert  7 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203746.203746 lmp.py:1935]   Expert 31 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203389.203389 lmp.py:1935]   Expert 34 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203794.203794 lmp.py:1935]   Expert 60 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203199.203199 lmp.py:1935]   Expert 62 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203318.203318 lmp.py:1935]   Expert 59 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203961.203961 lmp.py:1935]   Expert  9 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203843.203843 lmp.py:1935]   Expert 17 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203962.203962 lmp.py:1935]   Expert 10 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203321.203321 lmp.py:1935]   Expert 29 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203202.203202 lmp.py:1935]   Expert  4 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203084.203084 lmp.py:1935]   Expert 15 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203965.203965 lmp.py:1935]   Expert 58 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203608.203608 lmp.py:1935]   Expert 26 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203251.203251 lmp.py:1935]   Expert 51 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203609.203609 lmp.py:1935]   Expert 11 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203252.203252 lmp.py:1935]   Expert 44 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.203372.203372 lmp.py:1935]   Expert 56 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203776.203776 lmp.py:1935]   Expert 27 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.203419.203419 lmp.py:1935]   Expert  1 |    328 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.204778.204778 lmp.py:1935]   Expert 45 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.204374.204374 lmp.py:1935]   Expert 25 |    461 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.204256.204256 lmp.py:1935]   Expert 35 |    521 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.204899.204899 lmp.py:1935]   Expert 48 |    647 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.204350.204350 lmp.py:1937] 
DEBUG 01-15 16:09:09.204350.204350 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.204993.204993 lmp.py:1938]   CPU:   2634 tokens
DEBUG 01-15 16:09:09.204112.204112 lmp.py:1942]   cuda:1:   4912 tokens (20 experts)
DEBUG 01-15 16:09:09.204232.204232 lmp.py:1942]   cuda:2:   4742 tokens (19 experts)
DEBUG 01-15 16:09:09.204398.204398 lmp.py:1943]   Total GPU:   9654 tokens
DEBUG 01-15 16:09:09.204803.204803 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.204803.204803 lmp.py:1944] 
DEBUG 01-15 16:09:09.204168.204168 cuda_h.py:19] end experts_map_get cost 0.0017020702362060547 seconds
DEBUG 01-15 16:09:09.204110.204110 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.204959.204959 lmp.py:1953] 
DEBUG 01-15 16:09:09.204959.204959 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.204650.204650 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:09.204392.204392 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.204362.204362 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.205140.205140 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.2.gate_proj.weight': 0, 'model.layers.20.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.20.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.20.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.20.mlp.experts.62.up_proj.weight': 340262912}, 2: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.26.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.26.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.26.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.31.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.31.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.31.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.34.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.34.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.34.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.35.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.35.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.35.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.59.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(23933747200, 5767168, 0, 0), (23939514368, 5767168, 5767168, 0), (23927980032, 5767168, 11534336, 0), (23985651712, 5767168, 17301504, 0), (23991418880, 5767168, 23068672, 0), (23979884544, 5767168, 28835840, 0), (24020254720, 5767168, 34603008, 0), (24026021888, 5767168, 40370176, 0), (24014487552, 5767168, 46137344, 0), (24158666752, 5767168, 51904512, 0), (24164433920, 5767168, 57671680, 0), (24152899584, 5767168, 63438848, 0), (24193269760, 5767168, 69206016, 0), (24199036928, 5767168, 74973184, 0), (24187502592, 5767168, 80740352, 0), (24210571264, 5767168, 86507520, 0), (24216338432, 5767168, 92274688, 0), (24204804096, 5767168, 98041856, 0), (24366284800, 5767168, 103809024, 0), (24372051968, 5767168, 109576192, 0), (24360517632, 5767168, 115343360, 0), (24400887808, 5767168, 121110528, 0), (24406654976, 5767168, 126877696, 0), (24395120640, 5767168, 132644864, 0), (24418189312, 5767168, 138412032, 0), (24423956480, 5767168, 144179200, 0), (24412422144, 5767168, 149946368, 0), (24452792320, 5767168, 155713536, 0), (24458559488, 5767168, 161480704, 0), (24447025152, 5767168, 167247872, 0), (24539299840, 5767168, 173015040, 0), (24545067008, 5767168, 178782208, 0), (24533532672, 5767168, 184549376, 0), (24625807360, 5767168, 190316544, 0), (24631574528, 5767168, 196083712, 0), (24620040192, 5767168, 201850880, 0), (24677711872, 5767168, 207618048, 0), (24683479040, 5767168, 213385216, 0), (24671944704, 5767168, 219152384, 0), (24729616384, 5767168, 224919552, 0), (24735383552, 5767168, 230686720, 0), (24723849216, 5767168, 236453888, 0), (24781520896, 5767168, 242221056, 0), (24787288064, 5767168, 247988224, 0), (24775753728, 5767168, 253755392, 0), (24868028416, 5767168, 259522560, 0), (24873795584, 5767168, 265289728, 0), (24862261248, 5767168, 271056896, 0), (24902631424, 5767168, 276824064, 0), (24908398592, 5767168, 282591232, 0), (24896864256, 5767168, 288358400, 0), (24937234432, 5767168, 294125568, 0), (24943001600, 5767168, 299892736, 0), (24931467264, 5767168, 305659904, 0), (24954535936, 5767168, 311427072, 0), (24960303104, 5767168, 317194240, 0), (24948768768, 5767168, 322961408, 0), (24971837440, 5767168, 328728576, 0), (24977604608, 5767168, 334495744, 0), (24966070272, 5767168, 340262912, 0)], 2: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (23916445696, 5767168, 17301504, 0), (23922212864, 5767168, 23068672, 0), (23910678528, 5767168, 28835840, 0), (23968350208, 5767168, 34603008, 0), (23974117376, 5767168, 40370176, 0), (23962583040, 5767168, 46137344, 0), (24054857728, 5767168, 51904512, 0), (24060624896, 5767168, 57671680, 0), (24049090560, 5767168, 63438848, 0), (24072159232, 5767168, 69206016, 0), (24077926400, 5767168, 74973184, 0), (24066392064, 5767168, 80740352, 0), (24089460736, 5767168, 86507520, 0), (24095227904, 5767168, 92274688, 0), (24083693568, 5767168, 98041856, 0), (24141365248, 5767168, 103809024, 0), (24147132416, 5767168, 109576192, 0), (24135598080, 5767168, 115343360, 0), (24175968256, 5767168, 121110528, 0), (24181735424, 5767168, 126877696, 0), (24170201088, 5767168, 132644864, 0), (24262475776, 5767168, 138412032, 0), (24268242944, 5767168, 144179200, 0), (24256708608, 5767168, 149946368, 0), (24331681792, 5767168, 155713536, 0), (24337448960, 5767168, 161480704, 0), (24325914624, 5767168, 167247872, 0), (24348983296, 5767168, 173015040, 0), (24354750464, 5767168, 178782208, 0), (24343216128, 5767168, 184549376, 0), (24435490816, 5767168, 190316544, 0), (24441257984, 5767168, 196083712, 0), (24429723648, 5767168, 201850880, 0), (24470093824, 5767168, 207618048, 0), (24475860992, 5767168, 213385216, 0), (24464326656, 5767168, 219152384, 0), (24487395328, 5767168, 224919552, 0), (24493162496, 5767168, 230686720, 0), (24481628160, 5767168, 236453888, 0), (24504696832, 5767168, 242221056, 0), (24510464000, 5767168, 247988224, 0), (24498929664, 5767168, 253755392, 0), (24660410368, 5767168, 259522560, 0), (24666177536, 5767168, 265289728, 0), (24654643200, 5767168, 271056896, 0), (24746917888, 5767168, 276824064, 0), (24752685056, 5767168, 282591232, 0), (24741150720, 5767168, 288358400, 0), (24816123904, 5767168, 294125568, 0), (24821891072, 5767168, 299892736, 0), (24810356736, 5767168, 305659904, 0), (24919932928, 5767168, 311427072, 0), (24925700096, 5767168, 317194240, 0), (24914165760, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a81475be10>, 2: <capsule object NULL at 0x74a6807aa2e0>}
DEBUG 01-15 16:09:09.205349.205349 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.205239.205239 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.205849.205849 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 530d140d-e0d2-40be-92e3-4ab8c7aaa8f2
DEBUG 01-15 16:09:09.205995.205995 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:09.206465.206465 client.py:127] Model loaded
DEBUG 01-15 16:09:09.206831.206831 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.206816.206816 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.206872.206872 cuda_h.py:19] end restore2model cost 0.00037741661071777344 seconds
DEBUG 01-15 16:09:09.206549.206549 cuda_h.py:19] end sllm_worker_task cost 0.010464191436767578 seconds
DEBUG 01-15 16:09:09.207529.207529 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:09.207267.207267 cuda_h.py:19] end move_flatidxs cost 0.0008342266082763672 seconds
DEBUG 01-15 16:09:09.208236.208236 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:09.208507.208507 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 530d140d-e0d2-40be-92e3-4ab8c7aaa8f2
DEBUG 01-15 16:09:09.208955.208955 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004232645034790039 seconds
DEBUG 01-15 16:09:09.208315.208315 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.211551.211551 cuda_h.py:19] end restore2model cost 0.0030541419982910156 seconds
DEBUG 01-15 16:09:09.211533.211533 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007561445236206055 seconds
DEBUG 01-15 16:09:09.211660.211660 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.212127.212127 cuda_h.py:19] end gpu_sexperts cost 0.0002799034118652344 seconds
DEBUG 01-15 16:09:09.212096.212096 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.212342.212342 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.430511474609375e-05 seconds
DEBUG 01-15 16:09:09.212475.212475 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.212344.212344 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.213843.213843 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009982585906982422 seconds
DEBUG 01-15 16:09:09.213408.213408 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.213138.213138 cuda_h.py:19] end gpu_group_list cost 0.00022935867309570312 seconds
DEBUG 01-15 16:09:09.214889.214889 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.215885.215885 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009889602661132812 seconds
DEBUG 01-15 16:09:09.215410.215410 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.216928.216928 cuda_h.py:19] end gpu_group_list cost 0.00021195411682128906 seconds
DEBUG 01-15 16:09:09.216342.216342 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.216556.216556 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 530d140d-e0d2-40be-92e3-4ab8c7aaa8f2
DEBUG 01-15 16:09:09.216127.216127 cuda_h.py:19] end group_tensors cost 0.00858926773071289 seconds
DEBUG 01-15 16:09:09.217112.217112 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.220599.220599 cuda_h.py:19] end group pad cost 0.003431558609008789 seconds
DEBUG 01-15 16:09:09.220389.220389 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:09.244455.244455 client.py:127] Model loaded
DEBUG 01-15 16:09:09.244197.244197 cuda_h.py:19] end wait_experts_multi_device cost 0.027924537658691406 seconds
DEBUG 01-15 16:09:09.245641.245641 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.248446.248446 cuda_h.py:19] end group_einsum cost 0.027903318405151367 seconds
DEBUG 01-15 16:09:09.249941.249941 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:09.251093.251093 cuda_h.py:19] end get_outputs_cpu1 cost 0.0026671886444091797 seconds
DEBUG 01-15 16:09:09.252770.252770 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04581856727600098 seconds
DEBUG 01-15 16:09:09.253668.253668 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008400678634643555 seconds
DEBUG 01-15 16:09:09.253940.253940 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.255138.255138 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.256182.256182 cuda_h.py:19] end index_scatter cost 0.00014925003051757812 seconds
DEBUG 01-15 16:09:09.256782.256782 cuda_h.py:19] end cpuoutputsdeal cost 0.0029015541076660156 seconds
DEBUG 01-15 16:09:09.256828.256828 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.257850.257850 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.257896.257896 cuda_h.py:19] end gpu_group_tensor cost 0.0003037452697753906 seconds
DEBUG 01-15 16:09:09.257827.257827 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.257269.257269 cuda_h.py:19] end gpu_group_tensor cost 0.0002827644348144531 seconds
DEBUG 01-15 16:09:09.258906.258906 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.259824.259824 cuda_h.py:19] end gpu_group_einsum cost 0.001386880874633789 seconds
DEBUG 01-15 16:09:09.259311.259311 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.260460.260460 cuda_h.py:19] end gpu_group_einsum cost 0.0006463527679443359 seconds
DEBUG 01-15 16:09:09.260419.260419 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.260688.260688 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.261420.261420 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002448558807373047 seconds
DEBUG 01-15 16:09:09.261189.261189 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.261285.261285 cuda_h.py:19] end concat_expert_out cost 6.127357482910156e-05 seconds
DEBUG 01-15 16:09:09.261043.261043 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.261808.261808 cuda_h.py:19] end index_scatter cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:09:09.261108.261108 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000942230224609375 seconds
DEBUG 01-15 16:09:09.261906.261906 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.261325.261325 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.262245.262245 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015592575073242188 seconds
DEBUG 01-15 16:09:09.262286.262286 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.262355.262355 cuda_h.py:19] end concat_expert_out cost 5.507469177246094e-05 seconds
DEBUG 01-15 16:09:09.262721.262721 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.262506.262506 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:09.262361.262361 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005099773406982422 seconds
DEBUG 01-15 16:09:09.262437.262437 cuda_h.py:19] end gpu_experts_multi_device cost 0.050037384033203125 seconds
DEBUG 01-15 16:09:09.262446.262446 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.06082725524902344 seconds
DEBUG 01-15 16:09:09.263530.263530 cuda_h.py:19] end prefill_layer cost 0.06701040267944336 seconds
DEBUG 01-15 16:09:09.263181.263181 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:09:09.263599.263599 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.263494.263494 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:09:09.263150.263150 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:09:09.263430.263430 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:09:09.263763.263763 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 4.315376281738281e-05 seconds
DEBUG 01-15 16:09:09.263426.263426 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 7.796287536621094e-05 seconds
DEBUG 01-15 16:09:09.263745.263745 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.263529.263529 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.263168.263168 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:09.263198.263198 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:09.263372.263372 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:09.264988.264988 cuda_h.py:19] end allocate_cuda_memory cost 0.00037860870361328125 seconds
DEBUG 01-15 16:09:09.264157.264157 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:09.264258.264258 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.264233.264233 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.264704.264704 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 460efa17-a267-46e1-81cc-5c021cbb0bbe
DEBUG 01-15 16:09:09.264682.264682 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.264765.264765 cuda_h.py:10] start self_attn
INFO 01-15 16:09:09.266963.266963 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 460efa17-a267-46e1-81cc-5c021cbb0bbe
DEBUG 01-15 16:09:09.266038.266038 cuda_h.py:19] end load_into_gpu_async cost 0.00194549560546875 seconds
DEBUG 01-15 16:09:09.266787.266787 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:09.266844.266844 cuda_h.py:19] end restore_tensors2 cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:09:09.266123.266123 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0026862621307373047 seconds
INFO 01-15 16:09:09.266529.266529 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 460efa17-a267-46e1-81cc-5c021cbb0bbe
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.267302.267302 cuda_h.py:19] end self_attn cost 0.0029218196868896484 seconds
DEBUG 01-15 16:09:09.268193.268193 cuda_h.py:19] end iln_self_attn_paln cost 0.004643440246582031 seconds
DEBUG 01-15 16:09:09.268115.268115 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:09:09.268395.268395 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.268821.268821 cuda_h.py:19] end gate cost 0.0006325244903564453 seconds
DEBUG 01-15 16:09:09.268220.268220 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.269323.269323 lmp.py:1912] 
DEBUG 01-15 16:09:09.269323.269323 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.269318.269318 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.269014.269014 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.269326.269326 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.269492.269492 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.269989.269989 lmp.py:1917] 
DEBUG 01-15 16:09:09.269989.269989 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.269678.269678 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.269851.269851 lmp.py:1935]   Expert 44 |     30 | CPU
DEBUG 01-15 16:09:09.269779.269779 lmp.py:1935]   Expert  9 |     34 | CPU
DEBUG 01-15 16:09:09.269753.269753 lmp.py:1935]   Expert 11 |     38 | CPU
DEBUG 01-15 16:09:09.269489.269489 lmp.py:1935]   Expert 56 |     59 | CPU
DEBUG 01-15 16:09:09.269986.269986 lmp.py:1935]   Expert 54 |     77 | CPU
DEBUG 01-15 16:09:09.269483.269483 lmp.py:1935]   Expert 62 |     93 | CPU
DEBUG 01-15 16:09:09.269172.269172 lmp.py:1935]   Expert  7 |     94 | CPU
DEBUG 01-15 16:09:09.269862.269862 lmp.py:1935]   Expert 47 |     96 | CPU
DEBUG 01-15 16:09:09.269359.269359 lmp.py:1935]   Expert 51 |    100 | CPU
DEBUG 01-15 16:09:09.269618.269618 lmp.py:1935]   Expert 60 |    106 | CPU
DEBUG 01-15 16:09:09.269400.269400 lmp.py:1935]   Expert 22 |    109 | CPU
DEBUG 01-15 16:09:09.269897.269897 lmp.py:1935]   Expert 52 |    109 | CPU
DEBUG 01-15 16:09:09.269156.269156 lmp.py:1935]   Expert 41 |    110 | CPU
DEBUG 01-15 16:09:09.269414.269414 lmp.py:1935]   Expert 53 |    110 | CPU
DEBUG 01-15 16:09:09.269435.269435 lmp.py:1935]   Expert 48 |    124 | CPU
DEBUG 01-15 16:09:09.269932.269932 lmp.py:1935]   Expert  1 |    127 | CPU
DEBUG 01-15 16:09:09.269191.269191 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:09:09.269403.269403 lmp.py:1935]   Expert  8 |    127 | CPU
DEBUG 01-15 16:09:09.269616.269616 lmp.py:1935]   Expert 32 |    129 | CPU
DEBUG 01-15 16:09:09.269875.269875 lmp.py:1935]   Expert  2 |    131 | CPU
DEBUG 01-15 16:09:09.269895.269895 lmp.py:1935]   Expert 27 |    140 | CPU
DEBUG 01-15 16:09:09.269392.269392 lmp.py:1935]   Expert 59 |    140 | CPU
DEBUG 01-15 16:09:09.269412.269412 lmp.py:1935]   Expert 23 |    141 | CPU
DEBUG 01-15 16:09:09.269671.269671 lmp.py:1935]   Expert 35 |    141 | CPU
DEBUG 01-15 16:09:09.269692.269692 lmp.py:1935]   Expert 39 |    146 | CPU
DEBUG 01-15 16:09:09.269619.269619 lmp.py:1935]   Expert 26 |    148 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.269454.269454 lmp.py:1935]   Expert 50 |    150 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.269574.269574 lmp.py:1935]   Expert 14 |    156 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.269932.269932 lmp.py:1935]   Expert 46 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.269337.269337 lmp.py:1935]   Expert 24 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.269980.269980 lmp.py:1935]   Expert 34 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.269385.269385 lmp.py:1935]   Expert 38 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.269789.269789 lmp.py:1935]   Expert  0 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.269717.269717 lmp.py:1935]   Expert 49 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.269883.269883 lmp.py:1935]   Expert  4 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.269049.269049 lmp.py:1935]   Expert 40 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.269977.269977 lmp.py:1935]   Expert  5 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.269620.269620 lmp.py:1935]   Expert 63 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.269760.269760 lmp.py:1935]   Expert 19 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.269880.269880 lmp.py:1935]   Expert 13 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.269284.269284 lmp.py:1935]   Expert 29 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.269689.269689 lmp.py:1935]   Expert 43 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270093.270093 lmp.py:1935]   Expert 57 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270498.270498 lmp.py:1935]   Expert 61 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270141.270141 lmp.py:1935]   Expert 33 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270546.270546 lmp.py:1935]   Expert 31 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270427.270427 lmp.py:1935]   Expert 16 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270831.270831 lmp.py:1935]   Expert  3 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270474.270474 lmp.py:1935]   Expert 37 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270117.270117 lmp.py:1935]   Expert 20 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270714.270714 lmp.py:1935]   Expert 15 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270595.270595 lmp.py:1935]   Expert 36 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270238.270238 lmp.py:1935]   Expert 18 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270881.270881 lmp.py:1935]   Expert 12 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270524.270524 lmp.py:1935]   Expert 17 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270929.270929 lmp.py:1935]   Expert 28 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270857.270857 lmp.py:1935]   Expert 55 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270261.270261 lmp.py:1935]   Expert 30 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270427.270427 lmp.py:1935]   Expert 25 |    327 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270024.270024 lmp.py:1935]   Expert 58 |    337 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270382.270382 lmp.py:1935]   Expert 10 |    362 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270787.270787 lmp.py:1935]   Expert 45 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270191.270191 lmp.py:1935]   Expert 21 |    389 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.270358.270358 lmp.py:1935]   Expert 42 |    644 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.270285.270285 lmp.py:1937] 
DEBUG 01-15 16:09:09.270285.270285 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.270213.270213 lmp.py:1938]   CPU:   2638 tokens
DEBUG 01-15 16:09:09.270810.270810 lmp.py:1942]   cuda:1:   4822 tokens (19 experts)
DEBUG 01-15 16:09:09.270691.270691 lmp.py:1942]   cuda:2:   4828 tokens (20 experts)
DEBUG 01-15 16:09:09.270818.270818 lmp.py:1943]   Total GPU:   9650 tokens
DEBUG 01-15 16:09:09.270984.270984 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.270984.270984 lmp.py:1944] 
DEBUG 01-15 16:09:09.270680.270680 cuda_h.py:19] end experts_map_get cost 0.0016453266143798828 seconds
DEBUG 01-15 16:09:09.270477.270477 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.270325.270325 lmp.py:1953] 
DEBUG 01-15 16:09:09.270325.270325 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.270731.270731 cuda_h.py:19] end cpu_experts_submit cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:09.270281.270281 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.270833.270833 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.272455.272455 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.0.gate_proj.weight': 0, 'model.layers.21.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.28.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.28.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.28.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.38.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.38.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.38.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.42.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.42.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.42.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.57.up_proj.weight': 322961408}, 2: {'model.layers.21.mlp.experts.3.gate_proj.weight': 0, 'model.layers.21.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.12.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.12.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.12.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.13.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.13.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.13.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.17.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.17.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.17.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.21.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.21.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.21.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(25006440448, 5767168, 0, 0), (25012207616, 5767168, 5767168, 0), (25000673280, 5767168, 11534336, 0), (25075646464, 5767168, 17301504, 0), (25081413632, 5767168, 23068672, 0), (25069879296, 5767168, 28835840, 0), (25092947968, 5767168, 34603008, 0), (25098715136, 5767168, 40370176, 0), (25087180800, 5767168, 46137344, 0), (25179455488, 5767168, 51904512, 0), (25185222656, 5767168, 57671680, 0), (25173688320, 5767168, 63438848, 0), (25248661504, 5767168, 69206016, 0), (25254428672, 5767168, 74973184, 0), (25242894336, 5767168, 80740352, 0), (25265963008, 5767168, 86507520, 0), (25271730176, 5767168, 92274688, 0), (25260195840, 5767168, 98041856, 0), (25283264512, 5767168, 103809024, 0), (25289031680, 5767168, 109576192, 0), (25277497344, 5767168, 115343360, 0), (25317867520, 5767168, 121110528, 0), (25323634688, 5767168, 126877696, 0), (25312100352, 5767168, 132644864, 0), (25335169024, 5767168, 138412032, 0), (25340936192, 5767168, 144179200, 0), (25329401856, 5767168, 149946368, 0), (25438978048, 5767168, 155713536, 0), (25444745216, 5767168, 161480704, 0), (25433210880, 5767168, 167247872, 0), (25490882560, 5767168, 173015040, 0), (25496649728, 5767168, 178782208, 0), (25485115392, 5767168, 184549376, 0), (25508184064, 5767168, 190316544, 0), (25513951232, 5767168, 196083712, 0), (25502416896, 5767168, 201850880, 0), (25577390080, 5767168, 207618048, 0), (25583157248, 5767168, 213385216, 0), (25571622912, 5767168, 219152384, 0), (25646596096, 5767168, 224919552, 0), (25652363264, 5767168, 230686720, 0), (25640828928, 5767168, 236453888, 0), (25663897600, 5767168, 242221056, 0), (25669664768, 5767168, 247988224, 0), (25658130432, 5767168, 253755392, 0), (25733103616, 5767168, 259522560, 0), (25738870784, 5767168, 265289728, 0), (25727336448, 5767168, 271056896, 0), (25871515648, 5767168, 276824064, 0), (25877282816, 5767168, 282591232, 0), (25865748480, 5767168, 288358400, 0), (25958023168, 5767168, 294125568, 0), (25963790336, 5767168, 299892736, 0), (25952256000, 5767168, 305659904, 0), (25992626176, 5767168, 311427072, 0), (25998393344, 5767168, 317194240, 0), (25986859008, 5767168, 322961408, 0)], 2: [(25058344960, 5767168, 0, 0), (25064112128, 5767168, 5767168, 0), (25052577792, 5767168, 11534336, 0), (25214058496, 5767168, 17301504, 0), (25219825664, 5767168, 23068672, 0), (25208291328, 5767168, 28835840, 0), (25231360000, 5767168, 34603008, 0), (25237127168, 5767168, 40370176, 0), (25225592832, 5767168, 46137344, 0), (25300566016, 5767168, 51904512, 0), (25306333184, 5767168, 57671680, 0), (25294798848, 5767168, 63438848, 0), (25352470528, 5767168, 69206016, 0), (25358237696, 5767168, 74973184, 0), (25346703360, 5767168, 80740352, 0), (25369772032, 5767168, 86507520, 0), (25375539200, 5767168, 92274688, 0), (25364004864, 5767168, 98041856, 0), (25421676544, 5767168, 103809024, 0), (25427443712, 5767168, 109576192, 0), (25415909376, 5767168, 115343360, 0), (25456279552, 5767168, 121110528, 0), (25462046720, 5767168, 126877696, 0), (25450512384, 5767168, 132644864, 0), (25525485568, 5767168, 138412032, 0), (25531252736, 5767168, 144179200, 0), (25519718400, 5767168, 149946368, 0), (25542787072, 5767168, 155713536, 0), (25548554240, 5767168, 161480704, 0), (25537019904, 5767168, 167247872, 0), (25594691584, 5767168, 173015040, 0), (25600458752, 5767168, 178782208, 0), (25588924416, 5767168, 184549376, 0), (25629294592, 5767168, 190316544, 0), (25635061760, 5767168, 196083712, 0), (25623527424, 5767168, 201850880, 0), (25698500608, 5767168, 207618048, 0), (25704267776, 5767168, 213385216, 0), (25692733440, 5767168, 219152384, 0), (25750405120, 5767168, 224919552, 0), (25756172288, 5767168, 230686720, 0), (25744637952, 5767168, 236453888, 0), (25785008128, 5767168, 242221056, 0), (25790775296, 5767168, 247988224, 0), (25779240960, 5767168, 253755392, 0), (25802309632, 5767168, 259522560, 0), (25808076800, 5767168, 265289728, 0), (25796542464, 5767168, 271056896, 0), (25854214144, 5767168, 276824064, 0), (25859981312, 5767168, 282591232, 0), (25848446976, 5767168, 288358400, 0), (26009927680, 5767168, 294125568, 0), (26015694848, 5767168, 299892736, 0), (26004160512, 5767168, 305659904, 0), (26061832192, 5767168, 311427072, 0), (26067599360, 5767168, 317194240, 0), (26056065024, 5767168, 322961408, 0), (26096435200, 5767168, 328728576, 0), (26102202368, 5767168, 334495744, 0), (26090668032, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a688603db0>, 2: <capsule object NULL at 0x74a6bc5efed0>}
DEBUG 01-15 16:09:09.272010.272010 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.272939.272939 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.272311.272311 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ebbe61c4-4c88-4029-9cdc-59843fbeb728
DEBUG 01-15 16:09:09.273239.273239 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.273506.273506 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.273190.273190 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:09.274941.274941 client.py:127] Model loaded
DEBUG 01-15 16:09:09.274671.274671 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.274312.274312 cuda_h.py:19] end restore2model cost 0.0003368854522705078 seconds
DEBUG 01-15 16:09:09.274936.274936 cuda_h.py:19] end sllm_worker_task cost 0.010982036590576172 seconds
DEBUG 01-15 16:09:09.274127.274127 cuda_h.py:19] end move_flatidxs cost 0.0008397102355957031 seconds
DEBUG 01-15 16:09:09.274380.274380 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:09.275830.275830 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ebbe61c4-4c88-4029-9cdc-59843fbeb728
DEBUG 01-15 16:09:09.275680.275680 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004912137985229492 seconds
DEBUG 01-15 16:09:09.275571.275571 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.278098.278098 cuda_h.py:19] end restore2model cost 0.0030591487884521484 seconds
DEBUG 01-15 16:09:09.278796.278796 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008223295211791992 seconds
DEBUG 01-15 16:09:09.278373.278373 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.279165.279165 cuda_h.py:19] end gpu_sexperts cost 0.00027179718017578125 seconds
DEBUG 01-15 16:09:09.279087.279087 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.279049.279049 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.430511474609375e-05 seconds
DEBUG 01-15 16:09:09.279745.279745 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.279640.279640 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.280360.280360 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009212493896484375 seconds
DEBUG 01-15 16:09:09.280455.280455 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.280066.280066 cuda_h.py:19] end gpu_group_list cost 0.00020956993103027344 seconds
DEBUG 01-15 16:09:09.281490.281490 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.282501.282501 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010366439819335938 seconds
DEBUG 01-15 16:09:09.282265.282265 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.282220.282220 cuda_h.py:19] end gpu_group_list cost 0.00021910667419433594 seconds
DEBUG 01-15 16:09:09.283243.283243 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.283549.283549 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ebbe61c4-4c88-4029-9cdc-59843fbeb728
DEBUG 01-15 16:09:09.284773.284773 cuda_h.py:19] end group_tensors cost 0.010116338729858398 seconds
DEBUG 01-15 16:09:09.285985.285985 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.289280.289280 cuda_h.py:19] end group pad cost 0.003818035125732422 seconds
DEBUG 01-15 16:09:09.289262.289262 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:09.310740.310740 client.py:127] Model loaded
DEBUG 01-15 16:09:09.310487.310487 cuda_h.py:19] end wait_experts_multi_device cost 0.02662491798400879 seconds
DEBUG 01-15 16:09:09.310634.310634 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.318289.318289 cuda_h.py:19] end group_einsum cost 0.02841019630432129 seconds
DEBUG 01-15 16:09:09.318037.318037 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:09.321943.321943 cuda_h.py:19] end get_outputs_cpu1 cost 0.0026717185974121094 seconds
DEBUG 01-15 16:09:09.321561.321561 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0484156608581543 seconds
DEBUG 01-15 16:09:09.322964.322964 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012516975402832031 seconds
DEBUG 01-15 16:09:09.323096.323096 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.325766.325766 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.325340.325340 cuda_h.py:19] end index_scatter cost 0.0001533031463623047 seconds
DEBUG 01-15 16:09:09.326782.326782 cuda_h.py:19] end cpuoutputsdeal cost 0.0029649734497070312 seconds
DEBUG 01-15 16:09:09.326344.326344 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.326843.326843 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.326373.326373 cuda_h.py:19] end gpu_group_tensor cost 0.0003063678741455078 seconds
DEBUG 01-15 16:09:09.326449.326449 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.327799.327799 cuda_h.py:19] end gpu_group_tensor cost 0.0002856254577636719 seconds
DEBUG 01-15 16:09:09.327820.327820 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.328910.328910 cuda_h.py:19] end gpu_group_einsum cost 0.0010073184967041016 seconds
DEBUG 01-15 16:09:09.328541.328541 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.329626.329626 cuda_h.py:19] end gpu_group_einsum cost 0.0008223056793212891 seconds
DEBUG 01-15 16:09:09.330621.330621 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.330257.330257 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.330110.330110 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004584789276123047 seconds
DEBUG 01-15 16:09:09.330603.330603 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.331298.331298 cuda_h.py:19] end concat_expert_out cost 0.00012040138244628906 seconds
DEBUG 01-15 16:09:09.331548.331548 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.331026.331026 cuda_h.py:19] end index_scatter cost 0.00012564659118652344 seconds
DEBUG 01-15 16:09:09.331698.331698 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001804351806640625 seconds
DEBUG 01-15 16:09:09.332698.332698 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.332087.332087 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.332791.332791 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003476142883300781 seconds
DEBUG 01-15 16:09:09.332847.332847 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.333542.333542 cuda_h.py:19] end concat_expert_out cost 0.00012111663818359375 seconds
DEBUG 01-15 16:09:09.333686.333686 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.333911.333911 cuda_h.py:19] end index_scatter cost 0.0001163482666015625 seconds
DEBUG 01-15 16:09:09.333464.333464 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012083053588867188 seconds
DEBUG 01-15 16:09:09.333688.333688 cuda_h.py:19] end gpu_experts_multi_device cost 0.05424332618713379 seconds
DEBUG 01-15 16:09:09.333747.333747 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.06569838523864746 seconds
DEBUG 01-15 16:09:09.334250.334250 cuda_h.py:19] end prefill_layer cost 0.07142925262451172 seconds
DEBUG 01-15 16:09:09.334844.334844 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:09:09.334508.334508 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.334604.334604 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:09:09.334532.334532 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:09:09.334057.334057 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:09:09.335656.335656 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:09.335803.335803 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 9.72747802734375e-05 seconds
DEBUG 01-15 16:09:09.335413.335413 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.335125.335125 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:09.335549.335549 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.335035.335035 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:09.335863.335863 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:09.336129.336129 cuda_h.py:19] end allocate_cuda_memory cost 0.00039458274841308594 seconds
DEBUG 01-15 16:09:09.336053.336053 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:09.336769.336769 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.336075.336075 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.336401.336401 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8318833a-5c43-447c-a11f-e36d831055c7
DEBUG 01-15 16:09:09.336133.336133 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.336339.336339 cuda_h.py:10] start self_attn
INFO 01-15 16:09:09.337537.337537 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8318833a-5c43-447c-a11f-e36d831055c7
DEBUG 01-15 16:09:09.337938.337938 cuda_h.py:19] end load_into_gpu_async cost 0.0016660690307617188 seconds
DEBUG 01-15 16:09:09.337647.337647 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:09.338141.338141 cuda_h.py:19] end restore_tensors2 cost 8.702278137207031e-05 seconds
DEBUG 01-15 16:09:09.338089.338089 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002445220947265625 seconds
INFO 01-15 16:09:09.338344.338344 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8318833a-5c43-447c-a11f-e36d831055c7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.340374.340374 cuda_h.py:19] end self_attn cost 0.003929853439331055 seconds
DEBUG 01-15 16:09:09.341971.341971 cuda_h.py:19] end iln_self_attn_paln cost 0.006125688552856445 seconds
DEBUG 01-15 16:09:09.341973.341973 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:09:09.341557.341557 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.342525.342525 cuda_h.py:19] end gate cost 0.0007727146148681641 seconds
DEBUG 01-15 16:09:09.342474.342474 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.342139.342139 lmp.py:1912] 
DEBUG 01-15 16:09:09.342139.342139 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.342869.342869 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.342439.342439 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.342480.342480 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.342706.342706 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.342740.342740 lmp.py:1917] 
DEBUG 01-15 16:09:09.342740.342740 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.342443.342443 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.342252.342252 lmp.py:1935]   Expert 25 |     14 | CPU
DEBUG 01-15 16:09:09.343969.343969 lmp.py:1935]   Expert 48 |     32 | CPU
DEBUG 01-15 16:09:09.343287.343287 lmp.py:1935]   Expert 45 |     36 | CPU
DEBUG 01-15 16:09:09.343937.343937 lmp.py:1935]   Expert  9 |     62 | CPU
DEBUG 01-15 16:09:09.343825.343825 lmp.py:1935]   Expert 54 |     85 | CPU
DEBUG 01-15 16:09:09.343190.343190 lmp.py:1935]   Expert 43 |     86 | CPU
DEBUG 01-15 16:09:09.343078.343078 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:09:09.343205.343205 lmp.py:1935]   Expert 20 |     88 | CPU
DEBUG 01-15 16:09:09.343285.343285 lmp.py:1935]   Expert 47 |     90 | CPU
DEBUG 01-15 16:09:09.343174.343174 lmp.py:1935]   Expert 57 |     91 | CPU
DEBUG 01-15 16:09:09.343585.343585 lmp.py:1935]   Expert  6 |     95 | CPU
DEBUG 01-15 16:09:09.343758.343758 lmp.py:1935]   Expert 36 |     95 | CPU
DEBUG 01-15 16:09:09.343169.343169 lmp.py:1935]   Expert 61 |    102 | CPU
DEBUG 01-15 16:09:09.343342.343342 lmp.py:1935]   Expert 62 |    102 | CPU
DEBUG 01-15 16:09:09.343277.343277 lmp.py:1935]   Expert 15 |    105 | CPU
DEBUG 01-15 16:09:09.343926.343926 lmp.py:1935]   Expert 50 |    107 | CPU
DEBUG 01-15 16:09:09.343007.343007 lmp.py:1935]   Expert  1 |    108 | CPU
DEBUG 01-15 16:09:09.343895.343895 lmp.py:1935]   Expert 13 |    108 | CPU
DEBUG 01-15 16:09:09.343829.343829 lmp.py:1935]   Expert 37 |    113 | CPU
DEBUG 01-15 16:09:09.343764.343764 lmp.py:1935]   Expert 38 |    113 | CPU
DEBUG 01-15 16:09:09.343698.343698 lmp.py:1935]   Expert 46 |    123 | CPU
DEBUG 01-15 16:09:09.343871.343871 lmp.py:1935]   Expert 14 |    124 | CPU
DEBUG 01-15 16:09:09.343806.343806 lmp.py:1935]   Expert 21 |    136 | CPU
DEBUG 01-15 16:09:09.343171.343171 lmp.py:1935]   Expert  7 |    138 | CPU
DEBUG 01-15 16:09:09.343344.343344 lmp.py:1935]   Expert 28 |    139 | CPU
DEBUG 01-15 16:09:09.343854.343854 lmp.py:1935]   Expert 44 |    141 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.343365.343365 lmp.py:1935]   Expert 52 |    141 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.343922.343922 lmp.py:1935]   Expert 24 |    151 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.343003.343003 lmp.py:1935]   Expert 42 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.343321.343321 lmp.py:1935]   Expert 10 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.343402.343402 lmp.py:1935]   Expert 11 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.343244.343244 lmp.py:1935]   Expert  2 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.343609.343609 lmp.py:1935]   Expert 35 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.343212.343212 lmp.py:1935]   Expert 26 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.343292.343292 lmp.py:1935]   Expert 31 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.343657.343657 lmp.py:1935]   Expert 19 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.343022.343022 lmp.py:1935]   Expert  3 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.343387.343387 lmp.py:1935]   Expert 32 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.343137.343137 lmp.py:1935]   Expert 12 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.343978.343978 lmp.py:1935]   Expert 56 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.343343.343343 lmp.py:1935]   Expert 60 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.343947.343947 lmp.py:1935]   Expert 40 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.343312.343312 lmp.py:1935]   Expert 41 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344154.344154 lmp.py:1935]   Expert 53 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344188.344188 lmp.py:1935]   Expert 23 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344030.344030 lmp.py:1935]   Expert 16 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344395.344395 lmp.py:1935]   Expert  8 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344859.344859 lmp.py:1935]   Expert 51 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344562.344562 lmp.py:1935]   Expert 58 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344788.344788 lmp.py:1935]   Expert 59 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344968.344968 lmp.py:1935]   Expert  4 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344671.344671 lmp.py:1935]   Expert 55 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344420.344420 lmp.py:1935]   Expert 49 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344407.344407 lmp.py:1935]   Expert 29 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344395.344395 lmp.py:1935]   Expert 18 |    284 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344621.344621 lmp.py:1935]   Expert 34 |    287 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344278.344278 lmp.py:1935]   Expert 63 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344504.344504 lmp.py:1935]   Expert 27 |    354 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344990.344990 lmp.py:1935]   Expert 39 |    383 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344646.344646 lmp.py:1935]   Expert 17 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344780.344780 lmp.py:1935]   Expert 22 |    430 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344244.344244 lmp.py:1935]   Expert 30 |    453 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344947.344947 lmp.py:1935]   Expert 33 |    456 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.344696.344696 lmp.py:1935]   Expert  5 |    709 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.344969.344969 lmp.py:1937] 
DEBUG 01-15 16:09:09.344969.344969 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.344956.344956 lmp.py:1938]   CPU:   2379 tokens
DEBUG 01-15 16:09:09.344613.344613 lmp.py:1942]   cuda:1:   4921 tokens (19 experts)
DEBUG 01-15 16:09:09.344792.344792 lmp.py:1942]   cuda:2:   4988 tokens (20 experts)
DEBUG 01-15 16:09:09.344780.344780 lmp.py:1943]   Total GPU:   9909 tokens
DEBUG 01-15 16:09:09.344052.344052 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.344052.344052 lmp.py:1944] 
DEBUG 01-15 16:09:09.344477.344477 cuda_h.py:19] end experts_map_get cost 0.002421855926513672 seconds
DEBUG 01-15 16:09:09.344262.344262 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.344031.344031 lmp.py:1953] 
DEBUG 01-15 16:09:09.344031.344031 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.344040.344040 cuda_h.py:19] end cpu_experts_submit cost 7.462501525878906e-05 seconds
DEBUG 01-15 16:09:09.345664.345664 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.345137.345137 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.345317.345317 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.3.gate_proj.weight': 0, 'model.layers.22.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.22.mlp.experts.2.gate_proj.weight': 0, 'model.layers.22.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.22.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.22.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.22.mlp.experts.59.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(26165641216, 5767168, 0, 0), (26171408384, 5767168, 5767168, 0), (26159874048, 5767168, 11534336, 0), (26182942720, 5767168, 17301504, 0), (26188709888, 5767168, 23068672, 0), (26177175552, 5767168, 28835840, 0), (26200244224, 5767168, 34603008, 0), (26206011392, 5767168, 40370176, 0), (26194477056, 5767168, 46137344, 0), (26304053248, 5767168, 51904512, 0), (26309820416, 5767168, 57671680, 0), (26298286080, 5767168, 63438848, 0), (26321354752, 5767168, 69206016, 0), (26327121920, 5767168, 74973184, 0), (26315587584, 5767168, 80740352, 0), (26425163776, 5767168, 86507520, 0), (26430930944, 5767168, 92274688, 0), (26419396608, 5767168, 98041856, 0), (26494369792, 5767168, 103809024, 0), (26500136960, 5767168, 109576192, 0), (26488602624, 5767168, 115343360, 0), (26511671296, 5767168, 121110528, 0), (26517438464, 5767168, 126877696, 0), (26505904128, 5767168, 132644864, 0), (26528972800, 5767168, 138412032, 0), (26534739968, 5767168, 144179200, 0), (26523205632, 5767168, 149946368, 0), (26650083328, 5767168, 155713536, 0), (26655850496, 5767168, 161480704, 0), (26644316160, 5767168, 167247872, 0), (26719289344, 5767168, 173015040, 0), (26725056512, 5767168, 178782208, 0), (26713522176, 5767168, 184549376, 0), (26788495360, 5767168, 190316544, 0), (26794262528, 5767168, 196083712, 0), (26782728192, 5767168, 201850880, 0), (26823098368, 5767168, 207618048, 0), (26828865536, 5767168, 213385216, 0), (26817331200, 5767168, 219152384, 0), (26875002880, 5767168, 224919552, 0), (26880770048, 5767168, 230686720, 0), (26869235712, 5767168, 236453888, 0), (26961510400, 5767168, 242221056, 0), (26967277568, 5767168, 247988224, 0), (26955743232, 5767168, 253755392, 0), (26996113408, 5767168, 259522560, 0), (27001880576, 5767168, 265289728, 0), (26990346240, 5767168, 271056896, 0), (27117223936, 5767168, 276824064, 0), (27122991104, 5767168, 282591232, 0), (27111456768, 5767168, 288358400, 0), (27151826944, 5767168, 294125568, 0), (27157594112, 5767168, 299892736, 0), (27146059776, 5767168, 305659904, 0), (27203731456, 5767168, 311427072, 0), (27209498624, 5767168, 317194240, 0), (27197964288, 5767168, 322961408, 0)], 2: [(26148339712, 5767168, 0, 0), (26154106880, 5767168, 5767168, 0), (26142572544, 5767168, 11534336, 0), (26252148736, 5767168, 17301504, 0), (26257915904, 5767168, 23068672, 0), (26246381568, 5767168, 28835840, 0), (26286751744, 5767168, 34603008, 0), (26292518912, 5767168, 40370176, 0), (26280984576, 5767168, 46137344, 0), (26390560768, 5767168, 51904512, 0), (26396327936, 5767168, 57671680, 0), (26384793600, 5767168, 63438848, 0), (26407862272, 5767168, 69206016, 0), (26413629440, 5767168, 74973184, 0), (26402095104, 5767168, 80740352, 0), (26442465280, 5767168, 86507520, 0), (26448232448, 5767168, 92274688, 0), (26436698112, 5767168, 98041856, 0), (26563575808, 5767168, 103809024, 0), (26569342976, 5767168, 109576192, 0), (26557808640, 5767168, 115343360, 0), (26580877312, 5767168, 121110528, 0), (26586644480, 5767168, 126877696, 0), (26575110144, 5767168, 132644864, 0), (26615480320, 5767168, 138412032, 0), (26621247488, 5767168, 144179200, 0), (26609713152, 5767168, 149946368, 0), (26632781824, 5767168, 155713536, 0), (26638548992, 5767168, 161480704, 0), (26627014656, 5767168, 167247872, 0), (26667384832, 5767168, 173015040, 0), (26673152000, 5767168, 178782208, 0), (26661617664, 5767168, 184549376, 0), (26684686336, 5767168, 190316544, 0), (26690453504, 5767168, 196083712, 0), (26678919168, 5767168, 201850880, 0), (26701987840, 5767168, 207618048, 0), (26707755008, 5767168, 213385216, 0), (26696220672, 5767168, 219152384, 0), (26805796864, 5767168, 224919552, 0), (26811564032, 5767168, 230686720, 0), (26800029696, 5767168, 236453888, 0), (26840399872, 5767168, 242221056, 0), (26846167040, 5767168, 247988224, 0), (26834632704, 5767168, 253755392, 0), (27013414912, 5767168, 259522560, 0), (27019182080, 5767168, 265289728, 0), (27007647744, 5767168, 271056896, 0), (27030716416, 5767168, 276824064, 0), (27036483584, 5767168, 282591232, 0), (27024949248, 5767168, 288358400, 0), (27065319424, 5767168, 294125568, 0), (27071086592, 5767168, 299892736, 0), (27059552256, 5767168, 305659904, 0), (27082620928, 5767168, 311427072, 0), (27088388096, 5767168, 317194240, 0), (27076853760, 5767168, 322961408, 0), (27134525440, 5767168, 328728576, 0), (27140292608, 5767168, 334495744, 0), (27128758272, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a688117de0>, 2: <capsule object NULL at 0x74a6807aa0d0>}
INFO 01-15 16:09:09.346315.346315 client.py:127] Model loaded
DEBUG 01-15 16:09:09.346637.346637 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.346740.346740 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.346756.346756 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.347336.347336 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a5c78455-802d-4a00-843d-0b8bbb18f8cb
DEBUG 01-15 16:09:09.347995.347995 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.347517.347517 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.347998.347998 cuda_h.py:19] end restore2model cost 0.001001119613647461 seconds
DEBUG 01-15 16:09:09.347974.347974 cuda_h.py:19] end sllm_worker_task cost 0.012444019317626953 seconds
DEBUG 01-15 16:09:09.348903.348903 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:09.348149.348149 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a5c78455-802d-4a00-843d-0b8bbb18f8cb
DEBUG 01-15 16:09:09.349372.349372 cuda_h.py:19] end move_flatidxs cost 0.0009016990661621094 seconds
DEBUG 01-15 16:09:09.349315.349315 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:09.349757.349757 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004248619079589844 seconds
DEBUG 01-15 16:09:09.349595.349595 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.352169.352169 cuda_h.py:19] end restore2model cost 0.0030906200408935547 seconds
DEBUG 01-15 16:09:09.352311.352311 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007617473602294922 seconds
DEBUG 01-15 16:09:09.352298.352298 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.352395.352395 cuda_h.py:19] end gpu_sexperts cost 0.0002846717834472656 seconds
DEBUG 01-15 16:09:09.353609.353609 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.353061.353061 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1696090698242188e-05 seconds
DEBUG 01-15 16:09:09.353380.353380 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.353420.353420 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.354374.354374 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009529590606689453 seconds
DEBUG 01-15 16:09:09.354701.354701 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.354251.354251 cuda_h.py:19] end gpu_group_list cost 0.0002014636993408203 seconds
DEBUG 01-15 16:09:09.355630.355630 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.355033.355033 cuda_h.py:19] end group_tensors cost 0.005779743194580078 seconds
DEBUG 01-15 16:09:09.355410.355410 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.356353.356353 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011382102966308594 seconds
DEBUG 01-15 16:09:09.356309.356309 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.356556.356556 cuda_h.py:19] end gpu_group_list cost 0.0002200603485107422 seconds
DEBUG 01-15 16:09:09.357832.357832 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.357953.357953 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a5c78455-802d-4a00-843d-0b8bbb18f8cb
DEBUG 01-15 16:09:09.358004.358004 cuda_h.py:19] end group pad cost 0.003045797348022461 seconds
DEBUG 01-15 16:09:09.358940.358940 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:09.384962.384962 client.py:127] Model loaded
DEBUG 01-15 16:09:09.384173.384173 cuda_h.py:19] end wait_experts_multi_device cost 0.02731466293334961 seconds
DEBUG 01-15 16:09:09.384664.384664 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.385264.385264 cuda_h.py:19] end group_einsum cost 0.026800870895385742 seconds
DEBUG 01-15 16:09:09.386428.386428 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:09.388925.388925 cuda_h.py:19] end get_outputs_cpu1 cost 0.002510547637939453 seconds
DEBUG 01-15 16:09:09.389060.389060 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0418543815612793 seconds
DEBUG 01-15 16:09:09.389536.389536 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.004807949066162109 seconds
DEBUG 01-15 16:09:09.389843.389843 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.391651.391651 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.391478.391478 cuda_h.py:19] end index_scatter cost 0.00010561943054199219 seconds
DEBUG 01-15 16:09:09.391165.391165 cuda_h.py:19] end cpuoutputsdeal cost 0.0018291473388671875 seconds
DEBUG 01-15 16:09:09.391599.391599 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.391766.391766 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.392059.392059 cuda_h.py:19] end gpu_group_tensor cost 0.00020432472229003906 seconds
DEBUG 01-15 16:09:09.392372.392372 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.392487.392487 cuda_h.py:19] end gpu_group_tensor cost 0.00021314620971679688 seconds
DEBUG 01-15 16:09:09.392644.392644 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.393284.393284 cuda_h.py:19] end gpu_group_einsum cost 0.0009932518005371094 seconds
DEBUG 01-15 16:09:09.394717.394717 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.394654.394654 cuda_h.py:19] end gpu_group_einsum cost 0.0006272792816162109 seconds
DEBUG 01-15 16:09:09.394474.394474 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.395836.395836 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.395058.395058 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025582313537597656 seconds
DEBUG 01-15 16:09:09.395411.395411 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.395493.395493 cuda_h.py:19] end concat_expert_out cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:09:09.395535.395535 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.395678.395678 cuda_h.py:19] end index_scatter cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:09:09.395316.395316 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009493827819824219 seconds
DEBUG 01-15 16:09:09.396544.396544 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.396533.396533 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.396446.396446 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015091896057128906 seconds
DEBUG 01-15 16:09:09.396249.396249 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.396556.396556 cuda_h.py:19] end concat_expert_out cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:09:09.396896.396896 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.396350.396350 cuda_h.py:19] end index_scatter cost 5.507469177246094e-05 seconds
DEBUG 01-15 16:09:09.396444.396444 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005214214324951172 seconds
DEBUG 01-15 16:09:09.396235.396235 cuda_h.py:19] end gpu_experts_multi_device cost 0.04351449012756348 seconds
DEBUG 01-15 16:09:09.396767.396767 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.05532264709472656 seconds
DEBUG 01-15 16:09:09.397871.397871 cuda_h.py:19] end prefill_layer cost 0.062386512756347656 seconds
DEBUG 01-15 16:09:09.397145.397145 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:09:09.397040.397040 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.397650.397650 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:09:09.397545.397545 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:09:09.397109.397109 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:09:09.397204.397204 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 4.267692565917969e-05 seconds
DEBUG 01-15 16:09:09.397014.397014 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:09.397658.397658 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 0.00016570091247558594 seconds
DEBUG 01-15 16:09:09.397443.397443 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:09.397325.397325 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.397626.397626 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:09.398954.398954 cuda_h.py:19] end allocate_cuda_memory cost 0.00039196014404296875 seconds
DEBUG 01-15 16:09:09.398415.398415 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.398543.398543 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:09.398884.398884 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.398290.398290 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.398430.398430 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 18ef7ff0-9282-46f6-98c8-4e5e6fe03a09
DEBUG 01-15 16:09:09.398586.398586 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.399015.399015 cuda_h.py:10] start self_attn
INFO 01-15 16:09:09.400806.400806 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 18ef7ff0-9282-46f6-98c8-4e5e6fe03a09
DEBUG 01-15 16:09:09.400457.400457 cuda_h.py:19] end load_into_gpu_async cost 0.0014774799346923828 seconds
DEBUG 01-15 16:09:09.400259.400259 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:09.400031.400031 cuda_h.py:19] end restore_tensors2 cost 8.106231689453125e-05 seconds
DEBUG 01-15 16:09:09.400509.400509 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024721622467041016 seconds
INFO 01-15 16:09:09.400127.400127 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 18ef7ff0-9282-46f6-98c8-4e5e6fe03a09
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.402482.402482 cuda_h.py:19] end self_attn cost 0.0031888484954833984 seconds
DEBUG 01-15 16:09:09.402690.402690 cuda_h.py:19] end iln_self_attn_paln cost 0.004836082458496094 seconds
DEBUG 01-15 16:09:09.402566.402566 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:09:09.402899.402899 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.403225.403225 cuda_h.py:19] end gate cost 0.0006282329559326172 seconds
DEBUG 01-15 16:09:09.403068.403068 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.403457.403457 lmp.py:1912] 
DEBUG 01-15 16:09:09.403457.403457 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.403564.403564 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.403691.403691 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.403049.403049 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.403215.403215 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.404712.404712 lmp.py:1917] 
DEBUG 01-15 16:09:09.404712.404712 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.404163.404163 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.404051.404051 lmp.py:1935]   Expert  5 |     13 | CPU
DEBUG 01-15 16:09:09.404502.404502 lmp.py:1935]   Expert 56 |     30 | CPU
DEBUG 01-15 16:09:09.404953.404953 lmp.py:1935]   Expert 16 |     85 | CPU
DEBUG 01-15 16:09:09.404927.404927 lmp.py:1935]   Expert 27 |     85 | CPU
DEBUG 01-15 16:09:09.404378.404378 lmp.py:1935]   Expert 17 |     89 | CPU
DEBUG 01-15 16:09:09.404352.404352 lmp.py:1935]   Expert 40 |     96 | CPU
DEBUG 01-15 16:09:09.404326.404326 lmp.py:1935]   Expert 63 |    100 | CPU
DEBUG 01-15 16:09:09.404823.404823 lmp.py:1935]   Expert 49 |    105 | CPU
DEBUG 01-15 16:09:09.404559.404559 lmp.py:1935]   Expert 51 |    106 | CPU
DEBUG 01-15 16:09:09.404056.404056 lmp.py:1935]   Expert 28 |    109 | CPU
DEBUG 01-15 16:09:09.404030.404030 lmp.py:1935]   Expert 53 |    109 | CPU
DEBUG 01-15 16:09:09.404004.404004 lmp.py:1935]   Expert  7 |    113 | CPU
DEBUG 01-15 16:09:09.404740.404740 lmp.py:1935]   Expert 47 |    119 | CPU
DEBUG 01-15 16:09:09.404237.404237 lmp.py:1935]   Expert 38 |    121 | CPU
DEBUG 01-15 16:09:09.404450.404450 lmp.py:1935]   Expert 62 |    124 | CPU
DEBUG 01-15 16:09:09.404900.404900 lmp.py:1935]   Expert 11 |    129 | CPU
DEBUG 01-15 16:09:09.404113.404113 lmp.py:1935]   Expert 37 |    129 | CPU
DEBUG 01-15 16:09:09.404849.404849 lmp.py:1935]   Expert 58 |    131 | CPU
DEBUG 01-15 16:09:09.404346.404346 lmp.py:1935]   Expert 57 |    138 | CPU
DEBUG 01-15 16:09:09.404843.404843 lmp.py:1935]   Expert 39 |    143 | CPU
DEBUG 01-15 16:09:09.404340.404340 lmp.py:1935]   Expert  1 |    147 | CPU
DEBUG 01-15 16:09:09.404837.404837 lmp.py:1935]   Expert 14 |    148 | CPU
DEBUG 01-15 16:09:09.404487.404487 lmp.py:1935]   Expert 52 |    153 | CPU
DEBUG 01-15 16:09:09.404700.404700 lmp.py:1935]   Expert 25 |    155 | CPU
DEBUG 01-15 16:09:09.404197.404197 lmp.py:1935]   Expert 23 |    158 | CPU
DEBUG 01-15 16:09:09.404363.404363 lmp.py:1935]   Expert 33 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404244.404244 lmp.py:1935]   Expert 21 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404172.404172 lmp.py:1935]   Expert  6 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404053.404053 lmp.py:1935]   Expert 60 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404458.404458 lmp.py:1935]   Expert 45 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404147.404147 lmp.py:1935]   Expert  4 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404598.404598 lmp.py:1935]   Expert 19 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404287.404287 lmp.py:1935]   Expert 44 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404215.404215 lmp.py:1935]   Expert 12 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404666.404666 lmp.py:1935]   Expert 30 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404594.404594 lmp.py:1935]   Expert 55 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404760.404760 lmp.py:1935]   Expert 31 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404449.404449 lmp.py:1935]   Expert 36 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404900.404900 lmp.py:1935]   Expert  3 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404351.404351 lmp.py:1935]   Expert  9 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404563.404563 lmp.py:1935]   Expert  0 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404776.404776 lmp.py:1935]   Expert 34 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404227.404227 lmp.py:1935]   Expert 22 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404916.404916 lmp.py:1935]   Expert 41 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404129.404129 lmp.py:1935]   Expert 54 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404818.404818 lmp.py:1935]   Expert 26 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404507.404507 lmp.py:1935]   Expert 43 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404958.404958 lmp.py:1935]   Expert 59 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404932.404932 lmp.py:1935]   Expert 18 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404145.404145 lmp.py:1935]   Expert 13 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404119.404119 lmp.py:1935]   Expert 50 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404331.404331 lmp.py:1935]   Expert 20 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404544.404544 lmp.py:1935]   Expert 15 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404756.404756 lmp.py:1935]   Expert 42 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404730.404730 lmp.py:1935]   Expert 24 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404419.404419 lmp.py:1935]   Expert 61 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404109.404109 lmp.py:1935]   Expert 29 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.404321.404321 lmp.py:1935]   Expert 35 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.404249.404249 lmp.py:1935]   Expert 32 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.405415.405415 lmp.py:1935]   Expert  8 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.405343.405343 lmp.py:1935]   Expert  2 |    342 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.405509.405509 lmp.py:1935]   Expert 10 |    346 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.405437.405437 lmp.py:1935]   Expert 46 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.405364.405364 lmp.py:1935]   Expert 48 |    448 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.405338.405338 lmp.py:1937] 
DEBUG 01-15 16:09:09.405338.405338 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.405981.405981 lmp.py:1938]   CPU:   2835 tokens
DEBUG 01-15 16:09:09.405101.405101 lmp.py:1942]   cuda:1:   4807 tokens (20 experts)
DEBUG 01-15 16:09:09.405267.405267 lmp.py:1942]   cuda:2:   4646 tokens (19 experts)
DEBUG 01-15 16:09:09.405241.405241 lmp.py:1943]   Total GPU:   9453 tokens
DEBUG 01-15 16:09:09.405454.405454 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.405454.405454 lmp.py:1944] 
DEBUG 01-15 16:09:09.405150.405150 cuda_h.py:19] end experts_map_get cost 0.001611948013305664 seconds
DEBUG 01-15 16:09:09.405092.405092 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.405226.405226 lmp.py:1953] 
DEBUG 01-15 16:09:09.405226.405226 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.405585.405585 cuda_h.py:19] end cpu_experts_submit cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:09.405302.405302 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.405483.405483 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.407568.407568 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.2.gate_proj.weight': 0, 'model.layers.23.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.42.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.42.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.42.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.43.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.43.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.43.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.45.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.45.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.45.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.48.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.48.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.48.up_proj.weight': 322961408, 'model.layers.23.mlp.experts.61.gate_proj.weight': 328728576, 'model.layers.23.mlp.experts.61.down_proj.weight': 334495744, 'model.layers.23.mlp.experts.61.up_proj.weight': 340262912}, 2: {'model.layers.23.mlp.experts.0.gate_proj.weight': 0, 'model.layers.23.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.60.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(27255635968, 5767168, 0, 0), (27261403136, 5767168, 5767168, 0), (27249868800, 5767168, 11534336, 0), (27290238976, 5767168, 17301504, 0), (27296006144, 5767168, 23068672, 0), (27284471808, 5767168, 28835840, 0), (27376746496, 5767168, 34603008, 0), (27382513664, 5767168, 40370176, 0), (27370979328, 5767168, 46137344, 0), (27445952512, 5767168, 51904512, 0), (27451719680, 5767168, 57671680, 0), (27440185344, 5767168, 63438848, 0), (27549761536, 5767168, 69206016, 0), (27555528704, 5767168, 74973184, 0), (27543994368, 5767168, 80740352, 0), (27567063040, 5767168, 86507520, 0), (27572830208, 5767168, 92274688, 0), (27561295872, 5767168, 98041856, 0), (27584364544, 5767168, 103809024, 0), (27590131712, 5767168, 109576192, 0), (27578597376, 5767168, 115343360, 0), (27670872064, 5767168, 121110528, 0), (27676639232, 5767168, 126877696, 0), (27665104896, 5767168, 132644864, 0), (27740078080, 5767168, 138412032, 0), (27745845248, 5767168, 144179200, 0), (27734310912, 5767168, 149946368, 0), (27757379584, 5767168, 155713536, 0), (27763146752, 5767168, 161480704, 0), (27751612416, 5767168, 167247872, 0), (27774681088, 5767168, 173015040, 0), (27780448256, 5767168, 178782208, 0), (27768913920, 5767168, 184549376, 0), (27791982592, 5767168, 190316544, 0), (27797749760, 5767168, 196083712, 0), (27786215424, 5767168, 201850880, 0), (27809284096, 5767168, 207618048, 0), (27815051264, 5767168, 213385216, 0), (27803516928, 5767168, 219152384, 0), (27826585600, 5767168, 224919552, 0), (27832352768, 5767168, 230686720, 0), (27820818432, 5767168, 236453888, 0), (27930394624, 5767168, 242221056, 0), (27936161792, 5767168, 247988224, 0), (27924627456, 5767168, 253755392, 0), (27947696128, 5767168, 259522560, 0), (27953463296, 5767168, 265289728, 0), (27941928960, 5767168, 271056896, 0), (27964997632, 5767168, 276824064, 0), (27970764800, 5767168, 282591232, 0), (27959230464, 5767168, 288358400, 0), (27999600640, 5767168, 294125568, 0), (28005367808, 5767168, 299892736, 0), (27993833472, 5767168, 305659904, 0), (28051505152, 5767168, 311427072, 0), (28057272320, 5767168, 317194240, 0), (28045737984, 5767168, 322961408, 0), (28276424704, 5767168, 328728576, 0), (28282191872, 5767168, 334495744, 0), (28270657536, 5767168, 340262912, 0)], 2: [(27221032960, 5767168, 0, 0), (27226800128, 5767168, 5767168, 0), (27215265792, 5767168, 11534336, 0), (27272937472, 5767168, 17301504, 0), (27278704640, 5767168, 23068672, 0), (27267170304, 5767168, 28835840, 0), (27324841984, 5767168, 34603008, 0), (27330609152, 5767168, 40370176, 0), (27319074816, 5767168, 46137344, 0), (27359444992, 5767168, 51904512, 0), (27365212160, 5767168, 57671680, 0), (27353677824, 5767168, 63438848, 0), (27394048000, 5767168, 69206016, 0), (27399815168, 5767168, 74973184, 0), (27388280832, 5767168, 80740352, 0), (27428651008, 5767168, 86507520, 0), (27434418176, 5767168, 92274688, 0), (27422883840, 5767168, 98041856, 0), (27480555520, 5767168, 103809024, 0), (27486322688, 5767168, 109576192, 0), (27474788352, 5767168, 115343360, 0), (27532460032, 5767168, 121110528, 0), (27538227200, 5767168, 126877696, 0), (27526692864, 5767168, 132644864, 0), (27601666048, 5767168, 138412032, 0), (27607433216, 5767168, 144179200, 0), (27595898880, 5767168, 149946368, 0), (27636269056, 5767168, 155713536, 0), (27642036224, 5767168, 161480704, 0), (27630501888, 5767168, 167247872, 0), (27722776576, 5767168, 173015040, 0), (27728543744, 5767168, 178782208, 0), (27717009408, 5767168, 184549376, 0), (27843887104, 5767168, 190316544, 0), (27849654272, 5767168, 196083712, 0), (27838119936, 5767168, 201850880, 0), (27982299136, 5767168, 207618048, 0), (27988066304, 5767168, 213385216, 0), (27976531968, 5767168, 219152384, 0), (28016902144, 5767168, 224919552, 0), (28022669312, 5767168, 230686720, 0), (28011134976, 5767168, 236453888, 0), (28086108160, 5767168, 242221056, 0), (28091875328, 5767168, 247988224, 0), (28080340992, 5767168, 253755392, 0), (28155314176, 5767168, 259522560, 0), (28161081344, 5767168, 265289728, 0), (28149547008, 5767168, 271056896, 0), (28172615680, 5767168, 276824064, 0), (28178382848, 5767168, 282591232, 0), (28166848512, 5767168, 288358400, 0), (28241821696, 5767168, 294125568, 0), (28247588864, 5767168, 299892736, 0), (28236054528, 5767168, 305659904, 0), (28259123200, 5767168, 311427072, 0), (28264890368, 5767168, 317194240, 0), (28253356032, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6807aa4f0>, 2: <capsule object NULL at 0x74a6807aa490>}
DEBUG 01-15 16:09:09.407425.407425 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.407401.407401 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.407342.407342 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9ceadb56-325a-4118-8e47-86ca1c7ac59f
DEBUG 01-15 16:09:09.407535.407535 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:09.408093.408093 client.py:127] Model loaded
DEBUG 01-15 16:09:09.408068.408068 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.408702.408702 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.408213.408213 cuda_h.py:19] end restore2model cost 0.0003554821014404297 seconds
DEBUG 01-15 16:09:09.408506.408506 cuda_h.py:19] end sllm_worker_task cost 0.010803937911987305 seconds
INFO 01-15 16:09:09.408999.408999 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9ceadb56-325a-4118-8e47-86ca1c7ac59f
DEBUG 01-15 16:09:09.408204.408204 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:09.409445.409445 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0035927295684814453 seconds
DEBUG 01-15 16:09:09.409051.409051 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.409227.409227 cuda_h.py:19] end move_flatidxs cost 0.0008344650268554688 seconds
DEBUG 01-15 16:09:09.409242.409242 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:09.412624.412624 cuda_h.py:19] end restore2model cost 0.002847909927368164 seconds
DEBUG 01-15 16:09:09.412553.412553 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006694793701171875 seconds
DEBUG 01-15 16:09:09.412938.412938 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.412274.412274 cuda_h.py:19] end gpu_sexperts cost 0.00027942657470703125 seconds
DEBUG 01-15 16:09:09.412957.412957 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.412157.412157 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.430511474609375e-05 seconds
DEBUG 01-15 16:09:09.412138.412138 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.412887.412887 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.413014.413014 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009763240814208984 seconds
DEBUG 01-15 16:09:09.413095.413095 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.413673.413673 cuda_h.py:19] end gpu_group_list cost 0.00022101402282714844 seconds
DEBUG 01-15 16:09:09.414443.414443 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.415578.415578 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009891986846923828 seconds
DEBUG 01-15 16:09:09.415388.415388 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.416522.416522 cuda_h.py:19] end gpu_group_list cost 0.0002105236053466797 seconds
DEBUG 01-15 16:09:09.416929.416929 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.416189.416189 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9ceadb56-325a-4118-8e47-86ca1c7ac59f
DEBUG 01-15 16:09:09.418285.418285 cuda_h.py:19] end group_tensors cost 0.008563041687011719 seconds
DEBUG 01-15 16:09:09.419376.419376 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.422924.422924 cuda_h.py:19] end group pad cost 0.0030891895294189453 seconds
DEBUG 01-15 16:09:09.422191.422191 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:09.446572.446572 client.py:127] Model loaded
DEBUG 01-15 16:09:09.447675.447675 cuda_h.py:19] end wait_experts_multi_device cost 0.030308246612548828 seconds
DEBUG 01-15 16:09:09.447599.447599 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.449039.449039 cuda_h.py:19] end group_einsum cost 0.026805639266967773 seconds
DEBUG 01-15 16:09:09.449090.449090 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:09.452788.452788 cuda_h.py:19] end get_outputs_cpu1 cost 0.0028009414672851562 seconds
DEBUG 01-15 16:09:09.452720.452720 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.044358253479003906 seconds
DEBUG 01-15 16:09:09.453201.453201 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0064122676849365234 seconds
DEBUG 01-15 16:09:09.453605.453605 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.456466.456466 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.456565.456565 cuda_h.py:19] end index_scatter cost 0.000232696533203125 seconds
DEBUG 01-15 16:09:09.457959.457959 cuda_h.py:19] end cpuoutputsdeal cost 0.0029687881469726562 seconds
DEBUG 01-15 16:09:09.457375.457375 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.457067.457067 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.457405.457405 cuda_h.py:19] end gpu_group_tensor cost 0.0003056526184082031 seconds
DEBUG 01-15 16:09:09.457288.457288 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.458877.458877 cuda_h.py:19] end gpu_group_tensor cost 0.00028395652770996094 seconds
DEBUG 01-15 16:09:09.458898.458898 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.459013.459013 cuda_h.py:19] end gpu_group_einsum cost 0.001165151596069336 seconds
DEBUG 01-15 16:09:09.459883.459883 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.460898.460898 cuda_h.py:19] end gpu_group_einsum cost 0.0009450912475585938 seconds
DEBUG 01-15 16:09:09.461523.461523 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.461544.461544 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.462946.462946 cuda_h.py:19] end all_expert_outputs_slices cost 0.00043654441833496094 seconds
DEBUG 01-15 16:09:09.462677.462677 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.462419.462419 cuda_h.py:19] end concat_expert_out cost 0.00011754035949707031 seconds
DEBUG 01-15 16:09:09.462523.462523 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.462477.462477 cuda_h.py:19] end index_scatter cost 0.0001251697540283203 seconds
DEBUG 01-15 16:09:09.463202.463202 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0017917156219482422 seconds
DEBUG 01-15 16:09:09.463395.463395 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.463393.463393 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.463090.463090 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003628730773925781 seconds
DEBUG 01-15 16:09:09.464669.464669 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.464172.464172 cuda_h.py:19] end concat_expert_out cost 0.00012087821960449219 seconds
DEBUG 01-15 16:09:09.464031.464031 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.464529.464529 cuda_h.py:19] end index_scatter cost 0.00012063980102539062 seconds
DEBUG 01-15 16:09:09.464797.464797 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012199878692626953 seconds
DEBUG 01-15 16:09:09.464835.464835 cuda_h.py:19] end gpu_experts_multi_device cost 0.05223727226257324 seconds
DEBUG 01-15 16:09:09.465517.465517 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.06217384338378906 seconds
DEBUG 01-15 16:09:09.465129.465129 cuda_h.py:19] end prefill_layer cost 0.06828951835632324 seconds
DEBUG 01-15 16:09:09.465164.465164 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:09:09.465635.465635 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.465391.465391 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:09:09.465385.465385 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:09:09.465910.465910 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:09:09.465741.465741 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:09.465411.465411 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 9.441375732421875e-05 seconds
DEBUG 01-15 16:09:09.466498.466498 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.466971.466971 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.466989.466989 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:09.466120.466120 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:09.466308.466308 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:09.466541.466541 cuda_h.py:19] end allocate_cuda_memory cost 0.0004057884216308594 seconds
DEBUG 01-15 16:09:09.467458.467458 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:09.467267.467267 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.467666.467666 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.467038.467038 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 04b2230a-538f-4939-907c-dcacba709e8f
DEBUG 01-15 16:09:09.467697.467697 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.467649.467649 cuda_h.py:10] start self_attn
INFO 01-15 16:09:09.468160.468160 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 04b2230a-538f-4939-907c-dcacba709e8f
DEBUG 01-15 16:09:09.468071.468071 cuda_h.py:19] end load_into_gpu_async cost 0.0016427040100097656 seconds
DEBUG 01-15 16:09:09.468827.468827 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:09.468976.468976 cuda_h.py:19] end restore_tensors2 cost 8.344650268554688e-05 seconds
DEBUG 01-15 16:09:09.468540.468540 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024154186248779297 seconds
INFO 01-15 16:09:09.468741.468741 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 04b2230a-538f-4939-907c-dcacba709e8f
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.471510.471510 cuda_h.py:19] end self_attn cost 0.0034477710723876953 seconds
DEBUG 01-15 16:09:09.471000.471000 cuda_h.py:19] end iln_self_attn_paln cost 0.005444049835205078 seconds
DEBUG 01-15 16:09:09.471187.471187 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:09:09.471772.471772 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.472607.472607 cuda_h.py:19] end gate cost 0.0007805824279785156 seconds
DEBUG 01-15 16:09:09.472987.472987 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.472512.472512 lmp.py:1912] 
DEBUG 01-15 16:09:09.472512.472512 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.473189.473189 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.473661.473661 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.473079.473079 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.473397.473397 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.473332.473332 lmp.py:1917] 
DEBUG 01-15 16:09:09.473332.473332 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.473458.473458 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.473691.473691 lmp.py:1935]   Expert 36 |     21 | CPU
DEBUG 01-15 16:09:09.473533.473533 lmp.py:1935]   Expert 35 |     29 | CPU
DEBUG 01-15 16:09:09.473613.473613 lmp.py:1935]   Expert 25 |     46 | CPU
DEBUG 01-15 16:09:09.473263.473263 lmp.py:1935]   Expert 46 |     49 | CPU
DEBUG 01-15 16:09:09.473390.473390 lmp.py:1935]   Expert 51 |     51 | CPU
DEBUG 01-15 16:09:09.473278.473278 lmp.py:1935]   Expert 16 |     60 | CPU
DEBUG 01-15 16:09:09.473451.473451 lmp.py:1935]   Expert 30 |     62 | CPU
DEBUG 01-15 16:09:09.473624.473624 lmp.py:1935]   Expert  0 |     64 | CPU
DEBUG 01-15 16:09:09.473797.473797 lmp.py:1935]   Expert 43 |     70 | CPU
DEBUG 01-15 16:09:09.473023.473023 lmp.py:1935]   Expert 47 |     71 | CPU
DEBUG 01-15 16:09:09.473772.473772 lmp.py:1935]   Expert 44 |     72 | CPU
DEBUG 01-15 16:09:09.473283.473283 lmp.py:1935]   Expert 55 |     75 | CPU
DEBUG 01-15 16:09:09.473317.473317 lmp.py:1935]   Expert 39 |     76 | CPU
DEBUG 01-15 16:09:09.473589.473589 lmp.py:1935]   Expert 42 |     77 | CPU
DEBUG 01-15 16:09:09.473385.473385 lmp.py:1935]   Expert  2 |     86 | CPU
DEBUG 01-15 16:09:09.473180.473180 lmp.py:1935]   Expert  4 |    105 | CPU
DEBUG 01-15 16:09:09.473691.473691 lmp.py:1935]   Expert 48 |    115 | CPU
DEBUG 01-15 16:09:09.473632.473632 lmp.py:1935]   Expert 33 |    119 | CPU
DEBUG 01-15 16:09:09.473905.473905 lmp.py:1935]   Expert 13 |    122 | CPU
DEBUG 01-15 16:09:09.473654.473654 lmp.py:1935]   Expert  6 |    124 | CPU
DEBUG 01-15 16:09:09.473688.473688 lmp.py:1935]   Expert 24 |    125 | CPU
DEBUG 01-15 16:09:09.473722.473722 lmp.py:1935]   Expert 61 |    125 | CPU
DEBUG 01-15 16:09:09.473186.473186 lmp.py:1935]   Expert 56 |    131 | CPU
DEBUG 01-15 16:09:09.473697.473697 lmp.py:1935]   Expert 29 |    132 | CPU
DEBUG 01-15 16:09:09.473254.473254 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:09:09.473388.473388 lmp.py:1935]   Expert 38 |    141 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.473521.473521 lmp.py:1935]   Expert  7 |    143 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.473462.473462 lmp.py:1935]   Expert  9 |    143 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.473404.473404 lmp.py:1935]   Expert 54 |    143 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.473537.473537 lmp.py:1935]   Expert 20 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.473240.473240 lmp.py:1935]   Expert 59 |    150 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.473181.473181 lmp.py:1935]   Expert 45 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474407.474407 lmp.py:1935]   Expert 62 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474872.474872 lmp.py:1935]   Expert 19 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474098.474098 lmp.py:1935]   Expert 34 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474668.474668 lmp.py:1935]   Expert 57 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474610.474610 lmp.py:1935]   Expert 50 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474597.474597 lmp.py:1935]   Expert 10 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474346.474346 lmp.py:1935]   Expert 31 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474334.474334 lmp.py:1935]   Expert 23 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474845.474845 lmp.py:1935]   Expert  8 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474117.474117 lmp.py:1935]   Expert 60 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474535.474535 lmp.py:1935]   Expert 18 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474761.474761 lmp.py:1935]   Expert 22 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474987.474987 lmp.py:1935]   Expert 53 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474452.474452 lmp.py:1935]   Expert 37 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474201.474201 lmp.py:1935]   Expert 52 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474189.474189 lmp.py:1935]   Expert  5 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474653.474653 lmp.py:1935]   Expert 17 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474356.474356 lmp.py:1935]   Expert 11 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474867.474867 lmp.py:1935]   Expert  1 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474093.474093 lmp.py:1935]   Expert 41 |    280 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474604.474604 lmp.py:1935]   Expert 49 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474353.474353 lmp.py:1935]   Expert 28 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474579.474579 lmp.py:1935]   Expert 26 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474328.474328 lmp.py:1935]   Expert 32 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474269.474269 lmp.py:1935]   Expert 58 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474496.474496 lmp.py:1935]   Expert 40 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474245.474245 lmp.py:1935]   Expert 14 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474709.474709 lmp.py:1935]   Expert 12 |    330 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474220.474220 lmp.py:1935]   Expert 63 |    331 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474446.474446 lmp.py:1935]   Expert 21 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474864.474864 lmp.py:1935]   Expert 27 |    669 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.474329.474329 lmp.py:1935]   Expert  3 |   1015 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.474409.474409 lmp.py:1937] 
DEBUG 01-15 16:09:09.474409.474409 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.474158.474158 lmp.py:1938]   CPU:   2142 tokens
DEBUG 01-15 16:09:09.474623.474623 lmp.py:1942]   cuda:1:   5065 tokens (19 experts)
DEBUG 01-15 16:09:09.474325.474325 lmp.py:1942]   cuda:2:   5081 tokens (20 experts)
DEBUG 01-15 16:09:09.474883.474883 lmp.py:1943]   Total GPU:  10146 tokens
DEBUG 01-15 16:09:09.474440.474440 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.474440.474440 lmp.py:1944] 
DEBUG 01-15 16:09:09.475673.475673 cuda_h.py:19] end experts_map_get cost 0.0024330615997314453 seconds
DEBUG 01-15 16:09:09.475688.475688 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.475412.475412 lmp.py:1953] 
DEBUG 01-15 16:09:09.475412.475412 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.475845.475845 cuda_h.py:19] end cpu_experts_submit cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:09:09.475263.475263 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.475901.475901 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.476255.476255 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.1.gate_proj.weight': 0, 'model.layers.24.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.50.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.50.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.50.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.53.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.53.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.53.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.54.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.54.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.54.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.58.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.58.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.58.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.24.mlp.experts.5.gate_proj.weight': 0, 'model.layers.24.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.38.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.38.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.38.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.40.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.40.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.40.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.24.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.24.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.24.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(28345630720, 5767168, 0, 0), (28351397888, 5767168, 5767168, 0), (28339863552, 5767168, 11534336, 0), (28380233728, 5767168, 17301504, 0), (28386000896, 5767168, 23068672, 0), (28374466560, 5767168, 28835840, 0), (28449439744, 5767168, 34603008, 0), (28455206912, 5767168, 40370176, 0), (28443672576, 5767168, 46137344, 0), (28501344256, 5767168, 51904512, 0), (28507111424, 5767168, 57671680, 0), (28495577088, 5767168, 63438848, 0), (28570550272, 5767168, 69206016, 0), (28576317440, 5767168, 74973184, 0), (28564783104, 5767168, 80740352, 0), (28622454784, 5767168, 86507520, 0), (28628221952, 5767168, 92274688, 0), (28616687616, 5767168, 98041856, 0), (28639756288, 5767168, 103809024, 0), (28645523456, 5767168, 109576192, 0), (28633989120, 5767168, 115343360, 0), (28657057792, 5767168, 121110528, 0), (28662824960, 5767168, 126877696, 0), (28651290624, 5767168, 132644864, 0), (28726263808, 5767168, 138412032, 0), (28732030976, 5767168, 144179200, 0), (28720496640, 5767168, 149946368, 0), (28778168320, 5767168, 155713536, 0), (28783935488, 5767168, 161480704, 0), (28772401152, 5767168, 167247872, 0), (28968484864, 5767168, 173015040, 0), (28974252032, 5767168, 178782208, 0), (28962717696, 5767168, 184549376, 0), (29037690880, 5767168, 190316544, 0), (29043458048, 5767168, 196083712, 0), (29031923712, 5767168, 201850880, 0), (29193404416, 5767168, 207618048, 0), (29199171584, 5767168, 213385216, 0), (29187637248, 5767168, 219152384, 0), (29245308928, 5767168, 224919552, 0), (29251076096, 5767168, 230686720, 0), (29239541760, 5767168, 236453888, 0), (29262610432, 5767168, 242221056, 0), (29268377600, 5767168, 247988224, 0), (29256843264, 5767168, 253755392, 0), (29331816448, 5767168, 259522560, 0), (29337583616, 5767168, 265289728, 0), (29326049280, 5767168, 271056896, 0), (29349117952, 5767168, 276824064, 0), (29354885120, 5767168, 282591232, 0), (29343350784, 5767168, 288358400, 0), (29401022464, 5767168, 294125568, 0), (29406789632, 5767168, 299892736, 0), (29395255296, 5767168, 305659904, 0), (29418323968, 5767168, 311427072, 0), (29424091136, 5767168, 317194240, 0), (29412556800, 5767168, 322961408, 0)], 2: [(28414836736, 5767168, 0, 0), (28420603904, 5767168, 5767168, 0), (28409069568, 5767168, 11534336, 0), (28466741248, 5767168, 17301504, 0), (28472508416, 5767168, 23068672, 0), (28460974080, 5767168, 28835840, 0), (28484042752, 5767168, 34603008, 0), (28489809920, 5767168, 40370176, 0), (28478275584, 5767168, 46137344, 0), (28518645760, 5767168, 51904512, 0), (28524412928, 5767168, 57671680, 0), (28512878592, 5767168, 63438848, 0), (28535947264, 5767168, 69206016, 0), (28541714432, 5767168, 74973184, 0), (28530180096, 5767168, 80740352, 0), (28674359296, 5767168, 86507520, 0), (28680126464, 5767168, 92274688, 0), (28668592128, 5767168, 98041856, 0), (28691660800, 5767168, 103809024, 0), (28697427968, 5767168, 109576192, 0), (28685893632, 5767168, 115343360, 0), (28708962304, 5767168, 121110528, 0), (28714729472, 5767168, 126877696, 0), (28703195136, 5767168, 132644864, 0), (28795469824, 5767168, 138412032, 0), (28801236992, 5767168, 144179200, 0), (28789702656, 5767168, 149946368, 0), (28812771328, 5767168, 155713536, 0), (28818538496, 5767168, 161480704, 0), (28807004160, 5767168, 167247872, 0), (28864675840, 5767168, 173015040, 0), (28870443008, 5767168, 178782208, 0), (28858908672, 5767168, 184549376, 0), (28881977344, 5767168, 190316544, 0), (28887744512, 5767168, 196083712, 0), (28876210176, 5767168, 201850880, 0), (28916580352, 5767168, 207618048, 0), (28922347520, 5767168, 213385216, 0), (28910813184, 5767168, 219152384, 0), (28985786368, 5767168, 224919552, 0), (28991553536, 5767168, 230686720, 0), (28980019200, 5767168, 236453888, 0), (29020389376, 5767168, 242221056, 0), (29026156544, 5767168, 247988224, 0), (29014622208, 5767168, 253755392, 0), (29106896896, 5767168, 259522560, 0), (29112664064, 5767168, 265289728, 0), (29101129728, 5767168, 271056896, 0), (29176102912, 5767168, 276824064, 0), (29181870080, 5767168, 282591232, 0), (29170335744, 5767168, 288358400, 0), (29228007424, 5767168, 294125568, 0), (29233774592, 5767168, 299892736, 0), (29222240256, 5767168, 305659904, 0), (29314514944, 5767168, 311427072, 0), (29320282112, 5767168, 317194240, 0), (29308747776, 5767168, 322961408, 0), (29366419456, 5767168, 328728576, 0), (29372186624, 5767168, 334495744, 0), (29360652288, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814716190>, 2: <capsule object NULL at 0x74a81456dbf0>}
DEBUG 01-15 16:09:09.476410.476410 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.476082.476082 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.476937.476937 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f2e67fed-c49b-49af-a007-1d29eb412ca8
DEBUG 01-15 16:09:09.477728.477728 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.477545.477545 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:09.477385.477385 client.py:127] Model loaded
DEBUG 01-15 16:09:09.477116.477116 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.477145.477145 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:09.477938.477938 cuda_h.py:19] end restore2model cost 0.000392913818359375 seconds
DEBUG 01-15 16:09:09.477138.477138 cuda_h.py:19] end sllm_worker_task cost 0.011442899703979492 seconds
INFO 01-15 16:09:09.478664.478664 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f2e67fed-c49b-49af-a007-1d29eb412ca8
DEBUG 01-15 16:09:09.478641.478641 cuda_h.py:19] end move_flatidxs cost 0.0008981227874755859 seconds
DEBUG 01-15 16:09:09.478246.478246 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:09.478861.478861 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003509044647216797 seconds
DEBUG 01-15 16:09:09.478321.478321 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.482494.482494 cuda_h.py:19] end restore2model cost 0.003144979476928711 seconds
DEBUG 01-15 16:09:09.482033.482033 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006962776184082031 seconds
DEBUG 01-15 16:09:09.482213.482213 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.482025.482025 cuda_h.py:19] end gpu_sexperts cost 0.00028443336486816406 seconds
DEBUG 01-15 16:09:09.482530.482530 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.482929.482929 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.9550323486328125e-05 seconds
DEBUG 01-15 16:09:09.482387.482387 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.482381.482381 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.483613.483613 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009455680847167969 seconds
DEBUG 01-15 16:09:09.483648.483648 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.484212.484212 cuda_h.py:19] end gpu_group_list cost 0.00021123886108398438 seconds
DEBUG 01-15 16:09:09.484911.484911 cuda_h.py:19] end group_tensors cost 0.005570173263549805 seconds
DEBUG 01-15 16:09:09.484486.484486 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.485325.485325 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.488618.488618 cuda_h.py:19] end group pad cost 0.003265380859375 seconds
DEBUG 01-15 16:09:09.488660.488660 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:09.488274.488274 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.002685546875 seconds
DEBUG 01-15 16:09:09.488305.488305 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.489794.489794 cuda_h.py:19] end gpu_group_list cost 0.0010578632354736328 seconds
DEBUG 01-15 16:09:09.494234.494234 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.494453.494453 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f2e67fed-c49b-49af-a007-1d29eb412ca8
DEBUG 01-15 16:09:09.508382.508382 cuda_h.py:19] end group_einsum cost 0.02009749412536621 seconds
DEBUG 01-15 16:09:09.508845.508845 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:09.512815.512815 client.py:127] Model loaded
DEBUG 01-15 16:09:09.512881.512881 cuda_h.py:19] end wait_experts_multi_device cost 0.017536401748657227 seconds
DEBUG 01-15 16:09:09.512935.512935 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.512585.512585 cuda_h.py:19] end get_outputs_cpu1 cost 0.004229307174682617 seconds
DEBUG 01-15 16:09:09.513212.513212 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03647971153259277 seconds
DEBUG 01-15 16:09:09.514614.514614 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.001802682876586914 seconds
DEBUG 01-15 16:09:09.514214.514214 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.515684.515684 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.516856.516856 cuda_h.py:19] end index_scatter cost 0.00010228157043457031 seconds
DEBUG 01-15 16:09:09.516331.516331 cuda_h.py:19] end cpuoutputsdeal cost 0.0018494129180908203 seconds
DEBUG 01-15 16:09:09.516918.516918 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.516085.516085 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.516332.516332 cuda_h.py:19] end gpu_group_tensor cost 0.00020384788513183594 seconds
DEBUG 01-15 16:09:09.516645.516645 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.517952.517952 cuda_h.py:19] end gpu_group_tensor cost 0.00021076202392578125 seconds
DEBUG 01-15 16:09:09.517349.517349 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.518391.518391 cuda_h.py:19] end gpu_group_einsum cost 0.0007805824279785156 seconds
DEBUG 01-15 16:09:09.518054.518054 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.519515.519515 cuda_h.py:19] end gpu_group_einsum cost 0.0010020732879638672 seconds
DEBUG 01-15 16:09:09.519881.519881 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.519210.519210 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.520411.520411 cuda_h.py:19] end all_expert_outputs_slices cost 0.000240325927734375 seconds
DEBUG 01-15 16:09:09.520552.520552 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.520350.520350 cuda_h.py:19] end concat_expert_out cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:09:09.520895.520895 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.520568.520568 cuda_h.py:19] end index_scatter cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:09:09.520729.520729 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009491443634033203 seconds
DEBUG 01-15 16:09:09.520872.520872 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.520874.520874 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.521125.521125 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015282630920410156 seconds
DEBUG 01-15 16:09:09.521928.521928 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.521997.521997 cuda_h.py:19] end concat_expert_out cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:09:09.521270.521270 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.521578.521578 cuda_h.py:19] end index_scatter cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:09.521148.521148 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005159378051757812 seconds
DEBUG 01-15 16:09:09.521847.521847 cuda_h.py:19] end gpu_experts_multi_device cost 0.0388340950012207 seconds
DEBUG 01-15 16:09:09.521618.521618 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.04999876022338867 seconds
DEBUG 01-15 16:09:09.522622.522622 cuda_h.py:19] end prefill_layer cost 0.05634593963623047 seconds
DEBUG 01-15 16:09:09.522916.522916 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:09:09.522811.522811 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.522435.522435 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:09:09.522760.522760 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:09:09.522516.522516 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:09:09.522135.522135 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 4.100799560546875e-05 seconds
DEBUG 01-15 16:09:09.522560.522560 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:09:09.522448.522448 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.522616.522616 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.522348.522348 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:09.522178.522178 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:09.522783.522783 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:09.523983.523983 cuda_h.py:19] end allocate_cuda_memory cost 0.0003905296325683594 seconds
DEBUG 01-15 16:09:09.523813.523813 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:09.523960.523960 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.523459.523459 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.523738.523738 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5e2dafc4-edf9-440a-aeaf-47fabcce4392
DEBUG 01-15 16:09:09.523357.523357 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.523996.523996 cuda_h.py:10] start self_attn
INFO 01-15 16:09:09.524551.524551 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5e2dafc4-edf9-440a-aeaf-47fabcce4392
DEBUG 01-15 16:09:09.525063.525063 cuda_h.py:19] end load_into_gpu_async cost 0.0016908645629882812 seconds
DEBUG 01-15 16:09:09.525581.525581 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:09.525313.525313 cuda_h.py:19] end restore_tensors2 cost 8.654594421386719e-05 seconds
DEBUG 01-15 16:09:09.525599.525599 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024538040161132812 seconds
INFO 01-15 16:09:09.525509.525509 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5e2dafc4-edf9-440a-aeaf-47fabcce4392
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.527133.527133 cuda_h.py:19] end self_attn cost 0.0032231807708740234 seconds
DEBUG 01-15 16:09:09.527719.527719 cuda_h.py:19] end iln_self_attn_paln cost 0.0049054622650146484 seconds
DEBUG 01-15 16:09:09.527025.527025 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:09:09.527166.527166 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.528897.528897 cuda_h.py:19] end gate cost 0.0006351470947265625 seconds
DEBUG 01-15 16:09:09.528727.528727 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.528209.528209 lmp.py:1912] 
DEBUG 01-15 16:09:09.528209.528209 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.528064.528064 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.528098.528098 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.528840.528840 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.528437.528437 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.528842.528842 lmp.py:1917] 
DEBUG 01-15 16:09:09.528842.528842 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.528246.528246 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.528565.528565 lmp.py:1935]   Expert 13 |     30 | CPU
DEBUG 01-15 16:09:09.528446.528446 lmp.py:1935]   Expert  9 |     40 | CPU
DEBUG 01-15 16:09:09.528851.528851 lmp.py:1935]   Expert 44 |     41 | CPU
DEBUG 01-15 16:09:09.528779.528779 lmp.py:1935]   Expert 25 |     42 | CPU
DEBUG 01-15 16:09:09.528468.528468 lmp.py:1935]   Expert 38 |     46 | CPU
DEBUG 01-15 16:09:09.528396.528396 lmp.py:1935]   Expert 16 |     48 | CPU
DEBUG 01-15 16:09:09.528323.528323 lmp.py:1935]   Expert  2 |     51 | CPU
DEBUG 01-15 16:09:09.528013.528013 lmp.py:1935]   Expert 22 |     53 | CPU
DEBUG 01-15 16:09:09.528179.528179 lmp.py:1935]   Expert 33 |     56 | CPU
DEBUG 01-15 16:09:09.528967.528967 lmp.py:1935]   Expert 42 |     61 | CPU
DEBUG 01-15 16:09:09.528895.528895 lmp.py:1935]   Expert  5 |     68 | CPU
DEBUG 01-15 16:09:09.528108.528108 lmp.py:1935]   Expert 23 |     75 | CPU
DEBUG 01-15 16:09:09.528605.528605 lmp.py:1935]   Expert 24 |     80 | CPU
DEBUG 01-15 16:09:09.528579.528579 lmp.py:1935]   Expert 10 |     82 | CPU
DEBUG 01-15 16:09:09.528076.528076 lmp.py:1935]   Expert 21 |    104 | CPU
DEBUG 01-15 16:09:09.529050.529050 lmp.py:1935]   Expert 59 |    104 | CPU
DEBUG 01-15 16:09:09.529024.529024 lmp.py:1935]   Expert 46 |    117 | CPU
DEBUG 01-15 16:09:09.529760.529760 lmp.py:1935]   Expert 55 |    117 | CPU
DEBUG 01-15 16:09:09.529495.529495 lmp.py:1935]   Expert 45 |    120 | CPU
DEBUG 01-15 16:09:09.529754.529754 lmp.py:1935]   Expert 61 |    121 | CPU
DEBUG 01-15 16:09:09.529490.529490 lmp.py:1935]   Expert 31 |    128 | CPU
DEBUG 01-15 16:09:09.529987.529987 lmp.py:1935]   Expert 51 |    138 | CPU
DEBUG 01-15 16:09:09.529915.529915 lmp.py:1935]   Expert 36 |    139 | CPU
DEBUG 01-15 16:09:09.529604.529604 lmp.py:1935]   Expert  8 |    141 | CPU
DEBUG 01-15 16:09:09.529340.529340 lmp.py:1935]   Expert  6 |    143 | CPU
DEBUG 01-15 16:09:09.529983.529983 lmp.py:1935]   Expert 43 |    146 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529672.529672 lmp.py:1935]   Expert  3 |    153 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529600.529600 lmp.py:1935]   Expert  0 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529289.529289 lmp.py:1935]   Expert 26 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529740.529740 lmp.py:1935]   Expert 18 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529714.529714 lmp.py:1935]   Expert 48 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529403.529403 lmp.py:1935]   Expert 41 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529331.529331 lmp.py:1935]   Expert 12 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529497.529497 lmp.py:1935]   Expert  7 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529425.529425 lmp.py:1935]   Expert 20 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529876.529876 lmp.py:1935]   Expert 56 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529327.529327 lmp.py:1935]   Expert 28 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529539.529539 lmp.py:1935]   Expert 27 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529990.529990 lmp.py:1935]   Expert 34 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529202.529202 lmp.py:1935]   Expert  1 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529892.529892 lmp.py:1935]   Expert 47 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529343.529343 lmp.py:1935]   Expert 32 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529509.529509 lmp.py:1935]   Expert 11 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529913.529913 lmp.py:1935]   Expert 40 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529126.529126 lmp.py:1935]   Expert 49 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529577.529577 lmp.py:1935]   Expert 53 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529027.529027 lmp.py:1935]   Expert 63 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529478.529478 lmp.py:1935]   Expert 15 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529691.529691 lmp.py:1935]   Expert 50 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529142.529142 lmp.py:1935]   Expert  4 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529831.529831 lmp.py:1935]   Expert 29 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529011.529011 lmp.py:1935]   Expert 30 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529746.529746 lmp.py:1935]   Expert 14 |    273 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529959.529959 lmp.py:1935]   Expert 35 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529648.529648 lmp.py:1935]   Expert 37 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529099.529099 lmp.py:1935]   Expert 52 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529311.529311 lmp.py:1935]   Expert 17 |    361 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529524.529524 lmp.py:1935]   Expert 54 |    375 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529260.529260 lmp.py:1935]   Expert 39 |    387 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529234.529234 lmp.py:1935]   Expert 57 |    409 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529446.529446 lmp.py:1935]   Expert 60 |    457 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529182.529182 lmp.py:1935]   Expert 62 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529156.529156 lmp.py:1935]   Expert 19 |    542 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.529130.529130 lmp.py:1935]   Expert 58 |    572 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.529389.529389 lmp.py:1937] 
DEBUG 01-15 16:09:09.529389.529389 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.529124.529124 lmp.py:1938]   CPU:   2145 tokens
DEBUG 01-15 16:09:09.529767.529767 lmp.py:1942]   cuda:1:   5144 tokens (20 experts)
DEBUG 01-15 16:09:09.529456.529456 lmp.py:1942]   cuda:2:   4999 tokens (19 experts)
DEBUG 01-15 16:09:09.529431.529431 lmp.py:1943]   Total GPU:  10143 tokens
DEBUG 01-15 16:09:09.529451.529451 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.529451.529451 lmp.py:1944] 
DEBUG 01-15 16:09:09.529909.529909 cuda_h.py:19] end experts_map_get cost 0.0016520023345947266 seconds
DEBUG 01-15 16:09:09.529467.529467 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.530077.530077 lmp.py:1953] 
DEBUG 01-15 16:09:09.530077.530077 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.530483.530483 cuda_h.py:19] end cpu_experts_submit cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:09.530484.530484 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.530566.530566 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.532304.532304 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.1.gate_proj.weight': 0, 'model.layers.25.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.25.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.25.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.25.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.25.mlp.experts.0.gate_proj.weight': 0, 'model.layers.25.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.12.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.12.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.12.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.15.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.15.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.15.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.26.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.26.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.26.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.27.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.27.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.27.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.28.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.28.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.28.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(29452926976, 5767168, 0, 0), (29458694144, 5767168, 5767168, 0), (29447159808, 5767168, 11534336, 0), (29487529984, 5767168, 17301504, 0), (29493297152, 5767168, 23068672, 0), (29481762816, 5767168, 28835840, 0), (29504831488, 5767168, 34603008, 0), (29510598656, 5767168, 40370176, 0), (29499064320, 5767168, 46137344, 0), (29556736000, 5767168, 51904512, 0), (29562503168, 5767168, 57671680, 0), (29550968832, 5767168, 63438848, 0), (29625942016, 5767168, 69206016, 0), (29631709184, 5767168, 74973184, 0), (29620174848, 5767168, 80740352, 0), (29677846528, 5767168, 86507520, 0), (29683613696, 5767168, 92274688, 0), (29672079360, 5767168, 98041856, 0), (29729751040, 5767168, 103809024, 0), (29735518208, 5767168, 109576192, 0), (29723983872, 5767168, 115343360, 0), (29747052544, 5767168, 121110528, 0), (29752819712, 5767168, 126877696, 0), (29741285376, 5767168, 132644864, 0), (29781655552, 5767168, 138412032, 0), (29787422720, 5767168, 144179200, 0), (29775888384, 5767168, 149946368, 0), (30023876608, 5767168, 155713536, 0), (30029643776, 5767168, 161480704, 0), (30018109440, 5767168, 167247872, 0), (30110384128, 5767168, 173015040, 0), (30116151296, 5767168, 178782208, 0), (30104616960, 5767168, 184549376, 0), (30179590144, 5767168, 190316544, 0), (30185357312, 5767168, 196083712, 0), (30173822976, 5767168, 201850880, 0), (30248796160, 5767168, 207618048, 0), (30254563328, 5767168, 213385216, 0), (30243028992, 5767168, 219152384, 0), (30266097664, 5767168, 224919552, 0), (30271864832, 5767168, 230686720, 0), (30260330496, 5767168, 236453888, 0), (30283399168, 5767168, 242221056, 0), (30289166336, 5767168, 247988224, 0), (30277632000, 5767168, 253755392, 0), (30300700672, 5767168, 259522560, 0), (30306467840, 5767168, 265289728, 0), (30294933504, 5767168, 271056896, 0), (30335303680, 5767168, 276824064, 0), (30341070848, 5767168, 282591232, 0), (30329536512, 5767168, 288358400, 0), (30439112704, 5767168, 294125568, 0), (30444879872, 5767168, 299892736, 0), (30433345536, 5767168, 305659904, 0), (30473715712, 5767168, 311427072, 0), (30479482880, 5767168, 317194240, 0), (30467948544, 5767168, 322961408, 0), (30525620224, 5767168, 328728576, 0), (30531387392, 5767168, 334495744, 0), (30519853056, 5767168, 340262912, 0)], 2: [(29435625472, 5767168, 0, 0), (29441392640, 5767168, 5767168, 0), (29429858304, 5767168, 11534336, 0), (29643243520, 5767168, 17301504, 0), (29649010688, 5767168, 23068672, 0), (29637476352, 5767168, 28835840, 0), (29695148032, 5767168, 34603008, 0), (29700915200, 5767168, 40370176, 0), (29689380864, 5767168, 46137344, 0), (29764354048, 5767168, 51904512, 0), (29770121216, 5767168, 57671680, 0), (29758586880, 5767168, 63438848, 0), (29885464576, 5767168, 69206016, 0), (29891231744, 5767168, 74973184, 0), (29879697408, 5767168, 80740352, 0), (29902766080, 5767168, 86507520, 0), (29908533248, 5767168, 92274688, 0), (29896998912, 5767168, 98041856, 0), (29920067584, 5767168, 103809024, 0), (29925834752, 5767168, 109576192, 0), (29914300416, 5767168, 115343360, 0), (29937369088, 5767168, 121110528, 0), (29943136256, 5767168, 126877696, 0), (29931601920, 5767168, 132644864, 0), (29954670592, 5767168, 138412032, 0), (29960437760, 5767168, 144179200, 0), (29948903424, 5767168, 149946368, 0), (29989273600, 5767168, 155713536, 0), (29995040768, 5767168, 161480704, 0), (29983506432, 5767168, 167247872, 0), (30041178112, 5767168, 173015040, 0), (30046945280, 5767168, 178782208, 0), (30035410944, 5767168, 184549376, 0), (30075781120, 5767168, 190316544, 0), (30081548288, 5767168, 196083712, 0), (30070013952, 5767168, 201850880, 0), (30127685632, 5767168, 207618048, 0), (30133452800, 5767168, 213385216, 0), (30121918464, 5767168, 219152384, 0), (30144987136, 5767168, 224919552, 0), (30150754304, 5767168, 230686720, 0), (30139219968, 5767168, 236453888, 0), (30352605184, 5767168, 242221056, 0), (30358372352, 5767168, 247988224, 0), (30346838016, 5767168, 253755392, 0), (30369906688, 5767168, 259522560, 0), (30375673856, 5767168, 265289728, 0), (30364139520, 5767168, 271056896, 0), (30404509696, 5767168, 276824064, 0), (30410276864, 5767168, 282591232, 0), (30398742528, 5767168, 288358400, 0), (30421811200, 5767168, 294125568, 0), (30427578368, 5767168, 299892736, 0), (30416044032, 5767168, 305659904, 0), (30508318720, 5767168, 311427072, 0), (30514085888, 5767168, 317194240, 0), (30502551552, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b351c427f0>, 2: <capsule object NULL at 0x74a6787cdf20>}
DEBUG 01-15 16:09:09.532659.532659 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.532805.532805 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.532184.532184 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ca533d0f-daf6-4626-812a-3d69b72c4ae1
DEBUG 01-15 16:09:09.533913.533913 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:09.533223.533223 client.py:127] Model loaded
DEBUG 01-15 16:09:09.533322.533322 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.533860.533860 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.533419.533419 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:09.533462.533462 cuda_h.py:19] end restore2model cost 0.000335693359375 seconds
DEBUG 01-15 16:09:09.533801.533801 cuda_h.py:19] end sllm_worker_task cost 0.010896444320678711 seconds
DEBUG 01-15 16:09:09.534171.534171 cuda_h.py:19] end move_flatidxs cost 0.0008442401885986328 seconds
DEBUG 01-15 16:09:09.534663.534663 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:09.535759.535759 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ca533d0f-daf6-4626-812a-3d69b72c4ae1
DEBUG 01-15 16:09:09.535046.535046 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005415439605712891 seconds
DEBUG 01-15 16:09:09.535393.535393 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.538638.538638 cuda_h.py:19] end restore2model cost 0.0029201507568359375 seconds
DEBUG 01-15 16:09:09.538381.538381 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008575677871704102 seconds
DEBUG 01-15 16:09:09.538508.538508 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.539538.539538 cuda_h.py:19] end gpu_sexperts cost 0.0002734661102294922 seconds
DEBUG 01-15 16:09:09.539698.539698 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.539898.539898 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4066696166992188e-05 seconds
DEBUG 01-15 16:09:09.539833.539833 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.539774.539774 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.540841.540841 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009670257568359375 seconds
DEBUG 01-15 16:09:09.540114.540114 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.540215.540215 cuda_h.py:19] end gpu_group_list cost 0.00022101402282714844 seconds
DEBUG 01-15 16:09:09.541535.541535 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.542000.542000 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009856224060058594 seconds
DEBUG 01-15 16:09:09.542380.542380 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.542361.542361 cuda_h.py:19] end gpu_group_list cost 0.00020265579223632812 seconds
DEBUG 01-15 16:09:09.543788.543788 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.543094.543094 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ca533d0f-daf6-4626-812a-3d69b72c4ae1
DEBUG 01-15 16:09:09.542702.542702 cuda_h.py:19] end group_tensors cost 0.008453130722045898 seconds
DEBUG 01-15 16:09:09.543494.543494 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.547663.547663 cuda_h.py:19] end group pad cost 0.0034050941467285156 seconds
DEBUG 01-15 16:09:09.547407.547407 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:09.572184.572184 client.py:127] Model loaded
DEBUG 01-15 16:09:09.572144.572144 cuda_h.py:19] end wait_experts_multi_device cost 0.02916407585144043 seconds
DEBUG 01-15 16:09:09.572622.572622 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.578733.578733 cuda_h.py:19] end group_einsum cost 0.031377553939819336 seconds
DEBUG 01-15 16:09:09.578382.578382 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:09.581135.581135 cuda_h.py:19] end get_outputs_cpu1 cost 0.0030448436737060547 seconds
DEBUG 01-15 16:09:09.583078.583078 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.049777984619140625 seconds
DEBUG 01-15 16:09:09.583527.583527 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.011324167251586914 seconds
DEBUG 01-15 16:09:09.584898.584898 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.586768.586768 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.586911.586911 cuda_h.py:19] end index_scatter cost 0.00015354156494140625 seconds
DEBUG 01-15 16:09:09.587101.587101 cuda_h.py:19] end cpuoutputsdeal cost 0.0029969215393066406 seconds
DEBUG 01-15 16:09:09.587094.587094 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.587739.587739 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.587560.587560 cuda_h.py:19] end gpu_group_tensor cost 0.00031256675720214844 seconds
DEBUG 01-15 16:09:09.588305.588305 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.588874.588874 cuda_h.py:19] end gpu_group_tensor cost 0.0003037452697753906 seconds
DEBUG 01-15 16:09:09.588670.588670 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.589799.589799 cuda_h.py:19] end gpu_group_einsum cost 0.0011718273162841797 seconds
DEBUG 01-15 16:09:09.590821.590821 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.591794.591794 cuda_h.py:19] end gpu_group_einsum cost 0.0008428096771240234 seconds
DEBUG 01-15 16:09:09.591650.591650 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.591101.591101 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.592749.592749 cuda_h.py:19] end all_expert_outputs_slices cost 0.0006175041198730469 seconds
DEBUG 01-15 16:09:09.592582.592582 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.592535.592535 cuda_h.py:19] end concat_expert_out cost 0.00013947486877441406 seconds
DEBUG 01-15 16:09:09.592220.592220 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.592953.592953 cuda_h.py:19] end index_scatter cost 7.867813110351562e-05 seconds
DEBUG 01-15 16:09:09.593432.593432 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001703500747680664 seconds
DEBUG 01-15 16:09:09.593715.593715 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.593579.593579 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.593605.593605 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001628398895263672 seconds
DEBUG 01-15 16:09:09.593692.593692 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.593814.593814 cuda_h.py:19] end concat_expert_out cost 5.841255187988281e-05 seconds
DEBUG 01-15 16:09:09.593465.593465 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.593534.593534 cuda_h.py:19] end index_scatter cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:09:09.593867.593867 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005311965942382812 seconds
DEBUG 01-15 16:09:09.594201.594201 cuda_h.py:19] end gpu_experts_multi_device cost 0.05488419532775879 seconds
DEBUG 01-15 16:09:09.594210.594210 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.06660914421081543 seconds
DEBUG 01-15 16:09:09.594685.594685 cuda_h.py:19] end prefill_layer cost 0.07234525680541992 seconds
DEBUG 01-15 16:09:09.594833.594833 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:09:09.594967.594967 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.594385.594385 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:09:09.594902.594902 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:09:09.594897.594897 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:09:09.594323.594323 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 4.124641418457031e-05 seconds
DEBUG 01-15 16:09:09.594483.594483 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 9.059906005859375e-05 seconds
DEBUG 01-15 16:09:09.594279.594279 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.595010.595010 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:09.595197.595197 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.595616.595616 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:09.595443.595443 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:09.595258.595258 cuda_h.py:19] end allocate_cuda_memory cost 0.00038123130798339844 seconds
DEBUG 01-15 16:09:09.596930.596930 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:09.596169.596169 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.596827.596827 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.596629.596629 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 407984ee-5332-48c8-b4ce-56c82445b1f3
DEBUG 01-15 16:09:09.596619.596619 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.596649.596649 cuda_h.py:10] start self_attn
INFO 01-15 16:09:09.598150.598150 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 407984ee-5332-48c8-b4ce-56c82445b1f3
DEBUG 01-15 16:09:09.598324.598324 cuda_h.py:19] end load_into_gpu_async cost 0.0020449161529541016 seconds
DEBUG 01-15 16:09:09.598881.598881 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:09.598600.598600 cuda_h.py:19] end restore_tensors2 cost 8.130073547363281e-05 seconds
DEBUG 01-15 16:09:09.598443.598443 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002797842025756836 seconds
INFO 01-15 16:09:09.598955.598955 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 407984ee-5332-48c8-b4ce-56c82445b1f3
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.599424.599424 cuda_h.py:19] end self_attn cost 0.002896547317504883 seconds
DEBUG 01-15 16:09:09.599427.599427 cuda_h.py:19] end iln_self_attn_paln cost 0.004781961441040039 seconds
DEBUG 01-15 16:09:09.599872.599872 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:09:09.599628.599628 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.600094.600094 cuda_h.py:19] end gate cost 0.0006270408630371094 seconds
DEBUG 01-15 16:09:09.600923.600923 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.600696.600696 lmp.py:1912] 
DEBUG 01-15 16:09:09.600696.600696 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.600611.600611 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.600215.600215 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.600288.600288 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.601216.601216 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.601667.601667 lmp.py:1917] 
DEBUG 01-15 16:09:09.601667.601667 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.601594.601594 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.601483.601483 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:09:09.601649.601649 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:09:09.601100.601100 lmp.py:1935]   Expert 11 |     30 | CPU
DEBUG 01-15 16:09:09.601266.601266 lmp.py:1935]   Expert  7 |     37 | CPU
DEBUG 01-15 16:09:09.601955.601955 lmp.py:1935]   Expert  3 |     43 | CPU
DEBUG 01-15 16:09:09.601644.601644 lmp.py:1935]   Expert 62 |     43 | CPU
DEBUG 01-15 16:09:09.601857.601857 lmp.py:1935]   Expert 51 |     44 | CPU
DEBUG 01-15 16:09:09.601546.601546 lmp.py:1935]   Expert 30 |     51 | CPU
DEBUG 01-15 16:09:09.601997.601997 lmp.py:1935]   Expert 17 |     54 | CPU
DEBUG 01-15 16:09:09.601971.601971 lmp.py:1935]   Expert 29 |     54 | CPU
DEBUG 01-15 16:09:09.601945.601945 lmp.py:1935]   Expert  6 |     61 | CPU
DEBUG 01-15 16:09:09.601442.601442 lmp.py:1935]   Expert  9 |     66 | CPU
DEBUG 01-15 16:09:09.601178.601178 lmp.py:1935]   Expert 38 |     77 | CPU
DEBUG 01-15 16:09:09.601152.601152 lmp.py:1935]   Expert 63 |     77 | CPU
DEBUG 01-15 16:09:09.601649.601649 lmp.py:1935]   Expert 55 |     84 | CPU
DEBUG 01-15 16:09:09.601908.601908 lmp.py:1935]   Expert 59 |     87 | CPU
DEBUG 01-15 16:09:09.601882.601882 lmp.py:1935]   Expert 48 |     92 | CPU
DEBUG 01-15 16:09:09.601856.601856 lmp.py:1935]   Expert  8 |     95 | CPU
DEBUG 01-15 16:09:09.601353.601353 lmp.py:1935]   Expert 19 |     97 | CPU
DEBUG 01-15 16:09:09.601327.601327 lmp.py:1935]   Expert 22 |    101 | CPU
DEBUG 01-15 16:09:09.601540.601540 lmp.py:1935]   Expert 49 |    102 | CPU
DEBUG 01-15 16:09:09.601229.601229 lmp.py:1935]   Expert 24 |    108 | CPU
DEBUG 01-15 16:09:09.601157.601157 lmp.py:1935]   Expert 36 |    116 | CPU
DEBUG 01-15 16:09:09.601846.601846 lmp.py:1935]   Expert 42 |    116 | CPU
DEBUG 01-15 16:09:09.601820.601820 lmp.py:1935]   Expert 34 |    117 | CPU
DEBUG 01-15 16:09:09.601655.601655 lmp.py:1935]   Expert 50 |    120 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601060.601060 lmp.py:1935]   Expert 39 |    122 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601464.601464 lmp.py:1935]   Expert  4 |    130 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601392.601392 lmp.py:1935]   Expert 37 |    144 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601081.601081 lmp.py:1935]   Expert 15 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601532.601532 lmp.py:1935]   Expert 41 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601460.601460 lmp.py:1935]   Expert 23 |    156 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601911.601911 lmp.py:1935]   Expert 56 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601329.601329 lmp.py:1935]   Expert 60 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601495.601495 lmp.py:1935]   Expert 44 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601920.601920 lmp.py:1935]   Expert 16 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601563.601563 lmp.py:1935]   Expert 21 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601444.601444 lmp.py:1935]   Expert  1 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601087.601087 lmp.py:1935]   Expert 43 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601254.601254 lmp.py:1935]   Expert 53 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601181.601181 lmp.py:1935]   Expert 47 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601871.601871 lmp.py:1935]   Expert 33 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601560.601560 lmp.py:1935]   Expert 12 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601011.601011 lmp.py:1935]   Expert 13 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601700.601700 lmp.py:1935]   Expert 32 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601912.601912 lmp.py:1935]   Expert 28 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601840.601840 lmp.py:1935]   Expert  0 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601291.601291 lmp.py:1935]   Expert 26 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601742.601742 lmp.py:1935]   Expert 54 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601193.601193 lmp.py:1935]   Expert 31 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601644.601644 lmp.py:1935]   Expert 10 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601810.601810 lmp.py:1935]   Expert 18 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601738.601738 lmp.py:1935]   Expert 57 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.601665.601665 lmp.py:1935]   Expert  2 |    281 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.601070.601070 lmp.py:1935]   Expert 58 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.602905.602905 lmp.py:1935]   Expert 40 |    341 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.602025.602025 lmp.py:1935]   Expert 45 |    361 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.602668.602668 lmp.py:1935]   Expert 25 |    363 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.602072.602072 lmp.py:1935]   Expert  5 |    441 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.602715.602715 lmp.py:1935]   Expert 35 |    465 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.602358.602358 lmp.py:1935]   Expert 27 |    484 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.602763.602763 lmp.py:1935]   Expert 46 |    551 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.602691.602691 lmp.py:1935]   Expert 52 |    592 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.602095.602095 lmp.py:1935]   Expert 14 |    887 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.602546.602546 lmp.py:1937] 
DEBUG 01-15 16:09:09.602546.602546 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.602712.602712 lmp.py:1938]   CPU:   1774 tokens
DEBUG 01-15 16:09:09.602070.602070 lmp.py:1942]   cuda:1:   5242 tokens (19 experts)
DEBUG 01-15 16:09:09.602429.602429 lmp.py:1942]   cuda:2:   5272 tokens (20 experts)
DEBUG 01-15 16:09:09.602833.602833 lmp.py:1943]   Total GPU:  10514 tokens
DEBUG 01-15 16:09:09.602999.602999 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.602999.602999 lmp.py:1944] 
DEBUG 01-15 16:09:09.602126.602126 cuda_h.py:19] end experts_map_get cost 0.001665353775024414 seconds
DEBUG 01-15 16:09:09.602022.602022 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.602632.602632 lmp.py:1953] 
DEBUG 01-15 16:09:09.602632.602632 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.602846.602846 cuda_h.py:19] end cpu_experts_submit cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:09:09.602111.602111 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.602617.602617 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.604414.604414 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.58.up_proj.weight': 322961408}, 2: {'model.layers.26.mlp.experts.1.gate_proj.weight': 0, 'model.layers.26.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.54.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.54.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.54.up_proj.weight': 322961408, 'model.layers.26.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.26.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.26.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (30629429248, 5767168, 17301504, 0), (30635196416, 5767168, 23068672, 0), (30623662080, 5767168, 28835840, 0), (30715936768, 5767168, 34603008, 0), (30721703936, 5767168, 40370176, 0), (30710169600, 5767168, 46137344, 0), (30750539776, 5767168, 51904512, 0), (30756306944, 5767168, 57671680, 0), (30744772608, 5767168, 63438848, 0), (30785142784, 5767168, 69206016, 0), (30790909952, 5767168, 74973184, 0), (30779375616, 5767168, 80740352, 0), (30906253312, 5767168, 86507520, 0), (30912020480, 5767168, 92274688, 0), (30900486144, 5767168, 98041856, 0), (30992760832, 5767168, 103809024, 0), (30998528000, 5767168, 109576192, 0), (30986993664, 5767168, 115343360, 0), (31010062336, 5767168, 121110528, 0), (31015829504, 5767168, 126877696, 0), (31004295168, 5767168, 132644864, 0), (31096569856, 5767168, 138412032, 0), (31102337024, 5767168, 144179200, 0), (31090802688, 5767168, 149946368, 0), (31183077376, 5767168, 155713536, 0), (31188844544, 5767168, 161480704, 0), (31177310208, 5767168, 167247872, 0), (31217680384, 5767168, 173015040, 0), (31223447552, 5767168, 178782208, 0), (31211913216, 5767168, 184549376, 0), (31252283392, 5767168, 190316544, 0), (31258050560, 5767168, 196083712, 0), (31246516224, 5767168, 201850880, 0), (31286886400, 5767168, 207618048, 0), (31292653568, 5767168, 213385216, 0), (31281119232, 5767168, 219152384, 0), (31304187904, 5767168, 224919552, 0), (31309955072, 5767168, 230686720, 0), (31298420736, 5767168, 236453888, 0), (31321489408, 5767168, 242221056, 0), (31327256576, 5767168, 247988224, 0), (31315722240, 5767168, 253755392, 0), (31356092416, 5767168, 259522560, 0), (31361859584, 5767168, 265289728, 0), (31350325248, 5767168, 271056896, 0), (31511805952, 5767168, 276824064, 0), (31517573120, 5767168, 282591232, 0), (31506038784, 5767168, 288358400, 0), (31529107456, 5767168, 294125568, 0), (31534874624, 5767168, 299892736, 0), (31523340288, 5767168, 305659904, 0), (31546408960, 5767168, 311427072, 0), (31552176128, 5767168, 317194240, 0), (31540641792, 5767168, 322961408, 0)], 2: [(30560223232, 5767168, 0, 0), (30565990400, 5767168, 5767168, 0), (30554456064, 5767168, 11534336, 0), (30577524736, 5767168, 17301504, 0), (30583291904, 5767168, 23068672, 0), (30571757568, 5767168, 28835840, 0), (30612127744, 5767168, 34603008, 0), (30617894912, 5767168, 40370176, 0), (30606360576, 5767168, 46137344, 0), (30767841280, 5767168, 51904512, 0), (30773608448, 5767168, 57671680, 0), (30762074112, 5767168, 63438848, 0), (30802444288, 5767168, 69206016, 0), (30808211456, 5767168, 74973184, 0), (30796677120, 5767168, 80740352, 0), (30819745792, 5767168, 86507520, 0), (30825512960, 5767168, 92274688, 0), (30813978624, 5767168, 98041856, 0), (30854348800, 5767168, 103809024, 0), (30860115968, 5767168, 109576192, 0), (30848581632, 5767168, 115343360, 0), (30940856320, 5767168, 121110528, 0), (30946623488, 5767168, 126877696, 0), (30935089152, 5767168, 132644864, 0), (30975459328, 5767168, 138412032, 0), (30981226496, 5767168, 144179200, 0), (30969692160, 5767168, 149946368, 0), (31027363840, 5767168, 155713536, 0), (31033131008, 5767168, 161480704, 0), (31021596672, 5767168, 167247872, 0), (31079268352, 5767168, 173015040, 0), (31085035520, 5767168, 178782208, 0), (31073501184, 5767168, 184549376, 0), (31113871360, 5767168, 190316544, 0), (31119638528, 5767168, 196083712, 0), (31108104192, 5767168, 201850880, 0), (31148474368, 5767168, 207618048, 0), (31154241536, 5767168, 213385216, 0), (31142707200, 5767168, 219152384, 0), (31234981888, 5767168, 224919552, 0), (31240749056, 5767168, 230686720, 0), (31229214720, 5767168, 236453888, 0), (31338790912, 5767168, 242221056, 0), (31344558080, 5767168, 247988224, 0), (31333023744, 5767168, 253755392, 0), (31407996928, 5767168, 259522560, 0), (31413764096, 5767168, 265289728, 0), (31402229760, 5767168, 271056896, 0), (31442599936, 5767168, 276824064, 0), (31448367104, 5767168, 282591232, 0), (31436832768, 5767168, 288358400, 0), (31459901440, 5767168, 294125568, 0), (31465668608, 5767168, 299892736, 0), (31454134272, 5767168, 305659904, 0), (31477202944, 5767168, 311427072, 0), (31482970112, 5767168, 317194240, 0), (31471435776, 5767168, 322961408, 0), (31581011968, 5767168, 328728576, 0), (31586779136, 5767168, 334495744, 0), (31575244800, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc4fc2d0>, 2: <capsule object NULL at 0x74a6807aa100>}
DEBUG 01-15 16:09:09.604421.604421 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.605602.605602 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.605497.605497 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7c912fb0-ccf6-4d44-9f0a-96955700ff36
DEBUG 01-15 16:09:09.605073.605073 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.605798.605798 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.605340.605340 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:09.606444.606444 client.py:127] Model loaded
DEBUG 01-15 16:09:09.606469.606469 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.606908.606908 cuda_h.py:19] end move_flatidxs cost 0.0008859634399414062 seconds
DEBUG 01-15 16:09:09.606923.606923 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:09.607751.607751 cuda_h.py:19] end restore2model cost 0.001157999038696289 seconds
DEBUG 01-15 16:09:09.607080.607080 cuda_h.py:19] end sllm_worker_task cost 0.012452363967895508 seconds
INFO 01-15 16:09:09.608685.608685 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7c912fb0-ccf6-4d44-9f0a-96955700ff36
DEBUG 01-15 16:09:09.608231.608231 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.006038188934326172 seconds
DEBUG 01-15 16:09:09.608054.608054 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.611827.611827 cuda_h.py:19] end restore2model cost 0.0028531551361083984 seconds
DEBUG 01-15 16:09:09.611140.611140 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.009128332138061523 seconds
DEBUG 01-15 16:09:09.611525.611525 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.611006.611006 cuda_h.py:19] end gpu_sexperts cost 0.0002892017364501953 seconds
DEBUG 01-15 16:09:09.611690.611690 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:09.612367.612367 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.430511474609375e-05 seconds
DEBUG 01-15 16:09:09.612586.612586 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.612004.612004 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.613441.613441 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000957489013671875 seconds
DEBUG 01-15 16:09:09.613476.613476 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.613795.613795 cuda_h.py:19] end gpu_group_list cost 0.00020623207092285156 seconds
DEBUG 01-15 16:09:09.614214.614214 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.615013.615013 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010554790496826172 seconds
DEBUG 01-15 16:09:09.615062.615062 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.615329.615329 cuda_h.py:19] end gpu_group_list cost 0.0002372264862060547 seconds
DEBUG 01-15 16:09:09.616479.616479 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.616606.616606 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7c912fb0-ccf6-4d44-9f0a-96955700ff36
DEBUG 01-15 16:09:09.616320.616320 cuda_h.py:19] end group_tensors cost 0.009238243103027344 seconds
DEBUG 01-15 16:09:09.617646.617646 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.619647.619647 cuda_h.py:19] end group pad cost 0.002751588821411133 seconds
DEBUG 01-15 16:09:09.619768.619768 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:09.639289.639289 cuda_h.py:19] end group_einsum cost 0.019817113876342773 seconds
DEBUG 01-15 16:09:09.639553.639553 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:09.642912.642912 cuda_h.py:19] end get_outputs_cpu1 cost 0.002170085906982422 seconds
INFO 01-15 16:09:09.642588.642588 client.py:127] Model loaded
DEBUG 01-15 16:09:09.642831.642831 cuda_h.py:19] end wait_experts_multi_device cost 0.026266098022460938 seconds
DEBUG 01-15 16:09:09.642071.642071 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.642012.642012 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03737616539001465 seconds
DEBUG 01-15 16:09:09.643013.643013 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.000965118408203125 seconds
DEBUG 01-15 16:09:09.643211.643211 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.646598.646598 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.646528.646528 cuda_h.py:19] end index_scatter cost 0.00015020370483398438 seconds
DEBUG 01-15 16:09:09.647367.647367 cuda_h.py:19] end cpuoutputsdeal cost 0.0029633045196533203 seconds
DEBUG 01-15 16:09:09.647068.647068 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.647236.647236 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.647243.647243 cuda_h.py:19] end gpu_group_tensor cost 0.0003082752227783203 seconds
DEBUG 01-15 16:09:09.647603.647603 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.648284.648284 cuda_h.py:19] end gpu_group_tensor cost 0.00028228759765625 seconds
DEBUG 01-15 16:09:09.648537.648537 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.649921.649921 cuda_h.py:19] end gpu_group_einsum cost 0.0014538764953613281 seconds
DEBUG 01-15 16:09:09.650015.650015 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.651140.651140 cuda_h.py:19] end gpu_group_einsum cost 0.0008580684661865234 seconds
DEBUG 01-15 16:09:09.651883.651883 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.651314.651314 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.652981.652981 cuda_h.py:19] end all_expert_outputs_slices cost 0.00043320655822753906 seconds
DEBUG 01-15 16:09:09.652420.652420 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.652037.652037 cuda_h.py:19] end concat_expert_out cost 0.00011229515075683594 seconds
DEBUG 01-15 16:09:09.652849.652849 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.652929.652929 cuda_h.py:19] end index_scatter cost 0.00011801719665527344 seconds
DEBUG 01-15 16:09:09.653408.653408 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0017180442810058594 seconds
DEBUG 01-15 16:09:09.653858.653858 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.653657.653657 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.653578.653578 cuda_h.py:19] end all_expert_outputs_slices cost 0.000324249267578125 seconds
DEBUG 01-15 16:09:09.654196.654196 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.654202.654202 cuda_h.py:19] end concat_expert_out cost 0.000110626220703125 seconds
DEBUG 01-15 16:09:09.654094.654094 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.654729.654729 cuda_h.py:19] end index_scatter cost 0.00010848045349121094 seconds
DEBUG 01-15 16:09:09.654745.654745 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010938644409179688 seconds
DEBUG 01-15 16:09:09.654717.654717 cuda_h.py:19] end gpu_experts_multi_device cost 0.04265618324279785 seconds
DEBUG 01-15 16:09:09.654471.654471 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.05503582954406738 seconds
DEBUG 01-15 16:09:09.655396.655396 cuda_h.py:19] end prefill_layer cost 0.06083965301513672 seconds
DEBUG 01-15 16:09:09.655063.655063 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:09:09.655436.655436 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:09.655239.655239 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:09:09.655420.655420 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:09.656346.656346 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:09.656137.656137 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:09.660328.660328 cuda_h.py:19] end self_attn cost 0.0034034252166748047 seconds
DEBUG 01-15 16:09:09.660213.660213 cuda_h.py:19] end iln_self_attn_paln cost 0.0046329498291015625 seconds
DEBUG 01-15 16:09:09.660096.660096 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:09:09.660812.660812 cuda_h.py:10] start gate
DEBUG 01-15 16:09:09.661743.661743 cuda_h.py:19] end gate cost 0.0006487369537353516 seconds
DEBUG 01-15 16:09:09.661817.661817 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:09.661440.661440 lmp.py:1912] 
DEBUG 01-15 16:09:09.661440.661440 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:09.661679.661679 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:09.661859.661859 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:09.661985.661985 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:09.661443.661443 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:09.662947.662947 lmp.py:1917] 
DEBUG 01-15 16:09:09.662947.662947 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:09.662690.662690 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:09.662346.662346 lmp.py:1935]   Expert 18 |     65 | CPU
DEBUG 01-15 16:09:09.662612.662612 lmp.py:1935]   Expert 47 |     68 | CPU
DEBUG 01-15 16:09:09.662685.662685 lmp.py:1935]   Expert 54 |     70 | CPU
DEBUG 01-15 16:09:09.662520.662520 lmp.py:1935]   Expert 23 |     78 | CPU
DEBUG 01-15 16:09:09.662355.662355 lmp.py:1935]   Expert 48 |     79 | CPU
DEBUG 01-15 16:09:09.662952.662952 lmp.py:1935]   Expert 44 |     83 | CPU
DEBUG 01-15 16:09:09.662787.662787 lmp.py:1935]   Expert 45 |     84 | CPU
DEBUG 01-15 16:09:09.662384.662384 lmp.py:1935]   Expert 20 |     92 | CPU
DEBUG 01-15 16:09:09.662696.662696 lmp.py:1935]   Expert 31 |     96 | CPU
DEBUG 01-15 16:09:09.662008.662008 lmp.py:1935]   Expert 36 |    105 | CPU
DEBUG 01-15 16:09:09.662227.662227 lmp.py:1935]   Expert 61 |    114 | CPU
DEBUG 01-15 16:09:09.662731.662731 lmp.py:1935]   Expert 33 |    119 | CPU
DEBUG 01-15 16:09:09.662996.662996 lmp.py:1935]   Expert 42 |    120 | CPU
DEBUG 01-15 16:09:09.662070.662070 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:09:09.662143.662143 lmp.py:1935]   Expert 24 |    121 | CPU
DEBUG 01-15 16:09:09.662217.662217 lmp.py:1935]   Expert 43 |    122 | CPU
DEBUG 01-15 16:09:09.662290.662290 lmp.py:1935]   Expert 49 |    126 | CPU
DEBUG 01-15 16:09:09.662364.662364 lmp.py:1935]   Expert 11 |    128 | CPU
DEBUG 01-15 16:09:09.662199.662199 lmp.py:1935]   Expert 56 |    132 | CPU
DEBUG 01-15 16:09:09.662272.662272 lmp.py:1935]   Expert  6 |    136 | CPU
DEBUG 01-15 16:09:09.662107.662107 lmp.py:1935]   Expert 51 |    143 | CPU
DEBUG 01-15 16:09:09.662419.662419 lmp.py:1935]   Expert  0 |    148 | CPU
DEBUG 01-15 16:09:09.662731.662731 lmp.py:1935]   Expert 17 |    149 | CPU
DEBUG 01-15 16:09:09.662997.662997 lmp.py:1935]   Expert  5 |    157 | CPU
DEBUG 01-15 16:09:09.662262.662262 lmp.py:1935]   Expert 40 |    157 | CPU
DEBUG 01-15 16:09:09.662389.662389 lmp.py:1935]   Expert 12 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662562.662562 lmp.py:1935]   Expert 55 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662543.662543 lmp.py:1935]   Expert 57 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662047.662047 lmp.py:1935]   Expert 59 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662551.662551 lmp.py:1935]   Expert 26 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662578.662578 lmp.py:1935]   Expert 38 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662128.662128 lmp.py:1935]   Expert 46 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662155.662155 lmp.py:1935]   Expert 13 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662659.662659 lmp.py:1935]   Expert 58 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662879.662879 lmp.py:1935]   Expert 50 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662575.662575 lmp.py:1935]   Expert 35 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662509.662509 lmp.py:1935]   Expert  7 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662205.662205 lmp.py:1935]   Expert 30 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662709.662709 lmp.py:1935]   Expert 16 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662736.662736 lmp.py:1935]   Expert 15 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662764.662764 lmp.py:1935]   Expert 32 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662791.662791 lmp.py:1935]   Expert 14 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662818.662818 lmp.py:1935]   Expert  1 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662845.662845 lmp.py:1935]   Expert  3 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662395.662395 lmp.py:1935]   Expert  4 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662184.662184 lmp.py:1935]   Expert 34 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.662450.662450 lmp.py:1935]   Expert 39 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.662669.662669 lmp.py:1935]   Expert 28 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.663650.663650 lmp.py:1935]   Expert 52 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.663631.663631 lmp.py:1935]   Expert 25 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.663327.663327 lmp.py:1935]   Expert 22 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.663592.663592 lmp.py:1935]   Expert  2 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.663620.663620 lmp.py:1935]   Expert 21 |    281 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.663408.663408 lmp.py:1935]   Expert 41 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.663959.663959 lmp.py:1935]   Expert 60 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.663509.663509 lmp.py:1935]   Expert 29 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.663536.663536 lmp.py:1935]   Expert 63 |    292 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.663086.663086 lmp.py:1935]   Expert 62 |    296 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.663875.663875 lmp.py:1935]   Expert 27 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.663902.663902 lmp.py:1935]   Expert 37 |    330 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.663122.663122 lmp.py:1935]   Expert 53 |    331 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.663579.663579 lmp.py:1935]   Expert  8 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.663560.663560 lmp.py:1935]   Expert 19 |    443 | GPU2(cuda:2)
DEBUG 01-15 16:09:09.663826.663826 lmp.py:1935]   Expert  9 |    616 | GPU1(cuda:1)
DEBUG 01-15 16:09:09.663661.663661 lmp.py:1937] 
DEBUG 01-15 16:09:09.663661.663661 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:09.663211.663211 lmp.py:1938]   CPU:   2813 tokens
DEBUG 01-15 16:09:09.663715.663715 lmp.py:1942]   cuda:1:   4687 tokens (19 experts)
DEBUG 01-15 16:09:09.663027.663027 lmp.py:1942]   cuda:2:   4788 tokens (20 experts)
DEBUG 01-15 16:09:09.663100.663100 lmp.py:1943]   Total GPU:   9475 tokens
DEBUG 01-15 16:09:09.663697.663697 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:09.663697.663697 lmp.py:1944] 
DEBUG 01-15 16:09:09.663844.663844 cuda_h.py:19] end experts_map_get cost 0.0019397735595703125 seconds
DEBUG 01-15 16:09:09.663555.663555 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:09.663126.663126 lmp.py:1953] 
DEBUG 01-15 16:09:09.663126.663126 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:09.663757.663757 cuda_h.py:19] end cpu_experts_submit cost 7.796287536621094e-05 seconds
DEBUG 01-15 16:09:09.663030.663030 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:09.663608.663608 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:09.664125.664125 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.1.gate_proj.weight': 0, 'model.layers.27.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.50.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.50.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.50.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.60.up_proj.weight': 322961408}, 2: {'model.layers.27.mlp.experts.2.gate_proj.weight': 0, 'model.layers.27.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.26.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.26.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.26.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.28.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.28.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.28.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.30.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.30.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.30.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.37.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.37.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.37.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.39.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.39.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.39.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.46.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.46.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.46.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.27.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.27.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.27.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(31667519488, 5767168, 0, 0), (31673286656, 5767168, 5767168, 0), (31661752320, 5767168, 11534336, 0), (31719424000, 5767168, 17301504, 0), (31725191168, 5767168, 23068672, 0), (31713656832, 5767168, 28835840, 0), (31771328512, 5767168, 34603008, 0), (31777095680, 5767168, 40370176, 0), (31765561344, 5767168, 46137344, 0), (31805931520, 5767168, 51904512, 0), (31811698688, 5767168, 57671680, 0), (31800164352, 5767168, 63438848, 0), (31927042048, 5767168, 69206016, 0), (31932809216, 5767168, 74973184, 0), (31921274880, 5767168, 80740352, 0), (32030851072, 5767168, 86507520, 0), (32036618240, 5767168, 92274688, 0), (32025083904, 5767168, 98041856, 0), (32117358592, 5767168, 103809024, 0), (32123125760, 5767168, 109576192, 0), (32111591424, 5767168, 115343360, 0), (32151961600, 5767168, 121110528, 0), (32157728768, 5767168, 126877696, 0), (32146194432, 5767168, 132644864, 0), (32203866112, 5767168, 138412032, 0), (32209633280, 5767168, 144179200, 0), (32198098944, 5767168, 149946368, 0), (32238469120, 5767168, 155713536, 0), (32244236288, 5767168, 161480704, 0), (32232701952, 5767168, 167247872, 0), (32307675136, 5767168, 173015040, 0), (32313442304, 5767168, 178782208, 0), (32301907968, 5767168, 184549376, 0), (32359579648, 5767168, 190316544, 0), (32365346816, 5767168, 196083712, 0), (32353812480, 5767168, 201850880, 0), (32515293184, 5767168, 207618048, 0), (32521060352, 5767168, 213385216, 0), (32509526016, 5767168, 219152384, 0), (32549896192, 5767168, 224919552, 0), (32555663360, 5767168, 230686720, 0), (32544129024, 5767168, 236453888, 0), (32567197696, 5767168, 242221056, 0), (32572964864, 5767168, 247988224, 0), (32561430528, 5767168, 253755392, 0), (32601800704, 5767168, 259522560, 0), (32607567872, 5767168, 265289728, 0), (32596033536, 5767168, 271056896, 0), (32636403712, 5767168, 276824064, 0), (32642170880, 5767168, 282591232, 0), (32630636544, 5767168, 288358400, 0), (32653705216, 5767168, 294125568, 0), (32659472384, 5767168, 299892736, 0), (32647938048, 5767168, 305659904, 0), (32688308224, 5767168, 311427072, 0), (32694075392, 5767168, 317194240, 0), (32682541056, 5767168, 322961408, 0)], 2: [(31684820992, 5767168, 0, 0), (31690588160, 5767168, 5767168, 0), (31679053824, 5767168, 11534336, 0), (31702122496, 5767168, 17301504, 0), (31707889664, 5767168, 23068672, 0), (31696355328, 5767168, 28835840, 0), (31788630016, 5767168, 34603008, 0), (31794397184, 5767168, 40370176, 0), (31782862848, 5767168, 46137344, 0), (31857836032, 5767168, 51904512, 0), (31863603200, 5767168, 57671680, 0), (31852068864, 5767168, 63438848, 0), (31875137536, 5767168, 69206016, 0), (31880904704, 5767168, 74973184, 0), (31869370368, 5767168, 80740352, 0), (31892439040, 5767168, 86507520, 0), (31898206208, 5767168, 92274688, 0), (31886671872, 5767168, 98041856, 0), (31909740544, 5767168, 103809024, 0), (31915507712, 5767168, 109576192, 0), (31903973376, 5767168, 115343360, 0), (31978946560, 5767168, 121110528, 0), (31984713728, 5767168, 126877696, 0), (31973179392, 5767168, 132644864, 0), (32013549568, 5767168, 138412032, 0), (32019316736, 5767168, 144179200, 0), (32007782400, 5767168, 149946368, 0), (32082755584, 5767168, 155713536, 0), (32088522752, 5767168, 161480704, 0), (32076988416, 5767168, 167247872, 0), (32100057088, 5767168, 173015040, 0), (32105824256, 5767168, 178782208, 0), (32094289920, 5767168, 184549376, 0), (32134660096, 5767168, 190316544, 0), (32140427264, 5767168, 196083712, 0), (32128892928, 5767168, 201850880, 0), (32169263104, 5767168, 207618048, 0), (32175030272, 5767168, 213385216, 0), (32163495936, 5767168, 219152384, 0), (32255770624, 5767168, 224919552, 0), (32261537792, 5767168, 230686720, 0), (32250003456, 5767168, 236453888, 0), (32290373632, 5767168, 242221056, 0), (32296140800, 5767168, 247988224, 0), (32284606464, 5767168, 253755392, 0), (32324976640, 5767168, 259522560, 0), (32330743808, 5767168, 265289728, 0), (32319209472, 5767168, 271056896, 0), (32446087168, 5767168, 276824064, 0), (32451854336, 5767168, 282591232, 0), (32440320000, 5767168, 288358400, 0), (32671006720, 5767168, 294125568, 0), (32676773888, 5767168, 299892736, 0), (32665239552, 5767168, 305659904, 0), (32722911232, 5767168, 311427072, 0), (32728678400, 5767168, 317194240, 0), (32717144064, 5767168, 322961408, 0), (32740212736, 5767168, 328728576, 0), (32745979904, 5767168, 334495744, 0), (32734445568, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a680612280>, 2: <capsule object NULL at 0x74a68069bba0>}
DEBUG 01-15 16:09:09.665209.665209 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:09.665225.665225 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:09.665849.665849 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9c6913db-b596-4876-9d67-6e070d44a909
DEBUG 01-15 16:09:09.665601.665601 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:09.665795.665795 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:09.665547.665547 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:09.666473.666473 cuda_h.py:19] end move_flatidxs cost 0.0008647441864013672 seconds
DEBUG 01-15 16:09:09.666203.666203 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:09.668328.668328 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9c6913db-b596-4876-9d67-6e070d44a909
DEBUG 01-15 16:09:09.668129.668129 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004838466644287109 seconds
DEBUG 01-15 16:09:09.668060.668060 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:09.671164.671164 cuda_h.py:19] end restore2model cost 0.003093719482421875 seconds
DEBUG 01-15 16:09:09.671153.671153 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008212089538574219 seconds
DEBUG 01-15 16:09:09.671041.671041 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:09.671498.671498 cuda_h.py:19] end group_tensors cost 0.004629373550415039 seconds
DEBUG 01-15 16:09:09.672932.672932 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:09.672787.672787 cuda_h.py:19] end gpu_sexperts cost 0.0002732276916503906 seconds
DEBUG 01-15 16:09:09.672755.672755 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:09.672087.672087 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.673701.673701 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010867118835449219 seconds
DEBUG 01-15 16:09:09.673505.673505 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.673672.673672 cuda_h.py:19] end gpu_group_list cost 0.0002357959747314453 seconds
DEBUG 01-15 16:09:09.674047.674047 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:09.675746.675746 cuda_h.py:19] end group pad cost 0.003077268600463867 seconds
DEBUG 01-15 16:09:09.675635.675635 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:09.675273.675273 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011527538299560547 seconds
DEBUG 01-15 16:09:09.676425.676425 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:09.676448.676448 cuda_h.py:19] end gpu_group_list cost 0.00042700767517089844 seconds
DEBUG 01-15 16:09:09.677369.677369 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:09.677174.677174 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9c6913db-b596-4876-9d67-6e070d44a909
DEBUG 01-15 16:09:09.704965.704965 cuda_h.py:19] end group_einsum cost 0.02892160415649414 seconds
DEBUG 01-15 16:09:09.704851.704851 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:09.707710.707710 client.py:127] Model loaded
DEBUG 01-15 16:09:09.707444.707444 cuda_h.py:19] end wait_experts_multi_device cost 0.029630422592163086 seconds
DEBUG 01-15 16:09:09.707611.707611 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:09.707252.707252 cuda_h.py:19] end get_outputs_cpu1 cost 0.002790689468383789 seconds
DEBUG 01-15 16:09:09.708199.708199 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.042647600173950195 seconds
DEBUG 01-15 16:09:09.708983.708983 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0011653900146484375 seconds
DEBUG 01-15 16:09:09.708237.708237 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:09.710709.710709 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.710989.710989 cuda_h.py:19] end index_scatter cost 0.0001308917999267578 seconds
DEBUG 01-15 16:09:09.710718.710718 cuda_h.py:19] end cpuoutputsdeal cost 0.0020117759704589844 seconds
DEBUG 01-15 16:09:09.711901.711901 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:09.711565.711565 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.711576.711576 cuda_h.py:19] end gpu_group_tensor cost 0.0002601146697998047 seconds
DEBUG 01-15 16:09:09.711625.711625 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:09.711509.711509 cuda_h.py:19] end gpu_group_tensor cost 0.00023865699768066406 seconds
DEBUG 01-15 16:09:09.712761.712761 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.713980.713980 cuda_h.py:19] end gpu_group_einsum cost 0.0010952949523925781 seconds
DEBUG 01-15 16:09:09.713566.713566 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:09.714484.714484 cuda_h.py:19] end gpu_group_einsum cost 0.0006508827209472656 seconds
DEBUG 01-15 16:09:09.714688.714688 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.714056.714056 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.714848.714848 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002532005310058594 seconds
DEBUG 01-15 16:09:09.714624.714624 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.714422.714422 cuda_h.py:19] end concat_expert_out cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:09:09.715371.715371 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.715560.715560 cuda_h.py:19] end index_scatter cost 7.128715515136719e-05 seconds
DEBUG 01-15 16:09:09.715622.715622 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009450912475585938 seconds
DEBUG 01-15 16:09:09.715950.715950 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:09.715276.715276 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:09.715905.715905 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015163421630859375 seconds
DEBUG 01-15 16:09:09.715754.715754 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:09.715492.715492 cuda_h.py:19] end concat_expert_out cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:09:09.715573.715573 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:09.715550.715550 cuda_h.py:19] end index_scatter cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:09:09.716359.716359 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005109310150146484 seconds
DEBUG 01-15 16:09:09.716719.716719 cuda_h.py:19] end gpu_experts_multi_device cost 0.04380345344543457 seconds
DEBUG 01-15 16:09:09.716821.716821 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.05545330047607422 seconds
DEBUG 01-15 16:09:09.716582.716582 cuda_h.py:19] end prefill_layer cost 0.06086015701293945 seconds
DEBUG 01-15 16:09:09.716915.716915 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:09:09.716625.716625 cuda_h.py:19] end prefill cost 1.7788019180297852 seconds
DEBUG 01-15 16:09:11.940523.940523 cuda_h.py:10] start generate_input_ids
generate input ids cost 0.09083890914916992 s
DEBUG 01-15 16:09:12.302763.302763 cuda_h.py:19] end generate_input_ids cost 0.360506534576416 seconds
DEBUG 01-15 16:09:12.302160.302160 cuda_h.py:10] start init_cache
DEBUG 01-15 16:09:12.302978.302978 cuda_h.py:19] end init_cache cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:09:14.777851.777851 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:09:14.780774.780774 cuda_h.py:19] end init_meta_layer cost 1.7404556274414062e-05 seconds
DEBUG 01-15 16:09:14.780675.780675 cuda_h.py:10] start init_weights
DEBUG 01-15 16:09:14.780068.780068 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:14.780115.780115 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:14.781586.781586 cuda_h.py:19] end allocate_cuda_memory cost 0.0009837150573730469 seconds
DEBUG 01-15 16:09:14.781019.781019 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:14.781305.781305 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:14.781393.781393 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:14.781811.781811 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5276adac-0753-49fd-a27e-ca376d4c9a0c
DEBUG 01-15 16:09:14.781007.781007 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:14.783760.783760 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5276adac-0753-49fd-a27e-ca376d4c9a0c
DEBUG 01-15 16:09:14.783095.783095 cuda_h.py:19] end load_into_gpu_async cost 0.0024306774139404297 seconds
DEBUG 01-15 16:09:14.783813.783813 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:14.784620.784620 cuda_h.py:19] end restore_tensors2 cost 0.00011444091796875 seconds
DEBUG 01-15 16:09:14.784846.784846 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0038518905639648438 seconds
DEBUG 01-15 16:09:14.784635.784635 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:14.784893.784893 cuda_h.py:19] end restore2model cost 0.0001678466796875 seconds
INFO 01-15 16:09:14.784630.784630 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5276adac-0753-49fd-a27e-ca376d4c9a0c
INFO 01-15 16:09:14.862667.862667 client.py:127] Model loaded
DEBUG 01-15 16:09:14.862844.862844 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:09:14.862167.862167 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:14.862906.862906 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:14.863949.863949 cuda_h.py:19] end allocate_cuda_memory cost 0.00039839744567871094 seconds
DEBUG 01-15 16:09:14.863662.863662 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:14.863579.863579 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:14.863323.863323 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:14.863226.863226 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 04109ec7-cf03-4d23-9ddf-f42956225b0d
DEBUG 01-15 16:09:14.863219.863219 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:14.865156.865156 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 04109ec7-cf03-4d23-9ddf-f42956225b0d
DEBUG 01-15 16:09:14.865849.865849 cuda_h.py:19] end load_into_gpu_async cost 0.0021233558654785156 seconds
DEBUG 01-15 16:09:14.865652.865652 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:14.865083.865083 cuda_h.py:19] end restore_tensors2 cost 0.00014209747314453125 seconds
DEBUG 01-15 16:09:14.865536.865536 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003246784210205078 seconds
INFO 01-15 16:09:14.865009.865009 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 04109ec7-cf03-4d23-9ddf-f42956225b0d
INFO 01-15 16:09:14.882225.882225 client.py:127] Model loaded
DEBUG 01-15 16:09:14.882985.882985 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:14.883857.883857 cuda_h.py:19] end restore2model cost 0.0008280277252197266 seconds
DEBUG 01-15 16:09:14.883557.883557 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.021181821823120117 seconds
DEBUG 01-15 16:09:14.883765.883765 cuda_h.py:19] end init_weights cost 0.10350251197814941 seconds
DEBUG 01-15 16:09:14.883522.883522 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:09:15.588076.588076 cuda_h.py:19] end copy_emodel cost 0.7045154571533203 seconds
DEBUG 01-15 16:09:15.589321.589321 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:09:15.589359.589359 cuda_h.py:19] end init_inputs_tokens cost 0.0003025531768798828 seconds
DEBUG 01-15 16:09:15.589606.589606 cuda_h.py:10] start prefill
DEBUG 01-15 16:09:15.589753.589753 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:15.589641.589641 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:09:15.589906.589906 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:09:15.589702.589702 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:09:15.589274.589274 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 4.3392181396484375e-05 seconds
DEBUG 01-15 16:09:15.589805.589805 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 9.107589721679688e-05 seconds
DEBUG 01-15 16:09:15.589448.589448 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:15.590629.590629 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:15.590744.590744 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:15.590968.590968 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:15.590593.590593 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:15.590993.590993 cuda_h.py:19] end allocate_cuda_memory cost 0.00021457672119140625 seconds
DEBUG 01-15 16:09:15.590413.590413 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:15.590475.590475 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.590510.590510 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.590465.590465 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 264c3ed0-1d5d-4113-bc47-64310c6145b9
DEBUG 01-15 16:09:15.590085.590085 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.591905.591905 cuda_h.py:10] start self_attn
INFO 01-15 16:09:15.593726.593726 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 264c3ed0-1d5d-4113-bc47-64310c6145b9
DEBUG 01-15 16:09:15.593445.593445 cuda_h.py:19] end load_into_gpu_async cost 0.0024564266204833984 seconds
DEBUG 01-15 16:09:15.593341.593341 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:15.593545.593545 cuda_h.py:19] end restore_tensors2 cost 0.000102996826171875 seconds
DEBUG 01-15 16:09:15.593725.593725 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003180265426635742 seconds
INFO 01-15 16:09:15.593702.593702 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 264c3ed0-1d5d-4113-bc47-64310c6145b9
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:15.595475.595475 cuda_h.py:19] end self_attn cost 0.0038988590240478516 seconds
DEBUG 01-15 16:09:15.595200.595200 cuda_h.py:19] end iln_self_attn_paln cost 0.005616664886474609 seconds
DEBUG 01-15 16:09:15.595261.595261 cuda_h.py:10] start dense_mlp
INFO 01-15 16:09:15.602922.602922 client.py:127] Model loaded
DEBUG 01-15 16:09:15.602786.602786 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.603682.603682 cuda_h.py:19] end restore2model cost 0.0009596347808837891 seconds
DEBUG 01-15 16:09:15.603924.603924 cuda_h.py:19] end sllm_worker_task cost 0.013519525527954102 seconds
DEBUG 01-15 16:09:15.603055.603055 cuda_h.py:19] end dense_mlp cost 0.008233308792114258 seconds
DEBUG 01-15 16:09:15.604940.604940 cuda_h.py:19] end prefill_layer cost 0.014259576797485352 seconds
DEBUG 01-15 16:09:15.604140.604140 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:09:15.604035.604035 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:15.604837.604837 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:09:15.604732.604732 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:09:15.604488.604488 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:09:15.604185.604185 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 2.5987625122070312e-05 seconds
DEBUG 01-15 16:09:15.604279.604279 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 6.29425048828125e-05 seconds
DEBUG 01-15 16:09:15.604413.604413 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:15.604448.604448 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:15.604671.604671 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:15.604489.604489 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:15.604693.604693 cuda_h.py:19] end allocate_cuda_memory cost 0.0002770423889160156 seconds
DEBUG 01-15 16:09:15.605194.605194 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:15.605555.605555 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:15.605461.605461 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.605101.605101 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.605237.605237 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6e3fdd81-3b50-448d-a321-750085b2b71d
DEBUG 01-15 16:09:15.605978.605978 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.606728.606728 cuda_h.py:10] start self_attn
INFO 01-15 16:09:15.607626.607626 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6e3fdd81-3b50-448d-a321-750085b2b71d
DEBUG 01-15 16:09:15.607769.607769 cuda_h.py:19] end load_into_gpu_async cost 0.002084016799926758 seconds
DEBUG 01-15 16:09:15.607633.607633 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:15.607117.607117 cuda_h.py:19] end restore_tensors2 cost 0.0001285076141357422 seconds
DEBUG 01-15 16:09:15.607167.607167 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003273487091064453 seconds
INFO 01-15 16:09:15.607045.607045 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6e3fdd81-3b50-448d-a321-750085b2b71d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:15.609419.609419 cuda_h.py:19] end self_attn cost 0.0035567283630371094 seconds
DEBUG 01-15 16:09:15.610848.610848 cuda_h.py:19] end iln_self_attn_paln cost 0.005653858184814453 seconds
DEBUG 01-15 16:09:15.610513.610513 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:09:15.610998.610998 cuda_h.py:10] start gate
DEBUG 01-15 16:09:15.611705.611705 cuda_h.py:19] end gate cost 0.0009016990661621094 seconds
DEBUG 01-15 16:09:15.611455.611455 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:15.611430.611430 lmp.py:1912] 
DEBUG 01-15 16:09:15.611430.611430 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:15.611352.611352 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:15.611585.611585 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:15.611288.611288 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:15.611845.611845 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:15.611210.611210 lmp.py:1917] 
DEBUG 01-15 16:09:15.611210.611210 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:15.611006.611006 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:15.611669.611669 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:09:15.611511.611511 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:09:15.611637.611637 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:09:15.611287.611287 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:09:15.611414.611414 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:09:15.611302.611302 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:09:15.611952.611952 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:09:15.611601.611601 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:09:15.611251.611251 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:09:15.611616.611616 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:09:15.611697.611697 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:09:15.612538.612538 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:09:15.612380.612380 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:09:15.612792.612792 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:09:15.612965.612965 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:09:15.612614.612614 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:09:15.612086.612086 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:09:15.612729.612729 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:09:15.612610.612610 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:09:15.612207.612207 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:09:15.612565.612565 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:09:15.612161.612161 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:09:15.612281.612281 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:09:15.612447.612447 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:09:15.612614.612614 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:09:15.612925.612925 lmp.py:1935]   Expert 34 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612284.612284 lmp.py:1935]   Expert 35 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612403.612403 lmp.py:1935]   Expert 36 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612762.612762 lmp.py:1935]   Expert 11 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612643.612643 lmp.py:1935]   Expert 17 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612492.612492 lmp.py:1935]   Expert 59 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612373.612373 lmp.py:1935]   Expert 10 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612731.612731 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612043.612043 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612832.612832 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612144.612144 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612456.612456 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612052.612052 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612172.612172 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612815.612815 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612458.612458 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612340.612340 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612221.612221 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612102.612102 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612984.612984 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612342.612342 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612985.612985 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612628.612628 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612271.612271 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612914.612914 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612557.612557 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612677.612677 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612320.612320 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612155.612155 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612275.612275 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612077.612077 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612958.612958 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612363.612363 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612767.612767 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612172.612172 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612577.612577 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.612981.612981 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.612386.612386 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.613029.613029 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.613003.613003 lmp.py:1937] 
DEBUG 01-15 16:09:15.613003.613003 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:15.613884.613884 lmp.py:1938]   CPU:   2872 tokens
DEBUG 01-15 16:09:15.613004.613004 lmp.py:1942]   cuda:1:   4628 tokens (19 experts)
DEBUG 01-15 16:09:15.613647.613647 lmp.py:1942]   cuda:2:   4788 tokens (20 experts)
DEBUG 01-15 16:09:15.613098.613098 lmp.py:1943]   Total GPU:   9416 tokens
DEBUG 01-15 16:09:15.613310.613310 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:15.613310.613310 lmp.py:1944] 
DEBUG 01-15 16:09:15.613437.613437 cuda_h.py:19] end experts_map_get cost 0.001988649368286133 seconds
DEBUG 01-15 16:09:15.613678.613678 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:15.613672.613672 lmp.py:1953] 
DEBUG 01-15 16:09:15.613672.613672 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:15.613376.613376 cuda_h.py:19] end cpu_experts_submit cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:09:15.613139.613139 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:15.613684.613684 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:15.614484.614484 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.39.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.39.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.39.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.1.mlp.experts.2.gate_proj.weight': 0, 'model.layers.1.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.46.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.46.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.46.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.1.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.1.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.1.mlp.experts.59.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3016228864, 5767168, 34603008, 0), (3021996032, 5767168, 40370176, 0), (3010461696, 5767168, 46137344, 0), (3033530368, 5767168, 51904512, 0), (3039297536, 5767168, 57671680, 0), (3027763200, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3154640896, 5767168, 86507520, 0), (3160408064, 5767168, 92274688, 0), (3148873728, 5767168, 98041856, 0), (3189243904, 5767168, 103809024, 0), (3195011072, 5767168, 109576192, 0), (3183476736, 5767168, 115343360, 0), (3206545408, 5767168, 121110528, 0), (3212312576, 5767168, 126877696, 0), (3200778240, 5767168, 132644864, 0), (3275751424, 5767168, 138412032, 0), (3281518592, 5767168, 144179200, 0), (3269984256, 5767168, 149946368, 0), (3310354432, 5767168, 155713536, 0), (3316121600, 5767168, 161480704, 0), (3304587264, 5767168, 167247872, 0), (3362258944, 5767168, 173015040, 0), (3368026112, 5767168, 178782208, 0), (3356491776, 5767168, 184549376, 0), (3379560448, 5767168, 190316544, 0), (3385327616, 5767168, 196083712, 0), (3373793280, 5767168, 201850880, 0), (3466067968, 5767168, 207618048, 0), (3471835136, 5767168, 213385216, 0), (3460300800, 5767168, 219152384, 0), (3535273984, 5767168, 224919552, 0), (3541041152, 5767168, 230686720, 0), (3529506816, 5767168, 236453888, 0), (3690987520, 5767168, 242221056, 0), (3696754688, 5767168, 247988224, 0), (3685220352, 5767168, 253755392, 0), (3725590528, 5767168, 259522560, 0), (3731357696, 5767168, 265289728, 0), (3719823360, 5767168, 271056896, 0), (3846701056, 5767168, 276824064, 0), (3852468224, 5767168, 282591232, 0), (3840933888, 5767168, 288358400, 0), (3915907072, 5767168, 294125568, 0), (3921674240, 5767168, 299892736, 0), (3910139904, 5767168, 305659904, 0), (3950510080, 5767168, 311427072, 0), (3956277248, 5767168, 317194240, 0), (3944742912, 5767168, 322961408, 0)], 2: [(2895118336, 5767168, 0, 0), (2900885504, 5767168, 5767168, 0), (2889351168, 5767168, 11534336, 0), (2929721344, 5767168, 17301504, 0), (2935488512, 5767168, 23068672, 0), (2923954176, 5767168, 28835840, 0), (2947022848, 5767168, 34603008, 0), (2952790016, 5767168, 40370176, 0), (2941255680, 5767168, 46137344, 0), (2998927360, 5767168, 51904512, 0), (3004694528, 5767168, 57671680, 0), (2993160192, 5767168, 63438848, 0), (3050831872, 5767168, 69206016, 0), (3056599040, 5767168, 74973184, 0), (3045064704, 5767168, 80740352, 0), (3102736384, 5767168, 86507520, 0), (3108503552, 5767168, 92274688, 0), (3096969216, 5767168, 98041856, 0), (3120037888, 5767168, 103809024, 0), (3125805056, 5767168, 109576192, 0), (3114270720, 5767168, 115343360, 0), (3137339392, 5767168, 121110528, 0), (3143106560, 5767168, 126877696, 0), (3131572224, 5767168, 132644864, 0), (3223846912, 5767168, 138412032, 0), (3229614080, 5767168, 144179200, 0), (3218079744, 5767168, 149946368, 0), (3258449920, 5767168, 155713536, 0), (3264217088, 5767168, 161480704, 0), (3252682752, 5767168, 167247872, 0), (3431464960, 5767168, 173015040, 0), (3437232128, 5767168, 178782208, 0), (3425697792, 5767168, 184549376, 0), (3448766464, 5767168, 190316544, 0), (3454533632, 5767168, 196083712, 0), (3442999296, 5767168, 201850880, 0), (3483369472, 5767168, 207618048, 0), (3489136640, 5767168, 213385216, 0), (3477602304, 5767168, 219152384, 0), (3552575488, 5767168, 224919552, 0), (3558342656, 5767168, 230686720, 0), (3546808320, 5767168, 236453888, 0), (3587178496, 5767168, 242221056, 0), (3592945664, 5767168, 247988224, 0), (3581411328, 5767168, 253755392, 0), (3639083008, 5767168, 259522560, 0), (3644850176, 5767168, 265289728, 0), (3633315840, 5767168, 271056896, 0), (3656384512, 5767168, 276824064, 0), (3662151680, 5767168, 282591232, 0), (3650617344, 5767168, 288358400, 0), (3777495040, 5767168, 294125568, 0), (3783262208, 5767168, 299892736, 0), (3771727872, 5767168, 305659904, 0), (3829399552, 5767168, 311427072, 0), (3835166720, 5767168, 317194240, 0), (3823632384, 5767168, 322961408, 0), (3881304064, 5767168, 328728576, 0), (3887071232, 5767168, 334495744, 0), (3875536896, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a9f3729b00>, 2: <capsule object NULL at 0x74a9f372a640>}
DEBUG 01-15 16:09:15.614329.614329 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.614404.614404 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.614253.614253 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 64e868bc-7d55-4a86-8222-3213d132e899
DEBUG 01-15 16:09:15.615310.615310 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:15.615352.615352 client.py:127] Model loaded
DEBUG 01-15 16:09:15.615786.615786 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.616544.616544 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:15.616433.616433 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:15.616825.616825 cuda_h.py:19] end restore2model cost 0.0008916854858398438 seconds
DEBUG 01-15 16:09:15.616398.616398 cuda_h.py:19] end sllm_worker_task cost 0.012446403503417969 seconds
INFO 01-15 16:09:15.617059.617059 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 64e868bc-7d55-4a86-8222-3213d132e899
DEBUG 01-15 16:09:15.617366.617366 cuda_h.py:19] end move_flatidxs cost 0.0008840560913085938 seconds
DEBUG 01-15 16:09:15.617878.617878 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:15.617208.617208 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004357814788818359 seconds
DEBUG 01-15 16:09:15.617224.617224 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.620329.620329 cuda_h.py:19] end restore2model cost 0.003095865249633789 seconds
DEBUG 01-15 16:09:15.621000.621000 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0077054500579833984 seconds
DEBUG 01-15 16:09:15.621034.621034 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:15.621726.621726 cuda_h.py:19] end gpu_sexperts cost 0.0002682209014892578 seconds
DEBUG 01-15 16:09:15.621602.621602 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:15.621849.621849 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.3589859008789062e-05 seconds
DEBUG 01-15 16:09:15.621114.621114 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:15.621579.621579 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.622498.622498 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009288787841796875 seconds
DEBUG 01-15 16:09:15.622580.622580 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.622607.622607 cuda_h.py:19] end gpu_group_list cost 0.00020265579223632812 seconds
DEBUG 01-15 16:09:15.623573.623573 cuda_h.py:19] end group_tensors cost 0.005696296691894531 seconds
DEBUG 01-15 16:09:15.623203.623203 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.624061.624061 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:15.625253.625253 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0018351078033447266 seconds
DEBUG 01-15 16:09:15.625905.625905 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.626220.626220 cuda_h.py:19] end gpu_group_list cost 0.0002627372741699219 seconds
DEBUG 01-15 16:09:15.627996.627996 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:15.627799.627799 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 64e868bc-7d55-4a86-8222-3213d132e899
DEBUG 01-15 16:09:15.627715.627715 cuda_h.py:19] end group pad cost 0.003274679183959961 seconds
DEBUG 01-15 16:09:15.627459.627459 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:15.655008.655008 client.py:127] Model loaded
DEBUG 01-15 16:09:15.656702.656702 cuda_h.py:19] end wait_experts_multi_device cost 0.02891373634338379 seconds
DEBUG 01-15 16:09:15.656439.656439 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:15.656921.656921 cuda_h.py:19] end group_einsum cost 0.02855372428894043 seconds
DEBUG 01-15 16:09:15.656773.656773 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:15.659806.659806 cuda_h.py:19] end get_outputs_cpu1 cost 0.002869129180908203 seconds
DEBUG 01-15 16:09:15.660245.660245 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04377007484436035 seconds
DEBUG 01-15 16:09:15.660152.660152 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.004791975021362305 seconds
DEBUG 01-15 16:09:15.661290.661290 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:15.663690.663690 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.663901.663901 cuda_h.py:19] end index_scatter cost 0.0002148151397705078 seconds
DEBUG 01-15 16:09:15.667311.667311 cuda_h.py:19] end cpuoutputsdeal cost 0.00664830207824707 seconds
DEBUG 01-15 16:09:15.668529.668529 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:15.668851.668851 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.669563.669563 cuda_h.py:19] end gpu_group_tensor cost 0.0014128684997558594 seconds
DEBUG 01-15 16:09:15.669018.669018 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.671654.671654 cuda_h.py:19] end gpu_group_tensor cost 0.0011169910430908203 seconds
DEBUG 01-15 16:09:15.671972.671972 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.672956.672956 cuda_h.py:19] end gpu_group_einsum cost 0.001409769058227539 seconds
DEBUG 01-15 16:09:15.672785.672785 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.674076.674076 cuda_h.py:19] end gpu_group_einsum cost 0.0018801689147949219 seconds
DEBUG 01-15 16:09:15.675718.675718 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.675691.675691 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.675559.675559 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003407001495361328 seconds
DEBUG 01-15 16:09:15.676932.676932 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.676964.676964 cuda_h.py:19] end concat_expert_out cost 0.00027632713317871094 seconds
DEBUG 01-15 16:09:15.676956.676956 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.676658.676658 cuda_h.py:19] end index_scatter cost 0.00013184547424316406 seconds
DEBUG 01-15 16:09:15.677659.677659 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0019078254699707031 seconds
DEBUG 01-15 16:09:15.677817.677817 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.677052.677052 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.677260.677260 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023865699768066406 seconds
DEBUG 01-15 16:09:15.677043.677043 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.677618.677618 cuda_h.py:19] end concat_expert_out cost 8.320808410644531e-05 seconds
DEBUG 01-15 16:09:15.677277.677277 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.678997.678997 cuda_h.py:19] end index_scatter cost 8.392333984375e-05 seconds
DEBUG 01-15 16:09:15.678701.678701 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008342266082763672 seconds
DEBUG 01-15 16:09:15.678056.678056 cuda_h.py:19] end gpu_experts_multi_device cost 0.056777238845825195 seconds
DEBUG 01-15 16:09:15.678855.678855 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.06832742691040039 seconds
DEBUG 01-15 16:09:15.679863.679863 cuda_h.py:19] end prefill_layer cost 0.07493758201599121 seconds
DEBUG 01-15 16:09:15.679278.679278 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:09:15.679439.679439 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:15.679122.679122 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:09:15.679283.679283 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:09:15.679596.679596 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:09:15.679414.679414 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 6.246566772460938e-05 seconds
DEBUG 01-15 16:09:15.679913.679913 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 0.00011992454528808594 seconds
DEBUG 01-15 16:09:15.679113.679113 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:15.679773.679773 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:15.679289.679289 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:15.680204.680204 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:15.680647.680647 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:15.681533.681533 cuda_h.py:19] end allocate_cuda_memory cost 0.0005593299865722656 seconds
DEBUG 01-15 16:09:15.681711.681711 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:15.681682.681682 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.681621.681621 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.681486.681486 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 41dbddec-2b58-4e28-9b48-e5aac79105ad
DEBUG 01-15 16:09:15.681023.681023 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.682652.682652 cuda_h.py:10] start self_attn
INFO 01-15 16:09:15.683743.683743 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 41dbddec-2b58-4e28-9b48-e5aac79105ad
DEBUG 01-15 16:09:15.683119.683119 cuda_h.py:19] end load_into_gpu_async cost 0.0021812915802001953 seconds
DEBUG 01-15 16:09:15.683744.683744 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:15.683487.683487 cuda_h.py:19] end restore_tensors2 cost 0.0001556873321533203 seconds
DEBUG 01-15 16:09:15.683748.683748 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0036695003509521484 seconds
INFO 01-15 16:09:15.684230.684230 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 41dbddec-2b58-4e28-9b48-e5aac79105ad
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
INFO 01-15 16:09:15.691305.691305 client.py:127] Model loaded
DEBUG 01-15 16:09:15.691561.691561 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.692705.692705 cuda_h.py:19] end restore2model cost 0.001245260238647461 seconds
DEBUG 01-15 16:09:15.692180.692180 cuda_h.py:19] end sllm_worker_task cost 0.012540340423583984 seconds
DEBUG 01-15 16:09:15.692989.692989 cuda_h.py:19] end self_attn cost 0.01035761833190918 seconds
DEBUG 01-15 16:09:15.693326.693326 cuda_h.py:19] end iln_self_attn_paln cost 0.01382899284362793 seconds
DEBUG 01-15 16:09:15.693421.693421 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:09:15.693873.693873 cuda_h.py:10] start gate
DEBUG 01-15 16:09:15.694735.694735 cuda_h.py:19] end gate cost 0.0009682178497314453 seconds
DEBUG 01-15 16:09:15.694745.694745 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:15.695793.695793 lmp.py:1912] 
DEBUG 01-15 16:09:15.695793.695793 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:15.695576.695576 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:15.695531.695531 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:15.695432.695432 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:15.695427.695427 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:15.695229.695229 lmp.py:1917] 
DEBUG 01-15 16:09:15.695229.695229 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:15.695747.695747 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:15.695563.695563 lmp.py:1935]   Expert 58 |     50 | CPU
DEBUG 01-15 16:09:15.695988.695988 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:09:15.695552.695552 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:09:15.695877.695877 lmp.py:1935]   Expert 17 |     84 | CPU
DEBUG 01-15 16:09:15.695964.695964 lmp.py:1935]   Expert 24 |     86 | CPU
DEBUG 01-15 16:09:15.695574.695574 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:09:15.695423.695423 lmp.py:1935]   Expert 28 |    105 | CPU
DEBUG 01-15 16:09:15.696795.696795 lmp.py:1935]   Expert 34 |    116 | CPU
DEBUG 01-15 16:09:15.696405.696405 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:09:15.696777.696777 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:09:15.696149.696149 lmp.py:1935]   Expert  9 |    128 | CPU
DEBUG 01-15 16:09:15.696997.696997 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:09:15.696038.696038 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:09:15.696794.696794 lmp.py:1935]   Expert 23 |    136 | CPU
DEBUG 01-15 16:09:15.696312.696312 lmp.py:1935]   Expert 26 |    137 | CPU
DEBUG 01-15 16:09:15.696160.696160 lmp.py:1935]   Expert 30 |    144 | CPU
DEBUG 01-15 16:09:15.696770.696770 lmp.py:1935]   Expert 45 |    145 | CPU
DEBUG 01-15 16:09:15.696904.696904 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:09:15.696799.696799 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:09:15.696694.696694 lmp.py:1935]   Expert  1 |    152 | CPU
DEBUG 01-15 16:09:15.696516.696516 lmp.py:1935]   Expert 36 |    156 | CPU
DEBUG 01-15 16:09:15.696796.696796 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:09:15.696075.696075 lmp.py:1935]   Expert 29 |    161 | CPU
DEBUG 01-15 16:09:15.696592.696592 lmp.py:1935]   Expert 25 |    164 | CPU
DEBUG 01-15 16:09:15.696679.696679 lmp.py:1935]   Expert 54 |    166 | CPU
DEBUG 01-15 16:09:15.696435.696435 lmp.py:1935]   Expert  6 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.696384.696384 lmp.py:1935]   Expert 49 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.696901.696901 lmp.py:1935]   Expert 48 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.696942.696942 lmp.py:1935]   Expert 12 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.696652.696652 lmp.py:1935]   Expert 35 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.696123.696123 lmp.py:1935]   Expert 37 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.696309.696309 lmp.py:1935]   Expert 60 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.696588.696588 lmp.py:1935]   Expert 13 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.696391.696391 lmp.py:1935]   Expert 53 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.696955.696955 lmp.py:1935]   Expert 33 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.696757.696757 lmp.py:1935]   Expert 10 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.696751.696751 lmp.py:1935]   Expert 16 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.696223.696223 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.696456.696456 lmp.py:1935]   Expert 40 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.696496.696496 lmp.py:1935]   Expert 43 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697537.697537 lmp.py:1935]   Expert 38 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697624.697624 lmp.py:1935]   Expert  5 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697188.697188 lmp.py:1935]   Expert 44 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697752.697752 lmp.py:1935]   Expert 52 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697746.697746 lmp.py:1935]   Expert 41 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697694.697694 lmp.py:1935]   Expert 50 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697735.697735 lmp.py:1935]   Expert 19 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697299.697299 lmp.py:1935]   Expert  4 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697863.697863 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697619.697619 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697852.697852 lmp.py:1935]   Expert 31 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697323.697323 lmp.py:1935]   Expert 56 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697364.697364 lmp.py:1935]   Expert 20 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697451.697451 lmp.py:1935]   Expert 39 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697015.697015 lmp.py:1935]   Expert 22 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697963.697963 lmp.py:1935]   Expert  2 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697196.697196 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697667.697667 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697946.697946 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697272.697272 lmp.py:1935]   Expert 18 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697835.697835 lmp.py:1935]   Expert 14 |    318 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697923.697923 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697963.697963 lmp.py:1935]   Expert 11 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.697196.697196 lmp.py:1935]   Expert 61 |    460 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.697760.697760 lmp.py:1937] 
DEBUG 01-15 16:09:15.697760.697760 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:15.697562.697562 lmp.py:1938]   CPU:   3105 tokens
DEBUG 01-15 16:09:15.697365.697365 lmp.py:1942]   cuda:1:   4674 tokens (20 experts)
DEBUG 01-15 16:09:15.697167.697167 lmp.py:1942]   cuda:2:   4509 tokens (19 experts)
DEBUG 01-15 16:09:15.697300.697300 lmp.py:1943]   Total GPU:   9183 tokens
DEBUG 01-15 16:09:15.697957.697957 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:15.697957.697957 lmp.py:1944] 
DEBUG 01-15 16:09:15.697865.697865 cuda_h.py:19] end experts_map_get cost 0.0031681060791015625 seconds
DEBUG 01-15 16:09:15.698193.698193 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:15.698639.698639 lmp.py:1953] 
DEBUG 01-15 16:09:15.698639.698639 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:15.698078.698078 cuda_h.py:19] end cpu_experts_submit cost 7.62939453125e-05 seconds
DEBUG 01-15 16:09:15.698841.698841 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:15.698010.698010 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:15.699207.699207 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.4.gate_proj.weight': 0, 'model.layers.2.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.2.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.2.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.2.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.2.mlp.experts.2.gate_proj.weight': 0, 'model.layers.2.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.11.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.11.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.11.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.18.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.18.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.18.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.33.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.33.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.33.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.38.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.38.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.38.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.39.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.39.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.39.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.43.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.43.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.43.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.46.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.46.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.46.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.47.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.47.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.47.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.60.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(4037017600, 5767168, 0, 0), (4042784768, 5767168, 5767168, 0), (4031250432, 5767168, 11534336, 0), (4054319104, 5767168, 17301504, 0), (4060086272, 5767168, 23068672, 0), (4048551936, 5767168, 28835840, 0), (4071620608, 5767168, 34603008, 0), (4077387776, 5767168, 40370176, 0), (4065853440, 5767168, 46137344, 0), (4140826624, 5767168, 51904512, 0), (4146593792, 5767168, 57671680, 0), (4135059456, 5767168, 63438848, 0), (4192731136, 5767168, 69206016, 0), (4198498304, 5767168, 74973184, 0), (4186963968, 5767168, 80740352, 0), (4210032640, 5767168, 86507520, 0), (4215799808, 5767168, 92274688, 0), (4204265472, 5767168, 98041856, 0), (4244635648, 5767168, 103809024, 0), (4250402816, 5767168, 109576192, 0), (4238868480, 5767168, 115343360, 0), (4313841664, 5767168, 121110528, 0), (4319608832, 5767168, 126877696, 0), (4308074496, 5767168, 132644864, 0), (4348444672, 5767168, 138412032, 0), (4354211840, 5767168, 144179200, 0), (4342677504, 5767168, 149946368, 0), (4504158208, 5767168, 155713536, 0), (4509925376, 5767168, 161480704, 0), (4498391040, 5767168, 167247872, 0), (4573364224, 5767168, 173015040, 0), (4579131392, 5767168, 178782208, 0), (4567597056, 5767168, 184549376, 0), (4607967232, 5767168, 190316544, 0), (4613734400, 5767168, 196083712, 0), (4602200064, 5767168, 201850880, 0), (4659871744, 5767168, 207618048, 0), (4665638912, 5767168, 213385216, 0), (4654104576, 5767168, 219152384, 0), (4677173248, 5767168, 224919552, 0), (4682940416, 5767168, 230686720, 0), (4671406080, 5767168, 236453888, 0), (4694474752, 5767168, 242221056, 0), (4700241920, 5767168, 247988224, 0), (4688707584, 5767168, 253755392, 0), (4729077760, 5767168, 259522560, 0), (4734844928, 5767168, 265289728, 0), (4723310592, 5767168, 271056896, 0), (4798283776, 5767168, 276824064, 0), (4804050944, 5767168, 282591232, 0), (4792516608, 5767168, 288358400, 0), (4988600320, 5767168, 294125568, 0), (4994367488, 5767168, 299892736, 0), (4982833152, 5767168, 305659904, 0), (5023203328, 5767168, 311427072, 0), (5028970496, 5767168, 317194240, 0), (5017436160, 5767168, 322961408, 0), (5057806336, 5767168, 328728576, 0), (5063573504, 5767168, 334495744, 0), (5052039168, 5767168, 340262912, 0)], 2: [(4002414592, 5767168, 0, 0), (4008181760, 5767168, 5767168, 0), (3996647424, 5767168, 11534336, 0), (4158128128, 5767168, 17301504, 0), (4163895296, 5767168, 23068672, 0), (4152360960, 5767168, 28835840, 0), (4175429632, 5767168, 34603008, 0), (4181196800, 5767168, 40370176, 0), (4169662464, 5767168, 46137344, 0), (4279238656, 5767168, 51904512, 0), (4285005824, 5767168, 57671680, 0), (4273471488, 5767168, 63438848, 0), (4296540160, 5767168, 69206016, 0), (4302307328, 5767168, 74973184, 0), (4290772992, 5767168, 80740352, 0), (4331143168, 5767168, 86507520, 0), (4336910336, 5767168, 92274688, 0), (4325376000, 5767168, 98041856, 0), (4538761216, 5767168, 103809024, 0), (4544528384, 5767168, 109576192, 0), (4532994048, 5767168, 115343360, 0), (4625268736, 5767168, 121110528, 0), (4631035904, 5767168, 126877696, 0), (4619501568, 5767168, 132644864, 0), (4642570240, 5767168, 138412032, 0), (4648337408, 5767168, 144179200, 0), (4636803072, 5767168, 149946368, 0), (4711776256, 5767168, 155713536, 0), (4717543424, 5767168, 161480704, 0), (4706009088, 5767168, 167247872, 0), (4763680768, 5767168, 173015040, 0), (4769447936, 5767168, 178782208, 0), (4757913600, 5767168, 184549376, 0), (4780982272, 5767168, 190316544, 0), (4786749440, 5767168, 196083712, 0), (4775215104, 5767168, 201850880, 0), (4815585280, 5767168, 207618048, 0), (4821352448, 5767168, 213385216, 0), (4809818112, 5767168, 219152384, 0), (4832886784, 5767168, 224919552, 0), (4838653952, 5767168, 230686720, 0), (4827119616, 5767168, 236453888, 0), (4867489792, 5767168, 242221056, 0), (4873256960, 5767168, 247988224, 0), (4861722624, 5767168, 253755392, 0), (4884791296, 5767168, 259522560, 0), (4890558464, 5767168, 265289728, 0), (4879024128, 5767168, 271056896, 0), (4919394304, 5767168, 276824064, 0), (4925161472, 5767168, 282591232, 0), (4913627136, 5767168, 288358400, 0), (4936695808, 5767168, 294125568, 0), (4942462976, 5767168, 299892736, 0), (4930928640, 5767168, 305659904, 0), (5005901824, 5767168, 311427072, 0), (5011668992, 5767168, 317194240, 0), (5000134656, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a9f372a010>, 2: <capsule object NULL at 0x74a9f372a3d0>}
DEBUG 01-15 16:09:15.699232.699232 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.699686.699686 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.699085.699085 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b00daf5c-d9c1-4122-b465-c49a943f1eac
DEBUG 01-15 16:09:15.699711.699711 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.701635.701635 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:15.701397.701397 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:15.702619.702619 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b00daf5c-d9c1-4122-b465-c49a943f1eac
DEBUG 01-15 16:09:15.702253.702253 cuda_h.py:19] end move_flatidxs cost 0.0009188652038574219 seconds
DEBUG 01-15 16:09:15.702620.702620 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:15.702206.702206 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004628658294677734 seconds
DEBUG 01-15 16:09:15.703812.703812 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.706966.706966 cuda_h.py:19] end restore2model cost 0.0037581920623779297 seconds
DEBUG 01-15 16:09:15.706273.706273 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008711576461791992 seconds
DEBUG 01-15 16:09:15.706545.706545 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:15.707034.707034 cuda_h.py:19] end gpu_sexperts cost 0.00032782554626464844 seconds
DEBUG 01-15 16:09:15.707771.707771 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:15.707309.707309 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:09:15.707720.707720 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:15.707331.707331 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.708515.708515 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013318061828613281 seconds
DEBUG 01-15 16:09:15.708770.708770 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.709149.709149 cuda_h.py:19] end gpu_group_list cost 0.00021409988403320312 seconds
DEBUG 01-15 16:09:15.710220.710220 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.711050.711050 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013918876647949219 seconds
DEBUG 01-15 16:09:15.711305.711305 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.712823.712823 cuda_h.py:19] end gpu_group_list cost 0.000209808349609375 seconds
DEBUG 01-15 16:09:15.711005.711005 cuda_h.py:19] end group_tensors cost 0.008841991424560547 seconds
DEBUG 01-15 16:09:15.712504.712504 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:15.713902.713902 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:15.713892.713892 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b00daf5c-d9c1-4122-b465-c49a943f1eac
DEBUG 01-15 16:09:15.716400.716400 cuda_h.py:19] end group pad cost 0.0035555362701416016 seconds
DEBUG 01-15 16:09:15.716097.716097 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:15.740744.740744 client.py:127] Model loaded
DEBUG 01-15 16:09:15.740068.740068 cuda_h.py:19] end wait_experts_multi_device cost 0.027571439743041992 seconds
DEBUG 01-15 16:09:15.740076.740076 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:15.748093.748093 cuda_h.py:19] end group_einsum cost 0.03241610527038574 seconds
DEBUG 01-15 16:09:15.748449.748449 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:15.753920.753920 cuda_h.py:19] end get_outputs_cpu1 cost 0.004489421844482422 seconds
DEBUG 01-15 16:09:15.754904.754904 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05322146415710449 seconds
DEBUG 01-15 16:09:15.755849.755849 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.014134407043457031 seconds
DEBUG 01-15 16:09:15.755283.755283 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:15.756298.756298 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.756324.756324 cuda_h.py:19] end index_scatter cost 8.416175842285156e-05 seconds
DEBUG 01-15 16:09:15.756592.756592 cuda_h.py:19] end cpuoutputsdeal cost 0.001489400863647461 seconds
DEBUG 01-15 16:09:15.756363.756363 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:15.756934.756934 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.757817.757817 cuda_h.py:19] end gpu_group_tensor cost 0.0006101131439208984 seconds
DEBUG 01-15 16:09:15.757805.757805 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.757142.757142 cuda_h.py:19] end gpu_group_tensor cost 0.00014400482177734375 seconds
DEBUG 01-15 16:09:15.757901.757901 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.758953.758953 cuda_h.py:19] end gpu_group_einsum cost 0.0005168914794921875 seconds
DEBUG 01-15 16:09:15.758924.758924 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.759891.759891 cuda_h.py:19] end gpu_group_einsum cost 0.00034928321838378906 seconds
DEBUG 01-15 16:09:15.759318.759318 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.759500.759500 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.759130.759130 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001823902130126953 seconds
DEBUG 01-15 16:09:15.759409.759409 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.759134.759134 cuda_h.py:19] end concat_expert_out cost 4.673004150390625e-05 seconds
DEBUG 01-15 16:09:15.759116.759116 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.759761.759761 cuda_h.py:19] end index_scatter cost 5.507469177246094e-05 seconds
DEBUG 01-15 16:09:15.759855.759855 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007729530334472656 seconds
DEBUG 01-15 16:09:15.760600.760600 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.760450.760450 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.760808.760808 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015926361083984375 seconds
DEBUG 01-15 16:09:15.760756.760756 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.760183.760183 cuda_h.py:19] end concat_expert_out cost 6.914138793945312e-05 seconds
DEBUG 01-15 16:09:15.760510.760510 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.760248.760248 cuda_h.py:19] end index_scatter cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:09:15.760918.760918 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005474090576171875 seconds
DEBUG 01-15 16:09:15.760682.760682 cuda_h.py:19] end gpu_experts_multi_device cost 0.05321311950683594 seconds
DEBUG 01-15 16:09:15.760260.760260 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.06714892387390137 seconds
DEBUG 01-15 16:09:15.761334.761334 cuda_h.py:19] end prefill_layer cost 0.08186459541320801 seconds
DEBUG 01-15 16:09:15.761283.761283 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:09:15.761463.761463 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:15.761119.761119 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:09:15.761299.761299 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:09:15.761624.761624 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:09:15.761766.761766 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 4.172325134277344e-05 seconds
DEBUG 01-15 16:09:15.761614.761614 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:09:15.761218.761218 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:15.761546.761546 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:15.761417.761417 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:15.761828.761828 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:15.762542.762542 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:15.762520.762520 cuda_h.py:19] end allocate_cuda_memory cost 0.0003769397735595703 seconds
DEBUG 01-15 16:09:15.762731.762731 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:15.762318.762318 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.763058.763058 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.763546.763546 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 820bf8bb-5e7c-4aa4-88a2-e6835ea2110e
DEBUG 01-15 16:09:15.763675.763675 cuda_h.py:10] start self_attn
DEBUG 01-15 16:09:15.763298.763298 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:15.765702.765702 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 820bf8bb-5e7c-4aa4-88a2-e6835ea2110e
DEBUG 01-15 16:09:15.765297.765297 cuda_h.py:19] end load_into_gpu_async cost 0.002332925796508789 seconds
DEBUG 01-15 16:09:15.765453.765453 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:15.765614.765614 cuda_h.py:19] end restore_tensors2 cost 0.00015735626220703125 seconds
DEBUG 01-15 16:09:15.765791.765791 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003706216812133789 seconds
INFO 01-15 16:09:15.765545.765545 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 820bf8bb-5e7c-4aa4-88a2-e6835ea2110e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:15.767523.767523 cuda_h.py:19] end self_attn cost 0.003854990005493164 seconds
DEBUG 01-15 16:09:15.767202.767202 cuda_h.py:19] end iln_self_attn_paln cost 0.0062046051025390625 seconds
DEBUG 01-15 16:09:15.767051.767051 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:09:15.767523.767523 cuda_h.py:10] start gate
DEBUG 01-15 16:09:15.768221.768221 cuda_h.py:19] end gate cost 0.0006582736968994141 seconds
DEBUG 01-15 16:09:15.768243.768243 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:15.768962.768962 lmp.py:1912] 
DEBUG 01-15 16:09:15.768962.768962 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:15.768387.768387 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:15.768083.768083 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:15.768110.768110 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:15.768707.768707 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:15.768396.768396 lmp.py:1917] 
DEBUG 01-15 16:09:15.768396.768396 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:15.768039.768039 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:15.769928.769928 lmp.py:1935]   Expert  1 |     51 | CPU
DEBUG 01-15 16:09:15.769570.769570 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:09:15.769498.769498 lmp.py:1935]   Expert  7 |     75 | CPU
DEBUG 01-15 16:09:15.769949.769949 lmp.py:1935]   Expert 48 |     81 | CPU
DEBUG 01-15 16:09:15.769638.769638 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:09:15.769851.769851 lmp.py:1935]   Expert 30 |    109 | CPU
DEBUG 01-15 16:09:15.769971.769971 lmp.py:1935]   Expert 61 |    116 | CPU
DEBUG 01-15 16:09:15.769422.769422 lmp.py:1935]   Expert 32 |    118 | CPU
DEBUG 01-15 16:09:15.769872.769872 lmp.py:1935]   Expert 18 |    119 | CPU
DEBUG 01-15 16:09:15.769085.769085 lmp.py:1935]   Expert 45 |    119 | CPU
DEBUG 01-15 16:09:15.769059.769059 lmp.py:1935]   Expert 34 |    134 | CPU
DEBUG 01-15 16:09:15.769510.769510 lmp.py:1935]   Expert 39 |    136 | CPU
DEBUG 01-15 16:09:15.769722.769722 lmp.py:1935]   Expert 26 |    139 | CPU
DEBUG 01-15 16:09:15.769458.769458 lmp.py:1935]   Expert 36 |    139 | CPU
DEBUG 01-15 16:09:15.769432.769432 lmp.py:1935]   Expert 11 |    140 | CPU
DEBUG 01-15 16:09:15.769406.769406 lmp.py:1935]   Expert 59 |    142 | CPU
DEBUG 01-15 16:09:15.769380.769380 lmp.py:1935]   Expert  5 |    143 | CPU
DEBUG 01-15 16:09:15.769546.769546 lmp.py:1935]   Expert  6 |    144 | CPU
DEBUG 01-15 16:09:15.769759.769759 lmp.py:1935]   Expert 51 |    145 | CPU
DEBUG 01-15 16:09:15.769494.769494 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:09:15.769468.769468 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:09:15.769966.769966 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:09:15.769701.769701 lmp.py:1935]   Expert  9 |    158 | CPU
DEBUG 01-15 16:09:15.769198.769198 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:09:15.769172.769172 lmp.py:1935]   Expert 52 |    168 | CPU
DEBUG 01-15 16:09:15.769577.769577 lmp.py:1935]   Expert 56 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769127.769127 lmp.py:1935]   Expert 40 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769201.769201 lmp.py:1935]   Expert 16 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769559.769559 lmp.py:1935]   Expert 35 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769586.769586 lmp.py:1935]   Expert  4 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769706.769706 lmp.py:1935]   Expert 37 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769303.769303 lmp.py:1935]   Expert 13 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769422.769422 lmp.py:1935]   Expert 42 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769542.769542 lmp.py:1935]   Expert 38 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769662.769662 lmp.py:1935]   Expert 17 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769259.769259 lmp.py:1935]   Expert 62 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769107.769107 lmp.py:1935]   Expert 21 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769466.769466 lmp.py:1935]   Expert  3 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769076.769076 lmp.py:1935]   Expert 44 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769480.769480 lmp.py:1935]   Expert 60 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769647.769647 lmp.py:1935]   Expert 28 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769574.769574 lmp.py:1935]   Expert 58 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769740.769740 lmp.py:1935]   Expert 10 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769906.769906 lmp.py:1935]   Expert 47 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769834.769834 lmp.py:1935]   Expert 53 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769762.769762 lmp.py:1935]   Expert 55 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769690.769690 lmp.py:1935]   Expert 20 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769617.769617 lmp.py:1935]   Expert 57 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769545.769545 lmp.py:1935]   Expert 33 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769956.769956 lmp.py:1935]   Expert 31 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769884.769884 lmp.py:1935]   Expert 46 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769289.769289 lmp.py:1935]   Expert  8 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769216.769216 lmp.py:1935]   Expert 19 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769859.769859 lmp.py:1935]   Expert 24 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769549.769549 lmp.py:1935]   Expert 14 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.769715.769715 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.769881.769881 lmp.py:1935]   Expert 12 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.770047.770047 lmp.py:1935]   Expert 29 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.770736.770736 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.770141.770141 lmp.py:1935]   Expert  0 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.770592.770592 lmp.py:1935]   Expert 43 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.770996.770996 lmp.py:1935]   Expert 54 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.770924.770924 lmp.py:1935]   Expert 41 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.770613.770613 lmp.py:1935]   Expert 25 |    413 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.770826.770826 lmp.py:1937] 
DEBUG 01-15 16:09:15.770826.770826 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:15.770277.770277 lmp.py:1938]   CPU:   3166 tokens
DEBUG 01-15 16:09:15.770681.770681 lmp.py:1942]   cuda:1:   4642 tokens (20 experts)
DEBUG 01-15 16:09:15.770847.770847 lmp.py:1942]   cuda:2:   4480 tokens (19 experts)
DEBUG 01-15 16:09:15.770060.770060 lmp.py:1943]   Total GPU:   9122 tokens
DEBUG 01-15 16:09:15.770749.770749 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:15.770749.770749 lmp.py:1944] 
DEBUG 01-15 16:09:15.770968.770968 cuda_h.py:19] end experts_map_get cost 0.0016667842864990234 seconds
DEBUG 01-15 16:09:15.770480.770480 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:15.770899.770899 lmp.py:1953] 
DEBUG 01-15 16:09:15.770899.770899 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:15.770967.770967 cuda_h.py:19] end cpu_experts_submit cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:09:15.770252.770252 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:15.770320.770320 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:15.772179.772179 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.14.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.14.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.14.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.16.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.16.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.16.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.25.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.25.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.25.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.3.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.3.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.3.mlp.experts.62.up_proj.weight': 340262912}, 2: {'model.layers.3.mlp.experts.3.gate_proj.weight': 0, 'model.layers.3.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.28.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.28.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.28.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5317328896, 5767168, 17301504, 0), (5323096064, 5767168, 23068672, 0), (5311561728, 5767168, 28835840, 0), (5351931904, 5767168, 34603008, 0), (5357699072, 5767168, 40370176, 0), (5346164736, 5767168, 46137344, 0), (5403836416, 5767168, 51904512, 0), (5409603584, 5767168, 57671680, 0), (5398069248, 5767168, 63438848, 0), (5438439424, 5767168, 69206016, 0), (5444206592, 5767168, 74973184, 0), (5432672256, 5767168, 80740352, 0), (5507645440, 5767168, 86507520, 0), (5513412608, 5767168, 92274688, 0), (5501878272, 5767168, 98041856, 0), (5576851456, 5767168, 103809024, 0), (5582618624, 5767168, 109576192, 0), (5571084288, 5767168, 115343360, 0), (5611454464, 5767168, 121110528, 0), (5617221632, 5767168, 126877696, 0), (5605687296, 5767168, 132644864, 0), (5646057472, 5767168, 138412032, 0), (5651824640, 5767168, 144179200, 0), (5640290304, 5767168, 149946368, 0), (5680660480, 5767168, 155713536, 0), (5686427648, 5767168, 161480704, 0), (5674893312, 5767168, 167247872, 0), (5715263488, 5767168, 173015040, 0), (5721030656, 5767168, 178782208, 0), (5709496320, 5767168, 184549376, 0), (5732564992, 5767168, 190316544, 0), (5738332160, 5767168, 196083712, 0), (5726797824, 5767168, 201850880, 0), (5819072512, 5767168, 207618048, 0), (5824839680, 5767168, 213385216, 0), (5813305344, 5767168, 219152384, 0), (5888278528, 5767168, 224919552, 0), (5894045696, 5767168, 230686720, 0), (5882511360, 5767168, 236453888, 0), (6026690560, 5767168, 242221056, 0), (6032457728, 5767168, 247988224, 0), (6020923392, 5767168, 253755392, 0), (6043992064, 5767168, 259522560, 0), (6049759232, 5767168, 265289728, 0), (6038224896, 5767168, 271056896, 0), (6061293568, 5767168, 276824064, 0), (6067060736, 5767168, 282591232, 0), (6055526400, 5767168, 288358400, 0), (6078595072, 5767168, 294125568, 0), (6084362240, 5767168, 299892736, 0), (6072827904, 5767168, 305659904, 0), (6113198080, 5767168, 311427072, 0), (6118965248, 5767168, 317194240, 0), (6107430912, 5767168, 322961408, 0), (6147801088, 5767168, 328728576, 0), (6153568256, 5767168, 334495744, 0), (6142033920, 5767168, 340262912, 0)], 2: [(5127012352, 5767168, 0, 0), (5132779520, 5767168, 5767168, 0), (5121245184, 5767168, 11534336, 0), (5144313856, 5767168, 17301504, 0), (5150081024, 5767168, 23068672, 0), (5138546688, 5767168, 28835840, 0), (5213519872, 5767168, 34603008, 0), (5219287040, 5767168, 40370176, 0), (5207752704, 5767168, 46137344, 0), (5248122880, 5767168, 51904512, 0), (5253890048, 5767168, 57671680, 0), (5242355712, 5767168, 63438848, 0), (5282725888, 5767168, 69206016, 0), (5288493056, 5767168, 74973184, 0), (5276958720, 5767168, 80740352, 0), (5300027392, 5767168, 86507520, 0), (5305794560, 5767168, 92274688, 0), (5294260224, 5767168, 98041856, 0), (5369233408, 5767168, 103809024, 0), (5375000576, 5767168, 109576192, 0), (5363466240, 5767168, 115343360, 0), (5421137920, 5767168, 121110528, 0), (5426905088, 5767168, 126877696, 0), (5415370752, 5767168, 132644864, 0), (5455740928, 5767168, 138412032, 0), (5461508096, 5767168, 144179200, 0), (5449973760, 5767168, 149946368, 0), (5490343936, 5767168, 155713536, 0), (5496111104, 5767168, 161480704, 0), (5484576768, 5767168, 167247872, 0), (5559549952, 5767168, 173015040, 0), (5565317120, 5767168, 178782208, 0), (5553782784, 5767168, 184549376, 0), (5767168000, 5767168, 190316544, 0), (5772935168, 5767168, 196083712, 0), (5761400832, 5767168, 201850880, 0), (5784469504, 5767168, 207618048, 0), (5790236672, 5767168, 213385216, 0), (5778702336, 5767168, 219152384, 0), (5801771008, 5767168, 224919552, 0), (5807538176, 5767168, 230686720, 0), (5796003840, 5767168, 236453888, 0), (5836374016, 5767168, 242221056, 0), (5842141184, 5767168, 247988224, 0), (5830606848, 5767168, 253755392, 0), (5870977024, 5767168, 259522560, 0), (5876744192, 5767168, 265289728, 0), (5865209856, 5767168, 271056896, 0), (5992087552, 5767168, 276824064, 0), (5997854720, 5767168, 282591232, 0), (5986320384, 5767168, 288358400, 0), (6009389056, 5767168, 294125568, 0), (6015156224, 5767168, 299892736, 0), (6003621888, 5767168, 305659904, 0), (6165102592, 5767168, 311427072, 0), (6170869760, 5767168, 317194240, 0), (6159335424, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a9f372a250>, 2: <capsule object NULL at 0x74a9f3729c50>}
DEBUG 01-15 16:09:15.772958.772958 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.772894.772894 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.772372.772372 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 177e96b6-087a-4928-9ec8-d7d29f91fe2e
DEBUG 01-15 16:09:15.772823.772823 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:15.773798.773798 client.py:127] Model loaded
DEBUG 01-15 16:09:15.773532.773532 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.773664.773664 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:15.773448.773448 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 177e96b6-087a-4928-9ec8-d7d29f91fe2e
DEBUG 01-15 16:09:15.773072.773072 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:15.774877.774877 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0037491321563720703 seconds
DEBUG 01-15 16:09:15.774305.774305 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.774832.774832 cuda_h.py:19] end move_flatidxs cost 0.0008792877197265625 seconds
DEBUG 01-15 16:09:15.774788.774788 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:15.775793.775793 cuda_h.py:19] end restore2model cost 0.001276254653930664 seconds
DEBUG 01-15 16:09:15.775929.775929 cuda_h.py:19] end sllm_worker_task cost 0.014095544815063477 seconds
DEBUG 01-15 16:09:15.778195.778195 cuda_h.py:19] end restore2model cost 0.00439763069152832 seconds
DEBUG 01-15 16:09:15.778880.778880 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008446931838989258 seconds
DEBUG 01-15 16:09:15.778630.778630 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:15.779838.779838 cuda_h.py:19] end gpu_sexperts cost 0.0002636909484863281 seconds
DEBUG 01-15 16:09:15.779853.779853 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:15.779960.779960 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:09:15.779226.779226 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:15.779214.779214 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.780088.780088 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009660720825195312 seconds
DEBUG 01-15 16:09:15.780800.780800 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.780497.780497 cuda_h.py:19] end gpu_group_list cost 0.0002014636993408203 seconds
DEBUG 01-15 16:09:15.781186.781186 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.782493.782493 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009763240814208984 seconds
DEBUG 01-15 16:09:15.782097.782097 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.782111.782111 cuda_h.py:19] end gpu_group_list cost 0.00019216537475585938 seconds
DEBUG 01-15 16:09:15.782301.782301 cuda_h.py:19] end group_tensors cost 0.007341146469116211 seconds
DEBUG 01-15 16:09:15.783186.783186 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:15.783863.783863 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:15.783760.783760 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 177e96b6-087a-4928-9ec8-d7d29f91fe2e
DEBUG 01-15 16:09:15.786628.786628 cuda_h.py:19] end group pad cost 0.003835439682006836 seconds
DEBUG 01-15 16:09:15.787849.787849 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:15.815965.815965 cuda_h.py:19] end group_einsum cost 0.027914762496948242 seconds
DEBUG 01-15 16:09:15.815322.815322 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:15.815314.815314 client.py:127] Model loaded
DEBUG 01-15 16:09:15.815643.815643 cuda_h.py:19] end wait_experts_multi_device cost 0.03150582313537598 seconds
DEBUG 01-15 16:09:15.815167.815167 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:15.818772.818772 cuda_h.py:19] end get_outputs_cpu1 cost 0.003101348876953125 seconds
DEBUG 01-15 16:09:15.819510.819510 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.045738935470581055 seconds
DEBUG 01-15 16:09:15.820411.820411 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.004635334014892578 seconds
DEBUG 01-15 16:09:15.820913.820913 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:15.822577.822577 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.822220.822220 cuda_h.py:19] end index_scatter cost 8.58306884765625e-05 seconds
DEBUG 01-15 16:09:15.822108.822108 cuda_h.py:19] end cpuoutputsdeal cost 0.002601146697998047 seconds
DEBUG 01-15 16:09:15.822032.822032 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:15.822794.822794 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.823191.823191 cuda_h.py:19] end gpu_group_tensor cost 0.0001544952392578125 seconds
DEBUG 01-15 16:09:15.823623.823623 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.823132.823132 cuda_h.py:19] end gpu_group_tensor cost 0.00013566017150878906 seconds
DEBUG 01-15 16:09:15.823321.823321 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.824047.824047 cuda_h.py:19] end gpu_group_einsum cost 0.0004889965057373047 seconds
DEBUG 01-15 16:09:15.824595.824595 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.824416.824416 cuda_h.py:19] end gpu_group_einsum cost 0.0005509853363037109 seconds
DEBUG 01-15 16:09:15.824944.824944 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.825730.825730 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.825925.825925 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023508071899414062 seconds
DEBUG 01-15 16:09:15.825463.825463 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.825187.825187 cuda_h.py:19] end concat_expert_out cost 4.7206878662109375e-05 seconds
DEBUG 01-15 16:09:15.825269.825269 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.825715.825715 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:15.825041.825041 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008563995361328125 seconds
DEBUG 01-15 16:09:15.825852.825852 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.825987.825987 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.826430.826430 cuda_h.py:19] end all_expert_outputs_slices cost 0.000156402587890625 seconds
DEBUG 01-15 16:09:15.826233.826233 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.826533.826533 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:15.826277.826277 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.826531.826531 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:15.826433.826433 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004982948303222656 seconds
DEBUG 01-15 16:09:15.826581.826581 cuda_h.py:19] end gpu_experts_multi_device cost 0.04718518257141113 seconds
DEBUG 01-15 16:09:15.826206.826206 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.05879330635070801 seconds
DEBUG 01-15 16:09:15.827970.827970 cuda_h.py:19] end prefill_layer cost 0.06575679779052734 seconds
DEBUG 01-15 16:09:15.827112.827112 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:09:15.827768.827768 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:15.827186.827186 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:09:15.827320.827320 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:09:15.827029.827029 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:09:15.827071.827071 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:09:15.827635.827635 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:09:15.827808.827808 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:15.827486.827486 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:15.827605.827605 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:15.827608.827608 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:15.827682.827682 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:15.828708.828708 cuda_h.py:19] end allocate_cuda_memory cost 0.000461578369140625 seconds
DEBUG 01-15 16:09:15.828263.828263 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:15.828194.828194 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.828655.828655 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.829863.829863 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ba112bf4-e6e8-44a9-81f0-f7f187e0921e
DEBUG 01-15 16:09:15.829778.829778 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.829270.829270 cuda_h.py:10] start self_attn
INFO 01-15 16:09:15.830924.830924 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ba112bf4-e6e8-44a9-81f0-f7f187e0921e
DEBUG 01-15 16:09:15.830287.830287 cuda_h.py:19] end load_into_gpu_async cost 0.001786947250366211 seconds
DEBUG 01-15 16:09:15.830409.830409 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:15.830643.830643 cuda_h.py:19] end restore_tensors2 cost 0.00014829635620117188 seconds
DEBUG 01-15 16:09:15.831792.831792 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003139972686767578 seconds
INFO 01-15 16:09:15.831644.831644 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ba112bf4-e6e8-44a9-81f0-f7f187e0921e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:15.833647.833647 cuda_h.py:19] end self_attn cost 0.003851652145385742 seconds
DEBUG 01-15 16:09:15.833565.833565 cuda_h.py:19] end iln_self_attn_paln cost 0.0063555240631103516 seconds
DEBUG 01-15 16:09:15.833057.833057 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:09:15.833289.833289 cuda_h.py:10] start gate
DEBUG 01-15 16:09:15.834133.834133 cuda_h.py:19] end gate cost 0.0006253719329833984 seconds
DEBUG 01-15 16:09:15.834200.834200 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:15.834066.834066 lmp.py:1912] 
DEBUG 01-15 16:09:15.834066.834066 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:15.834895.834895 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:15.834691.834691 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:15.834764.834764 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:15.834692.834692 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:15.834189.834189 lmp.py:1917] 
DEBUG 01-15 16:09:15.834189.834189 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:15.834401.834401 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:15.835813.835813 lmp.py:1935]   Expert 14 |     65 | CPU
DEBUG 01-15 16:09:15.835502.835502 lmp.py:1935]   Expert 57 |     72 | CPU
DEBUG 01-15 16:09:15.835999.835999 lmp.py:1935]   Expert 13 |     75 | CPU
DEBUG 01-15 16:09:15.835735.835735 lmp.py:1935]   Expert 26 |     82 | CPU
DEBUG 01-15 16:09:15.835994.835994 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:09:15.835729.835729 lmp.py:1935]   Expert 54 |     92 | CPU
DEBUG 01-15 16:09:15.835180.835180 lmp.py:1935]   Expert 11 |     94 | CPU
DEBUG 01-15 16:09:15.835346.835346 lmp.py:1935]   Expert 45 |     94 | CPU
DEBUG 01-15 16:09:15.835135.835135 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:09:15.835540.835540 lmp.py:1935]   Expert 30 |    107 | CPU
DEBUG 01-15 16:09:15.835275.835275 lmp.py:1935]   Expert 51 |    108 | CPU
DEBUG 01-15 16:09:15.835773.835773 lmp.py:1935]   Expert 36 |    111 | CPU
DEBUG 01-15 16:09:15.835031.835031 lmp.py:1935]   Expert 10 |    114 | CPU
DEBUG 01-15 16:09:15.835529.835529 lmp.py:1935]   Expert 32 |    116 | CPU
DEBUG 01-15 16:09:15.835026.835026 lmp.py:1935]   Expert 20 |    129 | CPU
DEBUG 01-15 16:09:15.835000.835000 lmp.py:1935]   Expert  8 |    134 | CPU
DEBUG 01-15 16:09:15.835259.835259 lmp.py:1935]   Expert  4 |    138 | CPU
DEBUG 01-15 16:09:15.835756.835756 lmp.py:1935]   Expert 63 |    139 | CPU
DEBUG 01-15 16:09:15.835538.835538 lmp.py:1935]   Expert 53 |    140 | CPU
DEBUG 01-15 16:09:15.835273.835273 lmp.py:1935]   Expert 34 |    143 | CPU
DEBUG 01-15 16:09:15.835108.835108 lmp.py:1935]   Expert 61 |    144 | CPU
DEBUG 01-15 16:09:15.835036.835036 lmp.py:1935]   Expert 47 |    147 | CPU
DEBUG 01-15 16:09:15.835110.835110 lmp.py:1935]   Expert 16 |    148 | CPU
DEBUG 01-15 16:09:15.835845.835845 lmp.py:1935]   Expert 60 |    158 | CPU
DEBUG 01-15 16:09:15.835104.835104 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:09:15.835270.835270 lmp.py:1935]   Expert 42 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835675.835675 lmp.py:1935]   Expert 17 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835602.835602 lmp.py:1935]   Expert 29 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835053.835053 lmp.py:1935]   Expert 44 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835981.835981 lmp.py:1935]   Expert 27 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835955.835955 lmp.py:1935]   Expert  7 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835406.835406 lmp.py:1935]   Expert 41 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835618.835618 lmp.py:1935]   Expert 48 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835069.835069 lmp.py:1935]   Expert 56 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835282.835282 lmp.py:1935]   Expert  9 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835733.835733 lmp.py:1935]   Expert  2 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835184.835184 lmp.py:1935]   Expert  3 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835634.835634 lmp.py:1935]   Expert 15 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835516.835516 lmp.py:1935]   Expert  0 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835066.835066 lmp.py:1935]   Expert 24 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835093.835093 lmp.py:1935]   Expert 18 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835783.835783 lmp.py:1935]   Expert 55 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835995.835995 lmp.py:1935]   Expert 40 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835446.835446 lmp.py:1935]   Expert 38 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835897.835897 lmp.py:1935]   Expert 23 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835109.835109 lmp.py:1935]   Expert 22 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835322.835322 lmp.py:1935]   Expert  6 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835534.835534 lmp.py:1935]   Expert 37 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835985.835985 lmp.py:1935]   Expert 46 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835436.835436 lmp.py:1935]   Expert 19 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835648.835648 lmp.py:1935]   Expert 39 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835099.835099 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835073.835073 lmp.py:1935]   Expert 50 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835763.835763 lmp.py:1935]   Expert 12 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835167.835167 lmp.py:1935]   Expert 62 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835572.835572 lmp.py:1935]   Expert 21 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835261.835261 lmp.py:1935]   Expert 35 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835904.835904 lmp.py:1935]   Expert 49 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.835262.835262 lmp.py:1935]   Expert 33 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.835382.835382 lmp.py:1935]   Expert 52 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.836025.836025 lmp.py:1935]   Expert  1 |    347 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.836906.836906 lmp.py:1935]   Expert  5 |    381 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.836311.836311 lmp.py:1935]   Expert 43 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.836192.836192 lmp.py:1935]   Expert 59 |    583 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.836643.836643 lmp.py:1937] 
DEBUG 01-15 16:09:15.836643.836643 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:15.836286.836286 lmp.py:1938]   CPU:   2901 tokens
DEBUG 01-15 16:09:15.836168.836168 lmp.py:1942]   cuda:1:   4640 tokens (19 experts)
DEBUG 01-15 16:09:15.836977.836977 lmp.py:1942]   cuda:2:   4747 tokens (20 experts)
DEBUG 01-15 16:09:15.836527.836527 lmp.py:1943]   Total GPU:   9387 tokens
DEBUG 01-15 16:09:15.836932.836932 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:15.836932.836932 lmp.py:1944] 
DEBUG 01-15 16:09:15.836012.836012 cuda_h.py:19] end experts_map_get cost 0.001657724380493164 seconds
DEBUG 01-15 16:09:15.836908.836908 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:15.836995.836995 lmp.py:1953] 
DEBUG 01-15 16:09:15.836995.836995 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:15.836017.836017 cuda_h.py:19] end cpu_experts_submit cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:09:15.836951.836951 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:15.836755.836755 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:15.837602.837602 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.38.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.38.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.38.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.49.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.49.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.49.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.52.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.52.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.52.up_proj.weight': 322961408, 'model.layers.4.mlp.experts.55.gate_proj.weight': 328728576, 'model.layers.4.mlp.experts.55.down_proj.weight': 334495744, 'model.layers.4.mlp.experts.55.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6234308608, 5767168, 17301504, 0), (6240075776, 5767168, 23068672, 0), (6228541440, 5767168, 28835840, 0), (6286213120, 5767168, 34603008, 0), (6291980288, 5767168, 40370176, 0), (6280445952, 5767168, 46137344, 0), (6303514624, 5767168, 51904512, 0), (6309281792, 5767168, 57671680, 0), (6297747456, 5767168, 63438848, 0), (6338117632, 5767168, 69206016, 0), (6343884800, 5767168, 74973184, 0), (6332350464, 5767168, 80740352, 0), (6476529664, 5767168, 86507520, 0), (6482296832, 5767168, 92274688, 0), (6470762496, 5767168, 98041856, 0), (6493831168, 5767168, 103809024, 0), (6499598336, 5767168, 109576192, 0), (6488064000, 5767168, 115343360, 0), (6580338688, 5767168, 121110528, 0), (6586105856, 5767168, 126877696, 0), (6574571520, 5767168, 132644864, 0), (6597640192, 5767168, 138412032, 0), (6603407360, 5767168, 144179200, 0), (6591873024, 5767168, 149946368, 0), (6753353728, 5767168, 155713536, 0), (6759120896, 5767168, 161480704, 0), (6747586560, 5767168, 167247872, 0), (6787956736, 5767168, 173015040, 0), (6793723904, 5767168, 178782208, 0), (6782189568, 5767168, 184549376, 0), (6857162752, 5767168, 190316544, 0), (6862929920, 5767168, 196083712, 0), (6851395584, 5767168, 201850880, 0), (6874464256, 5767168, 207618048, 0), (6880231424, 5767168, 213385216, 0), (6868697088, 5767168, 219152384, 0), (6943670272, 5767168, 224919552, 0), (6949437440, 5767168, 230686720, 0), (6937903104, 5767168, 236453888, 0), (6978273280, 5767168, 242221056, 0), (6984040448, 5767168, 247988224, 0), (6972506112, 5767168, 253755392, 0), (7047479296, 5767168, 259522560, 0), (7053246464, 5767168, 265289728, 0), (7041712128, 5767168, 271056896, 0), (7151288320, 5767168, 276824064, 0), (7157055488, 5767168, 282591232, 0), (7145521152, 5767168, 288358400, 0), (7203192832, 5767168, 294125568, 0), (7208960000, 5767168, 299892736, 0), (7197425664, 5767168, 305659904, 0), (7255097344, 5767168, 311427072, 0), (7260864512, 5767168, 317194240, 0), (7249330176, 5767168, 322961408, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6217007104, 5767168, 17301504, 0), (6222774272, 5767168, 23068672, 0), (6211239936, 5767168, 28835840, 0), (6268911616, 5767168, 34603008, 0), (6274678784, 5767168, 40370176, 0), (6263144448, 5767168, 46137344, 0), (6390022144, 5767168, 51904512, 0), (6395789312, 5767168, 57671680, 0), (6384254976, 5767168, 63438848, 0), (6441926656, 5767168, 69206016, 0), (6447693824, 5767168, 74973184, 0), (6436159488, 5767168, 80740352, 0), (6511132672, 5767168, 86507520, 0), (6516899840, 5767168, 92274688, 0), (6505365504, 5767168, 98041856, 0), (6545735680, 5767168, 103809024, 0), (6551502848, 5767168, 109576192, 0), (6539968512, 5767168, 115343360, 0), (6563037184, 5767168, 121110528, 0), (6568804352, 5767168, 126877696, 0), (6557270016, 5767168, 132644864, 0), (6614941696, 5767168, 138412032, 0), (6620708864, 5767168, 144179200, 0), (6609174528, 5767168, 149946368, 0), (6649544704, 5767168, 155713536, 0), (6655311872, 5767168, 161480704, 0), (6643777536, 5767168, 167247872, 0), (6684147712, 5767168, 173015040, 0), (6689914880, 5767168, 178782208, 0), (6678380544, 5767168, 184549376, 0), (6822559744, 5767168, 190316544, 0), (6828326912, 5767168, 196083712, 0), (6816792576, 5767168, 201850880, 0), (6839861248, 5767168, 207618048, 0), (6845628416, 5767168, 213385216, 0), (6834094080, 5767168, 219152384, 0), (6891765760, 5767168, 224919552, 0), (6897532928, 5767168, 230686720, 0), (6885998592, 5767168, 236453888, 0), (6909067264, 5767168, 242221056, 0), (6914834432, 5767168, 247988224, 0), (6903300096, 5767168, 253755392, 0), (6926368768, 5767168, 259522560, 0), (6932135936, 5767168, 265289728, 0), (6920601600, 5767168, 271056896, 0), (7012876288, 5767168, 276824064, 0), (7018643456, 5767168, 282591232, 0), (7007109120, 5767168, 288358400, 0), (7030177792, 5767168, 294125568, 0), (7035944960, 5767168, 299892736, 0), (7024410624, 5767168, 305659904, 0), (7082082304, 5767168, 311427072, 0), (7087849472, 5767168, 317194240, 0), (7076315136, 5767168, 322961408, 0), (7133986816, 5767168, 328728576, 0), (7139753984, 5767168, 334495744, 0), (7128219648, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814575020>, 2: <capsule object NULL at 0x74a6807a9f20>}
DEBUG 01-15 16:09:15.838590.838590 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.838201.838201 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.838434.838434 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4aeaadae-e96e-4198-bccd-710fb3a6dc24
DEBUG 01-15 16:09:15.838673.838673 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.838617.838617 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:15.838106.838106 client.py:127] Model loaded
DEBUG 01-15 16:09:15.838653.838653 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:15.838362.838362 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.839385.839385 cuda_h.py:19] end move_flatidxs cost 0.0008330345153808594 seconds
DEBUG 01-15 16:09:15.839638.839638 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:15.839200.839200 cuda_h.py:19] end restore2model cost 0.0010199546813964844 seconds
INFO 01-15 16:09:15.839615.839615 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4aeaadae-e96e-4198-bccd-710fb3a6dc24
DEBUG 01-15 16:09:15.840552.840552 cuda_h.py:19] end sllm_worker_task cost 0.012400150299072266 seconds
DEBUG 01-15 16:09:15.840400.840400 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004195690155029297 seconds
DEBUG 01-15 16:09:15.840392.840392 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.843022.843022 cuda_h.py:19] end restore2model cost 0.0029540061950683594 seconds
DEBUG 01-15 16:09:15.843700.843700 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007508516311645508 seconds
DEBUG 01-15 16:09:15.843781.843781 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:15.844459.844459 cuda_h.py:19] end gpu_sexperts cost 0.00025963783264160156 seconds
DEBUG 01-15 16:09:15.844666.844666 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:15.844866.844866 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4781951904296875e-05 seconds
DEBUG 01-15 16:09:15.844085.844085 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:15.844596.844596 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.844441.844441 cuda_h.py:19] end group_tensors cost 0.004557132720947266 seconds
DEBUG 01-15 16:09:15.844166.844166 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:15.845263.845263 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012936592102050781 seconds
DEBUG 01-15 16:09:15.845499.845499 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.846212.846212 cuda_h.py:19] end gpu_group_list cost 0.00030732154846191406 seconds
DEBUG 01-15 16:09:15.847513.847513 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.848895.848895 cuda_h.py:19] end group pad cost 0.003118276596069336 seconds
DEBUG 01-15 16:09:15.848254.848254 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:15.853778.853778 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0057981014251708984 seconds
DEBUG 01-15 16:09:15.854003.854003 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.856518.856518 cuda_h.py:19] end gpu_group_list cost 0.0009870529174804688 seconds
DEBUG 01-15 16:09:15.858145.858145 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:15.858270.858270 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4aeaadae-e96e-4198-bccd-710fb3a6dc24
DEBUG 01-15 16:09:15.876184.876184 cuda_h.py:19] end group_einsum cost 0.028679609298706055 seconds
DEBUG 01-15 16:09:15.877561.877561 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:15.880406.880406 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029833316802978516 seconds
DEBUG 01-15 16:09:15.881455.881455 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04258465766906738 seconds
INFO 01-15 16:09:15.881246.881246 client.py:127] Model loaded
DEBUG 01-15 16:09:15.881800.881800 cuda_h.py:19] end wait_experts_multi_device cost 0.023373842239379883 seconds
DEBUG 01-15 16:09:15.881563.881563 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:15.882820.882820 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005383491516113281 seconds
DEBUG 01-15 16:09:15.882756.882756 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:15.883073.883073 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.883926.883926 cuda_h.py:19] end index_scatter cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:09:15.883611.883611 cuda_h.py:19] end cpuoutputsdeal cost 0.0013668537139892578 seconds
DEBUG 01-15 16:09:15.883852.883852 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:15.883277.883277 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.884329.884329 cuda_h.py:19] end gpu_group_tensor cost 0.0001494884490966797 seconds
DEBUG 01-15 16:09:15.884993.884993 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.884380.884380 cuda_h.py:19] end gpu_group_tensor cost 0.0004589557647705078 seconds
DEBUG 01-15 16:09:15.884119.884119 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.885964.885964 cuda_h.py:19] end gpu_group_einsum cost 0.0006420612335205078 seconds
DEBUG 01-15 16:09:15.885670.885670 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.886806.886806 cuda_h.py:19] end gpu_group_einsum cost 0.00041985511779785156 seconds
DEBUG 01-15 16:09:15.886326.886326 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.886170.886170 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.886290.886290 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019216537475585938 seconds
DEBUG 01-15 16:09:15.886331.886331 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.886579.886579 cuda_h.py:19] end concat_expert_out cost 4.76837158203125e-05 seconds
DEBUG 01-15 16:09:15.886230.886230 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.886345.886345 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:15.886326.886326 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007643699645996094 seconds
DEBUG 01-15 16:09:15.887203.887203 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.887007.887007 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.887397.887397 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014972686767578125 seconds
DEBUG 01-15 16:09:15.887722.887722 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.887361.887361 cuda_h.py:19] end concat_expert_out cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:09:15.887250.887250 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.887988.887988 cuda_h.py:19] end index_scatter cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:09:15.887844.887844 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005061626434326172 seconds
DEBUG 01-15 16:09:15.887469.887469 cuda_h.py:19] end gpu_experts_multi_device cost 0.043273210525512695 seconds
DEBUG 01-15 16:09:15.887286.887286 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.05389714241027832 seconds
DEBUG 01-15 16:09:15.888412.888412 cuda_h.py:19] end prefill_layer cost 0.060942649841308594 seconds
DEBUG 01-15 16:09:15.888746.888746 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:09:15.888449.888449 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:15.888582.888582 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:09:15.888954.888954 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:09:15.888710.888710 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:09:15.888699.888699 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 3.4332275390625e-05 seconds
DEBUG 01-15 16:09:15.888482.888482 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 8.726119995117188e-05 seconds
DEBUG 01-15 16:09:15.888608.888608 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:15.888969.888969 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:15.888119.888119 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:15.888138.888138 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:15.888772.888772 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:15.889671.889671 cuda_h.py:19] end allocate_cuda_memory cost 0.0003955364227294922 seconds
DEBUG 01-15 16:09:15.889332.889332 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:15.889468.889468 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.889545.889545 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.890588.890588 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a3b32ab2-2f93-4fa2-ad4d-c65b376ff82f
DEBUG 01-15 16:09:15.890899.890899 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.890032.890032 cuda_h.py:10] start self_attn
INFO 01-15 16:09:15.891056.891056 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a3b32ab2-2f93-4fa2-ad4d-c65b376ff82f
DEBUG 01-15 16:09:15.891519.891519 cuda_h.py:19] end load_into_gpu_async cost 0.0017194747924804688 seconds
DEBUG 01-15 16:09:15.891217.891217 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:15.891543.891543 cuda_h.py:19] end restore_tensors2 cost 0.0001456737518310547 seconds
DEBUG 01-15 16:09:15.891693.891693 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0030002593994140625 seconds
INFO 01-15 16:09:15.892605.892605 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a3b32ab2-2f93-4fa2-ad4d-c65b376ff82f
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:15.894618.894618 cuda_h.py:19] end self_attn cost 0.003796815872192383 seconds
DEBUG 01-15 16:09:15.894695.894695 cuda_h.py:19] end iln_self_attn_paln cost 0.006105661392211914 seconds
DEBUG 01-15 16:09:15.894094.894094 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:09:15.894234.894234 cuda_h.py:10] start gate
DEBUG 01-15 16:09:15.895104.895104 cuda_h.py:19] end gate cost 0.0006434917449951172 seconds
DEBUG 01-15 16:09:15.895695.895695 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:15.895646.895646 lmp.py:1912] 
DEBUG 01-15 16:09:15.895646.895646 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:15.895164.895164 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:15.895675.895675 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:15.895225.895225 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:15.895345.895345 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:15.895657.895657 lmp.py:1917] 
DEBUG 01-15 16:09:15.895657.895657 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:15.895114.895114 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:15.895433.895433 lmp.py:1935]   Expert 34 |     23 | CPU
DEBUG 01-15 16:09:15.895553.895553 lmp.py:1935]   Expert 45 |     64 | CPU
DEBUG 01-15 16:09:15.895957.895957 lmp.py:1935]   Expert 22 |     74 | CPU
DEBUG 01-15 16:09:15.895124.895124 lmp.py:1935]   Expert 57 |     78 | CPU
DEBUG 01-15 16:09:15.895243.895243 lmp.py:1935]   Expert 17 |     96 | CPU
DEBUG 01-15 16:09:15.895933.895933 lmp.py:1935]   Expert 15 |     99 | CPU
DEBUG 01-15 16:09:15.895814.895814 lmp.py:1935]   Expert  4 |    100 | CPU
DEBUG 01-15 16:09:15.895503.895503 lmp.py:1935]   Expert 28 |    107 | CPU
DEBUG 01-15 16:09:15.896623.896623 lmp.py:1935]   Expert 32 |    112 | CPU
DEBUG 01-15 16:09:15.896981.896981 lmp.py:1935]   Expert 60 |    112 | CPU
DEBUG 01-15 16:09:15.896817.896817 lmp.py:1935]   Expert 36 |    125 | CPU
DEBUG 01-15 16:09:15.896175.896175 lmp.py:1935]   Expert 14 |    126 | CPU
DEBUG 01-15 16:09:15.896818.896818 lmp.py:1935]   Expert 16 |    127 | CPU
DEBUG 01-15 16:09:15.896222.896222 lmp.py:1935]   Expert 12 |    128 | CPU
DEBUG 01-15 16:09:15.896627.896627 lmp.py:1935]   Expert 25 |    130 | CPU
DEBUG 01-15 16:09:15.896555.896555 lmp.py:1935]   Expert 52 |    130 | CPU
DEBUG 01-15 16:09:15.896674.896674 lmp.py:1935]   Expert  8 |    135 | CPU
DEBUG 01-15 16:09:15.896079.896079 lmp.py:1935]   Expert  2 |    140 | CPU
DEBUG 01-15 16:09:15.896722.896722 lmp.py:1935]   Expert 35 |    144 | CPU
DEBUG 01-15 16:09:15.896650.896650 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:09:15.896054.896054 lmp.py:1935]   Expert 23 |    153 | CPU
DEBUG 01-15 16:09:15.896936.896936 lmp.py:1935]   Expert 30 |    154 | CPU
DEBUG 01-15 16:09:15.896817.896817 lmp.py:1935]   Expert  0 |    156 | CPU
DEBUG 01-15 16:09:15.896222.896222 lmp.py:1935]   Expert 39 |    156 | CPU
DEBUG 01-15 16:09:15.896818.896818 lmp.py:1935]   Expert 61 |    158 | CPU
DEBUG 01-15 16:09:15.896892.896892 lmp.py:1935]   Expert  3 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896488.896488 lmp.py:1935]   Expert 42 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896847.896847 lmp.py:1935]   Expert 13 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896966.896966 lmp.py:1935]   Expert 31 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896848.896848 lmp.py:1935]   Expert 41 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896312.896312 lmp.py:1935]   Expert 44 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896121.896121 lmp.py:1935]   Expert 46 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896956.896956 lmp.py:1935]   Expert  9 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896745.896745 lmp.py:1935]   Expert 43 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896534.896534 lmp.py:1935]   Expert 62 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896607.896607 lmp.py:1935]   Expert 18 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896966.896966 lmp.py:1935]   Expert 26 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896085.896085 lmp.py:1935]   Expert 50 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896920.896920 lmp.py:1935]   Expert 27 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896802.896802 lmp.py:1935]   Expert 51 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896922.896922 lmp.py:1935]   Expert 49 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896803.896803 lmp.py:1935]   Expert 11 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896400.896400 lmp.py:1935]   Expert 47 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896758.896758 lmp.py:1935]   Expert 19 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896547.896547 lmp.py:1935]   Expert 20 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896097.896097 lmp.py:1935]   Expert 63 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896363.896363 lmp.py:1935]   Expert 55 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896721.896721 lmp.py:1935]   Expert 56 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896079.896079 lmp.py:1935]   Expert 38 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896960.896960 lmp.py:1935]   Expert 48 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896080.896080 lmp.py:1935]   Expert  1 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896915.896915 lmp.py:1935]   Expert 10 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896512.896512 lmp.py:1935]   Expert 54 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896155.896155 lmp.py:1935]   Expert  7 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896275.896275 lmp.py:1935]   Expert 21 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896587.896587 lmp.py:1935]   Expert 33 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896137.896137 lmp.py:1935]   Expert 29 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896734.896734 lmp.py:1935]   Expert 40 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896092.896092 lmp.py:1935]   Expert 24 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896212.896212 lmp.py:1935]   Expert 59 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.896808.896808 lmp.py:1935]   Expert 37 |    330 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.896451.896451 lmp.py:1935]   Expert 58 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.897571.897571 lmp.py:1935]   Expert  6 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.897214.897214 lmp.py:1935]   Expert 53 |    854 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.897380.897380 lmp.py:1937] 
DEBUG 01-15 16:09:15.897380.897380 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:15.897262.897262 lmp.py:1938]   CPU:   2974 tokens
DEBUG 01-15 16:09:15.897620.897620 lmp.py:1942]   cuda:1:   4667 tokens (19 experts)
DEBUG 01-15 16:09:15.897455.897455 lmp.py:1942]   cuda:2:   4647 tokens (20 experts)
DEBUG 01-15 16:09:15.897813.897813 lmp.py:1943]   Total GPU:   9314 tokens
DEBUG 01-15 16:09:15.897171.897171 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:15.897171.897171 lmp.py:1944] 
DEBUG 01-15 16:09:15.897728.897728 cuda_h.py:19] end experts_map_get cost 0.0017516613006591797 seconds
DEBUG 01-15 16:09:15.897764.897764 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:15.897135.897135 lmp.py:1953] 
DEBUG 01-15 16:09:15.897135.897135 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:15.897588.897588 cuda_h.py:19] end cpu_experts_submit cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:09:15.897092.897092 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:15.897736.897736 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:15.898212.898212 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.3.gate_proj.weight': 0, 'model.layers.5.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.26.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.26.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.26.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.40.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.40.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.40.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.41.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.41.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.41.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.46.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.46.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.46.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.5.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.5.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.5.mlp.experts.62.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(7341604864, 5767168, 0, 0), (7347372032, 5767168, 5767168, 0), (7335837696, 5767168, 11534336, 0), (7410810880, 5767168, 17301504, 0), (7416578048, 5767168, 23068672, 0), (7405043712, 5767168, 28835840, 0), (7462715392, 5767168, 34603008, 0), (7468482560, 5767168, 40370176, 0), (7456948224, 5767168, 46137344, 0), (7480016896, 5767168, 51904512, 0), (7485784064, 5767168, 57671680, 0), (7474249728, 5767168, 63438848, 0), (7618428928, 5767168, 69206016, 0), (7624196096, 5767168, 74973184, 0), (7612661760, 5767168, 80740352, 0), (7739539456, 5767168, 86507520, 0), (7745306624, 5767168, 92274688, 0), (7733772288, 5767168, 98041856, 0), (7756840960, 5767168, 103809024, 0), (7762608128, 5767168, 109576192, 0), (7751073792, 5767168, 115343360, 0), (7826046976, 5767168, 121110528, 0), (7831814144, 5767168, 126877696, 0), (7820279808, 5767168, 132644864, 0), (7860649984, 5767168, 138412032, 0), (7866417152, 5767168, 144179200, 0), (7854882816, 5767168, 149946368, 0), (7981760512, 5767168, 155713536, 0), (7987527680, 5767168, 161480704, 0), (7975993344, 5767168, 167247872, 0), (7999062016, 5767168, 173015040, 0), (8004829184, 5767168, 178782208, 0), (7993294848, 5767168, 184549376, 0), (8033665024, 5767168, 190316544, 0), (8039432192, 5767168, 196083712, 0), (8027897856, 5767168, 201850880, 0), (8085569536, 5767168, 207618048, 0), (8091336704, 5767168, 213385216, 0), (8079802368, 5767168, 219152384, 0), (8120172544, 5767168, 224919552, 0), (8125939712, 5767168, 230686720, 0), (8114405376, 5767168, 236453888, 0), (8154775552, 5767168, 242221056, 0), (8160542720, 5767168, 247988224, 0), (8149008384, 5767168, 253755392, 0), (8206680064, 5767168, 259522560, 0), (8212447232, 5767168, 265289728, 0), (8200912896, 5767168, 271056896, 0), (8258584576, 5767168, 276824064, 0), (8264351744, 5767168, 282591232, 0), (8252817408, 5767168, 288358400, 0), (8310489088, 5767168, 294125568, 0), (8316256256, 5767168, 299892736, 0), (8304721920, 5767168, 305659904, 0), (8379695104, 5767168, 311427072, 0), (8385462272, 5767168, 317194240, 0), (8373927936, 5767168, 322961408, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (7393509376, 5767168, 17301504, 0), (7399276544, 5767168, 23068672, 0), (7387742208, 5767168, 28835840, 0), (7445413888, 5767168, 34603008, 0), (7451181056, 5767168, 40370176, 0), (7439646720, 5767168, 46137344, 0), (7514619904, 5767168, 51904512, 0), (7520387072, 5767168, 57671680, 0), (7508852736, 5767168, 63438848, 0), (7601127424, 5767168, 69206016, 0), (7606894592, 5767168, 74973184, 0), (7595360256, 5767168, 80740352, 0), (7635730432, 5767168, 86507520, 0), (7641497600, 5767168, 92274688, 0), (7629963264, 5767168, 98041856, 0), (7653031936, 5767168, 103809024, 0), (7658799104, 5767168, 109576192, 0), (7647264768, 5767168, 115343360, 0), (7704936448, 5767168, 121110528, 0), (7710703616, 5767168, 126877696, 0), (7699169280, 5767168, 132644864, 0), (7791443968, 5767168, 138412032, 0), (7797211136, 5767168, 144179200, 0), (7785676800, 5767168, 149946368, 0), (7929856000, 5767168, 155713536, 0), (7935623168, 5767168, 161480704, 0), (7924088832, 5767168, 167247872, 0), (7947157504, 5767168, 173015040, 0), (7952924672, 5767168, 178782208, 0), (7941390336, 5767168, 184549376, 0), (8016363520, 5767168, 190316544, 0), (8022130688, 5767168, 196083712, 0), (8010596352, 5767168, 201850880, 0), (8050966528, 5767168, 207618048, 0), (8056733696, 5767168, 213385216, 0), (8045199360, 5767168, 219152384, 0), (8102871040, 5767168, 224919552, 0), (8108638208, 5767168, 230686720, 0), (8097103872, 5767168, 236453888, 0), (8137474048, 5767168, 242221056, 0), (8143241216, 5767168, 247988224, 0), (8131706880, 5767168, 253755392, 0), (8172077056, 5767168, 259522560, 0), (8177844224, 5767168, 265289728, 0), (8166309888, 5767168, 271056896, 0), (8223981568, 5767168, 276824064, 0), (8229748736, 5767168, 282591232, 0), (8218214400, 5767168, 288358400, 0), (8241283072, 5767168, 294125568, 0), (8247050240, 5767168, 299892736, 0), (8235515904, 5767168, 305659904, 0), (8293187584, 5767168, 311427072, 0), (8298954752, 5767168, 317194240, 0), (8287420416, 5767168, 322961408, 0), (8362393600, 5767168, 328728576, 0), (8368160768, 5767168, 334495744, 0), (8356626432, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b354beee80>, 2: <capsule object NULL at 0x74a9f372a370>}
DEBUG 01-15 16:09:15.899072.899072 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.899875.899875 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.899393.899393 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a6670310-c0d4-4d70-8837-e317370a4681
DEBUG 01-15 16:09:15.899817.899817 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.899450.899450 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:15.899621.899621 client.py:127] Model loaded
DEBUG 01-15 16:09:15.899923.899923 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.899519.899519 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:15.900304.900304 cuda_h.py:19] end move_flatidxs cost 0.0008373260498046875 seconds
DEBUG 01-15 16:09:15.900604.900604 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:15.900550.900550 cuda_h.py:19] end restore2model cost 0.0010390281677246094 seconds
INFO 01-15 16:09:15.901534.901534 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a6670310-c0d4-4d70-8837-e317370a4681
DEBUG 01-15 16:09:15.901663.901663 cuda_h.py:19] end sllm_worker_task cost 0.012333869934082031 seconds
DEBUG 01-15 16:09:15.901709.901709 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00429844856262207 seconds
DEBUG 01-15 16:09:15.901449.901449 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.904043.904043 cuda_h.py:19] end restore2model cost 0.003070831298828125 seconds
DEBUG 01-15 16:09:15.905363.905363 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007691383361816406 seconds
DEBUG 01-15 16:09:15.905397.905397 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:15.905937.905937 cuda_h.py:19] end gpu_sexperts cost 0.00026226043701171875 seconds
DEBUG 01-15 16:09:15.905905.905905 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:15.905728.905728 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.71661376953125e-05 seconds
DEBUG 01-15 16:09:15.905709.905709 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:15.905411.905411 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.905703.905703 cuda_h.py:19] end group_tensors cost 0.00470733642578125 seconds
DEBUG 01-15 16:09:15.906565.906565 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:15.906921.906921 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013239383697509766 seconds
DEBUG 01-15 16:09:15.906521.906521 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.907500.907500 cuda_h.py:19] end gpu_group_list cost 0.00032591819763183594 seconds
DEBUG 01-15 16:09:15.908608.908608 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.909259.909259 cuda_h.py:19] end group pad cost 0.003271818161010742 seconds
DEBUG 01-15 16:09:15.909996.909996 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:15.912087.912087 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0034830570220947266 seconds
DEBUG 01-15 16:09:15.913141.913141 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.915249.915249 cuda_h.py:19] end gpu_group_list cost 0.0011324882507324219 seconds
DEBUG 01-15 16:09:15.918130.918130 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:15.919894.919894 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a6670310-c0d4-4d70-8837-e317370a4681
DEBUG 01-15 16:09:15.938871.938871 cuda_h.py:19] end group_einsum cost 0.028605937957763672 seconds
DEBUG 01-15 16:09:15.938697.938697 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:15.941448.941448 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029790401458740234 seconds
INFO 01-15 16:09:15.941313.941313 client.py:127] Model loaded
DEBUG 01-15 16:09:15.941397.941397 cuda_h.py:19] end wait_experts_multi_device cost 0.022551298141479492 seconds
DEBUG 01-15 16:09:15.941067.941067 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:15.942225.942225 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0426785945892334 seconds
DEBUG 01-15 16:09:15.943583.943583 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0012807846069335938 seconds
DEBUG 01-15 16:09:15.943265.943265 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:15.944760.944760 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.945648.945648 cuda_h.py:19] end index_scatter cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:09:15.945941.945941 cuda_h.py:19] end cpuoutputsdeal cost 0.0023031234741210938 seconds
DEBUG 01-15 16:09:15.945103.945103 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:15.945865.945865 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.945038.945038 cuda_h.py:19] end gpu_group_tensor cost 0.00016379356384277344 seconds
DEBUG 01-15 16:09:15.945277.945277 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:15.946932.946932 cuda_h.py:19] end gpu_group_tensor cost 0.0001366138458251953 seconds
DEBUG 01-15 16:09:15.946836.946836 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.946681.946681 cuda_h.py:19] end gpu_group_einsum cost 0.0004718303680419922 seconds
DEBUG 01-15 16:09:15.946420.946420 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:15.947984.947984 cuda_h.py:19] end gpu_group_einsum cost 0.00036644935607910156 seconds
DEBUG 01-15 16:09:15.947756.947756 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.947375.947375 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.947019.947019 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019311904907226562 seconds
DEBUG 01-15 16:09:15.947490.947490 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.947691.947691 cuda_h.py:19] end concat_expert_out cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:09:15.947534.947534 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.948504.948504 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:09:15.948592.948592 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007817745208740234 seconds
DEBUG 01-15 16:09:15.948422.948422 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:15.948901.948901 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:15.948053.948053 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001499652862548828 seconds
DEBUG 01-15 16:09:15.948525.948525 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:15.948878.948878 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:09:15.948245.948245 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:15.948453.948453 cuda_h.py:19] end index_scatter cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:15.948785.948785 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005137920379638672 seconds
DEBUG 01-15 16:09:15.948317.948317 cuda_h.py:19] end gpu_experts_multi_device cost 0.04347395896911621 seconds
DEBUG 01-15 16:09:15.949327.949327 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.05438995361328125 seconds
DEBUG 01-15 16:09:15.949500.949500 cuda_h.py:19] end prefill_layer cost 0.06124234199523926 seconds
DEBUG 01-15 16:09:15.949734.949734 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:09:15.949676.949676 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:15.949094.949094 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:09:15.949181.949181 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:09:15.949652.949652 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:09:15.949171.949171 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 3.886222839355469e-05 seconds
DEBUG 01-15 16:09:15.949020.949020 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 6.985664367675781e-05 seconds
DEBUG 01-15 16:09:15.949193.949193 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:15.949023.949023 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:15.949550.949550 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:15.950848.950848 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:15.950243.950243 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:15.950156.950156 cuda_h.py:19] end allocate_cuda_memory cost 0.0004410743713378906 seconds
DEBUG 01-15 16:09:15.951804.951804 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:15.951907.951907 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.951329.951329 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.951266.951266 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e12142bb-fa41-427d-9dd7-030dd1102e4d
DEBUG 01-15 16:09:15.951260.951260 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.951200.951200 cuda_h.py:10] start self_attn
INFO 01-15 16:09:15.952231.952231 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e12142bb-fa41-427d-9dd7-030dd1102e4d
DEBUG 01-15 16:09:15.952131.952131 cuda_h.py:19] end load_into_gpu_async cost 0.001741647720336914 seconds
DEBUG 01-15 16:09:15.952922.952922 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:15.953838.953838 cuda_h.py:19] end restore_tensors2 cost 0.000148773193359375 seconds
DEBUG 01-15 16:09:15.953180.953180 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0030829906463623047 seconds
INFO 01-15 16:09:15.953536.953536 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e12142bb-fa41-427d-9dd7-030dd1102e4d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:15.955420.955420 cuda_h.py:19] end self_attn cost 0.003912448883056641 seconds
DEBUG 01-15 16:09:15.956251.956251 cuda_h.py:19] end iln_self_attn_paln cost 0.006262063980102539 seconds
DEBUG 01-15 16:09:15.956995.956995 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:09:15.956751.956751 cuda_h.py:10] start gate
DEBUG 01-15 16:09:15.956389.956389 cuda_h.py:19] end gate cost 0.0006492137908935547 seconds
DEBUG 01-15 16:09:15.956172.956172 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:15.957799.957799 lmp.py:1912] 
DEBUG 01-15 16:09:15.957799.957799 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:15.957701.957701 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:15.957642.957642 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:15.957623.957623 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:15.957412.957412 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:15.957770.957770 lmp.py:1917] 
DEBUG 01-15 16:09:15.957770.957770 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:15.957605.957605 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:15.957447.957447 lmp.py:1935]   Expert  1 |     45 | CPU
DEBUG 01-15 16:09:15.957282.957282 lmp.py:1935]   Expert  7 |     60 | CPU
DEBUG 01-15 16:09:15.957163.957163 lmp.py:1935]   Expert 37 |     70 | CPU
DEBUG 01-15 16:09:15.957045.957045 lmp.py:1935]   Expert 54 |     75 | CPU
DEBUG 01-15 16:09:15.957926.957926 lmp.py:1935]   Expert 17 |     76 | CPU
DEBUG 01-15 16:09:15.957807.957807 lmp.py:1935]   Expert 18 |     83 | CPU
DEBUG 01-15 16:09:15.957689.957689 lmp.py:1935]   Expert  9 |     90 | CPU
DEBUG 01-15 16:09:15.957809.957809 lmp.py:1935]   Expert 13 |     90 | CPU
DEBUG 01-15 16:09:15.957690.957690 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:09:15.957002.957002 lmp.py:1935]   Expert 22 |    102 | CPU
DEBUG 01-15 16:09:15.957360.957360 lmp.py:1935]   Expert  0 |    110 | CPU
DEBUG 01-15 16:09:15.957447.957447 lmp.py:1935]   Expert 26 |    116 | CPU
DEBUG 01-15 16:09:15.957375.957375 lmp.py:1935]   Expert 16 |    119 | CPU
DEBUG 01-15 16:09:15.957541.957541 lmp.py:1935]   Expert 10 |    122 | CPU
DEBUG 01-15 16:09:15.957230.957230 lmp.py:1935]   Expert 63 |    129 | CPU
DEBUG 01-15 16:09:15.957681.957681 lmp.py:1935]   Expert 59 |    130 | CPU
DEBUG 01-15 16:09:15.957907.957907 lmp.py:1935]   Expert 62 |    142 | CPU
DEBUG 01-15 16:09:15.957074.957074 lmp.py:1935]   Expert 43 |    143 | CPU
DEBUG 01-15 16:09:15.957478.957478 lmp.py:1935]   Expert 28 |    145 | CPU
DEBUG 01-15 16:09:15.957167.957167 lmp.py:1935]   Expert 33 |    147 | CPU
DEBUG 01-15 16:09:15.957334.957334 lmp.py:1935]   Expert 29 |    150 | CPU
DEBUG 01-15 16:09:15.957500.957500 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:09:15.957189.957189 lmp.py:1935]   Expert 51 |    162 | CPU
DEBUG 01-15 16:09:15.957786.957786 lmp.py:1935]   Expert  3 |    166 | CPU
DEBUG 01-15 16:09:15.957190.957190 lmp.py:1935]   Expert 23 |    167 | CPU
DEBUG 01-15 16:09:15.957025.957025 lmp.py:1935]   Expert 32 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.957145.957145 lmp.py:1935]   Expert 53 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.957742.957742 lmp.py:1935]   Expert 55 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.957385.957385 lmp.py:1935]   Expert 11 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.957504.957504 lmp.py:1935]   Expert 45 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.957909.957909 lmp.py:1935]   Expert 40 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.957790.957790 lmp.py:1935]   Expert 14 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.957672.957672 lmp.py:1935]   Expert 34 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.957792.957792 lmp.py:1935]   Expert 41 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958673.958673 lmp.py:1935]   Expert 52 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958793.958793 lmp.py:1935]   Expert 42 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958197.958197 lmp.py:1935]   Expert 21 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958079.958079 lmp.py:1935]   Expert 57 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958960.958960 lmp.py:1935]   Expert 30 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958511.958511 lmp.py:1935]   Expert 15 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958061.958061 lmp.py:1935]   Expert 35 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958757.958757 lmp.py:1935]   Expert 12 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958400.958400 lmp.py:1935]   Expert  4 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958520.958520 lmp.py:1935]   Expert 19 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958163.958163 lmp.py:1935]   Expert 50 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958282.958282 lmp.py:1935]   Expert 24 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958164.958164 lmp.py:1935]   Expert 46 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958045.958045 lmp.py:1935]   Expert 44 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958927.958927 lmp.py:1935]   Expert 49 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958523.958523 lmp.py:1935]   Expert  8 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958597.958597 lmp.py:1935]   Expert 38 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958670.958670 lmp.py:1935]   Expert  6 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958982.958982 lmp.py:1935]   Expert 47 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958102.958102 lmp.py:1935]   Expert 31 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958222.958222 lmp.py:1935]   Expert 61 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958580.958580 lmp.py:1935]   Expert 39 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958223.958223 lmp.py:1935]   Expert  5 |    305 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958343.958343 lmp.py:1935]   Expert 27 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958986.958986 lmp.py:1935]   Expert 36 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958106.958106 lmp.py:1935]   Expert 60 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958749.958749 lmp.py:1935]   Expert 20 |    338 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958061.958061 lmp.py:1935]   Expert 48 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958134.958134 lmp.py:1935]   Expert 25 |    398 | GPU2(cuda:2)
DEBUG 01-15 16:09:15.958446.958446 lmp.py:1935]   Expert 56 |    557 | GPU1(cuda:1)
DEBUG 01-15 16:09:15.958950.958950 lmp.py:1937] 
DEBUG 01-15 16:09:15.958950.958950 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:15.958831.958831 lmp.py:1938]   CPU:   2896 tokens
DEBUG 01-15 16:09:15.958713.958713 lmp.py:1942]   cuda:1:   4623 tokens (19 experts)
DEBUG 01-15 16:09:15.958356.958356 lmp.py:1942]   cuda:2:   4769 tokens (20 experts)
DEBUG 01-15 16:09:15.958522.958522 lmp.py:1943]   Total GPU:   9392 tokens
DEBUG 01-15 16:09:15.958211.958211 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:15.958211.958211 lmp.py:1944] 
DEBUG 01-15 16:09:15.958384.958384 cuda_h.py:19] end experts_map_get cost 0.001743316650390625 seconds
DEBUG 01-15 16:09:15.958419.958419 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:15.958837.958837 lmp.py:1953] 
DEBUG 01-15 16:09:15.958837.958837 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:15.958336.958336 cuda_h.py:19] end cpu_experts_submit cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:15.958648.958648 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:15.958292.958292 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:15.960350.960350 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.4.gate_proj.weight': 0, 'model.layers.6.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.14.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.14.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.14.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.30.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.30.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.30.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.36.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.36.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.36.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.6.mlp.experts.6.gate_proj.weight': 0, 'model.layers.6.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.25.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.25.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.25.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.46.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.46.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.46.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.6.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.6.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.6.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(8466202624, 5767168, 0, 0), (8471969792, 5767168, 5767168, 0), (8460435456, 5767168, 11534336, 0), (8483504128, 5767168, 17301504, 0), (8489271296, 5767168, 23068672, 0), (8477736960, 5767168, 28835840, 0), (8639217664, 5767168, 34603008, 0), (8644984832, 5767168, 40370176, 0), (8633450496, 5767168, 46137344, 0), (8725725184, 5767168, 51904512, 0), (8731492352, 5767168, 57671680, 0), (8719958016, 5767168, 63438848, 0), (8743026688, 5767168, 69206016, 0), (8748793856, 5767168, 74973184, 0), (8737259520, 5767168, 80740352, 0), (8760328192, 5767168, 86507520, 0), (8766095360, 5767168, 92274688, 0), (8754561024, 5767168, 98041856, 0), (8812232704, 5767168, 103809024, 0), (8817999872, 5767168, 109576192, 0), (8806465536, 5767168, 115343360, 0), (8916041728, 5767168, 121110528, 0), (8921808896, 5767168, 126877696, 0), (8910274560, 5767168, 132644864, 0), (9002549248, 5767168, 138412032, 0), (9008316416, 5767168, 144179200, 0), (8996782080, 5767168, 149946368, 0), (9019850752, 5767168, 155713536, 0), (9025617920, 5767168, 161480704, 0), (9014083584, 5767168, 167247872, 0), (9054453760, 5767168, 173015040, 0), (9060220928, 5767168, 178782208, 0), (9048686592, 5767168, 184549376, 0), (9089056768, 5767168, 190316544, 0), (9094823936, 5767168, 196083712, 0), (9083289600, 5767168, 201850880, 0), (9106358272, 5767168, 207618048, 0), (9112125440, 5767168, 213385216, 0), (9100591104, 5767168, 219152384, 0), (9158262784, 5767168, 224919552, 0), (9164029952, 5767168, 230686720, 0), (9152495616, 5767168, 236453888, 0), (9175564288, 5767168, 242221056, 0), (9181331456, 5767168, 247988224, 0), (9169797120, 5767168, 253755392, 0), (9210167296, 5767168, 259522560, 0), (9215934464, 5767168, 265289728, 0), (9204400128, 5767168, 271056896, 0), (9313976320, 5767168, 276824064, 0), (9319743488, 5767168, 282591232, 0), (9308209152, 5767168, 288358400, 0), (9365880832, 5767168, 294125568, 0), (9371648000, 5767168, 299892736, 0), (9360113664, 5767168, 305659904, 0), (9452388352, 5767168, 311427072, 0), (9458155520, 5767168, 317194240, 0), (9446621184, 5767168, 322961408, 0)], 2: [(8500805632, 5767168, 0, 0), (8506572800, 5767168, 5767168, 0), (8495038464, 5767168, 11534336, 0), (8535408640, 5767168, 17301504, 0), (8541175808, 5767168, 23068672, 0), (8529641472, 5767168, 28835840, 0), (8587313152, 5767168, 34603008, 0), (8593080320, 5767168, 40370176, 0), (8581545984, 5767168, 46137344, 0), (8604614656, 5767168, 51904512, 0), (8610381824, 5767168, 57671680, 0), (8598847488, 5767168, 63438848, 0), (8656519168, 5767168, 69206016, 0), (8662286336, 5767168, 74973184, 0), (8650752000, 5767168, 80740352, 0), (8829534208, 5767168, 86507520, 0), (8835301376, 5767168, 92274688, 0), (8823767040, 5767168, 98041856, 0), (8864137216, 5767168, 103809024, 0), (8869904384, 5767168, 109576192, 0), (8858370048, 5767168, 115343360, 0), (8933343232, 5767168, 121110528, 0), (8939110400, 5767168, 126877696, 0), (8927576064, 5767168, 132644864, 0), (8950644736, 5767168, 138412032, 0), (8956411904, 5767168, 144179200, 0), (8944877568, 5767168, 149946368, 0), (8985247744, 5767168, 155713536, 0), (8991014912, 5767168, 161480704, 0), (8979480576, 5767168, 167247872, 0), (9071755264, 5767168, 173015040, 0), (9077522432, 5767168, 178782208, 0), (9065988096, 5767168, 184549376, 0), (9123659776, 5767168, 190316544, 0), (9129426944, 5767168, 196083712, 0), (9117892608, 5767168, 201850880, 0), (9192865792, 5767168, 207618048, 0), (9198632960, 5767168, 213385216, 0), (9187098624, 5767168, 219152384, 0), (9227468800, 5767168, 224919552, 0), (9233235968, 5767168, 230686720, 0), (9221701632, 5767168, 236453888, 0), (9244770304, 5767168, 242221056, 0), (9250537472, 5767168, 247988224, 0), (9239003136, 5767168, 253755392, 0), (9262071808, 5767168, 259522560, 0), (9267838976, 5767168, 265289728, 0), (9256304640, 5767168, 271056896, 0), (9296674816, 5767168, 276824064, 0), (9302441984, 5767168, 282591232, 0), (9290907648, 5767168, 288358400, 0), (9348579328, 5767168, 294125568, 0), (9354346496, 5767168, 299892736, 0), (9342812160, 5767168, 305659904, 0), (9383182336, 5767168, 311427072, 0), (9388949504, 5767168, 317194240, 0), (9377415168, 5767168, 322961408, 0), (9435086848, 5767168, 328728576, 0), (9440854016, 5767168, 334495744, 0), (9429319680, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b351c25dd0>, 2: <capsule object NULL at 0x74a6881a5fe0>}
DEBUG 01-15 16:09:15.960977.960977 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:15.960602.960602 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:15.960690.960690 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 72f974d5-41ba-4b5a-98b0-158a0059a97b
DEBUG 01-15 16:09:15.960704.960704 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:15.960807.960807 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:15.961883.961883 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:15.961326.961326 client.py:127] Model loaded
DEBUG 01-15 16:09:15.961290.961290 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.962767.962767 cuda_h.py:19] end move_flatidxs cost 0.0008394718170166016 seconds
DEBUG 01-15 16:09:15.962828.962828 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:15.962089.962089 cuda_h.py:19] end restore2model cost 0.0010268688201904297 seconds
INFO 01-15 16:09:15.962265.962265 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 72f974d5-41ba-4b5a-98b0-158a0059a97b
DEBUG 01-15 16:09:15.962633.962633 cuda_h.py:19] end sllm_worker_task cost 0.012384653091430664 seconds
DEBUG 01-15 16:09:15.963121.963121 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004157304763793945 seconds
DEBUG 01-15 16:09:15.963961.963961 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:15.966027.966027 cuda_h.py:19] end restore2model cost 0.0031385421752929688 seconds
DEBUG 01-15 16:09:15.966533.966533 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0076181888580322266 seconds
DEBUG 01-15 16:09:15.966633.966633 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:15.966166.966166 cuda_h.py:19] end gpu_sexperts cost 0.00025773048400878906 seconds
DEBUG 01-15 16:09:15.966088.966088 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:15.966533.966533 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.0265579223632812e-05 seconds
DEBUG 01-15 16:09:15.966514.966514 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:15.966548.966548 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.966782.966782 cuda_h.py:19] end group_tensors cost 0.004704713821411133 seconds
DEBUG 01-15 16:09:15.967380.967380 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:15.968886.968886 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013396739959716797 seconds
DEBUG 01-15 16:09:15.968386.968386 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.968537.968537 cuda_h.py:19] end gpu_group_list cost 0.0003142356872558594 seconds
DEBUG 01-15 16:09:15.970712.970712 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:15.970819.970819 cuda_h.py:19] end group pad cost 0.0033643245697021484 seconds
DEBUG 01-15 16:09:15.970416.970416 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:15.975599.975599 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.004997730255126953 seconds
DEBUG 01-15 16:09:15.976944.976944 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:15.978029.978029 cuda_h.py:19] end gpu_group_list cost 0.0011119842529296875 seconds
DEBUG 01-15 16:09:15.980867.980867 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:15.981563.981563 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 72f974d5-41ba-4b5a-98b0-158a0059a97b
DEBUG 01-15 16:09:15.999135.999135 cuda_h.py:19] end group_einsum cost 0.02887272834777832 seconds
DEBUG 01-15 16:09:16.000544.000544 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:16.002965.002965 client.py:127] Model loaded
DEBUG 01-15 16:09:16.002063.002063 cuda_h.py:19] end wait_experts_multi_device cost 0.02166604995727539 seconds
DEBUG 01-15 16:09:16.002064.002064 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.003096.003096 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029783248901367188 seconds
DEBUG 01-15 16:09:16.003515.003515 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04304933547973633 seconds
DEBUG 01-15 16:09:16.004125.004125 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.001964569091796875 seconds
DEBUG 01-15 16:09:16.004664.004664 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.005704.005704 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.005630.005630 cuda_h.py:19] end index_scatter cost 7.963180541992188e-05 seconds
DEBUG 01-15 16:09:16.006144.006144 cuda_h.py:19] end cpuoutputsdeal cost 0.0014352798461914062 seconds
DEBUG 01-15 16:09:16.006676.006676 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.006578.006578 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.006061.006061 cuda_h.py:19] end gpu_group_tensor cost 0.00015020370483398438 seconds
DEBUG 01-15 16:09:16.006631.006631 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.006140.006140 cuda_h.py:19] end gpu_group_tensor cost 0.0001342296600341797 seconds
DEBUG 01-15 16:09:16.006945.006945 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.007910.007910 cuda_h.py:19] end gpu_group_einsum cost 0.0004911422729492188 seconds
DEBUG 01-15 16:09:16.007841.007841 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.007676.007676 cuda_h.py:19] end gpu_group_einsum cost 0.00035691261291503906 seconds
DEBUG 01-15 16:09:16.008256.008256 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.008200.008200 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.008387.008387 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019431114196777344 seconds
DEBUG 01-15 16:09:16.008765.008765 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.008066.008066 cuda_h.py:19] end concat_expert_out cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:09:16.008194.008194 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.008548.008548 cuda_h.py:19] end index_scatter cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:16.008211.008211 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007987022399902344 seconds
DEBUG 01-15 16:09:16.008625.008625 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.009521.009521 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.009150.009150 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015044212341308594 seconds
DEBUG 01-15 16:09:16.009714.009714 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.009253.009253 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:16.009904.009904 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.009827.009827 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:09:16.009729.009729 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004947185516357422 seconds
DEBUG 01-15 16:09:16.009308.009308 cuda_h.py:19] end gpu_experts_multi_device cost 0.0426630973815918 seconds
DEBUG 01-15 16:09:16.009032.009032 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.05351114273071289 seconds
DEBUG 01-15 16:09:16.010265.010265 cuda_h.py:19] end prefill_layer cost 0.06048011779785156 seconds
DEBUG 01-15 16:09:16.010214.010214 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:09:16.010155.010155 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.010289.010289 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:09:16.010899.010899 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:09:16.010384.010384 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:09:16.010234.010234 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 3.62396240234375e-05 seconds
DEBUG 01-15 16:09:16.010321.010321 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 6.771087646484375e-05 seconds
DEBUG 01-15 16:09:16.010494.010494 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.010139.010139 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.010013.010013 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.010823.010823 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.010082.010082 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.011106.011106 cuda_h.py:19] end allocate_cuda_memory cost 0.0003921985626220703 seconds
DEBUG 01-15 16:09:16.011370.011370 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.011195.011195 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.011510.011510 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.011871.011871 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 76bdebe0-9441-44f8-9e79-66e2fc175bdb
DEBUG 01-15 16:09:16.012693.012693 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.012019.012019 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.013520.013520 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 76bdebe0-9441-44f8-9e79-66e2fc175bdb
DEBUG 01-15 16:09:16.013513.013513 cuda_h.py:19] end load_into_gpu_async cost 0.0017747879028320312 seconds
DEBUG 01-15 16:09:16.013065.013065 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.013007.013007 cuda_h.py:19] end restore_tensors2 cost 0.0001468658447265625 seconds
DEBUG 01-15 16:09:16.013203.013203 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0030438899993896484 seconds
INFO 01-15 16:09:16.014956.014956 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 76bdebe0-9441-44f8-9e79-66e2fc175bdb
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.016993.016993 cuda_h.py:19] end self_attn cost 0.0039005279541015625 seconds
DEBUG 01-15 16:09:16.016812.016812 cuda_h.py:19] end iln_self_attn_paln cost 0.006312847137451172 seconds
DEBUG 01-15 16:09:16.016449.016449 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:09:16.016636.016636 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.017374.017374 cuda_h.py:19] end gate cost 0.0006492137908935547 seconds
DEBUG 01-15 16:09:16.017065.017065 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.017135.017135 lmp.py:1912] 
DEBUG 01-15 16:09:16.017135.017135 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.017560.017560 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.017687.017687 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.017906.017906 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.017503.017503 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.018338.018338 lmp.py:1917] 
DEBUG 01-15 16:09:16.018338.018338 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.018219.018219 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.018823.018823 lmp.py:1935]   Expert 50 |     44 | CPU
DEBUG 01-15 16:09:16.018466.018466 lmp.py:1935]   Expert  3 |     52 | CPU
DEBUG 01-15 16:09:16.018870.018870 lmp.py:1935]   Expert 46 |     55 | CPU
DEBUG 01-15 16:09:16.018036.018036 lmp.py:1935]   Expert  1 |     74 | CPU
DEBUG 01-15 16:09:16.018441.018441 lmp.py:1935]   Expert  4 |     87 | CPU
DEBUG 01-15 16:09:16.018892.018892 lmp.py:1935]   Expert 29 |     88 | CPU
DEBUG 01-15 16:09:16.018820.018820 lmp.py:1935]   Expert 15 |     96 | CPU
DEBUG 01-15 16:09:16.018224.018224 lmp.py:1935]   Expert 40 |     97 | CPU
DEBUG 01-15 16:09:16.018119.018119 lmp.py:1935]   Expert  8 |    109 | CPU
DEBUG 01-15 16:09:16.018193.018193 lmp.py:1935]   Expert 28 |    112 | CPU
DEBUG 01-15 16:09:16.018458.018458 lmp.py:1935]   Expert 41 |    112 | CPU
DEBUG 01-15 16:09:16.018386.018386 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:09:16.018314.018314 lmp.py:1935]   Expert 27 |    127 | CPU
DEBUG 01-15 16:09:16.018003.018003 lmp.py:1935]   Expert 48 |    128 | CPU
DEBUG 01-15 16:09:16.018931.018931 lmp.py:1935]   Expert  6 |    131 | CPU
DEBUG 01-15 16:09:16.018858.018858 lmp.py:1935]   Expert 13 |    132 | CPU
DEBUG 01-15 16:09:16.018024.018024 lmp.py:1935]   Expert  7 |    134 | CPU
DEBUG 01-15 16:09:16.018429.018429 lmp.py:1935]   Expert 54 |    134 | CPU
DEBUG 01-15 16:09:16.018595.018595 lmp.py:1935]   Expert 51 |    138 | CPU
DEBUG 01-15 16:09:16.018761.018761 lmp.py:1935]   Expert 39 |    139 | CPU
DEBUG 01-15 16:09:16.018119.018119 lmp.py:1935]   Expert 18 |    140 | CPU
DEBUG 01-15 16:09:16.018001.018001 lmp.py:1935]   Expert 60 |    140 | CPU
DEBUG 01-15 16:09:16.018929.018929 lmp.py:1935]   Expert 14 |    147 | CPU
DEBUG 01-15 16:09:16.018856.018856 lmp.py:1935]   Expert 56 |    147 | CPU
DEBUG 01-15 16:09:16.018546.018546 lmp.py:1935]   Expert 43 |    148 | CPU
DEBUG 01-15 16:09:16.018665.018665 lmp.py:1935]   Expert 55 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018024.018024 lmp.py:1935]   Expert 20 |    149 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018382.018382 lmp.py:1935]   Expert 52 |    150 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018025.018025 lmp.py:1935]   Expert 36 |    153 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018906.018906 lmp.py:1935]   Expert 10 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018788.018788 lmp.py:1935]   Expert 45 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018146.018146 lmp.py:1935]   Expert 11 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018458.018458 lmp.py:1935]   Expert  5 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018293.018293 lmp.py:1935]   Expert 62 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018605.018605 lmp.py:1935]   Expert 57 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018486.018486 lmp.py:1935]   Expert 33 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018129.018129 lmp.py:1935]   Expert 44 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018295.018295 lmp.py:1935]   Expert 25 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018415.018415 lmp.py:1935]   Expert 58 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018058.018058 lmp.py:1935]   Expert 53 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018178.018178 lmp.py:1935]   Expert 32 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018059.018059 lmp.py:1935]   Expert  2 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018941.018941 lmp.py:1935]   Expert 31 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018537.018537 lmp.py:1935]   Expert 35 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018849.018849 lmp.py:1935]   Expert 63 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018446.018446 lmp.py:1935]   Expert 21 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018804.018804 lmp.py:1935]   Expert 49 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018685.018685 lmp.py:1935]   Expert 17 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018328.018328 lmp.py:1935]   Expert 42 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018733.018733 lmp.py:1935]   Expert 34 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018614.018614 lmp.py:1935]   Expert 37 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.018019.018019 lmp.py:1935]   Expert 59 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.018662.018662 lmp.py:1935]   Expert 22 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.019066.019066 lmp.py:1935]   Expert  0 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.019186.019186 lmp.py:1935]   Expert 19 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.019690.019690 lmp.py:1935]   Expert 24 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.019002.019002 lmp.py:1935]   Expert 61 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.019076.019076 lmp.py:1935]   Expert 30 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.019195.019195 lmp.py:1935]   Expert 47 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.019838.019838 lmp.py:1935]   Expert 38 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.019958.019958 lmp.py:1935]   Expert 26 |    373 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.019363.019363 lmp.py:1935]   Expert 12 |    428 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.019006.019006 lmp.py:1935]   Expert  9 |    679 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.019410.019410 lmp.py:1935]   Expert 23 |    705 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.019100.019100 lmp.py:1937] 
DEBUG 01-15 16:09:16.019100.019100 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.019027.019027 lmp.py:1938]   CPU:   2836 tokens
DEBUG 01-15 16:09:16.019147.019147 lmp.py:1942]   cuda:1:   4800 tokens (20 experts)
DEBUG 01-15 16:09:16.019790.019790 lmp.py:1942]   cuda:2:   4652 tokens (19 experts)
DEBUG 01-15 16:09:16.019956.019956 lmp.py:1943]   Total GPU:   9452 tokens
DEBUG 01-15 16:09:16.019361.019361 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.019361.019361 lmp.py:1944] 
DEBUG 01-15 16:09:16.019441.019441 cuda_h.py:19] end experts_map_get cost 0.0017237663269042969 seconds
DEBUG 01-15 16:09:16.019384.019384 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.019755.019755 lmp.py:1953] 
DEBUG 01-15 16:09:16.019755.019755 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.019062.019062 cuda_h.py:19] end cpu_experts_submit cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:09:16.019135.019135 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.019157.019157 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.020653.020653 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.17.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.17.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.17.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.7.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.7.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.7.mlp.experts.62.up_proj.weight': 340262912}, 2: {'model.layers.7.mlp.experts.5.gate_proj.weight': 0, 'model.layers.7.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.9.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.9.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.9.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (9538895872, 5767168, 17301504, 0), (9544663040, 5767168, 23068672, 0), (9533128704, 5767168, 28835840, 0), (9798418432, 5767168, 34603008, 0), (9804185600, 5767168, 40370176, 0), (9792651264, 5767168, 46137344, 0), (9833021440, 5767168, 51904512, 0), (9838788608, 5767168, 57671680, 0), (9827254272, 5767168, 63438848, 0), (9867624448, 5767168, 69206016, 0), (9873391616, 5767168, 74973184, 0), (9861857280, 5767168, 80740352, 0), (9902227456, 5767168, 86507520, 0), (9907994624, 5767168, 92274688, 0), (9896460288, 5767168, 98041856, 0), (9936830464, 5767168, 103809024, 0), (9942597632, 5767168, 109576192, 0), (9931063296, 5767168, 115343360, 0), (9954131968, 5767168, 121110528, 0), (9959899136, 5767168, 126877696, 0), (9948364800, 5767168, 132644864, 0), (10040639488, 5767168, 138412032, 0), (10046406656, 5767168, 144179200, 0), (10034872320, 5767168, 149946368, 0), (10092544000, 5767168, 155713536, 0), (10098311168, 5767168, 161480704, 0), (10086776832, 5767168, 167247872, 0), (10127147008, 5767168, 173015040, 0), (10132914176, 5767168, 178782208, 0), (10121379840, 5767168, 184549376, 0), (10161750016, 5767168, 190316544, 0), (10167517184, 5767168, 196083712, 0), (10155982848, 5767168, 201850880, 0), (10282860544, 5767168, 207618048, 0), (10288627712, 5767168, 213385216, 0), (10277093376, 5767168, 219152384, 0), (10403971072, 5767168, 224919552, 0), (10409738240, 5767168, 230686720, 0), (10398203904, 5767168, 236453888, 0), (10421272576, 5767168, 242221056, 0), (10427039744, 5767168, 247988224, 0), (10415505408, 5767168, 253755392, 0), (10455875584, 5767168, 259522560, 0), (10461642752, 5767168, 265289728, 0), (10450108416, 5767168, 271056896, 0), (10490478592, 5767168, 276824064, 0), (10496245760, 5767168, 282591232, 0), (10484711424, 5767168, 288358400, 0), (10525081600, 5767168, 294125568, 0), (10530848768, 5767168, 299892736, 0), (10519314432, 5767168, 305659904, 0), (10559684608, 5767168, 311427072, 0), (10565451776, 5767168, 317194240, 0), (10553917440, 5767168, 322961408, 0), (10576986112, 5767168, 328728576, 0), (10582753280, 5767168, 334495744, 0), (10571218944, 5767168, 340262912, 0)], 2: [(9590800384, 5767168, 0, 0), (9596567552, 5767168, 5767168, 0), (9585033216, 5767168, 11534336, 0), (9660006400, 5767168, 17301504, 0), (9665773568, 5767168, 23068672, 0), (9654239232, 5767168, 28835840, 0), (9677307904, 5767168, 34603008, 0), (9683075072, 5767168, 40370176, 0), (9671540736, 5767168, 46137344, 0), (9694609408, 5767168, 51904512, 0), (9700376576, 5767168, 57671680, 0), (9688842240, 5767168, 63438848, 0), (9711910912, 5767168, 69206016, 0), (9717678080, 5767168, 74973184, 0), (9706143744, 5767168, 80740352, 0), (9850322944, 5767168, 86507520, 0), (9856090112, 5767168, 92274688, 0), (9844555776, 5767168, 98041856, 0), (9884925952, 5767168, 103809024, 0), (9890693120, 5767168, 109576192, 0), (9879158784, 5767168, 115343360, 0), (9919528960, 5767168, 121110528, 0), (9925296128, 5767168, 126877696, 0), (9913761792, 5767168, 132644864, 0), (10023337984, 5767168, 138412032, 0), (10029105152, 5767168, 144179200, 0), (10017570816, 5767168, 149946368, 0), (10057940992, 5767168, 155713536, 0), (10063708160, 5767168, 161480704, 0), (10052173824, 5767168, 167247872, 0), (10075242496, 5767168, 173015040, 0), (10081009664, 5767168, 178782208, 0), (10069475328, 5767168, 184549376, 0), (10109845504, 5767168, 190316544, 0), (10115612672, 5767168, 196083712, 0), (10104078336, 5767168, 201850880, 0), (10144448512, 5767168, 207618048, 0), (10150215680, 5767168, 213385216, 0), (10138681344, 5767168, 219152384, 0), (10230956032, 5767168, 224919552, 0), (10236723200, 5767168, 230686720, 0), (10225188864, 5767168, 236453888, 0), (10265559040, 5767168, 242221056, 0), (10271326208, 5767168, 247988224, 0), (10259791872, 5767168, 253755392, 0), (10317463552, 5767168, 259522560, 0), (10323230720, 5767168, 265289728, 0), (10311696384, 5767168, 271056896, 0), (10352066560, 5767168, 276824064, 0), (10357833728, 5767168, 282591232, 0), (10346299392, 5767168, 288358400, 0), (10507780096, 5767168, 294125568, 0), (10513547264, 5767168, 299892736, 0), (10502012928, 5767168, 305659904, 0), (10594287616, 5767168, 311427072, 0), (10600054784, 5767168, 317194240, 0), (10588520448, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b354bed860>, 2: <capsule object NULL at 0x74a814083fc0>}
DEBUG 01-15 16:09:16.021221.021221 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.021925.021925 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.021727.021727 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 22f2c711-1fab-47e7-a312-0b4aa886f3c1
DEBUG 01-15 16:09:16.021179.021179 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.021333.021333 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:16.021332.021332 client.py:127] Model loaded
DEBUG 01-15 16:09:16.021786.021786 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.021925.021925 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.022763.022763 cuda_h.py:19] end move_flatidxs cost 0.0008325576782226562 seconds
DEBUG 01-15 16:09:16.022586.022586 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.022146.022146 cuda_h.py:19] end restore2model cost 0.0009520053863525391 seconds
DEBUG 01-15 16:09:16.022725.022725 cuda_h.py:19] end sllm_worker_task cost 0.012125253677368164 seconds
INFO 01-15 16:09:16.022345.022345 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 22f2c711-1fab-47e7-a312-0b4aa886f3c1
DEBUG 01-15 16:09:16.023529.023529 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0039403438568115234 seconds
DEBUG 01-15 16:09:16.023512.023512 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.026802.026802 cuda_h.py:19] end restore2model cost 0.0030927658081054688 seconds
DEBUG 01-15 16:09:16.026176.026176 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007311820983886719 seconds
DEBUG 01-15 16:09:16.026945.026945 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.027160.027160 cuda_h.py:19] end gpu_sexperts cost 0.0002684593200683594 seconds
DEBUG 01-15 16:09:16.027083.027083 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.027382.027382 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.71661376953125e-05 seconds
DEBUG 01-15 16:09:16.027363.027363 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.027635.027635 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.027200.027200 cuda_h.py:19] end group_tensors cost 0.004664897918701172 seconds
DEBUG 01-15 16:09:16.028945.028945 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.028448.028448 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012731552124023438 seconds
DEBUG 01-15 16:09:16.028864.028864 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.029254.029254 cuda_h.py:19] end gpu_group_list cost 0.0004763603210449219 seconds
DEBUG 01-15 16:09:16.030917.030917 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.032516.032516 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015311241149902344 seconds
DEBUG 01-15 16:09:16.032460.032460 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.032683.032683 cuda_h.py:19] end gpu_group_list cost 0.0003006458282470703 seconds
DEBUG 01-15 16:09:16.033915.033915 cuda_h.py:19] end group pad cost 0.00498509407043457 seconds
DEBUG 01-15 16:09:16.033228.033228 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:16.034830.034830 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.035180.035180 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 22f2c711-1fab-47e7-a312-0b4aa886f3c1
DEBUG 01-15 16:09:16.061654.061654 cuda_h.py:19] end group_einsum cost 0.028624534606933594 seconds
DEBUG 01-15 16:09:16.062156.062156 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:16.064148.064148 client.py:127] Model loaded
DEBUG 01-15 16:09:16.064960.064960 cuda_h.py:19] end wait_experts_multi_device cost 0.029893159866333008 seconds
DEBUG 01-15 16:09:16.064485.064485 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.065018.065018 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029594898223876953 seconds
DEBUG 01-15 16:09:16.066781.066781 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04457426071166992 seconds
DEBUG 01-15 16:09:16.066617.066617 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0018761157989501953 seconds
DEBUG 01-15 16:09:16.067106.067106 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.069387.069387 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.069086.069086 cuda_h.py:19] end index_scatter cost 0.0001537799835205078 seconds
DEBUG 01-15 16:09:16.070434.070434 cuda_h.py:19] end cpuoutputsdeal cost 0.00293731689453125 seconds
DEBUG 01-15 16:09:16.070181.070181 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.070588.070588 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.070117.070117 cuda_h.py:19] end gpu_group_tensor cost 0.00030803680419921875 seconds
DEBUG 01-15 16:09:16.070862.070862 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.071411.071411 cuda_h.py:19] end gpu_group_tensor cost 0.00029015541076660156 seconds
DEBUG 01-15 16:09:16.071909.071909 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.072627.072627 cuda_h.py:19] end gpu_group_einsum cost 0.0011487007141113281 seconds
DEBUG 01-15 16:09:16.072999.072999 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.074232.074232 cuda_h.py:19] end gpu_group_einsum cost 0.0013680458068847656 seconds
DEBUG 01-15 16:09:16.074184.074184 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.074248.074248 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.074455.074455 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021386146545410156 seconds
DEBUG 01-15 16:09:16.074549.074549 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.075314.075314 cuda_h.py:19] end concat_expert_out cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:09:16.075263.075263 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.075075.075075 cuda_h.py:19] end index_scatter cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:09:16.075606.075606 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008902549743652344 seconds
DEBUG 01-15 16:09:16.075981.075981 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.075745.075745 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.075334.075334 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001556873321533203 seconds
DEBUG 01-15 16:09:16.075613.075613 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.075159.075159 cuda_h.py:19] end concat_expert_out cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:09:16.076287.076287 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.076786.076786 cuda_h.py:19] end index_scatter cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:09:16.076119.076119 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000514984130859375 seconds
DEBUG 01-15 16:09:16.076287.076287 cuda_h.py:19] end gpu_experts_multi_device cost 0.04893970489501953 seconds
DEBUG 01-15 16:09:16.076581.076581 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.059477806091308594 seconds
DEBUG 01-15 16:09:16.076658.076658 cuda_h.py:19] end prefill_layer cost 0.06659388542175293 seconds
DEBUG 01-15 16:09:16.076468.076468 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:09:16.076125.076125 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.076066.076066 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:09:16.076107.076107 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:09:16.077008.077008 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:09:16.077574.077574 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 3.743171691894531e-05 seconds
DEBUG 01-15 16:09:16.077137.077137 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 6.914138793945312e-05 seconds
DEBUG 01-15 16:09:16.077310.077310 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.077035.077035 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.077730.077730 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.077163.077163 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.077052.077052 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.078010.078010 cuda_h.py:19] end allocate_cuda_memory cost 0.0004134178161621094 seconds
DEBUG 01-15 16:09:16.078460.078460 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.078014.078014 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.078098.078098 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.078160.078160 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7be21738-a11b-4091-ab33-be1d85dda2d7
DEBUG 01-15 16:09:16.078862.078862 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.079989.079989 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.080075.080075 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7be21738-a11b-4091-ab33-be1d85dda2d7
DEBUG 01-15 16:09:16.080471.080471 cuda_h.py:19] end load_into_gpu_async cost 0.001787424087524414 seconds
DEBUG 01-15 16:09:16.080713.080713 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.080118.080118 cuda_h.py:19] end restore_tensors2 cost 0.00014901161193847656 seconds
DEBUG 01-15 16:09:16.080075.080075 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0031118392944335938 seconds
INFO 01-15 16:09:16.080325.080325 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7be21738-a11b-4091-ab33-be1d85dda2d7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.082365.082365 cuda_h.py:19] end self_attn cost 0.0036475658416748047 seconds
DEBUG 01-15 16:09:16.083740.083740 cuda_h.py:19] end iln_self_attn_paln cost 0.0060787200927734375 seconds
DEBUG 01-15 16:09:16.083854.083854 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:09:16.083041.083041 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.083527.083527 cuda_h.py:19] end gate cost 0.0006330013275146484 seconds
DEBUG 01-15 16:09:16.084310.084310 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.084321.084321 lmp.py:1912] 
DEBUG 01-15 16:09:16.084321.084321 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.084521.084521 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.084363.084363 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.084105.084105 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.084133.084133 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.084729.084729 lmp.py:1917] 
DEBUG 01-15 16:09:16.084729.084729 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.084233.084233 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.084221.084221 lmp.py:1935]   Expert 38 |     13 | CPU
DEBUG 01-15 16:09:16.084294.084294 lmp.py:1935]   Expert 39 |     61 | CPU
DEBUG 01-15 16:09:16.084414.084414 lmp.py:1935]   Expert  7 |     73 | CPU
DEBUG 01-15 16:09:16.084772.084772 lmp.py:1935]   Expert 30 |     74 | CPU
DEBUG 01-15 16:09:16.084415.084415 lmp.py:1935]   Expert 14 |     93 | CPU
DEBUG 01-15 16:09:16.084058.084058 lmp.py:1935]   Expert 24 |     93 | CPU
DEBUG 01-15 16:09:16.084701.084701 lmp.py:1935]   Expert 27 |     94 | CPU
DEBUG 01-15 16:09:16.084821.084821 lmp.py:1935]   Expert 36 |     97 | CPU
DEBUG 01-15 16:09:16.084464.084464 lmp.py:1935]   Expert 40 |     97 | CPU
DEBUG 01-15 16:09:16.084299.084299 lmp.py:1935]   Expert 17 |     98 | CPU
DEBUG 01-15 16:09:16.084134.084134 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:09:16.084777.084777 lmp.py:1935]   Expert 32 |    105 | CPU
DEBUG 01-15 16:09:16.084182.084182 lmp.py:1935]   Expert 18 |    110 | CPU
DEBUG 01-15 16:09:16.084825.084825 lmp.py:1935]   Expert 48 |    113 | CPU
DEBUG 01-15 16:09:16.084229.084229 lmp.py:1935]   Expert 12 |    114 | CPU
DEBUG 01-15 16:09:16.084349.084349 lmp.py:1935]   Expert  1 |    116 | CPU
DEBUG 01-15 16:09:16.084754.084754 lmp.py:1935]   Expert  6 |    126 | CPU
DEBUG 01-15 16:09:16.084635.084635 lmp.py:1935]   Expert 59 |    129 | CPU
DEBUG 01-15 16:09:16.084040.084040 lmp.py:1935]   Expert 42 |    136 | CPU
DEBUG 01-15 16:09:16.084444.084444 lmp.py:1935]   Expert  0 |    141 | CPU
DEBUG 01-15 16:09:16.084372.084372 lmp.py:1935]   Expert 22 |    145 | CPU
DEBUG 01-15 16:09:16.084538.084538 lmp.py:1935]   Expert 53 |    146 | CPU
DEBUG 01-15 16:09:16.084419.084419 lmp.py:1935]   Expert 51 |    150 | CPU
DEBUG 01-15 16:09:16.084062.084062 lmp.py:1935]   Expert  8 |    163 | CPU
DEBUG 01-15 16:09:16.084182.084182 lmp.py:1935]   Expert 44 |    165 | CPU
DEBUG 01-15 16:09:16.084209.084209 lmp.py:1935]   Expert 15 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085760.085760 lmp.py:1935]   Expert 60 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085072.085072 lmp.py:1935]   Expert 29 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085430.085430 lmp.py:1935]   Expert 54 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085788.085788 lmp.py:1935]   Expert 34 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085669.085669 lmp.py:1935]   Expert 35 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085028.085028 lmp.py:1935]   Expert 33 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085386.085386 lmp.py:1935]   Expert 19 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085744.085744 lmp.py:1935]   Expert 47 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085864.085864 lmp.py:1935]   Expert  9 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085222.085222 lmp.py:1935]   Expert 21 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085580.085580 lmp.py:1935]   Expert 56 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085177.085177 lmp.py:1935]   Expert  3 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085297.085297 lmp.py:1935]   Expert 46 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085609.085609 lmp.py:1935]   Expert 45 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085259.085259 lmp.py:1935]   Expert 20 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085286.085286 lmp.py:1935]   Expert 49 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085406.085406 lmp.py:1935]   Expert 28 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085764.085764 lmp.py:1935]   Expert 57 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085122.085122 lmp.py:1935]   Expert  2 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085719.085719 lmp.py:1935]   Expert 13 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085077.085077 lmp.py:1935]   Expert 43 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085674.085674 lmp.py:1935]   Expert  4 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085793.085793 lmp.py:1935]   Expert 10 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085913.085913 lmp.py:1935]   Expert 50 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085271.085271 lmp.py:1935]   Expert 41 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085822.085822 lmp.py:1935]   Expert 26 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085942.085942 lmp.py:1935]   Expert 63 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085300.085300 lmp.py:1935]   Expert 37 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085612.085612 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085970.085970 lmp.py:1935]   Expert 31 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085613.085613 lmp.py:1935]   Expert 52 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085494.085494 lmp.py:1935]   Expert 58 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085137.085137 lmp.py:1935]   Expert 62 |    325 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085495.085495 lmp.py:1935]   Expert 55 |    341 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085615.085615 lmp.py:1935]   Expert 11 |    377 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085735.085735 lmp.py:1935]   Expert 23 |    380 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085616.085616 lmp.py:1935]   Expert 25 |    410 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.085213.085213 lmp.py:1935]   Expert  5 |    516 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.085148.085148 lmp.py:1937] 
DEBUG 01-15 16:09:16.085148.085148 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.085506.085506 lmp.py:1938]   CPU:   2756 tokens
DEBUG 01-15 16:09:16.085295.085295 lmp.py:1942]   cuda:1:   4693 tokens (19 experts)
DEBUG 01-15 16:09:16.085414.085414 lmp.py:1942]   cuda:2:   4839 tokens (20 experts)
DEBUG 01-15 16:09:16.085342.085342 lmp.py:1943]   Total GPU:   9532 tokens
DEBUG 01-15 16:09:16.085508.085508 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.085508.085508 lmp.py:1944] 
DEBUG 01-15 16:09:16.085158.085158 cuda_h.py:19] end experts_map_get cost 0.0017552375793457031 seconds
DEBUG 01-15 16:09:16.085101.085101 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.085664.085664 lmp.py:1953] 
DEBUG 01-15 16:09:16.085664.085664 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.085070.085070 cuda_h.py:19] end cpu_experts_submit cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:16.085859.085859 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.086026.086026 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.087890.087890 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.28.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.28.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.28.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.35.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.35.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.35.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.41.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.41.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.41.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.43.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.43.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.43.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.46.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.46.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.46.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.47.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.47.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.47.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.54.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.54.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.54.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.60.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.60.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.60.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.61.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.61.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.61.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.8.mlp.experts.3.gate_proj.weight': 0, 'model.layers.8.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.13.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.13.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.13.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.8.mlp.experts.58.gate_proj.weight': 328728576, 'model.layers.8.mlp.experts.58.down_proj.weight': 334495744, 'model.layers.8.mlp.experts.58.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (10698096640, 5767168, 17301504, 0), (10703863808, 5767168, 23068672, 0), (10692329472, 5767168, 28835840, 0), (10767302656, 5767168, 34603008, 0), (10773069824, 5767168, 40370176, 0), (10761535488, 5767168, 46137344, 0), (10784604160, 5767168, 51904512, 0), (10790371328, 5767168, 57671680, 0), (10778836992, 5767168, 63438848, 0), (10801905664, 5767168, 69206016, 0), (10807672832, 5767168, 74973184, 0), (10796138496, 5767168, 80740352, 0), (10974920704, 5767168, 86507520, 0), (10980687872, 5767168, 92274688, 0), (10969153536, 5767168, 98041856, 0), (11096031232, 5767168, 103809024, 0), (11101798400, 5767168, 109576192, 0), (11090264064, 5767168, 115343360, 0), (11217141760, 5767168, 121110528, 0), (11222908928, 5767168, 126877696, 0), (11211374592, 5767168, 132644864, 0), (11320950784, 5767168, 138412032, 0), (11326717952, 5767168, 144179200, 0), (11315183616, 5767168, 149946368, 0), (11355553792, 5767168, 155713536, 0), (11361320960, 5767168, 161480704, 0), (11349786624, 5767168, 167247872, 0), (11407458304, 5767168, 173015040, 0), (11413225472, 5767168, 178782208, 0), (11401691136, 5767168, 184549376, 0), (11424759808, 5767168, 190316544, 0), (11430526976, 5767168, 196083712, 0), (11418992640, 5767168, 201850880, 0), (11459362816, 5767168, 207618048, 0), (11465129984, 5767168, 213385216, 0), (11453595648, 5767168, 219152384, 0), (11511267328, 5767168, 224919552, 0), (11517034496, 5767168, 230686720, 0), (11505500160, 5767168, 236453888, 0), (11545870336, 5767168, 242221056, 0), (11551637504, 5767168, 247988224, 0), (11540103168, 5767168, 253755392, 0), (11649679360, 5767168, 259522560, 0), (11655446528, 5767168, 265289728, 0), (11643912192, 5767168, 271056896, 0), (11666980864, 5767168, 276824064, 0), (11672748032, 5767168, 282591232, 0), (11661213696, 5767168, 288358400, 0), (11684282368, 5767168, 294125568, 0), (11690049536, 5767168, 299892736, 0), (11678515200, 5767168, 305659904, 0), (11701583872, 5767168, 311427072, 0), (11707351040, 5767168, 317194240, 0), (11695816704, 5767168, 322961408, 0)], 2: [(10663493632, 5767168, 0, 0), (10669260800, 5767168, 5767168, 0), (10657726464, 5767168, 11534336, 0), (10680795136, 5767168, 17301504, 0), (10686562304, 5767168, 23068672, 0), (10675027968, 5767168, 28835840, 0), (10836508672, 5767168, 34603008, 0), (10842275840, 5767168, 40370176, 0), (10830741504, 5767168, 46137344, 0), (10871111680, 5767168, 51904512, 0), (10876878848, 5767168, 57671680, 0), (10865344512, 5767168, 63438848, 0), (10940317696, 5767168, 69206016, 0), (10946084864, 5767168, 74973184, 0), (10934550528, 5767168, 80740352, 0), (10957619200, 5767168, 86507520, 0), (10963386368, 5767168, 92274688, 0), (10951852032, 5767168, 98041856, 0), (11009523712, 5767168, 103809024, 0), (11015290880, 5767168, 109576192, 0), (11003756544, 5767168, 115343360, 0), (11044126720, 5767168, 121110528, 0), (11049893888, 5767168, 126877696, 0), (11038359552, 5767168, 132644864, 0), (11061428224, 5767168, 138412032, 0), (11067195392, 5767168, 144179200, 0), (11055661056, 5767168, 149946368, 0), (11113332736, 5767168, 155713536, 0), (11119099904, 5767168, 161480704, 0), (11107565568, 5767168, 167247872, 0), (11147935744, 5767168, 173015040, 0), (11153702912, 5767168, 178782208, 0), (11142168576, 5767168, 184549376, 0), (11182538752, 5767168, 190316544, 0), (11188305920, 5767168, 196083712, 0), (11176771584, 5767168, 201850880, 0), (11199840256, 5767168, 207618048, 0), (11205607424, 5767168, 213385216, 0), (11194073088, 5767168, 219152384, 0), (11251744768, 5767168, 224919552, 0), (11257511936, 5767168, 230686720, 0), (11245977600, 5767168, 236453888, 0), (11390156800, 5767168, 242221056, 0), (11395923968, 5767168, 247988224, 0), (11384389632, 5767168, 253755392, 0), (11476664320, 5767168, 259522560, 0), (11482431488, 5767168, 265289728, 0), (11470897152, 5767168, 271056896, 0), (11563171840, 5767168, 276824064, 0), (11568939008, 5767168, 282591232, 0), (11557404672, 5767168, 288358400, 0), (11580473344, 5767168, 294125568, 0), (11586240512, 5767168, 299892736, 0), (11574706176, 5767168, 305659904, 0), (11597774848, 5767168, 311427072, 0), (11603542016, 5767168, 317194240, 0), (11592007680, 5767168, 322961408, 0), (11615076352, 5767168, 328728576, 0), (11620843520, 5767168, 334495744, 0), (11609309184, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a680611fe0>, 2: <capsule object NULL at 0x74a6785de220>}
DEBUG 01-15 16:09:16.087153.087153 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.087327.087327 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.087652.087652 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, fa2fd59e-9c3c-4191-8948-62c678f37188
DEBUG 01-15 16:09:16.087785.087785 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:16.088074.088074 client.py:127] Model loaded
DEBUG 01-15 16:09:16.088760.088760 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.088172.088172 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:16.089796.089796 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.089208.089208 cuda_h.py:19] end restore2model cost 0.001013040542602539 seconds
INFO 01-15 16:09:16.089954.089954 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, fa2fd59e-9c3c-4191-8948-62c678f37188
DEBUG 01-15 16:09:16.090728.090728 cuda_h.py:19] end move_flatidxs cost 0.0008630752563476562 seconds
DEBUG 01-15 16:09:16.090564.090564 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.089229.089229 cuda_h.py:19] end sllm_worker_task cost 0.012454509735107422 seconds
DEBUG 01-15 16:09:16.090260.090260 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004387378692626953 seconds
DEBUG 01-15 16:09:16.090292.090292 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.093292.093292 cuda_h.py:19] end restore2model cost 0.0031239986419677734 seconds
DEBUG 01-15 16:09:16.093943.093943 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00783538818359375 seconds
DEBUG 01-15 16:09:16.093805.093805 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.094391.094391 cuda_h.py:19] end gpu_sexperts cost 0.00026154518127441406 seconds
DEBUG 01-15 16:09:16.094790.094790 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.094944.094944 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:09:16.094925.094925 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.094913.094913 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.095998.095998 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015401840209960938 seconds
DEBUG 01-15 16:09:16.095961.095961 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.096658.096658 cuda_h.py:19] end gpu_group_list cost 0.00020360946655273438 seconds
DEBUG 01-15 16:09:16.097593.097593 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.098318.098318 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010356903076171875 seconds
DEBUG 01-15 16:09:16.098599.098599 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.098262.098262 cuda_h.py:19] end gpu_group_list cost 0.000213623046875 seconds
DEBUG 01-15 16:09:16.099193.099193 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.099214.099214 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, fa2fd59e-9c3c-4191-8948-62c678f37188
DEBUG 01-15 16:09:16.100293.100293 cuda_h.py:19] end group_tensors cost 0.009871244430541992 seconds
DEBUG 01-15 16:09:16.100854.100854 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.103464.103464 cuda_h.py:19] end group pad cost 0.0029287338256835938 seconds
DEBUG 01-15 16:09:16.103631.103631 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:16.128904.128904 client.py:127] Model loaded
DEBUG 01-15 16:09:16.128631.128631 cuda_h.py:19] end wait_experts_multi_device cost 0.029048919677734375 seconds
DEBUG 01-15 16:09:16.128440.128440 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.133280.133280 cuda_h.py:19] end group_einsum cost 0.029982566833496094 seconds
DEBUG 01-15 16:09:16.134994.134994 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.136547.136547 cuda_h.py:19] end get_outputs_cpu1 cost 0.0028085708618164062 seconds
DEBUG 01-15 16:09:16.137622.137622 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04895305633544922 seconds
DEBUG 01-15 16:09:16.138980.138980 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01051950454711914 seconds
DEBUG 01-15 16:09:16.138944.138944 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.139017.139017 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.140420.140420 cuda_h.py:19] end index_scatter cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:09:16.140920.140920 cuda_h.py:19] end cpuoutputsdeal cost 0.0016248226165771484 seconds
DEBUG 01-15 16:09:16.140135.140135 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.140375.140375 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.140527.140527 cuda_h.py:19] end gpu_group_tensor cost 0.00014972686767578125 seconds
DEBUG 01-15 16:09:16.140574.140574 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.141321.141321 cuda_h.py:19] end gpu_group_tensor cost 0.0001347064971923828 seconds
DEBUG 01-15 16:09:16.141841.141841 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.141009.141009 cuda_h.py:19] end gpu_group_einsum cost 0.0005967617034912109 seconds
DEBUG 01-15 16:09:16.142067.142067 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.142216.142216 cuda_h.py:19] end gpu_group_einsum cost 0.0004367828369140625 seconds
DEBUG 01-15 16:09:16.142743.142743 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.142686.142686 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.142476.142476 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019407272338867188 seconds
DEBUG 01-15 16:09:16.143947.143947 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.143194.143194 cuda_h.py:19] end concat_expert_out cost 4.696846008300781e-05 seconds
DEBUG 01-15 16:09:16.143674.143674 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.143365.143365 cuda_h.py:19] end index_scatter cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:16.143546.143546 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007908344268798828 seconds
DEBUG 01-15 16:09:16.143244.143244 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.143710.143710 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.143669.143669 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015091896057128906 seconds
DEBUG 01-15 16:09:16.143757.143757 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.143534.143534 cuda_h.py:19] end concat_expert_out cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:09:16.143993.143993 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.144632.144632 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:16.144772.144772 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004937648773193359 seconds
DEBUG 01-15 16:09:16.144774.144774 cuda_h.py:19] end gpu_experts_multi_device cost 0.049866676330566406 seconds
DEBUG 01-15 16:09:16.144638.144638 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.06094217300415039 seconds
DEBUG 01-15 16:09:16.144395.144395 cuda_h.py:19] end prefill_layer cost 0.06775951385498047 seconds
DEBUG 01-15 16:09:16.144297.144297 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:09:16.144000.144000 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.144372.144372 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:09:16.144982.144982 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:09:16.144738.144738 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:09:16.144542.144542 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:09:16.144675.144675 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:09:16.145133.145133 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.145341.145341 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.145705.145705 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.145946.145946 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.145384.145384 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.146045.146045 cuda_h.py:19] end allocate_cuda_memory cost 0.00044083595275878906 seconds
DEBUG 01-15 16:09:16.146097.146097 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.146756.146756 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.146595.146595 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.146101.146101 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6bacacc6-4237-4e6b-9044-f964d86d20f4
DEBUG 01-15 16:09:16.146956.146956 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.147892.147892 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.148264.148264 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6bacacc6-4237-4e6b-9044-f964d86d20f4
DEBUG 01-15 16:09:16.148164.148164 cuda_h.py:19] end load_into_gpu_async cost 0.002088308334350586 seconds
DEBUG 01-15 16:09:16.148617.148617 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.148499.148499 cuda_h.py:19] end restore_tensors2 cost 0.00014829635620117188 seconds
DEBUG 01-15 16:09:16.148317.148317 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003397226333618164 seconds
INFO 01-15 16:09:16.149442.149442 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6bacacc6-4237-4e6b-9044-f964d86d20f4
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.151652.151652 cuda_h.py:19] end self_attn cost 0.0037984848022460938 seconds
DEBUG 01-15 16:09:16.151761.151761 cuda_h.py:19] end iln_self_attn_paln cost 0.0062596797943115234 seconds
DEBUG 01-15 16:09:16.151643.151643 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:09:16.151559.151559 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.152753.152753 cuda_h.py:19] end gate cost 0.0006344318389892578 seconds
DEBUG 01-15 16:09:16.152868.152868 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.152753.152753 lmp.py:1912] 
DEBUG 01-15 16:09:16.152753.152753 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.152893.152893 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.152305.152305 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.152332.152332 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.152836.152836 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.152002.152002 lmp.py:1917] 
DEBUG 01-15 16:09:16.152002.152002 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.152075.152075 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.152917.152917 lmp.py:1935]   Expert 24 |     38 | CPU
DEBUG 01-15 16:09:16.152560.152560 lmp.py:1935]   Expert  2 |     47 | CPU
DEBUG 01-15 16:09:16.152395.152395 lmp.py:1935]   Expert 26 |     62 | CPU
DEBUG 01-15 16:09:16.152992.152992 lmp.py:1935]   Expert 32 |     65 | CPU
DEBUG 01-15 16:09:16.152635.152635 lmp.py:1935]   Expert 19 |     69 | CPU
DEBUG 01-15 16:09:16.152470.152470 lmp.py:1935]   Expert 50 |     70 | CPU
DEBUG 01-15 16:09:16.152067.152067 lmp.py:1935]   Expert 15 |     78 | CPU
DEBUG 01-15 16:09:16.152140.152140 lmp.py:1935]   Expert 60 |     81 | CPU
DEBUG 01-15 16:09:16.152214.152214 lmp.py:1935]   Expert  4 |     82 | CPU
DEBUG 01-15 16:09:16.152333.152333 lmp.py:1935]   Expert  7 |     82 | CPU
DEBUG 01-15 16:09:16.152500.152500 lmp.py:1935]   Expert 28 |     82 | CPU
DEBUG 01-15 16:09:16.152143.152143 lmp.py:1935]   Expert 59 |     90 | CPU
DEBUG 01-15 16:09:16.152547.152547 lmp.py:1935]   Expert 49 |     96 | CPU
DEBUG 01-15 16:09:16.152952.152952 lmp.py:1935]   Expert 23 |     97 | CPU
DEBUG 01-15 16:09:16.152118.152118 lmp.py:1935]   Expert  5 |    101 | CPU
DEBUG 01-15 16:09:16.152476.152476 lmp.py:1935]   Expert 12 |    105 | CPU
DEBUG 01-15 16:09:16.152834.152834 lmp.py:1935]   Expert 10 |    112 | CPU
DEBUG 01-15 16:09:16.152054.152054 lmp.py:1935]   Expert 27 |    112 | CPU
DEBUG 01-15 16:09:16.152412.152412 lmp.py:1935]   Expert 41 |    121 | CPU
DEBUG 01-15 16:09:16.152055.152055 lmp.py:1935]   Expert  3 |    126 | CPU
DEBUG 01-15 16:09:16.152221.152221 lmp.py:1935]   Expert 40 |    128 | CPU
DEBUG 01-15 16:09:16.152625.152625 lmp.py:1935]   Expert 25 |    129 | CPU
DEBUG 01-15 16:09:16.153792.153792 lmp.py:1935]   Expert 20 |    130 | CPU
DEBUG 01-15 16:09:16.153196.153196 lmp.py:1935]   Expert 16 |    132 | CPU
DEBUG 01-15 16:09:16.153362.153362 lmp.py:1935]   Expert 13 |    133 | CPU
DEBUG 01-15 16:09:16.153436.153436 lmp.py:1935]   Expert 37 |    144 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153271.153271 lmp.py:1935]   Expert 35 |    146 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153629.153629 lmp.py:1935]   Expert 17 |    147 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153987.153987 lmp.py:1935]   Expert 47 |    156 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153107.153107 lmp.py:1935]   Expert 22 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153988.153988 lmp.py:1935]   Expert 53 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153585.153585 lmp.py:1935]   Expert 39 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153135.153135 lmp.py:1935]   Expert 38 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153924.153924 lmp.py:1935]   Expert 36 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153475.153475 lmp.py:1935]   Expert 44 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153310.153310 lmp.py:1935]   Expert 52 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153429.153429 lmp.py:1935]   Expert 58 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153788.153788 lmp.py:1935]   Expert 18 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153384.153384 lmp.py:1935]   Expert 62 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153266.153266 lmp.py:1935]   Expert 11 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153909.153909 lmp.py:1935]   Expert 48 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153505.153505 lmp.py:1935]   Expert 14 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153625.153625 lmp.py:1935]   Expert 30 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153460.153460 lmp.py:1935]   Expert  1 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153209.153209 lmp.py:1935]   Expert 45 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153091.153091 lmp.py:1935]   Expert 42 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153018.153018 lmp.py:1935]   Expert 31 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153946.153946 lmp.py:1935]   Expert  6 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153636.153636 lmp.py:1935]   Expert 51 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153325.153325 lmp.py:1935]   Expert 29 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153776.153776 lmp.py:1935]   Expert 34 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153988.153988 lmp.py:1935]   Expert 33 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153439.153439 lmp.py:1935]   Expert 57 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153890.153890 lmp.py:1935]   Expert 61 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153354.153354 lmp.py:1935]   Expert 43 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153567.153567 lmp.py:1935]   Expert  0 |    323 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153541.153541 lmp.py:1935]   Expert 46 |    351 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153277.153277 lmp.py:1935]   Expert  8 |    380 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153489.153489 lmp.py:1935]   Expert 56 |    391 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153463.153463 lmp.py:1935]   Expert  9 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153437.153437 lmp.py:1935]   Expert 54 |    397 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153888.153888 lmp.py:1935]   Expert 63 |    411 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153293.153293 lmp.py:1935]   Expert 55 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.153697.153697 lmp.py:1935]   Expert 21 |    493 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.153910.153910 lmp.py:1937] 
DEBUG 01-15 16:09:16.153910.153910 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.153360.153360 lmp.py:1938]   CPU:   2368 tokens
DEBUG 01-15 16:09:16.153811.153811 lmp.py:1942]   cuda:1:   5032 tokens (20 experts)
DEBUG 01-15 16:09:16.153024.153024 lmp.py:1942]   cuda:2:   4888 tokens (19 experts)
DEBUG 01-15 16:09:16.153521.153521 lmp.py:1943]   Total GPU:   9920 tokens
DEBUG 01-15 16:09:16.153780.153780 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.153780.153780 lmp.py:1944] 
DEBUG 01-15 16:09:16.153522.153522 cuda_h.py:19] end experts_map_get cost 0.0017189979553222656 seconds
DEBUG 01-15 16:09:16.153319.153319 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.153929.153929 lmp.py:1953] 
DEBUG 01-15 16:09:16.153929.153929 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.154143.154143 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:16.154455.154455 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.154284.154284 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.155089.155089 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.1.gate_proj.weight': 0, 'model.layers.9.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.17.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.17.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.17.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.33.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.33.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.33.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.37.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.37.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.37.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.38.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.38.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.38.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.39.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.39.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.39.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.42.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.42.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.42.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.43.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.43.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.43.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.44.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.44.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.44.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.46.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.46.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.46.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.9.mlp.experts.58.gate_proj.weight': 328728576, 'model.layers.9.mlp.experts.58.down_proj.weight': 334495744, 'model.layers.9.mlp.experts.58.up_proj.weight': 340262912}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.30.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.30.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.30.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.61.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.61.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.61.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(11736186880, 5767168, 0, 0), (11741954048, 5767168, 5767168, 0), (11730419712, 5767168, 11534336, 0), (11822694400, 5767168, 17301504, 0), (11828461568, 5767168, 23068672, 0), (11816927232, 5767168, 28835840, 0), (11909201920, 5767168, 34603008, 0), (11914969088, 5767168, 40370176, 0), (11903434752, 5767168, 46137344, 0), (12013010944, 5767168, 51904512, 0), (12018778112, 5767168, 57671680, 0), (12007243776, 5767168, 63438848, 0), (12082216960, 5767168, 69206016, 0), (12087984128, 5767168, 74973184, 0), (12076449792, 5767168, 80740352, 0), (12289835008, 5767168, 86507520, 0), (12295602176, 5767168, 92274688, 0), (12284067840, 5767168, 98041856, 0), (12359041024, 5767168, 103809024, 0), (12364808192, 5767168, 109576192, 0), (12353273856, 5767168, 115343360, 0), (12376342528, 5767168, 121110528, 0), (12382109696, 5767168, 126877696, 0), (12370575360, 5767168, 132644864, 0), (12393644032, 5767168, 138412032, 0), (12399411200, 5767168, 144179200, 0), (12387876864, 5767168, 149946368, 0), (12445548544, 5767168, 155713536, 0), (12451315712, 5767168, 161480704, 0), (12439781376, 5767168, 167247872, 0), (12462850048, 5767168, 173015040, 0), (12468617216, 5767168, 178782208, 0), (12457082880, 5767168, 184549376, 0), (12480151552, 5767168, 190316544, 0), (12485918720, 5767168, 196083712, 0), (12474384384, 5767168, 201850880, 0), (12514754560, 5767168, 207618048, 0), (12520521728, 5767168, 213385216, 0), (12508987392, 5767168, 219152384, 0), (12532056064, 5767168, 224919552, 0), (12537823232, 5767168, 230686720, 0), (12526288896, 5767168, 236453888, 0), (12549357568, 5767168, 242221056, 0), (12555124736, 5767168, 247988224, 0), (12543590400, 5767168, 253755392, 0), (12601262080, 5767168, 259522560, 0), (12607029248, 5767168, 265289728, 0), (12595494912, 5767168, 271056896, 0), (12653166592, 5767168, 276824064, 0), (12658933760, 5767168, 282591232, 0), (12647399424, 5767168, 288358400, 0), (12687769600, 5767168, 294125568, 0), (12693536768, 5767168, 299892736, 0), (12682002432, 5767168, 305659904, 0), (12705071104, 5767168, 311427072, 0), (12710838272, 5767168, 317194240, 0), (12699303936, 5767168, 322961408, 0), (12722372608, 5767168, 328728576, 0), (12728139776, 5767168, 334495744, 0), (12716605440, 5767168, 340262912, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (11857297408, 5767168, 17301504, 0), (11863064576, 5767168, 23068672, 0), (11851530240, 5767168, 28835840, 0), (11874598912, 5767168, 34603008, 0), (11880366080, 5767168, 40370176, 0), (11868831744, 5767168, 46137344, 0), (11961106432, 5767168, 51904512, 0), (11966873600, 5767168, 57671680, 0), (11955339264, 5767168, 63438848, 0), (12030312448, 5767168, 69206016, 0), (12036079616, 5767168, 74973184, 0), (12024545280, 5767168, 80740352, 0), (12099518464, 5767168, 86507520, 0), (12105285632, 5767168, 92274688, 0), (12093751296, 5767168, 98041856, 0), (12220628992, 5767168, 103809024, 0), (12226396160, 5767168, 109576192, 0), (12214861824, 5767168, 115343360, 0), (12237930496, 5767168, 121110528, 0), (12243697664, 5767168, 126877696, 0), (12232163328, 5767168, 132644864, 0), (12255232000, 5767168, 138412032, 0), (12260999168, 5767168, 144179200, 0), (12249464832, 5767168, 149946368, 0), (12307136512, 5767168, 155713536, 0), (12312903680, 5767168, 161480704, 0), (12301369344, 5767168, 167247872, 0), (12324438016, 5767168, 173015040, 0), (12330205184, 5767168, 178782208, 0), (12318670848, 5767168, 184549376, 0), (12341739520, 5767168, 190316544, 0), (12347506688, 5767168, 196083712, 0), (12335972352, 5767168, 201850880, 0), (12497453056, 5767168, 207618048, 0), (12503220224, 5767168, 213385216, 0), (12491685888, 5767168, 219152384, 0), (12618563584, 5767168, 224919552, 0), (12624330752, 5767168, 230686720, 0), (12612796416, 5767168, 236453888, 0), (12635865088, 5767168, 242221056, 0), (12641632256, 5767168, 247988224, 0), (12630097920, 5767168, 253755392, 0), (12670468096, 5767168, 259522560, 0), (12676235264, 5767168, 265289728, 0), (12664700928, 5767168, 271056896, 0), (12774277120, 5767168, 276824064, 0), (12780044288, 5767168, 282591232, 0), (12768509952, 5767168, 288358400, 0), (12791578624, 5767168, 294125568, 0), (12797345792, 5767168, 299892736, 0), (12785811456, 5767168, 305659904, 0), (12808880128, 5767168, 311427072, 0), (12814647296, 5767168, 317194240, 0), (12803112960, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc703ed0>, 2: <capsule object NULL at 0x74a6785de190>}
DEBUG 01-15 16:09:16.156546.156546 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.156535.156535 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.156099.156099 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7e1785c9-f761-4bce-8e18-b37665012b37
DEBUG 01-15 16:09:16.156682.156682 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.156016.156016 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:16.156801.156801 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:16.156735.156735 client.py:127] Model loaded
DEBUG 01-15 16:09:16.156560.156560 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.157155.157155 cuda_h.py:19] end move_flatidxs cost 0.0008356571197509766 seconds
DEBUG 01-15 16:09:16.157216.157216 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.158425.158425 cuda_h.py:19] end restore2model cost 0.0010035037994384766 seconds
INFO 01-15 16:09:16.158125.158125 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7e1785c9-f761-4bce-8e18-b37665012b37
DEBUG 01-15 16:09:16.158969.158969 cuda_h.py:19] end sllm_worker_task cost 0.012949705123901367 seconds
DEBUG 01-15 16:09:16.158524.158524 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00474095344543457 seconds
DEBUG 01-15 16:09:16.159741.159741 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.162195.162195 cuda_h.py:19] end restore2model cost 0.003038644790649414 seconds
DEBUG 01-15 16:09:16.162575.162575 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008098125457763672 seconds
DEBUG 01-15 16:09:16.162675.162675 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.162361.162361 cuda_h.py:19] end gpu_sexperts cost 0.00026535987854003906 seconds
DEBUG 01-15 16:09:16.162567.162567 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.162337.162337 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4066696166992188e-05 seconds
DEBUG 01-15 16:09:16.162841.162841 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.162352.162352 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.163597.163597 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009589195251464844 seconds
DEBUG 01-15 16:09:16.163155.163155 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.163236.163236 cuda_h.py:19] end gpu_group_list cost 0.00020623207092285156 seconds
DEBUG 01-15 16:09:16.163158.163158 cuda_h.py:19] end group_tensors cost 0.005746126174926758 seconds
DEBUG 01-15 16:09:16.164770.164770 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.164335.164335 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.166476.166476 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001348733901977539 seconds
DEBUG 01-15 16:09:16.166454.166454 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.166452.166452 cuda_h.py:19] end gpu_group_list cost 0.0003077983856201172 seconds
DEBUG 01-15 16:09:16.167061.167061 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.167672.167672 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7e1785c9-f761-4bce-8e18-b37665012b37
DEBUG 01-15 16:09:16.168578.168578 cuda_h.py:19] end group pad cost 0.0040776729583740234 seconds
DEBUG 01-15 16:09:16.168507.168507 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:16.184318.184318 cuda_h.py:19] end group_einsum cost 0.016032934188842773 seconds
DEBUG 01-15 16:09:16.184475.184475 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.188609.188609 cuda_h.py:19] end get_outputs_cpu1 cost 0.0031328201293945312 seconds
DEBUG 01-15 16:09:16.188916.188916 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03233766555786133 seconds
INFO 01-15 16:09:16.191625.191625 client.py:127] Model loaded
DEBUG 01-15 16:09:16.191908.191908 cuda_h.py:19] end wait_experts_multi_device cost 0.024033069610595703 seconds
DEBUG 01-15 16:09:16.191386.191386 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.192122.192122 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005755424499511719 seconds
DEBUG 01-15 16:09:16.192588.192588 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.193692.193692 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.193883.193883 cuda_h.py:19] end index_scatter cost 7.2479248046875e-05 seconds
DEBUG 01-15 16:09:16.193899.193899 cuda_h.py:19] end cpuoutputsdeal cost 0.001352071762084961 seconds
DEBUG 01-15 16:09:16.194147.194147 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.194048.194048 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.194478.194478 cuda_h.py:19] end gpu_group_tensor cost 0.00014591217041015625 seconds
DEBUG 01-15 16:09:16.194810.194810 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.194220.194220 cuda_h.py:19] end gpu_group_tensor cost 0.00013303756713867188 seconds
DEBUG 01-15 16:09:16.194594.194594 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.195857.195857 cuda_h.py:19] end gpu_group_einsum cost 0.0006690025329589844 seconds
DEBUG 01-15 16:09:16.195491.195491 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.195143.195143 cuda_h.py:19] end gpu_group_einsum cost 0.00042366981506347656 seconds
DEBUG 01-15 16:09:16.196762.196762 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.196421.196421 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.196528.196528 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018262863159179688 seconds
DEBUG 01-15 16:09:16.196880.196880 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.196088.196088 cuda_h.py:19] end concat_expert_out cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:09:16.196362.196362 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.196193.196193 cuda_h.py:19] end index_scatter cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:09:16.196850.196850 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000789642333984375 seconds
DEBUG 01-15 16:09:16.196025.196025 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.197875.197875 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.197656.197656 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015926361083984375 seconds
DEBUG 01-15 16:09:16.197220.197220 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.197805.197805 cuda_h.py:19] end concat_expert_out cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:16.197503.197503 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.197472.197472 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:16.197328.197328 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005028247833251953 seconds
DEBUG 01-15 16:09:16.197377.197377 cuda_h.py:19] end gpu_experts_multi_device cost 0.034958839416503906 seconds
DEBUG 01-15 16:09:16.197525.197525 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.04626655578613281 seconds
DEBUG 01-15 16:09:16.198426.198426 cuda_h.py:19] end prefill_layer cost 0.05323171615600586 seconds
DEBUG 01-15 16:09:16.198998.198998 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:09:16.198178.198178 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.198834.198834 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:09:16.198683.198683 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:09:16.198916.198916 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:09:16.198812.198812 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 3.62396240234375e-05 seconds
DEBUG 01-15 16:09:16.198283.198283 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 7.009506225585938e-05 seconds
DEBUG 01-15 16:09:16.198887.198887 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.198380.198380 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.198329.198329 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.198313.198313 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.198156.198156 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.198735.198735 cuda_h.py:19] end allocate_cuda_memory cost 0.00024175643920898438 seconds
DEBUG 01-15 16:09:16.199705.199705 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.199283.199283 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.199682.199682 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.199961.199961 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 940ea669-df6a-4c2e-8f1a-e2ccb74c6922
DEBUG 01-15 16:09:16.199434.199434 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.199881.199881 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.200290.200290 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 940ea669-df6a-4c2e-8f1a-e2ccb74c6922
DEBUG 01-15 16:09:16.200418.200418 cuda_h.py:19] end load_into_gpu_async cost 0.0010840892791748047 seconds
DEBUG 01-15 16:09:16.200790.200790 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.200708.200708 cuda_h.py:19] end restore_tensors2 cost 8.440017700195312e-05 seconds
DEBUG 01-15 16:09:16.200153.200153 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0016949176788330078 seconds
INFO 01-15 16:09:16.200679.200679 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 940ea669-df6a-4c2e-8f1a-e2ccb74c6922
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.202412.202412 cuda_h.py:19] end self_attn cost 0.003131389617919922 seconds
DEBUG 01-15 16:09:16.203575.203575 cuda_h.py:19] end iln_self_attn_paln cost 0.0046863555908203125 seconds
DEBUG 01-15 16:09:16.203305.203305 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:09:16.203061.203061 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.203328.203328 cuda_h.py:19] end gate cost 0.0006206035614013672 seconds
DEBUG 01-15 16:09:16.203395.203395 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.204009.204009 lmp.py:1912] 
DEBUG 01-15 16:09:16.204009.204009 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.204526.204526 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.204130.204130 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.204965.204965 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.204892.204892 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.204105.204105 lmp.py:1917] 
DEBUG 01-15 16:09:16.204105.204105 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.204794.204794 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.204205.204205 lmp.py:1935]   Expert 43 |     17 | CPU
DEBUG 01-15 16:09:16.204372.204372 lmp.py:1935]   Expert 27 |     31 | CPU
DEBUG 01-15 16:09:16.204822.204822 lmp.py:1935]   Expert 26 |     52 | CPU
DEBUG 01-15 16:09:16.204333.204333 lmp.py:1935]   Expert 34 |     54 | CPU
DEBUG 01-15 16:09:16.204791.204791 lmp.py:1935]   Expert 56 |     56 | CPU
DEBUG 01-15 16:09:16.204957.204957 lmp.py:1935]   Expert  3 |     59 | CPU
DEBUG 01-15 16:09:16.204931.204931 lmp.py:1935]   Expert  4 |     70 | CPU
DEBUG 01-15 16:09:16.204667.204667 lmp.py:1935]   Expert 61 |     82 | CPU
DEBUG 01-15 16:09:16.204402.204402 lmp.py:1935]   Expert 14 |     95 | CPU
DEBUG 01-15 16:09:16.204900.204900 lmp.py:1935]   Expert 38 |    100 | CPU
DEBUG 01-15 16:09:16.204635.204635 lmp.py:1935]   Expert  2 |    110 | CPU
DEBUG 01-15 16:09:16.204609.204609 lmp.py:1935]   Expert 22 |    121 | CPU
DEBUG 01-15 16:09:16.204106.204106 lmp.py:1935]   Expert 17 |    122 | CPU
DEBUG 01-15 16:09:16.204842.204842 lmp.py:1935]   Expert 47 |    127 | CPU
DEBUG 01-15 16:09:16.204339.204339 lmp.py:1935]   Expert 55 |    129 | CPU
DEBUG 01-15 16:09:16.204075.204075 lmp.py:1935]   Expert 37 |    130 | CPU
DEBUG 01-15 16:09:16.204811.204811 lmp.py:1935]   Expert 28 |    135 | CPU
DEBUG 01-15 16:09:16.204500.204500 lmp.py:1935]   Expert 54 |    136 | CPU
DEBUG 01-15 16:09:16.204335.204335 lmp.py:1935]   Expert  7 |    145 | CPU
DEBUG 01-15 16:09:16.204746.204746 lmp.py:1935]   Expert 45 |    145 | CPU
DEBUG 01-15 16:09:16.204912.204912 lmp.py:1935]   Expert 15 |    146 | CPU
DEBUG 01-15 16:09:16.204648.204648 lmp.py:1935]   Expert 48 |    146 | CPU
DEBUG 01-15 16:09:16.204384.204384 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:09:16.204642.204642 lmp.py:1935]   Expert 51 |    149 | CPU
DEBUG 01-15 16:09:16.204378.204378 lmp.py:1935]   Expert 60 |    149 | CPU
DEBUG 01-15 16:09:16.204021.204021 lmp.py:1935]   Expert 12 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.204426.204426 lmp.py:1935]   Expert 63 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.204353.204353 lmp.py:1935]   Expert 19 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.204804.204804 lmp.py:1935]   Expert  6 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.204732.204732 lmp.py:1935]   Expert 57 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.204421.204421 lmp.py:1935]   Expert 52 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.204972.204972 lmp.py:1935]   Expert 44 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.204568.204568 lmp.py:1935]   Expert 18 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.204642.204642 lmp.py:1935]   Expert 50 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.204761.204761 lmp.py:1935]   Expert 31 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.204928.204928 lmp.py:1935]   Expert 13 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.204094.204094 lmp.py:1935]   Expert 30 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.204260.204260 lmp.py:1935]   Expert 23 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.204426.204426 lmp.py:1935]   Expert 59 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205354.205354 lmp.py:1935]   Expert 39 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205520.205520 lmp.py:1935]   Expert 53 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205686.205686 lmp.py:1935]   Expert 20 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205852.205852 lmp.py:1935]   Expert 21 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205025.205025 lmp.py:1935]   Expert 29 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205906.205906 lmp.py:1935]   Expert 16 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205596.205596 lmp.py:1935]   Expert 36 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205808.205808 lmp.py:1935]   Expert 25 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205021.205021 lmp.py:1935]   Expert 41 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205948.205948 lmp.py:1935]   Expert 32 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205399.205399 lmp.py:1935]   Expert 49 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205612.205612 lmp.py:1935]   Expert 46 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205063.205063 lmp.py:1935]   Expert  8 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205752.205752 lmp.py:1935]   Expert 10 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205726.205726 lmp.py:1935]   Expert 42 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205938.205938 lmp.py:1935]   Expert 62 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205389.205389 lmp.py:1935]   Expert 35 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205840.205840 lmp.py:1935]   Expert  9 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205576.205576 lmp.py:1935]   Expert 33 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205027.205027 lmp.py:1935]   Expert 58 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205001.205001 lmp.py:1935]   Expert 40 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205121.205121 lmp.py:1935]   Expert 11 |    422 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205479.205479 lmp.py:1935]   Expert  0 |    429 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205122.205122 lmp.py:1935]   Expert 24 |    562 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.205765.205765 lmp.py:1935]   Expert  1 |    645 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.205739.205739 lmp.py:1937] 
DEBUG 01-15 16:09:16.205739.205739 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.205474.205474 lmp.py:1938]   CPU:   2653 tokens
DEBUG 01-15 16:09:16.205402.205402 lmp.py:1942]   cuda:1:   4893 tokens (20 experts)
DEBUG 01-15 16:09:16.205615.205615 lmp.py:1942]   cuda:2:   4742 tokens (19 experts)
DEBUG 01-15 16:09:16.205112.205112 lmp.py:1943]   Total GPU:   9635 tokens
DEBUG 01-15 16:09:16.205609.205609 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.205609.205609 lmp.py:1944] 
DEBUG 01-15 16:09:16.205067.205067 cuda_h.py:19] end experts_map_get cost 0.0016255378723144531 seconds
DEBUG 01-15 16:09:16.205433.205433 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.205282.205282 lmp.py:1953] 
DEBUG 01-15 16:09:16.205282.205282 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.205210.205210 cuda_h.py:19] end cpu_experts_submit cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:09:16.205046.205046 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.205067.205067 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.207618.207618 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.1.gate_proj.weight': 0, 'model.layers.10.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.59.up_proj.weight': 322961408, 'model.layers.10.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.10.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.10.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.9.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.9.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.9.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.46.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.46.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.46.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(12843483136, 5767168, 0, 0), (12849250304, 5767168, 5767168, 0), (12837715968, 5767168, 11534336, 0), (12929990656, 5767168, 17301504, 0), (12935757824, 5767168, 23068672, 0), (12924223488, 5767168, 28835840, 0), (12964593664, 5767168, 34603008, 0), (12970360832, 5767168, 40370176, 0), (12958826496, 5767168, 46137344, 0), (13016498176, 5767168, 51904512, 0), (13022265344, 5767168, 57671680, 0), (13010731008, 5767168, 63438848, 0), (13051101184, 5767168, 69206016, 0), (13056868352, 5767168, 74973184, 0), (13045334016, 5767168, 80740352, 0), (13103005696, 5767168, 86507520, 0), (13108772864, 5767168, 92274688, 0), (13097238528, 5767168, 98041856, 0), (13154910208, 5767168, 103809024, 0), (13160677376, 5767168, 109576192, 0), (13149143040, 5767168, 115343360, 0), (13189513216, 5767168, 121110528, 0), (13195280384, 5767168, 126877696, 0), (13183746048, 5767168, 132644864, 0), (13258719232, 5767168, 138412032, 0), (13264486400, 5767168, 144179200, 0), (13252952064, 5767168, 149946368, 0), (13345226752, 5767168, 155713536, 0), (13350993920, 5767168, 161480704, 0), (13339459584, 5767168, 167247872, 0), (13397131264, 5767168, 173015040, 0), (13402898432, 5767168, 178782208, 0), (13391364096, 5767168, 184549376, 0), (13431734272, 5767168, 190316544, 0), (13437501440, 5767168, 196083712, 0), (13425967104, 5767168, 201850880, 0), (13552844800, 5767168, 207618048, 0), (13558611968, 5767168, 213385216, 0), (13547077632, 5767168, 219152384, 0), (13673955328, 5767168, 224919552, 0), (13679722496, 5767168, 230686720, 0), (13668188160, 5767168, 236453888, 0), (13691256832, 5767168, 242221056, 0), (13697024000, 5767168, 247988224, 0), (13685489664, 5767168, 253755392, 0), (13725859840, 5767168, 259522560, 0), (13731627008, 5767168, 265289728, 0), (13720092672, 5767168, 271056896, 0), (13743161344, 5767168, 276824064, 0), (13748928512, 5767168, 282591232, 0), (13737394176, 5767168, 288358400, 0), (13829668864, 5767168, 294125568, 0), (13835436032, 5767168, 299892736, 0), (13823901696, 5767168, 305659904, 0), (13846970368, 5767168, 311427072, 0), (13852737536, 5767168, 317194240, 0), (13841203200, 5767168, 322961408, 0), (13916176384, 5767168, 328728576, 0), (13921943552, 5767168, 334495744, 0), (13910409216, 5767168, 340262912, 0)], 2: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (12981895168, 5767168, 17301504, 0), (12987662336, 5767168, 23068672, 0), (12976128000, 5767168, 28835840, 0), (12999196672, 5767168, 34603008, 0), (13004963840, 5767168, 40370176, 0), (12993429504, 5767168, 46137344, 0), (13033799680, 5767168, 51904512, 0), (13039566848, 5767168, 57671680, 0), (13028032512, 5767168, 63438848, 0), (13137608704, 5767168, 69206016, 0), (13143375872, 5767168, 74973184, 0), (13131841536, 5767168, 80740352, 0), (13172211712, 5767168, 86507520, 0), (13177978880, 5767168, 92274688, 0), (13166444544, 5767168, 98041856, 0), (13224116224, 5767168, 103809024, 0), (13229883392, 5767168, 109576192, 0), (13218349056, 5767168, 115343360, 0), (13241417728, 5767168, 121110528, 0), (13247184896, 5767168, 126877696, 0), (13235650560, 5767168, 132644864, 0), (13327925248, 5767168, 138412032, 0), (13333692416, 5767168, 144179200, 0), (13322158080, 5767168, 149946368, 0), (13362528256, 5767168, 155713536, 0), (13368295424, 5767168, 161480704, 0), (13356761088, 5767168, 167247872, 0), (13379829760, 5767168, 173015040, 0), (13385596928, 5767168, 178782208, 0), (13374062592, 5767168, 184549376, 0), (13449035776, 5767168, 190316544, 0), (13454802944, 5767168, 196083712, 0), (13443268608, 5767168, 201850880, 0), (13500940288, 5767168, 207618048, 0), (13506707456, 5767168, 213385216, 0), (13495173120, 5767168, 219152384, 0), (13518241792, 5767168, 224919552, 0), (13524008960, 5767168, 230686720, 0), (13512474624, 5767168, 236453888, 0), (13535543296, 5767168, 242221056, 0), (13541310464, 5767168, 247988224, 0), (13529776128, 5767168, 253755392, 0), (13587447808, 5767168, 259522560, 0), (13593214976, 5767168, 265289728, 0), (13581680640, 5767168, 271056896, 0), (13622050816, 5767168, 276824064, 0), (13627817984, 5767168, 282591232, 0), (13616283648, 5767168, 288358400, 0), (13812367360, 5767168, 294125568, 0), (13818134528, 5767168, 299892736, 0), (13806600192, 5767168, 305659904, 0), (13898874880, 5767168, 311427072, 0), (13904642048, 5767168, 317194240, 0), (13893107712, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6785de550>, 2: <capsule object NULL at 0x74a6785de250>}
DEBUG 01-15 16:09:16.207145.207145 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.207875.207875 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.207578.207578 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 92c16f04-ba22-438f-b5b5-70c837542a66
DEBUG 01-15 16:09:16.207617.207617 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:16.208650.208650 client.py:127] Model loaded
DEBUG 01-15 16:09:16.208572.208572 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.208466.208466 cuda_h.py:19] end restore2model cost 0.00034809112548828125 seconds
DEBUG 01-15 16:09:16.208598.208598 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:16.208235.208235 cuda_h.py:19] end sllm_worker_task cost 0.010134220123291016 seconds
INFO 01-15 16:09:16.208821.208821 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 92c16f04-ba22-438f-b5b5-70c837542a66
DEBUG 01-15 16:09:16.209216.209216 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.209970.209970 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0035851001739501953 seconds
DEBUG 01-15 16:09:16.209562.209562 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.209133.209133 cuda_h.py:19] end move_flatidxs cost 0.0008311271667480469 seconds
DEBUG 01-15 16:09:16.209956.209956 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.212136.212136 cuda_h.py:19] end restore2model cost 0.0030465126037597656 seconds
DEBUG 01-15 16:09:16.212416.212416 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006887912750244141 seconds
DEBUG 01-15 16:09:16.212378.212378 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.212970.212970 cuda_h.py:19] end gpu_sexperts cost 0.0002644062042236328 seconds
DEBUG 01-15 16:09:16.212177.212177 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.213616.213616 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4781951904296875e-05 seconds
DEBUG 01-15 16:09:16.213597.213597 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.213869.213869 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.214903.214903 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000978231430053711 seconds
DEBUG 01-15 16:09:16.214031.214031 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.214708.214708 cuda_h.py:19] end gpu_group_list cost 0.00022554397583007812 seconds
DEBUG 01-15 16:09:16.215768.215768 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.216089.216089 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001020669937133789 seconds
DEBUG 01-15 16:09:16.216344.216344 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.216743.216743 cuda_h.py:19] end gpu_group_list cost 0.00022077560424804688 seconds
DEBUG 01-15 16:09:16.217607.217607 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.217675.217675 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 92c16f04-ba22-438f-b5b5-70c837542a66
DEBUG 01-15 16:09:16.218888.218888 cuda_h.py:19] end group_tensors cost 0.009008646011352539 seconds
DEBUG 01-15 16:09:16.219530.219530 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.223165.223165 cuda_h.py:19] end group pad cost 0.0032579898834228516 seconds
DEBUG 01-15 16:09:16.223524.223524 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:16.249342.249342 cuda_h.py:19] end group_einsum cost 0.026705265045166016 seconds
DEBUG 01-15 16:09:16.250645.250645 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:16.250309.250309 client.py:127] Model loaded
DEBUG 01-15 16:09:16.250393.250393 cuda_h.py:19] end wait_experts_multi_device cost 0.0329279899597168 seconds
DEBUG 01-15 16:09:16.250063.250063 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.253301.253301 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029151439666748047 seconds
DEBUG 01-15 16:09:16.253713.253713 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04510354995727539 seconds
DEBUG 01-15 16:09:16.254246.254246 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.004378080368041992 seconds
DEBUG 01-15 16:09:16.254701.254701 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.256354.256354 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.257760.257760 cuda_h.py:19] end index_scatter cost 0.00013256072998046875 seconds
DEBUG 01-15 16:09:16.257835.257835 cuda_h.py:19] end cpuoutputsdeal cost 0.002509593963623047 seconds
DEBUG 01-15 16:09:16.257952.257952 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.257975.257975 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.258790.258790 cuda_h.py:19] end gpu_group_tensor cost 0.0003077983856201172 seconds
DEBUG 01-15 16:09:16.258196.258196 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.258123.258123 cuda_h.py:19] end gpu_group_tensor cost 0.0002884864807128906 seconds
DEBUG 01-15 16:09:16.258282.258282 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.260194.260194 cuda_h.py:19] end gpu_group_einsum cost 0.0013933181762695312 seconds
DEBUG 01-15 16:09:16.260660.260660 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.261590.261590 cuda_h.py:19] end gpu_group_einsum cost 0.0006248950958251953 seconds
DEBUG 01-15 16:09:16.261642.261642 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.261626.261626 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.261219.261219 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024628639221191406 seconds
DEBUG 01-15 16:09:16.261664.261664 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.262224.262224 cuda_h.py:19] end concat_expert_out cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:09:16.262458.262458 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.262746.262746 cuda_h.py:19] end index_scatter cost 7.2479248046875e-05 seconds
DEBUG 01-15 16:09:16.262338.262338 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009462833404541016 seconds
DEBUG 01-15 16:09:16.262089.262089 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.262323.262323 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.262104.262104 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015807151794433594 seconds
DEBUG 01-15 16:09:16.262430.262430 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.262545.262545 cuda_h.py:19] end concat_expert_out cost 5.340576171875e-05 seconds
DEBUG 01-15 16:09:16.262580.262580 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.263696.263696 cuda_h.py:19] end index_scatter cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:16.263267.263267 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005152225494384766 seconds
DEBUG 01-15 16:09:16.263819.263819 cuda_h.py:19] end gpu_experts_multi_device cost 0.050142526626586914 seconds
DEBUG 01-15 16:09:16.263259.263259 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.060120582580566406 seconds
DEBUG 01-15 16:09:16.263070.263070 cuda_h.py:19] end prefill_layer cost 0.06558370590209961 seconds
DEBUG 01-15 16:09:16.263927.263927 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:09:16.263345.263345 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.263432.263432 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:09:16.263758.263758 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:09:16.263467.263467 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:09:16.264655.264655 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 3.933906555175781e-05 seconds
DEBUG 01-15 16:09:16.264649.264649 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:09:16.264968.264968 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.264329.264329 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.264549.264549 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.264534.264534 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.264397.264397 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.264655.264655 cuda_h.py:19] end allocate_cuda_memory cost 0.000362396240234375 seconds
DEBUG 01-15 16:09:16.264294.264294 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.264441.264441 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.265509.265509 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.265788.265788 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 421b4b7a-1028-416f-b19e-e9c24b6a5443
DEBUG 01-15 16:09:16.265838.265838 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.265239.265239 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.266334.266334 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 421b4b7a-1028-416f-b19e-e9c24b6a5443
DEBUG 01-15 16:09:16.266270.266270 cuda_h.py:19] end load_into_gpu_async cost 0.0012073516845703125 seconds
DEBUG 01-15 16:09:16.266357.266357 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.266566.266566 cuda_h.py:19] end restore_tensors2 cost 8.749961853027344e-05 seconds
DEBUG 01-15 16:09:16.266276.266276 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0019376277923583984 seconds
INFO 01-15 16:09:16.266841.266841 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 421b4b7a-1028-416f-b19e-e9c24b6a5443
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.268826.268826 cuda_h.py:19] end self_attn cost 0.0030281543731689453 seconds
DEBUG 01-15 16:09:16.268419.268419 cuda_h.py:19] end iln_self_attn_paln cost 0.004722118377685547 seconds
DEBUG 01-15 16:09:16.268818.268818 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:09:16.268243.268243 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.269146.269146 cuda_h.py:19] end gate cost 0.0006215572357177734 seconds
DEBUG 01-15 16:09:16.269021.269021 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.269496.269496 lmp.py:1912] 
DEBUG 01-15 16:09:16.269496.269496 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.269066.269066 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.270954.270954 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.270743.270743 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.270625.270625 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.270851.270851 lmp.py:1917] 
DEBUG 01-15 16:09:16.270851.270851 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.270447.270447 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.270051.270051 lmp.py:1935]   Expert 39 |     16 | CPU
DEBUG 01-15 16:09:16.270217.270217 lmp.py:1935]   Expert 13 |     17 | CPU
DEBUG 01-15 16:09:16.270191.270191 lmp.py:1935]   Expert 49 |     37 | CPU
DEBUG 01-15 16:09:16.270403.270403 lmp.py:1935]   Expert 35 |     54 | CPU
DEBUG 01-15 16:09:16.270377.270377 lmp.py:1935]   Expert 19 |     63 | CPU
DEBUG 01-15 16:09:16.270113.270113 lmp.py:1935]   Expert  9 |     75 | CPU
DEBUG 01-15 16:09:16.270087.270087 lmp.py:1935]   Expert 26 |     75 | CPU
DEBUG 01-15 16:09:16.270591.270591 lmp.py:1935]   Expert 41 |     75 | CPU
DEBUG 01-15 16:09:16.270996.270996 lmp.py:1935]   Expert 32 |     76 | CPU
DEBUG 01-15 16:09:16.270400.270400 lmp.py:1935]   Expert 33 |     82 | CPU
DEBUG 01-15 16:09:16.270566.270566 lmp.py:1935]   Expert 23 |     87 | CPU
DEBUG 01-15 16:09:16.270302.270302 lmp.py:1935]   Expert 46 |     88 | CPU
DEBUG 01-15 16:09:16.270038.270038 lmp.py:1935]   Expert 18 |     91 | CPU
DEBUG 01-15 16:09:16.270535.270535 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:09:16.270032.270032 lmp.py:1935]   Expert 38 |    100 | CPU
DEBUG 01-15 16:09:16.270768.270768 lmp.py:1935]   Expert  3 |    103 | CPU
DEBUG 01-15 16:09:16.270265.270265 lmp.py:1935]   Expert 17 |    105 | CPU
DEBUG 01-15 16:09:16.270000.270000 lmp.py:1935]   Expert  6 |    107 | CPU
DEBUG 01-15 16:09:16.270975.270975 lmp.py:1935]   Expert 20 |    117 | CPU
DEBUG 01-15 16:09:16.270710.270710 lmp.py:1935]   Expert 61 |    129 | CPU
DEBUG 01-15 16:09:16.270876.270876 lmp.py:1935]   Expert 40 |    130 | CPU
DEBUG 01-15 16:09:16.270042.270042 lmp.py:1935]   Expert 15 |    132 | CPU
DEBUG 01-15 16:09:16.270354.270354 lmp.py:1935]   Expert 62 |    132 | CPU
DEBUG 01-15 16:09:16.270090.270090 lmp.py:1935]   Expert 44 |    133 | CPU
DEBUG 01-15 16:09:16.270826.270826 lmp.py:1935]   Expert 43 |    135 | CPU
DEBUG 01-15 16:09:16.270992.270992 lmp.py:1935]   Expert 50 |    137 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270350.270350 lmp.py:1935]   Expert 16 |    139 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270278.270278 lmp.py:1935]   Expert 59 |    140 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270967.270967 lmp.py:1935]   Expert 63 |    140 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270418.270418 lmp.py:1935]   Expert 42 |    144 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270346.270346 lmp.py:1935]   Expert  2 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270035.270035 lmp.py:1935]   Expert 36 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270963.270963 lmp.py:1935]   Expert 10 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270606.270606 lmp.py:1935]   Expert  5 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270871.270871 lmp.py:1935]   Expert 34 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270799.270799 lmp.py:1935]   Expert 52 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270011.270011 lmp.py:1935]   Expert 27 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270701.270701 lmp.py:1935]   Expert 45 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270628.270628 lmp.py:1935]   Expert 60 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270318.270318 lmp.py:1935]   Expert 48 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270769.270769 lmp.py:1935]   Expert 51 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270981.270981 lmp.py:1935]   Expert 56 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270193.270193 lmp.py:1935]   Expert 53 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270644.270644 lmp.py:1935]   Expert 24 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270857.270857 lmp.py:1935]   Expert  7 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270308.270308 lmp.py:1935]   Expert  8 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270951.270951 lmp.py:1935]   Expert 57 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270832.270832 lmp.py:1935]   Expert 47 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270521.270521 lmp.py:1935]   Expert 29 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270211.270211 lmp.py:1935]   Expert 21 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270423.270423 lmp.py:1935]   Expert 14 |    287 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.270874.270874 lmp.py:1935]   Expert  0 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.270325.270325 lmp.py:1935]   Expert  4 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.271776.271776 lmp.py:1935]   Expert 22 |    312 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.271465.271465 lmp.py:1935]   Expert 58 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.271439.271439 lmp.py:1935]   Expert 55 |    318 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.271082.271082 lmp.py:1935]   Expert 37 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.271725.271725 lmp.py:1935]   Expert  1 |    320 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.271560.271560 lmp.py:1935]   Expert 54 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.271918.271918 lmp.py:1935]   Expert 28 |    359 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.271230.271230 lmp.py:1935]   Expert 12 |    376 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.271873.271873 lmp.py:1935]   Expert 25 |    395 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.271039.271039 lmp.py:1935]   Expert 11 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.271206.271206 lmp.py:1935]   Expert 30 |    835 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.271418.271418 lmp.py:1937] 
DEBUG 01-15 16:09:16.271418.271418 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.271346.271346 lmp.py:1938]   CPU:   2250 tokens
DEBUG 01-15 16:09:16.271227.271227 lmp.py:1942]   cuda:1:   5020 tokens (19 experts)
DEBUG 01-15 16:09:16.271632.271632 lmp.py:1942]   cuda:2:   5018 tokens (20 experts)
DEBUG 01-15 16:09:16.271559.271559 lmp.py:1943]   Total GPU:  10038 tokens
DEBUG 01-15 16:09:16.271202.271202 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.271202.271202 lmp.py:1944] 
DEBUG 01-15 16:09:16.271852.271852 cuda_h.py:19] end experts_map_get cost 0.0016379356384277344 seconds
DEBUG 01-15 16:09:16.271033.271033 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.271405.271405 lmp.py:1953] 
DEBUG 01-15 16:09:16.271405.271405 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.271142.271142 cuda_h.py:19] end cpu_experts_submit cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:16.271169.271169 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.271197.271197 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.273833.273833 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.45.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.45.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.45.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.48.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.48.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.48.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.11.mlp.experts.2.gate_proj.weight': 0, 'model.layers.11.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.59.up_proj.weight': 322961408, 'model.layers.11.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.11.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.11.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (13950779392, 5767168, 17301504, 0), (13956546560, 5767168, 23068672, 0), (13945012224, 5767168, 28835840, 0), (14019985408, 5767168, 34603008, 0), (14025752576, 5767168, 40370176, 0), (14014218240, 5767168, 46137344, 0), (14054588416, 5767168, 51904512, 0), (14060355584, 5767168, 57671680, 0), (14048821248, 5767168, 63438848, 0), (14106492928, 5767168, 69206016, 0), (14112260096, 5767168, 74973184, 0), (14100725760, 5767168, 80740352, 0), (14296809472, 5767168, 86507520, 0), (14302576640, 5767168, 92274688, 0), (14291042304, 5767168, 98041856, 0), (14314110976, 5767168, 103809024, 0), (14319878144, 5767168, 109576192, 0), (14308343808, 5767168, 115343360, 0), (14417920000, 5767168, 121110528, 0), (14423687168, 5767168, 126877696, 0), (14412152832, 5767168, 132644864, 0), (14435221504, 5767168, 138412032, 0), (14440988672, 5767168, 144179200, 0), (14429454336, 5767168, 149946368, 0), (14452523008, 5767168, 155713536, 0), (14458290176, 5767168, 161480704, 0), (14446755840, 5767168, 167247872, 0), (14660141056, 5767168, 173015040, 0), (14665908224, 5767168, 178782208, 0), (14654373888, 5767168, 184549376, 0), (14712045568, 5767168, 190316544, 0), (14717812736, 5767168, 196083712, 0), (14706278400, 5767168, 201850880, 0), (14763950080, 5767168, 207618048, 0), (14769717248, 5767168, 213385216, 0), (14758182912, 5767168, 219152384, 0), (14798553088, 5767168, 224919552, 0), (14804320256, 5767168, 230686720, 0), (14792785920, 5767168, 236453888, 0), (14833156096, 5767168, 242221056, 0), (14838923264, 5767168, 247988224, 0), (14827388928, 5767168, 253755392, 0), (14850457600, 5767168, 259522560, 0), (14856224768, 5767168, 265289728, 0), (14844690432, 5767168, 271056896, 0), (14885060608, 5767168, 276824064, 0), (14890827776, 5767168, 282591232, 0), (14879293440, 5767168, 288358400, 0), (14919663616, 5767168, 294125568, 0), (14925430784, 5767168, 299892736, 0), (14913896448, 5767168, 305659904, 0), (15023472640, 5767168, 311427072, 0), (15029239808, 5767168, 317194240, 0), (15017705472, 5767168, 322961408, 0)], 2: [(13968080896, 5767168, 0, 0), (13973848064, 5767168, 5767168, 0), (13962313728, 5767168, 11534336, 0), (14002683904, 5767168, 17301504, 0), (14008451072, 5767168, 23068672, 0), (13996916736, 5767168, 28835840, 0), (14071889920, 5767168, 34603008, 0), (14077657088, 5767168, 40370176, 0), (14066122752, 5767168, 46137344, 0), (14123794432, 5767168, 51904512, 0), (14129561600, 5767168, 57671680, 0), (14118027264, 5767168, 63438848, 0), (14141095936, 5767168, 69206016, 0), (14146863104, 5767168, 74973184, 0), (14135328768, 5767168, 80740352, 0), (14175698944, 5767168, 86507520, 0), (14181466112, 5767168, 92274688, 0), (14169931776, 5767168, 98041856, 0), (14210301952, 5767168, 103809024, 0), (14216069120, 5767168, 109576192, 0), (14204534784, 5767168, 115343360, 0), (14348713984, 5767168, 121110528, 0), (14354481152, 5767168, 126877696, 0), (14342946816, 5767168, 132644864, 0), (14366015488, 5767168, 138412032, 0), (14371782656, 5767168, 144179200, 0), (14360248320, 5767168, 149946368, 0), (14400618496, 5767168, 155713536, 0), (14406385664, 5767168, 161480704, 0), (14394851328, 5767168, 167247872, 0), (14521729024, 5767168, 173015040, 0), (14527496192, 5767168, 178782208, 0), (14515961856, 5767168, 184549376, 0), (14556332032, 5767168, 190316544, 0), (14562099200, 5767168, 196083712, 0), (14550564864, 5767168, 201850880, 0), (14573633536, 5767168, 207618048, 0), (14579400704, 5767168, 213385216, 0), (14567866368, 5767168, 219152384, 0), (14746648576, 5767168, 224919552, 0), (14752415744, 5767168, 230686720, 0), (14740881408, 5767168, 236453888, 0), (14815854592, 5767168, 242221056, 0), (14821621760, 5767168, 247988224, 0), (14810087424, 5767168, 253755392, 0), (14867759104, 5767168, 259522560, 0), (14873526272, 5767168, 265289728, 0), (14861991936, 5767168, 271056896, 0), (14902362112, 5767168, 276824064, 0), (14908129280, 5767168, 282591232, 0), (14896594944, 5767168, 288358400, 0), (14936965120, 5767168, 294125568, 0), (14942732288, 5767168, 299892736, 0), (14931197952, 5767168, 305659904, 0), (14954266624, 5767168, 311427072, 0), (14960033792, 5767168, 317194240, 0), (14948499456, 5767168, 322961408, 0), (14971568128, 5767168, 328728576, 0), (14977335296, 5767168, 334495744, 0), (14965800960, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6785de730>, 2: <capsule object NULL at 0x74a6785de430>}
DEBUG 01-15 16:09:16.273667.273667 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.273920.273920 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.273339.273339 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b86e0c34-c460-4a41-8cea-c49c51bd61ac
DEBUG 01-15 16:09:16.273723.273723 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:16.274717.274717 client.py:127] Model loaded
DEBUG 01-15 16:09:16.274832.274832 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.274364.274364 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:16.274387.274387 cuda_h.py:19] end restore2model cost 0.0003421306610107422 seconds
DEBUG 01-15 16:09:16.274726.274726 cuda_h.py:19] end sllm_worker_task cost 0.01050877571105957 seconds
DEBUG 01-15 16:09:16.275309.275309 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:16.275543.275543 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b86e0c34-c460-4a41-8cea-c49c51bd61ac
DEBUG 01-15 16:09:16.275170.275170 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004091739654541016 seconds
DEBUG 01-15 16:09:16.275000.275000 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.275186.275186 cuda_h.py:19] end move_flatidxs cost 0.0008358955383300781 seconds
DEBUG 01-15 16:09:16.275532.275532 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.278745.278745 cuda_h.py:19] end restore2model cost 0.0030074119567871094 seconds
DEBUG 01-15 16:09:16.278423.278423 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007363796234130859 seconds
DEBUG 01-15 16:09:16.278980.278980 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.279765.279765 cuda_h.py:19] end gpu_sexperts cost 0.0002675056457519531 seconds
DEBUG 01-15 16:09:16.279972.279972 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.279695.279695 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4066696166992188e-05 seconds
DEBUG 01-15 16:09:16.279199.279199 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.279948.279948 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.280602.280602 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009093284606933594 seconds
DEBUG 01-15 16:09:16.280491.280491 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.280122.280122 cuda_h.py:19] end gpu_group_list cost 0.0002262592315673828 seconds
DEBUG 01-15 16:09:16.280923.280923 cuda_h.py:19] end group_tensors cost 0.004711627960205078 seconds
DEBUG 01-15 16:09:16.281607.281607 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.281435.281435 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.283499.283499 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001674652099609375 seconds
DEBUG 01-15 16:09:16.283928.283928 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.283748.283748 cuda_h.py:19] end gpu_group_list cost 0.00032210350036621094 seconds
DEBUG 01-15 16:09:16.284459.284459 cuda_h.py:19] end group pad cost 0.0032341480255126953 seconds
DEBUG 01-15 16:09:16.284023.284023 cuda_h.py:10] start wait_experts_multi_device
DEBUG 01-15 16:09:16.284864.284864 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:16.284435.284435 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b86e0c34-c460-4a41-8cea-c49c51bd61ac
DEBUG 01-15 16:09:16.305816.305816 cuda_h.py:19] end group_einsum cost 0.020804643630981445 seconds
DEBUG 01-15 16:09:16.305179.305179 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.309884.309884 cuda_h.py:19] end get_outputs_cpu1 cost 0.003197908401489258 seconds
DEBUG 01-15 16:09:16.309058.309058 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03509807586669922 seconds
INFO 01-15 16:09:16.314832.314832 client.py:127] Model loaded
DEBUG 01-15 16:09:16.314578.314578 cuda_h.py:19] end wait_experts_multi_device cost 0.029766559600830078 seconds
DEBUG 01-15 16:09:16.314102.314102 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.315453.315453 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005357265472412109 seconds
DEBUG 01-15 16:09:16.315812.315812 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.316665.316665 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.316040.316040 cuda_h.py:19] end index_scatter cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:09:16.316389.316389 cuda_h.py:19] end cpuoutputsdeal cost 0.001371145248413086 seconds
DEBUG 01-15 16:09:16.316736.316736 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.316115.316115 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.316882.316882 cuda_h.py:19] end gpu_group_tensor cost 0.0001480579376220703 seconds
DEBUG 01-15 16:09:16.316022.316022 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.317246.317246 cuda_h.py:19] end gpu_group_tensor cost 0.00013589859008789062 seconds
DEBUG 01-15 16:09:16.317236.317236 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.317847.317847 cuda_h.py:19] end gpu_group_einsum cost 0.0005764961242675781 seconds
DEBUG 01-15 16:09:16.317322.317322 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.318285.318285 cuda_h.py:19] end gpu_group_einsum cost 0.00044465065002441406 seconds
DEBUG 01-15 16:09:16.318282.318282 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.318988.318988 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.318492.318492 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019502639770507812 seconds
DEBUG 01-15 16:09:16.318486.318486 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.319165.319165 cuda_h.py:19] end concat_expert_out cost 4.7206878662109375e-05 seconds
DEBUG 01-15 16:09:16.319292.319292 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.319600.319600 cuda_h.py:19] end index_scatter cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:09:16.319866.319866 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007736682891845703 seconds
DEBUG 01-15 16:09:16.319789.319789 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.319970.319970 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.319559.319559 cuda_h.py:19] end all_expert_outputs_slices cost 0.000156402587890625 seconds
DEBUG 01-15 16:09:16.319600.319600 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.319186.319186 cuda_h.py:19] end concat_expert_out cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:09:16.319691.319691 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.319515.319515 cuda_h.py:19] end index_scatter cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:09:16.320608.320608 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004987716674804688 seconds
DEBUG 01-15 16:09:16.320233.320233 cuda_h.py:19] end gpu_experts_multi_device cost 0.040769338607788086 seconds
DEBUG 01-15 16:09:16.320951.320951 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.05122733116149902 seconds
DEBUG 01-15 16:09:16.320522.320522 cuda_h.py:19] end prefill_layer cost 0.05666303634643555 seconds
DEBUG 01-15 16:09:16.320232.320232 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:09:16.320412.320412 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.320877.320877 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:09:16.320487.320487 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:09:16.320150.320150 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:09:16.320762.320762 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 3.695487976074219e-05 seconds
DEBUG 01-15 16:09:16.320279.320279 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 6.937980651855469e-05 seconds
DEBUG 01-15 16:09:16.320167.320167 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.320216.320216 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.321830.321830 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.321845.321845 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.321213.321213 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.321230.321230 cuda_h.py:19] end allocate_cuda_memory cost 0.0002532005310058594 seconds
DEBUG 01-15 16:09:16.321577.321577 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.321439.321439 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.321222.321222 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.321025.321025 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 17310f64-20c5-4e0a-a183-80aa40462c3e
DEBUG 01-15 16:09:16.321545.321545 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.322885.322885 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.323557.323557 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 17310f64-20c5-4e0a-a183-80aa40462c3e
DEBUG 01-15 16:09:16.323109.323109 cuda_h.py:19] end load_into_gpu_async cost 0.0014071464538574219 seconds
DEBUG 01-15 16:09:16.323242.323242 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.323776.323776 cuda_h.py:19] end restore_tensors2 cost 8.392333984375e-05 seconds
DEBUG 01-15 16:09:16.323022.323022 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0020270347595214844 seconds
INFO 01-15 16:09:16.323520.323520 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 17310f64-20c5-4e0a-a183-80aa40462c3e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.325273.325273 cuda_h.py:19] end self_attn cost 0.002817869186401367 seconds
DEBUG 01-15 16:09:16.325250.325250 cuda_h.py:19] end iln_self_attn_paln cost 0.00441741943359375 seconds
DEBUG 01-15 16:09:16.325411.325411 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:09:16.325644.325644 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.326917.326917 cuda_h.py:19] end gate cost 0.0006260871887207031 seconds
DEBUG 01-15 16:09:16.326700.326700 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.326506.326506 lmp.py:1912] 
DEBUG 01-15 16:09:16.326506.326506 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.326023.326023 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.326150.326150 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.326938.326938 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.326058.326058 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.326032.326032 lmp.py:1917] 
DEBUG 01-15 16:09:16.326032.326032 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.326437.326437 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.326087.326087 lmp.py:1935]   Expert 12 |     19 | CPU
DEBUG 01-15 16:09:16.326014.326014 lmp.py:1935]   Expert 47 |     25 | CPU
DEBUG 01-15 16:09:16.326465.326465 lmp.py:1935]   Expert 38 |     31 | CPU
DEBUG 01-15 16:09:16.326108.326108 lmp.py:1935]   Expert 27 |     35 | CPU
DEBUG 01-15 16:09:16.326513.326513 lmp.py:1935]   Expert 16 |     36 | CPU
DEBUG 01-15 16:09:16.326964.326964 lmp.py:1935]   Expert 52 |     39 | CPU
DEBUG 01-15 16:09:16.326415.326415 lmp.py:1935]   Expert 63 |     44 | CPU
DEBUG 01-15 16:09:16.326104.326104 lmp.py:1935]   Expert  4 |     59 | CPU
DEBUG 01-15 16:09:16.326793.326793 lmp.py:1935]   Expert 61 |     62 | CPU
DEBUG 01-15 16:09:16.326006.326006 lmp.py:1935]   Expert 44 |     64 | CPU
DEBUG 01-15 16:09:16.326933.326933 lmp.py:1935]   Expert 43 |     65 | CPU
DEBUG 01-15 16:09:16.326437.326437 lmp.py:1935]   Expert 34 |     75 | CPU
DEBUG 01-15 16:09:16.326226.326226 lmp.py:1935]   Expert 53 |     83 | CPU
DEBUG 01-15 16:09:16.326392.326392 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:09:16.326128.326128 lmp.py:1935]   Expert 32 |     87 | CPU
DEBUG 01-15 16:09:16.326340.326340 lmp.py:1935]   Expert 37 |     91 | CPU
DEBUG 01-15 16:09:16.326076.326076 lmp.py:1935]   Expert 13 |    103 | CPU
DEBUG 01-15 16:09:16.326573.326573 lmp.py:1935]   Expert 39 |    113 | CPU
DEBUG 01-15 16:09:16.326547.326547 lmp.py:1935]   Expert 21 |    118 | CPU
DEBUG 01-15 16:09:16.326044.326044 lmp.py:1935]   Expert 11 |    121 | CPU
DEBUG 01-15 16:09:16.326780.326780 lmp.py:1935]   Expert 20 |    127 | CPU
DEBUG 01-15 16:09:16.326039.326039 lmp.py:1935]   Expert 60 |    130 | CPU
DEBUG 01-15 16:09:16.326774.326774 lmp.py:1935]   Expert  8 |    131 | CPU
DEBUG 01-15 16:09:16.326272.326272 lmp.py:1935]   Expert 14 |    134 | CPU
DEBUG 01-15 16:09:16.326246.326246 lmp.py:1935]   Expert 57 |    138 | CPU
DEBUG 01-15 16:09:16.326889.326889 lmp.py:1935]   Expert 22 |    141 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327770.327770 lmp.py:1935]   Expert 45 |    149 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327890.327890 lmp.py:1935]   Expert  2 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327818.327818 lmp.py:1935]   Expert 18 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327268.327268 lmp.py:1935]   Expert 23 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327242.327242 lmp.py:1935]   Expert 17 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327693.327693 lmp.py:1935]   Expert  7 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327667.327667 lmp.py:1935]   Expert 58 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327641.327641 lmp.py:1935]   Expert 30 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327616.327616 lmp.py:1935]   Expert 42 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327828.327828 lmp.py:1935]   Expert 49 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327802.327802 lmp.py:1935]   Expert 62 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327253.327253 lmp.py:1935]   Expert 35 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327942.327942 lmp.py:1935]   Expert 55 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327347.327347 lmp.py:1935]   Expert 48 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327228.327228 lmp.py:1935]   Expert 51 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327871.327871 lmp.py:1935]   Expert 29 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327560.327560 lmp.py:1935]   Expert  6 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327773.327773 lmp.py:1935]   Expert 25 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327224.327224 lmp.py:1935]   Expert 36 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327675.327675 lmp.py:1935]   Expert  1 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327887.327887 lmp.py:1935]   Expert 31 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327861.327861 lmp.py:1935]   Expert 28 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327835.327835 lmp.py:1935]   Expert 54 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327809.327809 lmp.py:1935]   Expert 41 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327473.327473 lmp.py:1935]   Expert  5 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327354.327354 lmp.py:1935]   Expert  9 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327235.327235 lmp.py:1935]   Expert 19 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327117.327117 lmp.py:1935]   Expert 24 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327336.327336 lmp.py:1935]   Expert 50 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327502.327502 lmp.py:1935]   Expert 46 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327953.327953 lmp.py:1935]   Expert 59 |    313 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327642.327642 lmp.py:1935]   Expert 56 |    377 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327093.327093 lmp.py:1935]   Expert 26 |    406 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327783.327783 lmp.py:1935]   Expert 33 |    421 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327233.327233 lmp.py:1935]   Expert  3 |    588 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327923.327923 lmp.py:1935]   Expert 10 |    641 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327374.327374 lmp.py:1935]   Expert 15 |    648 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.327348.327348 lmp.py:1935]   Expert 40 |    791 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.327845.327845 lmp.py:1937] 
DEBUG 01-15 16:09:16.327845.327845 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.327057.327057 lmp.py:1938]   CPU:   2017 tokens
DEBUG 01-15 16:09:16.327177.327177 lmp.py:1942]   cuda:1:   5069 tokens (19 experts)
DEBUG 01-15 16:09:16.327820.327820 lmp.py:1942]   cuda:2:   5202 tokens (20 experts)
DEBUG 01-15 16:09:16.327794.327794 lmp.py:1943]   Total GPU:  10271 tokens
DEBUG 01-15 16:09:16.327768.327768 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.327768.327768 lmp.py:1944] 
DEBUG 01-15 16:09:16.327802.327802 cuda_h.py:19] end experts_map_get cost 0.0016245841979980469 seconds
DEBUG 01-15 16:09:16.327645.327645 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.327063.327063 lmp.py:1953] 
DEBUG 01-15 16:09:16.327063.327063 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.327608.327608 cuda_h.py:19] end cpu_experts_submit cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:09:16.327066.327066 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.327995.327995 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.330776.330776 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.1.gate_proj.weight': 0, 'model.layers.12.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.59.up_proj.weight': 322961408}, 2: {'model.layers.12.mlp.experts.6.gate_proj.weight': 0, 'model.layers.12.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.51.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.51.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.51.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.12.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.12.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.12.mlp.experts.62.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(15058075648, 5767168, 0, 0), (15063842816, 5767168, 5767168, 0), (15052308480, 5767168, 11534336, 0), (15075377152, 5767168, 17301504, 0), (15081144320, 5767168, 23068672, 0), (15069609984, 5767168, 28835840, 0), (15092678656, 5767168, 34603008, 0), (15098445824, 5767168, 40370176, 0), (15086911488, 5767168, 46137344, 0), (15127281664, 5767168, 51904512, 0), (15133048832, 5767168, 57671680, 0), (15121514496, 5767168, 63438848, 0), (15352201216, 5767168, 69206016, 0), (15357968384, 5767168, 74973184, 0), (15346434048, 5767168, 80740352, 0), (15456010240, 5767168, 86507520, 0), (15461777408, 5767168, 92274688, 0), (15450243072, 5767168, 98041856, 0), (15473311744, 5767168, 103809024, 0), (15479078912, 5767168, 109576192, 0), (15467544576, 5767168, 115343360, 0), (15490613248, 5767168, 121110528, 0), (15496380416, 5767168, 126877696, 0), (15484846080, 5767168, 132644864, 0), (15542517760, 5767168, 138412032, 0), (15548284928, 5767168, 144179200, 0), (15536750592, 5767168, 149946368, 0), (15559819264, 5767168, 155713536, 0), (15565586432, 5767168, 161480704, 0), (15554052096, 5767168, 167247872, 0), (15577120768, 5767168, 173015040, 0), (15582887936, 5767168, 178782208, 0), (15571353600, 5767168, 184549376, 0), (15646326784, 5767168, 190316544, 0), (15652093952, 5767168, 196083712, 0), (15640559616, 5767168, 201850880, 0), (15732834304, 5767168, 207618048, 0), (15738601472, 5767168, 213385216, 0), (15727067136, 5767168, 219152384, 0), (15767437312, 5767168, 224919552, 0), (15773204480, 5767168, 230686720, 0), (15761670144, 5767168, 236453888, 0), (15905849344, 5767168, 242221056, 0), (15911616512, 5767168, 247988224, 0), (15900082176, 5767168, 253755392, 0), (15975055360, 5767168, 259522560, 0), (15980822528, 5767168, 265289728, 0), (15969288192, 5767168, 271056896, 0), (15992356864, 5767168, 276824064, 0), (15998124032, 5767168, 282591232, 0), (15986589696, 5767168, 288358400, 0), (16044261376, 5767168, 294125568, 0), (16050028544, 5767168, 299892736, 0), (16038494208, 5767168, 305659904, 0), (16061562880, 5767168, 311427072, 0), (16067330048, 5767168, 317194240, 0), (16055795712, 5767168, 322961408, 0)], 2: [(15144583168, 5767168, 0, 0), (15150350336, 5767168, 5767168, 0), (15138816000, 5767168, 11534336, 0), (15161884672, 5767168, 17301504, 0), (15167651840, 5767168, 23068672, 0), (15156117504, 5767168, 28835840, 0), (15196487680, 5767168, 34603008, 0), (15202254848, 5767168, 40370176, 0), (15190720512, 5767168, 46137344, 0), (15213789184, 5767168, 51904512, 0), (15219556352, 5767168, 57671680, 0), (15208022016, 5767168, 63438848, 0), (15300296704, 5767168, 69206016, 0), (15306063872, 5767168, 74973184, 0), (15294529536, 5767168, 80740352, 0), (15334899712, 5767168, 86507520, 0), (15340666880, 5767168, 92274688, 0), (15329132544, 5767168, 98041856, 0), (15369502720, 5767168, 103809024, 0), (15375269888, 5767168, 109576192, 0), (15363735552, 5767168, 115343360, 0), (15421407232, 5767168, 121110528, 0), (15427174400, 5767168, 126877696, 0), (15415640064, 5767168, 132644864, 0), (15438708736, 5767168, 138412032, 0), (15444475904, 5767168, 144179200, 0), (15432941568, 5767168, 149946368, 0), (15525216256, 5767168, 155713536, 0), (15530983424, 5767168, 161480704, 0), (15519449088, 5767168, 167247872, 0), (15611723776, 5767168, 173015040, 0), (15617490944, 5767168, 178782208, 0), (15605956608, 5767168, 184549376, 0), (15663628288, 5767168, 190316544, 0), (15669395456, 5767168, 196083712, 0), (15657861120, 5767168, 201850880, 0), (15750135808, 5767168, 207618048, 0), (15755902976, 5767168, 213385216, 0), (15744368640, 5767168, 219152384, 0), (15819341824, 5767168, 224919552, 0), (15825108992, 5767168, 230686720, 0), (15813574656, 5767168, 236453888, 0), (15836643328, 5767168, 242221056, 0), (15842410496, 5767168, 247988224, 0), (15830876160, 5767168, 253755392, 0), (15871246336, 5767168, 259522560, 0), (15877013504, 5767168, 265289728, 0), (15865479168, 5767168, 271056896, 0), (15888547840, 5767168, 276824064, 0), (15894315008, 5767168, 282591232, 0), (15882780672, 5767168, 288358400, 0), (15923150848, 5767168, 294125568, 0), (15928918016, 5767168, 299892736, 0), (15917383680, 5767168, 305659904, 0), (16009658368, 5767168, 311427072, 0), (16015425536, 5767168, 317194240, 0), (16003891200, 5767168, 322961408, 0), (16113467392, 5767168, 328728576, 0), (16119234560, 5767168, 334495744, 0), (16107700224, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814163090>, 2: <capsule object NULL at 0x74a814571cb0>}
DEBUG 01-15 16:09:16.330773.330773 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.330260.330260 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.330393.330393 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 258646a2-93db-493b-b452-7acf679b2e57
DEBUG 01-15 16:09:16.330533.330533 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:16.331091.331091 client.py:127] Model loaded
DEBUG 01-15 16:09:16.331033.331033 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.331766.331766 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:16.331101.331101 cuda_h.py:19] end restore2model cost 0.000396728515625 seconds
DEBUG 01-15 16:09:16.331116.331116 cuda_h.py:19] end sllm_worker_task cost 0.010674476623535156 seconds
INFO 01-15 16:09:16.331277.331277 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 258646a2-93db-493b-b452-7acf679b2e57
DEBUG 01-15 16:09:16.331518.331518 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.332256.332256 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0044078826904296875 seconds
DEBUG 01-15 16:09:16.332762.332762 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.332277.332277 cuda_h.py:19] end move_flatidxs cost 0.0008497238159179688 seconds
DEBUG 01-15 16:09:16.332868.332868 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.335867.335867 cuda_h.py:19] end restore2model cost 0.0030984878540039062 seconds
DEBUG 01-15 16:09:16.335180.335180 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007750988006591797 seconds
DEBUG 01-15 16:09:16.335545.335545 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.336840.336840 cuda_h.py:19] end gpu_sexperts cost 0.0002574920654296875 seconds
DEBUG 01-15 16:09:16.336331.336331 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.336816.336816 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.3828277587890625e-05 seconds
DEBUG 01-15 16:09:16.336320.336320 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.336116.336116 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.337892.337892 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009982585906982422 seconds
DEBUG 01-15 16:09:16.337603.337603 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.337213.337213 cuda_h.py:19] end gpu_group_list cost 0.00021004676818847656 seconds
DEBUG 01-15 16:09:16.338459.338459 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.339589.339589 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010497570037841797 seconds
DEBUG 01-15 16:09:16.339658.339658 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.339898.339898 cuda_h.py:19] end gpu_group_list cost 0.000217437744140625 seconds
DEBUG 01-15 16:09:16.340432.340432 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.340030.340030 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 258646a2-93db-493b-b452-7acf679b2e57
DEBUG 01-15 16:09:16.342388.342388 cuda_h.py:19] end group_tensors cost 0.009543180465698242 seconds
DEBUG 01-15 16:09:16.343890.343890 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.346381.346381 cuda_h.py:19] end group pad cost 0.0033330917358398438 seconds
DEBUG 01-15 16:09:16.346025.346025 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:16.367154.367154 cuda_h.py:19] end group_einsum cost 0.0209653377532959 seconds
DEBUG 01-15 16:09:16.367357.367357 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:16.368381.368381 client.py:127] Model loaded
DEBUG 01-15 16:09:16.369513.369513 cuda_h.py:19] end wait_experts_multi_device cost 0.0286405086517334 seconds
DEBUG 01-15 16:09:16.369666.369666 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.370625.370625 cuda_h.py:19] end get_outputs_cpu1 cost 0.0025970935821533203 seconds
DEBUG 01-15 16:09:16.371049.371049 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039641618728637695 seconds
DEBUG 01-15 16:09:16.372766.372766 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0029075145721435547 seconds
DEBUG 01-15 16:09:16.372203.372203 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.374809.374809 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.374839.374839 cuda_h.py:19] end index_scatter cost 0.00015234947204589844 seconds
DEBUG 01-15 16:09:16.375465.375465 cuda_h.py:19] end cpuoutputsdeal cost 0.0028810501098632812 seconds
DEBUG 01-15 16:09:16.375365.375365 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.375911.375911 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.376725.376725 cuda_h.py:19] end gpu_group_tensor cost 0.00030994415283203125 seconds
DEBUG 01-15 16:09:16.376132.376132 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.376377.376377 cuda_h.py:19] end gpu_group_tensor cost 0.00031185150146484375 seconds
DEBUG 01-15 16:09:16.376782.376782 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.378575.378575 cuda_h.py:19] end gpu_group_einsum cost 0.0014195442199707031 seconds
DEBUG 01-15 16:09:16.378616.378616 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.379087.379087 cuda_h.py:19] end gpu_group_einsum cost 0.0005681514739990234 seconds
DEBUG 01-15 16:09:16.379926.379926 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.379976.379976 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.379235.379235 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001800060272216797 seconds
DEBUG 01-15 16:09:16.379329.379329 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.379697.379697 cuda_h.py:19] end concat_expert_out cost 6.103515625e-05 seconds
DEBUG 01-15 16:09:16.379798.379798 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.379788.379788 cuda_h.py:19] end index_scatter cost 6.699562072753906e-05 seconds
DEBUG 01-15 16:09:16.380087.380087 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000804901123046875 seconds
DEBUG 01-15 16:09:16.380587.380587 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.380807.380807 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.380409.380409 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001342296600341797 seconds
DEBUG 01-15 16:09:16.380542.380542 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.380313.380313 cuda_h.py:19] end concat_expert_out cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:09:16.380911.380911 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.380549.380549 cuda_h.py:19] end index_scatter cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:09:16.380120.380120 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046539306640625 seconds
DEBUG 01-15 16:09:16.380229.380229 cuda_h.py:19] end gpu_experts_multi_device cost 0.04471540451049805 seconds
DEBUG 01-15 16:09:16.380092.380092 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.05553078651428223 seconds
DEBUG 01-15 16:09:16.381875.381875 cuda_h.py:19] end prefill_layer cost 0.06066751480102539 seconds
DEBUG 01-15 16:09:16.381950.381950 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:09:16.381076.381076 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.381395.381395 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:09:16.381429.381429 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:09:16.381085.381085 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:09:16.381459.381459 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 3.3855438232421875e-05 seconds
DEBUG 01-15 16:09:16.381129.381129 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 7.867813110351562e-05 seconds
DEBUG 01-15 16:09:16.381778.381778 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.381124.381124 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.381702.381702 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.381624.381624 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.382689.382689 cuda_h.py:19] end allocate_cuda_memory cost 0.00032401084899902344 seconds
DEBUG 01-15 16:09:16.382983.382983 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.382436.382436 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.382815.382815 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.382017.382017 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.382217.382217 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4b755395-124a-471b-8955-37a66e42382a
DEBUG 01-15 16:09:16.382485.382485 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.382093.382093 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.383815.383815 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4b755395-124a-471b-8955-37a66e42382a
DEBUG 01-15 16:09:16.383096.383096 cuda_h.py:19] end load_into_gpu_async cost 0.0012619495391845703 seconds
DEBUG 01-15 16:09:16.383474.383474 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.383233.383233 cuda_h.py:19] end restore_tensors2 cost 7.104873657226562e-05 seconds
DEBUG 01-15 16:09:16.383664.383664 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002013683319091797 seconds
INFO 01-15 16:09:16.383653.383653 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4b755395-124a-471b-8955-37a66e42382a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.385487.385487 cuda_h.py:19] end self_attn cost 0.003004789352416992 seconds
DEBUG 01-15 16:09:16.386299.386299 cuda_h.py:19] end iln_self_attn_paln cost 0.004635334014892578 seconds
DEBUG 01-15 16:09:16.386413.386413 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:09:16.386361.386361 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.387112.387112 cuda_h.py:19] end gate cost 0.0006265640258789062 seconds
DEBUG 01-15 16:09:16.387325.387325 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.387919.387919 lmp.py:1912] 
DEBUG 01-15 16:09:16.387919.387919 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.387543.387543 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.387200.387200 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.387280.387280 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.387976.387976 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.387719.387719 lmp.py:1917] 
DEBUG 01-15 16:09:16.387719.387719 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.387938.387938 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.387594.387594 lmp.py:1935]   Expert 19 |     24 | CPU
DEBUG 01-15 16:09:16.387575.387575 lmp.py:1935]   Expert 42 |     25 | CPU
DEBUG 01-15 16:09:16.387602.387602 lmp.py:1935]   Expert 30 |     26 | CPU
DEBUG 01-15 16:09:16.387106.387106 lmp.py:1935]   Expert 32 |     46 | CPU
DEBUG 01-15 16:09:16.387849.387849 lmp.py:1935]   Expert  6 |     57 | CPU
DEBUG 01-15 16:09:16.387114.387114 lmp.py:1935]   Expert 53 |     74 | CPU
DEBUG 01-15 16:09:16.387618.387618 lmp.py:1935]   Expert  5 |     75 | CPU
DEBUG 01-15 16:09:16.387599.387599 lmp.py:1935]   Expert  1 |     78 | CPU
DEBUG 01-15 16:09:16.387818.387818 lmp.py:1935]   Expert 63 |    123 | CPU
DEBUG 01-15 16:09:16.387084.387084 lmp.py:1935]   Expert  9 |    124 | CPU
DEBUG 01-15 16:09:16.387826.387826 lmp.py:1935]   Expert 13 |    126 | CPU
DEBUG 01-15 16:09:16.387807.387807 lmp.py:1935]   Expert 34 |    129 | CPU
DEBUG 01-15 16:09:16.387788.387788 lmp.py:1935]   Expert 58 |    130 | CPU
DEBUG 01-15 16:09:16.387246.387246 lmp.py:1935]   Expert 50 |    131 | CPU
DEBUG 01-15 16:09:16.387465.387465 lmp.py:1935]   Expert 11 |    135 | CPU
DEBUG 01-15 16:09:16.387684.387684 lmp.py:1935]   Expert 26 |    136 | CPU
DEBUG 01-15 16:09:16.387440.387440 lmp.py:1935]   Expert 18 |    138 | CPU
DEBUG 01-15 16:09:16.387752.387752 lmp.py:1935]   Expert 31 |    138 | CPU
DEBUG 01-15 16:09:16.388395.388395 lmp.py:1935]   Expert 59 |    139 | CPU
DEBUG 01-15 16:09:16.388800.388800 lmp.py:1935]   Expert 40 |    143 | CPU
DEBUG 01-15 16:09:16.388251.388251 lmp.py:1935]   Expert 12 |    145 | CPU
DEBUG 01-15 16:09:16.388463.388463 lmp.py:1935]   Expert 46 |    149 | CPU
DEBUG 01-15 16:09:16.388437.388437 lmp.py:1935]   Expert  2 |    152 | CPU
DEBUG 01-15 16:09:16.388411.388411 lmp.py:1935]   Expert 48 |    152 | CPU
DEBUG 01-15 16:09:16.388385.388385 lmp.py:1935]   Expert 56 |    152 | CPU
DEBUG 01-15 16:09:16.388743.388743 lmp.py:1935]   Expert  4 |    153 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388148.388148 lmp.py:1935]   Expert 20 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388221.388221 lmp.py:1935]   Expert 61 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388580.388580 lmp.py:1935]   Expert 33 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388176.388176 lmp.py:1935]   Expert 35 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388581.388581 lmp.py:1935]   Expert 10 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388509.388509 lmp.py:1935]   Expert 55 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388436.388436 lmp.py:1935]   Expert 51 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388364.388364 lmp.py:1935]   Expert 36 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388053.388053 lmp.py:1935]   Expert  8 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388981.388981 lmp.py:1935]   Expert 52 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388670.388670 lmp.py:1935]   Expert 37 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388598.388598 lmp.py:1935]   Expert  0 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388526.388526 lmp.py:1935]   Expert 57 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388407.388407 lmp.py:1935]   Expert 39 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388348.388348 lmp.py:1935]   Expert 25 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388422.388422 lmp.py:1935]   Expert 62 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388873.388873 lmp.py:1935]   Expert 38 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388039.388039 lmp.py:1935]   Expert  7 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388967.388967 lmp.py:1935]   Expert  3 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388656.388656 lmp.py:1935]   Expert 24 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388107.388107 lmp.py:1935]   Expert 27 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388558.388558 lmp.py:1935]   Expert 28 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388247.388247 lmp.py:1935]   Expert 21 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388936.388936 lmp.py:1935]   Expert 60 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388626.388626 lmp.py:1935]   Expert 49 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388315.388315 lmp.py:1935]   Expert 16 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388296.388296 lmp.py:1935]   Expert 43 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388654.388654 lmp.py:1935]   Expert 23 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388774.388774 lmp.py:1935]   Expert 29 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388178.388178 lmp.py:1935]   Expert 47 |    293 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388868.388868 lmp.py:1935]   Expert 22 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388319.388319 lmp.py:1935]   Expert 15 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388531.388531 lmp.py:1935]   Expert 41 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388982.388982 lmp.py:1935]   Expert 44 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388433.388433 lmp.py:1935]   Expert 54 |    355 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388361.388361 lmp.py:1935]   Expert 14 |    376 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388811.388811 lmp.py:1935]   Expert 17 |    405 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.388262.388262 lmp.py:1935]   Expert 45 |    451 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.388760.388760 lmp.py:1937] 
DEBUG 01-15 16:09:16.388760.388760 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.388449.388449 lmp.py:1938]   CPU:   2747 tokens
DEBUG 01-15 16:09:16.388045.388045 lmp.py:1942]   cuda:1:   4844 tokens (20 experts)
DEBUG 01-15 16:09:16.388404.388404 lmp.py:1942]   cuda:2:   4697 tokens (19 experts)
DEBUG 01-15 16:09:16.388616.388616 lmp.py:1943]   Total GPU:   9541 tokens
DEBUG 01-15 16:09:16.388829.388829 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.388829.388829 lmp.py:1944] 
DEBUG 01-15 16:09:16.388002.388002 cuda_h.py:19] end experts_map_get cost 0.0017681121826171875 seconds
DEBUG 01-15 16:09:16.388990.388990 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.388316.388316 lmp.py:1953] 
DEBUG 01-15 16:09:16.388316.388316 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.389682.389682 cuda_h.py:19] end cpu_experts_submit cost 5.888938903808594e-05 seconds
DEBUG 01-15 16:09:16.389147.389147 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.389513.389513 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.390705.390705 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.13.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.13.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.13.mlp.experts.62.up_proj.weight': 340262912}, 2: {'model.layers.13.mlp.experts.3.gate_proj.weight': 0, 'model.layers.13.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.14.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.14.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.14.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.61.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16217276416, 5767168, 17301504, 0), (16223043584, 5767168, 23068672, 0), (16211509248, 5767168, 28835840, 0), (16286482432, 5767168, 34603008, 0), (16292249600, 5767168, 40370176, 0), (16280715264, 5767168, 46137344, 0), (16321085440, 5767168, 51904512, 0), (16326852608, 5767168, 57671680, 0), (16315318272, 5767168, 63438848, 0), (16494100480, 5767168, 69206016, 0), (16499867648, 5767168, 74973184, 0), (16488333312, 5767168, 80740352, 0), (16528703488, 5767168, 86507520, 0), (16534470656, 5767168, 92274688, 0), (16522936320, 5767168, 98041856, 0), (16546004992, 5767168, 103809024, 0), (16551772160, 5767168, 109576192, 0), (16540237824, 5767168, 115343360, 0), (16615211008, 5767168, 121110528, 0), (16620978176, 5767168, 126877696, 0), (16609443840, 5767168, 132644864, 0), (16632512512, 5767168, 138412032, 0), (16638279680, 5767168, 144179200, 0), (16626745344, 5767168, 149946368, 0), (16649814016, 5767168, 155713536, 0), (16655581184, 5767168, 161480704, 0), (16644046848, 5767168, 167247872, 0), (16719020032, 5767168, 173015040, 0), (16724787200, 5767168, 178782208, 0), (16713252864, 5767168, 184549376, 0), (16805527552, 5767168, 190316544, 0), (16811294720, 5767168, 196083712, 0), (16799760384, 5767168, 201850880, 0), (16857432064, 5767168, 207618048, 0), (16863199232, 5767168, 213385216, 0), (16851664896, 5767168, 219152384, 0), (16926638080, 5767168, 224919552, 0), (16932405248, 5767168, 230686720, 0), (16920870912, 5767168, 236453888, 0), (16995844096, 5767168, 242221056, 0), (17001611264, 5767168, 247988224, 0), (16990076928, 5767168, 253755392, 0), (17030447104, 5767168, 259522560, 0), (17036214272, 5767168, 265289728, 0), (17024679936, 5767168, 271056896, 0), (17082351616, 5767168, 276824064, 0), (17088118784, 5767168, 282591232, 0), (17076584448, 5767168, 288358400, 0), (17134256128, 5767168, 294125568, 0), (17140023296, 5767168, 299892736, 0), (17128488960, 5767168, 305659904, 0), (17186160640, 5767168, 311427072, 0), (17191927808, 5767168, 317194240, 0), (17180393472, 5767168, 322961408, 0), (17220763648, 5767168, 328728576, 0), (17226530816, 5767168, 334495744, 0), (17214996480, 5767168, 340262912, 0)], 2: [(16199974912, 5767168, 0, 0), (16205742080, 5767168, 5767168, 0), (16194207744, 5767168, 11534336, 0), (16269180928, 5767168, 17301504, 0), (16274948096, 5767168, 23068672, 0), (16263413760, 5767168, 28835840, 0), (16390291456, 5767168, 34603008, 0), (16396058624, 5767168, 40370176, 0), (16384524288, 5767168, 46137344, 0), (16407592960, 5767168, 51904512, 0), (16413360128, 5767168, 57671680, 0), (16401825792, 5767168, 63438848, 0), (16424894464, 5767168, 69206016, 0), (16430661632, 5767168, 74973184, 0), (16419127296, 5767168, 80740352, 0), (16442195968, 5767168, 86507520, 0), (16447963136, 5767168, 92274688, 0), (16436428800, 5767168, 98041856, 0), (16511401984, 5767168, 103809024, 0), (16517169152, 5767168, 109576192, 0), (16505634816, 5767168, 115343360, 0), (16563306496, 5767168, 121110528, 0), (16569073664, 5767168, 126877696, 0), (16557539328, 5767168, 132644864, 0), (16580608000, 5767168, 138412032, 0), (16586375168, 5767168, 144179200, 0), (16574840832, 5767168, 149946368, 0), (16753623040, 5767168, 155713536, 0), (16759390208, 5767168, 161480704, 0), (16747855872, 5767168, 167247872, 0), (16770924544, 5767168, 173015040, 0), (16776691712, 5767168, 178782208, 0), (16765157376, 5767168, 184549376, 0), (16788226048, 5767168, 190316544, 0), (16793993216, 5767168, 196083712, 0), (16782458880, 5767168, 201850880, 0), (16822829056, 5767168, 207618048, 0), (16828596224, 5767168, 213385216, 0), (16817061888, 5767168, 219152384, 0), (16892035072, 5767168, 224919552, 0), (16897802240, 5767168, 230686720, 0), (16886267904, 5767168, 236453888, 0), (16909336576, 5767168, 242221056, 0), (16915103744, 5767168, 247988224, 0), (16903569408, 5767168, 253755392, 0), (16961241088, 5767168, 259522560, 0), (16967008256, 5767168, 265289728, 0), (16955473920, 5767168, 271056896, 0), (17047748608, 5767168, 276824064, 0), (17053515776, 5767168, 282591232, 0), (17041981440, 5767168, 288358400, 0), (17099653120, 5767168, 294125568, 0), (17105420288, 5767168, 299892736, 0), (17093885952, 5767168, 305659904, 0), (17203462144, 5767168, 311427072, 0), (17209229312, 5767168, 317194240, 0), (17197694976, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6785de310>, 2: <capsule object NULL at 0x74a6785de340>}
DEBUG 01-15 16:09:16.391964.391964 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.391707.391707 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.391702.391702 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 841f2190-6f9d-4229-9ca9-c157da07d152
DEBUG 01-15 16:09:16.391808.391808 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.391018.391018 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:16.391050.391050 client.py:127] Model loaded
DEBUG 01-15 16:09:16.391018.391018 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.392955.392955 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.392507.392507 cuda_h.py:19] end restore2model cost 0.0003287792205810547 seconds
DEBUG 01-15 16:09:16.392131.392131 cuda_h.py:19] end sllm_worker_task cost 0.010641813278198242 seconds
INFO 01-15 16:09:16.392295.392295 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 841f2190-6f9d-4229-9ca9-c157da07d152
DEBUG 01-15 16:09:16.392886.392886 cuda_h.py:19] end move_flatidxs cost 0.0008399486541748047 seconds
DEBUG 01-15 16:09:16.393708.393708 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.393001.393001 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003875732421875 seconds
DEBUG 01-15 16:09:16.393448.393448 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.396715.396715 cuda_h.py:19] end restore2model cost 0.003009796142578125 seconds
DEBUG 01-15 16:09:16.396651.396651 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0071239471435546875 seconds
DEBUG 01-15 16:09:16.396731.396731 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.396444.396444 cuda_h.py:19] end gpu_sexperts cost 0.0002841949462890625 seconds
DEBUG 01-15 16:09:16.396843.396843 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.396473.396473 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:09:16.396646.396646 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.396634.396634 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.397177.397177 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009679794311523438 seconds
DEBUG 01-15 16:09:16.397120.397120 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.398267.398267 cuda_h.py:19] end gpu_group_list cost 0.00022125244140625 seconds
DEBUG 01-15 16:09:16.398739.398739 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.399252.399252 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010144710540771484 seconds
DEBUG 01-15 16:09:16.400983.400983 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.400832.400832 cuda_h.py:19] end gpu_group_list cost 0.000209808349609375 seconds
DEBUG 01-15 16:09:16.400517.400517 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.401022.401022 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 841f2190-6f9d-4229-9ca9-c157da07d152
DEBUG 01-15 16:09:16.401521.401521 cuda_h.py:19] end group_tensors cost 0.00859832763671875 seconds
DEBUG 01-15 16:09:16.402275.402275 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.406258.406258 cuda_h.py:19] end group pad cost 0.0035889148712158203 seconds
DEBUG 01-15 16:09:16.406140.406140 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:16.437812.437812 cuda_h.py:19] end group_einsum cost 0.030848264694213867 seconds
DEBUG 01-15 16:09:16.437347.437347 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:16.437379.437379 client.py:127] Model loaded
DEBUG 01-15 16:09:16.437702.437702 cuda_h.py:19] end wait_experts_multi_device cost 0.03650641441345215 seconds
DEBUG 01-15 16:09:16.437082.437082 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.440290.440290 cuda_h.py:19] end get_outputs_cpu1 cost 0.002986907958984375 seconds
DEBUG 01-15 16:09:16.441940.441940 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04924654960632324 seconds
DEBUG 01-15 16:09:16.441948.441948 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00418853759765625 seconds
DEBUG 01-15 16:09:16.442278.442278 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.444020.444020 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.444840.444840 cuda_h.py:19] end index_scatter cost 0.00020933151245117188 seconds
DEBUG 01-15 16:09:16.445712.445712 cuda_h.py:19] end cpuoutputsdeal cost 0.002959012985229492 seconds
DEBUG 01-15 16:09:16.445989.445989 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.445965.445965 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.445309.445309 cuda_h.py:19] end gpu_group_tensor cost 0.00031113624572753906 seconds
DEBUG 01-15 16:09:16.445240.445240 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.446113.446113 cuda_h.py:19] end gpu_group_tensor cost 0.00028514862060546875 seconds
DEBUG 01-15 16:09:16.446749.446749 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.447494.447494 cuda_h.py:19] end gpu_group_einsum cost 0.001171112060546875 seconds
DEBUG 01-15 16:09:16.447609.447609 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.448761.448761 cuda_h.py:19] end gpu_group_einsum cost 0.0008709430694580078 seconds
DEBUG 01-15 16:09:16.449386.449386 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.449453.449453 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.450524.450524 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004456043243408203 seconds
DEBUG 01-15 16:09:16.450348.450348 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.450182.450182 cuda_h.py:19] end concat_expert_out cost 0.00011706352233886719 seconds
DEBUG 01-15 16:09:16.450386.450386 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.450264.450264 cuda_h.py:19] end index_scatter cost 6.866455078125e-05 seconds
DEBUG 01-15 16:09:16.451601.451601 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0017480850219726562 seconds
DEBUG 01-15 16:09:16.451774.451774 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.451572.451572 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.451003.451003 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017833709716796875 seconds
DEBUG 01-15 16:09:16.451713.451713 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.451716.451716 cuda_h.py:19] end concat_expert_out cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:09:16.451619.451619 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.451915.451915 cuda_h.py:19] end index_scatter cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:09:16.452393.452393 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006237030029296875 seconds
DEBUG 01-15 16:09:16.452151.452151 cuda_h.py:19] end gpu_experts_multi_device cost 0.055406808853149414 seconds
DEBUG 01-15 16:09:16.452114.452114 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.06579709053039551 seconds
DEBUG 01-15 16:09:16.452099.452099 cuda_h.py:19] end prefill_layer cost 0.07126379013061523 seconds
DEBUG 01-15 16:09:16.452280.452280 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:09:16.452745.452745 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.452448.452448 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:09:16.452011.452011 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:09:16.452721.452721 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:09:16.452386.452386 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 4.0531158447265625e-05 seconds
DEBUG 01-15 16:09:16.453950.453950 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 7.2479248046875e-05 seconds
DEBUG 01-15 16:09:16.453646.453646 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.453529.453529 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.453646.453646 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.453520.453520 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.453658.453658 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.454038.454038 cuda_h.py:19] end allocate_cuda_memory cost 0.0005359649658203125 seconds
DEBUG 01-15 16:09:16.454593.454593 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.454299.454299 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.454595.454595 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.454432.454432 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 27a02106-a0c3-40e2-b444-8134f56845c7
DEBUG 01-15 16:09:16.454018.454018 cuda_h.py:10] start self_attn
DEBUG 01-15 16:09:16.455356.455356 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:16.456062.456062 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 27a02106-a0c3-40e2-b444-8134f56845c7
DEBUG 01-15 16:09:16.456121.456121 cuda_h.py:19] end load_into_gpu_async cost 0.001992940902709961 seconds
DEBUG 01-15 16:09:16.456342.456342 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.456231.456231 cuda_h.py:19] end restore_tensors2 cost 0.00015354156494140625 seconds
DEBUG 01-15 16:09:16.456221.456221 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0034050941467285156 seconds
INFO 01-15 16:09:16.457795.457795 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 27a02106-a0c3-40e2-b444-8134f56845c7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.459794.459794 cuda_h.py:19] end self_attn cost 0.004086732864379883 seconds
DEBUG 01-15 16:09:16.459845.459845 cuda_h.py:19] end iln_self_attn_paln cost 0.006505727767944336 seconds
DEBUG 01-15 16:09:16.459674.459674 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:09:16.459576.459576 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.460540.460540 cuda_h.py:19] end gate cost 0.0006771087646484375 seconds
DEBUG 01-15 16:09:16.460345.460345 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.460071.460071 lmp.py:1912] 
DEBUG 01-15 16:09:16.460071.460071 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.460926.460926 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.460291.460291 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.460795.460795 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.460015.460015 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.460373.460373 lmp.py:1917] 
DEBUG 01-15 16:09:16.460373.460373 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.460261.460261 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.460818.460818 lmp.py:1935]   Expert 34 |     27 | CPU
DEBUG 01-15 16:09:16.460938.460938 lmp.py:1935]   Expert  7 |     33 | CPU
DEBUG 01-15 16:09:16.460343.460343 lmp.py:1935]   Expert 13 |     42 | CPU
DEBUG 01-15 16:09:16.461270.461270 lmp.py:1935]   Expert 54 |     76 | CPU
DEBUG 01-15 16:09:16.461675.461675 lmp.py:1935]   Expert 18 |     83 | CPU
DEBUG 01-15 16:09:16.461841.461841 lmp.py:1935]   Expert 39 |     88 | CPU
DEBUG 01-15 16:09:16.461530.461530 lmp.py:1935]   Expert 49 |     89 | CPU
DEBUG 01-15 16:09:16.461220.461220 lmp.py:1935]   Expert 59 |    102 | CPU
DEBUG 01-15 16:09:16.461863.461863 lmp.py:1935]   Expert 16 |    108 | CPU
DEBUG 01-15 16:09:16.461029.461029 lmp.py:1935]   Expert  0 |    109 | CPU
DEBUG 01-15 16:09:16.461387.461387 lmp.py:1935]   Expert 21 |    109 | CPU
DEBUG 01-15 16:09:16.461553.461553 lmp.py:1935]   Expert 41 |    117 | CPU
DEBUG 01-15 16:09:16.461481.461481 lmp.py:1935]   Expert 15 |    118 | CPU
DEBUG 01-15 16:09:16.461932.461932 lmp.py:1935]   Expert 45 |    120 | CPU
DEBUG 01-15 16:09:16.461859.461859 lmp.py:1935]   Expert 22 |    122 | CPU
DEBUG 01-15 16:09:16.461549.461549 lmp.py:1935]   Expert 17 |    125 | CPU
DEBUG 01-15 16:09:16.461238.461238 lmp.py:1935]   Expert 52 |    135 | CPU
DEBUG 01-15 16:09:16.461404.461404 lmp.py:1935]   Expert  8 |    136 | CPU
DEBUG 01-15 16:09:16.461332.461332 lmp.py:1935]   Expert 61 |    136 | CPU
DEBUG 01-15 16:09:16.461021.461021 lmp.py:1935]   Expert 35 |    139 | CPU
DEBUG 01-15 16:09:16.461710.461710 lmp.py:1935]   Expert 38 |    140 | CPU
DEBUG 01-15 16:09:16.461400.461400 lmp.py:1935]   Expert 12 |    144 | CPU
DEBUG 01-15 16:09:16.461327.461327 lmp.py:1935]   Expert 48 |    149 | CPU
DEBUG 01-15 16:09:16.461255.461255 lmp.py:1935]   Expert 31 |    150 | CPU
DEBUG 01-15 16:09:16.461375.461375 lmp.py:1935]   Expert 36 |    154 | CPU
DEBUG 01-15 16:09:16.461833.461833 lmp.py:1935]   Expert 53 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461621.461621 lmp.py:1935]   Expert 50 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461980.461980 lmp.py:1935]   Expert 60 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461338.461338 lmp.py:1935]   Expert 40 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461219.461219 lmp.py:1935]   Expert 27 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461339.461339 lmp.py:1935]   Expert 19 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461459.461459 lmp.py:1935]   Expert  4 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461340.461340 lmp.py:1935]   Expert 29 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461983.461983 lmp.py:1935]   Expert 30 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461865.461865 lmp.py:1935]   Expert 11 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461508.461508 lmp.py:1935]   Expert 26 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461820.461820 lmp.py:1935]   Expert 20 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461701.461701 lmp.py:1935]   Expert 57 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461774.461774 lmp.py:1935]   Expert  6 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461894.461894 lmp.py:1935]   Expert 46 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461776.461776 lmp.py:1935]   Expert 43 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461180.461180 lmp.py:1935]   Expert  2 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461062.461062 lmp.py:1935]   Expert 23 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461420.461420 lmp.py:1935]   Expert 33 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461063.461063 lmp.py:1935]   Expert 42 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461706.461706 lmp.py:1935]   Expert 55 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461349.461349 lmp.py:1935]   Expert 56 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461753.461753 lmp.py:1935]   Expert 32 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461218.461218 lmp.py:1935]   Expert  3 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461291.461291 lmp.py:1935]   Expert  9 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461034.461034 lmp.py:1935]   Expert 14 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461915.461915 lmp.py:1935]   Expert 28 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461796.461796 lmp.py:1935]   Expert 51 |    275 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461631.461631 lmp.py:1935]   Expert  1 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461467.461467 lmp.py:1935]   Expert 58 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461540.461540 lmp.py:1935]   Expert 44 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.461614.461614 lmp.py:1935]   Expert 47 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.461925.461925 lmp.py:1935]   Expert 37 |    287 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.462761.462761 lmp.py:1935]   Expert 63 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.462596.462596 lmp.py:1935]   Expert 24 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.462431.462431 lmp.py:1935]   Expert 62 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.462266.462266 lmp.py:1935]   Expert 10 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.462339.462339 lmp.py:1935]   Expert 25 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.462221.462221 lmp.py:1935]   Expert  5 |    365 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.462148.462148 lmp.py:1937] 
DEBUG 01-15 16:09:16.462148.462148 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.462553.462553 lmp.py:1938]   CPU:   2751 tokens
DEBUG 01-15 16:09:16.462150.462150 lmp.py:1942]   cuda:1:   4843 tokens (20 experts)
DEBUG 01-15 16:09:16.462792.462792 lmp.py:1942]   cuda:2:   4694 tokens (19 experts)
DEBUG 01-15 16:09:16.462720.462720 lmp.py:1943]   Total GPU:   9537 tokens
DEBUG 01-15 16:09:16.462886.462886 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.462886.462886 lmp.py:1944] 
DEBUG 01-15 16:09:16.462013.462013 cuda_h.py:19] end experts_map_get cost 0.0017278194427490234 seconds
DEBUG 01-15 16:09:16.462578.462578 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.462619.462619 lmp.py:1953] 
DEBUG 01-15 16:09:16.462619.462619 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.462117.462117 cuda_h.py:19] end cpu_experts_submit cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:16.462429.462429 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.462689.462689 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.463561.463561 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.3.gate_proj.weight': 0, 'model.layers.14.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.30.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.30.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.30.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.40.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.40.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.40.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.51.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.51.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.51.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.14.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.14.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.14.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.14.mlp.experts.1.gate_proj.weight': 0, 'model.layers.14.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.28.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.28.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.28.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.32.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.32.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.32.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.57.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(17307271168, 5767168, 0, 0), (17313038336, 5767168, 5767168, 0), (17301504000, 5767168, 11534336, 0), (17324572672, 5767168, 17301504, 0), (17330339840, 5767168, 23068672, 0), (17318805504, 5767168, 28835840, 0), (17341874176, 5767168, 34603008, 0), (17347641344, 5767168, 40370176, 0), (17336107008, 5767168, 46137344, 0), (17359175680, 5767168, 51904512, 0), (17364942848, 5767168, 57671680, 0), (17353408512, 5767168, 63438848, 0), (17497587712, 5767168, 69206016, 0), (17503354880, 5767168, 74973184, 0), (17491820544, 5767168, 80740352, 0), (17601396736, 5767168, 86507520, 0), (17607163904, 5767168, 92274688, 0), (17595629568, 5767168, 98041856, 0), (17705205760, 5767168, 103809024, 0), (17710972928, 5767168, 109576192, 0), (17699438592, 5767168, 115343360, 0), (17774411776, 5767168, 121110528, 0), (17780178944, 5767168, 126877696, 0), (17768644608, 5767168, 132644864, 0), (17826316288, 5767168, 138412032, 0), (17832083456, 5767168, 144179200, 0), (17820549120, 5767168, 149946368, 0), (17947426816, 5767168, 155713536, 0), (17953193984, 5767168, 161480704, 0), (17941659648, 5767168, 167247872, 0), (17982029824, 5767168, 173015040, 0), (17987796992, 5767168, 178782208, 0), (17976262656, 5767168, 184549376, 0), (17999331328, 5767168, 190316544, 0), (18005098496, 5767168, 196083712, 0), (17993564160, 5767168, 201850880, 0), (18068537344, 5767168, 207618048, 0), (18074304512, 5767168, 213385216, 0), (18062770176, 5767168, 219152384, 0), (18137743360, 5767168, 224919552, 0), (18143510528, 5767168, 230686720, 0), (18131976192, 5767168, 236453888, 0), (18172346368, 5767168, 242221056, 0), (18178113536, 5767168, 247988224, 0), (18166579200, 5767168, 253755392, 0), (18224250880, 5767168, 259522560, 0), (18230018048, 5767168, 265289728, 0), (18218483712, 5767168, 271056896, 0), (18258853888, 5767168, 276824064, 0), (18264621056, 5767168, 282591232, 0), (18253086720, 5767168, 288358400, 0), (18293456896, 5767168, 294125568, 0), (18299224064, 5767168, 299892736, 0), (18287689728, 5767168, 305659904, 0), (18328059904, 5767168, 311427072, 0), (18333827072, 5767168, 317194240, 0), (18322292736, 5767168, 322961408, 0), (18345361408, 5767168, 328728576, 0), (18351128576, 5767168, 334495744, 0), (18339594240, 5767168, 340262912, 0)], 2: [(17272668160, 5767168, 0, 0), (17278435328, 5767168, 5767168, 0), (17266900992, 5767168, 11534336, 0), (17289969664, 5767168, 17301504, 0), (17295736832, 5767168, 23068672, 0), (17284202496, 5767168, 28835840, 0), (17411080192, 5767168, 34603008, 0), (17416847360, 5767168, 40370176, 0), (17405313024, 5767168, 46137344, 0), (17428381696, 5767168, 51904512, 0), (17434148864, 5767168, 57671680, 0), (17422614528, 5767168, 63438848, 0), (17445683200, 5767168, 69206016, 0), (17451450368, 5767168, 74973184, 0), (17439916032, 5767168, 80740352, 0), (17584095232, 5767168, 86507520, 0), (17589862400, 5767168, 92274688, 0), (17578328064, 5767168, 98041856, 0), (17653301248, 5767168, 103809024, 0), (17659068416, 5767168, 109576192, 0), (17647534080, 5767168, 115343360, 0), (17670602752, 5767168, 121110528, 0), (17676369920, 5767168, 126877696, 0), (17664835584, 5767168, 132644864, 0), (17687904256, 5767168, 138412032, 0), (17693671424, 5767168, 144179200, 0), (17682137088, 5767168, 149946368, 0), (17722507264, 5767168, 155713536, 0), (17728274432, 5767168, 161480704, 0), (17716740096, 5767168, 167247872, 0), (17739808768, 5767168, 173015040, 0), (17745575936, 5767168, 178782208, 0), (17734041600, 5767168, 184549376, 0), (17757110272, 5767168, 190316544, 0), (17762877440, 5767168, 196083712, 0), (17751343104, 5767168, 201850880, 0), (17809014784, 5767168, 207618048, 0), (17814781952, 5767168, 213385216, 0), (17803247616, 5767168, 219152384, 0), (17895522304, 5767168, 224919552, 0), (17901289472, 5767168, 230686720, 0), (17889755136, 5767168, 236453888, 0), (18016632832, 5767168, 242221056, 0), (18022400000, 5767168, 247988224, 0), (18010865664, 5767168, 253755392, 0), (18051235840, 5767168, 259522560, 0), (18057003008, 5767168, 265289728, 0), (18045468672, 5767168, 271056896, 0), (18120441856, 5767168, 276824064, 0), (18126209024, 5767168, 282591232, 0), (18114674688, 5767168, 288358400, 0), (18206949376, 5767168, 294125568, 0), (18212716544, 5767168, 299892736, 0), (18201182208, 5767168, 305659904, 0), (18241552384, 5767168, 311427072, 0), (18247319552, 5767168, 317194240, 0), (18235785216, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc24c090>, 2: <capsule object NULL at 0x74a6785de3a0>}
DEBUG 01-15 16:09:16.463090.463090 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.463841.463841 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.463928.463928 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 64b608c4-e616-4ad0-8b97-e1b180090163
DEBUG 01-15 16:09:16.463683.463683 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.463429.463429 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:16.464633.464633 client.py:127] Model loaded
DEBUG 01-15 16:09:16.464743.464743 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.464824.464824 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.465120.465120 cuda_h.py:19] end move_flatidxs cost 0.0008580684661865234 seconds
DEBUG 01-15 16:09:16.465903.465903 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.465078.465078 cuda_h.py:19] end restore2model cost 0.0010366439819335938 seconds
INFO 01-15 16:09:16.465354.465354 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 64b608c4-e616-4ad0-8b97-e1b180090163
DEBUG 01-15 16:09:16.465960.465960 cuda_h.py:19] end sllm_worker_task cost 0.012166738510131836 seconds
DEBUG 01-15 16:09:16.466535.466535 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036780834197998047 seconds
DEBUG 01-15 16:09:16.466945.466945 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.469348.469348 cuda_h.py:19] end restore2model cost 0.003105640411376953 seconds
DEBUG 01-15 16:09:16.469436.469436 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007112264633178711 seconds
DEBUG 01-15 16:09:16.469331.469331 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.469407.469407 cuda_h.py:19] end gpu_sexperts cost 0.0002713203430175781 seconds
DEBUG 01-15 16:09:16.469806.469806 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.469722.469722 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:09:16.469941.469941 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.469213.469213 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.471460.471460 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000993490219116211 seconds
DEBUG 01-15 16:09:16.471515.471515 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.470698.470698 cuda_h.py:19] end group_tensors cost 0.005315303802490234 seconds
DEBUG 01-15 16:09:16.471364.471364 cuda_h.py:19] end gpu_group_list cost 0.00021123886108398438 seconds
DEBUG 01-15 16:09:16.471147.471147 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.472978.472978 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.474470.474470 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015032291412353516 seconds
DEBUG 01-15 16:09:16.474050.474050 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.474956.474956 cuda_h.py:19] end gpu_group_list cost 0.0003161430358886719 seconds
DEBUG 01-15 16:09:16.474974.474974 cuda_h.py:19] end group pad cost 0.003470182418823242 seconds
DEBUG 01-15 16:09:16.474095.474095 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:16.476563.476563 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.477601.477601 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 64b608c4-e616-4ad0-8b97-e1b180090163
DEBUG 01-15 16:09:16.504574.504574 cuda_h.py:19] end group_einsum cost 0.029439449310302734 seconds
DEBUG 01-15 16:09:16.504784.504784 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.507036.507036 cuda_h.py:19] end get_outputs_cpu1 cost 0.003108501434326172 seconds
DEBUG 01-15 16:09:16.508761.508761 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.044658660888671875 seconds
INFO 01-15 16:09:16.511840.511840 client.py:127] Model loaded
DEBUG 01-15 16:09:16.511109.511109 cuda_h.py:19] end wait_experts_multi_device cost 0.033840179443359375 seconds
DEBUG 01-15 16:09:16.511110.511110 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.511898.511898 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005424022674560547 seconds
DEBUG 01-15 16:09:16.511595.511595 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.512589.512589 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.513330.513330 cuda_h.py:19] end index_scatter cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:09:16.513360.513360 cuda_h.py:19] end cpuoutputsdeal cost 0.0014348030090332031 seconds
DEBUG 01-15 16:09:16.513077.513077 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.513410.513410 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.513654.513654 cuda_h.py:19] end gpu_group_tensor cost 0.00014925003051757812 seconds
DEBUG 01-15 16:09:16.513986.513986 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.513409.513409 cuda_h.py:19] end gpu_group_tensor cost 0.0001385211944580078 seconds
DEBUG 01-15 16:09:16.513022.513022 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.514582.514582 cuda_h.py:19] end gpu_group_einsum cost 0.0004711151123046875 seconds
DEBUG 01-15 16:09:16.514123.514123 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.515285.515285 cuda_h.py:19] end gpu_group_einsum cost 0.0004544258117675781 seconds
DEBUG 01-15 16:09:16.515608.515608 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.515386.515386 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.515244.515244 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023412704467773438 seconds
DEBUG 01-15 16:09:16.515775.515775 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.515003.515003 cuda_h.py:19] end concat_expert_out cost 6.0558319091796875e-05 seconds
DEBUG 01-15 16:09:16.515608.515608 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.515723.515723 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:16.516142.516142 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008664131164550781 seconds
DEBUG 01-15 16:09:16.516026.516026 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.516253.516253 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.516379.516379 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016546249389648438 seconds
DEBUG 01-15 16:09:16.516611.516611 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.516965.516965 cuda_h.py:19] end concat_expert_out cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:09:16.516994.516994 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.516294.516294 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:09:16.516627.516627 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005137920379638672 seconds
DEBUG 01-15 16:09:16.516914.516914 cuda_h.py:19] end gpu_experts_multi_device cost 0.04683375358581543 seconds
DEBUG 01-15 16:09:16.516585.516585 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.057242631912231445 seconds
DEBUG 01-15 16:09:16.517778.517778 cuda_h.py:19] end prefill_layer cost 0.06446027755737305 seconds
DEBUG 01-15 16:09:16.517753.517753 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:09:16.517741.517741 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.517398.517398 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:09:16.517293.517293 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:09:16.517287.517287 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:09:16.517614.517614 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:09:16.517986.517986 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 6.914138793945312e-05 seconds
DEBUG 01-15 16:09:16.517874.517874 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.517936.517936 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.517277.517277 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.517827.517827 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.518362.518362 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.518233.518233 cuda_h.py:19] end allocate_cuda_memory cost 0.0004036426544189453 seconds
DEBUG 01-15 16:09:16.518443.518443 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.518030.518030 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.519776.519776 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.519283.519283 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b4ad1e37-0f6a-4e5a-bd66-e0627ece729a
DEBUG 01-15 16:09:16.519693.519693 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.519350.519350 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.520091.520091 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b4ad1e37-0f6a-4e5a-bd66-e0627ece729a
DEBUG 01-15 16:09:16.520785.520785 cuda_h.py:19] end load_into_gpu_async cost 0.0017821788787841797 seconds
DEBUG 01-15 16:09:16.520576.520576 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.521935.521935 cuda_h.py:19] end restore_tensors2 cost 0.0001499652862548828 seconds
DEBUG 01-15 16:09:16.521985.521985 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0030252933502197266 seconds
INFO 01-15 16:09:16.521791.521791 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b4ad1e37-0f6a-4e5a-bd66-e0627ece729a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.523083.523083 cuda_h.py:19] end self_attn cost 0.003622770309448242 seconds
DEBUG 01-15 16:09:16.523372.523372 cuda_h.py:19] end iln_self_attn_paln cost 0.005953073501586914 seconds
DEBUG 01-15 16:09:16.523341.523341 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:09:16.523858.523858 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.524734.524734 cuda_h.py:19] end gate cost 0.0006139278411865234 seconds
DEBUG 01-15 16:09:16.524802.524802 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.524423.524423 lmp.py:1912] 
DEBUG 01-15 16:09:16.524423.524423 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.524186.524186 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.524935.524935 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.524201.524201 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.524466.524466 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.524824.524824 lmp.py:1917] 
DEBUG 01-15 16:09:16.524824.524824 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.524375.524375 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.524455.524455 lmp.py:1935]   Expert 15 |     68 | CPU
DEBUG 01-15 16:09:16.524575.524575 lmp.py:1935]   Expert 41 |     70 | CPU
DEBUG 01-15 16:09:16.524456.524456 lmp.py:1935]   Expert  0 |     74 | CPU
DEBUG 01-15 16:09:16.524576.524576 lmp.py:1935]   Expert 63 |     77 | CPU
DEBUG 01-15 16:09:16.524219.524219 lmp.py:1935]   Expert 20 |     83 | CPU
DEBUG 01-15 16:09:16.524100.524100 lmp.py:1935]   Expert 45 |     89 | CPU
DEBUG 01-15 16:09:16.524028.524028 lmp.py:1935]   Expert  7 |     92 | CPU
DEBUG 01-15 16:09:16.525671.525671 lmp.py:1935]   Expert 28 |     99 | CPU
DEBUG 01-15 16:09:16.525268.525268 lmp.py:1935]   Expert 12 |    106 | CPU
DEBUG 01-15 16:09:16.525341.525341 lmp.py:1935]   Expert 54 |    106 | CPU
DEBUG 01-15 16:09:16.525938.525938 lmp.py:1935]   Expert 52 |    118 | CPU
DEBUG 01-15 16:09:16.525342.525342 lmp.py:1935]   Expert 40 |    120 | CPU
DEBUG 01-15 16:09:16.525337.525337 lmp.py:1935]   Expert  5 |    123 | CPU
DEBUG 01-15 16:09:16.525695.525695 lmp.py:1935]   Expert 59 |    123 | CPU
DEBUG 01-15 16:09:16.525861.525861 lmp.py:1935]   Expert  4 |    129 | CPU
DEBUG 01-15 16:09:16.525027.525027 lmp.py:1935]   Expert 34 |    134 | CPU
DEBUG 01-15 16:09:16.525432.525432 lmp.py:1935]   Expert 61 |    135 | CPU
DEBUG 01-15 16:09:16.525075.525075 lmp.py:1935]   Expert 62 |    135 | CPU
DEBUG 01-15 16:09:16.525956.525956 lmp.py:1935]   Expert 21 |    137 | CPU
DEBUG 01-15 16:09:16.525076.525076 lmp.py:1935]   Expert 55 |    137 | CPU
DEBUG 01-15 16:09:16.525196.525196 lmp.py:1935]   Expert 13 |    139 | CPU
DEBUG 01-15 16:09:16.525269.525269 lmp.py:1935]   Expert 42 |    141 | CPU
DEBUG 01-15 16:09:16.525628.525628 lmp.py:1935]   Expert 14 |    148 | CPU
DEBUG 01-15 16:09:16.525271.525271 lmp.py:1935]   Expert 10 |    149 | CPU
DEBUG 01-15 16:09:16.525960.525960 lmp.py:1935]   Expert 22 |    149 | CPU
DEBUG 01-15 16:09:16.525749.525749 lmp.py:1935]   Expert 32 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525584.525584 lmp.py:1935]   Expert 51 |    156 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525657.525657 lmp.py:1935]   Expert 25 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525015.525015 lmp.py:1935]   Expert  1 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525612.525612 lmp.py:1935]   Expert 47 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525447.525447 lmp.py:1935]   Expert 53 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525805.525805 lmp.py:1935]   Expert 19 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525117.525117 lmp.py:1935]   Expert 26 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525668.525668 lmp.py:1935]   Expert  6 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525456.525456 lmp.py:1935]   Expert 50 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525053.525053 lmp.py:1935]   Expert  2 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525411.525411 lmp.py:1935]   Expert 11 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525531.525531 lmp.py:1935]   Expert 30 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525903.525903 lmp.py:1935]   Expert 35 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525546.525546 lmp.py:1935]   Expert 56 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525950.525950 lmp.py:1935]   Expert 57 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525832.525832 lmp.py:1935]   Expert 48 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525475.525475 lmp.py:1935]   Expert 44 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525118.525118 lmp.py:1935]   Expert 24 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525999.525999 lmp.py:1935]   Expert 16 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525596.525596 lmp.py:1935]   Expert 46 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525477.525477 lmp.py:1935]   Expert 18 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525882.525882 lmp.py:1935]   Expert 39 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525048.525048 lmp.py:1935]   Expert 29 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525691.525691 lmp.py:1935]   Expert 37 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525857.525857 lmp.py:1935]   Expert 31 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525500.525500 lmp.py:1935]   Expert 36 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525666.525666 lmp.py:1935]   Expert 60 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525071.525071 lmp.py:1935]   Expert  3 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525475.525475 lmp.py:1935]   Expert 38 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525833.525833 lmp.py:1935]   Expert  9 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525669.525669 lmp.py:1935]   Expert 17 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525504.525504 lmp.py:1935]   Expert 23 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525862.525862 lmp.py:1935]   Expert 27 |    347 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.525505.525505 lmp.py:1935]   Expert 43 |    364 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.525148.525148 lmp.py:1935]   Expert  8 |    397 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.526791.526791 lmp.py:1935]   Expert 33 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.526672.526672 lmp.py:1935]   Expert 58 |    447 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.526077.526077 lmp.py:1935]   Expert 49 |    542 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.526289.526289 lmp.py:1937] 
DEBUG 01-15 16:09:16.526289.526289 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.526694.526694 lmp.py:1938]   CPU:   2881 tokens
DEBUG 01-15 16:09:16.526814.526814 lmp.py:1942]   cuda:1:   4631 tokens (19 experts)
DEBUG 01-15 16:09:16.526649.526649 lmp.py:1942]   cuda:2:   4776 tokens (20 experts)
DEBUG 01-15 16:09:16.526007.526007 lmp.py:1943]   Total GPU:   9407 tokens
DEBUG 01-15 16:09:16.526365.526365 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.526365.526365 lmp.py:1944] 
DEBUG 01-15 16:09:16.526253.526253 cuda_h.py:19] end experts_map_get cost 0.0017669200897216797 seconds
DEBUG 01-15 16:09:16.526673.526673 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.526283.526283 lmp.py:1953] 
DEBUG 01-15 16:09:16.526283.526283 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.526258.526258 cuda_h.py:19] end cpu_experts_submit cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:09:16.526000.526000 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.526214.526214 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.527315.527315 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.6.gate_proj.weight': 0, 'model.layers.15.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.60.up_proj.weight': 322961408}, 2: {'model.layers.15.mlp.experts.1.gate_proj.weight': 0, 'model.layers.15.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.50.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.50.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.50.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.15.mlp.experts.58.gate_proj.weight': 328728576, 'model.layers.15.mlp.experts.58.down_proj.weight': 334495744, 'model.layers.15.mlp.experts.58.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(18466471936, 5767168, 0, 0), (18472239104, 5767168, 5767168, 0), (18460704768, 5767168, 11534336, 0), (18501074944, 5767168, 17301504, 0), (18506842112, 5767168, 23068672, 0), (18495307776, 5767168, 28835840, 0), (18552979456, 5767168, 34603008, 0), (18558746624, 5767168, 40370176, 0), (18547212288, 5767168, 46137344, 0), (18639486976, 5767168, 51904512, 0), (18645254144, 5767168, 57671680, 0), (18633719808, 5767168, 63438848, 0), (18656788480, 5767168, 69206016, 0), (18662555648, 5767168, 74973184, 0), (18651021312, 5767168, 80740352, 0), (18691391488, 5767168, 86507520, 0), (18697158656, 5767168, 92274688, 0), (18685624320, 5767168, 98041856, 0), (18829803520, 5767168, 103809024, 0), (18835570688, 5767168, 109576192, 0), (18824036352, 5767168, 115343360, 0), (18864406528, 5767168, 121110528, 0), (18870173696, 5767168, 126877696, 0), (18858639360, 5767168, 132644864, 0), (18881708032, 5767168, 138412032, 0), (18887475200, 5767168, 144179200, 0), (18875940864, 5767168, 149946368, 0), (18899009536, 5767168, 155713536, 0), (18904776704, 5767168, 161480704, 0), (18893242368, 5767168, 167247872, 0), (19020120064, 5767168, 173015040, 0), (19025887232, 5767168, 178782208, 0), (19014352896, 5767168, 184549376, 0), (19037421568, 5767168, 190316544, 0), (19043188736, 5767168, 196083712, 0), (19031654400, 5767168, 201850880, 0), (19123929088, 5767168, 207618048, 0), (19129696256, 5767168, 213385216, 0), (19118161920, 5767168, 219152384, 0), (19175833600, 5767168, 224919552, 0), (19181600768, 5767168, 230686720, 0), (19170066432, 5767168, 236453888, 0), (19210436608, 5767168, 242221056, 0), (19216203776, 5767168, 247988224, 0), (19204669440, 5767168, 253755392, 0), (19245039616, 5767168, 259522560, 0), (19250806784, 5767168, 265289728, 0), (19239272448, 5767168, 271056896, 0), (19279642624, 5767168, 276824064, 0), (19285409792, 5767168, 282591232, 0), (19273875456, 5767168, 288358400, 0), (19331547136, 5767168, 294125568, 0), (19337314304, 5767168, 299892736, 0), (19325779968, 5767168, 305659904, 0), (19400753152, 5767168, 311427072, 0), (19406520320, 5767168, 317194240, 0), (19394985984, 5767168, 322961408, 0)], 2: [(18379964416, 5767168, 0, 0), (18385731584, 5767168, 5767168, 0), (18374197248, 5767168, 11534336, 0), (18397265920, 5767168, 17301504, 0), (18403033088, 5767168, 23068672, 0), (18391498752, 5767168, 28835840, 0), (18414567424, 5767168, 34603008, 0), (18420334592, 5767168, 40370176, 0), (18408800256, 5767168, 46137344, 0), (18518376448, 5767168, 51904512, 0), (18524143616, 5767168, 57671680, 0), (18512609280, 5767168, 63438848, 0), (18674089984, 5767168, 69206016, 0), (18679857152, 5767168, 74973184, 0), (18668322816, 5767168, 80740352, 0), (18760597504, 5767168, 86507520, 0), (18766364672, 5767168, 92274688, 0), (18754830336, 5767168, 98041856, 0), (18777899008, 5767168, 103809024, 0), (18783666176, 5767168, 109576192, 0), (18772131840, 5767168, 115343360, 0), (18795200512, 5767168, 121110528, 0), (18800967680, 5767168, 126877696, 0), (18789433344, 5767168, 132644864, 0), (18812502016, 5767168, 138412032, 0), (18818269184, 5767168, 144179200, 0), (18806734848, 5767168, 149946368, 0), (18916311040, 5767168, 155713536, 0), (18922078208, 5767168, 161480704, 0), (18910543872, 5767168, 167247872, 0), (18933612544, 5767168, 173015040, 0), (18939379712, 5767168, 178782208, 0), (18927845376, 5767168, 184549376, 0), (18968215552, 5767168, 190316544, 0), (18973982720, 5767168, 196083712, 0), (18962448384, 5767168, 201850880, 0), (18985517056, 5767168, 207618048, 0), (18991284224, 5767168, 213385216, 0), (18979749888, 5767168, 219152384, 0), (19002818560, 5767168, 224919552, 0), (19008585728, 5767168, 230686720, 0), (18997051392, 5767168, 236453888, 0), (19106627584, 5767168, 242221056, 0), (19112394752, 5767168, 247988224, 0), (19100860416, 5767168, 253755392, 0), (19158532096, 5767168, 259522560, 0), (19164299264, 5767168, 265289728, 0), (19152764928, 5767168, 271056896, 0), (19193135104, 5767168, 276824064, 0), (19198902272, 5767168, 282591232, 0), (19187367936, 5767168, 288358400, 0), (19227738112, 5767168, 294125568, 0), (19233505280, 5767168, 299892736, 0), (19221970944, 5767168, 305659904, 0), (19348848640, 5767168, 311427072, 0), (19354615808, 5767168, 317194240, 0), (19343081472, 5767168, 322961408, 0), (19366150144, 5767168, 328728576, 0), (19371917312, 5767168, 334495744, 0), (19360382976, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814575b30>, 2: <capsule object NULL at 0x74a81469c210>}
DEBUG 01-15 16:09:16.528449.528449 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.528093.528093 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.528657.528657 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ca45a383-7404-4546-9bfa-1210939b290a
DEBUG 01-15 16:09:16.528286.528286 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.528792.528792 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:16.528062.528062 client.py:127] Model loaded
DEBUG 01-15 16:09:16.528463.528463 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.528271.528271 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.529765.529765 cuda_h.py:19] end move_flatidxs cost 0.0008342266082763672 seconds
DEBUG 01-15 16:09:16.529826.529826 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.529434.529434 cuda_h.py:19] end restore2model cost 0.0010135173797607422 seconds
INFO 01-15 16:09:16.530227.530227 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ca45a383-7404-4546-9bfa-1210939b290a
DEBUG 01-15 16:09:16.530832.530832 cuda_h.py:19] end sllm_worker_task cost 0.012231826782226562 seconds
DEBUG 01-15 16:09:16.530957.530957 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0043392181396484375 seconds
DEBUG 01-15 16:09:16.530704.530704 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.534630.534630 cuda_h.py:19] end restore2model cost 0.0031058788299560547 seconds
DEBUG 01-15 16:09:16.534507.534507 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00779414176940918 seconds
DEBUG 01-15 16:09:16.534826.534826 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.534796.534796 cuda_h.py:19] end gpu_sexperts cost 0.0002646446228027344 seconds
DEBUG 01-15 16:09:16.534195.534195 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.534872.534872 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5020370483398438e-05 seconds
DEBUG 01-15 16:09:16.534853.534853 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.534125.534125 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.534398.534398 cuda_h.py:19] end group_tensors cost 0.004981517791748047 seconds
DEBUG 01-15 16:09:16.535627.535627 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.535065.535065 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009438991546630859 seconds
DEBUG 01-15 16:09:16.535644.535644 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.536811.536811 cuda_h.py:19] end gpu_group_list cost 0.00040268898010253906 seconds
DEBUG 01-15 16:09:16.537535.537535 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.538378.538378 cuda_h.py:19] end group pad cost 0.0031981468200683594 seconds
DEBUG 01-15 16:09:16.538075.538075 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:16.538616.538616 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001522064208984375 seconds
DEBUG 01-15 16:09:16.539685.539685 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.540856.540856 cuda_h.py:19] end gpu_group_list cost 0.000843048095703125 seconds
DEBUG 01-15 16:09:16.546646.546646 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.547177.547177 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ca45a383-7404-4546-9bfa-1210939b290a
DEBUG 01-15 16:09:16.567227.567227 cuda_h.py:19] end group_einsum cost 0.028782367706298828 seconds
DEBUG 01-15 16:09:16.567961.567961 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.571475.571475 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036165714263916016 seconds
DEBUG 01-15 16:09:16.572855.572855 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.043764352798461914 seconds
INFO 01-15 16:09:16.573519.573519 client.py:127] Model loaded
DEBUG 01-15 16:09:16.574934.574934 cuda_h.py:19] end wait_experts_multi_device cost 0.026716947555541992 seconds
DEBUG 01-15 16:09:16.574697.574697 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.574459.574459 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005598068237304688 seconds
DEBUG 01-15 16:09:16.574633.574633 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.575825.575825 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.575552.575552 cuda_h.py:19] end index_scatter cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:09:16.576396.576396 cuda_h.py:19] end cpuoutputsdeal cost 0.0013995170593261719 seconds
DEBUG 01-15 16:09:16.576644.576644 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.576367.576367 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.576880.576880 cuda_h.py:19] end gpu_group_tensor cost 0.0002384185791015625 seconds
DEBUG 01-15 16:09:16.576318.576318 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.576204.576204 cuda_h.py:19] end gpu_group_tensor cost 0.00013256072998046875 seconds
DEBUG 01-15 16:09:16.576625.576625 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.577165.577165 cuda_h.py:19] end gpu_group_einsum cost 0.0004582405090332031 seconds
DEBUG 01-15 16:09:16.577951.577951 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.577094.577094 cuda_h.py:19] end gpu_group_einsum cost 0.0004730224609375 seconds
DEBUG 01-15 16:09:16.578754.578754 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.578301.578301 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.578940.578940 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024819374084472656 seconds
DEBUG 01-15 16:09:16.578948.578948 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.578932.578932 cuda_h.py:19] end concat_expert_out cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:16.578583.578583 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.578175.578175 cuda_h.py:19] end index_scatter cost 5.340576171875e-05 seconds
DEBUG 01-15 16:09:16.579494.579494 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008766651153564453 seconds
DEBUG 01-15 16:09:16.579001.579001 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.579135.579135 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.579995.579995 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014638900756835938 seconds
DEBUG 01-15 16:09:16.579559.579559 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.579383.579383 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:09:16.579080.579080 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.579004.579004 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:09:16.579098.579098 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004909038543701172 seconds
DEBUG 01-15 16:09:16.579630.579630 cuda_h.py:19] end gpu_experts_multi_device cost 0.04514431953430176 seconds
DEBUG 01-15 16:09:16.579778.579778 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.05615878105163574 seconds
DEBUG 01-15 16:09:16.580971.580971 cuda_h.py:19] end prefill_layer cost 0.06281423568725586 seconds
DEBUG 01-15 16:09:16.580185.580185 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:09:16.580172.580172 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.580591.580591 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:09:16.580247.580247 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:09:16.580931.580931 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:09:16.580781.580781 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 3.695487976074219e-05 seconds
DEBUG 01-15 16:09:16.580868.580868 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 6.842613220214844e-05 seconds
DEBUG 01-15 16:09:16.580802.580802 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.580196.580196 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.580729.580729 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.580418.580418 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.581760.581760 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.581706.581706 cuda_h.py:19] end allocate_cuda_memory cost 0.0004305839538574219 seconds
DEBUG 01-15 16:09:16.581097.581097 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.581160.581160 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.582031.582031 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.582459.582459 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, efefc48b-83f4-49a1-95eb-d73a51a68daf
DEBUG 01-15 16:09:16.582850.582850 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.582605.582605 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.583636.583636 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, efefc48b-83f4-49a1-95eb-d73a51a68daf
DEBUG 01-15 16:09:16.583793.583793 cuda_h.py:19] end load_into_gpu_async cost 0.0016925334930419922 seconds
DEBUG 01-15 16:09:16.583644.583644 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.583877.583877 cuda_h.py:19] end restore_tensors2 cost 0.000148773193359375 seconds
DEBUG 01-15 16:09:16.584550.584550 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003041982650756836 seconds
INFO 01-15 16:09:16.584647.584647 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, efefc48b-83f4-49a1-95eb-d73a51a68daf
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.586183.586183 cuda_h.py:19] end self_attn cost 0.003719806671142578 seconds
DEBUG 01-15 16:09:16.586657.586657 cuda_h.py:19] end iln_self_attn_paln cost 0.006111621856689453 seconds
DEBUG 01-15 16:09:16.586341.586341 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:09:16.586766.586766 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.587523.587523 cuda_h.py:19] end gate cost 0.0006301403045654297 seconds
DEBUG 01-15 16:09:16.587068.587068 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.587827.587827 lmp.py:1912] 
DEBUG 01-15 16:09:16.587827.587827 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.587775.587775 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.587809.587809 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.587121.587121 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.587763.587763 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.587691.587691 lmp.py:1917] 
DEBUG 01-15 16:09:16.587691.587691 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.587672.587672 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.587991.587991 lmp.py:1935]   Expert 58 |     35 | CPU
DEBUG 01-15 16:09:16.587634.587634 lmp.py:1935]   Expert 31 |     60 | CPU
DEBUG 01-15 16:09:16.587515.587515 lmp.py:1935]   Expert 47 |     60 | CPU
DEBUG 01-15 16:09:16.588204.588204 lmp.py:1935]   Expert 49 |     60 | CPU
DEBUG 01-15 16:09:16.588371.588371 lmp.py:1935]   Expert  4 |     66 | CPU
DEBUG 01-15 16:09:16.588298.588298 lmp.py:1935]   Expert 38 |     70 | CPU
DEBUG 01-15 16:09:16.588226.588226 lmp.py:1935]   Expert 45 |     74 | CPU
DEBUG 01-15 16:09:16.588154.588154 lmp.py:1935]   Expert 41 |     83 | CPU
DEBUG 01-15 16:09:16.588081.588081 lmp.py:1935]   Expert 43 |     84 | CPU
DEBUG 01-15 16:09:16.588248.588248 lmp.py:1935]   Expert 33 |     96 | CPU
DEBUG 01-15 16:09:16.588652.588652 lmp.py:1935]   Expert 57 |    101 | CPU
DEBUG 01-15 16:09:16.588209.588209 lmp.py:1935]   Expert 50 |    102 | CPU
DEBUG 01-15 16:09:16.588614.588614 lmp.py:1935]   Expert 11 |    107 | CPU
DEBUG 01-15 16:09:16.588111.588111 lmp.py:1935]   Expert  2 |    113 | CPU
DEBUG 01-15 16:09:16.588847.588847 lmp.py:1935]   Expert 51 |    115 | CPU
DEBUG 01-15 16:09:16.588105.588105 lmp.py:1935]   Expert  0 |    123 | CPU
DEBUG 01-15 16:09:16.588603.588603 lmp.py:1935]   Expert 14 |    124 | CPU
DEBUG 01-15 16:09:16.588338.588338 lmp.py:1935]   Expert 54 |    128 | CPU
DEBUG 01-15 16:09:16.588835.588835 lmp.py:1935]   Expert 34 |    142 | CPU
DEBUG 01-15 16:09:16.588094.588094 lmp.py:1935]   Expert 56 |    143 | CPU
DEBUG 01-15 16:09:16.588115.588115 lmp.py:1935]   Expert 26 |    145 | CPU
DEBUG 01-15 16:09:16.588612.588612 lmp.py:1935]   Expert 27 |    152 | CPU
DEBUG 01-15 16:09:16.588632.588632 lmp.py:1935]   Expert 55 |    155 | CPU
DEBUG 01-15 16:09:16.588129.588129 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:09:16.588296.588296 lmp.py:1935]   Expert 25 |    163 | CPU
DEBUG 01-15 16:09:16.588938.588938 lmp.py:1935]   Expert 10 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588357.588357 lmp.py:1935]   Expert  9 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588000.588000 lmp.py:1935]   Expert 13 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588450.588450 lmp.py:1935]   Expert 61 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588140.588140 lmp.py:1935]   Expert  6 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588591.588591 lmp.py:1935]   Expert 48 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588280.588280 lmp.py:1935]   Expert  7 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588969.588969 lmp.py:1935]   Expert 46 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588182.588182 lmp.py:1935]   Expert 24 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588633.588633 lmp.py:1935]   Expert 42 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588083.588083 lmp.py:1935]   Expert 18 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588296.588296 lmp.py:1935]   Expert 40 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588985.588985 lmp.py:1935]   Expert 63 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588959.588959 lmp.py:1935]   Expert 12 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588602.588602 lmp.py:1935]   Expert 22 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588530.588530 lmp.py:1935]   Expert 29 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588411.588411 lmp.py:1935]   Expert 59 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588101.588101 lmp.py:1935]   Expert 21 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588313.588313 lmp.py:1935]   Expert 19 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588287.588287 lmp.py:1935]   Expert 32 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588738.588738 lmp.py:1935]   Expert 36 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588474.588474 lmp.py:1935]   Expert  3 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588686.588686 lmp.py:1935]   Expert 16 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588660.588660 lmp.py:1935]   Expert 37 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588873.588873 lmp.py:1935]   Expert  1 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588847.588847 lmp.py:1935]   Expert 20 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588298.588298 lmp.py:1935]   Expert  5 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588702.588702 lmp.py:1935]   Expert  8 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588584.588584 lmp.py:1935]   Expert 30 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588273.588273 lmp.py:1935]   Expert 62 |    273 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588485.588485 lmp.py:1935]   Expert 15 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588698.588698 lmp.py:1935]   Expert 39 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588910.588910 lmp.py:1935]   Expert 35 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588884.588884 lmp.py:1935]   Expert 17 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588335.588335 lmp.py:1935]   Expert 60 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588786.588786 lmp.py:1935]   Expert 52 |    348 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.588999.588999 lmp.py:1935]   Expert 23 |    364 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.588211.588211 lmp.py:1935]   Expert 44 |    380 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.589423.589423 lmp.py:1935]   Expert 53 |    436 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.589159.589159 lmp.py:1937] 
DEBUG 01-15 16:09:16.589159.589159 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.589564.589564 lmp.py:1938]   CPU:   2660 tokens
DEBUG 01-15 16:09:16.589491.589491 lmp.py:1942]   cuda:1:   4731 tokens (19 experts)
DEBUG 01-15 16:09:16.589896.589896 lmp.py:1942]   cuda:2:   4897 tokens (20 experts)
DEBUG 01-15 16:09:16.589393.589393 lmp.py:1943]   Total GPU:   9628 tokens
DEBUG 01-15 16:09:16.589413.589413 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.589413.589413 lmp.py:1944] 
DEBUG 01-15 16:09:16.589394.589394 cuda_h.py:19] end experts_map_get cost 0.0016171932220458984 seconds
DEBUG 01-15 16:09:16.589999.589999 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.589894.589894 lmp.py:1953] 
DEBUG 01-15 16:09:16.589894.589894 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.589869.589869 cuda_h.py:19] end cpu_experts_submit cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:09:16.589493.589493 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.589482.589482 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.590767.590767 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.3.gate_proj.weight': 0, 'model.layers.16.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.46.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.46.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.46.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.59.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.59.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.59.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.61.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.61.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.61.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.16.mlp.experts.1.gate_proj.weight': 0, 'model.layers.16.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.30.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.30.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.30.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.36.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.36.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.36.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.37.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.37.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.37.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.40.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.40.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.40.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.44.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.44.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.44.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.48.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.48.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.48.up_proj.weight': 322961408, 'model.layers.16.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.16.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.16.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(19521863680, 5767168, 0, 0), (19527630848, 5767168, 5767168, 0), (19516096512, 5767168, 11534336, 0), (19591069696, 5767168, 17301504, 0), (19596836864, 5767168, 23068672, 0), (19585302528, 5767168, 28835840, 0), (19608371200, 5767168, 34603008, 0), (19614138368, 5767168, 40370176, 0), (19602604032, 5767168, 46137344, 0), (19694878720, 5767168, 51904512, 0), (19700645888, 5767168, 57671680, 0), (19689111552, 5767168, 63438848, 0), (19746783232, 5767168, 69206016, 0), (19752550400, 5767168, 74973184, 0), (19741016064, 5767168, 80740352, 0), (19764084736, 5767168, 86507520, 0), (19769851904, 5767168, 92274688, 0), (19758317568, 5767168, 98041856, 0), (19815989248, 5767168, 103809024, 0), (19821756416, 5767168, 109576192, 0), (19810222080, 5767168, 115343360, 0), (19833290752, 5767168, 121110528, 0), (19839057920, 5767168, 126877696, 0), (19827523584, 5767168, 132644864, 0), (19850592256, 5767168, 138412032, 0), (19856359424, 5767168, 144179200, 0), (19844825088, 5767168, 149946368, 0), (20023607296, 5767168, 155713536, 0), (20029374464, 5767168, 161480704, 0), (20017840128, 5767168, 167247872, 0), (20144717824, 5767168, 173015040, 0), (20150484992, 5767168, 178782208, 0), (20138950656, 5767168, 184549376, 0), (20196622336, 5767168, 190316544, 0), (20202389504, 5767168, 196083712, 0), (20190855168, 5767168, 201850880, 0), (20265828352, 5767168, 207618048, 0), (20271595520, 5767168, 213385216, 0), (20260061184, 5767168, 219152384, 0), (20369637376, 5767168, 224919552, 0), (20375404544, 5767168, 230686720, 0), (20363870208, 5767168, 236453888, 0), (20386938880, 5767168, 242221056, 0), (20392706048, 5767168, 247988224, 0), (20381171712, 5767168, 253755392, 0), (20490747904, 5767168, 259522560, 0), (20496515072, 5767168, 265289728, 0), (20484980736, 5767168, 271056896, 0), (20525350912, 5767168, 276824064, 0), (20531118080, 5767168, 282591232, 0), (20519583744, 5767168, 288358400, 0), (20542652416, 5767168, 294125568, 0), (20548419584, 5767168, 299892736, 0), (20536885248, 5767168, 305659904, 0), (20559953920, 5767168, 311427072, 0), (20565721088, 5767168, 317194240, 0), (20554186752, 5767168, 322961408, 0)], 2: [(19487260672, 5767168, 0, 0), (19493027840, 5767168, 5767168, 0), (19481493504, 5767168, 11534336, 0), (19556466688, 5767168, 17301504, 0), (19562233856, 5767168, 23068672, 0), (19550699520, 5767168, 28835840, 0), (19573768192, 5767168, 34603008, 0), (19579535360, 5767168, 40370176, 0), (19568001024, 5767168, 46137344, 0), (19625672704, 5767168, 51904512, 0), (19631439872, 5767168, 57671680, 0), (19619905536, 5767168, 63438848, 0), (19642974208, 5767168, 69206016, 0), (19648741376, 5767168, 74973184, 0), (19637207040, 5767168, 80740352, 0), (19677577216, 5767168, 86507520, 0), (19683344384, 5767168, 92274688, 0), (19671810048, 5767168, 98041856, 0), (19729481728, 5767168, 103809024, 0), (19735248896, 5767168, 109576192, 0), (19723714560, 5767168, 115343360, 0), (19781386240, 5767168, 121110528, 0), (19787153408, 5767168, 126877696, 0), (19775619072, 5767168, 132644864, 0), (19798687744, 5767168, 138412032, 0), (19804454912, 5767168, 144179200, 0), (19792920576, 5767168, 149946368, 0), (19867893760, 5767168, 155713536, 0), (19873660928, 5767168, 161480704, 0), (19862126592, 5767168, 167247872, 0), (19885195264, 5767168, 173015040, 0), (19890962432, 5767168, 178782208, 0), (19879428096, 5767168, 184549376, 0), (19971702784, 5767168, 190316544, 0), (19977469952, 5767168, 196083712, 0), (19965935616, 5767168, 201850880, 0), (19989004288, 5767168, 207618048, 0), (19994771456, 5767168, 213385216, 0), (19983237120, 5767168, 219152384, 0), (20075511808, 5767168, 224919552, 0), (20081278976, 5767168, 230686720, 0), (20069744640, 5767168, 236453888, 0), (20092813312, 5767168, 242221056, 0), (20098580480, 5767168, 247988224, 0), (20087046144, 5767168, 253755392, 0), (20110114816, 5767168, 259522560, 0), (20115881984, 5767168, 265289728, 0), (20104347648, 5767168, 271056896, 0), (20162019328, 5767168, 276824064, 0), (20167786496, 5767168, 282591232, 0), (20156252160, 5767168, 288358400, 0), (20231225344, 5767168, 294125568, 0), (20236992512, 5767168, 299892736, 0), (20225458176, 5767168, 305659904, 0), (20300431360, 5767168, 311427072, 0), (20306198528, 5767168, 317194240, 0), (20294664192, 5767168, 322961408, 0), (20508049408, 5767168, 328728576, 0), (20513816576, 5767168, 334495744, 0), (20502282240, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a680302f10>, 2: <capsule object NULL at 0x74a688114870>}
DEBUG 01-15 16:09:16.591191.591191 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.591126.591126 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.591783.591783 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 518624a6-9176-41ee-8d15-4fdcbb7399ab
DEBUG 01-15 16:09:16.591221.591221 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.591344.591344 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:16.591587.591587 client.py:127] Model loaded
DEBUG 01-15 16:09:16.591208.591208 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.591923.591923 cuda_h.py:10] start restore2model
INFO 01-15 16:09:16.592367.592367 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 518624a6-9176-41ee-8d15-4fdcbb7399ab
DEBUG 01-15 16:09:16.592893.592893 cuda_h.py:19] end move_flatidxs cost 0.0008349418640136719 seconds
DEBUG 01-15 16:09:16.592670.592670 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.593403.593403 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0037374496459960938 seconds
DEBUG 01-15 16:09:16.593949.593949 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.594930.594930 cuda_h.py:19] end restore2model cost 0.000774383544921875 seconds
DEBUG 01-15 16:09:16.594632.594632 cuda_h.py:19] end sllm_worker_task cost 0.013335227966308594 seconds
DEBUG 01-15 16:09:16.597650.597650 cuda_h.py:19] end restore2model cost 0.0038831233978271484 seconds
DEBUG 01-15 16:09:16.597712.597712 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007889509201049805 seconds
DEBUG 01-15 16:09:16.597984.597984 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.597577.597577 cuda_h.py:19] end gpu_sexperts cost 0.0002658367156982422 seconds
DEBUG 01-15 16:09:16.597930.597930 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.597944.597944 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8596649169921875e-05 seconds
DEBUG 01-15 16:09:16.597640.597640 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.597582.597582 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.598019.598019 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009589195251464844 seconds
DEBUG 01-15 16:09:16.598313.598313 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.598354.598354 cuda_h.py:19] end gpu_group_list cost 0.0002105236053466797 seconds
DEBUG 01-15 16:09:16.599567.599567 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.600645.600645 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010776519775390625 seconds
DEBUG 01-15 16:09:16.601992.601992 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.601292.601292 cuda_h.py:19] end gpu_group_list cost 0.000225067138671875 seconds
DEBUG 01-15 16:09:16.602057.602057 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.602371.602371 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 518624a6-9176-41ee-8d15-4fdcbb7399ab
DEBUG 01-15 16:09:16.603841.603841 cuda_h.py:19] end group_tensors cost 0.010413646697998047 seconds
DEBUG 01-15 16:09:16.604900.604900 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.607008.607008 cuda_h.py:19] end group pad cost 0.0033614635467529297 seconds
DEBUG 01-15 16:09:16.607175.607175 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:16.632237.632237 client.py:127] Model loaded
DEBUG 01-15 16:09:16.632110.632110 cuda_h.py:19] end wait_experts_multi_device cost 0.0307619571685791 seconds
DEBUG 01-15 16:09:16.632349.632349 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.638708.638708 cuda_h.py:19] end group_einsum cost 0.03124380111694336 seconds
DEBUG 01-15 16:09:16.638011.638011 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.642470.642470 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029709339141845703 seconds
DEBUG 01-15 16:09:16.642213.642213 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05120849609375 seconds
DEBUG 01-15 16:09:16.643344.643344 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010830163955688477 seconds
DEBUG 01-15 16:09:16.644530.644530 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.646499.646499 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.646649.646649 cuda_h.py:19] end index_scatter cost 0.00015091896057128906 seconds
DEBUG 01-15 16:09:16.647767.647767 cuda_h.py:19] end cpuoutputsdeal cost 0.0030210018157958984 seconds
DEBUG 01-15 16:09:16.647375.647375 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.647636.647636 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.647020.647020 cuda_h.py:19] end gpu_group_tensor cost 0.00030612945556640625 seconds
DEBUG 01-15 16:09:16.647050.647050 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.648930.648930 cuda_h.py:19] end gpu_group_tensor cost 0.0002892017364501953 seconds
DEBUG 01-15 16:09:16.648309.648309 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.650513.650513 cuda_h.py:19] end gpu_group_einsum cost 0.0014204978942871094 seconds
DEBUG 01-15 16:09:16.650953.650953 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.651712.651712 cuda_h.py:19] end gpu_group_einsum cost 0.0006372928619384766 seconds
DEBUG 01-15 16:09:16.651008.651008 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.651185.651185 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.651360.651360 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025534629821777344 seconds
DEBUG 01-15 16:09:16.651706.651706 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.651219.651219 cuda_h.py:19] end concat_expert_out cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:09:16.651646.651646 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.651980.651980 cuda_h.py:19] end index_scatter cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:09:16.652757.652757 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009486675262451172 seconds
DEBUG 01-15 16:09:16.652800.652800 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.652081.652081 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.652445.652445 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001647472381591797 seconds
DEBUG 01-15 16:09:16.652439.652439 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.652462.652462 cuda_h.py:19] end concat_expert_out cost 5.6743621826171875e-05 seconds
DEBUG 01-15 16:09:16.652782.652782 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.652089.652089 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:16.652899.652899 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005266666412353516 seconds
DEBUG 01-15 16:09:16.652882.652882 cuda_h.py:19] end gpu_experts_multi_device cost 0.05524611473083496 seconds
DEBUG 01-15 16:09:16.653937.653937 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.06624579429626465 seconds
DEBUG 01-15 16:09:16.653412.653412 cuda_h.py:19] end prefill_layer cost 0.07317757606506348 seconds
DEBUG 01-15 16:09:16.653540.653540 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:09:16.653196.653196 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.653091.653091 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:09:16.653132.653132 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:09:16.653318.653318 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:09:16.653983.653983 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 4.029273986816406e-05 seconds
DEBUG 01-15 16:09:16.653593.653593 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:09:16.653289.653289 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.653835.653835 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.654998.654998 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.654925.654925 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.654824.654824 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.655640.655640 cuda_h.py:19] end allocate_cuda_memory cost 0.0005099773406982422 seconds
DEBUG 01-15 16:09:16.655149.655149 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.655212.655212 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.655243.655243 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.655418.655418 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e03bc7ac-e921-46f0-9003-6cb7f282cdc5
DEBUG 01-15 16:09:16.655969.655969 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.655275.655275 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.657967.657967 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e03bc7ac-e921-46f0-9003-6cb7f282cdc5
DEBUG 01-15 16:09:16.657554.657554 cuda_h.py:19] end load_into_gpu_async cost 0.002111196517944336 seconds
DEBUG 01-15 16:09:16.657008.657008 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.657009.657009 cuda_h.py:19] end restore_tensors2 cost 0.00015163421630859375 seconds
DEBUG 01-15 16:09:16.657013.657013 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0035142898559570312 seconds
INFO 01-15 16:09:16.657634.657634 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e03bc7ac-e921-46f0-9003-6cb7f282cdc5
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.659562.659562 cuda_h.py:19] end self_attn cost 0.0037572383880615234 seconds
DEBUG 01-15 16:09:16.660195.660195 cuda_h.py:19] end iln_self_attn_paln cost 0.006245136260986328 seconds
DEBUG 01-15 16:09:16.660640.660640 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:09:16.660635.660635 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.660665.660665 cuda_h.py:19] end gate cost 0.0006563663482666016 seconds
DEBUG 01-15 16:09:16.660878.660878 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.661095.661095 lmp.py:1912] 
DEBUG 01-15 16:09:16.661095.661095 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.661520.661520 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.661169.661169 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.661197.661197 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.661939.661939 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.661820.661820 lmp.py:1917] 
DEBUG 01-15 16:09:16.661820.661820 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.661609.661609 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.661689.661689 lmp.py:1935]   Expert  4 |      9 | CPU
DEBUG 01-15 16:09:16.661094.661094 lmp.py:1935]   Expert 28 |     26 | CPU
DEBUG 01-15 16:09:16.661022.661022 lmp.py:1935]   Expert  7 |     46 | CPU
DEBUG 01-15 16:09:16.661711.661711 lmp.py:1935]   Expert 53 |     57 | CPU
DEBUG 01-15 16:09:16.661162.661162 lmp.py:1935]   Expert 52 |     69 | CPU
DEBUG 01-15 16:09:16.661520.661520 lmp.py:1935]   Expert 43 |     72 | CPU
DEBUG 01-15 16:09:16.661402.661402 lmp.py:1935]   Expert 49 |     85 | CPU
DEBUG 01-15 16:09:16.661045.661045 lmp.py:1935]   Expert 12 |     89 | CPU
DEBUG 01-15 16:09:16.661449.661449 lmp.py:1935]   Expert 47 |    101 | CPU
DEBUG 01-15 16:09:16.661775.661775 lmp.py:1935]   Expert 24 |    106 | CPU
DEBUG 01-15 16:09:16.661517.661517 lmp.py:1935]   Expert 33 |    108 | CPU
DEBUG 01-15 16:09:16.661259.661259 lmp.py:1935]   Expert 15 |    109 | CPU
DEBUG 01-15 16:09:16.661902.661902 lmp.py:1935]   Expert  2 |    112 | CPU
DEBUG 01-15 16:09:16.661353.661353 lmp.py:1935]   Expert 50 |    113 | CPU
DEBUG 01-15 16:09:16.661281.661281 lmp.py:1935]   Expert 39 |    115 | CPU
DEBUG 01-15 16:09:16.661209.661209 lmp.py:1935]   Expert 60 |    115 | CPU
DEBUG 01-15 16:09:16.661249.661249 lmp.py:1935]   Expert 36 |    119 | CPU
DEBUG 01-15 16:09:16.661369.661369 lmp.py:1935]   Expert 25 |    121 | CPU
DEBUG 01-15 16:09:16.661774.661774 lmp.py:1935]   Expert  6 |    125 | CPU
DEBUG 01-15 16:09:16.661702.661702 lmp.py:1935]   Expert 61 |    132 | CPU
DEBUG 01-15 16:09:16.661821.661821 lmp.py:1935]   Expert 59 |    133 | CPU
DEBUG 01-15 16:09:16.661564.661564 lmp.py:1935]   Expert  3 |    141 | CPU
DEBUG 01-15 16:09:16.661968.661968 lmp.py:1935]   Expert 27 |    143 | CPU
DEBUG 01-15 16:09:16.661134.661134 lmp.py:1935]   Expert 58 |    143 | CPU
DEBUG 01-15 16:09:16.661539.661539 lmp.py:1935]   Expert  8 |    149 | CPU
DEBUG 01-15 16:09:16.661612.661612 lmp.py:1935]   Expert 31 |    150 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.661209.661209 lmp.py:1935]   Expert 30 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.661806.661806 lmp.py:1935]   Expert 10 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.661641.661641 lmp.py:1935]   Expert 40 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.661284.661284 lmp.py:1935]   Expert 57 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.661404.661404 lmp.py:1935]   Expert 14 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662285.662285 lmp.py:1935]   Expert 38 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662166.662166 lmp.py:1935]   Expert 41 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662478.662478 lmp.py:1935]   Expert 32 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662837.662837 lmp.py:1935]   Expert 37 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662195.662195 lmp.py:1935]   Expert 46 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662553.662553 lmp.py:1935]   Expert 54 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662958.662958 lmp.py:1935]   Expert 19 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662839.662839 lmp.py:1935]   Expert 42 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662197.662197 lmp.py:1935]   Expert 11 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662317.662317 lmp.py:1935]   Expert 22 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662675.662675 lmp.py:1935]   Expert 34 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662557.662557 lmp.py:1935]   Expert 18 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662676.662676 lmp.py:1935]   Expert 26 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662750.662750 lmp.py:1935]   Expert  0 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662631.662631 lmp.py:1935]   Expert 56 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662943.662943 lmp.py:1935]   Expert  1 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662162.662162 lmp.py:1935]   Expert 44 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662805.662805 lmp.py:1935]   Expert 51 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662925.662925 lmp.py:1935]   Expert 20 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662045.662045 lmp.py:1935]   Expert 29 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662688.662688 lmp.py:1935]   Expert 48 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662046.662046 lmp.py:1935]   Expert 45 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662689.662689 lmp.py:1935]   Expert 21 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662047.662047 lmp.py:1935]   Expert 35 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662690.662690 lmp.py:1935]   Expert 16 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662810.662810 lmp.py:1935]   Expert 55 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662361.662361 lmp.py:1935]   Expert  5 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662480.662480 lmp.py:1935]   Expert 23 |    376 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662792.662792 lmp.py:1935]   Expert 13 |    380 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662151.662151 lmp.py:1935]   Expert 17 |    435 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662794.662794 lmp.py:1935]   Expert  9 |    457 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662390.662390 lmp.py:1935]   Expert 63 |    461 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.662033.662033 lmp.py:1935]   Expert 62 |   1183 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.662199.662199 lmp.py:1937] 
DEBUG 01-15 16:09:16.662199.662199 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.662081.662081 lmp.py:1938]   CPU:   2538 tokens
DEBUG 01-15 16:09:16.662916.662916 lmp.py:1942]   cuda:1:   4912 tokens (19 experts)
DEBUG 01-15 16:09:16.662036.662036 lmp.py:1942]   cuda:2:   4838 tokens (20 experts)
DEBUG 01-15 16:09:16.662440.662440 lmp.py:1943]   Total GPU:   9750 tokens
DEBUG 01-15 16:09:16.662845.662845 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.662845.662845 lmp.py:1944] 
DEBUG 01-15 16:09:16.662687.662687 cuda_h.py:19] end experts_map_get cost 0.0017609596252441406 seconds
DEBUG 01-15 16:09:16.662967.662967 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.662246.662246 lmp.py:1953] 
DEBUG 01-15 16:09:16.662246.662246 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.662844.662844 cuda_h.py:19] end cpu_experts_submit cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:09:16.662394.662394 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.662562.662562 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.664475.664475 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.5.gate_proj.weight': 0, 'model.layers.17.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.38.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.38.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.38.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.41.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.41.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.41.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.42.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.42.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.42.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.45.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.45.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.45.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.51.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.51.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.51.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.54.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.54.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.54.up_proj.weight': 322961408, 'model.layers.17.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.17.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.17.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(20663762944, 5767168, 0, 0), (20669530112, 5767168, 5767168, 0), (20657995776, 5767168, 11534336, 0), (20750270464, 5767168, 17301504, 0), (20756037632, 5767168, 23068672, 0), (20744503296, 5767168, 28835840, 0), (20767571968, 5767168, 34603008, 0), (20773339136, 5767168, 40370176, 0), (20761804800, 5767168, 46137344, 0), (20802174976, 5767168, 51904512, 0), (20807942144, 5767168, 57671680, 0), (20796407808, 5767168, 63438848, 0), (20819476480, 5767168, 69206016, 0), (20825243648, 5767168, 74973184, 0), (20813709312, 5767168, 80740352, 0), (20888682496, 5767168, 86507520, 0), (20894449664, 5767168, 92274688, 0), (20882915328, 5767168, 98041856, 0), (20905984000, 5767168, 103809024, 0), (20911751168, 5767168, 109576192, 0), (20900216832, 5767168, 115343360, 0), (20923285504, 5767168, 121110528, 0), (20929052672, 5767168, 126877696, 0), (20917518336, 5767168, 132644864, 0), (20940587008, 5767168, 138412032, 0), (20946354176, 5767168, 144179200, 0), (20934819840, 5767168, 149946368, 0), (20957888512, 5767168, 155713536, 0), (20963655680, 5767168, 161480704, 0), (20952121344, 5767168, 167247872, 0), (21113602048, 5767168, 173015040, 0), (21119369216, 5767168, 178782208, 0), (21107834880, 5767168, 184549376, 0), (21130903552, 5767168, 190316544, 0), (21136670720, 5767168, 196083712, 0), (21125136384, 5767168, 201850880, 0), (21338521600, 5767168, 207618048, 0), (21344288768, 5767168, 213385216, 0), (21332754432, 5767168, 219152384, 0), (21373124608, 5767168, 224919552, 0), (21378891776, 5767168, 230686720, 0), (21367357440, 5767168, 236453888, 0), (21407727616, 5767168, 242221056, 0), (21413494784, 5767168, 247988224, 0), (21401960448, 5767168, 253755392, 0), (21528838144, 5767168, 259522560, 0), (21534605312, 5767168, 265289728, 0), (21523070976, 5767168, 271056896, 0), (21546139648, 5767168, 276824064, 0), (21551906816, 5767168, 282591232, 0), (21540372480, 5767168, 288358400, 0), (21563441152, 5767168, 294125568, 0), (21569208320, 5767168, 299892736, 0), (21557673984, 5767168, 305659904, 0), (21649948672, 5767168, 311427072, 0), (21655715840, 5767168, 317194240, 0), (21644181504, 5767168, 322961408, 0)], 2: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (20594556928, 5767168, 17301504, 0), (20600324096, 5767168, 23068672, 0), (20588789760, 5767168, 28835840, 0), (20732968960, 5767168, 34603008, 0), (20738736128, 5767168, 40370176, 0), (20727201792, 5767168, 46137344, 0), (20854079488, 5767168, 51904512, 0), (20859846656, 5767168, 57671680, 0), (20848312320, 5767168, 63438848, 0), (20871380992, 5767168, 69206016, 0), (20877148160, 5767168, 74973184, 0), (20865613824, 5767168, 80740352, 0), (20975190016, 5767168, 86507520, 0), (20980957184, 5767168, 92274688, 0), (20969422848, 5767168, 98041856, 0), (21027094528, 5767168, 103809024, 0), (21032861696, 5767168, 109576192, 0), (21021327360, 5767168, 115343360, 0), (21078999040, 5767168, 121110528, 0), (21084766208, 5767168, 126877696, 0), (21073231872, 5767168, 132644864, 0), (21096300544, 5767168, 138412032, 0), (21102067712, 5767168, 144179200, 0), (21090533376, 5767168, 149946368, 0), (21165506560, 5767168, 155713536, 0), (21171273728, 5767168, 161480704, 0), (21159739392, 5767168, 167247872, 0), (21182808064, 5767168, 173015040, 0), (21188575232, 5767168, 178782208, 0), (21177040896, 5767168, 184549376, 0), (21217411072, 5767168, 190316544, 0), (21223178240, 5767168, 196083712, 0), (21211643904, 5767168, 201850880, 0), (21234712576, 5767168, 207618048, 0), (21240479744, 5767168, 213385216, 0), (21228945408, 5767168, 219152384, 0), (21269315584, 5767168, 224919552, 0), (21275082752, 5767168, 230686720, 0), (21263548416, 5767168, 236453888, 0), (21286617088, 5767168, 242221056, 0), (21292384256, 5767168, 247988224, 0), (21280849920, 5767168, 253755392, 0), (21303918592, 5767168, 259522560, 0), (21309685760, 5767168, 265289728, 0), (21298151424, 5767168, 271056896, 0), (21355823104, 5767168, 276824064, 0), (21361590272, 5767168, 282591232, 0), (21350055936, 5767168, 288358400, 0), (21459632128, 5767168, 294125568, 0), (21465399296, 5767168, 299892736, 0), (21453864960, 5767168, 305659904, 0), (21511536640, 5767168, 311427072, 0), (21517303808, 5767168, 317194240, 0), (21505769472, 5767168, 322961408, 0), (21667250176, 5767168, 328728576, 0), (21673017344, 5767168, 334495744, 0), (21661483008, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6887ab0f0>, 2: <capsule object NULL at 0x74a6785de100>}
DEBUG 01-15 16:09:16.664754.664754 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.664080.664080 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.664552.664552 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 94ae163e-ef78-4105-b590-3e8866f49c60
DEBUG 01-15 16:09:16.664234.664234 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.664377.664377 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:16.665658.665658 client.py:127] Model loaded
DEBUG 01-15 16:09:16.665230.665230 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.665721.665721 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.666592.666592 cuda_h.py:19] end move_flatidxs cost 0.0008649826049804688 seconds
DEBUG 01-15 16:09:16.666952.666952 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.666572.666572 cuda_h.py:19] end restore2model cost 0.000993967056274414 seconds
INFO 01-15 16:09:16.666365.666365 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 94ae163e-ef78-4105-b590-3e8866f49c60
DEBUG 01-15 16:09:16.666547.666547 cuda_h.py:19] end sllm_worker_task cost 0.012437820434570312 seconds
DEBUG 01-15 16:09:16.667737.667737 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004201650619506836 seconds
DEBUG 01-15 16:09:16.667239.667239 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.670103.670103 cuda_h.py:19] end restore2model cost 0.0030241012573242188 seconds
DEBUG 01-15 16:09:16.670323.670323 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007545948028564453 seconds
DEBUG 01-15 16:09:16.670901.670901 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.670308.670308 cuda_h.py:19] end gpu_sexperts cost 0.0002701282501220703 seconds
DEBUG 01-15 16:09:16.670230.670230 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.670715.670715 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4543533325195312e-05 seconds
DEBUG 01-15 16:09:16.670457.670457 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.670160.670160 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.671617.671617 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009379386901855469 seconds
DEBUG 01-15 16:09:16.671135.671135 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.672348.672348 cuda_h.py:19] end gpu_group_list cost 0.0001983642578125 seconds
DEBUG 01-15 16:09:16.673907.673907 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.674389.674389 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010640621185302734 seconds
DEBUG 01-15 16:09:16.674729.674729 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.674201.674201 cuda_h.py:19] end gpu_group_list cost 0.00021266937255859375 seconds
DEBUG 01-15 16:09:16.675038.675038 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.675583.675583 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 94ae163e-ef78-4105-b590-3e8866f49c60
DEBUG 01-15 16:09:16.676262.676262 cuda_h.py:19] end group_tensors cost 0.010226964950561523 seconds
DEBUG 01-15 16:09:16.677280.677280 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.680783.680783 cuda_h.py:19] end group pad cost 0.003304004669189453 seconds
DEBUG 01-15 16:09:16.680473.680473 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:16.708285.708285 client.py:127] Model loaded
DEBUG 01-15 16:09:16.709363.709363 cuda_h.py:19] end wait_experts_multi_device cost 0.03377652168273926 seconds
DEBUG 01-15 16:09:16.709841.709841 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.709597.709597 cuda_h.py:19] end group_einsum cost 0.028342723846435547 seconds
DEBUG 01-15 16:09:16.709966.709966 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.712128.712128 cuda_h.py:19] end get_outputs_cpu1 cost 0.003010272979736328 seconds
DEBUG 01-15 16:09:16.713942.713942 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04823470115661621 seconds
DEBUG 01-15 16:09:16.713837.713837 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.004739522933959961 seconds
DEBUG 01-15 16:09:16.714543.714543 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.715907.715907 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.716870.716870 cuda_h.py:19] end index_scatter cost 0.00012946128845214844 seconds
DEBUG 01-15 16:09:16.716448.716448 cuda_h.py:19] end cpuoutputsdeal cost 0.0024263858795166016 seconds
DEBUG 01-15 16:09:16.716214.716214 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.716740.716740 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.717373.717373 cuda_h.py:19] end gpu_group_tensor cost 0.0002617835998535156 seconds
DEBUG 01-15 16:09:16.717045.717045 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.717082.717082 cuda_h.py:19] end gpu_group_tensor cost 0.0002434253692626953 seconds
DEBUG 01-15 16:09:16.717194.717194 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.719570.719570 cuda_h.py:19] end gpu_group_einsum cost 0.0018231868743896484 seconds
DEBUG 01-15 16:09:16.719173.719173 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.720897.720897 cuda_h.py:19] end gpu_group_einsum cost 0.0005707740783691406 seconds
DEBUG 01-15 16:09:16.720869.720869 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.720668.720668 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.721652.721652 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025653839111328125 seconds
DEBUG 01-15 16:09:16.721792.721792 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.721928.721928 cuda_h.py:19] end concat_expert_out cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:09:16.721685.721685 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.721351.721351 cuda_h.py:19] end index_scatter cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:09:16.721028.721028 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000934600830078125 seconds
DEBUG 01-15 16:09:16.721740.721740 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.721305.721305 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.721603.721603 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015163421630859375 seconds
DEBUG 01-15 16:09:16.721120.721120 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.722474.722474 cuda_h.py:19] end concat_expert_out cost 5.459785461425781e-05 seconds
DEBUG 01-15 16:09:16.722648.722648 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.722433.722433 cuda_h.py:19] end index_scatter cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:09:16.722288.722288 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005095005035400391 seconds
DEBUG 01-15 16:09:16.722549.722549 cuda_h.py:19] end gpu_experts_multi_device cost 0.05138039588928223 seconds
DEBUG 01-15 16:09:16.722890.722890 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.06219935417175293 seconds
DEBUG 01-15 16:09:16.722608.722608 cuda_h.py:19] end prefill_layer cost 0.06923127174377441 seconds
DEBUG 01-15 16:09:16.722259.722259 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:09:16.722724.722724 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.722188.722188 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:09:16.723321.723321 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:09:16.723077.723077 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:09:16.723265.723265 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 3.981590270996094e-05 seconds
DEBUG 01-15 16:09:16.723114.723114 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 7.104873657226562e-05 seconds
DEBUG 01-15 16:09:16.723048.723048 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.723064.723064 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.723306.723306 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.723849.723849 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.723456.723456 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.724421.724421 cuda_h.py:19] end allocate_cuda_memory cost 0.00040984153747558594 seconds
DEBUG 01-15 16:09:16.724196.724196 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.724021.724021 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.724952.724952 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.724114.724114 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, af94a590-7ce4-4ec2-8cfe-4c0e19351425
DEBUG 01-15 16:09:16.724147.724147 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.725785.725785 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.726578.726578 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, af94a590-7ce4-4ec2-8cfe-4c0e19351425
DEBUG 01-15 16:09:16.726225.726225 cuda_h.py:19] end load_into_gpu_async cost 0.0017621517181396484 seconds
DEBUG 01-15 16:09:16.726540.726540 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.726912.726912 cuda_h.py:19] end restore_tensors2 cost 0.0001499652862548828 seconds
DEBUG 01-15 16:09:16.726399.726399 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003083944320678711 seconds
INFO 01-15 16:09:16.726365.726365 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, af94a590-7ce4-4ec2-8cfe-4c0e19351425
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.729517.729517 cuda_h.py:19] end self_attn cost 0.003796815872192383 seconds
DEBUG 01-15 16:09:16.729395.729395 cuda_h.py:19] end iln_self_attn_paln cost 0.006157875061035156 seconds
DEBUG 01-15 16:09:16.729986.729986 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:09:16.729411.729411 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.730367.730367 cuda_h.py:19] end gate cost 0.0006368160247802734 seconds
DEBUG 01-15 16:09:16.730243.730243 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.730346.730346 lmp.py:1912] 
DEBUG 01-15 16:09:16.730346.730346 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.730341.730341 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.730321.730321 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.730395.730395 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.730184.730184 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.730734.730734 lmp.py:1917] 
DEBUG 01-15 16:09:16.730734.730734 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.730092.730092 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.730649.730649 lmp.py:1935]   Expert 32 |     32 | CPU
DEBUG 01-15 16:09:16.730008.730008 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:09:16.730366.730366 lmp.py:1935]   Expert  5 |     54 | CPU
DEBUG 01-15 16:09:16.730293.730293 lmp.py:1935]   Expert 46 |     75 | CPU
DEBUG 01-15 16:09:16.730698.730698 lmp.py:1935]   Expert 40 |     91 | CPU
DEBUG 01-15 16:09:16.730579.730579 lmp.py:1935]   Expert  8 |     92 | CPU
DEBUG 01-15 16:09:16.730984.730984 lmp.py:1935]   Expert 12 |     99 | CPU
DEBUG 01-15 16:09:16.730435.730435 lmp.py:1935]   Expert 17 |    106 | CPU
DEBUG 01-15 16:09:16.730376.730376 lmp.py:1935]   Expert  3 |    111 | CPU
DEBUG 01-15 16:09:16.730734.730734 lmp.py:1935]   Expert 60 |    112 | CPU
DEBUG 01-15 16:09:16.730238.730238 lmp.py:1935]   Expert 27 |    113 | CPU
DEBUG 01-15 16:09:16.730835.730835 lmp.py:1935]   Expert 58 |    116 | CPU
DEBUG 01-15 16:09:16.730478.730478 lmp.py:1935]   Expert 21 |    119 | CPU
DEBUG 01-15 16:09:16.730929.730929 lmp.py:1935]   Expert 28 |    120 | CPU
DEBUG 01-15 16:09:16.730095.730095 lmp.py:1935]   Expert 29 |    120 | CPU
DEBUG 01-15 16:09:16.730261.730261 lmp.py:1935]   Expert 41 |    128 | CPU
DEBUG 01-15 16:09:16.730189.730189 lmp.py:1935]   Expert 25 |    129 | CPU
DEBUG 01-15 16:09:16.730878.730878 lmp.py:1935]   Expert 35 |    131 | CPU
DEBUG 01-15 16:09:16.730044.730044 lmp.py:1935]   Expert 19 |    135 | CPU
DEBUG 01-15 16:09:16.730972.730972 lmp.py:1935]   Expert  0 |    143 | CPU
DEBUG 01-15 16:09:16.731138.731138 lmp.py:1935]   Expert  6 |    146 | CPU
DEBUG 01-15 16:09:16.731304.731304 lmp.py:1935]   Expert 52 |    147 | CPU
DEBUG 01-15 16:09:16.731709.731709 lmp.py:1935]   Expert 56 |    148 | CPU
DEBUG 01-15 16:09:16.731875.731875 lmp.py:1935]   Expert 54 |    149 | CPU
DEBUG 01-15 16:09:16.731995.731995 lmp.py:1935]   Expert 37 |    150 | CPU
DEBUG 01-15 16:09:16.731591.731591 lmp.py:1935]   Expert 48 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731903.731903 lmp.py:1935]   Expert 53 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731546.731546 lmp.py:1935]   Expert 63 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731666.731666 lmp.py:1935]   Expert 36 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731309.731309 lmp.py:1935]   Expert 59 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731714.731714 lmp.py:1935]   Expert  9 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731357.731357 lmp.py:1935]   Expert  1 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731476.731476 lmp.py:1935]   Expert 39 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731881.731881 lmp.py:1935]   Expert 20 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731762.731762 lmp.py:1935]   Expert 61 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731598.731598 lmp.py:1935]   Expert  7 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731671.731671 lmp.py:1935]   Expert 11 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731744.731744 lmp.py:1935]   Expert 42 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731864.731864 lmp.py:1935]   Expert 43 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731223.731223 lmp.py:1935]   Expert 34 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731104.731104 lmp.py:1935]   Expert 47 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731985.731985 lmp.py:1935]   Expert 55 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731390.731390 lmp.py:1935]   Expert 16 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731033.731033 lmp.py:1935]   Expert 57 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731914.731914 lmp.py:1935]   Expert 13 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731796.731796 lmp.py:1935]   Expert 18 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731915.731915 lmp.py:1935]   Expert 15 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731989.731989 lmp.py:1935]   Expert  4 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731301.731301 lmp.py:1935]   Expert 50 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731613.731613 lmp.py:1935]   Expert 45 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731733.731733 lmp.py:1935]   Expert 22 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731852.731852 lmp.py:1935]   Expert 33 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731734.731734 lmp.py:1935]   Expert 31 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731377.731377 lmp.py:1935]   Expert 51 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731020.731020 lmp.py:1935]   Expert 49 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731901.731901 lmp.py:1935]   Expert 26 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731511.731511 lmp.py:1935]   Expert 38 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731870.731870 lmp.py:1935]   Expert 10 |    287 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731751.731751 lmp.py:1935]   Expert 44 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731348.731348 lmp.py:1935]   Expert  2 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731421.731421 lmp.py:1935]   Expert 24 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731256.731256 lmp.py:1935]   Expert 14 |    312 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731614.731614 lmp.py:1935]   Expert 23 |    409 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.731734.731734 lmp.py:1935]   Expert 62 |    672 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.731423.731423 lmp.py:1937] 
DEBUG 01-15 16:09:16.731423.731423 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.731543.731543 lmp.py:1938]   CPU:   2818 tokens
DEBUG 01-15 16:09:16.731902.731902 lmp.py:1942]   cuda:1:   4733 tokens (19 experts)
DEBUG 01-15 16:09:16.731783.731783 lmp.py:1942]   cuda:2:   4737 tokens (20 experts)
DEBUG 01-15 16:09:16.731949.731949 lmp.py:1943]   Total GPU:   9470 tokens
DEBUG 01-15 16:09:16.731400.731400 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.731400.731400 lmp.py:1944] 
DEBUG 01-15 16:09:16.731527.731527 cuda_h.py:19] end experts_map_get cost 0.001725912094116211 seconds
DEBUG 01-15 16:09:16.731754.731754 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.732841.732841 lmp.py:1953] 
DEBUG 01-15 16:09:16.732841.732841 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.732723.732723 cuda_h.py:19] end cpu_experts_submit cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:09:16.732082.732082 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.732011.732011 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.733395.733395 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.1.gate_proj.weight': 0, 'model.layers.18.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.9.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.9.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.9.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.61.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.61.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.61.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.18.mlp.experts.2.gate_proj.weight': 0, 'model.layers.18.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.36.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.36.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.36.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.18.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.18.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.18.mlp.experts.59.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(21701853184, 5767168, 0, 0), (21707620352, 5767168, 5767168, 0), (21696086016, 5767168, 11534336, 0), (21840265216, 5767168, 17301504, 0), (21846032384, 5767168, 23068672, 0), (21834498048, 5767168, 28835840, 0), (21874868224, 5767168, 34603008, 0), (21880635392, 5767168, 40370176, 0), (21869101056, 5767168, 46137344, 0), (21909471232, 5767168, 51904512, 0), (21915238400, 5767168, 57671680, 0), (21903704064, 5767168, 63438848, 0), (21944074240, 5767168, 69206016, 0), (21949841408, 5767168, 74973184, 0), (21938307072, 5767168, 80740352, 0), (21961375744, 5767168, 86507520, 0), (21967142912, 5767168, 92274688, 0), (21955608576, 5767168, 98041856, 0), (22065184768, 5767168, 103809024, 0), (22070951936, 5767168, 109576192, 0), (22059417600, 5767168, 115343360, 0), (22099787776, 5767168, 121110528, 0), (22105554944, 5767168, 126877696, 0), (22094020608, 5767168, 132644864, 0), (22134390784, 5767168, 138412032, 0), (22140157952, 5767168, 144179200, 0), (22128623616, 5767168, 149946368, 0), (22220898304, 5767168, 155713536, 0), (22226665472, 5767168, 161480704, 0), (22215131136, 5767168, 167247872, 0), (22272802816, 5767168, 173015040, 0), (22278569984, 5767168, 178782208, 0), (22267035648, 5767168, 184549376, 0), (22428516352, 5767168, 190316544, 0), (22434283520, 5767168, 196083712, 0), (22422749184, 5767168, 201850880, 0), (22445817856, 5767168, 207618048, 0), (22451585024, 5767168, 213385216, 0), (22440050688, 5767168, 219152384, 0), (22515023872, 5767168, 224919552, 0), (22520791040, 5767168, 230686720, 0), (22509256704, 5767168, 236453888, 0), (22532325376, 5767168, 242221056, 0), (22538092544, 5767168, 247988224, 0), (22526558208, 5767168, 253755392, 0), (22549626880, 5767168, 259522560, 0), (22555394048, 5767168, 265289728, 0), (22543859712, 5767168, 271056896, 0), (22739943424, 5767168, 276824064, 0), (22745710592, 5767168, 282591232, 0), (22734176256, 5767168, 288358400, 0), (22757244928, 5767168, 294125568, 0), (22763012096, 5767168, 299892736, 0), (22751477760, 5767168, 305659904, 0), (22774546432, 5767168, 311427072, 0), (22780313600, 5767168, 317194240, 0), (22768779264, 5767168, 322961408, 0)], 2: [(21719154688, 5767168, 0, 0), (21724921856, 5767168, 5767168, 0), (21713387520, 5767168, 11534336, 0), (21753757696, 5767168, 17301504, 0), (21759524864, 5767168, 23068672, 0), (21747990528, 5767168, 28835840, 0), (21805662208, 5767168, 34603008, 0), (21811429376, 5767168, 40370176, 0), (21799895040, 5767168, 46137344, 0), (21857566720, 5767168, 51904512, 0), (21863333888, 5767168, 57671680, 0), (21851799552, 5767168, 63438848, 0), (21926772736, 5767168, 69206016, 0), (21932539904, 5767168, 74973184, 0), (21921005568, 5767168, 80740352, 0), (21995978752, 5767168, 86507520, 0), (22001745920, 5767168, 92274688, 0), (21990211584, 5767168, 98041856, 0), (22030581760, 5767168, 103809024, 0), (22036348928, 5767168, 109576192, 0), (22024814592, 5767168, 115343360, 0), (22082486272, 5767168, 121110528, 0), (22088253440, 5767168, 126877696, 0), (22076719104, 5767168, 132644864, 0), (22255501312, 5767168, 138412032, 0), (22261268480, 5767168, 144179200, 0), (22249734144, 5767168, 149946368, 0), (22307405824, 5767168, 155713536, 0), (22313172992, 5767168, 161480704, 0), (22301638656, 5767168, 167247872, 0), (22342008832, 5767168, 173015040, 0), (22347776000, 5767168, 178782208, 0), (22336241664, 5767168, 184549376, 0), (22359310336, 5767168, 190316544, 0), (22365077504, 5767168, 196083712, 0), (22353543168, 5767168, 201850880, 0), (22411214848, 5767168, 207618048, 0), (22416982016, 5767168, 213385216, 0), (22405447680, 5767168, 219152384, 0), (22463119360, 5767168, 224919552, 0), (22468886528, 5767168, 230686720, 0), (22457352192, 5767168, 236453888, 0), (22497722368, 5767168, 242221056, 0), (22503489536, 5767168, 247988224, 0), (22491955200, 5767168, 253755392, 0), (22566928384, 5767168, 259522560, 0), (22572695552, 5767168, 265289728, 0), (22561161216, 5767168, 271056896, 0), (22601531392, 5767168, 276824064, 0), (22607298560, 5767168, 282591232, 0), (22595764224, 5767168, 288358400, 0), (22636134400, 5767168, 294125568, 0), (22641901568, 5767168, 299892736, 0), (22630367232, 5767168, 305659904, 0), (22670737408, 5767168, 311427072, 0), (22676504576, 5767168, 317194240, 0), (22664970240, 5767168, 322961408, 0), (22705340416, 5767168, 328728576, 0), (22711107584, 5767168, 334495744, 0), (22699573248, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a81469c060>, 2: <capsule object NULL at 0x74a6785de400>}
DEBUG 01-15 16:09:16.733825.733825 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.733152.733152 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.733385.733385 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8edefec3-4e99-477c-a98f-890df9223a04
DEBUG 01-15 16:09:16.734538.734538 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:16.734335.734335 client.py:127] Model loaded
DEBUG 01-15 16:09:16.734829.734829 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.734891.734891 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:16.734365.734365 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:16.735467.735467 cuda_h.py:19] end restore2model cost 0.0009768009185791016 seconds
INFO 01-15 16:09:16.735644.735644 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8edefec3-4e99-477c-a98f-890df9223a04
DEBUG 01-15 16:09:16.736480.736480 cuda_h.py:19] end move_flatidxs cost 0.0010013580322265625 seconds
DEBUG 01-15 16:09:16.736747.736747 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.735819.735819 cuda_h.py:19] end sllm_worker_task cost 0.012171506881713867 seconds
DEBUG 01-15 16:09:16.736817.736817 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0040700435638427734 seconds
DEBUG 01-15 16:09:16.736942.736942 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.739770.739770 cuda_h.py:19] end restore2model cost 0.002962827682495117 seconds
DEBUG 01-15 16:09:16.739322.739322 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007354021072387695 seconds
DEBUG 01-15 16:09:16.739925.739925 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.739081.739081 cuda_h.py:19] end gpu_sexperts cost 0.00026035308837890625 seconds
DEBUG 01-15 16:09:16.739241.739241 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.739117.739117 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1219253540039062e-05 seconds
DEBUG 01-15 16:09:16.739575.739575 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.739563.739563 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.740191.740191 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009248256683349609 seconds
DEBUG 01-15 16:09:16.740510.740510 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.741723.741723 cuda_h.py:19] end gpu_group_list cost 0.0001983642578125 seconds
DEBUG 01-15 16:09:16.741611.741611 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.743939.743939 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010251998901367188 seconds
DEBUG 01-15 16:09:16.743510.743510 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.743214.743214 cuda_h.py:19] end gpu_group_list cost 0.00020766258239746094 seconds
DEBUG 01-15 16:09:16.743384.743384 cuda_h.py:19] end group_tensors cost 0.006913423538208008 seconds
DEBUG 01-15 16:09:16.743372.743372 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.744654.744654 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.744291.744291 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8edefec3-4e99-477c-a98f-890df9223a04
DEBUG 01-15 16:09:16.747305.747305 cuda_h.py:19] end group pad cost 0.0036840438842773438 seconds
DEBUG 01-15 16:09:16.747618.747618 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:16.777085.777085 client.py:127] Model loaded
DEBUG 01-15 16:09:16.777680.777680 cuda_h.py:19] end wait_experts_multi_device cost 0.033557891845703125 seconds
DEBUG 01-15 16:09:16.777251.777251 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.777511.777511 cuda_h.py:19] end group_einsum cost 0.029929399490356445 seconds
DEBUG 01-15 16:09:16.777509.777509 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.781112.781112 cuda_h.py:19] end get_outputs_cpu1 cost 0.0031194686889648438 seconds
DEBUG 01-15 16:09:16.782999.782999 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.047432661056518555 seconds
DEBUG 01-15 16:09:16.782213.782213 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005124807357788086 seconds
DEBUG 01-15 16:09:16.783080.783080 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.785250.785250 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.785314.785314 cuda_h.py:19] end index_scatter cost 0.0001556873321533203 seconds
DEBUG 01-15 16:09:16.786675.786675 cuda_h.py:19] end cpuoutputsdeal cost 0.0030395984649658203 seconds
DEBUG 01-15 16:09:16.786999.786999 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.786644.786644 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.786366.786366 cuda_h.py:19] end gpu_group_tensor cost 0.0003094673156738281 seconds
DEBUG 01-15 16:09:16.786250.786250 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.787699.787699 cuda_h.py:19] end gpu_group_tensor cost 0.0002865791320800781 seconds
DEBUG 01-15 16:09:16.787058.787058 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.788053.788053 cuda_h.py:19] end gpu_group_einsum cost 0.0011038780212402344 seconds
DEBUG 01-15 16:09:16.789988.789988 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.789204.789204 cuda_h.py:19] end gpu_group_einsum cost 0.00048470497131347656 seconds
DEBUG 01-15 16:09:16.789249.789249 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.789180.789180 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.790495.790495 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025391578674316406 seconds
DEBUG 01-15 16:09:16.790218.790218 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.790507.790507 cuda_h.py:19] end concat_expert_out cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:09:16.790072.790072 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.790976.790976 cuda_h.py:19] end index_scatter cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:09:16.790037.790037 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009455680847167969 seconds
DEBUG 01-15 16:09:16.790551.790551 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.790877.790877 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.791837.791837 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014853477478027344 seconds
DEBUG 01-15 16:09:16.791116.791116 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.791708.791708 cuda_h.py:19] end concat_expert_out cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:09:16.791121.791121 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.791905.791905 cuda_h.py:19] end index_scatter cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:09:16.791238.791238 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005064010620117188 seconds
DEBUG 01-15 16:09:16.791446.791446 cuda_h.py:19] end gpu_experts_multi_device cost 0.051497697830200195 seconds
DEBUG 01-15 16:09:16.791832.791832 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.062041521072387695 seconds
DEBUG 01-15 16:09:16.792890.792890 cuda_h.py:19] end prefill_layer cost 0.06900405883789062 seconds
DEBUG 01-15 16:09:16.792163.792163 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:09:16.792582.792582 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.792477.792477 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:09:16.792848.792848 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:09:16.792128.792128 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:09:16.792693.792693 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:09:16.792733.792733 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:09:16.792337.792337 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.792658.792658 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.792371.792371 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.792093.792093 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.792111.792111 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.793551.793551 cuda_h.py:19] end allocate_cuda_memory cost 0.0005466938018798828 seconds
DEBUG 01-15 16:09:16.793590.793590 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.793223.793223 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.793955.793955 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.794322.794322 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b3397fb0-129c-4bd5-bcc4-631715c80b34
DEBUG 01-15 16:09:16.794038.794038 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.794591.794591 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.795886.795886 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b3397fb0-129c-4bd5-bcc4-631715c80b34
DEBUG 01-15 16:09:16.795487.795487 cuda_h.py:19] end load_into_gpu_async cost 0.001771688461303711 seconds
DEBUG 01-15 16:09:16.795947.795947 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.795909.795909 cuda_h.py:19] end restore_tensors2 cost 0.00015282630920410156 seconds
DEBUG 01-15 16:09:16.796867.796867 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0032317638397216797 seconds
INFO 01-15 16:09:16.796998.796998 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b3397fb0-129c-4bd5-bcc4-631715c80b34
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.798419.798419 cuda_h.py:19] end self_attn cost 0.0041620731353759766 seconds
DEBUG 01-15 16:09:16.799880.799880 cuda_h.py:19] end iln_self_attn_paln cost 0.00668787956237793 seconds
DEBUG 01-15 16:09:16.799371.799371 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:09:16.799604.799604 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.799103.799103 cuda_h.py:19] end gate cost 0.0006163120269775391 seconds
DEBUG 01-15 16:09:16.799263.799263 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.800360.800360 lmp.py:1912] 
DEBUG 01-15 16:09:16.800360.800360 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.800739.800739 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.800388.800388 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.800608.800608 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.800681.800681 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.800801.800801 lmp.py:1917] 
DEBUG 01-15 16:09:16.800801.800801 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.800113.800113 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.800147.800147 lmp.py:1935]   Expert 44 |     40 | CPU
DEBUG 01-15 16:09:16.800505.800505 lmp.py:1935]   Expert  1 |     48 | CPU
DEBUG 01-15 16:09:16.800148.800148 lmp.py:1935]   Expert 60 |     61 | CPU
DEBUG 01-15 16:09:16.800029.800029 lmp.py:1935]   Expert 28 |     73 | CPU
DEBUG 01-15 16:09:16.800672.800672 lmp.py:1935]   Expert 48 |     79 | CPU
DEBUG 01-15 16:09:16.800077.800077 lmp.py:1935]   Expert 27 |     82 | CPU
DEBUG 01-15 16:09:16.800780.800780 lmp.py:1935]   Expert  0 |     99 | CPU
DEBUG 01-15 16:09:16.800112.800112 lmp.py:1935]   Expert 62 |    105 | CPU
DEBUG 01-15 16:09:16.800947.800947 lmp.py:1935]   Expert 22 |    113 | CPU
DEBUG 01-15 16:09:16.800352.800352 lmp.py:1935]   Expert 30 |    114 | CPU
DEBUG 01-15 16:09:16.800948.800948 lmp.py:1935]   Expert 42 |    115 | CPU
DEBUG 01-15 16:09:16.800591.800591 lmp.py:1935]   Expert 59 |    115 | CPU
DEBUG 01-15 16:09:16.800234.800234 lmp.py:1935]   Expert 58 |    123 | CPU
DEBUG 01-15 16:09:16.800116.800116 lmp.py:1935]   Expert 16 |    124 | CPU
DEBUG 01-15 16:09:16.800235.800235 lmp.py:1935]   Expert  8 |    127 | CPU
DEBUG 01-15 16:09:16.800163.800163 lmp.py:1935]   Expert 12 |    128 | CPU
DEBUG 01-15 16:09:16.800045.800045 lmp.py:1935]   Expert 50 |    134 | CPU
DEBUG 01-15 16:09:16.800449.800449 lmp.py:1935]   Expert  5 |    142 | CPU
DEBUG 01-15 16:09:16.800092.800092 lmp.py:1935]   Expert 56 |    143 | CPU
DEBUG 01-15 16:09:16.800497.800497 lmp.py:1935]   Expert 15 |    151 | CPU
DEBUG 01-15 16:09:16.800901.800901 lmp.py:1935]   Expert 55 |    151 | CPU
DEBUG 01-15 16:09:16.800306.800306 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:09:16.800949.800949 lmp.py:1935]   Expert 26 |    154 | CPU
DEBUG 01-15 16:09:16.800353.800353 lmp.py:1935]   Expert 32 |    158 | CPU
DEBUG 01-15 16:09:16.800996.800996 lmp.py:1935]   Expert 47 |    159 | CPU
DEBUG 01-15 16:09:16.800547.800547 lmp.py:1935]   Expert 24 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.800574.800574 lmp.py:1935]   Expert 34 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.800700.800700 lmp.py:1935]   Expert 52 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.800297.800297 lmp.py:1935]   Expert  2 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.800417.800417 lmp.py:1935]   Expert  6 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.800775.800775 lmp.py:1935]   Expert 40 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.800895.800895 lmp.py:1935]   Expert 41 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.800776.800776 lmp.py:1935]   Expert 13 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.800373.800373 lmp.py:1935]   Expert 18 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.800731.800731 lmp.py:1935]   Expert 54 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.800613.800613 lmp.py:1935]   Expert  3 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.800971.800971 lmp.py:1935]   Expert 19 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.800329.800329 lmp.py:1935]   Expert 20 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801787.801787 lmp.py:1935]   Expert 46 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801099.801099 lmp.py:1935]   Expert 37 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801887.801887 lmp.py:1935]   Expert 25 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801246.801246 lmp.py:1935]   Expert 51 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801081.801081 lmp.py:1935]   Expert 43 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801962.801962 lmp.py:1935]   Expert 17 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801843.801843 lmp.py:1935]   Expert 11 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801725.801725 lmp.py:1935]   Expert 31 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801606.801606 lmp.py:1935]   Expert 35 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801249.801249 lmp.py:1935]   Expert 23 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801607.801607 lmp.py:1935]   Expert 49 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801489.801489 lmp.py:1935]   Expert 39 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801609.801609 lmp.py:1935]   Expert 53 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801252.801252 lmp.py:1935]   Expert 10 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801040.801040 lmp.py:1935]   Expert 33 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801352.801352 lmp.py:1935]   Expert 36 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801141.801141 lmp.py:1935]   Expert 38 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801499.801499 lmp.py:1935]   Expert  4 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801857.801857 lmp.py:1935]   Expert 21 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801739.801739 lmp.py:1935]   Expert 14 |    347 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801382.801382 lmp.py:1935]   Expert 63 |    369 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801786.801786 lmp.py:1935]   Expert 45 |    376 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801072.801072 lmp.py:1935]   Expert 61 |    391 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801192.801192 lmp.py:1935]   Expert  9 |    392 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801120.801120 lmp.py:1935]   Expert 29 |    488 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.801571.801571 lmp.py:1935]   Expert  7 |    513 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.801022.801022 lmp.py:1937] 
DEBUG 01-15 16:09:16.801022.801022 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.801903.801903 lmp.py:1938]   CPU:   2889 tokens
DEBUG 01-15 16:09:16.801308.801308 lmp.py:1942]   cuda:1:   4781 tokens (20 experts)
DEBUG 01-15 16:09:16.801997.801997 lmp.py:1942]   cuda:2:   4618 tokens (19 experts)
DEBUG 01-15 16:09:16.801256.801256 lmp.py:1943]   Total GPU:   9399 tokens
DEBUG 01-15 16:09:16.801243.801243 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.801243.801243 lmp.py:1944] 
DEBUG 01-15 16:09:16.801939.801939 cuda_h.py:19] end experts_map_get cost 0.0017697811126708984 seconds
DEBUG 01-15 16:09:16.801021.801021 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.801154.801154 lmp.py:1953] 
DEBUG 01-15 16:09:16.801154.801154 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.801514.801514 cuda_h.py:19] end cpu_experts_submit cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:09:16.801018.801018 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.801755.801755 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.802747.802747 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.2.gate_proj.weight': 0, 'model.layers.19.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.38.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.38.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.38.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.39.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.39.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.39.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.40.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.40.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.40.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.19.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.19.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.19.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.19.mlp.experts.4.gate_proj.weight': 0, 'model.layers.19.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.54.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.54.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.54.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(22826450944, 5767168, 0, 0), (22832218112, 5767168, 5767168, 0), (22820683776, 5767168, 11534336, 0), (22843752448, 5767168, 17301504, 0), (22849519616, 5767168, 23068672, 0), (22837985280, 5767168, 28835840, 0), (22912958464, 5767168, 34603008, 0), (22918725632, 5767168, 40370176, 0), (22907191296, 5767168, 46137344, 0), (22964862976, 5767168, 51904512, 0), (22970630144, 5767168, 57671680, 0), (22959095808, 5767168, 63438848, 0), (23085973504, 5767168, 69206016, 0), (23091740672, 5767168, 74973184, 0), (23080206336, 5767168, 80740352, 0), (23103275008, 5767168, 86507520, 0), (23109042176, 5767168, 92274688, 0), (23097507840, 5767168, 98041856, 0), (23120576512, 5767168, 103809024, 0), (23126343680, 5767168, 109576192, 0), (23114809344, 5767168, 115343360, 0), (23155179520, 5767168, 121110528, 0), (23160946688, 5767168, 126877696, 0), (23149412352, 5767168, 132644864, 0), (23189782528, 5767168, 138412032, 0), (23195549696, 5767168, 144179200, 0), (23184015360, 5767168, 149946368, 0), (23207084032, 5767168, 155713536, 0), (23212851200, 5767168, 161480704, 0), (23201316864, 5767168, 167247872, 0), (23328194560, 5767168, 173015040, 0), (23333961728, 5767168, 178782208, 0), (23322427392, 5767168, 184549376, 0), (23414702080, 5767168, 190316544, 0), (23420469248, 5767168, 196083712, 0), (23408934912, 5767168, 201850880, 0), (23432003584, 5767168, 207618048, 0), (23437770752, 5767168, 213385216, 0), (23426236416, 5767168, 219152384, 0), (23449305088, 5767168, 224919552, 0), (23455072256, 5767168, 230686720, 0), (23443537920, 5767168, 236453888, 0), (23466606592, 5767168, 242221056, 0), (23472373760, 5767168, 247988224, 0), (23460839424, 5767168, 253755392, 0), (23483908096, 5767168, 259522560, 0), (23489675264, 5767168, 265289728, 0), (23478140928, 5767168, 271056896, 0), (23674224640, 5767168, 276824064, 0), (23679991808, 5767168, 282591232, 0), (23668457472, 5767168, 288358400, 0), (23691526144, 5767168, 294125568, 0), (23697293312, 5767168, 299892736, 0), (23685758976, 5767168, 305659904, 0), (23847239680, 5767168, 311427072, 0), (23853006848, 5767168, 317194240, 0), (23841472512, 5767168, 322961408, 0), (23881842688, 5767168, 328728576, 0), (23887609856, 5767168, 334495744, 0), (23876075520, 5767168, 340262912, 0)], 2: [(22861053952, 5767168, 0, 0), (22866821120, 5767168, 5767168, 0), (22855286784, 5767168, 11534336, 0), (22895656960, 5767168, 17301504, 0), (22901424128, 5767168, 23068672, 0), (22889889792, 5767168, 28835840, 0), (22947561472, 5767168, 34603008, 0), (22953328640, 5767168, 40370176, 0), (22941794304, 5767168, 46137344, 0), (22982164480, 5767168, 51904512, 0), (22987931648, 5767168, 57671680, 0), (22976397312, 5767168, 63438848, 0), (23016767488, 5767168, 69206016, 0), (23022534656, 5767168, 74973184, 0), (23011000320, 5767168, 80740352, 0), (23034068992, 5767168, 86507520, 0), (23039836160, 5767168, 92274688, 0), (23028301824, 5767168, 98041856, 0), (23137878016, 5767168, 103809024, 0), (23143645184, 5767168, 109576192, 0), (23132110848, 5767168, 115343360, 0), (23224385536, 5767168, 121110528, 0), (23230152704, 5767168, 126877696, 0), (23218618368, 5767168, 132644864, 0), (23293591552, 5767168, 138412032, 0), (23299358720, 5767168, 144179200, 0), (23287824384, 5767168, 149946368, 0), (23362797568, 5767168, 155713536, 0), (23368564736, 5767168, 161480704, 0), (23357030400, 5767168, 167247872, 0), (23380099072, 5767168, 173015040, 0), (23385866240, 5767168, 178782208, 0), (23374331904, 5767168, 184549376, 0), (23397400576, 5767168, 190316544, 0), (23403167744, 5767168, 196083712, 0), (23391633408, 5767168, 201850880, 0), (23501209600, 5767168, 207618048, 0), (23506976768, 5767168, 213385216, 0), (23495442432, 5767168, 219152384, 0), (23535812608, 5767168, 224919552, 0), (23541579776, 5767168, 230686720, 0), (23530045440, 5767168, 236453888, 0), (23570415616, 5767168, 242221056, 0), (23576182784, 5767168, 247988224, 0), (23564648448, 5767168, 253755392, 0), (23587717120, 5767168, 259522560, 0), (23593484288, 5767168, 265289728, 0), (23581949952, 5767168, 271056896, 0), (23639621632, 5767168, 276824064, 0), (23645388800, 5767168, 282591232, 0), (23633854464, 5767168, 288358400, 0), (23708827648, 5767168, 294125568, 0), (23714594816, 5767168, 299892736, 0), (23703060480, 5767168, 305659904, 0), (23726129152, 5767168, 311427072, 0), (23731896320, 5767168, 317194240, 0), (23720361984, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6782e6be0>, 2: <capsule object NULL at 0x74a6785de280>}
DEBUG 01-15 16:09:16.802872.802872 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.803861.803861 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.803710.803710 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, dfcf14ef-c850-4ea4-9a58-1444078c63aa
DEBUG 01-15 16:09:16.803571.803571 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.803111.803111 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:16.803044.803044 client.py:127] Model loaded
DEBUG 01-15 16:09:16.803776.803776 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.803539.803539 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:16.804157.804157 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, dfcf14ef-c850-4ea4-9a58-1444078c63aa
DEBUG 01-15 16:09:16.804563.804563 cuda_h.py:19] end move_flatidxs cost 0.0008382797241210938 seconds
DEBUG 01-15 16:09:16.804624.804624 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.804246.804246 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0027894973754882812 seconds
DEBUG 01-15 16:09:16.804646.804646 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.805453.805453 cuda_h.py:19] end restore2model cost 0.0008924007415771484 seconds
DEBUG 01-15 16:09:16.805368.805368 cuda_h.py:19] end sllm_worker_task cost 0.0131683349609375 seconds
DEBUG 01-15 16:09:16.808258.808258 cuda_h.py:19] end restore2model cost 0.003992795944213867 seconds
DEBUG 01-15 16:09:16.808738.808738 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007039785385131836 seconds
DEBUG 01-15 16:09:16.808295.808295 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.809596.809596 cuda_h.py:19] end gpu_sexperts cost 0.0002624988555908203 seconds
DEBUG 01-15 16:09:16.809041.809041 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.809149.809149 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:09:16.809606.809606 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.809355.809355 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.810369.810369 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009629726409912109 seconds
DEBUG 01-15 16:09:16.810212.810212 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.810121.810121 cuda_h.py:19] end gpu_group_list cost 0.0002205371856689453 seconds
DEBUG 01-15 16:09:16.810797.810797 cuda_h.py:19] end group_tensors cost 0.005742788314819336 seconds
DEBUG 01-15 16:09:16.811982.811982 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.811716.811716 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.813400.813400 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016393661499023438 seconds
DEBUG 01-15 16:09:16.813954.813954 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.813874.813874 cuda_h.py:19] end gpu_group_list cost 0.0003211498260498047 seconds
DEBUG 01-15 16:09:16.814662.814662 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.814141.814141 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, dfcf14ef-c850-4ea4-9a58-1444078c63aa
DEBUG 01-15 16:09:16.815443.815443 cuda_h.py:19] end group pad cost 0.0038254261016845703 seconds
DEBUG 01-15 16:09:16.815339.815339 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:16.845952.845952 cuda_h.py:19] end group_einsum cost 0.030055999755859375 seconds
DEBUG 01-15 16:09:16.845162.845162 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.849516.849516 cuda_h.py:19] end get_outputs_cpu1 cost 0.0035719871520996094 seconds
DEBUG 01-15 16:09:16.849829.849829 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04649972915649414 seconds
INFO 01-15 16:09:16.851150.851150 client.py:127] Model loaded
DEBUG 01-15 16:09:16.851035.851035 cuda_h.py:19] end wait_experts_multi_device cost 0.03656959533691406 seconds
DEBUG 01-15 16:09:16.851275.851275 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.852904.852904 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005671977996826172 seconds
DEBUG 01-15 16:09:16.852171.852171 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.853317.853317 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.853513.853513 cuda_h.py:19] end index_scatter cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:09:16.853219.853219 cuda_h.py:19] end cpuoutputsdeal cost 0.0013995170593261719 seconds
DEBUG 01-15 16:09:16.853175.853175 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.853223.853223 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.853414.853414 cuda_h.py:19] end gpu_group_tensor cost 0.0001442432403564453 seconds
DEBUG 01-15 16:09:16.853793.853793 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.854248.854248 cuda_h.py:19] end gpu_group_tensor cost 0.00013065338134765625 seconds
DEBUG 01-15 16:09:16.854669.854669 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.854176.854176 cuda_h.py:19] end gpu_group_einsum cost 0.0006392002105712891 seconds
DEBUG 01-15 16:09:16.855750.855750 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.855263.855263 cuda_h.py:19] end gpu_group_einsum cost 0.00042700767517089844 seconds
DEBUG 01-15 16:09:16.855544.855544 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.855534.855534 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.855403.855403 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018262863159179688 seconds
DEBUG 01-15 16:09:16.856397.856397 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.856837.856837 cuda_h.py:19] end concat_expert_out cost 4.7206878662109375e-05 seconds
DEBUG 01-15 16:09:16.856534.856534 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.856027.856027 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:09:16.856505.856505 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007693767547607422 seconds
DEBUG 01-15 16:09:16.856151.856151 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.856716.856716 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.856821.856821 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015211105346679688 seconds
DEBUG 01-15 16:09:16.856385.856385 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.856640.856640 cuda_h.py:19] end concat_expert_out cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:09:16.856383.856383 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.857591.857591 cuda_h.py:19] end index_scatter cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:09:16.857731.857731 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004966259002685547 seconds
DEBUG 01-15 16:09:16.857972.857972 cuda_h.py:19] end gpu_experts_multi_device cost 0.04789614677429199 seconds
DEBUG 01-15 16:09:16.857359.857359 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.05814361572265625 seconds
DEBUG 01-15 16:09:16.857372.857372 cuda_h.py:19] end prefill_layer cost 0.06550908088684082 seconds
DEBUG 01-15 16:09:16.857553.857553 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:09:16.857733.857733 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.857866.857866 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:09:16.857238.857238 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:09:16.857232.857232 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:09:16.857652.857652 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 3.5762786865234375e-05 seconds
DEBUG 01-15 16:09:16.857024.857024 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 6.651878356933594e-05 seconds
DEBUG 01-15 16:09:16.857958.857958 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.858557.858557 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.858484.858484 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.858963.858963 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.858773.858773 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.859692.859692 cuda_h.py:19] end allocate_cuda_memory cost 0.000415802001953125 seconds
DEBUG 01-15 16:09:16.859862.859862 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.859542.859542 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.859566.859566 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.859403.859403 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e4977065-61a6-4293-a27e-392330d9cda8
DEBUG 01-15 16:09:16.859133.859133 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.860663.860663 cuda_h.py:10] start self_attn
INFO 01-15 16:09:16.860297.860297 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e4977065-61a6-4293-a27e-392330d9cda8
DEBUG 01-15 16:09:16.861514.861514 cuda_h.py:19] end load_into_gpu_async cost 0.00173187255859375 seconds
DEBUG 01-15 16:09:16.861067.861067 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.861048.861048 cuda_h.py:19] end restore_tensors2 cost 0.00014972686767578125 seconds
DEBUG 01-15 16:09:16.861429.861429 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003016948699951172 seconds
INFO 01-15 16:09:16.861049.861049 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e4977065-61a6-4293-a27e-392330d9cda8
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.863364.863364 cuda_h.py:19] end self_attn cost 0.0038423538208007812 seconds
DEBUG 01-15 16:09:16.864824.864824 cuda_h.py:19] end iln_self_attn_paln cost 0.006259918212890625 seconds
DEBUG 01-15 16:09:16.864793.864793 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:09:16.864310.864310 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.864260.864260 cuda_h.py:19] end gate cost 0.0006330013275146484 seconds
DEBUG 01-15 16:09:16.865659.865659 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.865504.865504 lmp.py:1912] 
DEBUG 01-15 16:09:16.865504.865504 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.865167.865167 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.865055.865055 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.865605.865605 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.865679.865679 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.865468.865468 lmp.py:1917] 
DEBUG 01-15 16:09:16.865468.865468 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.865495.865495 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.865337.865337 lmp.py:1935]   Expert 54 |     21 | CPU
DEBUG 01-15 16:09:16.865695.865695 lmp.py:1935]   Expert  3 |     33 | CPU
DEBUG 01-15 16:09:16.865099.865099 lmp.py:1935]   Expert  8 |     42 | CPU
DEBUG 01-15 16:09:16.865219.865219 lmp.py:1935]   Expert 28 |     43 | CPU
DEBUG 01-15 16:09:16.865862.865862 lmp.py:1935]   Expert 63 |     53 | CPU
DEBUG 01-15 16:09:16.865267.865267 lmp.py:1935]   Expert 43 |     54 | CPU
DEBUG 01-15 16:09:16.865433.865433 lmp.py:1935]   Expert 36 |     76 | CPU
DEBUG 01-15 16:09:16.865076.865076 lmp.py:1935]   Expert 38 |     77 | CPU
DEBUG 01-15 16:09:16.865480.865480 lmp.py:1935]   Expert  6 |     83 | CPU
DEBUG 01-15 16:09:16.865885.865885 lmp.py:1935]   Expert 57 |     97 | CPU
DEBUG 01-15 16:09:16.865290.865290 lmp.py:1935]   Expert 39 |     99 | CPU
DEBUG 01-15 16:09:16.865694.865694 lmp.py:1935]   Expert 41 |    103 | CPU
DEBUG 01-15 16:09:16.865099.865099 lmp.py:1935]   Expert 12 |    109 | CPU
DEBUG 01-15 16:09:16.865457.865457 lmp.py:1935]   Expert 52 |    114 | CPU
DEBUG 01-15 16:09:16.865530.865530 lmp.py:1935]   Expert 19 |    121 | CPU
DEBUG 01-15 16:09:16.865650.865650 lmp.py:1935]   Expert 47 |    125 | CPU
DEBUG 01-15 16:09:16.865055.865055 lmp.py:1935]   Expert 13 |    135 | CPU
DEBUG 01-15 16:09:16.865221.865221 lmp.py:1935]   Expert 22 |    141 | CPU
DEBUG 01-15 16:09:16.865387.865387 lmp.py:1935]   Expert 46 |    146 | CPU
DEBUG 01-15 16:09:16.865553.865553 lmp.py:1935]   Expert 50 |    150 | CPU
DEBUG 01-15 16:09:16.865719.865719 lmp.py:1935]   Expert 24 |    163 | CPU
DEBUG 01-15 16:09:16.865885.865885 lmp.py:1935]   Expert 40 |    163 | CPU
DEBUG 01-15 16:09:16.865575.865575 lmp.py:1935]   Expert 55 |    164 | CPU
DEBUG 01-15 16:09:16.865741.865741 lmp.py:1935]   Expert 20 |    165 | CPU
DEBUG 01-15 16:09:16.865430.865430 lmp.py:1935]   Expert 37 |    168 | CPU
DEBUG 01-15 16:09:16.865027.865027 lmp.py:1935]   Expert 23 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.865816.865816 lmp.py:1935]   Expert 53 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.865081.865081 lmp.py:1935]   Expert  2 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.865393.865393 lmp.py:1935]   Expert 49 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866943.866943 lmp.py:1935]   Expert 61 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866540.866540 lmp.py:1935]   Expert 21 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866421.866421 lmp.py:1935]   Expert 42 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866303.866303 lmp.py:1935]   Expert 33 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866423.866423 lmp.py:1935]   Expert 18 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866542.866542 lmp.py:1935]   Expert 32 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866424.866424 lmp.py:1935]   Expert  0 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866544.866544 lmp.py:1935]   Expert  5 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866663.866663 lmp.py:1935]   Expert 16 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866545.866545 lmp.py:1935]   Expert 30 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866857.866857 lmp.py:1935]   Expert 14 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866692.866692 lmp.py:1935]   Expert 34 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866242.866242 lmp.py:1935]   Expert  7 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866600.866600 lmp.py:1935]   Expert 31 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866243.866243 lmp.py:1935]   Expert 62 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866363.866363 lmp.py:1935]   Expert 59 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866483.866483 lmp.py:1935]   Expert 60 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866126.866126 lmp.py:1935]   Expert  9 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866007.866007 lmp.py:1935]   Expert 17 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866650.866650 lmp.py:1935]   Expert 10 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866532.866532 lmp.py:1935]   Expert 29 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866175.866175 lmp.py:1935]   Expert  4 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866579.866579 lmp.py:1935]   Expert 58 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866461.866461 lmp.py:1935]   Expert 15 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866580.866580 lmp.py:1935]   Expert 26 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866654.866654 lmp.py:1935]   Expert 51 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866251.866251 lmp.py:1935]   Expert 11 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866370.866370 lmp.py:1935]   Expert 44 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866205.866205 lmp.py:1935]   Expert 56 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866087.866087 lmp.py:1935]   Expert 27 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866207.866207 lmp.py:1935]   Expert  1 |    331 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866088.866088 lmp.py:1935]   Expert 45 |    364 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866208.866208 lmp.py:1935]   Expert 25 |    463 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866089.866089 lmp.py:1935]   Expert 35 |    520 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.866971.866971 lmp.py:1935]   Expert 48 |    644 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.866898.866898 lmp.py:1937] 
DEBUG 01-15 16:09:16.866898.866898 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.866780.866780 lmp.py:1938]   CPU:   2645 tokens
DEBUG 01-15 16:09:16.866853.866853 lmp.py:1942]   cuda:1:   4907 tokens (20 experts)
DEBUG 01-15 16:09:16.866165.866165 lmp.py:1942]   cuda:2:   4736 tokens (19 experts)
DEBUG 01-15 16:09:16.866808.866808 lmp.py:1943]   Total GPU:   9643 tokens
DEBUG 01-15 16:09:16.866690.866690 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.866690.866690 lmp.py:1944] 
DEBUG 01-15 16:09:16.866862.866862 cuda_h.py:19] end experts_map_get cost 0.0017039775848388672 seconds
DEBUG 01-15 16:09:16.866944.866944 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.866316.866316 lmp.py:1953] 
DEBUG 01-15 16:09:16.866316.866316 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.866145.866145 cuda_h.py:19] end cpu_experts_submit cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:09:16.866841.866841 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.866148.866148 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.868742.868742 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.20.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.20.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.20.mlp.experts.60.up_proj.weight': 340262912}, 2: {'model.layers.20.mlp.experts.1.gate_proj.weight': 0, 'model.layers.20.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.58.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.58.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.58.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (23968350208, 5767168, 17301504, 0), (23974117376, 5767168, 23068672, 0), (23962583040, 5767168, 28835840, 0), (23985651712, 5767168, 34603008, 0), (23991418880, 5767168, 40370176, 0), (23979884544, 5767168, 46137344, 0), (24020254720, 5767168, 51904512, 0), (24026021888, 5767168, 57671680, 0), (24014487552, 5767168, 63438848, 0), (24175968256, 5767168, 69206016, 0), (24181735424, 5767168, 74973184, 0), (24170201088, 5767168, 80740352, 0), (24193269760, 5767168, 86507520, 0), (24199036928, 5767168, 92274688, 0), (24187502592, 5767168, 98041856, 0), (24297078784, 5767168, 103809024, 0), (24302845952, 5767168, 109576192, 0), (24291311616, 5767168, 115343360, 0), (24348983296, 5767168, 121110528, 0), (24354750464, 5767168, 126877696, 0), (24343216128, 5767168, 132644864, 0), (24366284800, 5767168, 138412032, 0), (24372051968, 5767168, 144179200, 0), (24360517632, 5767168, 149946368, 0), (24400887808, 5767168, 155713536, 0), (24406654976, 5767168, 161480704, 0), (24395120640, 5767168, 167247872, 0), (24435490816, 5767168, 173015040, 0), (24441257984, 5767168, 178782208, 0), (24429723648, 5767168, 184549376, 0), (24470093824, 5767168, 190316544, 0), (24475860992, 5767168, 196083712, 0), (24464326656, 5767168, 201850880, 0), (24625807360, 5767168, 207618048, 0), (24631574528, 5767168, 213385216, 0), (24620040192, 5767168, 219152384, 0), (24677711872, 5767168, 224919552, 0), (24683479040, 5767168, 230686720, 0), (24671944704, 5767168, 236453888, 0), (24729616384, 5767168, 242221056, 0), (24735383552, 5767168, 247988224, 0), (24723849216, 5767168, 253755392, 0), (24746917888, 5767168, 259522560, 0), (24752685056, 5767168, 265289728, 0), (24741150720, 5767168, 271056896, 0), (24781520896, 5767168, 276824064, 0), (24787288064, 5767168, 282591232, 0), (24775753728, 5767168, 288358400, 0), (24816123904, 5767168, 294125568, 0), (24821891072, 5767168, 299892736, 0), (24810356736, 5767168, 305659904, 0), (24868028416, 5767168, 311427072, 0), (24873795584, 5767168, 317194240, 0), (24862261248, 5767168, 322961408, 0), (24937234432, 5767168, 328728576, 0), (24943001600, 5767168, 334495744, 0), (24931467264, 5767168, 340262912, 0)], 2: [(23916445696, 5767168, 0, 0), (23922212864, 5767168, 5767168, 0), (23910678528, 5767168, 11534336, 0), (23933747200, 5767168, 17301504, 0), (23939514368, 5767168, 23068672, 0), (23927980032, 5767168, 28835840, 0), (24054857728, 5767168, 34603008, 0), (24060624896, 5767168, 40370176, 0), (24049090560, 5767168, 46137344, 0), (24072159232, 5767168, 51904512, 0), (24077926400, 5767168, 57671680, 0), (24066392064, 5767168, 63438848, 0), (24089460736, 5767168, 69206016, 0), (24095227904, 5767168, 74973184, 0), (24083693568, 5767168, 80740352, 0), (24141365248, 5767168, 86507520, 0), (24147132416, 5767168, 92274688, 0), (24135598080, 5767168, 98041856, 0), (24158666752, 5767168, 103809024, 0), (24164433920, 5767168, 109576192, 0), (24152899584, 5767168, 115343360, 0), (24210571264, 5767168, 121110528, 0), (24216338432, 5767168, 126877696, 0), (24204804096, 5767168, 132644864, 0), (24262475776, 5767168, 138412032, 0), (24268242944, 5767168, 144179200, 0), (24256708608, 5767168, 149946368, 0), (24331681792, 5767168, 155713536, 0), (24337448960, 5767168, 161480704, 0), (24325914624, 5767168, 167247872, 0), (24418189312, 5767168, 173015040, 0), (24423956480, 5767168, 178782208, 0), (24412422144, 5767168, 184549376, 0), (24452792320, 5767168, 190316544, 0), (24458559488, 5767168, 196083712, 0), (24447025152, 5767168, 201850880, 0), (24487395328, 5767168, 207618048, 0), (24493162496, 5767168, 213385216, 0), (24481628160, 5767168, 219152384, 0), (24504696832, 5767168, 224919552, 0), (24510464000, 5767168, 230686720, 0), (24498929664, 5767168, 236453888, 0), (24660410368, 5767168, 242221056, 0), (24666177536, 5767168, 247988224, 0), (24654643200, 5767168, 253755392, 0), (24902631424, 5767168, 259522560, 0), (24908398592, 5767168, 265289728, 0), (24896864256, 5767168, 271056896, 0), (24919932928, 5767168, 276824064, 0), (24925700096, 5767168, 282591232, 0), (24914165760, 5767168, 288358400, 0), (24954535936, 5767168, 294125568, 0), (24960303104, 5767168, 299892736, 0), (24948768768, 5767168, 305659904, 0), (24971837440, 5767168, 311427072, 0), (24977604608, 5767168, 317194240, 0), (24966070272, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660702dc0>, 2: <capsule object NULL at 0x74a6785de820>}
DEBUG 01-15 16:09:16.868935.868935 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.868844.868844 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.868170.868170 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 76a04098-6cc6-42a3-a2b2-fc399f355697
DEBUG 01-15 16:09:16.868640.868640 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:16.869331.869331 client.py:127] Model loaded
DEBUG 01-15 16:09:16.869090.869090 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.869674.869674 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:16.869362.869362 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:16.869419.869419 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 76a04098-6cc6-42a3-a2b2-fc399f355697
DEBUG 01-15 16:09:16.869851.869851 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0029811859130859375 seconds
DEBUG 01-15 16:09:16.870291.870291 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.870941.870941 cuda_h.py:19] end move_flatidxs cost 0.0008356571197509766 seconds
DEBUG 01-15 16:09:16.870777.870777 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.871784.871784 cuda_h.py:19] end restore2model cost 0.0012249946594238281 seconds
DEBUG 01-15 16:09:16.871170.871170 cuda_h.py:19] end sllm_worker_task cost 0.013297080993652344 seconds
DEBUG 01-15 16:09:16.874756.874756 cuda_h.py:19] end restore2model cost 0.004167079925537109 seconds
DEBUG 01-15 16:09:16.874659.874659 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00739288330078125 seconds
DEBUG 01-15 16:09:16.874263.874263 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.874703.874703 cuda_h.py:19] end gpu_sexperts cost 0.00025963783264160156 seconds
DEBUG 01-15 16:09:16.874671.874671 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.874494.874494 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7404556274414062e-05 seconds
DEBUG 01-15 16:09:16.874475.874475 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.874793.874793 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.875548.875548 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009479522705078125 seconds
DEBUG 01-15 16:09:16.875537.875537 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.876293.876293 cuda_h.py:19] end gpu_group_list cost 0.00021314620971679688 seconds
DEBUG 01-15 16:09:16.876076.876076 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.877402.877402 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009875297546386719 seconds
DEBUG 01-15 16:09:16.878405.878405 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.878816.878816 cuda_h.py:19] end gpu_group_list cost 0.00020456314086914062 seconds
DEBUG 01-15 16:09:16.878368.878368 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.878244.878244 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 76a04098-6cc6-42a3-a2b2-fc399f355697
DEBUG 01-15 16:09:16.879353.879353 cuda_h.py:19] end group_tensors cost 0.008845806121826172 seconds
DEBUG 01-15 16:09:16.879358.879358 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.883499.883499 cuda_h.py:19] end group pad cost 0.003560781478881836 seconds
DEBUG 01-15 16:09:16.883097.883097 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:16.914791.914791 cuda_h.py:19] end group_einsum cost 0.03033161163330078 seconds
DEBUG 01-15 16:09:16.914094.914094 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:16.916774.916774 client.py:127] Model loaded
DEBUG 01-15 16:09:16.916705.916705 cuda_h.py:19] end wait_experts_multi_device cost 0.037689208984375 seconds
DEBUG 01-15 16:09:16.916468.916468 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.917176.917176 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029850006103515625 seconds
DEBUG 01-15 16:09:16.918494.918494 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0487515926361084 seconds
DEBUG 01-15 16:09:16.918927.918927 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0021529197692871094 seconds
DEBUG 01-15 16:09:16.919032.919032 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.920761.920761 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.921371.921371 cuda_h.py:19] end index_scatter cost 8.130073547363281e-05 seconds
DEBUG 01-15 16:09:16.921981.921981 cuda_h.py:19] end cpuoutputsdeal cost 0.0023355484008789062 seconds
DEBUG 01-15 16:09:16.921427.921427 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.921475.921475 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.921150.921150 cuda_h.py:19] end gpu_group_tensor cost 0.00014972686767578125 seconds
DEBUG 01-15 16:09:16.921720.921720 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.922222.922222 cuda_h.py:19] end gpu_group_tensor cost 0.00012993812561035156 seconds
DEBUG 01-15 16:09:16.922842.922842 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.922316.922316 cuda_h.py:19] end gpu_group_einsum cost 0.0004761219024658203 seconds
DEBUG 01-15 16:09:16.922049.922049 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.923417.923417 cuda_h.py:19] end gpu_group_einsum cost 0.0004639625549316406 seconds
DEBUG 01-15 16:09:16.923124.923124 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.923194.923194 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.923819.923819 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023865699768066406 seconds
DEBUG 01-15 16:09:16.923781.923781 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.923718.923718 cuda_h.py:19] end concat_expert_out cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:09:16.924323.924323 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.924961.924961 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:16.924896.924896 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008673667907714844 seconds
DEBUG 01-15 16:09:16.924848.924848 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.924274.924274 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.924578.924578 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001583099365234375 seconds
DEBUG 01-15 16:09:16.924858.924858 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.924973.924973 cuda_h.py:19] end concat_expert_out cost 5.340576171875e-05 seconds
DEBUG 01-15 16:09:16.924147.924147 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.924401.924401 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:16.925495.925495 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005054473876953125 seconds
DEBUG 01-15 16:09:16.925690.925690 cuda_h.py:19] end gpu_experts_multi_device cost 0.05031228065490723 seconds
DEBUG 01-15 16:09:16.925884.925884 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.060854196548461914 seconds
DEBUG 01-15 16:09:16.925396.925396 cuda_h.py:19] end prefill_layer cost 0.06783676147460938 seconds
DEBUG 01-15 16:09:16.925755.925755 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:09:16.925697.925697 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:16.925353.925353 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:09:16.925487.925487 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:09:16.925720.925720 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:09:16.925000.925000 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:09:16.925325.925325 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:09:16.925783.925783 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:16.925799.925799 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:16.926041.926041 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:16.926577.926577 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:16.926450.926450 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:16.927066.927066 cuda_h.py:19] end allocate_cuda_memory cost 0.00045943260192871094 seconds
DEBUG 01-15 16:09:16.927243.927243 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:16.927969.927969 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.927133.927133 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.927661.927661 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f9e323c6-9f96-4908-b3e9-ce3437f8eaaf
DEBUG 01-15 16:09:16.927388.927388 cuda_h.py:10] start self_attn
DEBUG 01-15 16:09:16.927656.927656 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:16.929633.929633 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f9e323c6-9f96-4908-b3e9-ce3437f8eaaf
DEBUG 01-15 16:09:16.929883.929883 cuda_h.py:19] end load_into_gpu_async cost 0.0019698143005371094 seconds
DEBUG 01-15 16:09:16.929575.929575 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:16.929185.929185 cuda_h.py:19] end restore_tensors2 cost 0.00015115737915039062 seconds
DEBUG 01-15 16:09:16.929335.929335 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0033273696899414062 seconds
INFO 01-15 16:09:16.929585.929585 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f9e323c6-9f96-4908-b3e9-ce3437f8eaaf
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:16.931748.931748 cuda_h.py:19] end self_attn cost 0.0038175582885742188 seconds
DEBUG 01-15 16:09:16.932294.932294 cuda_h.py:19] end iln_self_attn_paln cost 0.006139039993286133 seconds
DEBUG 01-15 16:09:16.932501.932501 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:09:16.932734.932734 cuda_h.py:10] start gate
DEBUG 01-15 16:09:16.932644.932644 cuda_h.py:19] end gate cost 0.000637054443359375 seconds
DEBUG 01-15 16:09:16.932473.932473 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:16.933630.933630 lmp.py:1912] 
DEBUG 01-15 16:09:16.933630.933630 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:16.933817.933817 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:16.933612.933612 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:16.933878.933878 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:16.933951.933951 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:16.933786.933786 lmp.py:1917] 
DEBUG 01-15 16:09:16.933786.933786 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:16.933144.933144 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:16.933509.933509 lmp.py:1935]   Expert 44 |     28 | CPU
DEBUG 01-15 16:09:16.933152.933152 lmp.py:1935]   Expert  9 |     34 | CPU
DEBUG 01-15 16:09:16.933842.933842 lmp.py:1935]   Expert 11 |     37 | CPU
DEBUG 01-15 16:09:16.933200.933200 lmp.py:1935]   Expert 56 |     59 | CPU
DEBUG 01-15 16:09:16.933558.933558 lmp.py:1935]   Expert 54 |     77 | CPU
DEBUG 01-15 16:09:16.933201.933201 lmp.py:1935]   Expert 62 |     92 | CPU
DEBUG 01-15 16:09:16.933083.933083 lmp.py:1935]   Expert 47 |     94 | CPU
DEBUG 01-15 16:09:16.933964.933964 lmp.py:1935]   Expert  7 |     95 | CPU
DEBUG 01-15 16:09:16.933322.933322 lmp.py:1935]   Expert 51 |     99 | CPU
DEBUG 01-15 16:09:16.933634.933634 lmp.py:1935]   Expert 60 |    106 | CPU
DEBUG 01-15 16:09:16.933992.933992 lmp.py:1935]   Expert 52 |    108 | CPU
DEBUG 01-15 16:09:16.933079.933079 lmp.py:1935]   Expert 22 |    109 | CPU
DEBUG 01-15 16:09:16.933007.933007 lmp.py:1935]   Expert 41 |    111 | CPU
DEBUG 01-15 16:09:16.933173.933173 lmp.py:1935]   Expert 53 |    114 | CPU
DEBUG 01-15 16:09:16.933339.933339 lmp.py:1935]   Expert 48 |    124 | CPU
DEBUG 01-15 16:09:16.933744.933744 lmp.py:1935]   Expert  6 |    126 | CPU
DEBUG 01-15 16:09:16.933910.933910 lmp.py:1935]   Expert  8 |    128 | CPU
DEBUG 01-15 16:09:16.933838.933838 lmp.py:1935]   Expert  1 |    129 | CPU
DEBUG 01-15 16:09:16.933527.933527 lmp.py:1935]   Expert 32 |    130 | CPU
DEBUG 01-15 16:09:16.933455.933455 lmp.py:1935]   Expert  2 |    132 | CPU
DEBUG 01-15 16:09:16.933383.933383 lmp.py:1935]   Expert 27 |    138 | CPU
DEBUG 01-15 16:09:16.933502.933502 lmp.py:1935]   Expert 35 |    140 | CPU
DEBUG 01-15 16:09:16.933384.933384 lmp.py:1935]   Expert 23 |    141 | CPU
DEBUG 01-15 16:09:16.933457.933457 lmp.py:1935]   Expert 39 |    144 | CPU
DEBUG 01-15 16:09:16.933147.933147 lmp.py:1935]   Expert 59 |    145 | CPU
DEBUG 01-15 16:09:16.933982.933982 lmp.py:1935]   Expert 26 |    146 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.933055.933055 lmp.py:1935]   Expert 50 |    148 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.933652.933652 lmp.py:1935]   Expert 14 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.933295.933295 lmp.py:1935]   Expert 24 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.933938.933938 lmp.py:1935]   Expert 46 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.933819.933819 lmp.py:1935]   Expert 38 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.933224.933224 lmp.py:1935]   Expert  4 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.933105.933105 lmp.py:1935]   Expert 34 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.933417.933417 lmp.py:1935]   Expert  0 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.933252.933252 lmp.py:1935]   Expert 40 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.933325.933325 lmp.py:1935]   Expert 49 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934968.934968 lmp.py:1935]   Expert  5 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934850.934850 lmp.py:1935]   Expert 63 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934254.934254 lmp.py:1935]   Expert 19 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934374.934374 lmp.py:1935]   Expert 13 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934779.934779 lmp.py:1935]   Expert 43 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934660.934660 lmp.py:1935]   Expert 29 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934065.934065 lmp.py:1935]   Expert 61 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934469.934469 lmp.py:1935]   Expert 57 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934351.934351 lmp.py:1935]   Expert 33 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934470.934470 lmp.py:1935]   Expert 31 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934352.934352 lmp.py:1935]   Expert 16 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934949.934949 lmp.py:1935]   Expert 37 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934022.934022 lmp.py:1935]   Expert 20 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934619.934619 lmp.py:1935]   Expert  3 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934023.934023 lmp.py:1935]   Expert 15 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934666.934666 lmp.py:1935]   Expert 36 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934309.934309 lmp.py:1935]   Expert 12 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934952.934952 lmp.py:1935]   Expert 18 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934595.934595 lmp.py:1935]   Expert 17 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934238.934238 lmp.py:1935]   Expert 28 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934643.934643 lmp.py:1935]   Expert 55 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934001.934001 lmp.py:1935]   Expert 25 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934644.934644 lmp.py:1935]   Expert 30 |    319 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934287.934287 lmp.py:1935]   Expert 58 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934506.934506 lmp.py:1935]   Expert 10 |    365 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934056.934056 lmp.py:1935]   Expert 45 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934176.934176 lmp.py:1935]   Expert 21 |    390 | GPU2(cuda:2)
DEBUG 01-15 16:09:16.934011.934011 lmp.py:1935]   Expert 42 |    643 | GPU1(cuda:1)
DEBUG 01-15 16:09:16.934939.934939 lmp.py:1937] 
DEBUG 01-15 16:09:16.934939.934939 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:16.934059.934059 lmp.py:1938]   CPU:   2640 tokens
DEBUG 01-15 16:09:16.934179.934179 lmp.py:1942]   cuda:1:   4827 tokens (19 experts)
DEBUG 01-15 16:09:16.934298.934298 lmp.py:1942]   cuda:2:   4821 tokens (20 experts)
DEBUG 01-15 16:09:16.934226.934226 lmp.py:1943]   Total GPU:   9648 tokens
DEBUG 01-15 16:09:16.934677.934677 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:16.934677.934677 lmp.py:1944] 
DEBUG 01-15 16:09:16.934327.934327 cuda_h.py:19] end experts_map_get cost 0.0017230510711669922 seconds
DEBUG 01-15 16:09:16.934362.934362 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:16.934734.934734 lmp.py:1953] 
DEBUG 01-15 16:09:16.934734.934734 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:16.934424.934424 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:16.934379.934379 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:16.934315.934315 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:16.936804.936804 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.5.gate_proj.weight': 0, 'model.layers.21.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.38.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.38.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.38.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.21.mlp.experts.0.gate_proj.weight': 0, 'model.layers.21.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.21.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.21.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.21.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(25092947968, 5767168, 0, 0), (25098715136, 5767168, 5767168, 0), (25087180800, 5767168, 11534336, 0), (25179455488, 5767168, 17301504, 0), (25185222656, 5767168, 23068672, 0), (25173688320, 5767168, 28835840, 0), (25214058496, 5767168, 34603008, 0), (25219825664, 5767168, 40370176, 0), (25208291328, 5767168, 46137344, 0), (25265963008, 5767168, 51904512, 0), (25271730176, 5767168, 57671680, 0), (25260195840, 5767168, 63438848, 0), (25283264512, 5767168, 69206016, 0), (25289031680, 5767168, 74973184, 0), (25277497344, 5767168, 80740352, 0), (25300566016, 5767168, 86507520, 0), (25306333184, 5767168, 92274688, 0), (25294798848, 5767168, 98041856, 0), (25335169024, 5767168, 103809024, 0), (25340936192, 5767168, 109576192, 0), (25329401856, 5767168, 115343360, 0), (25352470528, 5767168, 121110528, 0), (25358237696, 5767168, 126877696, 0), (25346703360, 5767168, 132644864, 0), (25421676544, 5767168, 138412032, 0), (25427443712, 5767168, 144179200, 0), (25415909376, 5767168, 149946368, 0), (25456279552, 5767168, 155713536, 0), (25462046720, 5767168, 161480704, 0), (25450512384, 5767168, 167247872, 0), (25525485568, 5767168, 173015040, 0), (25531252736, 5767168, 178782208, 0), (25519718400, 5767168, 184549376, 0), (25577390080, 5767168, 190316544, 0), (25583157248, 5767168, 196083712, 0), (25571622912, 5767168, 201850880, 0), (25594691584, 5767168, 207618048, 0), (25600458752, 5767168, 213385216, 0), (25588924416, 5767168, 219152384, 0), (25663897600, 5767168, 224919552, 0), (25669664768, 5767168, 230686720, 0), (25658130432, 5767168, 236453888, 0), (25733103616, 5767168, 242221056, 0), (25738870784, 5767168, 247988224, 0), (25727336448, 5767168, 253755392, 0), (25750405120, 5767168, 259522560, 0), (25756172288, 5767168, 265289728, 0), (25744637952, 5767168, 271056896, 0), (25854214144, 5767168, 276824064, 0), (25859981312, 5767168, 282591232, 0), (25848446976, 5767168, 288358400, 0), (25958023168, 5767168, 294125568, 0), (25963790336, 5767168, 299892736, 0), (25952256000, 5767168, 305659904, 0), (26061832192, 5767168, 311427072, 0), (26067599360, 5767168, 317194240, 0), (26056065024, 5767168, 322961408, 0)], 2: [(25006440448, 5767168, 0, 0), (25012207616, 5767168, 5767168, 0), (25000673280, 5767168, 11534336, 0), (25058344960, 5767168, 17301504, 0), (25064112128, 5767168, 23068672, 0), (25052577792, 5767168, 28835840, 0), (25075646464, 5767168, 34603008, 0), (25081413632, 5767168, 40370176, 0), (25069879296, 5767168, 46137344, 0), (25231360000, 5767168, 51904512, 0), (25237127168, 5767168, 57671680, 0), (25225592832, 5767168, 63438848, 0), (25248661504, 5767168, 69206016, 0), (25254428672, 5767168, 74973184, 0), (25242894336, 5767168, 80740352, 0), (25317867520, 5767168, 86507520, 0), (25323634688, 5767168, 92274688, 0), (25312100352, 5767168, 98041856, 0), (25369772032, 5767168, 103809024, 0), (25375539200, 5767168, 109576192, 0), (25364004864, 5767168, 115343360, 0), (25438978048, 5767168, 121110528, 0), (25444745216, 5767168, 126877696, 0), (25433210880, 5767168, 132644864, 0), (25490882560, 5767168, 138412032, 0), (25496649728, 5767168, 144179200, 0), (25485115392, 5767168, 149946368, 0), (25508184064, 5767168, 155713536, 0), (25513951232, 5767168, 161480704, 0), (25502416896, 5767168, 167247872, 0), (25542787072, 5767168, 173015040, 0), (25548554240, 5767168, 178782208, 0), (25537019904, 5767168, 184549376, 0), (25629294592, 5767168, 190316544, 0), (25635061760, 5767168, 196083712, 0), (25623527424, 5767168, 201850880, 0), (25646596096, 5767168, 207618048, 0), (25652363264, 5767168, 213385216, 0), (25640828928, 5767168, 219152384, 0), (25698500608, 5767168, 224919552, 0), (25704267776, 5767168, 230686720, 0), (25692733440, 5767168, 236453888, 0), (25785008128, 5767168, 242221056, 0), (25790775296, 5767168, 247988224, 0), (25779240960, 5767168, 253755392, 0), (25802309632, 5767168, 259522560, 0), (25808076800, 5767168, 265289728, 0), (25796542464, 5767168, 271056896, 0), (25871515648, 5767168, 276824064, 0), (25877282816, 5767168, 282591232, 0), (25865748480, 5767168, 288358400, 0), (25992626176, 5767168, 294125568, 0), (25998393344, 5767168, 299892736, 0), (25986859008, 5767168, 305659904, 0), (26009927680, 5767168, 311427072, 0), (26015694848, 5767168, 317194240, 0), (26004160512, 5767168, 322961408, 0), (26096435200, 5767168, 328728576, 0), (26102202368, 5767168, 334495744, 0), (26090668032, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6882a3240>, 2: <capsule object NULL at 0x74a6785de6d0>}
DEBUG 01-15 16:09:16.936658.936658 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:16.936700.936700 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:16.936979.936979 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6b7fb89e-3c82-4d1d-80d5-f920df838784
DEBUG 01-15 16:09:16.937556.937556 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:16.937808.937808 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:16.937337.937337 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:16.937320.937320 client.py:127] Model loaded
DEBUG 01-15 16:09:16.937192.937192 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.938282.938282 cuda_h.py:19] end move_flatidxs cost 0.0008864402770996094 seconds
DEBUG 01-15 16:09:16.938549.938549 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:16.938784.938784 cuda_h.py:19] end restore2model cost 0.0009789466857910156 seconds
INFO 01-15 16:09:16.938622.938622 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6b7fb89e-3c82-4d1d-80d5-f920df838784
DEBUG 01-15 16:09:16.938751.938751 cuda_h.py:19] end sllm_worker_task cost 0.012530803680419922 seconds
DEBUG 01-15 16:09:16.939616.939616 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004400491714477539 seconds
DEBUG 01-15 16:09:16.939296.939296 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:16.942276.942276 cuda_h.py:19] end restore2model cost 0.002935647964477539 seconds
DEBUG 01-15 16:09:16.942113.942113 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007647275924682617 seconds
DEBUG 01-15 16:09:16.942498.942498 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:16.942899.942899 cuda_h.py:19] end gpu_sexperts cost 0.0002655982971191406 seconds
DEBUG 01-15 16:09:16.942106.942106 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:16.942829.942829 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4543533325195312e-05 seconds
DEBUG 01-15 16:09:16.942333.942333 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:16.942036.942036 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.943134.943134 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009202957153320312 seconds
DEBUG 01-15 16:09:16.943454.943454 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.944421.944421 cuda_h.py:19] end gpu_group_list cost 0.000194549560546875 seconds
DEBUG 01-15 16:09:16.945363.945363 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:16.946625.946625 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010461807250976562 seconds
DEBUG 01-15 16:09:16.946806.946806 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:16.946510.946510 cuda_h.py:19] end gpu_group_list cost 0.0002086162567138672 seconds
DEBUG 01-15 16:09:16.947565.947565 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:16.947156.947156 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6b7fb89e-3c82-4d1d-80d5-f920df838784
DEBUG 01-15 16:09:16.949737.949737 cuda_h.py:19] end group_tensors cost 0.010910511016845703 seconds
DEBUG 01-15 16:09:16.950540.950540 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:16.955731.955731 cuda_h.py:19] end group pad cost 0.005457162857055664 seconds
DEBUG 01-15 16:09:16.955568.955568 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:16.977100.977100 client.py:127] Model loaded
DEBUG 01-15 16:09:16.977463.977463 cuda_h.py:19] end wait_experts_multi_device cost 0.03068685531616211 seconds
DEBUG 01-15 16:09:16.977510.977510 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:16.985127.985127 cuda_h.py:19] end group_einsum cost 0.029286861419677734 seconds
DEBUG 01-15 16:09:16.985331.985331 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:16.991263.991263 cuda_h.py:19] end get_outputs_cpu1 cost 0.005821704864501953 seconds
DEBUG 01-15 16:09:16.991956.991956 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.054787635803222656 seconds
DEBUG 01-15 16:09:16.992040.992040 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.014745235443115234 seconds
DEBUG 01-15 16:09:16.992343.992343 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:16.994779.994779 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.994919.994919 cuda_h.py:19] end index_scatter cost 0.00010371208190917969 seconds
DEBUG 01-15 16:09:16.994389.994389 cuda_h.py:19] end cpuoutputsdeal cost 0.0019028186798095703 seconds
DEBUG 01-15 16:09:16.994452.994452 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:16.994143.994143 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.995820.995820 cuda_h.py:19] end gpu_group_tensor cost 0.0002048015594482422 seconds
DEBUG 01-15 16:09:16.995987.995987 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:16.995823.995823 cuda_h.py:19] end gpu_group_tensor cost 0.00018548965454101562 seconds
DEBUG 01-15 16:09:16.995973.995973 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.996874.996874 cuda_h.py:19] end gpu_group_einsum cost 0.0010833740234375 seconds
DEBUG 01-15 16:09:16.997957.997957 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:16.997387.997387 cuda_h.py:19] end gpu_group_einsum cost 0.0007092952728271484 seconds
DEBUG 01-15 16:09:16.998227.998227 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.998232.998232 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.998041.998041 cuda_h.py:19] end all_expert_outputs_slices cost 0.00037169456481933594 seconds
DEBUG 01-15 16:09:16.998850.998850 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.998423.998423 cuda_h.py:19] end concat_expert_out cost 6.818771362304688e-05 seconds
DEBUG 01-15 16:09:16.998194.998194 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:16.998880.998880 cuda_h.py:19] end index_scatter cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:09:16.999843.999843 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011315345764160156 seconds
DEBUG 01-15 16:09:16.999655.999655 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:16.999842.999842 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:16.999931.999931 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023555755615234375 seconds
DEBUG 01-15 16:09:16.999641.999641 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:16.999585.999585 cuda_h.py:19] end concat_expert_out cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:09:16.999295.999295 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.000239.000239 cuda_h.py:19] end index_scatter cost 6.437301635742188e-05 seconds
DEBUG 01-15 16:09:17.000909.000909 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000640869140625 seconds
DEBUG 01-15 16:09:17.000337.000337 cuda_h.py:19] end gpu_experts_multi_device cost 0.05724811553955078 seconds
DEBUG 01-15 16:09:17.000320.000320 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.0681304931640625 seconds
DEBUG 01-15 16:09:17.000746.000746 cuda_h.py:19] end prefill_layer cost 0.0752115249633789 seconds
DEBUG 01-15 16:09:17.000927.000927 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:09:17.001491.001491 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:17.001962.001962 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:09:17.001910.001910 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:09:17.001243.001243 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:09:17.001199.001199 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 4.1484832763671875e-05 seconds
DEBUG 01-15 16:09:17.001769.001769 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 7.891654968261719e-05 seconds
DEBUG 01-15 16:09:17.001519.001519 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:17.001502.001502 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:17.001101.001101 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:17.001243.001243 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:17.001279.001279 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:17.002538.002538 cuda_h.py:19] end allocate_cuda_memory cost 0.0003986358642578125 seconds
DEBUG 01-15 16:09:17.002660.002660 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:17.002715.002715 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.002498.002498 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.002492.002492 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 33d49fa0-00c0-4bb3-a52e-c19b5bef8404
DEBUG 01-15 16:09:17.002211.002211 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.002526.002526 cuda_h.py:10] start self_attn
INFO 01-15 16:09:17.004012.004012 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 33d49fa0-00c0-4bb3-a52e-c19b5bef8404
DEBUG 01-15 16:09:17.004331.004331 cuda_h.py:19] end load_into_gpu_async cost 0.0019998550415039062 seconds
DEBUG 01-15 16:09:17.004895.004895 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:17.004151.004151 cuda_h.py:19] end restore_tensors2 cost 8.487701416015625e-05 seconds
DEBUG 01-15 16:09:17.004768.004768 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002774953842163086 seconds
INFO 01-15 16:09:17.004929.004929 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 33d49fa0-00c0-4bb3-a52e-c19b5bef8404
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:17.006512.006512 cuda_h.py:19] end self_attn cost 0.003846883773803711 seconds
DEBUG 01-15 16:09:17.006292.006292 cuda_h.py:19] end iln_self_attn_paln cost 0.0056610107421875 seconds
DEBUG 01-15 16:09:17.007075.007075 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:09:17.007407.007407 cuda_h.py:10] start gate
DEBUG 01-15 16:09:17.007002.007002 cuda_h.py:19] end gate cost 0.0007193088531494141 seconds
DEBUG 01-15 16:09:17.007501.007501 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:17.008835.008835 lmp.py:1912] 
DEBUG 01-15 16:09:17.008835.008835 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:17.008783.008783 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:17.008301.008301 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:17.008282.008282 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:17.008117.008117 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:17.008998.008998 lmp.py:1917] 
DEBUG 01-15 16:09:17.008998.008998 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:17.008833.008833 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:17.008390.008390 lmp.py:1935]   Expert 25 |     13 | CPU
DEBUG 01-15 16:09:17.008225.008225 lmp.py:1935]   Expert 48 |     30 | CPU
DEBUG 01-15 16:09:17.008868.008868 lmp.py:1935]   Expert 45 |     36 | CPU
DEBUG 01-15 16:09:17.008034.008034 lmp.py:1935]   Expert  9 |     61 | CPU
DEBUG 01-15 16:09:17.008677.008677 lmp.py:1935]   Expert  0 |     80 | CPU
DEBUG 01-15 16:09:17.008605.008605 lmp.py:1935]   Expert 54 |     85 | CPU
DEBUG 01-15 16:09:17.008533.008533 lmp.py:1935]   Expert 43 |     86 | CPU
DEBUG 01-15 16:09:17.008461.008461 lmp.py:1935]   Expert 57 |     88 | CPU
DEBUG 01-15 16:09:17.008388.008388 lmp.py:1935]   Expert 20 |     90 | CPU
DEBUG 01-15 16:09:17.008554.008554 lmp.py:1935]   Expert 36 |     92 | CPU
DEBUG 01-15 16:09:17.008721.008721 lmp.py:1935]   Expert 47 |     92 | CPU
DEBUG 01-15 16:09:17.008410.008410 lmp.py:1935]   Expert  6 |     95 | CPU
DEBUG 01-15 16:09:17.008768.008768 lmp.py:1935]   Expert 62 |    102 | CPU
DEBUG 01-15 16:09:17.008888.008888 lmp.py:1935]   Expert 15 |    106 | CPU
DEBUG 01-15 16:09:17.008769.008769 lmp.py:1935]   Expert 61 |    106 | CPU
DEBUG 01-15 16:09:17.008651.008651 lmp.py:1935]   Expert 50 |    107 | CPU
DEBUG 01-15 16:09:17.008578.008578 lmp.py:1935]   Expert  1 |    108 | CPU
DEBUG 01-15 16:09:17.008506.008506 lmp.py:1935]   Expert 13 |    108 | CPU
DEBUG 01-15 16:09:17.008818.008818 lmp.py:1935]   Expert 38 |    110 | CPU
DEBUG 01-15 16:09:17.008699.008699 lmp.py:1935]   Expert 37 |    115 | CPU
DEBUG 01-15 16:09:17.008342.008342 lmp.py:1935]   Expert 14 |    118 | CPU
DEBUG 01-15 16:09:17.008224.008224 lmp.py:1935]   Expert 46 |    123 | CPU
DEBUG 01-15 16:09:17.008867.008867 lmp.py:1935]   Expert 28 |    135 | CPU
DEBUG 01-15 16:09:17.008510.008510 lmp.py:1935]   Expert  7 |    137 | CPU
DEBUG 01-15 16:09:17.008153.008153 lmp.py:1935]   Expert 21 |    139 | CPU
DEBUG 01-15 16:09:17.008941.008941 lmp.py:1935]   Expert 52 |    139 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.008445.008445 lmp.py:1935]   Expert 44 |    143 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.008996.008996 lmp.py:1935]   Expert 24 |    150 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009546.009546 lmp.py:1935]   Expert 42 |    151 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009812.009812 lmp.py:1935]   Expert 10 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009885.009885 lmp.py:1935]   Expert 11 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009197.009197 lmp.py:1935]   Expert  2 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009032.009032 lmp.py:1935]   Expert 26 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009106.009106 lmp.py:1935]   Expert 35 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009464.009464 lmp.py:1935]   Expert 31 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009060.009060 lmp.py:1935]   Expert  3 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009657.009657 lmp.py:1935]   Expert 19 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009015.009015 lmp.py:1935]   Expert 32 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009612.009612 lmp.py:1935]   Expert 12 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009401.009401 lmp.py:1935]   Expert 56 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009189.009189 lmp.py:1935]   Expert 60 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009025.009025 lmp.py:1935]   Expert 40 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009336.009336 lmp.py:1935]   Expert 41 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009172.009172 lmp.py:1935]   Expert 23 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009768.009768 lmp.py:1935]   Expert 16 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009126.009126 lmp.py:1935]   Expert 51 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009485.009485 lmp.py:1935]   Expert  8 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009320.009320 lmp.py:1935]   Expert 53 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009155.009155 lmp.py:1935]   Expert 58 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009228.009228 lmp.py:1935]   Expert 59 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009779.009779 lmp.py:1935]   Expert  4 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009044.009044 lmp.py:1935]   Expert 49 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009118.009118 lmp.py:1935]   Expert 55 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009714.009714 lmp.py:1935]   Expert 29 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009311.009311 lmp.py:1935]   Expert 34 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009146.009146 lmp.py:1935]   Expert 18 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009504.009504 lmp.py:1935]   Expert 63 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009101.009101 lmp.py:1935]   Expert 27 |    354 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009936.009936 lmp.py:1935]   Expert 39 |    384 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009294.009294 lmp.py:1935]   Expert 17 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009652.009652 lmp.py:1935]   Expert 22 |    435 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009726.009726 lmp.py:1935]   Expert 33 |    452 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009799.009799 lmp.py:1935]   Expert 30 |    456 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.009588.009588 lmp.py:1935]   Expert  5 |    709 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.009708.009708 lmp.py:1937] 
DEBUG 01-15 16:09:17.009708.009708 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:17.009497.009497 lmp.py:1938]   CPU:   2362 tokens
DEBUG 01-15 16:09:17.009809.009809 lmp.py:1942]   cuda:1:   4930 tokens (19 experts)
DEBUG 01-15 16:09:17.009405.009405 lmp.py:1942]   cuda:2:   4996 tokens (20 experts)
DEBUG 01-15 16:09:17.009810.009810 lmp.py:1943]   Total GPU:   9926 tokens
DEBUG 01-15 16:09:17.009214.009214 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:17.009214.009214 lmp.py:1944] 
DEBUG 01-15 16:09:17.009533.009533 cuda_h.py:19] end experts_map_get cost 0.001924753189086914 seconds
DEBUG 01-15 16:09:17.009628.009628 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:17.009861.009861 lmp.py:1953] 
DEBUG 01-15 16:09:17.009861.009861 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:17.009174.009174 cuda_h.py:19] end cpu_experts_submit cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:09:17.009109.009109 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:17.010899.010899 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:17.010467.010467 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.4.gate_proj.weight': 0, 'model.layers.22.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.22.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.22.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.22.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.31.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.31.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.31.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.32.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.32.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.32.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.34.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.34.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.34.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.39.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.39.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.39.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.51.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.51.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.51.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.22.mlp.experts.2.gate_proj.weight': 0, 'model.layers.22.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.26.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.26.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.26.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.27.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.27.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.27.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.29.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.29.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.29.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.30.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.30.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.30.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.33.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.33.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.33.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.41.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.41.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.41.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.22.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.22.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.22.mlp.experts.59.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(26182942720, 5767168, 0, 0), (26188709888, 5767168, 5767168, 0), (26177175552, 5767168, 11534336, 0), (26200244224, 5767168, 17301504, 0), (26206011392, 5767168, 23068672, 0), (26194477056, 5767168, 28835840, 0), (26304053248, 5767168, 34603008, 0), (26309820416, 5767168, 40370176, 0), (26298286080, 5767168, 46137344, 0), (26321354752, 5767168, 51904512, 0), (26327121920, 5767168, 57671680, 0), (26315587584, 5767168, 63438848, 0), (26494369792, 5767168, 69206016, 0), (26500136960, 5767168, 74973184, 0), (26488602624, 5767168, 80740352, 0), (26650083328, 5767168, 86507520, 0), (26655850496, 5767168, 92274688, 0), (26644316160, 5767168, 98041856, 0), (26667384832, 5767168, 103809024, 0), (26673152000, 5767168, 109576192, 0), (26661617664, 5767168, 115343360, 0), (26701987840, 5767168, 121110528, 0), (26707755008, 5767168, 126877696, 0), (26696220672, 5767168, 132644864, 0), (26719289344, 5767168, 138412032, 0), (26725056512, 5767168, 144179200, 0), (26713522176, 5767168, 149946368, 0), (26788495360, 5767168, 155713536, 0), (26794262528, 5767168, 161480704, 0), (26782728192, 5767168, 167247872, 0), (26805796864, 5767168, 173015040, 0), (26811564032, 5767168, 178782208, 0), (26800029696, 5767168, 184549376, 0), (26840399872, 5767168, 190316544, 0), (26846167040, 5767168, 196083712, 0), (26834632704, 5767168, 201850880, 0), (26875002880, 5767168, 207618048, 0), (26880770048, 5767168, 213385216, 0), (26869235712, 5767168, 219152384, 0), (26996113408, 5767168, 224919552, 0), (27001880576, 5767168, 230686720, 0), (26990346240, 5767168, 236453888, 0), (27030716416, 5767168, 242221056, 0), (27036483584, 5767168, 247988224, 0), (27024949248, 5767168, 253755392, 0), (27065319424, 5767168, 259522560, 0), (27071086592, 5767168, 265289728, 0), (27059552256, 5767168, 271056896, 0), (27117223936, 5767168, 276824064, 0), (27122991104, 5767168, 282591232, 0), (27111456768, 5767168, 288358400, 0), (27151826944, 5767168, 294125568, 0), (27157594112, 5767168, 299892736, 0), (27146059776, 5767168, 305659904, 0), (27203731456, 5767168, 311427072, 0), (27209498624, 5767168, 317194240, 0), (27197964288, 5767168, 322961408, 0)], 2: [(26148339712, 5767168, 0, 0), (26154106880, 5767168, 5767168, 0), (26142572544, 5767168, 11534336, 0), (26165641216, 5767168, 17301504, 0), (26171408384, 5767168, 23068672, 0), (26159874048, 5767168, 28835840, 0), (26252148736, 5767168, 34603008, 0), (26257915904, 5767168, 40370176, 0), (26246381568, 5767168, 46137344, 0), (26286751744, 5767168, 51904512, 0), (26292518912, 5767168, 57671680, 0), (26280984576, 5767168, 63438848, 0), (26390560768, 5767168, 69206016, 0), (26396327936, 5767168, 74973184, 0), (26384793600, 5767168, 80740352, 0), (26407862272, 5767168, 86507520, 0), (26413629440, 5767168, 92274688, 0), (26402095104, 5767168, 98041856, 0), (26425163776, 5767168, 103809024, 0), (26430930944, 5767168, 109576192, 0), (26419396608, 5767168, 115343360, 0), (26442465280, 5767168, 121110528, 0), (26448232448, 5767168, 126877696, 0), (26436698112, 5767168, 132644864, 0), (26511671296, 5767168, 138412032, 0), (26517438464, 5767168, 144179200, 0), (26505904128, 5767168, 149946368, 0), (26528972800, 5767168, 155713536, 0), (26534739968, 5767168, 161480704, 0), (26523205632, 5767168, 167247872, 0), (26563575808, 5767168, 173015040, 0), (26569342976, 5767168, 178782208, 0), (26557808640, 5767168, 184549376, 0), (26580877312, 5767168, 190316544, 0), (26586644480, 5767168, 196083712, 0), (26575110144, 5767168, 201850880, 0), (26615480320, 5767168, 207618048, 0), (26621247488, 5767168, 213385216, 0), (26609713152, 5767168, 219152384, 0), (26632781824, 5767168, 224919552, 0), (26638548992, 5767168, 230686720, 0), (26627014656, 5767168, 236453888, 0), (26684686336, 5767168, 242221056, 0), (26690453504, 5767168, 247988224, 0), (26678919168, 5767168, 253755392, 0), (26823098368, 5767168, 259522560, 0), (26828865536, 5767168, 265289728, 0), (26817331200, 5767168, 271056896, 0), (26961510400, 5767168, 276824064, 0), (26967277568, 5767168, 282591232, 0), (26955743232, 5767168, 288358400, 0), (27013414912, 5767168, 294125568, 0), (27019182080, 5767168, 299892736, 0), (27007647744, 5767168, 305659904, 0), (27082620928, 5767168, 311427072, 0), (27088388096, 5767168, 317194240, 0), (27076853760, 5767168, 322961408, 0), (27134525440, 5767168, 328728576, 0), (27140292608, 5767168, 334495744, 0), (27128758272, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a9f37ca1c0>, 2: <capsule object NULL at 0x74a6785de520>}
DEBUG 01-15 16:09:17.011844.011844 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.011395.011395 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.011959.011959 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 16413914-2271-4456-9c92-f0ec1fa7fb0d
DEBUG 01-15 16:09:17.011561.011561 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:17.011450.011450 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.011930.011930 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:17.011602.011602 client.py:127] Model loaded
DEBUG 01-15 16:09:17.011147.011147 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.012642.012642 cuda_h.py:19] end restore2model cost 0.0003342628479003906 seconds
DEBUG 01-15 16:09:17.012028.012028 cuda_h.py:19] end sllm_worker_task cost 0.010770320892333984 seconds
DEBUG 01-15 16:09:17.012748.012748 cuda_h.py:19] end move_flatidxs cost 0.0008349418640136719 seconds
DEBUG 01-15 16:09:17.012809.012809 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:17.013184.013184 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 16413914-2271-4456-9c92-f0ec1fa7fb0d
DEBUG 01-15 16:09:17.014871.014871 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0040891170501708984 seconds
DEBUG 01-15 16:09:17.014768.014768 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.017627.017627 cuda_h.py:19] end restore2model cost 0.0036516189575195312 seconds
DEBUG 01-15 16:09:17.017490.017490 cuda_h.py:19] end group_tensors cost 0.004680156707763672 seconds
DEBUG 01-15 16:09:17.018324.018324 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008000612258911133 seconds
DEBUG 01-15 16:09:17.018809.018809 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:17.018440.018440 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:17.018559.018559 cuda_h.py:19] end gpu_sexperts cost 0.0004115104675292969 seconds
DEBUG 01-15 16:09:17.018879.018879 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:17.018954.018954 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.811981201171875e-05 seconds
DEBUG 01-15 16:09:17.018087.018087 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:17.018850.018850 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.020992.020992 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016291141510009766 seconds
DEBUG 01-15 16:09:17.020612.020612 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.020583.020583 cuda_h.py:19] end gpu_group_list cost 0.00029206275939941406 seconds
DEBUG 01-15 16:09:17.022413.022413 cuda_h.py:19] end group pad cost 0.003899812698364258 seconds
DEBUG 01-15 16:09:17.022580.022580 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:17.022502.022502 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.032463.032463 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.01020050048828125 seconds
DEBUG 01-15 16:09:17.033815.033815 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.033761.033761 cuda_h.py:19] end gpu_group_list cost 0.00033593177795410156 seconds
DEBUG 01-15 16:09:17.036473.036473 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:17.037177.037177 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 16413914-2271-4456-9c92-f0ec1fa7fb0d
DEBUG 01-15 16:09:17.052271.052271 cuda_h.py:19] end group_einsum cost 0.030443668365478516 seconds
DEBUG 01-15 16:09:17.052105.052105 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:17.055671.055671 cuda_h.py:19] end get_outputs_cpu1 cost 0.002811908721923828 seconds
DEBUG 01-15 16:09:17.056017.056017 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04482626914978027 seconds
INFO 01-15 16:09:17.057665.057665 client.py:127] Model loaded
DEBUG 01-15 16:09:17.057564.057564 cuda_h.py:19] end wait_experts_multi_device cost 0.01985955238342285 seconds
DEBUG 01-15 16:09:17.057665.057665 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:17.058057.058057 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005991458892822266 seconds
DEBUG 01-15 16:09:17.058231.058231 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:17.059100.059100 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.059364.059364 cuda_h.py:19] end index_scatter cost 8.702278137207031e-05 seconds
DEBUG 01-15 16:09:17.059852.059852 cuda_h.py:19] end cpuoutputsdeal cost 0.0016813278198242188 seconds
DEBUG 01-15 16:09:17.059722.059722 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:17.059438.059438 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.060962.060962 cuda_h.py:19] end gpu_group_tensor cost 0.0001773834228515625 seconds
DEBUG 01-15 16:09:17.060533.060533 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.060461.060461 cuda_h.py:19] end gpu_group_tensor cost 0.00019550323486328125 seconds
DEBUG 01-15 16:09:17.060047.060047 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.061014.061014 cuda_h.py:19] end gpu_group_einsum cost 0.0005221366882324219 seconds
DEBUG 01-15 16:09:17.061621.061621 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.061550.061550 cuda_h.py:19] end gpu_group_einsum cost 0.00042438507080078125 seconds
DEBUG 01-15 16:09:17.061276.061276 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.061518.061518 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.062450.062450 cuda_h.py:19] end all_expert_outputs_slices cost 0.00029587745666503906 seconds
DEBUG 01-15 16:09:17.062921.062921 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.062043.062043 cuda_h.py:19] end concat_expert_out cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:09:17.062609.062609 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.062314.062314 cuda_h.py:19] end index_scatter cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:09:17.062965.062965 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009524822235107422 seconds
DEBUG 01-15 16:09:17.062439.062439 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.063342.063342 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.063053.063053 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024390220642089844 seconds
DEBUG 01-15 16:09:17.063478.063478 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.063269.063269 cuda_h.py:19] end concat_expert_out cost 5.888938903808594e-05 seconds
DEBUG 01-15 16:09:17.063689.063689 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.063294.063294 cuda_h.py:19] end index_scatter cost 6.127357482910156e-05 seconds
DEBUG 01-15 16:09:17.063057.063057 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006291866302490234 seconds
DEBUG 01-15 16:09:17.063987.063987 cuda_h.py:19] end gpu_experts_multi_device cost 0.04505133628845215 seconds
DEBUG 01-15 16:09:17.063149.063149 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.05676531791687012 seconds
DEBUG 01-15 16:09:17.064895.064895 cuda_h.py:19] end prefill_layer cost 0.06325459480285645 seconds
DEBUG 01-15 16:09:17.064142.064142 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:09:17.064706.064706 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:17.064224.064224 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:09:17.064695.064695 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:09:17.064551.064551 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:09:17.064407.064407 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 3.8623809814453125e-05 seconds
DEBUG 01-15 16:09:17.064263.064263 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:09:17.064012.064012 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:17.064035.064035 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:17.064203.064203 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:17.064922.064922 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:17.064812.064812 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:17.065485.065485 cuda_h.py:19] end allocate_cuda_memory cost 0.00028204917907714844 seconds
DEBUG 01-15 16:09:17.065938.065938 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:17.065277.065277 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.065345.065345 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.065624.065624 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b5c5f771-f251-4031-87d2-923909229ca2
DEBUG 01-15 16:09:17.065190.065190 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.065723.065723 cuda_h.py:10] start self_attn
INFO 01-15 16:09:17.067318.067318 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b5c5f771-f251-4031-87d2-923909229ca2
DEBUG 01-15 16:09:17.067684.067684 cuda_h.py:19] end load_into_gpu_async cost 0.001669168472290039 seconds
DEBUG 01-15 16:09:17.067294.067294 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:17.067874.067874 cuda_h.py:19] end restore_tensors2 cost 8.082389831542969e-05 seconds
DEBUG 01-15 16:09:17.067160.067160 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002317667007446289 seconds
INFO 01-15 16:09:17.067116.067116 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b5c5f771-f251-4031-87d2-923909229ca2
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:17.069157.069157 cuda_h.py:19] end self_attn cost 0.0037915706634521484 seconds
DEBUG 01-15 16:09:17.070301.070301 cuda_h.py:19] end iln_self_attn_paln cost 0.005443572998046875 seconds
DEBUG 01-15 16:09:17.070575.070575 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:09:17.070430.070430 cuda_h.py:10] start gate
DEBUG 01-15 16:09:17.070032.070032 cuda_h.py:19] end gate cost 0.0007240772247314453 seconds
DEBUG 01-15 16:09:17.071246.071246 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:17.071898.071898 lmp.py:1912] 
DEBUG 01-15 16:09:17.071898.071898 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:17.071369.071369 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:17.071449.071449 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:17.071953.071953 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:17.071550.071550 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:17.071431.071431 lmp.py:1917] 
DEBUG 01-15 16:09:17.071431.071431 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:17.071028.071028 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:17.071108.071108 lmp.py:1935]   Expert  5 |     14 | CPU
DEBUG 01-15 16:09:17.071228.071228 lmp.py:1935]   Expert 56 |     34 | CPU
DEBUG 01-15 16:09:17.071632.071632 lmp.py:1935]   Expert 27 |     81 | CPU
DEBUG 01-15 16:09:17.071275.071275 lmp.py:1935]   Expert 16 |     86 | CPU
DEBUG 01-15 16:09:17.071441.071441 lmp.py:1935]   Expert 17 |     90 | CPU
DEBUG 01-15 16:09:17.071846.071846 lmp.py:1935]   Expert 40 |     96 | CPU
DEBUG 01-15 16:09:17.071251.071251 lmp.py:1935]   Expert 63 |    100 | CPU
DEBUG 01-15 16:09:17.071086.071086 lmp.py:1935]   Expert 51 |    104 | CPU
DEBUG 01-15 16:09:17.071967.071967 lmp.py:1935]   Expert 49 |    107 | CPU
DEBUG 01-15 16:09:17.071610.071610 lmp.py:1935]   Expert 53 |    107 | CPU
DEBUG 01-15 16:09:17.071253.071253 lmp.py:1935]   Expert 28 |    109 | CPU
DEBUG 01-15 16:09:17.071658.071658 lmp.py:1935]   Expert  7 |    111 | CPU
DEBUG 01-15 16:09:17.071824.071824 lmp.py:1935]   Expert 47 |    123 | CPU
DEBUG 01-15 16:09:17.071751.071751 lmp.py:1935]   Expert 37 |    124 | CPU
DEBUG 01-15 16:09:17.071679.071679 lmp.py:1935]   Expert 38 |    124 | CPU
DEBUG 01-15 16:09:17.071845.071845 lmp.py:1935]   Expert 62 |    127 | CPU
DEBUG 01-15 16:09:17.071773.071773 lmp.py:1935]   Expert 11 |    128 | CPU
DEBUG 01-15 16:09:17.071462.071462 lmp.py:1935]   Expert 58 |    131 | CPU
DEBUG 01-15 16:09:17.071628.071628 lmp.py:1935]   Expert 57 |    139 | CPU
DEBUG 01-15 16:09:17.072318.072318 lmp.py:1935]   Expert 39 |    145 | CPU
DEBUG 01-15 16:09:17.072199.072199 lmp.py:1935]   Expert  1 |    148 | CPU
DEBUG 01-15 16:09:17.072796.072796 lmp.py:1935]   Expert 14 |    148 | CPU
DEBUG 01-15 16:09:17.072439.072439 lmp.py:1935]   Expert 52 |    154 | CPU
DEBUG 01-15 16:09:17.072843.072843 lmp.py:1935]   Expert 25 |    155 | CPU
DEBUG 01-15 16:09:17.072009.072009 lmp.py:1935]   Expert 23 |    158 | CPU
DEBUG 01-15 16:09:17.072606.072606 lmp.py:1935]   Expert 33 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072964.072964 lmp.py:1935]   Expert 21 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072799.072799 lmp.py:1935]   Expert  6 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072919.072919 lmp.py:1935]   Expert 60 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072562.072562 lmp.py:1935]   Expert 45 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072444.072444 lmp.py:1935]   Expert 12 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072087.072087 lmp.py:1935]   Expert 19 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072683.072683 lmp.py:1935]   Expert 44 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072041.072041 lmp.py:1935]   Expert  4 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072638.072638 lmp.py:1935]   Expert 30 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072235.072235 lmp.py:1935]   Expert 31 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072308.072308 lmp.py:1935]   Expert 55 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072428.072428 lmp.py:1935]   Expert  3 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072071.072071 lmp.py:1935]   Expert 36 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072952.072952 lmp.py:1935]   Expert  9 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072324.072324 lmp.py:1935]   Expert  0 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072497.072497 lmp.py:1935]   Expert 41 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072332.072332 lmp.py:1935]   Expert 34 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072214.072214 lmp.py:1935]   Expert 22 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072287.072287 lmp.py:1935]   Expert 54 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072407.072407 lmp.py:1935]   Expert 26 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072004.072004 lmp.py:1935]   Expert 43 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072839.072839 lmp.py:1935]   Expert 59 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072720.072720 lmp.py:1935]   Expert 18 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072125.072125 lmp.py:1935]   Expert 13 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072006.072006 lmp.py:1935]   Expert 15 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072649.072649 lmp.py:1935]   Expert 20 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072292.072292 lmp.py:1935]   Expert 50 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072935.072935 lmp.py:1935]   Expert 24 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072816.072816 lmp.py:1935]   Expert 42 |    263 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072221.072221 lmp.py:1935]   Expert 29 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072579.072579 lmp.py:1935]   Expert 61 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072699.072699 lmp.py:1935]   Expert 35 |    280 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072296.072296 lmp.py:1935]   Expert 32 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072415.072415 lmp.py:1935]   Expert  2 |    337 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072535.072535 lmp.py:1935]   Expert  8 |    338 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072940.072940 lmp.py:1935]   Expert 10 |    342 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072967.072967 lmp.py:1935]   Expert 46 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.072087.072087 lmp.py:1935]   Expert 48 |    448 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.072491.072491 lmp.py:1937] 
DEBUG 01-15 16:09:17.072491.072491 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:17.072849.072849 lmp.py:1938]   CPU:   2843 tokens
DEBUG 01-15 16:09:17.072685.072685 lmp.py:1942]   cuda:1:   4801 tokens (20 experts)
DEBUG 01-15 16:09:17.072804.072804 lmp.py:1942]   cuda:2:   4644 tokens (19 experts)
DEBUG 01-15 16:09:17.072686.072686 lmp.py:1943]   Total GPU:   9445 tokens
DEBUG 01-15 16:09:17.072044.072044 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:17.072044.072044 lmp.py:1944] 
DEBUG 01-15 16:09:17.072316.072316 cuda_h.py:19] end experts_map_get cost 0.0018832683563232422 seconds
DEBUG 01-15 16:09:17.073796.073796 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:17.073505.073505 lmp.py:1953] 
DEBUG 01-15 16:09:17.073505.073505 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:17.073719.073719 cuda_h.py:19] end cpu_experts_submit cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:09:17.073389.073389 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:17.073987.073987 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:17.074697.074697 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.0.gate_proj.weight': 0, 'model.layers.23.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.45.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.45.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.45.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.48.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.48.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.48.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.50.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.50.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.50.up_proj.weight': 322961408, 'model.layers.23.mlp.experts.55.gate_proj.weight': 328728576, 'model.layers.23.mlp.experts.55.down_proj.weight': 334495744, 'model.layers.23.mlp.experts.55.up_proj.weight': 340262912}, 2: {'model.layers.23.mlp.experts.2.gate_proj.weight': 0, 'model.layers.23.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.61.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(27221032960, 5767168, 0, 0), (27226800128, 5767168, 5767168, 0), (27215265792, 5767168, 11534336, 0), (27359444992, 5767168, 17301504, 0), (27365212160, 5767168, 23068672, 0), (27353677824, 5767168, 28835840, 0), (27376746496, 5767168, 34603008, 0), (27382513664, 5767168, 40370176, 0), (27370979328, 5767168, 46137344, 0), (27428651008, 5767168, 51904512, 0), (27434418176, 5767168, 57671680, 0), (27422883840, 5767168, 63438848, 0), (27445952512, 5767168, 69206016, 0), (27451719680, 5767168, 74973184, 0), (27440185344, 5767168, 80740352, 0), (27480555520, 5767168, 86507520, 0), (27486322688, 5767168, 92274688, 0), (27474788352, 5767168, 98041856, 0), (27584364544, 5767168, 103809024, 0), (27590131712, 5767168, 109576192, 0), (27578597376, 5767168, 115343360, 0), (27601666048, 5767168, 121110528, 0), (27607433216, 5767168, 126877696, 0), (27595898880, 5767168, 132644864, 0), (27670872064, 5767168, 138412032, 0), (27676639232, 5767168, 144179200, 0), (27665104896, 5767168, 149946368, 0), (27722776576, 5767168, 155713536, 0), (27728543744, 5767168, 161480704, 0), (27717009408, 5767168, 167247872, 0), (27740078080, 5767168, 173015040, 0), (27745845248, 5767168, 178782208, 0), (27734310912, 5767168, 184549376, 0), (27774681088, 5767168, 190316544, 0), (27780448256, 5767168, 196083712, 0), (27768913920, 5767168, 201850880, 0), (27791982592, 5767168, 207618048, 0), (27797749760, 5767168, 213385216, 0), (27786215424, 5767168, 219152384, 0), (27826585600, 5767168, 224919552, 0), (27832352768, 5767168, 230686720, 0), (27820818432, 5767168, 236453888, 0), (27964997632, 5767168, 242221056, 0), (27970764800, 5767168, 247988224, 0), (27959230464, 5767168, 253755392, 0), (27982299136, 5767168, 259522560, 0), (27988066304, 5767168, 265289728, 0), (27976531968, 5767168, 271056896, 0), (27999600640, 5767168, 276824064, 0), (28005367808, 5767168, 282591232, 0), (27993833472, 5767168, 288358400, 0), (28051505152, 5767168, 294125568, 0), (28057272320, 5767168, 299892736, 0), (28045737984, 5767168, 305659904, 0), (28086108160, 5767168, 311427072, 0), (28091875328, 5767168, 317194240, 0), (28080340992, 5767168, 322961408, 0), (28172615680, 5767168, 328728576, 0), (28178382848, 5767168, 334495744, 0), (28166848512, 5767168, 340262912, 0)], 2: [(27255635968, 5767168, 0, 0), (27261403136, 5767168, 5767168, 0), (27249868800, 5767168, 11534336, 0), (27272937472, 5767168, 17301504, 0), (27278704640, 5767168, 23068672, 0), (27267170304, 5767168, 28835840, 0), (27290238976, 5767168, 34603008, 0), (27296006144, 5767168, 40370176, 0), (27284471808, 5767168, 46137344, 0), (27324841984, 5767168, 51904512, 0), (27330609152, 5767168, 57671680, 0), (27319074816, 5767168, 63438848, 0), (27394048000, 5767168, 69206016, 0), (27399815168, 5767168, 74973184, 0), (27388280832, 5767168, 80740352, 0), (27532460032, 5767168, 86507520, 0), (27538227200, 5767168, 92274688, 0), (27526692864, 5767168, 98041856, 0), (27549761536, 5767168, 103809024, 0), (27555528704, 5767168, 109576192, 0), (27543994368, 5767168, 115343360, 0), (27567063040, 5767168, 121110528, 0), (27572830208, 5767168, 126877696, 0), (27561295872, 5767168, 132644864, 0), (27636269056, 5767168, 138412032, 0), (27642036224, 5767168, 144179200, 0), (27630501888, 5767168, 149946368, 0), (27757379584, 5767168, 155713536, 0), (27763146752, 5767168, 161480704, 0), (27751612416, 5767168, 167247872, 0), (27809284096, 5767168, 173015040, 0), (27815051264, 5767168, 178782208, 0), (27803516928, 5767168, 184549376, 0), (27843887104, 5767168, 190316544, 0), (27849654272, 5767168, 196083712, 0), (27838119936, 5767168, 201850880, 0), (27930394624, 5767168, 207618048, 0), (27936161792, 5767168, 213385216, 0), (27924627456, 5767168, 219152384, 0), (27947696128, 5767168, 224919552, 0), (27953463296, 5767168, 230686720, 0), (27941928960, 5767168, 236453888, 0), (28016902144, 5767168, 242221056, 0), (28022669312, 5767168, 247988224, 0), (28011134976, 5767168, 253755392, 0), (28155314176, 5767168, 259522560, 0), (28161081344, 5767168, 265289728, 0), (28149547008, 5767168, 271056896, 0), (28241821696, 5767168, 276824064, 0), (28247588864, 5767168, 282591232, 0), (28236054528, 5767168, 288358400, 0), (28259123200, 5767168, 294125568, 0), (28264890368, 5767168, 299892736, 0), (28253356032, 5767168, 305659904, 0), (28276424704, 5767168, 311427072, 0), (28282191872, 5767168, 317194240, 0), (28270657536, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74b34ffffc90>, 2: <capsule object NULL at 0x74a6785de2b0>}
DEBUG 01-15 16:09:17.074992.074992 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.074153.074153 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.074478.074478 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 57be7de5-26a5-4338-b016-b9729a77208b
DEBUG 01-15 16:09:17.074916.074916 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:17.075372.075372 client.py:127] Model loaded
DEBUG 01-15 16:09:17.075009.075009 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.075936.075936 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:17.075207.075207 cuda_h.py:19] end restore2model cost 0.00032639503479003906 seconds
DEBUG 01-15 16:09:17.075160.075160 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:17.075877.075877 cuda_h.py:19] end sllm_worker_task cost 0.010721683502197266 seconds
DEBUG 01-15 16:09:17.076938.076938 cuda_h.py:19] end move_flatidxs cost 0.0008413791656494141 seconds
DEBUG 01-15 16:09:17.076476.076476 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:17.076325.076325 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 57be7de5-26a5-4338-b016-b9729a77208b
DEBUG 01-15 16:09:17.077878.077878 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004240274429321289 seconds
DEBUG 01-15 16:09:17.077702.077702 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.081696.081696 cuda_h.py:19] end restore2model cost 0.0035409927368164062 seconds
DEBUG 01-15 16:09:17.081208.081208 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008024454116821289 seconds
DEBUG 01-15 16:09:17.081719.081719 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:17.081704.081704 cuda_h.py:19] end gpu_sexperts cost 0.0003063678741455078 seconds
DEBUG 01-15 16:09:17.081725.081725 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:17.081277.081277 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:09:17.081735.081735 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:17.081153.081153 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.083892.083892 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012848377227783203 seconds
DEBUG 01-15 16:09:17.083955.083955 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.083804.083804 cuda_h.py:19] end gpu_group_list cost 0.00021147727966308594 seconds
DEBUG 01-15 16:09:17.084178.084178 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.085131.085131 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013072490692138672 seconds
DEBUG 01-15 16:09:17.085862.085862 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.085225.085225 cuda_h.py:19] end group_tensors cost 0.008661746978759766 seconds
DEBUG 01-15 16:09:17.086615.086615 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:17.086612.086612 cuda_h.py:19] end gpu_group_list cost 0.0002067089080810547 seconds
DEBUG 01-15 16:09:17.088950.088950 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:17.088854.088854 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 57be7de5-26a5-4338-b016-b9729a77208b
DEBUG 01-15 16:09:17.090814.090814 cuda_h.py:19] end group pad cost 0.0045185089111328125 seconds
DEBUG 01-15 16:09:17.090459.090459 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:17.121146.121146 cuda_h.py:19] end group_einsum cost 0.03093695640563965 seconds
DEBUG 01-15 16:09:17.121250.121250 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:17.121911.121911 client.py:127] Model loaded
DEBUG 01-15 16:09:17.121770.121770 cuda_h.py:19] end wait_experts_multi_device cost 0.03328728675842285 seconds
DEBUG 01-15 16:09:17.121633.121633 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:17.125920.125920 cuda_h.py:19] end get_outputs_cpu1 cost 0.0031321048736572266 seconds
DEBUG 01-15 16:09:17.125948.125948 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.050330400466918945 seconds
DEBUG 01-15 16:09:17.126191.126191 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.004717826843261719 seconds
DEBUG 01-15 16:09:17.126296.126296 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:17.129079.129079 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.129308.129308 cuda_h.py:19] end index_scatter cost 0.0001537799835205078 seconds
DEBUG 01-15 16:09:17.129186.129186 cuda_h.py:19] end cpuoutputsdeal cost 0.0029418468475341797 seconds
DEBUG 01-15 16:09:17.130271.130271 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:17.130963.130963 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.130201.130201 cuda_h.py:19] end gpu_group_tensor cost 0.00030350685119628906 seconds
DEBUG 01-15 16:09:17.130754.130754 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.131250.131250 cuda_h.py:19] end gpu_group_tensor cost 0.000286102294921875 seconds
DEBUG 01-15 16:09:17.131363.131363 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.132261.132261 cuda_h.py:19] end gpu_group_einsum cost 0.0009846687316894531 seconds
DEBUG 01-15 16:09:17.132946.132946 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.133125.133125 cuda_h.py:19] end gpu_group_einsum cost 0.0008909702301025391 seconds
DEBUG 01-15 16:09:17.133330.133330 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.133287.133287 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.134552.134552 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015783309936523438 seconds
DEBUG 01-15 16:09:17.134547.134547 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.134477.134477 cuda_h.py:19] end concat_expert_out cost 5.793571472167969e-05 seconds
DEBUG 01-15 16:09:17.134419.134419 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.134601.134601 cuda_h.py:19] end index_scatter cost 6.699562072753906e-05 seconds
DEBUG 01-15 16:09:17.134351.134351 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007677078247070312 seconds
DEBUG 01-15 16:09:17.134295.134295 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.134568.134568 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.135070.135070 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012826919555664062 seconds
DEBUG 01-15 16:09:17.135488.135488 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.135028.135028 cuda_h.py:19] end concat_expert_out cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:09:17.135579.135579 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.135502.135502 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:17.135166.135166 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046706199645996094 seconds
DEBUG 01-15 16:09:17.135844.135844 cuda_h.py:19] end gpu_experts_multi_device cost 0.053674936294555664 seconds
DEBUG 01-15 16:09:17.135893.135893 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.06523275375366211 seconds
DEBUG 01-15 16:09:17.135165.135165 cuda_h.py:19] end prefill_layer cost 0.07140707969665527 seconds
DEBUG 01-15 16:09:17.135160.135160 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:09:17.135764.135764 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:17.135606.135606 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:09:17.136163.136163 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:09:17.136342.136342 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:09:17.136848.136848 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 3.2901763916015625e-05 seconds
DEBUG 01-15 16:09:17.136835.136835 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 6.127357482910156e-05 seconds
DEBUG 01-15 16:09:17.136386.136386 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:17.136474.136474 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:17.136743.136743 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:17.136989.136989 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:17.136226.136226 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:17.136300.136300 cuda_h.py:19] end allocate_cuda_memory cost 0.00040078163146972656 seconds
DEBUG 01-15 16:09:17.137277.137277 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:17.137093.137093 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.137426.137426 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.137897.137897 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ddbb630e-46e0-4e63-b2de-0fa39d32315d
DEBUG 01-15 16:09:17.137848.137848 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.137260.137260 cuda_h.py:10] start self_attn
INFO 01-15 16:09:17.138208.138208 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ddbb630e-46e0-4e63-b2de-0fa39d32315d
DEBUG 01-15 16:09:17.138898.138898 cuda_h.py:19] end load_into_gpu_async cost 0.0017199516296386719 seconds
DEBUG 01-15 16:09:17.138932.138932 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:17.138452.138452 cuda_h.py:19] end restore_tensors2 cost 7.62939453125e-05 seconds
DEBUG 01-15 16:09:17.138208.138208 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002471923828125 seconds
INFO 01-15 16:09:17.139144.139144 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ddbb630e-46e0-4e63-b2de-0fa39d32315d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:17.140331.140331 cuda_h.py:19] end self_attn cost 0.002866506576538086 seconds
DEBUG 01-15 16:09:17.140248.140248 cuda_h.py:19] end iln_self_attn_paln cost 0.00457453727722168 seconds
DEBUG 01-15 16:09:17.140124.140124 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:09:17.140311.140311 cuda_h.py:10] start gate
DEBUG 01-15 16:09:17.141591.141591 cuda_h.py:19] end gate cost 0.000629425048828125 seconds
DEBUG 01-15 16:09:17.141897.141897 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:17.141425.141425 lmp.py:1912] 
DEBUG 01-15 16:09:17.141425.141425 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:17.141181.141181 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:17.141354.141354 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:17.141712.141712 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:17.141878.141878 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:17.141852.141852 lmp.py:1917] 
DEBUG 01-15 16:09:17.141852.141852 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:17.141972.141972 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:17.141622.141622 lmp.py:1935]   Expert 36 |     20 | CPU
DEBUG 01-15 16:09:17.142026.142026 lmp.py:1935]   Expert 35 |     29 | CPU
DEBUG 01-15 16:09:17.142477.142477 lmp.py:1935]   Expert 25 |     44 | CPU
DEBUG 01-15 16:09:17.142451.142451 lmp.py:1935]   Expert 46 |     44 | CPU
DEBUG 01-15 16:09:17.142664.142664 lmp.py:1935]   Expert 51 |     51 | CPU
DEBUG 01-15 16:09:17.142399.142399 lmp.py:1935]   Expert 16 |     60 | CPU
DEBUG 01-15 16:09:17.142135.142135 lmp.py:1935]   Expert 30 |     63 | CPU
DEBUG 01-15 16:09:17.142347.142347 lmp.py:1935]   Expert  0 |     64 | CPU
DEBUG 01-15 16:09:17.142275.142275 lmp.py:1935]   Expert 43 |     70 | CPU
DEBUG 01-15 16:09:17.142726.142726 lmp.py:1935]   Expert 47 |     71 | CPU
DEBUG 01-15 16:09:17.142177.142177 lmp.py:1935]   Expert 55 |     73 | CPU
DEBUG 01-15 16:09:17.142628.142628 lmp.py:1935]   Expert 39 |     74 | CPU
DEBUG 01-15 16:09:17.142840.142840 lmp.py:1935]   Expert 44 |     74 | CPU
DEBUG 01-15 16:09:17.142814.142814 lmp.py:1935]   Expert 42 |     77 | CPU
DEBUG 01-15 16:09:17.142311.142311 lmp.py:1935]   Expert  2 |     82 | CPU
DEBUG 01-15 16:09:17.142809.142809 lmp.py:1935]   Expert  4 |    105 | CPU
DEBUG 01-15 16:09:17.142067.142067 lmp.py:1935]   Expert 33 |    119 | CPU
DEBUG 01-15 16:09:17.142803.142803 lmp.py:1935]   Expert 48 |    119 | CPU
DEBUG 01-15 16:09:17.142823.142823 lmp.py:1935]   Expert  6 |    122 | CPU
DEBUG 01-15 16:09:17.142082.142082 lmp.py:1935]   Expert 13 |    125 | CPU
DEBUG 01-15 16:09:17.142487.142487 lmp.py:1935]   Expert 24 |    126 | CPU
DEBUG 01-15 16:09:17.142414.142414 lmp.py:1935]   Expert 61 |    127 | CPU
DEBUG 01-15 16:09:17.142865.142865 lmp.py:1935]   Expert 29 |    128 | CPU
DEBUG 01-15 16:09:17.142078.142078 lmp.py:1935]   Expert 56 |    131 | CPU
DEBUG 01-15 16:09:17.142005.142005 lmp.py:1935]   Expert 15 |    132 | CPU
DEBUG 01-15 16:09:17.142841.142841 lmp.py:1935]   Expert 38 |    140 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142676.142676 lmp.py:1935]   Expert 54 |    140 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142749.142749 lmp.py:1935]   Expert  9 |    141 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142915.142915 lmp.py:1935]   Expert 20 |    144 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142320.142320 lmp.py:1935]   Expert  7 |    148 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142247.142247 lmp.py:1935]   Expert 59 |    151 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142414.142414 lmp.py:1935]   Expert 62 |    156 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142341.142341 lmp.py:1935]   Expert 45 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142031.142031 lmp.py:1935]   Expert 19 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142197.142197 lmp.py:1935]   Expert 34 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142601.142601 lmp.py:1935]   Expert 57 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142006.142006 lmp.py:1935]   Expert 50 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142126.142126 lmp.py:1935]   Expert 10 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142007.142007 lmp.py:1935]   Expert 31 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142650.142650 lmp.py:1935]   Expert 23 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142293.142293 lmp.py:1935]   Expert  8 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142221.142221 lmp.py:1935]   Expert 60 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142387.142387 lmp.py:1935]   Expert 18 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142553.142553 lmp.py:1935]   Expert 53 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142242.142242 lmp.py:1935]   Expert 22 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142408.142408 lmp.py:1935]   Expert 52 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142575.142575 lmp.py:1935]   Expert 37 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142741.142741 lmp.py:1935]   Expert  5 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142099.142099 lmp.py:1935]   Expert 17 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142980.142980 lmp.py:1935]   Expert 11 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142623.142623 lmp.py:1935]   Expert  1 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142266.142266 lmp.py:1935]   Expert 49 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142909.142909 lmp.py:1935]   Expert 41 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142314.142314 lmp.py:1935]   Expert 28 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142242.142242 lmp.py:1935]   Expert 26 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142408.142408 lmp.py:1935]   Expert 32 |    293 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142574.142574 lmp.py:1935]   Expert 58 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142502.142502 lmp.py:1935]   Expert 40 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.142191.142191 lmp.py:1935]   Expert 14 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.142357.142357 lmp.py:1935]   Expert 12 |    331 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.143113.143113 lmp.py:1935]   Expert 63 |    333 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.143710.143710 lmp.py:1935]   Expert 21 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.143876.143876 lmp.py:1935]   Expert 27 |    669 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.143804.143804 lmp.py:1935]   Expert  3 |   1016 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.143539.143539 lmp.py:1937] 
DEBUG 01-15 16:09:17.143539.143539 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:17.143752.143752 lmp.py:1938]   CPU:   2130 tokens
DEBUG 01-15 16:09:17.143964.143964 lmp.py:1942]   cuda:1:   5072 tokens (19 experts)
DEBUG 01-15 16:09:17.143415.143415 lmp.py:1942]   cuda:2:   5086 tokens (20 experts)
DEBUG 01-15 16:09:17.143674.143674 lmp.py:1943]   Total GPU:  10158 tokens
DEBUG 01-15 16:09:17.143171.143171 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:17.143171.143171 lmp.py:1944] 
DEBUG 01-15 16:09:17.143390.143390 cuda_h.py:19] end experts_map_get cost 0.0016345977783203125 seconds
DEBUG 01-15 16:09:17.143710.143710 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:17.143274.143274 lmp.py:1953] 
DEBUG 01-15 16:09:17.143274.143274 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:17.143772.143772 cuda_h.py:19] end cpu_experts_submit cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:09:17.143323.143323 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:17.143821.143821 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:17.145792.145792 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.1.gate_proj.weight': 0, 'model.layers.24.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.14.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.14.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.14.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.17.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.17.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.17.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.58.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.58.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.58.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.24.mlp.experts.5.gate_proj.weight': 0, 'model.layers.24.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.27.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.27.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.27.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.28.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.28.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.28.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.32.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.32.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.32.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.34.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.34.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.34.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.40.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.40.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.40.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.24.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.24.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.24.mlp.experts.62.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(28345630720, 5767168, 0, 0), (28351397888, 5767168, 5767168, 0), (28339863552, 5767168, 11534336, 0), (28380233728, 5767168, 17301504, 0), (28386000896, 5767168, 23068672, 0), (28374466560, 5767168, 28835840, 0), (28570550272, 5767168, 34603008, 0), (28576317440, 5767168, 40370176, 0), (28564783104, 5767168, 46137344, 0), (28622454784, 5767168, 51904512, 0), (28628221952, 5767168, 57671680, 0), (28616687616, 5767168, 63438848, 0), (28657057792, 5767168, 69206016, 0), (28662824960, 5767168, 74973184, 0), (28651290624, 5767168, 80740352, 0), (28674359296, 5767168, 86507520, 0), (28680126464, 5767168, 92274688, 0), (28668592128, 5767168, 98041856, 0), (28726263808, 5767168, 103809024, 0), (28732030976, 5767168, 109576192, 0), (28720496640, 5767168, 115343360, 0), (28778168320, 5767168, 121110528, 0), (28783935488, 5767168, 126877696, 0), (28772401152, 5767168, 132644864, 0), (28864675840, 5767168, 138412032, 0), (28870443008, 5767168, 144179200, 0), (28858908672, 5767168, 149946368, 0), (28968484864, 5767168, 155713536, 0), (28974252032, 5767168, 161480704, 0), (28962717696, 5767168, 167247872, 0), (28985786368, 5767168, 173015040, 0), (28991553536, 5767168, 178782208, 0), (28980019200, 5767168, 184549376, 0), (29037690880, 5767168, 190316544, 0), (29043458048, 5767168, 196083712, 0), (29031923712, 5767168, 201850880, 0), (29106896896, 5767168, 207618048, 0), (29112664064, 5767168, 213385216, 0), (29101129728, 5767168, 219152384, 0), (29193404416, 5767168, 224919552, 0), (29199171584, 5767168, 230686720, 0), (29187637248, 5767168, 236453888, 0), (29245308928, 5767168, 242221056, 0), (29251076096, 5767168, 247988224, 0), (29239541760, 5767168, 253755392, 0), (29331816448, 5767168, 259522560, 0), (29337583616, 5767168, 265289728, 0), (29326049280, 5767168, 271056896, 0), (29349117952, 5767168, 276824064, 0), (29354885120, 5767168, 282591232, 0), (29343350784, 5767168, 288358400, 0), (29366419456, 5767168, 294125568, 0), (29372186624, 5767168, 299892736, 0), (29360652288, 5767168, 305659904, 0), (29418323968, 5767168, 311427072, 0), (29424091136, 5767168, 317194240, 0), (29412556800, 5767168, 322961408, 0)], 2: [(28414836736, 5767168, 0, 0), (28420603904, 5767168, 5767168, 0), (28409069568, 5767168, 11534336, 0), (28449439744, 5767168, 17301504, 0), (28455206912, 5767168, 23068672, 0), (28443672576, 5767168, 28835840, 0), (28466741248, 5767168, 34603008, 0), (28472508416, 5767168, 40370176, 0), (28460974080, 5767168, 46137344, 0), (28484042752, 5767168, 51904512, 0), (28489809920, 5767168, 57671680, 0), (28478275584, 5767168, 63438848, 0), (28501344256, 5767168, 69206016, 0), (28507111424, 5767168, 74973184, 0), (28495577088, 5767168, 80740352, 0), (28518645760, 5767168, 86507520, 0), (28524412928, 5767168, 92274688, 0), (28512878592, 5767168, 98041856, 0), (28535947264, 5767168, 103809024, 0), (28541714432, 5767168, 109576192, 0), (28530180096, 5767168, 115343360, 0), (28639756288, 5767168, 121110528, 0), (28645523456, 5767168, 126877696, 0), (28633989120, 5767168, 132644864, 0), (28691660800, 5767168, 138412032, 0), (28697427968, 5767168, 144179200, 0), (28685893632, 5767168, 149946368, 0), (28708962304, 5767168, 155713536, 0), (28714729472, 5767168, 161480704, 0), (28703195136, 5767168, 167247872, 0), (28795469824, 5767168, 173015040, 0), (28801236992, 5767168, 178782208, 0), (28789702656, 5767168, 184549376, 0), (28812771328, 5767168, 190316544, 0), (28818538496, 5767168, 196083712, 0), (28807004160, 5767168, 201850880, 0), (28881977344, 5767168, 207618048, 0), (28887744512, 5767168, 213385216, 0), (28876210176, 5767168, 219152384, 0), (28916580352, 5767168, 224919552, 0), (28922347520, 5767168, 230686720, 0), (28910813184, 5767168, 236453888, 0), (29020389376, 5767168, 242221056, 0), (29026156544, 5767168, 247988224, 0), (29014622208, 5767168, 253755392, 0), (29176102912, 5767168, 259522560, 0), (29181870080, 5767168, 265289728, 0), (29170335744, 5767168, 271056896, 0), (29228007424, 5767168, 276824064, 0), (29233774592, 5767168, 282591232, 0), (29222240256, 5767168, 288358400, 0), (29262610432, 5767168, 294125568, 0), (29268377600, 5767168, 299892736, 0), (29256843264, 5767168, 305659904, 0), (29314514944, 5767168, 311427072, 0), (29320282112, 5767168, 317194240, 0), (29308747776, 5767168, 322961408, 0), (29401022464, 5767168, 328728576, 0), (29406789632, 5767168, 334495744, 0), (29395255296, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a8143f5890>, 2: <capsule object NULL at 0x74a6785de4c0>}
DEBUG 01-15 16:09:17.145480.145480 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.145601.145601 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.145496.145496 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8cf61343-a3e7-4a6c-91f7-d0d3eb6029cc
DEBUG 01-15 16:09:17.145457.145457 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.145098.145098 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:17.145230.145230 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:17.146839.146839 client.py:127] Model loaded
DEBUG 01-15 16:09:17.146377.146377 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.146001.146001 cuda_h.py:19] end move_flatidxs cost 0.0008344650268554688 seconds
DEBUG 01-15 16:09:17.146109.146109 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:17.147707.147707 cuda_h.py:19] end restore2model cost 0.00031948089599609375 seconds
DEBUG 01-15 16:09:17.147900.147900 cuda_h.py:19] end sllm_worker_task cost 0.010795116424560547 seconds
INFO 01-15 16:09:17.148570.148570 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8cf61343-a3e7-4a6c-91f7-d0d3eb6029cc
DEBUG 01-15 16:09:17.149440.149440 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005586147308349609 seconds
DEBUG 01-15 16:09:17.149740.149740 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.151578.151578 cuda_h.py:19] end restore2model cost 0.0028328895568847656 seconds
DEBUG 01-15 16:09:17.152315.152315 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008646249771118164 seconds
DEBUG 01-15 16:09:17.152939.152939 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:17.151697.151697 cuda_h.py:19] end group_tensors cost 0.004682779312133789 seconds
DEBUG 01-15 16:09:17.152978.152978 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:17.152061.152061 cuda_h.py:19] end gpu_sexperts cost 0.000270843505859375 seconds
DEBUG 01-15 16:09:17.152851.152851 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:17.152111.152111 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1696090698242188e-05 seconds
DEBUG 01-15 16:09:17.152927.152927 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:17.152498.152498 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.153594.153594 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010454654693603516 seconds
DEBUG 01-15 16:09:17.153272.153272 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.153294.153294 cuda_h.py:19] end gpu_group_list cost 0.00022292137145996094 seconds
DEBUG 01-15 16:09:17.154768.154768 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.155494.155494 cuda_h.py:19] end group pad cost 0.0037033557891845703 seconds
DEBUG 01-15 16:09:17.156569.156569 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:17.156691.156691 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011715888977050781 seconds
DEBUG 01-15 16:09:17.156607.156607 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.156388.156388 cuda_h.py:19] end gpu_group_list cost 0.00031566619873046875 seconds
DEBUG 01-15 16:09:17.157468.157468 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:17.158587.158587 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8cf61343-a3e7-4a6c-91f7-d0d3eb6029cc
DEBUG 01-15 16:09:17.175897.175897 cuda_h.py:19] end group_einsum cost 0.019809961318969727 seconds
DEBUG 01-15 16:09:17.176115.176115 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:17.180137.180137 cuda_h.py:19] end get_outputs_cpu1 cost 0.004378318786621094 seconds
DEBUG 01-15 16:09:17.181630.181630 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.035584211349487305 seconds
INFO 01-15 16:09:17.187561.187561 client.py:127] Model loaded
DEBUG 01-15 16:09:17.187745.187745 cuda_h.py:19] end wait_experts_multi_device cost 0.029061079025268555 seconds
DEBUG 01-15 16:09:17.187481.187481 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:17.187550.187550 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005910396575927734 seconds
DEBUG 01-15 16:09:17.187936.187936 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:17.189843.189843 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.189425.189425 cuda_h.py:19] end index_scatter cost 9.369850158691406e-05 seconds
DEBUG 01-15 16:09:17.189153.189153 cuda_h.py:19] end cpuoutputsdeal cost 0.0018608570098876953 seconds
DEBUG 01-15 16:09:17.189427.189427 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:17.190257.190257 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.190483.190483 cuda_h.py:19] end gpu_group_tensor cost 0.00018906593322753906 seconds
DEBUG 01-15 16:09:17.190174.190174 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.190128.190128 cuda_h.py:19] end gpu_group_tensor cost 0.00016736984252929688 seconds
DEBUG 01-15 16:09:17.190027.190027 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.191509.191509 cuda_h.py:19] end gpu_group_einsum cost 0.0010902881622314453 seconds
DEBUG 01-15 16:09:17.192675.192675 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.192016.192016 cuda_h.py:19] end gpu_group_einsum cost 0.0006220340728759766 seconds
DEBUG 01-15 16:09:17.192731.192731 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.193848.193848 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.193881.193881 cuda_h.py:19] end all_expert_outputs_slices cost 0.00030612945556640625 seconds
DEBUG 01-15 16:09:17.193148.193148 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.193285.193285 cuda_h.py:19] end concat_expert_out cost 8.058547973632812e-05 seconds
DEBUG 01-15 16:09:17.193613.193613 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.193386.193386 cuda_h.py:19] end index_scatter cost 8.726119995117188e-05 seconds
DEBUG 01-15 16:09:17.194887.194887 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001226663589477539 seconds
DEBUG 01-15 16:09:17.194091.194091 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.194319.194319 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.194044.194044 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023365020751953125 seconds
DEBUG 01-15 16:09:17.194258.194258 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.194686.194686 cuda_h.py:19] end concat_expert_out cost 8.630752563476562e-05 seconds
DEBUG 01-15 16:09:17.195690.195690 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.195609.195609 cuda_h.py:19] end index_scatter cost 8.7738037109375e-05 seconds
DEBUG 01-15 16:09:17.195406.195406 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008358955383300781 seconds
DEBUG 01-15 16:09:17.195370.195370 cuda_h.py:19] end gpu_experts_multi_device cost 0.04285454750061035 seconds
DEBUG 01-15 16:09:17.195970.195970 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.05469679832458496 seconds
DEBUG 01-15 16:09:17.196292.196292 cuda_h.py:19] end prefill_layer cost 0.06004500389099121 seconds
DEBUG 01-15 16:09:17.196785.196785 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:09:17.196846.196846 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:17.196338.196338 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:09:17.196306.196306 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:09:17.196420.196420 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:09:17.196901.196901 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 6.008148193359375e-05 seconds
DEBUG 01-15 16:09:17.196313.196313 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 0.00012373924255371094 seconds
DEBUG 01-15 16:09:17.196321.196321 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:17.196907.196907 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:17.196837.196837 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:17.196719.196719 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:17.196330.196330 cuda_h.py:19] end allocate_cuda_memory cost 0.00020599365234375 seconds
DEBUG 01-15 16:09:17.197365.197365 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:17.197360.197360 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.197421.197421 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.197455.197455 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b22f389c-dcc2-4e4a-824f-7a3a4cae197c
DEBUG 01-15 16:09:17.197855.197855 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.197264.197264 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:17.197254.197254 cuda_h.py:10] start self_attn
INFO 01-15 16:09:17.198878.198878 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b22f389c-dcc2-4e4a-824f-7a3a4cae197c
DEBUG 01-15 16:09:17.198807.198807 cuda_h.py:19] end load_into_gpu_async cost 0.001699686050415039 seconds
DEBUG 01-15 16:09:17.198888.198888 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:17.198348.198348 cuda_h.py:19] end restore_tensors2 cost 6.866455078125e-05 seconds
DEBUG 01-15 16:09:17.198673.198673 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002199411392211914 seconds
INFO 01-15 16:09:17.198370.198370 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b22f389c-dcc2-4e4a-824f-7a3a4cae197c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:17.202362.202362 cuda_h.py:19] end self_attn cost 0.004137992858886719 seconds
DEBUG 01-15 16:09:17.202564.202564 cuda_h.py:19] end iln_self_attn_paln cost 0.005819082260131836 seconds
DEBUG 01-15 16:09:17.202440.202440 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:09:17.202150.202150 cuda_h.py:10] start gate
DEBUG 01-15 16:09:17.203754.203754 cuda_h.py:19] end gate cost 0.00061798095703125 seconds
DEBUG 01-15 16:09:17.203822.203822 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:17.203588.203588 lmp.py:1912] 
DEBUG 01-15 16:09:17.203588.203588 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:17.203344.203344 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:17.203517.203517 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:17.203305.203305 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:17.203948.203948 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:17.203545.203545 lmp.py:1917] 
DEBUG 01-15 16:09:17.203545.203545 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:17.203857.203857 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:17.203176.203176 lmp.py:1935]   Expert 13 |     27 | CPU
DEBUG 01-15 16:09:17.203534.203534 lmp.py:1935]   Expert 44 |     37 | CPU
DEBUG 01-15 16:09:17.203177.203177 lmp.py:1935]   Expert  9 |     41 | CPU
DEBUG 01-15 16:09:17.203820.203820 lmp.py:1935]   Expert 25 |     42 | CPU
DEBUG 01-15 16:09:17.203986.203986 lmp.py:1935]   Expert 16 |     47 | CPU
DEBUG 01-15 16:09:17.203152.203152 lmp.py:1935]   Expert 38 |     48 | CPU
DEBUG 01-15 16:09:17.203557.203557 lmp.py:1935]   Expert  2 |     53 | CPU
DEBUG 01-15 16:09:17.203723.203723 lmp.py:1935]   Expert 22 |     53 | CPU
DEBUG 01-15 16:09:17.203366.203366 lmp.py:1935]   Expert 33 |     59 | CPU
DEBUG 01-15 16:09:17.203770.203770 lmp.py:1935]   Expert 42 |     60 | CPU
DEBUG 01-15 16:09:17.203413.203413 lmp.py:1935]   Expert  5 |     67 | CPU
DEBUG 01-15 16:09:17.203818.203818 lmp.py:1935]   Expert 23 |     76 | CPU
DEBUG 01-15 16:09:17.203746.203746 lmp.py:1935]   Expert 24 |     78 | CPU
DEBUG 01-15 16:09:17.203435.203435 lmp.py:1935]   Expert 10 |     83 | CPU
DEBUG 01-15 16:09:17.203363.203363 lmp.py:1935]   Expert 59 |    102 | CPU
DEBUG 01-15 16:09:17.203814.203814 lmp.py:1935]   Expert 21 |    106 | CPU
DEBUG 01-15 16:09:17.203741.203741 lmp.py:1935]   Expert 55 |    113 | CPU
DEBUG 01-15 16:09:17.203669.203669 lmp.py:1935]   Expert 46 |    114 | CPU
DEBUG 01-15 16:09:17.203597.203597 lmp.py:1935]   Expert 45 |    118 | CPU
DEBUG 01-15 16:09:17.203286.203286 lmp.py:1935]   Expert 61 |    125 | CPU
DEBUG 01-15 16:09:17.203975.203975 lmp.py:1935]   Expert 31 |    128 | CPU
DEBUG 01-15 16:09:17.204903.204903 lmp.py:1935]   Expert 51 |    138 | CPU
DEBUG 01-15 16:09:17.204354.204354 lmp.py:1935]   Expert  6 |    142 | CPU
DEBUG 01-15 16:09:17.204758.204758 lmp.py:1935]   Expert 36 |    142 | CPU
DEBUG 01-15 16:09:17.204925.204925 lmp.py:1935]   Expert  8 |    145 | CPU
DEBUG 01-15 16:09:17.204521.204521 lmp.py:1935]   Expert 43 |    146 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204118.204118 lmp.py:1935]   Expert  3 |    151 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204715.204715 lmp.py:1935]   Expert  0 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204834.204834 lmp.py:1935]   Expert 48 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204239.204239 lmp.py:1935]   Expert 26 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204120.204120 lmp.py:1935]   Expert 18 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204525.204525 lmp.py:1935]   Expert 41 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204929.204929 lmp.py:1935]   Expert 12 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204334.204334 lmp.py:1935]   Expert  7 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204977.204977 lmp.py:1935]   Expert 20 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204905.204905 lmp.py:1935]   Expert 28 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204263.204263 lmp.py:1935]   Expert  1 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204383.204383 lmp.py:1935]   Expert 56 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204503.204503 lmp.py:1935]   Expert 27 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204338.204338 lmp.py:1935]   Expert 34 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204219.204219 lmp.py:1935]   Expert 47 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204862.204862 lmp.py:1935]   Expert 11 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204505.204505 lmp.py:1935]   Expert 32 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204493.204493 lmp.py:1935]   Expert 40 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204142.204142 lmp.py:1935]   Expert 49 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204308.204308 lmp.py:1935]   Expert 53 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204998.204998 lmp.py:1935]   Expert 63 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204925.204925 lmp.py:1935]   Expert 15 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204092.204092 lmp.py:1935]   Expert 50 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204735.204735 lmp.py:1935]   Expert 29 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204901.204901 lmp.py:1935]   Expert  4 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204305.204305 lmp.py:1935]   Expert 30 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204995.204995 lmp.py:1935]   Expert 35 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204445.204445 lmp.py:1935]   Expert 14 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204135.204135 lmp.py:1935]   Expert 37 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204824.204824 lmp.py:1935]   Expert 52 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204513.204513 lmp.py:1935]   Expert 17 |    357 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204487.204487 lmp.py:1935]   Expert 54 |    378 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204177.204177 lmp.py:1935]   Expert 39 |    388 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204866.204866 lmp.py:1935]   Expert 57 |    414 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204317.204317 lmp.py:1935]   Expert 60 |    456 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204529.204529 lmp.py:1935]   Expert 62 |    460 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204457.204457 lmp.py:1935]   Expert 19 |    544 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.204862.204862 lmp.py:1935]   Expert 58 |    572 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.204074.204074 lmp.py:1937] 
DEBUG 01-15 16:09:17.204074.204074 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:17.204002.204002 lmp.py:1938]   CPU:   2144 tokens
DEBUG 01-15 16:09:17.204883.204883 lmp.py:1942]   cuda:1:   5143 tokens (20 experts)
DEBUG 01-15 16:09:17.204573.204573 lmp.py:1942]   cuda:2:   5001 tokens (19 experts)
DEBUG 01-15 16:09:17.204070.204070 lmp.py:1943]   Total GPU:  10144 tokens
DEBUG 01-15 16:09:17.204044.204044 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:17.204044.204044 lmp.py:1944] 
DEBUG 01-15 16:09:17.204740.204740 cuda_h.py:19] end experts_map_get cost 0.0016667842864990234 seconds
DEBUG 01-15 16:09:17.204252.204252 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:17.204816.204816 lmp.py:1953] 
DEBUG 01-15 16:09:17.204816.204816 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:17.205361.205361 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:17.205341.205341 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:17.205793.205793 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:17.206861.206861 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.1.gate_proj.weight': 0, 'model.layers.25.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.28.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.28.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.28.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.30.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.30.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.30.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.34.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.34.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.34.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.25.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.25.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.25.mlp.experts.60.up_proj.weight': 340262912}, 2: {'model.layers.25.mlp.experts.0.gate_proj.weight': 0, 'model.layers.25.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.27.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.27.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.27.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(29452926976, 5767168, 0, 0), (29458694144, 5767168, 5767168, 0), (29447159808, 5767168, 11534336, 0), (29487529984, 5767168, 17301504, 0), (29493297152, 5767168, 23068672, 0), (29481762816, 5767168, 28835840, 0), (29556736000, 5767168, 34603008, 0), (29562503168, 5767168, 40370176, 0), (29550968832, 5767168, 46137344, 0), (29677846528, 5767168, 51904512, 0), (29683613696, 5767168, 57671680, 0), (29672079360, 5767168, 63438848, 0), (29695148032, 5767168, 69206016, 0), (29700915200, 5767168, 74973184, 0), (29689380864, 5767168, 80740352, 0), (29920067584, 5767168, 86507520, 0), (29925834752, 5767168, 92274688, 0), (29914300416, 5767168, 98041856, 0), (29937369088, 5767168, 103809024, 0), (29943136256, 5767168, 109576192, 0), (29931601920, 5767168, 115343360, 0), (29954670592, 5767168, 121110528, 0), (29960437760, 5767168, 126877696, 0), (29948903424, 5767168, 132644864, 0), (30023876608, 5767168, 138412032, 0), (30029643776, 5767168, 144179200, 0), (30018109440, 5767168, 149946368, 0), (30075781120, 5767168, 155713536, 0), (30081548288, 5767168, 161480704, 0), (30070013952, 5767168, 167247872, 0), (30110384128, 5767168, 173015040, 0), (30116151296, 5767168, 178782208, 0), (30104616960, 5767168, 184549376, 0), (30127685632, 5767168, 190316544, 0), (30133452800, 5767168, 196083712, 0), (30121918464, 5767168, 201850880, 0), (30144987136, 5767168, 207618048, 0), (30150754304, 5767168, 213385216, 0), (30139219968, 5767168, 219152384, 0), (30179590144, 5767168, 224919552, 0), (30185357312, 5767168, 230686720, 0), (30173822976, 5767168, 236453888, 0), (30248796160, 5767168, 242221056, 0), (30254563328, 5767168, 247988224, 0), (30243028992, 5767168, 253755392, 0), (30266097664, 5767168, 259522560, 0), (30271864832, 5767168, 265289728, 0), (30260330496, 5767168, 271056896, 0), (30352605184, 5767168, 276824064, 0), (30358372352, 5767168, 282591232, 0), (30346838016, 5767168, 288358400, 0), (30369906688, 5767168, 294125568, 0), (30375673856, 5767168, 299892736, 0), (30364139520, 5767168, 305659904, 0), (30439112704, 5767168, 311427072, 0), (30444879872, 5767168, 317194240, 0), (30433345536, 5767168, 322961408, 0), (30473715712, 5767168, 328728576, 0), (30479482880, 5767168, 334495744, 0), (30467948544, 5767168, 340262912, 0)], 2: [(29435625472, 5767168, 0, 0), (29441392640, 5767168, 5767168, 0), (29429858304, 5767168, 11534336, 0), (29504831488, 5767168, 17301504, 0), (29510598656, 5767168, 23068672, 0), (29499064320, 5767168, 28835840, 0), (29625942016, 5767168, 34603008, 0), (29631709184, 5767168, 40370176, 0), (29620174848, 5767168, 46137344, 0), (29643243520, 5767168, 51904512, 0), (29649010688, 5767168, 57671680, 0), (29637476352, 5767168, 63438848, 0), (29729751040, 5767168, 69206016, 0), (29735518208, 5767168, 74973184, 0), (29723983872, 5767168, 80740352, 0), (29747052544, 5767168, 86507520, 0), (29752819712, 5767168, 92274688, 0), (29741285376, 5767168, 98041856, 0), (29764354048, 5767168, 103809024, 0), (29770121216, 5767168, 109576192, 0), (29758586880, 5767168, 115343360, 0), (29781655552, 5767168, 121110528, 0), (29787422720, 5767168, 126877696, 0), (29775888384, 5767168, 132644864, 0), (29885464576, 5767168, 138412032, 0), (29891231744, 5767168, 144179200, 0), (29879697408, 5767168, 149946368, 0), (29902766080, 5767168, 155713536, 0), (29908533248, 5767168, 161480704, 0), (29896998912, 5767168, 167247872, 0), (29989273600, 5767168, 173015040, 0), (29995040768, 5767168, 178782208, 0), (29983506432, 5767168, 184549376, 0), (30041178112, 5767168, 190316544, 0), (30046945280, 5767168, 196083712, 0), (30035410944, 5767168, 201850880, 0), (30283399168, 5767168, 207618048, 0), (30289166336, 5767168, 213385216, 0), (30277632000, 5767168, 219152384, 0), (30300700672, 5767168, 224919552, 0), (30306467840, 5767168, 230686720, 0), (30294933504, 5767168, 236453888, 0), (30335303680, 5767168, 242221056, 0), (30341070848, 5767168, 247988224, 0), (30329536512, 5767168, 253755392, 0), (30404509696, 5767168, 259522560, 0), (30410276864, 5767168, 265289728, 0), (30398742528, 5767168, 271056896, 0), (30421811200, 5767168, 276824064, 0), (30427578368, 5767168, 282591232, 0), (30416044032, 5767168, 288358400, 0), (30508318720, 5767168, 294125568, 0), (30514085888, 5767168, 299892736, 0), (30502551552, 5767168, 305659904, 0), (30525620224, 5767168, 311427072, 0), (30531387392, 5767168, 317194240, 0), (30519853056, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6785de580>, 2: <capsule object NULL at 0x74a6785de160>}
DEBUG 01-15 16:09:17.206932.206932 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.206761.206761 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.206749.206749 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c13c8d3e-a315-4d6a-89b5-8e26451b417e
DEBUG 01-15 16:09:17.206604.206604 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.206484.206484 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:17.206562.206562 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:17.206728.206728 client.py:127] Model loaded
DEBUG 01-15 16:09:17.206710.206710 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.207275.207275 cuda_h.py:19] end restore2model cost 0.0004146099090576172 seconds
DEBUG 01-15 16:09:17.207436.207436 cuda_h.py:19] end sllm_worker_task cost 0.010772228240966797 seconds
DEBUG 01-15 16:09:17.207447.207447 cuda_h.py:19] end move_flatidxs cost 0.0008382797241210938 seconds
DEBUG 01-15 16:09:17.207269.207269 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:17.208853.208853 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c13c8d3e-a315-4d6a-89b5-8e26451b417e
DEBUG 01-15 16:09:17.209920.209920 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003960132598876953 seconds
DEBUG 01-15 16:09:17.209830.209830 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.212445.212445 cuda_h.py:19] end restore2model cost 0.0029141902923583984 seconds
DEBUG 01-15 16:09:17.212666.212666 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007100820541381836 seconds
DEBUG 01-15 16:09:17.212813.212813 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:17.212088.212088 cuda_h.py:19] end gpu_sexperts cost 0.000278472900390625 seconds
DEBUG 01-15 16:09:17.212202.212202 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:17.212402.212402 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4781951904296875e-05 seconds
DEBUG 01-15 16:09:17.212906.212906 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:17.212179.212179 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.213431.213431 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009617805480957031 seconds
DEBUG 01-15 16:09:17.213611.213611 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.213268.213268 cuda_h.py:19] end gpu_group_list cost 0.00021123886108398438 seconds
DEBUG 01-15 16:09:17.214998.214998 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.215080.215080 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010197162628173828 seconds
DEBUG 01-15 16:09:17.215136.215136 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.216892.216892 cuda_h.py:19] end gpu_group_list cost 0.00021123886108398438 seconds
DEBUG 01-15 16:09:17.216539.216539 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:17.216753.216753 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c13c8d3e-a315-4d6a-89b5-8e26451b417e
DEBUG 01-15 16:09:17.216718.216718 cuda_h.py:19] end group_tensors cost 0.008825540542602539 seconds
DEBUG 01-15 16:09:17.217876.217876 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:17.220102.220102 cuda_h.py:19] end group pad cost 0.0031380653381347656 seconds
DEBUG 01-15 16:09:17.220077.220077 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:17.249353.249353 cuda_h.py:19] end group_einsum cost 0.029114484786987305 seconds
DEBUG 01-15 16:09:17.250755.250755 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:17.252639.252639 cuda_h.py:19] end get_outputs_cpu1 cost 0.0028076171875 seconds
INFO 01-15 16:09:17.253354.253354 client.py:127] Model loaded
DEBUG 01-15 16:09:17.253386.253386 cuda_h.py:19] end wait_experts_multi_device cost 0.03668355941772461 seconds
DEBUG 01-15 16:09:17.253335.253335 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:17.253607.253607 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0469355583190918 seconds
DEBUG 01-15 16:09:17.254460.254460 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0006854534149169922 seconds
DEBUG 01-15 16:09:17.254927.254927 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:17.256559.256559 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.256607.256607 cuda_h.py:19] end index_scatter cost 0.00013875961303710938 seconds
DEBUG 01-15 16:09:17.256675.256675 cuda_h.py:19] end cpuoutputsdeal cost 0.0023047924041748047 seconds
DEBUG 01-15 16:09:17.257083.257083 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:17.257986.257986 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.257738.257738 cuda_h.py:19] end gpu_group_tensor cost 0.0002453327178955078 seconds
DEBUG 01-15 16:09:17.257310.257310 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.257359.257359 cuda_h.py:19] end gpu_group_tensor cost 0.0002193450927734375 seconds
DEBUG 01-15 16:09:17.257498.257498 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.258837.258837 cuda_h.py:19] end gpu_group_einsum cost 0.0007565021514892578 seconds
DEBUG 01-15 16:09:17.259400.259400 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.259841.259841 cuda_h.py:19] end gpu_group_einsum cost 0.0007903575897216797 seconds
DEBUG 01-15 16:09:17.260351.260351 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.260780.260780 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.260398.260398 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003771781921386719 seconds
DEBUG 01-15 16:09:17.260540.260540 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.260002.260002 cuda_h.py:19] end concat_expert_out cost 9.655952453613281e-05 seconds
DEBUG 01-15 16:09:17.261688.261688 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.261065.261065 cuda_h.py:19] end index_scatter cost 0.00010228157043457031 seconds
DEBUG 01-15 16:09:17.261065.261065 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0014514923095703125 seconds
DEBUG 01-15 16:09:17.261494.261494 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.261512.261512 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.262755.262755 cuda_h.py:19] end all_expert_outputs_slices cost 0.00029921531677246094 seconds
DEBUG 01-15 16:09:17.262657.262657 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.262402.262402 cuda_h.py:19] end concat_expert_out cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:09:17.262490.262490 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.262142.262142 cuda_h.py:19] end index_scatter cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:09:17.262004.262004 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007281303405761719 seconds
DEBUG 01-15 16:09:17.262629.262629 cuda_h.py:19] end gpu_experts_multi_device cost 0.05005502700805664 seconds
DEBUG 01-15 16:09:17.262268.262268 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.06030726432800293 seconds
DEBUG 01-15 16:09:17.263235.263235 cuda_h.py:19] end prefill_layer cost 0.06695246696472168 seconds
DEBUG 01-15 16:09:17.263025.263025 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:09:17.263112.263112 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:17.263676.263676 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:09:17.263670.263670 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:09:17.263380.263380 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:09:17.263137.263137 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 3.647804260253906e-05 seconds
DEBUG 01-15 16:09:17.263946.263946 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:09:17.263172.263172 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:17.263665.263665 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:17.263119.263119 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:17.263718.263718 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:17.263130.263130 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:17.264968.264968 cuda_h.py:19] end allocate_cuda_memory cost 0.00026345252990722656 seconds
DEBUG 01-15 16:09:17.264792.264792 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:17.264939.264939 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.264007.264007 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.264524.264524 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7502baf8-0bcc-4615-b789-baf37e54ff1b
DEBUG 01-15 16:09:17.264250.264250 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.264386.264386 cuda_h.py:10] start self_attn
INFO 01-15 16:09:17.265363.265363 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7502baf8-0bcc-4615-b789-baf37e54ff1b
DEBUG 01-15 16:09:17.265159.265159 cuda_h.py:19] end load_into_gpu_async cost 0.0016367435455322266 seconds
DEBUG 01-15 16:09:17.265054.265054 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:17.265071.265071 cuda_h.py:19] end restore_tensors2 cost 8.678436279296875e-05 seconds
DEBUG 01-15 16:09:17.266642.266642 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022630691528320312 seconds
INFO 01-15 16:09:17.266115.266115 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7502baf8-0bcc-4615-b789-baf37e54ff1b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:17.268471.268471 cuda_h.py:19] end self_attn cost 0.0032553672790527344 seconds
DEBUG 01-15 16:09:17.268782.268782 cuda_h.py:19] end iln_self_attn_paln cost 0.004905223846435547 seconds
DEBUG 01-15 16:09:17.268578.268578 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:09:17.268348.268348 cuda_h.py:10] start gate
DEBUG 01-15 16:09:17.269943.269943 cuda_h.py:19] end gate cost 0.0007126331329345703 seconds
DEBUG 01-15 16:09:17.269170.269170 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:17.269051.269051 lmp.py:1912] 
DEBUG 01-15 16:09:17.269051.269051 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:17.269894.269894 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:17.269604.269604 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:17.269307.269307 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:17.269672.269672 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:17.269606.269606 lmp.py:1917] 
DEBUG 01-15 16:09:17.269606.269606 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:17.269733.269733 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:17.269250.269250 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:09:17.269854.269854 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:09:17.270696.270696 lmp.py:1935]   Expert 11 |     28 | CPU
DEBUG 01-15 16:09:17.270260.270260 lmp.py:1935]   Expert  7 |     40 | CPU
DEBUG 01-15 16:09:17.270578.270578 lmp.py:1935]   Expert  3 |     45 | CPU
DEBUG 01-15 16:09:17.270751.270751 lmp.py:1935]   Expert 51 |     45 | CPU
DEBUG 01-15 16:09:17.270447.270447 lmp.py:1935]   Expert 62 |     45 | CPU
DEBUG 01-15 16:09:17.270905.270905 lmp.py:1935]   Expert 17 |     52 | CPU
DEBUG 01-15 16:09:17.270363.270363 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:09:17.270012.270012 lmp.py:1935]   Expert 29 |     58 | CPU
DEBUG 01-15 16:09:17.270377.270377 lmp.py:1935]   Expert  6 |     62 | CPU
DEBUG 01-15 16:09:17.270027.270027 lmp.py:1935]   Expert  9 |     65 | CPU
DEBUG 01-15 16:09:17.270154.270154 lmp.py:1935]   Expert 38 |     75 | CPU
DEBUG 01-15 16:09:17.270088.270088 lmp.py:1935]   Expert 63 |     78 | CPU
DEBUG 01-15 16:09:17.270308.270308 lmp.py:1935]   Expert 55 |     82 | CPU
DEBUG 01-15 16:09:17.270765.270765 lmp.py:1935]   Expert 59 |     87 | CPU
DEBUG 01-15 16:09:17.270461.270461 lmp.py:1935]   Expert  8 |     93 | CPU
DEBUG 01-15 16:09:17.270681.270681 lmp.py:1935]   Expert 48 |     93 | CPU
DEBUG 01-15 16:09:17.270900.270900 lmp.py:1935]   Expert 19 |     95 | CPU
DEBUG 01-15 16:09:17.270119.270119 lmp.py:1935]   Expert 22 |    102 | CPU
DEBUG 01-15 16:09:17.270292.270292 lmp.py:1935]   Expert 49 |    103 | CPU
DEBUG 01-15 16:09:17.270465.270465 lmp.py:1935]   Expert 24 |    109 | CPU
DEBUG 01-15 16:09:17.270115.270115 lmp.py:1935]   Expert 36 |    115 | CPU
DEBUG 01-15 16:09:17.270334.270334 lmp.py:1935]   Expert 42 |    117 | CPU
DEBUG 01-15 16:09:17.270553.270553 lmp.py:1935]   Expert 34 |    118 | CPU
DEBUG 01-15 16:09:17.270918.270918 lmp.py:1935]   Expert 50 |    119 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270283.270283 lmp.py:1935]   Expert 39 |    125 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270933.270933 lmp.py:1935]   Expert  4 |    132 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270583.270583 lmp.py:1935]   Expert 37 |    142 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270425.270425 lmp.py:1935]   Expert 41 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270790.270790 lmp.py:1935]   Expert 15 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270916.270916 lmp.py:1935]   Expert 23 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270520.270520 lmp.py:1935]   Expert 56 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270693.270693 lmp.py:1935]   Expert 16 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270104.270104 lmp.py:1935]   Expert 60 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270515.270515 lmp.py:1935]   Expert 44 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270688.270688 lmp.py:1935]   Expert  1 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270623.270623 lmp.py:1935]   Expert 21 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270796.270796 lmp.py:1935]   Expert 43 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270492.270492 lmp.py:1935]   Expert 53 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270433.270433 lmp.py:1935]   Expert 47 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270037.270037 lmp.py:1935]   Expert 33 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270878.270878 lmp.py:1935]   Expert 12 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270243.270243 lmp.py:1935]   Expert 13 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270893.270893 lmp.py:1935]   Expert 32 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.270543.270543 lmp.py:1935]   Expert 28 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.270477.270477 lmp.py:1935]   Expert  0 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.271650.271650 lmp.py:1935]   Expert 54 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.271585.271585 lmp.py:1935]   Expert 31 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.271281.271281 lmp.py:1935]   Expert 26 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.271692.271692 lmp.py:1935]   Expert 10 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.271388.271388 lmp.py:1935]   Expert 18 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.271992.271992 lmp.py:1935]   Expert 57 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.271118.271118 lmp.py:1935]   Expert  2 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.271483.271483 lmp.py:1935]   Expert 58 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.271848.271848 lmp.py:1935]   Expert 40 |    337 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.271498.271498 lmp.py:1935]   Expert 25 |    362 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.271910.271910 lmp.py:1935]   Expert 45 |    365 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.271083.271083 lmp.py:1935]   Expert  5 |    440 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.271732.271732 lmp.py:1935]   Expert 35 |    461 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.271620.271620 lmp.py:1935]   Expert 27 |    484 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.271985.271985 lmp.py:1935]   Expert 46 |    551 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.271112.271112 lmp.py:1935]   Expert 52 |    596 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.271716.271716 lmp.py:1935]   Expert 14 |    883 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.271935.271935 lmp.py:1937] 
DEBUG 01-15 16:09:17.271935.271935 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:17.271869.271869 lmp.py:1938]   CPU:   1781 tokens
DEBUG 01-15 16:09:17.271996.271996 lmp.py:1942]   cuda:1:   5235 tokens (19 experts)
DEBUG 01-15 16:09:17.271407.271407 lmp.py:1942]   cuda:2:   5272 tokens (20 experts)
DEBUG 01-15 16:09:17.271388.271388 lmp.py:1943]   Total GPU:  10507 tokens
DEBUG 01-15 16:09:17.271369.271369 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:17.271369.271369 lmp.py:1944] 
DEBUG 01-15 16:09:17.271171.271171 cuda_h.py:19] end experts_map_get cost 0.002176046371459961 seconds
DEBUG 01-15 16:09:17.271180.271180 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:17.271043.271043 lmp.py:1953] 
DEBUG 01-15 16:09:17.271043.271043 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:17.271508.271508 cuda_h.py:19] end cpu_experts_submit cost 6.127357482910156e-05 seconds
DEBUG 01-15 16:09:17.271264.271264 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:17.271413.271413 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:17.273961.273961 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.60.up_proj.weight': 322961408}, 2: {'model.layers.26.mlp.experts.2.gate_proj.weight': 0, 'model.layers.26.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.13.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.13.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.13.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.35.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.35.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.35.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.53.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.53.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.53.up_proj.weight': 322961408, 'model.layers.26.mlp.experts.54.gate_proj.weight': 328728576, 'model.layers.26.mlp.experts.54.down_proj.weight': 334495744, 'model.layers.26.mlp.experts.54.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (30560223232, 5767168, 17301504, 0), (30565990400, 5767168, 23068672, 0), (30554456064, 5767168, 28835840, 0), (30629429248, 5767168, 34603008, 0), (30635196416, 5767168, 40370176, 0), (30623662080, 5767168, 46137344, 0), (30715936768, 5767168, 51904512, 0), (30721703936, 5767168, 57671680, 0), (30710169600, 5767168, 63438848, 0), (30750539776, 5767168, 69206016, 0), (30756306944, 5767168, 74973184, 0), (30744772608, 5767168, 80740352, 0), (30785142784, 5767168, 86507520, 0), (30790909952, 5767168, 92274688, 0), (30779375616, 5767168, 98041856, 0), (30802444288, 5767168, 103809024, 0), (30808211456, 5767168, 109576192, 0), (30796677120, 5767168, 115343360, 0), (30975459328, 5767168, 121110528, 0), (30981226496, 5767168, 126877696, 0), (30969692160, 5767168, 132644864, 0), (31010062336, 5767168, 138412032, 0), (31015829504, 5767168, 144179200, 0), (31004295168, 5767168, 149946368, 0), (31079268352, 5767168, 155713536, 0), (31085035520, 5767168, 161480704, 0), (31073501184, 5767168, 167247872, 0), (31096569856, 5767168, 173015040, 0), (31102337024, 5767168, 178782208, 0), (31090802688, 5767168, 184549376, 0), (31183077376, 5767168, 190316544, 0), (31188844544, 5767168, 196083712, 0), (31177310208, 5767168, 201850880, 0), (31217680384, 5767168, 207618048, 0), (31223447552, 5767168, 213385216, 0), (31211913216, 5767168, 219152384, 0), (31286886400, 5767168, 224919552, 0), (31292653568, 5767168, 230686720, 0), (31281119232, 5767168, 236453888, 0), (31356092416, 5767168, 242221056, 0), (31361859584, 5767168, 247988224, 0), (31350325248, 5767168, 253755392, 0), (31511805952, 5767168, 259522560, 0), (31517573120, 5767168, 265289728, 0), (31506038784, 5767168, 271056896, 0), (31529107456, 5767168, 276824064, 0), (31534874624, 5767168, 282591232, 0), (31523340288, 5767168, 288358400, 0), (31546408960, 5767168, 294125568, 0), (31552176128, 5767168, 299892736, 0), (31540641792, 5767168, 305659904, 0), (31581011968, 5767168, 311427072, 0), (31586779136, 5767168, 317194240, 0), (31575244800, 5767168, 322961408, 0)], 2: [(30577524736, 5767168, 0, 0), (30583291904, 5767168, 5767168, 0), (30571757568, 5767168, 11534336, 0), (30612127744, 5767168, 17301504, 0), (30617894912, 5767168, 23068672, 0), (30606360576, 5767168, 28835840, 0), (30767841280, 5767168, 34603008, 0), (30773608448, 5767168, 40370176, 0), (30762074112, 5767168, 46137344, 0), (30819745792, 5767168, 51904512, 0), (30825512960, 5767168, 57671680, 0), (30813978624, 5767168, 63438848, 0), (30854348800, 5767168, 69206016, 0), (30860115968, 5767168, 74973184, 0), (30848581632, 5767168, 80740352, 0), (30906253312, 5767168, 86507520, 0), (30912020480, 5767168, 92274688, 0), (30900486144, 5767168, 98041856, 0), (30940856320, 5767168, 103809024, 0), (30946623488, 5767168, 109576192, 0), (30935089152, 5767168, 115343360, 0), (30992760832, 5767168, 121110528, 0), (30998528000, 5767168, 126877696, 0), (30986993664, 5767168, 132644864, 0), (31027363840, 5767168, 138412032, 0), (31033131008, 5767168, 144179200, 0), (31021596672, 5767168, 149946368, 0), (31113871360, 5767168, 155713536, 0), (31119638528, 5767168, 161480704, 0), (31108104192, 5767168, 167247872, 0), (31148474368, 5767168, 173015040, 0), (31154241536, 5767168, 178782208, 0), (31142707200, 5767168, 184549376, 0), (31234981888, 5767168, 190316544, 0), (31240749056, 5767168, 196083712, 0), (31229214720, 5767168, 201850880, 0), (31252283392, 5767168, 207618048, 0), (31258050560, 5767168, 213385216, 0), (31246516224, 5767168, 219152384, 0), (31304187904, 5767168, 224919552, 0), (31309955072, 5767168, 230686720, 0), (31298420736, 5767168, 236453888, 0), (31321489408, 5767168, 242221056, 0), (31327256576, 5767168, 247988224, 0), (31315722240, 5767168, 253755392, 0), (31338790912, 5767168, 259522560, 0), (31344558080, 5767168, 265289728, 0), (31333023744, 5767168, 271056896, 0), (31407996928, 5767168, 276824064, 0), (31413764096, 5767168, 282591232, 0), (31402229760, 5767168, 288358400, 0), (31442599936, 5767168, 294125568, 0), (31448367104, 5767168, 299892736, 0), (31436832768, 5767168, 305659904, 0), (31459901440, 5767168, 311427072, 0), (31465668608, 5767168, 317194240, 0), (31454134272, 5767168, 322961408, 0), (31477202944, 5767168, 328728576, 0), (31482970112, 5767168, 334495744, 0), (31471435776, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc758d50>, 2: <capsule object NULL at 0x74a6785de1f0>}
DEBUG 01-15 16:09:17.273006.273006 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.273650.273650 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.273260.273260 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 361e0130-2533-49af-8791-0039a47ebce5
DEBUG 01-15 16:09:17.273591.273591 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:17.273638.273638 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.273477.273477 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:17.273187.273187 client.py:127] Model loaded
DEBUG 01-15 16:09:17.274155.274155 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.274213.274213 cuda_h.py:19] end restore2model cost 0.0003299713134765625 seconds
DEBUG 01-15 16:09:17.274122.274122 cuda_h.py:19] end sllm_worker_task cost 0.010699748992919922 seconds
DEBUG 01-15 16:09:17.274394.274394 cuda_h.py:19] end move_flatidxs cost 0.0008320808410644531 seconds
DEBUG 01-15 16:09:17.274217.274217 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:17.275199.275199 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 361e0130-2533-49af-8791-0039a47ebce5
DEBUG 01-15 16:09:17.276538.276538 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004304409027099609 seconds
DEBUG 01-15 16:09:17.276355.276355 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.279845.279845 cuda_h.py:19] end restore2model cost 0.0029256343841552734 seconds
DEBUG 01-15 16:09:17.279966.279966 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0074787139892578125 seconds
DEBUG 01-15 16:09:17.279305.279305 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:17.279057.279057 cuda_h.py:19] end gpu_sexperts cost 0.0002715587615966797 seconds
DEBUG 01-15 16:09:17.279979.279979 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:17.279179.279179 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:09:17.279206.279206 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:17.279240.279240 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.280597.280597 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009353160858154297 seconds
DEBUG 01-15 16:09:17.280852.280852 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.281754.281754 cuda_h.py:19] end gpu_group_list cost 0.00021529197692871094 seconds
DEBUG 01-15 16:09:17.281497.281497 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.282461.282461 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010347366333007812 seconds
DEBUG 01-15 16:09:17.283947.283947 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.283710.283710 cuda_h.py:19] end gpu_group_list cost 0.000217437744140625 seconds
DEBUG 01-15 16:09:17.283210.283210 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:17.284516.284516 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 361e0130-2533-49af-8791-0039a47ebce5
DEBUG 01-15 16:09:17.283662.283662 cuda_h.py:19] end group_tensors cost 0.008713245391845703 seconds
DEBUG 01-15 16:09:17.284589.284589 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:17.287787.287787 cuda_h.py:19] end group pad cost 0.003080606460571289 seconds
DEBUG 01-15 16:09:17.287716.287716 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:17.308180.308180 cuda_h.py:19] end group_einsum cost 0.021114110946655273 seconds
DEBUG 01-15 16:09:17.308868.308868 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:17.311066.311066 cuda_h.py:19] end get_outputs_cpu1 cost 0.0025124549865722656 seconds
DEBUG 01-15 16:09:17.312048.312048 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.038510799407958984 seconds
INFO 01-15 16:09:17.313811.313811 client.py:127] Model loaded
DEBUG 01-15 16:09:17.313723.313723 cuda_h.py:19] end wait_experts_multi_device cost 0.0293123722076416 seconds
DEBUG 01-15 16:09:17.313752.313752 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:17.314584.314584 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0006539821624755859 seconds
DEBUG 01-15 16:09:17.314329.314329 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:17.315695.315695 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.315344.315344 cuda_h.py:19] end index_scatter cost 0.00010418891906738281 seconds
DEBUG 01-15 16:09:17.316715.316715 cuda_h.py:19] end cpuoutputsdeal cost 0.001978158950805664 seconds
DEBUG 01-15 16:09:17.316434.316434 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:17.316212.316212 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.316771.316771 cuda_h.py:19] end gpu_group_tensor cost 0.00021505355834960938 seconds
DEBUG 01-15 16:09:17.316568.316568 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.317993.317993 cuda_h.py:19] end gpu_group_tensor cost 0.00019216537475585938 seconds
DEBUG 01-15 16:09:17.317674.317674 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.317108.317108 cuda_h.py:19] end gpu_group_einsum cost 0.0006306171417236328 seconds
DEBUG 01-15 16:09:17.318677.318677 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.318748.318748 cuda_h.py:19] end gpu_group_einsum cost 0.0006735324859619141 seconds
DEBUG 01-15 16:09:17.319602.319602 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.319667.319667 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.319752.319752 cuda_h.py:19] end all_expert_outputs_slices cost 0.00031065940856933594 seconds
DEBUG 01-15 16:09:17.319350.319350 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.319633.319633 cuda_h.py:19] end concat_expert_out cost 8.320808410644531e-05 seconds
DEBUG 01-15 16:09:17.319961.319961 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.319303.319303 cuda_h.py:19] end index_scatter cost 8.678436279296875e-05 seconds
DEBUG 01-15 16:09:17.320348.320348 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012440681457519531 seconds
DEBUG 01-15 16:09:17.320817.320817 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.320569.320569 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.320440.320440 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023484230041503906 seconds
DEBUG 01-15 16:09:17.320508.320508 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.321552.321552 cuda_h.py:19] end concat_expert_out cost 8.678436279296875e-05 seconds
DEBUG 01-15 16:09:17.321211.321211 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.321169.321169 cuda_h.py:19] end index_scatter cost 8.392333984375e-05 seconds
DEBUG 01-15 16:09:17.321728.321728 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008187294006347656 seconds
DEBUG 01-15 16:09:17.321784.321784 cuda_h.py:19] end gpu_experts_multi_device cost 0.04176211357116699 seconds
DEBUG 01-15 16:09:17.321862.321862 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.053116559982299805 seconds
DEBUG 01-15 16:09:17.322098.322098 cuda_h.py:19] end prefill_layer cost 0.058867692947387695 seconds
DEBUG 01-15 16:09:17.322352.322352 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:09:17.322652.322652 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:17.322097.322097 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:09:17.322112.322112 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:17.322738.322738 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:17.322702.322702 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:17.325018.325018 cuda_h.py:19] end self_attn cost 0.002460479736328125 seconds
DEBUG 01-15 16:09:17.325697.325697 cuda_h.py:19] end iln_self_attn_paln cost 0.0031943321228027344 seconds
DEBUG 01-15 16:09:17.325288.325288 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:09:17.325428.325428 cuda_h.py:10] start gate
DEBUG 01-15 16:09:17.326171.326171 cuda_h.py:19] end gate cost 0.000583648681640625 seconds
DEBUG 01-15 16:09:17.326954.326954 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:17.326879.326879 lmp.py:1912] 
DEBUG 01-15 16:09:17.326879.326879 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:17.326066.326066 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:17.326146.326146 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:17.326127.326127 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:17.326962.326962 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:17.326843.326843 lmp.py:1917] 
DEBUG 01-15 16:09:17.326843.326843 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:17.326963.326963 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:17.326328.326328 lmp.py:1935]   Expert 18 |     66 | CPU
DEBUG 01-15 16:09:17.326686.326686 lmp.py:1935]   Expert 47 |     70 | CPU
DEBUG 01-15 16:09:17.326329.326329 lmp.py:1935]   Expert 54 |     70 | CPU
DEBUG 01-15 16:09:17.326495.326495 lmp.py:1935]   Expert 23 |     73 | CPU
DEBUG 01-15 16:09:17.326900.326900 lmp.py:1935]   Expert 48 |     80 | CPU
DEBUG 01-15 16:09:17.326828.326828 lmp.py:1935]   Expert 45 |     84 | CPU
DEBUG 01-15 16:09:17.326994.326994 lmp.py:1935]   Expert 44 |     86 | CPU
DEBUG 01-15 16:09:17.326352.326352 lmp.py:1935]   Expert 20 |     92 | CPU
DEBUG 01-15 16:09:17.326472.326472 lmp.py:1935]   Expert 31 |     96 | CPU
DEBUG 01-15 16:09:17.327876.327876 lmp.py:1935]   Expert 36 |    105 | CPU
DEBUG 01-15 16:09:17.327281.327281 lmp.py:1935]   Expert 61 |    114 | CPU
DEBUG 01-15 16:09:17.327447.327447 lmp.py:1935]   Expert 33 |    120 | CPU
DEBUG 01-15 16:09:17.327375.327375 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:09:17.327302.327302 lmp.py:1935]   Expert 42 |    121 | CPU
DEBUG 01-15 16:09:17.327992.327992 lmp.py:1935]   Expert 11 |    124 | CPU
DEBUG 01-15 16:09:17.327681.327681 lmp.py:1935]   Expert 24 |    124 | CPU
DEBUG 01-15 16:09:17.327370.327370 lmp.py:1935]   Expert 43 |    125 | CPU
DEBUG 01-15 16:09:17.327060.327060 lmp.py:1935]   Expert 49 |    128 | CPU
DEBUG 01-15 16:09:17.327226.327226 lmp.py:1935]   Expert 56 |    131 | CPU
DEBUG 01-15 16:09:17.327153.327153 lmp.py:1935]   Expert  6 |    134 | CPU
DEBUG 01-15 16:09:17.327081.327081 lmp.py:1935]   Expert 51 |    143 | CPU
DEBUG 01-15 16:09:17.327247.327247 lmp.py:1935]   Expert 17 |    148 | CPU
DEBUG 01-15 16:09:17.327413.327413 lmp.py:1935]   Expert  0 |    150 | CPU
DEBUG 01-15 16:09:17.327341.327341 lmp.py:1935]   Expert  5 |    154 | CPU
DEBUG 01-15 16:09:17.327223.327223 lmp.py:1935]   Expert 12 |    156 | CPU
DEBUG 01-15 16:09:17.327534.327534 lmp.py:1935]   Expert 40 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327846.327846 lmp.py:1935]   Expert 57 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327681.327681 lmp.py:1935]   Expert 55 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327801.327801 lmp.py:1935]   Expert 59 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327683.327683 lmp.py:1935]   Expert 26 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327087.327087 lmp.py:1935]   Expert 38 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327730.327730 lmp.py:1935]   Expert 13 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327612.327612 lmp.py:1935]   Expert 46 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327016.327016 lmp.py:1935]   Expert 35 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327136.327136 lmp.py:1935]   Expert 50 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327541.327541 lmp.py:1935]   Expert 58 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327660.327660 lmp.py:1935]   Expert 30 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327495.327495 lmp.py:1935]   Expert  7 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327615.327615 lmp.py:1935]   Expert 16 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327735.327735 lmp.py:1935]   Expert 15 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327378.327378 lmp.py:1935]   Expert 32 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327021.327021 lmp.py:1935]   Expert 14 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327902.327902 lmp.py:1935]   Expert  1 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327545.327545 lmp.py:1935]   Expert  3 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327188.327188 lmp.py:1935]   Expert  4 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327831.327831 lmp.py:1935]   Expert 34 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327474.327474 lmp.py:1935]   Expert 39 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327879.327879 lmp.py:1935]   Expert 28 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327283.327283 lmp.py:1935]   Expert 52 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327926.327926 lmp.py:1935]   Expert 25 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327569.327569 lmp.py:1935]   Expert 22 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327451.327451 lmp.py:1935]   Expert  2 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327809.327809 lmp.py:1935]   Expert 21 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327167.327167 lmp.py:1935]   Expert 41 |    280 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327002.327002 lmp.py:1935]   Expert 60 |    284 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327884.327884 lmp.py:1935]   Expert 63 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327765.327765 lmp.py:1935]   Expert 29 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327170.327170 lmp.py:1935]   Expert 62 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327813.327813 lmp.py:1935]   Expert 27 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327217.327217 lmp.py:1935]   Expert 37 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327860.327860 lmp.py:1935]   Expert 53 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327026.327026 lmp.py:1935]   Expert  8 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327431.327431 lmp.py:1935]   Expert 19 |    445 | GPU2(cuda:2)
DEBUG 01-15 16:09:17.327074.327074 lmp.py:1935]   Expert  9 |    611 | GPU1(cuda:1)
DEBUG 01-15 16:09:17.327240.327240 lmp.py:1937] 
DEBUG 01-15 16:09:17.327240.327240 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:17.328121.328121 lmp.py:1938]   CPU:   2815 tokens
DEBUG 01-15 16:09:17.328718.328718 lmp.py:1942]   cuda:1:   4683 tokens (19 experts)
DEBUG 01-15 16:09:17.328361.328361 lmp.py:1942]   cuda:2:   4790 tokens (20 experts)
DEBUG 01-15 16:09:17.328289.328289 lmp.py:1943]   Total GPU:   9473 tokens
DEBUG 01-15 16:09:17.328740.328740 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:17.328740.328740 lmp.py:1944] 
DEBUG 01-15 16:09:17.328151.328151 cuda_h.py:19] end experts_map_get cost 0.0016951560974121094 seconds
DEBUG 01-15 16:09:17.328901.328901 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:17.328180.328180 lmp.py:1953] 
DEBUG 01-15 16:09:17.328180.328180 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:17.328970.328970 cuda_h.py:19] end cpu_experts_submit cost 5.459785461425781e-05 seconds
DEBUG 01-15 16:09:17.328905.328905 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:17.328695.328695 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:17.329607.329607 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.1.gate_proj.weight': 0, 'model.layers.27.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.27.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.27.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.27.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.38.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.38.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.38.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.46.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.46.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.46.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.60.up_proj.weight': 322961408}, 2: {'model.layers.27.mlp.experts.2.gate_proj.weight': 0, 'model.layers.27.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.28.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.28.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.28.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.37.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.37.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.37.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.40.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.40.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.40.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.27.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.27.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.27.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(31667519488, 5767168, 0, 0), (31673286656, 5767168, 5767168, 0), (31661752320, 5767168, 11534336, 0), (31719424000, 5767168, 17301504, 0), (31725191168, 5767168, 23068672, 0), (31713656832, 5767168, 28835840, 0), (31805931520, 5767168, 34603008, 0), (31811698688, 5767168, 40370176, 0), (31800164352, 5767168, 46137344, 0), (31909740544, 5767168, 51904512, 0), (31915507712, 5767168, 57671680, 0), (31903973376, 5767168, 63438848, 0), (31927042048, 5767168, 69206016, 0), (31932809216, 5767168, 74973184, 0), (31921274880, 5767168, 80740352, 0), (32030851072, 5767168, 86507520, 0), (32036618240, 5767168, 92274688, 0), (32025083904, 5767168, 98041856, 0), (32117358592, 5767168, 103809024, 0), (32123125760, 5767168, 109576192, 0), (32111591424, 5767168, 115343360, 0), (32151961600, 5767168, 121110528, 0), (32157728768, 5767168, 126877696, 0), (32146194432, 5767168, 132644864, 0), (32169263104, 5767168, 138412032, 0), (32175030272, 5767168, 144179200, 0), (32163495936, 5767168, 149946368, 0), (32307675136, 5767168, 155713536, 0), (32313442304, 5767168, 161480704, 0), (32301907968, 5767168, 167247872, 0), (32324976640, 5767168, 173015040, 0), (32330743808, 5767168, 178782208, 0), (32319209472, 5767168, 184549376, 0), (32359579648, 5767168, 190316544, 0), (32365346816, 5767168, 196083712, 0), (32353812480, 5767168, 201850880, 0), (32446087168, 5767168, 207618048, 0), (32451854336, 5767168, 213385216, 0), (32440320000, 5767168, 219152384, 0), (32515293184, 5767168, 224919552, 0), (32521060352, 5767168, 230686720, 0), (32509526016, 5767168, 236453888, 0), (32549896192, 5767168, 242221056, 0), (32555663360, 5767168, 247988224, 0), (32544129024, 5767168, 253755392, 0), (32567197696, 5767168, 259522560, 0), (32572964864, 5767168, 265289728, 0), (32561430528, 5767168, 271056896, 0), (32601800704, 5767168, 276824064, 0), (32607567872, 5767168, 282591232, 0), (32596033536, 5767168, 288358400, 0), (32636403712, 5767168, 294125568, 0), (32642170880, 5767168, 299892736, 0), (32630636544, 5767168, 305659904, 0), (32688308224, 5767168, 311427072, 0), (32694075392, 5767168, 317194240, 0), (32682541056, 5767168, 322961408, 0)], 2: [(31684820992, 5767168, 0, 0), (31690588160, 5767168, 5767168, 0), (31679053824, 5767168, 11534336, 0), (31702122496, 5767168, 17301504, 0), (31707889664, 5767168, 23068672, 0), (31696355328, 5767168, 28835840, 0), (31771328512, 5767168, 34603008, 0), (31777095680, 5767168, 40370176, 0), (31765561344, 5767168, 46137344, 0), (31788630016, 5767168, 51904512, 0), (31794397184, 5767168, 57671680, 0), (31782862848, 5767168, 63438848, 0), (31875137536, 5767168, 69206016, 0), (31880904704, 5767168, 74973184, 0), (31869370368, 5767168, 80740352, 0), (31892439040, 5767168, 86507520, 0), (31898206208, 5767168, 92274688, 0), (31886671872, 5767168, 98041856, 0), (31978946560, 5767168, 103809024, 0), (31984713728, 5767168, 109576192, 0), (31973179392, 5767168, 115343360, 0), (32013549568, 5767168, 121110528, 0), (32019316736, 5767168, 126877696, 0), (32007782400, 5767168, 132644864, 0), (32082755584, 5767168, 138412032, 0), (32088522752, 5767168, 144179200, 0), (32076988416, 5767168, 149946368, 0), (32100057088, 5767168, 155713536, 0), (32105824256, 5767168, 161480704, 0), (32094289920, 5767168, 167247872, 0), (32134660096, 5767168, 173015040, 0), (32140427264, 5767168, 178782208, 0), (32128892928, 5767168, 184549376, 0), (32203866112, 5767168, 190316544, 0), (32209633280, 5767168, 196083712, 0), (32198098944, 5767168, 201850880, 0), (32238469120, 5767168, 207618048, 0), (32244236288, 5767168, 213385216, 0), (32232701952, 5767168, 219152384, 0), (32255770624, 5767168, 224919552, 0), (32261537792, 5767168, 230686720, 0), (32250003456, 5767168, 236453888, 0), (32290373632, 5767168, 242221056, 0), (32296140800, 5767168, 247988224, 0), (32284606464, 5767168, 253755392, 0), (32342278144, 5767168, 259522560, 0), (32348045312, 5767168, 265289728, 0), (32336510976, 5767168, 271056896, 0), (32653705216, 5767168, 276824064, 0), (32659472384, 5767168, 282591232, 0), (32647938048, 5767168, 288358400, 0), (32671006720, 5767168, 294125568, 0), (32676773888, 5767168, 299892736, 0), (32665239552, 5767168, 305659904, 0), (32722911232, 5767168, 311427072, 0), (32728678400, 5767168, 317194240, 0), (32717144064, 5767168, 322961408, 0), (32740212736, 5767168, 328728576, 0), (32745979904, 5767168, 334495744, 0), (32734445568, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc702100>, 2: <capsule object NULL at 0x74aa046863a0>}
DEBUG 01-15 16:09:17.329911.329911 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:17.329608.329608 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:17.329172.329172 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4958de09-57f2-4d74-8da7-886e89fdf743
DEBUG 01-15 16:09:17.329990.329990 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:17.329352.329352 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:17.329923.329923 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:17.330933.330933 cuda_h.py:19] end move_flatidxs cost 0.0008378028869628906 seconds
DEBUG 01-15 16:09:17.330040.330040 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:17.332809.332809 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4958de09-57f2-4d74-8da7-886e89fdf743
DEBUG 01-15 16:09:17.333043.333043 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004824399948120117 seconds
DEBUG 01-15 16:09:17.333383.333383 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:17.336777.336777 cuda_h.py:19] end restore2model cost 0.0028209686279296875 seconds
DEBUG 01-15 16:09:17.336421.336421 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00787210464477539 seconds
DEBUG 01-15 16:09:17.336568.336568 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:17.335809.335809 cuda_h.py:19] end group_tensors cost 0.004918098449707031 seconds
DEBUG 01-15 16:09:17.336707.336707 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:17.336341.336341 cuda_h.py:19] end gpu_sexperts cost 0.00031638145446777344 seconds
DEBUG 01-15 16:09:17.336601.336601 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:17.336014.336014 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.337718.337718 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012178421020507812 seconds
DEBUG 01-15 16:09:17.337052.337052 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.338007.338007 cuda_h.py:19] end gpu_group_list cost 0.0002186298370361328 seconds
DEBUG 01-15 16:09:17.339349.339349 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:17.340280.340280 cuda_h.py:19] end group pad cost 0.003599405288696289 seconds
DEBUG 01-15 16:09:17.340162.340162 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:17.340995.340995 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012080669403076172 seconds
DEBUG 01-15 16:09:17.340613.340613 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:17.341398.341398 cuda_h.py:19] end gpu_group_list cost 0.0004131793975830078 seconds
DEBUG 01-15 16:09:17.342412.342412 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:17.342156.342156 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4958de09-57f2-4d74-8da7-886e89fdf743
DEBUG 01-15 16:09:17.371003.371003 cuda_h.py:19] end group_einsum cost 0.031145811080932617 seconds
DEBUG 01-15 16:09:17.371246.371246 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:17.375243.375243 cuda_h.py:19] end get_outputs_cpu1 cost 0.0035991668701171875 seconds
DEBUG 01-15 16:09:17.375741.375741 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04626655578613281 seconds
INFO 01-15 16:09:17.376047.376047 client.py:127] Model loaded
DEBUG 01-15 16:09:17.376351.376351 cuda_h.py:19] end wait_experts_multi_device cost 0.034120798110961914 seconds
DEBUG 01-15 16:09:17.377154.377154 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:17.377153.377153 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0006988048553466797 seconds
DEBUG 01-15 16:09:17.377859.377859 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:17.379823.379823 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.379804.379804 cuda_h.py:19] end index_scatter cost 0.00011944770812988281 seconds
DEBUG 01-15 16:09:17.380494.380494 cuda_h.py:19] end cpuoutputsdeal cost 0.0022912025451660156 seconds
DEBUG 01-15 16:09:17.380089.380089 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:17.380807.380807 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.380420.380420 cuda_h.py:19] end gpu_group_tensor cost 0.0002474784851074219 seconds
DEBUG 01-15 16:09:17.380422.380422 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:17.381948.381948 cuda_h.py:19] end gpu_group_tensor cost 0.00022029876708984375 seconds
DEBUG 01-15 16:09:17.381232.381232 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.382628.382628 cuda_h.py:19] end gpu_group_einsum cost 0.000865936279296875 seconds
DEBUG 01-15 16:09:17.382768.382768 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:17.383952.383952 cuda_h.py:19] end gpu_group_einsum cost 0.0006723403930664062 seconds
DEBUG 01-15 16:09:17.383701.383701 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.383038.383038 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.384219.384219 cuda_h.py:19] end all_expert_outputs_slices cost 0.00036716461181640625 seconds
DEBUG 01-15 16:09:17.384983.384983 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.384132.384132 cuda_h.py:19] end concat_expert_out cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:09:17.384049.384049 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.384854.384854 cuda_h.py:19] end index_scatter cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:09:17.384380.384380 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012214183807373047 seconds
DEBUG 01-15 16:09:17.384259.384259 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:17.384983.384983 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:17.385090.385090 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017952919006347656 seconds
DEBUG 01-15 16:09:17.385144.385144 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:17.385101.385101 cuda_h.py:19] end concat_expert_out cost 7.009506225585938e-05 seconds
DEBUG 01-15 16:09:17.385064.385064 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:17.385373.385373 cuda_h.py:19] end index_scatter cost 6.628036499023438e-05 seconds
DEBUG 01-15 16:09:17.385534.385534 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006461143493652344 seconds
DEBUG 01-15 16:09:17.385702.385702 cuda_h.py:19] end gpu_experts_multi_device cost 0.04904675483703613 seconds
DEBUG 01-15 16:09:17.385076.385076 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.060099124908447266 seconds
DEBUG 01-15 16:09:17.386369.386369 cuda_h.py:19] end prefill_layer cost 0.06389236450195312 seconds
DEBUG 01-15 16:09:17.386670.386670 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:09:17.386532.386532 cuda_h.py:19] end prefill cost 1.7966201305389404 seconds
DEBUG 01-15 16:09:19.606426.606426 cuda_h.py:10] start generate_input_ids
generate input ids cost 0.09126162528991699 s
DEBUG 01-15 16:09:19.949762.949762 cuda_h.py:19] end generate_input_ids cost 0.34072065353393555 seconds
DEBUG 01-15 16:09:19.949955.949955 cuda_h.py:10] start init_cache
DEBUG 01-15 16:09:19.949852.949852 cuda_h.py:19] end init_cache cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:09:22.288157.288157 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:09:22.289876.289876 cuda_h.py:19] end init_meta_layer cost 1.1682510375976562e-05 seconds
DEBUG 01-15 16:09:22.289435.289435 cuda_h.py:10] start init_weights
DEBUG 01-15 16:09:22.289628.289628 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:22.289053.289053 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:22.290799.290799 cuda_h.py:19] end allocate_cuda_memory cost 0.0009090900421142578 seconds
DEBUG 01-15 16:09:22.290503.290503 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:22.290498.290498 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:22.291837.291837 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:22.291440.291440 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 60d3efeb-ffe4-4473-8abe-6f38e04be35c
DEBUG 01-15 16:09:22.291926.291926 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:22.293862.293862 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 60d3efeb-ffe4-4473-8abe-6f38e04be35c
DEBUG 01-15 16:09:22.293992.293992 cuda_h.py:19] end load_into_gpu_async cost 0.0022797584533691406 seconds
DEBUG 01-15 16:09:22.293895.293895 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:22.293795.293795 cuda_h.py:19] end restore_tensors2 cost 9.703636169433594e-05 seconds
DEBUG 01-15 16:09:22.293095.293095 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0036478042602539062 seconds
DEBUG 01-15 16:09:22.293422.293422 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:22.294682.294682 cuda_h.py:19] end restore2model cost 0.00037980079650878906 seconds
INFO 01-15 16:09:22.294630.294630 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 60d3efeb-ffe4-4473-8abe-6f38e04be35c
INFO 01-15 16:09:22.372045.372045 client.py:127] Model loaded
DEBUG 01-15 16:09:22.372110.372110 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:09:22.372372.372372 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:22.372431.372431 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:22.372759.372759 cuda_h.py:19] end allocate_cuda_memory cost 0.0003933906555175781 seconds
DEBUG 01-15 16:09:22.373711.373711 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:22.373058.373058 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:22.373948.373948 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:22.373851.373851 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4ad19d18-d03e-44d7-bec0-01203620fc40
DEBUG 01-15 16:09:22.373122.373122 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:22.375863.375863 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4ad19d18-d03e-44d7-bec0-01203620fc40
DEBUG 01-15 16:09:22.375597.375597 cuda_h.py:19] end load_into_gpu_async cost 0.0020394325256347656 seconds
DEBUG 01-15 16:09:22.375328.375328 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:22.375474.375474 cuda_h.py:19] end restore_tensors2 cost 0.00013589859008789062 seconds
DEBUG 01-15 16:09:22.375073.375073 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0032472610473632812 seconds
INFO 01-15 16:09:22.375791.375791 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4ad19d18-d03e-44d7-bec0-01203620fc40
INFO 01-15 16:09:22.392128.392128 client.py:127] Model loaded
DEBUG 01-15 16:09:22.392172.392172 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:22.393998.393998 cuda_h.py:19] end restore2model cost 0.0008296966552734375 seconds
DEBUG 01-15 16:09:22.393698.393698 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.02102375030517578 seconds
DEBUG 01-15 16:09:22.393336.393336 cuda_h.py:19] end init_weights cost 0.10343384742736816 seconds
DEBUG 01-15 16:09:22.393524.393524 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:09:23.170907.170907 cuda_h.py:19] end copy_emodel cost 0.7767744064331055 seconds
DEBUG 01-15 16:09:23.171774.171774 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:09:23.171525.171525 cuda_h.py:19] end init_inputs_tokens cost 0.0002315044403076172 seconds
DEBUG 01-15 16:09:23.171056.171056 cuda_h.py:10] start prefill
DEBUG 01-15 16:09:23.171719.171719 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.171892.171892 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:09:23.171920.171920 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:09:23.171092.171092 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:09:23.171028.171028 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 3.24249267578125e-05 seconds
DEBUG 01-15 16:09:23.171606.171606 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 7.796287536621094e-05 seconds
DEBUG 01-15 16:09:23.171010.171010 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.171323.171323 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.171432.171432 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.171272.171272 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.171976.171976 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.172103.172103 cuda_h.py:19] end allocate_cuda_memory cost 0.0001926422119140625 seconds
DEBUG 01-15 16:09:23.172325.172325 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.172094.172094 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.172349.172349 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.172588.172588 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9fc1d1e3-074a-4d12-af5e-f93bcfc6232e
DEBUG 01-15 16:09:23.172214.172214 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.172423.172423 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.174887.174887 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9fc1d1e3-074a-4d12-af5e-f93bcfc6232e
DEBUG 01-15 16:09:23.174903.174903 cuda_h.py:19] end load_into_gpu_async cost 0.0022013187408447266 seconds
DEBUG 01-15 16:09:23.174375.174375 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.174438.174438 cuda_h.py:19] end restore_tensors2 cost 8.225440979003906e-05 seconds
DEBUG 01-15 16:09:23.174016.174016 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002790212631225586 seconds
INFO 01-15 16:09:23.174469.174469 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9fc1d1e3-074a-4d12-af5e-f93bcfc6232e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.176871.176871 cuda_h.py:19] end self_attn cost 0.0036351680755615234 seconds
DEBUG 01-15 16:09:23.176987.176987 cuda_h.py:19] end iln_self_attn_paln cost 0.005221843719482422 seconds
DEBUG 01-15 16:09:23.176764.176764 cuda_h.py:10] start dense_mlp
INFO 01-15 16:09:23.183124.183124 client.py:127] Model loaded
DEBUG 01-15 16:09:23.183359.183359 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.184465.184465 cuda_h.py:19] end restore2model cost 0.0005495548248291016 seconds
DEBUG 01-15 16:09:23.184977.184977 cuda_h.py:19] end sllm_worker_task cost 0.01236104965209961 seconds
DEBUG 01-15 16:09:23.184100.184100 cuda_h.py:19] end dense_mlp cost 0.007442951202392578 seconds
DEBUG 01-15 16:09:23.184256.184256 cuda_h.py:19] end prefill_layer cost 0.013045310974121094 seconds
DEBUG 01-15 16:09:23.184158.184158 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:09:23.184616.184616 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.184888.184888 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:09:23.184300.184300 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:09:23.184141.184141 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:09:23.184249.184249 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 2.09808349609375e-05 seconds
DEBUG 01-15 16:09:23.184760.184760 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:09:23.184356.184356 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.184980.184980 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.184684.184684 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.184773.184773 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.185469.185469 cuda_h.py:19] end allocate_cuda_memory cost 0.00018286705017089844 seconds
DEBUG 01-15 16:09:23.185446.185446 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.185004.185004 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.185352.185352 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.185533.185533 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.185416.185416 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, caf13ca8-fa5f-43d5-8841-19cf2bb9fd6d
DEBUG 01-15 16:09:23.185942.185942 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.185838.185838 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.186862.186862 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, caf13ca8-fa5f-43d5-8841-19cf2bb9fd6d
DEBUG 01-15 16:09:23.187461.187461 cuda_h.py:19] end load_into_gpu_async cost 0.0016589164733886719 seconds
DEBUG 01-15 16:09:23.187721.187721 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.187698.187698 cuda_h.py:19] end restore_tensors2 cost 7.700920104980469e-05 seconds
DEBUG 01-15 16:09:23.187018.187018 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002414703369140625 seconds
INFO 01-15 16:09:23.187497.187497 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, caf13ca8-fa5f-43d5-8841-19cf2bb9fd6d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.188177.188177 cuda_h.py:19] end self_attn cost 0.002750873565673828 seconds
DEBUG 01-15 16:09:23.188465.188465 cuda_h.py:19] end iln_self_attn_paln cost 0.0041942596435546875 seconds
DEBUG 01-15 16:09:23.188148.188148 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:09:23.189858.189858 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.189379.189379 cuda_h.py:19] end gate cost 0.0007040500640869141 seconds
DEBUG 01-15 16:09:23.189686.189686 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.190915.190915 lmp.py:1912] 
DEBUG 01-15 16:09:23.190915.190915 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.190161.190161 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.190764.190764 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.190123.190123 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.190097.190097 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.190309.190309 lmp.py:1917] 
DEBUG 01-15 16:09:23.190309.190309 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.190760.190760 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.190363.190363 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:09:23.190006.190006 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:09:23.190318.190318 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:09:23.190531.190531 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:09:23.190028.190028 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:09:23.190002.190002 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:09:23.190499.190499 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:09:23.190996.190996 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:09:23.190255.190255 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:09:23.190991.190991 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:09:23.190727.190727 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:09:23.190462.190462 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:09:23.190959.190959 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:09:23.190218.190218 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:09:23.190715.190715 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:09:23.190974.190974 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:09:23.190233.190233 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:09:23.190253.190253 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:09:23.190512.190512 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:09:23.190963.190963 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:09:23.190652.190652 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:09:23.190342.190342 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:09:23.190269.190269 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:09:23.190767.190767 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:09:23.190025.190025 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:09:23.190430.190430 lmp.py:1935]   Expert 34 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.190119.190119 lmp.py:1935]   Expert 35 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190808.190808 lmp.py:1935]   Expert 36 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.190259.190259 lmp.py:1935]   Expert 11 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.190710.190710 lmp.py:1935]   Expert 17 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190161.190161 lmp.py:1935]   Expert 59 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.190374.190374 lmp.py:1935]   Expert 10 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190162.190162 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190759.190759 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.190594.190594 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190191.190191 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.190595.190595 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190761.190761 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.190928.190928 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190094.190094 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.190260.190260 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.190664.190664 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190830.190830 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190758.190758 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.190924.190924 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191806.191806 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191164.191164 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191999.191999 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.191788.191788 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191669.191669 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.191835.191835 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.191478.191478 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191406.191406 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.191049.191049 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191215.191215 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.191143.191143 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191309.191309 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.191475.191475 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.191641.191641 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191284.191284 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191166.191166 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.191524.191524 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191644.191644 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.191287.191287 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.191022.191022 lmp.py:1937] 
DEBUG 01-15 16:09:23.191022.191022 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.191188.191188 lmp.py:1938]   CPU:   2872 tokens
DEBUG 01-15 16:09:23.191355.191355 lmp.py:1942]   cuda:1:   4628 tokens (19 experts)
DEBUG 01-15 16:09:23.191282.191282 lmp.py:1942]   cuda:2:   4788 tokens (20 experts)
DEBUG 01-15 16:09:23.191495.191495 lmp.py:1943]   Total GPU:   9416 tokens
DEBUG 01-15 16:09:23.191230.191230 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.191230.191230 lmp.py:1944] 
DEBUG 01-15 16:09:23.191403.191403 cuda_h.py:19] end experts_map_get cost 0.0016257762908935547 seconds
DEBUG 01-15 16:09:23.191452.191452 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.191493.191493 lmp.py:1953] 
DEBUG 01-15 16:09:23.191493.191493 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.191561.191561 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:23.191873.191873 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.191133.191133 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.193536.193536 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.39.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.39.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.39.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.1.mlp.experts.2.gate_proj.weight': 0, 'model.layers.1.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.1.mlp.experts.46.gate_proj.weight': 276824064, 'model.layers.1.mlp.experts.46.down_proj.weight': 282591232, 'model.layers.1.mlp.experts.46.up_proj.weight': 288358400, 'model.layers.1.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.1.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.1.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.1.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.1.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.1.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.1.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.1.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.1.mlp.experts.59.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3016228864, 5767168, 34603008, 0), (3021996032, 5767168, 40370176, 0), (3010461696, 5767168, 46137344, 0), (3033530368, 5767168, 51904512, 0), (3039297536, 5767168, 57671680, 0), (3027763200, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3154640896, 5767168, 86507520, 0), (3160408064, 5767168, 92274688, 0), (3148873728, 5767168, 98041856, 0), (3189243904, 5767168, 103809024, 0), (3195011072, 5767168, 109576192, 0), (3183476736, 5767168, 115343360, 0), (3206545408, 5767168, 121110528, 0), (3212312576, 5767168, 126877696, 0), (3200778240, 5767168, 132644864, 0), (3275751424, 5767168, 138412032, 0), (3281518592, 5767168, 144179200, 0), (3269984256, 5767168, 149946368, 0), (3310354432, 5767168, 155713536, 0), (3316121600, 5767168, 161480704, 0), (3304587264, 5767168, 167247872, 0), (3362258944, 5767168, 173015040, 0), (3368026112, 5767168, 178782208, 0), (3356491776, 5767168, 184549376, 0), (3379560448, 5767168, 190316544, 0), (3385327616, 5767168, 196083712, 0), (3373793280, 5767168, 201850880, 0), (3466067968, 5767168, 207618048, 0), (3471835136, 5767168, 213385216, 0), (3460300800, 5767168, 219152384, 0), (3535273984, 5767168, 224919552, 0), (3541041152, 5767168, 230686720, 0), (3529506816, 5767168, 236453888, 0), (3690987520, 5767168, 242221056, 0), (3696754688, 5767168, 247988224, 0), (3685220352, 5767168, 253755392, 0), (3725590528, 5767168, 259522560, 0), (3731357696, 5767168, 265289728, 0), (3719823360, 5767168, 271056896, 0), (3846701056, 5767168, 276824064, 0), (3852468224, 5767168, 282591232, 0), (3840933888, 5767168, 288358400, 0), (3915907072, 5767168, 294125568, 0), (3921674240, 5767168, 299892736, 0), (3910139904, 5767168, 305659904, 0), (3950510080, 5767168, 311427072, 0), (3956277248, 5767168, 317194240, 0), (3944742912, 5767168, 322961408, 0)], 2: [(2895118336, 5767168, 0, 0), (2900885504, 5767168, 5767168, 0), (2889351168, 5767168, 11534336, 0), (2929721344, 5767168, 17301504, 0), (2935488512, 5767168, 23068672, 0), (2923954176, 5767168, 28835840, 0), (2947022848, 5767168, 34603008, 0), (2952790016, 5767168, 40370176, 0), (2941255680, 5767168, 46137344, 0), (2998927360, 5767168, 51904512, 0), (3004694528, 5767168, 57671680, 0), (2993160192, 5767168, 63438848, 0), (3050831872, 5767168, 69206016, 0), (3056599040, 5767168, 74973184, 0), (3045064704, 5767168, 80740352, 0), (3102736384, 5767168, 86507520, 0), (3108503552, 5767168, 92274688, 0), (3096969216, 5767168, 98041856, 0), (3120037888, 5767168, 103809024, 0), (3125805056, 5767168, 109576192, 0), (3114270720, 5767168, 115343360, 0), (3137339392, 5767168, 121110528, 0), (3143106560, 5767168, 126877696, 0), (3131572224, 5767168, 132644864, 0), (3223846912, 5767168, 138412032, 0), (3229614080, 5767168, 144179200, 0), (3218079744, 5767168, 149946368, 0), (3258449920, 5767168, 155713536, 0), (3264217088, 5767168, 161480704, 0), (3252682752, 5767168, 167247872, 0), (3431464960, 5767168, 173015040, 0), (3437232128, 5767168, 178782208, 0), (3425697792, 5767168, 184549376, 0), (3448766464, 5767168, 190316544, 0), (3454533632, 5767168, 196083712, 0), (3442999296, 5767168, 201850880, 0), (3483369472, 5767168, 207618048, 0), (3489136640, 5767168, 213385216, 0), (3477602304, 5767168, 219152384, 0), (3552575488, 5767168, 224919552, 0), (3558342656, 5767168, 230686720, 0), (3546808320, 5767168, 236453888, 0), (3587178496, 5767168, 242221056, 0), (3592945664, 5767168, 247988224, 0), (3581411328, 5767168, 253755392, 0), (3639083008, 5767168, 259522560, 0), (3644850176, 5767168, 265289728, 0), (3633315840, 5767168, 271056896, 0), (3656384512, 5767168, 276824064, 0), (3662151680, 5767168, 282591232, 0), (3650617344, 5767168, 288358400, 0), (3777495040, 5767168, 294125568, 0), (3783262208, 5767168, 299892736, 0), (3771727872, 5767168, 305659904, 0), (3829399552, 5767168, 311427072, 0), (3835166720, 5767168, 317194240, 0), (3823632384, 5767168, 322961408, 0), (3881304064, 5767168, 328728576, 0), (3887071232, 5767168, 334495744, 0), (3875536896, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc5edd10>, 2: <capsule object NULL at 0x74a6bc5ed170>}
DEBUG 01-15 16:09:23.193414.193414 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.194930.194930 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.194309.194309 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6864771f-de12-4c44-9892-8b6680c62488
DEBUG 01-15 16:09:23.194546.194546 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:23.195836.195836 client.py:127] Model loaded
DEBUG 01-15 16:09:23.195610.195610 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.195836.195836 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:23.195621.195621 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.196293.196293 cuda_h.py:19] end restore2model cost 0.0007441043853759766 seconds
DEBUG 01-15 16:09:23.196654.196654 cuda_h.py:19] end sllm_worker_task cost 0.011351346969604492 seconds
DEBUG 01-15 16:09:23.196758.196758 cuda_h.py:19] end move_flatidxs cost 0.0010318756103515625 seconds
DEBUG 01-15 16:09:23.196886.196886 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:23.197183.197183 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6864771f-de12-4c44-9892-8b6680c62488
DEBUG 01-15 16:09:23.197073.197073 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.006105661392211914 seconds
DEBUG 01-15 16:09:23.197996.197996 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.201149.201149 cuda_h.py:19] end restore2model cost 0.003130674362182617 seconds
DEBUG 01-15 16:09:23.201906.201906 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.009484291076660156 seconds
DEBUG 01-15 16:09:23.201986.201986 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.201486.201486 cuda_h.py:19] end gpu_sexperts cost 0.00026869773864746094 seconds
DEBUG 01-15 16:09:23.201362.201362 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.201847.201847 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4543533325195312e-05 seconds
DEBUG 01-15 16:09:23.201113.201113 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.201623.201623 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.202086.202086 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009415149688720703 seconds
DEBUG 01-15 16:09:23.202221.202221 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.202864.202864 cuda_h.py:19] end gpu_group_list cost 0.0002002716064453125 seconds
DEBUG 01-15 16:09:23.204215.204215 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.205218.205218 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016264915466308594 seconds
DEBUG 01-15 16:09:23.205248.205248 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.206866.206866 cuda_h.py:19] end gpu_group_list cost 0.00021529197692871094 seconds
DEBUG 01-15 16:09:23.205863.205863 cuda_h.py:19] end group_tensors cost 0.00856781005859375 seconds
DEBUG 01-15 16:09:23.206510.206510 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.206114.206114 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.206898.206898 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6864771f-de12-4c44-9892-8b6680c62488
DEBUG 01-15 16:09:23.211536.211536 cuda_h.py:19] end group pad cost 0.004263162612915039 seconds
DEBUG 01-15 16:09:23.211087.211087 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:23.236364.236364 client.py:127] Model loaded
DEBUG 01-15 16:09:23.236576.236576 cuda_h.py:19] end wait_experts_multi_device cost 0.030109167098999023 seconds
DEBUG 01-15 16:09:23.236504.236504 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.246762.246762 cuda_h.py:19] end group_einsum cost 0.03509330749511719 seconds
DEBUG 01-15 16:09:23.246211.246211 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.252222.252222 cuda_h.py:19] end get_outputs_cpu1 cost 0.0055999755859375 seconds
DEBUG 01-15 16:09:23.253209.253209 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05769801139831543 seconds
DEBUG 01-15 16:09:23.253186.253186 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.016724348068237305 seconds
DEBUG 01-15 16:09:23.253607.253607 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.254469.254469 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.254276.254276 cuda_h.py:19] end index_scatter cost 9.72747802734375e-05 seconds
DEBUG 01-15 16:09:23.255793.255793 cuda_h.py:19] end cpuoutputsdeal cost 0.001363515853881836 seconds
DEBUG 01-15 16:09:23.255563.255563 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.255558.255558 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.256844.256844 cuda_h.py:19] end gpu_group_tensor cost 0.0005977153778076172 seconds
DEBUG 01-15 16:09:23.256389.256389 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.256925.256925 cuda_h.py:19] end gpu_group_tensor cost 0.0005700588226318359 seconds
DEBUG 01-15 16:09:23.256082.256082 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.257194.257194 cuda_h.py:19] end gpu_group_einsum cost 0.0007376670837402344 seconds
DEBUG 01-15 16:09:23.257396.257396 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.258558.258558 cuda_h.py:19] end gpu_group_einsum cost 0.0010187625885009766 seconds
DEBUG 01-15 16:09:23.258608.258608 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.259999.259999 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.259854.259854 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001609325408935547 seconds
DEBUG 01-15 16:09:23.259848.259848 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.259577.259577 cuda_h.py:19] end concat_expert_out cost 0.0001544952392578125 seconds
DEBUG 01-15 16:09:23.259030.259030 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.259563.259563 cuda_h.py:19] end index_scatter cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:09:23.259855.259855 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010139942169189453 seconds
DEBUG 01-15 16:09:23.260924.260924 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.260668.260668 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.260296.260296 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001201629638671875 seconds
DEBUG 01-15 16:09:23.260283.260283 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.260292.260292 cuda_h.py:19] end concat_expert_out cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:09:23.260660.260660 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.260822.260822 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:23.260008.260008 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004858970642089844 seconds
DEBUG 01-15 16:09:23.260288.260288 cuda_h.py:19] end gpu_experts_multi_device cost 0.05902743339538574 seconds
DEBUG 01-15 16:09:23.260145.260145 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.07166934013366699 seconds
DEBUG 01-15 16:09:23.261501.261501 cuda_h.py:19] end prefill_layer cost 0.07643008232116699 seconds
DEBUG 01-15 16:09:23.261635.261635 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:09:23.261000.261000 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.261080.261080 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:09:23.261638.261638 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:09:23.261864.261864 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:09:23.261376.261376 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 3.5762786865234375e-05 seconds
DEBUG 01-15 16:09:23.261238.261238 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:09:23.261033.261033 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.261048.261048 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.261341.261341 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.261217.261217 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.261601.261601 cuda_h.py:19] end allocate_cuda_memory cost 0.00018072128295898438 seconds
DEBUG 01-15 16:09:23.261835.261835 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.261883.261883 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.261328.261328 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.261177.261177 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e096d2d1-4fe0-4f0d-ad58-8a17da9c78e8
DEBUG 01-15 16:09:23.261067.261067 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.262192.262192 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.262666.262666 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.263056.263056 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e096d2d1-4fe0-4f0d-ad58-8a17da9c78e8
DEBUG 01-15 16:09:23.263906.263906 cuda_h.py:19] end load_into_gpu_async cost 0.0016663074493408203 seconds
DEBUG 01-15 16:09:23.263046.263046 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.263082.263082 cuda_h.py:19] end restore_tensors2 cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:09:23.263037.263037 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021696090698242188 seconds
INFO 01-15 16:09:23.263847.263847 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e096d2d1-4fe0-4f0d-ad58-8a17da9c78e8
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.265788.265788 cuda_h.py:19] end self_attn cost 0.00298309326171875 seconds
DEBUG 01-15 16:09:23.265089.265089 cuda_h.py:19] end iln_self_attn_paln cost 0.00439143180847168 seconds
DEBUG 01-15 16:09:23.265488.265488 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:09:23.265436.265436 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.266870.266870 cuda_h.py:19] end gate cost 0.0006728172302246094 seconds
DEBUG 01-15 16:09:23.266607.266607 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.266002.266002 lmp.py:1912] 
DEBUG 01-15 16:09:23.266002.266002 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.266235.266235 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.266169.266169 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.266766.266766 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.266217.266217 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.266953.266953 lmp.py:1917] 
DEBUG 01-15 16:09:23.266953.266953 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.267741.267741 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.267597.267597 lmp.py:1935]   Expert 58 |     50 | CPU
DEBUG 01-15 16:09:23.267293.267293 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:09:23.267320.267320 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:09:23.267539.267539 lmp.py:1935]   Expert 17 |     84 | CPU
DEBUG 01-15 16:09:23.267805.267805 lmp.py:1935]   Expert 24 |     86 | CPU
DEBUG 01-15 16:09:23.267309.267309 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:09:23.267336.267336 lmp.py:1935]   Expert 28 |    106 | CPU
DEBUG 01-15 16:09:23.267648.267648 lmp.py:1935]   Expert 34 |    116 | CPU
DEBUG 01-15 16:09:23.267198.267198 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:09:23.267987.267987 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:09:23.267776.267776 lmp.py:1935]   Expert  9 |    129 | CPU
DEBUG 01-15 16:09:23.267565.267565 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:09:23.267069.267069 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:09:23.267573.267573 lmp.py:1935]   Expert 23 |    135 | CPU
DEBUG 01-15 16:09:23.267077.267077 lmp.py:1935]   Expert 26 |    137 | CPU
DEBUG 01-15 16:09:23.267581.267581 lmp.py:1935]   Expert 30 |    144 | CPU
DEBUG 01-15 16:09:23.267608.267608 lmp.py:1935]   Expert 45 |    145 | CPU
DEBUG 01-15 16:09:23.267920.267920 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:09:23.267708.267708 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:09:23.267735.267735 lmp.py:1935]   Expert  1 |    152 | CPU
DEBUG 01-15 16:09:23.267047.267047 lmp.py:1935]   Expert 36 |    156 | CPU
DEBUG 01-15 16:09:23.267836.267836 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:09:23.267340.267340 lmp.py:1935]   Expert 29 |    161 | CPU
DEBUG 01-15 16:09:23.267606.267606 lmp.py:1935]   Expert 25 |    164 | CPU
DEBUG 01-15 16:09:23.267110.267110 lmp.py:1935]   Expert 54 |    167 | CPU
DEBUG 01-15 16:09:23.267521.267521 lmp.py:1935]   Expert  6 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267217.267217 lmp.py:1935]   Expert 49 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267198.267198 lmp.py:1935]   Expert 48 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267656.267656 lmp.py:1935]   Expert 12 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267636.267636 lmp.py:1935]   Expert 35 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267617.267617 lmp.py:1935]   Expert 37 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267121.267121 lmp.py:1935]   Expert 60 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267864.267864 lmp.py:1935]   Expert 13 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267321.267321 lmp.py:1935]   Expert 53 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267779.267779 lmp.py:1935]   Expert 33 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267998.267998 lmp.py:1935]   Expert 10 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267979.267979 lmp.py:1935]   Expert 16 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267483.267483 lmp.py:1935]   Expert 21 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267987.267987 lmp.py:1935]   Expert 40 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267491.267491 lmp.py:1935]   Expert 43 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267995.267995 lmp.py:1935]   Expert 38 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267499.267499 lmp.py:1935]   Expert  5 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267003.267003 lmp.py:1935]   Expert 44 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267030.267030 lmp.py:1935]   Expert 52 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267587.267587 lmp.py:1935]   Expert 19 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.267045.267045 lmp.py:1935]   Expert 41 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.267264.267264 lmp.py:1935]   Expert 50 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.268484.268484 lmp.py:1935]   Expert  4 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.268749.268749 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.268253.268253 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.268519.268519 lmp.py:1935]   Expert 31 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.268023.268023 lmp.py:1935]   Expert 56 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.268765.268765 lmp.py:1935]   Expert 20 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.268269.268269 lmp.py:1935]   Expert 39 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.268773.268773 lmp.py:1935]   Expert 22 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.268992.268992 lmp.py:1935]   Expert  2 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.268212.268212 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.268431.268431 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.268889.268889 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.268869.268869 lmp.py:1935]   Expert 18 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.268897.268897 lmp.py:1935]   Expert 14 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.268401.268401 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.268905.268905 lmp.py:1935]   Expert 11 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.268362.268362 lmp.py:1935]   Expert 61 |    460 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.268151.268151 lmp.py:1937] 
DEBUG 01-15 16:09:23.268151.268151 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.268099.268099 lmp.py:1938]   CPU:   3107 tokens
DEBUG 01-15 16:09:23.268749.268749 lmp.py:1942]   cuda:1:   4509 tokens (19 experts)
DEBUG 01-15 16:09:23.268253.268253 lmp.py:1942]   cuda:2:   4672 tokens (20 experts)
DEBUG 01-15 16:09:23.268565.268565 lmp.py:1943]   Total GPU:   9181 tokens
DEBUG 01-15 16:09:23.268115.268115 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.268115.268115 lmp.py:1944] 
DEBUG 01-15 16:09:23.268864.268864 cuda_h.py:19] end experts_map_get cost 0.0019156932830810547 seconds
DEBUG 01-15 16:09:23.268337.268337 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.268000.268000 lmp.py:1953] 
DEBUG 01-15 16:09:23.268000.268000 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.268644.268644 cuda_h.py:19] end cpu_experts_submit cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:23.268433.268433 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.268455.268455 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.270363.270363 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.4.gate_proj.weight': 0, 'model.layers.2.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.14.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.14.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.14.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.20.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.20.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.20.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.31.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.31.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.31.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.33.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.33.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.33.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.40.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.40.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.40.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.41.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.41.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.41.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.59.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.59.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.59.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.60.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.60.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.60.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.2.mlp.experts.2.gate_proj.weight': 0, 'model.layers.2.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.2.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.2.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.2.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.2.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.2.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.2.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.2.mlp.experts.55.gate_proj.weight': 311427072, 'model.layers.2.mlp.experts.55.down_proj.weight': 317194240, 'model.layers.2.mlp.experts.55.up_proj.weight': 322961408, 'model.layers.2.mlp.experts.56.gate_proj.weight': 328728576, 'model.layers.2.mlp.experts.56.down_proj.weight': 334495744, 'model.layers.2.mlp.experts.56.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(4037017600, 5767168, 0, 0), (4042784768, 5767168, 5767168, 0), (4031250432, 5767168, 11534336, 0), (4054319104, 5767168, 17301504, 0), (4060086272, 5767168, 23068672, 0), (4048551936, 5767168, 28835840, 0), (4210032640, 5767168, 34603008, 0), (4215799808, 5767168, 40370176, 0), (4204265472, 5767168, 46137344, 0), (4313841664, 5767168, 51904512, 0), (4319608832, 5767168, 57671680, 0), (4308074496, 5767168, 63438848, 0), (4331143168, 5767168, 69206016, 0), (4336910336, 5767168, 74973184, 0), (4325376000, 5767168, 80740352, 0), (4348444672, 5767168, 86507520, 0), (4354211840, 5767168, 92274688, 0), (4342677504, 5767168, 98041856, 0), (4504158208, 5767168, 103809024, 0), (4509925376, 5767168, 109576192, 0), (4498391040, 5767168, 115343360, 0), (4538761216, 5767168, 121110528, 0), (4544528384, 5767168, 126877696, 0), (4532994048, 5767168, 132644864, 0), (4573364224, 5767168, 138412032, 0), (4579131392, 5767168, 144179200, 0), (4567597056, 5767168, 149946368, 0), (4659871744, 5767168, 155713536, 0), (4665638912, 5767168, 161480704, 0), (4654104576, 5767168, 167247872, 0), (4677173248, 5767168, 173015040, 0), (4682940416, 5767168, 178782208, 0), (4671406080, 5767168, 184549376, 0), (4694474752, 5767168, 190316544, 0), (4700241920, 5767168, 196083712, 0), (4688707584, 5767168, 201850880, 0), (4729077760, 5767168, 207618048, 0), (4734844928, 5767168, 213385216, 0), (4723310592, 5767168, 219152384, 0), (4815585280, 5767168, 224919552, 0), (4821352448, 5767168, 230686720, 0), (4809818112, 5767168, 236453888, 0), (4884791296, 5767168, 242221056, 0), (4890558464, 5767168, 247988224, 0), (4879024128, 5767168, 253755392, 0), (4988600320, 5767168, 259522560, 0), (4994367488, 5767168, 265289728, 0), (4982833152, 5767168, 271056896, 0), (5005901824, 5767168, 276824064, 0), (5011668992, 5767168, 282591232, 0), (5000134656, 5767168, 288358400, 0), (5023203328, 5767168, 294125568, 0), (5028970496, 5767168, 299892736, 0), (5017436160, 5767168, 305659904, 0), (5057806336, 5767168, 311427072, 0), (5063573504, 5767168, 317194240, 0), (5052039168, 5767168, 322961408, 0)], 2: [(4002414592, 5767168, 0, 0), (4008181760, 5767168, 5767168, 0), (3996647424, 5767168, 11534336, 0), (4071620608, 5767168, 17301504, 0), (4077387776, 5767168, 23068672, 0), (4065853440, 5767168, 28835840, 0), (4140826624, 5767168, 34603008, 0), (4146593792, 5767168, 40370176, 0), (4135059456, 5767168, 46137344, 0), (4158128128, 5767168, 51904512, 0), (4163895296, 5767168, 57671680, 0), (4152360960, 5767168, 63438848, 0), (4175429632, 5767168, 69206016, 0), (4181196800, 5767168, 74973184, 0), (4169662464, 5767168, 80740352, 0), (4192731136, 5767168, 86507520, 0), (4198498304, 5767168, 92274688, 0), (4186963968, 5767168, 98041856, 0), (4244635648, 5767168, 103809024, 0), (4250402816, 5767168, 109576192, 0), (4238868480, 5767168, 115343360, 0), (4279238656, 5767168, 121110528, 0), (4285005824, 5767168, 126877696, 0), (4273471488, 5767168, 132644864, 0), (4296540160, 5767168, 138412032, 0), (4302307328, 5767168, 144179200, 0), (4290772992, 5767168, 149946368, 0), (4607967232, 5767168, 155713536, 0), (4613734400, 5767168, 161480704, 0), (4602200064, 5767168, 167247872, 0), (4625268736, 5767168, 173015040, 0), (4631035904, 5767168, 178782208, 0), (4619501568, 5767168, 184549376, 0), (4642570240, 5767168, 190316544, 0), (4648337408, 5767168, 196083712, 0), (4636803072, 5767168, 201850880, 0), (4711776256, 5767168, 207618048, 0), (4717543424, 5767168, 213385216, 0), (4706009088, 5767168, 219152384, 0), (4763680768, 5767168, 224919552, 0), (4769447936, 5767168, 230686720, 0), (4757913600, 5767168, 236453888, 0), (4780982272, 5767168, 242221056, 0), (4786749440, 5767168, 247988224, 0), (4775215104, 5767168, 253755392, 0), (4798283776, 5767168, 259522560, 0), (4804050944, 5767168, 265289728, 0), (4792516608, 5767168, 271056896, 0), (4832886784, 5767168, 276824064, 0), (4838653952, 5767168, 282591232, 0), (4827119616, 5767168, 288358400, 0), (4867489792, 5767168, 294125568, 0), (4873256960, 5767168, 299892736, 0), (4861722624, 5767168, 305659904, 0), (4919394304, 5767168, 311427072, 0), (4925161472, 5767168, 317194240, 0), (4913627136, 5767168, 322961408, 0), (4936695808, 5767168, 328728576, 0), (4942462976, 5767168, 334495744, 0), (4930928640, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc335b60>, 2: <capsule object NULL at 0x74a6bc5ed4a0>}
DEBUG 01-15 16:09:23.270380.270380 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.270839.270839 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.270833.270833 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, fde06276-8463-4fe3-a814-94baa7151261
DEBUG 01-15 16:09:23.271549.271549 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:23.271484.271484 client.py:127] Model loaded
DEBUG 01-15 16:09:23.271168.271168 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.271238.271238 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:23.272439.272439 cuda_h.py:19] end restore2model cost 0.0003421306610107422 seconds
INFO 01-15 16:09:23.272216.272216 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, fde06276-8463-4fe3-a814-94baa7151261
DEBUG 01-15 16:09:23.272254.272254 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.272476.272476 cuda_h.py:19] end sllm_worker_task cost 0.010772943496704102 seconds
DEBUG 01-15 16:09:23.272865.272865 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003924131393432617 seconds
DEBUG 01-15 16:09:23.272215.272215 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.273092.273092 cuda_h.py:19] end move_flatidxs cost 0.0008380413055419922 seconds
DEBUG 01-15 16:09:23.273438.273438 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:23.276214.276214 cuda_h.py:19] end restore2model cost 0.0031244754791259766 seconds
DEBUG 01-15 16:09:23.276826.276826 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007361173629760742 seconds
DEBUG 01-15 16:09:23.276642.276642 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.276667.276667 cuda_h.py:19] end gpu_sexperts cost 0.0003325939178466797 seconds
DEBUG 01-15 16:09:23.276543.276543 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.276651.276651 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:09:23.276632.276632 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.276858.276858 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.277757.277757 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009136199951171875 seconds
DEBUG 01-15 16:09:23.277553.277553 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.277694.277694 cuda_h.py:19] end gpu_group_list cost 0.0002162456512451172 seconds
DEBUG 01-15 16:09:23.278065.278065 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.278975.278975 cuda_h.py:19] end group_tensors cost 0.0051364898681640625 seconds
DEBUG 01-15 16:09:23.279137.279137 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.279124.279124 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011086463928222656 seconds
DEBUG 01-15 16:09:23.279603.279603 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.280215.280215 cuda_h.py:19] end gpu_group_list cost 0.00024509429931640625 seconds
DEBUG 01-15 16:09:23.280002.280002 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.280700.280700 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, fde06276-8463-4fe3-a814-94baa7151261
DEBUG 01-15 16:09:23.283567.283567 cuda_h.py:19] end group pad cost 0.004084587097167969 seconds
DEBUG 01-15 16:09:23.283973.283973 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:23.315402.315402 client.py:127] Model loaded
DEBUG 01-15 16:09:23.316344.316344 cuda_h.py:19] end wait_experts_multi_device cost 0.03502774238586426 seconds
DEBUG 01-15 16:09:23.316334.316334 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.317303.317303 cuda_h.py:19] end group_einsum cost 0.03368330001831055 seconds
DEBUG 01-15 16:09:23.317519.317519 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.320328.320328 cuda_h.py:19] end get_outputs_cpu1 cost 0.0033528804779052734 seconds
DEBUG 01-15 16:09:23.321993.321993 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04919719696044922 seconds
DEBUG 01-15 16:09:23.321243.321243 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005491733551025391 seconds
DEBUG 01-15 16:09:23.321140.321140 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.322906.322906 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.322375.322375 cuda_h.py:19] end index_scatter cost 8.368492126464844e-05 seconds
DEBUG 01-15 16:09:23.323536.323536 cuda_h.py:19] end cpuoutputsdeal cost 0.0013079643249511719 seconds
DEBUG 01-15 16:09:23.323320.323320 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.323792.323792 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.323552.323552 cuda_h.py:19] end gpu_group_tensor cost 0.00014400482177734375 seconds
DEBUG 01-15 16:09:23.323169.323169 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.323718.323718 cuda_h.py:19] end gpu_group_tensor cost 0.00013303756713867188 seconds
DEBUG 01-15 16:09:23.323960.323960 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.325999.325999 cuda_h.py:19] end gpu_group_einsum cost 0.0012974739074707031 seconds
DEBUG 01-15 16:09:23.325728.325728 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.325485.325485 cuda_h.py:19] end gpu_group_einsum cost 0.00039768218994140625 seconds
DEBUG 01-15 16:09:23.325880.325880 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.326453.326453 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.326110.326110 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002014636993408203 seconds
DEBUG 01-15 16:09:23.326535.326535 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.326617.326617 cuda_h.py:19] end concat_expert_out cost 6.103515625e-05 seconds
DEBUG 01-15 16:09:23.326759.326759 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.326663.326663 cuda_h.py:19] end index_scatter cost 7.104873657226562e-05 seconds
DEBUG 01-15 16:09:23.326711.326711 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008451938629150391 seconds
DEBUG 01-15 16:09:23.326224.326224 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.326551.326551 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.327703.327703 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015091896057128906 seconds
DEBUG 01-15 16:09:23.327551.327551 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.327282.327282 cuda_h.py:19] end concat_expert_out cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:09:23.327410.327410 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.327241.327241 cuda_h.py:19] end index_scatter cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:23.327096.327096 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005028247833251953 seconds
DEBUG 01-15 16:09:23.327742.327742 cuda_h.py:19] end gpu_experts_multi_device cost 0.05093717575073242 seconds
DEBUG 01-15 16:09:23.327367.327367 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.061820268630981445 seconds
DEBUG 01-15 16:09:23.328946.328946 cuda_h.py:19] end prefill_layer cost 0.06699681282043457 seconds
DEBUG 01-15 16:09:23.328419.328419 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:09:23.328599.328599 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.328540.328540 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:09:23.328435.328435 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:09:23.328714.328714 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:09:23.328902.328902 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 4.00543212890625e-05 seconds
DEBUG 01-15 16:09:23.328963.328963 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 8.749961853027344e-05 seconds
DEBUG 01-15 16:09:23.328282.328282 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.328688.328688 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.328754.328754 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.328565.328565 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.329863.329863 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.329622.329622 cuda_h.py:19] end allocate_cuda_memory cost 0.00037741661071777344 seconds
DEBUG 01-15 16:09:23.329840.329840 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.329989.329989 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.329312.329312 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.330931.330931 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 58f49ae2-95c1-4b36-9869-14f433f827f2
DEBUG 01-15 16:09:23.330362.330362 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.330392.330392 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.331128.331128 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 58f49ae2-95c1-4b36-9869-14f433f827f2
DEBUG 01-15 16:09:23.331477.331477 cuda_h.py:19] end load_into_gpu_async cost 0.0019006729125976562 seconds
DEBUG 01-15 16:09:23.331838.331838 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.332826.332826 cuda_h.py:19] end restore_tensors2 cost 0.0001468658447265625 seconds
DEBUG 01-15 16:09:23.332253.332253 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0031690597534179688 seconds
INFO 01-15 16:09:23.332152.332152 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 58f49ae2-95c1-4b36-9869-14f433f827f2
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.334061.334061 cuda_h.py:19] end self_attn cost 0.004353523254394531 seconds
DEBUG 01-15 16:09:23.335700.335700 cuda_h.py:19] end iln_self_attn_paln cost 0.006734609603881836 seconds
DEBUG 01-15 16:09:23.335622.335622 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:09:23.335617.335617 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.335844.335844 cuda_h.py:19] end gate cost 0.0006279945373535156 seconds
DEBUG 01-15 16:09:23.335674.335674 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.336962.336962 lmp.py:1912] 
DEBUG 01-15 16:09:23.336962.336962 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.336003.336003 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.336891.336891 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.336171.336171 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.336450.336450 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.336139.336139 lmp.py:1917] 
DEBUG 01-15 16:09:23.336139.336139 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.336828.336828 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.336193.336193 lmp.py:1935]   Expert  1 |     50 | CPU
DEBUG 01-15 16:09:23.336598.336598 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:09:23.336049.336049 lmp.py:1935]   Expert  7 |     77 | CPU
DEBUG 01-15 16:09:23.336261.336261 lmp.py:1935]   Expert 48 |     81 | CPU
DEBUG 01-15 16:09:23.336474.336474 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:09:23.336448.336448 lmp.py:1935]   Expert 30 |    109 | CPU
DEBUG 01-15 16:09:23.336660.336660 lmp.py:1935]   Expert 61 |    116 | CPU
DEBUG 01-15 16:09:23.336157.336157 lmp.py:1935]   Expert 45 |    118 | CPU
DEBUG 01-15 16:09:23.336370.336370 lmp.py:1935]   Expert 18 |    119 | CPU
DEBUG 01-15 16:09:23.336867.336867 lmp.py:1935]   Expert 32 |    119 | CPU
DEBUG 01-15 16:09:23.336364.336364 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:09:23.336100.336100 lmp.py:1935]   Expert 39 |    136 | CPU
DEBUG 01-15 16:09:23.336836.336836 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:09:23.336571.336571 lmp.py:1935]   Expert 36 |    138 | CPU
DEBUG 01-15 16:09:23.336545.336545 lmp.py:1935]   Expert 11 |    140 | CPU
DEBUG 01-15 16:09:23.336758.336758 lmp.py:1935]   Expert 59 |    142 | CPU
DEBUG 01-15 16:09:23.336209.336209 lmp.py:1935]   Expert  5 |    143 | CPU
DEBUG 01-15 16:09:23.336183.336183 lmp.py:1935]   Expert  6 |    144 | CPU
DEBUG 01-15 16:09:23.336633.336633 lmp.py:1935]   Expert 51 |    144 | CPU
DEBUG 01-15 16:09:23.336131.336131 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:09:23.336105.336105 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:09:23.336363.336363 lmp.py:1935]   Expert 23 |    156 | CPU
DEBUG 01-15 16:09:23.336861.336861 lmp.py:1935]   Expert  9 |    159 | CPU
DEBUG 01-15 16:09:23.336358.336358 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:09:23.336855.336855 lmp.py:1935]   Expert 40 |    168 | CPU
DEBUG 01-15 16:09:23.336021.336021 lmp.py:1935]   Expert 52 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.336572.336572 lmp.py:1935]   Expert 56 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.336168.336168 lmp.py:1935]   Expert 16 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.336288.336288 lmp.py:1935]   Expert 35 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.336123.336123 lmp.py:1935]   Expert  4 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.336197.336197 lmp.py:1935]   Expert 37 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.336316.336316 lmp.py:1935]   Expert 13 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337390.337390 lmp.py:1935]   Expert 42 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337033.337033 lmp.py:1935]   Expert 17 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337153.337153 lmp.py:1935]   Expert 38 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337749.337749 lmp.py:1935]   Expert 62 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337631.337631 lmp.py:1935]   Expert 21 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337751.337751 lmp.py:1935]   Expert  3 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337394.337394 lmp.py:1935]   Expert 44 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337275.337275 lmp.py:1935]   Expert 60 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337156.337156 lmp.py:1935]   Expert 28 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337753.337753 lmp.py:1935]   Expert 58 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337873.337873 lmp.py:1935]   Expert 47 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337469.337469 lmp.py:1935]   Expert 10 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337828.337828 lmp.py:1935]   Expert 53 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337709.337709 lmp.py:1935]   Expert 55 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337829.337829 lmp.py:1935]   Expert 20 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337710.337710 lmp.py:1935]   Expert 57 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337592.337592 lmp.py:1935]   Expert 33 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337473.337473 lmp.py:1935]   Expert 31 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337639.337639 lmp.py:1935]   Expert 46 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337997.337997 lmp.py:1935]   Expert  8 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337640.337640 lmp.py:1935]   Expert 19 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337760.337760 lmp.py:1935]   Expert 24 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337165.337165 lmp.py:1935]   Expert 14 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337285.337285 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337643.337643 lmp.py:1935]   Expert 29 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337001.337001 lmp.py:1935]   Expert 12 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337359.337359 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337956.337956 lmp.py:1935]   Expert  0 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337599.337599 lmp.py:1935]   Expert 43 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337242.337242 lmp.py:1935]   Expert 54 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337646.337646 lmp.py:1935]   Expert 41 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.337289.337289 lmp.py:1935]   Expert 25 |    411 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.337979.337979 lmp.py:1937] 
DEBUG 01-15 16:09:23.337979.337979 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.337622.337622 lmp.py:1938]   CPU:   3165 tokens
DEBUG 01-15 16:09:23.337265.337265 lmp.py:1942]   cuda:1:   4481 tokens (19 experts)
DEBUG 01-15 16:09:23.337669.337669 lmp.py:1942]   cuda:2:   4642 tokens (20 experts)
DEBUG 01-15 16:09:23.337312.337312 lmp.py:1943]   Total GPU:   9123 tokens
DEBUG 01-15 16:09:23.337717.337717 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.337717.337717 lmp.py:1944] 
DEBUG 01-15 16:09:23.337843.337843 cuda_h.py:19] end experts_map_get cost 0.0016779899597167969 seconds
DEBUG 01-15 16:09:23.337024.337024 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.337919.337919 lmp.py:1953] 
DEBUG 01-15 16:09:23.337919.337919 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.337226.337226 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:23.337491.337491 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.337420.337420 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.338702.338702 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.58.up_proj.weight': 322961408}, 2: {'model.layers.3.mlp.experts.8.gate_proj.weight': 0, 'model.layers.3.mlp.experts.8.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.8.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.22.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.22.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.22.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.54.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.54.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.54.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.56.up_proj.weight': 271056896, 'model.layers.3.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.3.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.3.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.3.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.3.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.3.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.3.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.3.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.3.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.3.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.3.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.3.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5127012352, 5767168, 17301504, 0), (5132779520, 5767168, 23068672, 0), (5121245184, 5767168, 28835840, 0), (5144313856, 5767168, 34603008, 0), (5150081024, 5767168, 40370176, 0), (5138546688, 5767168, 46137344, 0), (5300027392, 5767168, 51904512, 0), (5305794560, 5767168, 57671680, 0), (5294260224, 5767168, 63438848, 0), (5317328896, 5767168, 69206016, 0), (5323096064, 5767168, 74973184, 0), (5311561728, 5767168, 80740352, 0), (5369233408, 5767168, 86507520, 0), (5375000576, 5767168, 92274688, 0), (5363466240, 5767168, 98041856, 0), (5403836416, 5767168, 103809024, 0), (5409603584, 5767168, 109576192, 0), (5398069248, 5767168, 115343360, 0), (5421137920, 5767168, 121110528, 0), (5426905088, 5767168, 126877696, 0), (5415370752, 5767168, 132644864, 0), (5507645440, 5767168, 138412032, 0), (5513412608, 5767168, 144179200, 0), (5501878272, 5767168, 149946368, 0), (5576851456, 5767168, 155713536, 0), (5582618624, 5767168, 161480704, 0), (5571084288, 5767168, 167247872, 0), (5611454464, 5767168, 173015040, 0), (5617221632, 5767168, 178782208, 0), (5605687296, 5767168, 184549376, 0), (5801771008, 5767168, 190316544, 0), (5807538176, 5767168, 196083712, 0), (5796003840, 5767168, 201850880, 0), (5819072512, 5767168, 207618048, 0), (5824839680, 5767168, 213385216, 0), (5813305344, 5767168, 219152384, 0), (5836374016, 5767168, 224919552, 0), (5842141184, 5767168, 230686720, 0), (5830606848, 5767168, 236453888, 0), (5870977024, 5767168, 242221056, 0), (5876744192, 5767168, 247988224, 0), (5865209856, 5767168, 253755392, 0), (5888278528, 5767168, 259522560, 0), (5894045696, 5767168, 265289728, 0), (5882511360, 5767168, 271056896, 0), (5974786048, 5767168, 276824064, 0), (5980553216, 5767168, 282591232, 0), (5969018880, 5767168, 288358400, 0), (5992087552, 5767168, 294125568, 0), (5997854720, 5767168, 299892736, 0), (5986320384, 5767168, 305659904, 0), (6078595072, 5767168, 311427072, 0), (6084362240, 5767168, 317194240, 0), (6072827904, 5767168, 322961408, 0)], 2: [(5213519872, 5767168, 0, 0), (5219287040, 5767168, 5767168, 0), (5207752704, 5767168, 11534336, 0), (5248122880, 5767168, 17301504, 0), (5253890048, 5767168, 23068672, 0), (5242355712, 5767168, 28835840, 0), (5282725888, 5767168, 34603008, 0), (5288493056, 5767168, 40370176, 0), (5276958720, 5767168, 46137344, 0), (5351931904, 5767168, 51904512, 0), (5357699072, 5767168, 57671680, 0), (5346164736, 5767168, 63438848, 0), (5438439424, 5767168, 69206016, 0), (5444206592, 5767168, 74973184, 0), (5432672256, 5767168, 80740352, 0), (5455740928, 5767168, 86507520, 0), (5461508096, 5767168, 92274688, 0), (5449973760, 5767168, 98041856, 0), (5490343936, 5767168, 103809024, 0), (5496111104, 5767168, 109576192, 0), (5484576768, 5767168, 115343360, 0), (5559549952, 5767168, 121110528, 0), (5565317120, 5767168, 126877696, 0), (5553782784, 5767168, 132644864, 0), (5646057472, 5767168, 138412032, 0), (5651824640, 5767168, 144179200, 0), (5640290304, 5767168, 149946368, 0), (5680660480, 5767168, 155713536, 0), (5686427648, 5767168, 161480704, 0), (5674893312, 5767168, 167247872, 0), (5715263488, 5767168, 173015040, 0), (5721030656, 5767168, 178782208, 0), (5709496320, 5767168, 184549376, 0), (5732564992, 5767168, 190316544, 0), (5738332160, 5767168, 196083712, 0), (5726797824, 5767168, 201850880, 0), (5784469504, 5767168, 207618048, 0), (5790236672, 5767168, 213385216, 0), (5778702336, 5767168, 219152384, 0), (6009389056, 5767168, 224919552, 0), (6015156224, 5767168, 230686720, 0), (6003621888, 5767168, 236453888, 0), (6026690560, 5767168, 242221056, 0), (6032457728, 5767168, 247988224, 0), (6020923392, 5767168, 253755392, 0), (6043992064, 5767168, 259522560, 0), (6049759232, 5767168, 265289728, 0), (6038224896, 5767168, 271056896, 0), (6061293568, 5767168, 276824064, 0), (6067060736, 5767168, 282591232, 0), (6055526400, 5767168, 288358400, 0), (6113198080, 5767168, 294125568, 0), (6118965248, 5767168, 299892736, 0), (6107430912, 5767168, 305659904, 0), (6147801088, 5767168, 311427072, 0), (6153568256, 5767168, 317194240, 0), (6142033920, 5767168, 322961408, 0), (6165102592, 5767168, 328728576, 0), (6170869760, 5767168, 334495744, 0), (6159335424, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a660701ec0>, 2: <capsule object NULL at 0x74a6bc5ed5c0>}
DEBUG 01-15 16:09:23.338747.338747 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.339795.339795 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.339405.339405 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 47fada07-569b-4aed-8f91-6940d9db72ea
DEBUG 01-15 16:09:23.339267.339267 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.339516.339516 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:23.339416.339416 client.py:127] Model loaded
DEBUG 01-15 16:09:23.339791.339791 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.339724.339724 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.340590.340590 cuda_h.py:19] end move_flatidxs cost 0.0008454322814941406 seconds
DEBUG 01-15 16:09:23.340333.340333 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:23.340886.340886 cuda_h.py:19] end restore2model cost 0.0009772777557373047 seconds
DEBUG 01-15 16:09:23.340361.340361 cuda_h.py:19] end sllm_worker_task cost 0.01211690902709961 seconds
INFO 01-15 16:09:23.341894.341894 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 47fada07-569b-4aed-8f91-6940d9db72ea
DEBUG 01-15 16:09:23.341623.341623 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036630630493164062 seconds
DEBUG 01-15 16:09:23.341720.341720 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.344278.344278 cuda_h.py:19] end restore2model cost 0.003185749053955078 seconds
DEBUG 01-15 16:09:23.345187.345187 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0071277618408203125 seconds
DEBUG 01-15 16:09:23.345506.345506 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.345582.345582 cuda_h.py:19] end gpu_sexperts cost 0.0002715587615966797 seconds
DEBUG 01-15 16:09:23.345313.345313 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.345089.345089 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8835067749023438e-05 seconds
DEBUG 01-15 16:09:23.345831.345831 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.345342.345342 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.346063.346063 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009236335754394531 seconds
DEBUG 01-15 16:09:23.346144.346144 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.346662.346662 cuda_h.py:19] end gpu_group_list cost 0.00021314620971679688 seconds
DEBUG 01-15 16:09:23.346455.346455 cuda_h.py:19] end group_tensors cost 0.0057675838470458984 seconds
DEBUG 01-15 16:09:23.347997.347997 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.347966.347966 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.349053.349053 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0017180442810058594 seconds
DEBUG 01-15 16:09:23.349184.349184 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.350442.350442 cuda_h.py:19] end gpu_group_list cost 0.00035643577575683594 seconds
DEBUG 01-15 16:09:23.351571.351571 cuda_h.py:19] end group pad cost 0.003844022750854492 seconds
DEBUG 01-15 16:09:23.351982.351982 cuda_h.py:10] start wait_experts_multi_device
DEBUG 01-15 16:09:23.351308.351308 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:23.351971.351971 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 47fada07-569b-4aed-8f91-6940d9db72ea
DEBUG 01-15 16:09:23.383212.383212 cuda_h.py:19] end group_einsum cost 0.03246951103210449 seconds
DEBUG 01-15 16:09:23.383780.383780 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:23.383958.383958 client.py:127] Model loaded
DEBUG 01-15 16:09:23.383473.383473 cuda_h.py:19] end wait_experts_multi_device cost 0.03276991844177246 seconds
DEBUG 01-15 16:09:23.384759.384759 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.387068.387068 cuda_h.py:19] end get_outputs_cpu1 cost 0.0034224987030029297 seconds
DEBUG 01-15 16:09:23.388641.388641 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04876708984375 seconds
DEBUG 01-15 16:09:23.389958.389958 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00508880615234375 seconds
DEBUG 01-15 16:09:23.389487.389487 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.391348.391348 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.391915.391915 cuda_h.py:19] end index_scatter cost 0.0001590251922607422 seconds
DEBUG 01-15 16:09:23.392858.392858 cuda_h.py:19] end cpuoutputsdeal cost 0.0028743743896484375 seconds
DEBUG 01-15 16:09:23.392507.392507 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.392438.392438 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.393921.393921 cuda_h.py:19] end gpu_group_tensor cost 0.0003104209899902344 seconds
DEBUG 01-15 16:09:23.393089.393089 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.393453.393453 cuda_h.py:19] end gpu_group_tensor cost 0.0002930164337158203 seconds
DEBUG 01-15 16:09:23.393997.393997 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.394276.394276 cuda_h.py:19] end gpu_group_einsum cost 0.0011081695556640625 seconds
DEBUG 01-15 16:09:23.395708.395708 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.395533.395533 cuda_h.py:19] end gpu_group_einsum cost 0.00043511390686035156 seconds
DEBUG 01-15 16:09:23.395696.395696 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.395561.395561 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.396933.396933 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002002716064453125 seconds
DEBUG 01-15 16:09:23.396643.396643 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.396441.396441 cuda_h.py:19] end concat_expert_out cost 6.270408630371094e-05 seconds
DEBUG 01-15 16:09:23.396152.396152 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.396606.396606 cuda_h.py:19] end index_scatter cost 9.012222290039062e-05 seconds
DEBUG 01-15 16:09:23.396535.396535 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008831024169921875 seconds
DEBUG 01-15 16:09:23.396956.396956 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.396567.396567 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.397865.397865 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015211105346679688 seconds
DEBUG 01-15 16:09:23.397429.397429 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.397975.397975 cuda_h.py:19] end concat_expert_out cost 5.626678466796875e-05 seconds
DEBUG 01-15 16:09:23.397864.397864 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.397172.397172 cuda_h.py:19] end index_scatter cost 5.340576171875e-05 seconds
DEBUG 01-15 16:09:23.397027.397027 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005087852478027344 seconds
DEBUG 01-15 16:09:23.397765.397765 cuda_h.py:19] end gpu_experts_multi_device cost 0.052000999450683594 seconds
DEBUG 01-15 16:09:23.397059.397059 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.06226778030395508 seconds
DEBUG 01-15 16:09:23.398546.398546 cuda_h.py:19] end prefill_layer cost 0.06980228424072266 seconds
DEBUG 01-15 16:09:23.398171.398171 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:09:23.398351.398351 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.398723.398723 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:09:23.398571.398571 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:09:23.398043.398043 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:09:23.398846.398846 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:09:23.398933.398933 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 6.937980651855469e-05 seconds
DEBUG 01-15 16:09:23.398391.398391 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.398335.398335 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.398884.398884 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.398840.398840 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.398563.398563 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.399689.399689 cuda_h.py:19] end allocate_cuda_memory cost 0.0005025863647460938 seconds
DEBUG 01-15 16:09:23.399510.399510 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.399488.399488 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.399803.399803 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.400594.400594 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 83e30090-a5ef-458a-bb43-319493c4c4af
DEBUG 01-15 16:09:23.400581.400581 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.400054.400054 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.401866.401866 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 83e30090-a5ef-458a-bb43-319493c4c4af
DEBUG 01-15 16:09:23.401157.401157 cuda_h.py:19] end load_into_gpu_async cost 0.0017886161804199219 seconds
DEBUG 01-15 16:09:23.401664.401664 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.401307.401307 cuda_h.py:19] end restore_tensors2 cost 0.00014901161193847656 seconds
DEBUG 01-15 16:09:23.402734.402734 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003186464309692383 seconds
INFO 01-15 16:09:23.402063.402063 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 83e30090-a5ef-458a-bb43-319493c4c4af
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.404120.404120 cuda_h.py:19] end self_attn cost 0.003657817840576172 seconds
DEBUG 01-15 16:09:23.404210.404210 cuda_h.py:19] end iln_self_attn_paln cost 0.006224870681762695 seconds
DEBUG 01-15 16:09:23.404562.404562 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:09:23.404656.404656 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.405705.405705 cuda_h.py:19] end gate cost 0.00063323974609375 seconds
DEBUG 01-15 16:09:23.405488.405488 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.405731.405731 lmp.py:1912] 
DEBUG 01-15 16:09:23.405731.405731 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.405725.405725 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.405421.405421 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.405256.405256 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.405138.405138 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.405655.405655 lmp.py:1917] 
DEBUG 01-15 16:09:23.405655.405655 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.405490.405490 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.405617.405617 lmp.py:1935]   Expert 14 |     65 | CPU
DEBUG 01-15 16:09:23.405783.405783 lmp.py:1935]   Expert 57 |     72 | CPU
DEBUG 01-15 16:09:23.405995.405995 lmp.py:1935]   Expert 13 |     76 | CPU
DEBUG 01-15 16:09:23.405969.405969 lmp.py:1935]   Expert 26 |     82 | CPU
DEBUG 01-15 16:09:23.405944.405944 lmp.py:1935]   Expert 54 |     90 | CPU
DEBUG 01-15 16:09:23.405394.405394 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:09:23.405561.405561 lmp.py:1935]   Expert 11 |     92 | CPU
DEBUG 01-15 16:09:23.406250.406250 lmp.py:1935]   Expert 45 |     95 | CPU
DEBUG 01-15 16:09:23.406416.406416 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:09:23.406344.406344 lmp.py:1935]   Expert 30 |    107 | CPU
DEBUG 01-15 16:09:23.406841.406841 lmp.py:1935]   Expert 51 |    109 | CPU
DEBUG 01-15 16:09:23.406577.406577 lmp.py:1935]   Expert 10 |    113 | CPU
DEBUG 01-15 16:09:23.406312.406312 lmp.py:1935]   Expert 36 |    113 | CPU
DEBUG 01-15 16:09:23.406048.406048 lmp.py:1935]   Expert 32 |    115 | CPU
DEBUG 01-15 16:09:23.406783.406783 lmp.py:1935]   Expert 20 |    128 | CPU
DEBUG 01-15 16:09:23.406519.406519 lmp.py:1935]   Expert  8 |    134 | CPU
DEBUG 01-15 16:09:23.406016.406016 lmp.py:1935]   Expert 63 |    137 | CPU
DEBUG 01-15 16:09:23.406752.406752 lmp.py:1935]   Expert  4 |    138 | CPU
DEBUG 01-15 16:09:23.406249.406249 lmp.py:1935]   Expert 53 |    140 | CPU
DEBUG 01-15 16:09:23.406746.406746 lmp.py:1935]   Expert 34 |    144 | CPU
DEBUG 01-15 16:09:23.406244.406244 lmp.py:1935]   Expert 61 |    144 | CPU
DEBUG 01-15 16:09:23.406694.406694 lmp.py:1935]   Expert 16 |    147 | CPU
DEBUG 01-15 16:09:23.406668.406668 lmp.py:1935]   Expert 47 |    147 | CPU
DEBUG 01-15 16:09:23.406881.406881 lmp.py:1935]   Expert 60 |    158 | CPU
DEBUG 01-15 16:09:23.406093.406093 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:09:23.406690.406690 lmp.py:1935]   Expert 42 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406856.406856 lmp.py:1935]   Expert 17 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406307.406307 lmp.py:1935]   Expert 29 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406235.406235 lmp.py:1935]   Expert 44 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406686.406686 lmp.py:1935]   Expert  7 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406137.406137 lmp.py:1935]   Expert 27 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406349.406349 lmp.py:1935]   Expert 41 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406800.406800 lmp.py:1935]   Expert 48 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406774.406774 lmp.py:1935]   Expert  9 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406225.406225 lmp.py:1935]   Expert 56 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406868.406868 lmp.py:1935]   Expert  2 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406034.406034 lmp.py:1935]   Expert  3 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406200.406200 lmp.py:1935]   Expert 15 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406366.406366 lmp.py:1935]   Expert 24 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406009.406009 lmp.py:1935]   Expert  0 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406937.406937 lmp.py:1935]   Expert 18 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406626.406626 lmp.py:1935]   Expert 55 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406839.406839 lmp.py:1935]   Expert 40 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406290.406290 lmp.py:1935]   Expert 23 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406740.406740 lmp.py:1935]   Expert 38 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406953.406953 lmp.py:1935]   Expert 22 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406404.406404 lmp.py:1935]   Expert 37 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406855.406855 lmp.py:1935]   Expert  6 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406544.406544 lmp.py:1935]   Expert 46 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406710.406710 lmp.py:1935]   Expert 19 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406115.406115 lmp.py:1935]   Expert 39 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406281.406281 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406208.406208 lmp.py:1935]   Expert 50 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406375.406375 lmp.py:1935]   Expert 12 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406064.406064 lmp.py:1935]   Expert 62 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406992.406992 lmp.py:1935]   Expert 21 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406443.406443 lmp.py:1935]   Expert 35 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406893.406893 lmp.py:1935]   Expert 49 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406344.406344 lmp.py:1935]   Expert 33 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406795.406795 lmp.py:1935]   Expert 52 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406484.406484 lmp.py:1935]   Expert  1 |    349 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406174.406174 lmp.py:1935]   Expert  5 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406386.406386 lmp.py:1935]   Expert 43 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.406129.406129 lmp.py:1935]   Expert 59 |    585 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.406487.406487 lmp.py:1937] 
DEBUG 01-15 16:09:23.406487.406487 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.406084.406084 lmp.py:1938]   CPU:   2897 tokens
DEBUG 01-15 16:09:23.406919.406919 lmp.py:1942]   cuda:1:   4636 tokens (19 experts)
DEBUG 01-15 16:09:23.407992.407992 lmp.py:1942]   cuda:2:   4755 tokens (20 experts)
DEBUG 01-15 16:09:23.407873.407873 lmp.py:1943]   Total GPU:   9391 tokens
DEBUG 01-15 16:09:23.407040.407040 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.407040.407040 lmp.py:1944] 
DEBUG 01-15 16:09:23.407166.407166 cuda_h.py:19] end experts_map_get cost 0.0016303062438964844 seconds
DEBUG 01-15 16:09:23.407870.407870 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.407481.407481 lmp.py:1953] 
DEBUG 01-15 16:09:23.407481.407481 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.407694.407694 cuda_h.py:19] end cpu_experts_submit cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:09:23.407960.407960 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.407889.407889 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.408387.408387 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.27.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.27.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.27.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.29.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.29.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.29.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.38.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.38.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.38.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.4.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.4.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.4.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.4.mlp.experts.52.gate_proj.weight': 294125568, 'model.layers.4.mlp.experts.52.down_proj.weight': 299892736, 'model.layers.4.mlp.experts.52.up_proj.weight': 305659904, 'model.layers.4.mlp.experts.55.gate_proj.weight': 311427072, 'model.layers.4.mlp.experts.55.down_proj.weight': 317194240, 'model.layers.4.mlp.experts.55.up_proj.weight': 322961408, 'model.layers.4.mlp.experts.56.gate_proj.weight': 328728576, 'model.layers.4.mlp.experts.56.down_proj.weight': 334495744, 'model.layers.4.mlp.experts.56.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6217007104, 5767168, 17301504, 0), (6222774272, 5767168, 23068672, 0), (6211239936, 5767168, 28835840, 0), (6303514624, 5767168, 34603008, 0), (6309281792, 5767168, 40370176, 0), (6297747456, 5767168, 46137344, 0), (6338117632, 5767168, 51904512, 0), (6343884800, 5767168, 57671680, 0), (6332350464, 5767168, 63438848, 0), (6476529664, 5767168, 69206016, 0), (6482296832, 5767168, 74973184, 0), (6470762496, 5767168, 80740352, 0), (6493831168, 5767168, 86507520, 0), (6499598336, 5767168, 92274688, 0), (6488064000, 5767168, 98041856, 0), (6580338688, 5767168, 103809024, 0), (6586105856, 5767168, 109576192, 0), (6574571520, 5767168, 115343360, 0), (6597640192, 5767168, 121110528, 0), (6603407360, 5767168, 126877696, 0), (6591873024, 5767168, 132644864, 0), (6753353728, 5767168, 138412032, 0), (6759120896, 5767168, 144179200, 0), (6747586560, 5767168, 149946368, 0), (6787956736, 5767168, 155713536, 0), (6793723904, 5767168, 161480704, 0), (6782189568, 5767168, 167247872, 0), (6822559744, 5767168, 173015040, 0), (6828326912, 5767168, 178782208, 0), (6816792576, 5767168, 184549376, 0), (6857162752, 5767168, 190316544, 0), (6862929920, 5767168, 196083712, 0), (6851395584, 5767168, 201850880, 0), (6874464256, 5767168, 207618048, 0), (6880231424, 5767168, 213385216, 0), (6868697088, 5767168, 219152384, 0), (6943670272, 5767168, 224919552, 0), (6949437440, 5767168, 230686720, 0), (6937903104, 5767168, 236453888, 0), (6978273280, 5767168, 242221056, 0), (6984040448, 5767168, 247988224, 0), (6972506112, 5767168, 253755392, 0), (7012876288, 5767168, 259522560, 0), (7018643456, 5767168, 265289728, 0), (7007109120, 5767168, 271056896, 0), (7047479296, 5767168, 276824064, 0), (7053246464, 5767168, 282591232, 0), (7041712128, 5767168, 288358400, 0), (7203192832, 5767168, 294125568, 0), (7208960000, 5767168, 299892736, 0), (7197425664, 5767168, 305659904, 0), (7255097344, 5767168, 311427072, 0), (7260864512, 5767168, 317194240, 0), (7249330176, 5767168, 322961408, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6234308608, 5767168, 17301504, 0), (6240075776, 5767168, 23068672, 0), (6228541440, 5767168, 28835840, 0), (6268911616, 5767168, 34603008, 0), (6274678784, 5767168, 40370176, 0), (6263144448, 5767168, 46137344, 0), (6286213120, 5767168, 51904512, 0), (6291980288, 5767168, 57671680, 0), (6280445952, 5767168, 63438848, 0), (6390022144, 5767168, 69206016, 0), (6395789312, 5767168, 74973184, 0), (6384254976, 5767168, 80740352, 0), (6441926656, 5767168, 86507520, 0), (6447693824, 5767168, 92274688, 0), (6436159488, 5767168, 98041856, 0), (6511132672, 5767168, 103809024, 0), (6516899840, 5767168, 109576192, 0), (6505365504, 5767168, 115343360, 0), (6545735680, 5767168, 121110528, 0), (6551502848, 5767168, 126877696, 0), (6539968512, 5767168, 132644864, 0), (6563037184, 5767168, 138412032, 0), (6568804352, 5767168, 144179200, 0), (6557270016, 5767168, 149946368, 0), (6614941696, 5767168, 155713536, 0), (6620708864, 5767168, 161480704, 0), (6609174528, 5767168, 167247872, 0), (6649544704, 5767168, 173015040, 0), (6655311872, 5767168, 178782208, 0), (6643777536, 5767168, 184549376, 0), (6684147712, 5767168, 190316544, 0), (6689914880, 5767168, 196083712, 0), (6678380544, 5767168, 201850880, 0), (6839861248, 5767168, 207618048, 0), (6845628416, 5767168, 213385216, 0), (6834094080, 5767168, 219152384, 0), (6891765760, 5767168, 224919552, 0), (6897532928, 5767168, 230686720, 0), (6885998592, 5767168, 236453888, 0), (6909067264, 5767168, 242221056, 0), (6914834432, 5767168, 247988224, 0), (6903300096, 5767168, 253755392, 0), (6926368768, 5767168, 259522560, 0), (6932135936, 5767168, 265289728, 0), (6920601600, 5767168, 271056896, 0), (7030177792, 5767168, 276824064, 0), (7035944960, 5767168, 282591232, 0), (7024410624, 5767168, 288358400, 0), (7082082304, 5767168, 294125568, 0), (7087849472, 5767168, 299892736, 0), (7076315136, 5767168, 305659904, 0), (7133986816, 5767168, 311427072, 0), (7139753984, 5767168, 317194240, 0), (7128219648, 5767168, 322961408, 0), (7151288320, 5767168, 328728576, 0), (7157055488, 5767168, 334495744, 0), (7145521152, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc1b9860>, 2: <capsule object NULL at 0x74a6bc335f50>}
DEBUG 01-15 16:09:23.409480.409480 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.409092.409092 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.409894.409894 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8302d7d7-e58c-4a7d-bf0f-d617a592f644
DEBUG 01-15 16:09:23.409901.409901 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.409176.409176 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:23.409769.409769 client.py:127] Model loaded
DEBUG 01-15 16:09:23.409833.409833 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.409829.409829 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.410382.410382 cuda_h.py:19] end move_flatidxs cost 0.0008404254913330078 seconds
DEBUG 01-15 16:09:23.410695.410695 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:23.410723.410723 cuda_h.py:19] end restore2model cost 0.0009894371032714844 seconds
INFO 01-15 16:09:23.410231.410231 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8302d7d7-e58c-4a7d-bf0f-d617a592f644
DEBUG 01-15 16:09:23.411790.411790 cuda_h.py:19] end sllm_worker_task cost 0.012368440628051758 seconds
DEBUG 01-15 16:09:23.411248.411248 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00435638427734375 seconds
DEBUG 01-15 16:09:23.411771.411771 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.415968.415968 cuda_h.py:19] end restore2model cost 0.003092527389526367 seconds
DEBUG 01-15 16:09:23.415686.415686 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007817268371582031 seconds
DEBUG 01-15 16:09:23.415481.415481 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.415458.415458 cuda_h.py:19] end gpu_sexperts cost 0.0002677440643310547 seconds
DEBUG 01-15 16:09:23.415857.415857 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.415011.415011 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5020370483398438e-05 seconds
DEBUG 01-15 16:09:23.415753.415753 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.415979.415979 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.416237.416237 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009315013885498047 seconds
DEBUG 01-15 16:09:23.416226.416226 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.416445.416445 cuda_h.py:19] end gpu_group_list cost 0.0002040863037109375 seconds
DEBUG 01-15 16:09:23.416905.416905 cuda_h.py:19] end group_tensors cost 0.00583338737487793 seconds
DEBUG 01-15 16:09:23.417328.417328 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.417108.417108 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.419985.419985 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016591548919677734 seconds
DEBUG 01-15 16:09:23.419903.419903 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.419081.419081 cuda_h.py:19] end gpu_group_list cost 0.00033593177795410156 seconds
DEBUG 01-15 16:09:23.420299.420299 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.420573.420573 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8302d7d7-e58c-4a7d-bf0f-d617a592f644
DEBUG 01-15 16:09:23.421638.421638 cuda_h.py:19] end group pad cost 0.004067659378051758 seconds
DEBUG 01-15 16:09:23.421091.421091 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:23.453492.453492 client.py:127] Model loaded
DEBUG 01-15 16:09:23.453901.453901 cuda_h.py:19] end wait_experts_multi_device cost 0.0325312614440918 seconds
DEBUG 01-15 16:09:23.453472.453472 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.453948.453948 cuda_h.py:19] end group_einsum cost 0.03205537796020508 seconds
DEBUG 01-15 16:09:23.453395.453395 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.457220.457220 cuda_h.py:19] end get_outputs_cpu1 cost 0.003222227096557617 seconds
DEBUG 01-15 16:09:23.457612.457612 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.048361778259277344 seconds
DEBUG 01-15 16:09:23.458121.458121 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005278825759887695 seconds
DEBUG 01-15 16:09:23.459035.459035 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.461468.461468 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.461836.461836 cuda_h.py:19] end index_scatter cost 0.000152587890625 seconds
DEBUG 01-15 16:09:23.462038.462038 cuda_h.py:19] end cpuoutputsdeal cost 0.0029511451721191406 seconds
DEBUG 01-15 16:09:23.462169.462169 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.462622.462622 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.462291.462291 cuda_h.py:19] end gpu_group_tensor cost 0.00030732154846191406 seconds
DEBUG 01-15 16:09:23.462413.462413 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.463647.463647 cuda_h.py:19] end gpu_group_tensor cost 0.0007505416870117188 seconds
DEBUG 01-15 16:09:23.464562.464562 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.465514.465514 cuda_h.py:19] end gpu_group_einsum cost 0.0013895034790039062 seconds
DEBUG 01-15 16:09:23.465104.465104 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.466430.466430 cuda_h.py:19] end gpu_group_einsum cost 0.0003952980041503906 seconds
DEBUG 01-15 16:09:23.466924.466924 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.466166.466166 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.466521.466521 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025773048400878906 seconds
DEBUG 01-15 16:09:23.466277.466277 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.466088.466088 cuda_h.py:19] end concat_expert_out cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:09:23.466760.466760 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.467439.467439 cuda_h.py:19] end index_scatter cost 8.034706115722656e-05 seconds
DEBUG 01-15 16:09:23.467003.467003 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00092315673828125 seconds
DEBUG 01-15 16:09:23.467616.467616 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.467227.467227 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.467732.467732 cuda_h.py:19] end all_expert_outputs_slices cost 0.000202178955078125 seconds
DEBUG 01-15 16:09:23.467104.467104 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.467703.467703 cuda_h.py:19] end concat_expert_out cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:09:23.467314.467314 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.467681.467681 cuda_h.py:19] end index_scatter cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:09:23.467775.467775 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005729198455810547 seconds
DEBUG 01-15 16:09:23.468414.468414 cuda_h.py:19] end gpu_experts_multi_device cost 0.0525360107421875 seconds
DEBUG 01-15 16:09:23.468330.468330 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.06345582008361816 seconds
DEBUG 01-15 16:09:23.468220.468220 cuda_h.py:19] end prefill_layer cost 0.07040858268737793 seconds
DEBUG 01-15 16:09:23.468851.468851 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:09:23.468124.468124 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.468827.468827 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:09:23.468291.468291 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:09:23.468901.468901 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:09:23.468029.468029 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 3.2901763916015625e-05 seconds
DEBUG 01-15 16:09:23.468686.468686 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:09:23.468951.468951 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.468014.468014 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.469063.469063 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.469262.469262 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.469081.469081 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.469012.469012 cuda_h.py:19] end allocate_cuda_memory cost 0.0003864765167236328 seconds
DEBUG 01-15 16:09:23.470733.470733 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.470665.470665 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.470828.470828 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.470725.470725 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 88f0b3ad-2b73-42f7-9393-5729fdd2162f
DEBUG 01-15 16:09:23.470640.470640 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.470239.470239 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.471477.471477 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 88f0b3ad-2b73-42f7-9393-5729fdd2162f
DEBUG 01-15 16:09:23.472396.472396 cuda_h.py:19] end load_into_gpu_async cost 0.0018703937530517578 seconds
DEBUG 01-15 16:09:23.472326.472326 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.472249.472249 cuda_h.py:19] end restore_tensors2 cost 0.00016736984252929688 seconds
DEBUG 01-15 16:09:23.472491.472491 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003183603286743164 seconds
INFO 01-15 16:09:23.472748.472748 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 88f0b3ad-2b73-42f7-9393-5729fdd2162f
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.475623.475623 cuda_h.py:19] end self_attn cost 0.004541158676147461 seconds
DEBUG 01-15 16:09:23.475310.475310 cuda_h.py:19] end iln_self_attn_paln cost 0.006941318511962891 seconds
DEBUG 01-15 16:09:23.475186.475186 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:09:23.475565.475565 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.476835.476835 cuda_h.py:19] end gate cost 0.0007252693176269531 seconds
DEBUG 01-15 16:09:23.476764.476764 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.477052.477052 lmp.py:1912] 
DEBUG 01-15 16:09:23.477052.477052 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.477762.477762 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.477842.477842 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.477730.477730 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.477234.477234 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.477785.477785 lmp.py:1917] 
DEBUG 01-15 16:09:23.477785.477785 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.477574.477574 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.477608.477608 lmp.py:1935]   Expert 34 |     23 | CPU
DEBUG 01-15 16:09:23.477919.477919 lmp.py:1935]   Expert 45 |     64 | CPU
DEBUG 01-15 16:09:23.477231.477231 lmp.py:1935]   Expert 22 |     74 | CPU
DEBUG 01-15 16:09:23.477543.477543 lmp.py:1935]   Expert 57 |     76 | CPU
DEBUG 01-15 16:09:23.477140.477140 lmp.py:1935]   Expert 17 |     96 | CPU
DEBUG 01-15 16:09:23.477690.477690 lmp.py:1935]   Expert  4 |    100 | CPU
DEBUG 01-15 16:09:23.477194.477194 lmp.py:1935]   Expert 15 |    100 | CPU
DEBUG 01-15 16:09:23.477460.477460 lmp.py:1935]   Expert 28 |    107 | CPU
DEBUG 01-15 16:09:23.477772.477772 lmp.py:1935]   Expert 32 |    112 | CPU
DEBUG 01-15 16:09:23.477560.477560 lmp.py:1935]   Expert 60 |    112 | CPU
DEBUG 01-15 16:09:23.477349.477349 lmp.py:1935]   Expert 36 |    124 | CPU
DEBUG 01-15 16:09:23.477661.477661 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:09:23.477973.477973 lmp.py:1935]   Expert 12 |    128 | CPU
DEBUG 01-15 16:09:23.477808.477808 lmp.py:1935]   Expert 14 |    128 | CPU
DEBUG 01-15 16:09:23.477120.477120 lmp.py:1935]   Expert 25 |    130 | CPU
DEBUG 01-15 16:09:23.477193.477193 lmp.py:1935]   Expert 52 |    130 | CPU
DEBUG 01-15 16:09:23.477267.477267 lmp.py:1935]   Expert  8 |    135 | CPU
DEBUG 01-15 16:09:23.477056.477056 lmp.py:1935]   Expert  2 |    139 | CPU
DEBUG 01-15 16:09:23.477083.477083 lmp.py:1935]   Expert 35 |    144 | CPU
DEBUG 01-15 16:09:23.477395.477395 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:09:23.477183.477183 lmp.py:1935]   Expert 23 |    154 | CPU
DEBUG 01-15 16:09:23.477780.477780 lmp.py:1935]   Expert 30 |    156 | CPU
DEBUG 01-15 16:09:23.477615.477615 lmp.py:1935]   Expert 39 |    157 | CPU
DEBUG 01-15 16:09:23.477212.477212 lmp.py:1935]   Expert 61 |    158 | CPU
DEBUG 01-15 16:09:23.477047.477047 lmp.py:1935]   Expert  0 |    159 | CPU
DEBUG 01-15 16:09:23.477551.477551 lmp.py:1935]   Expert  3 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.477055.477055 lmp.py:1935]   Expert 13 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.477320.477320 lmp.py:1935]   Expert 42 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.477586.477586 lmp.py:1935]   Expert 31 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.477090.477090 lmp.py:1935]   Expert 44 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478832.478832 lmp.py:1935]   Expert  9 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478575.478575 lmp.py:1935]   Expert 41 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478602.478602 lmp.py:1935]   Expert 46 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478391.478391 lmp.py:1935]   Expert 43 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478941.478941 lmp.py:1935]   Expert 18 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478253.478253 lmp.py:1935]   Expert 62 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478042.478042 lmp.py:1935]   Expert 26 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478592.478592 lmp.py:1935]   Expert 50 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478142.478142 lmp.py:1935]   Expert 27 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478170.478170 lmp.py:1935]   Expert 49 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478435.478435 lmp.py:1935]   Expert 51 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478985.478985 lmp.py:1935]   Expert 11 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478489.478489 lmp.py:1935]   Expert 47 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478755.478755 lmp.py:1935]   Expert 19 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478544.478544 lmp.py:1935]   Expert 63 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478379.478379 lmp.py:1935]   Expert 20 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478929.478929 lmp.py:1935]   Expert 55 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478764.478764 lmp.py:1935]   Expert 56 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478553.478553 lmp.py:1935]   Expert 38 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478388.478388 lmp.py:1935]   Expert 48 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478415.478415 lmp.py:1935]   Expert  1 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478357.478357 lmp.py:1935]   Expert 10 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478430.478430 lmp.py:1935]   Expert 21 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478027.478027 lmp.py:1935]   Expert 54 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478100.478100 lmp.py:1935]   Expert  7 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478220.478220 lmp.py:1935]   Expert 33 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478863.478863 lmp.py:1935]   Expert 29 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478983.478983 lmp.py:1935]   Expert 40 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478626.478626 lmp.py:1935]   Expert 24 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478269.478269 lmp.py:1935]   Expert 59 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478912.478912 lmp.py:1935]   Expert 37 |    332 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478316.478316 lmp.py:1935]   Expert 58 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478721.478721 lmp.py:1935]   Expert  6 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.478079.478079 lmp.py:1935]   Expert 53 |    853 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.478484.478484 lmp.py:1937] 
DEBUG 01-15 16:09:23.478484.478484 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.478603.478603 lmp.py:1938]   CPU:   2978 tokens
DEBUG 01-15 16:09:23.478677.478677 lmp.py:1942]   cuda:1:   4658 tokens (19 experts)
DEBUG 01-15 16:09:23.478558.478558 lmp.py:1942]   cuda:2:   4652 tokens (20 experts)
DEBUG 01-15 16:09:23.478486.478486 lmp.py:1943]   Total GPU:   9310 tokens
DEBUG 01-15 16:09:23.478414.478414 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.478414.478414 lmp.py:1944] 
DEBUG 01-15 16:09:23.478302.478302 cuda_h.py:19] end experts_map_get cost 0.001981496810913086 seconds
DEBUG 01-15 16:09:23.478490.478490 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.478676.478676 lmp.py:1953] 
DEBUG 01-15 16:09:23.478676.478676 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.478565.478565 cuda_h.py:19] end cpu_experts_submit cost 5.936622619628906e-05 seconds
DEBUG 01-15 16:09:23.478454.478454 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.479244.479244 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.479551.479551 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.3.gate_proj.weight': 0, 'model.layers.5.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.20.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.20.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.20.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.59.up_proj.weight': 322961408}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.24.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.24.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.24.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.27.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.27.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.27.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.37.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.37.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.37.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.38.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.38.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.38.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.42.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.42.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.42.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.46.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.46.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.46.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.5.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.5.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.5.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.5.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.5.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.5.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.5.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.5.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.5.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(7341604864, 5767168, 0, 0), (7347372032, 5767168, 5767168, 0), (7335837696, 5767168, 11534336, 0), (7462715392, 5767168, 17301504, 0), (7468482560, 5767168, 23068672, 0), (7456948224, 5767168, 28835840, 0), (7480016896, 5767168, 34603008, 0), (7485784064, 5767168, 40370176, 0), (7474249728, 5767168, 46137344, 0), (7514619904, 5767168, 51904512, 0), (7520387072, 5767168, 57671680, 0), (7508852736, 5767168, 63438848, 0), (7601127424, 5767168, 69206016, 0), (7606894592, 5767168, 74973184, 0), (7595360256, 5767168, 80740352, 0), (7618428928, 5767168, 86507520, 0), (7624196096, 5767168, 92274688, 0), (7612661760, 5767168, 98041856, 0), (7635730432, 5767168, 103809024, 0), (7641497600, 5767168, 109576192, 0), (7629963264, 5767168, 115343360, 0), (7653031936, 5767168, 121110528, 0), (7658799104, 5767168, 126877696, 0), (7647264768, 5767168, 132644864, 0), (7739539456, 5767168, 138412032, 0), (7745306624, 5767168, 144179200, 0), (7733772288, 5767168, 149946368, 0), (7860649984, 5767168, 155713536, 0), (7866417152, 5767168, 161480704, 0), (7854882816, 5767168, 167247872, 0), (7981760512, 5767168, 173015040, 0), (7987527680, 5767168, 178782208, 0), (7975993344, 5767168, 184549376, 0), (7999062016, 5767168, 190316544, 0), (8004829184, 5767168, 196083712, 0), (7993294848, 5767168, 201850880, 0), (8033665024, 5767168, 207618048, 0), (8039432192, 5767168, 213385216, 0), (8027897856, 5767168, 219152384, 0), (8050966528, 5767168, 224919552, 0), (8056733696, 5767168, 230686720, 0), (8045199360, 5767168, 236453888, 0), (8120172544, 5767168, 242221056, 0), (8125939712, 5767168, 247988224, 0), (8114405376, 5767168, 253755392, 0), (8172077056, 5767168, 259522560, 0), (8177844224, 5767168, 265289728, 0), (8166309888, 5767168, 271056896, 0), (8206680064, 5767168, 276824064, 0), (8212447232, 5767168, 282591232, 0), (8200912896, 5767168, 288358400, 0), (8241283072, 5767168, 294125568, 0), (8247050240, 5767168, 299892736, 0), (8235515904, 5767168, 305659904, 0), (8310489088, 5767168, 311427072, 0), (8316256256, 5767168, 317194240, 0), (8304721920, 5767168, 322961408, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (7393509376, 5767168, 17301504, 0), (7399276544, 5767168, 23068672, 0), (7387742208, 5767168, 28835840, 0), (7410810880, 5767168, 34603008, 0), (7416578048, 5767168, 40370176, 0), (7405043712, 5767168, 46137344, 0), (7445413888, 5767168, 51904512, 0), (7451181056, 5767168, 57671680, 0), (7439646720, 5767168, 63438848, 0), (7704936448, 5767168, 69206016, 0), (7710703616, 5767168, 74973184, 0), (7699169280, 5767168, 80740352, 0), (7756840960, 5767168, 86507520, 0), (7762608128, 5767168, 92274688, 0), (7751073792, 5767168, 98041856, 0), (7791443968, 5767168, 103809024, 0), (7797211136, 5767168, 109576192, 0), (7785676800, 5767168, 115343360, 0), (7826046976, 5767168, 121110528, 0), (7831814144, 5767168, 126877696, 0), (7820279808, 5767168, 132644864, 0), (7929856000, 5767168, 138412032, 0), (7935623168, 5767168, 144179200, 0), (7924088832, 5767168, 149946368, 0), (7947157504, 5767168, 155713536, 0), (7952924672, 5767168, 161480704, 0), (7941390336, 5767168, 167247872, 0), (8016363520, 5767168, 173015040, 0), (8022130688, 5767168, 178782208, 0), (8010596352, 5767168, 184549376, 0), (8085569536, 5767168, 190316544, 0), (8091336704, 5767168, 196083712, 0), (8079802368, 5767168, 201850880, 0), (8102871040, 5767168, 207618048, 0), (8108638208, 5767168, 213385216, 0), (8097103872, 5767168, 219152384, 0), (8137474048, 5767168, 224919552, 0), (8143241216, 5767168, 230686720, 0), (8131706880, 5767168, 236453888, 0), (8154775552, 5767168, 242221056, 0), (8160542720, 5767168, 247988224, 0), (8149008384, 5767168, 253755392, 0), (8223981568, 5767168, 259522560, 0), (8229748736, 5767168, 265289728, 0), (8218214400, 5767168, 271056896, 0), (8258584576, 5767168, 276824064, 0), (8264351744, 5767168, 282591232, 0), (8252817408, 5767168, 288358400, 0), (8293187584, 5767168, 294125568, 0), (8298954752, 5767168, 299892736, 0), (8287420416, 5767168, 305659904, 0), (8362393600, 5767168, 311427072, 0), (8368160768, 5767168, 317194240, 0), (8356626432, 5767168, 322961408, 0), (8379695104, 5767168, 328728576, 0), (8385462272, 5767168, 334495744, 0), (8373927936, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74aa041a2100>, 2: <capsule object NULL at 0x74a6bc762250>}
DEBUG 01-15 16:09:23.480465.480465 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:09:23.480748.480748 client.py:127] Model loaded
DEBUG 01-15 16:09:23.480341.480341 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.480140.480140 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 737bb6b8-6cda-4493-8b62-4d04cc5d3c49
DEBUG 01-15 16:09:23.480750.480750 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:23.480698.480698 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.480932.480932 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.480973.480973 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.481041.481041 cuda_h.py:19] end move_flatidxs cost 0.0008325576782226562 seconds
DEBUG 01-15 16:09:23.481387.481387 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:23.481895.481895 cuda_h.py:19] end restore2model cost 0.0009570121765136719 seconds
INFO 01-15 16:09:23.481615.481615 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 737bb6b8-6cda-4493-8b62-4d04cc5d3c49
DEBUG 01-15 16:09:23.482267.482267 cuda_h.py:19] end sllm_worker_task cost 0.012828350067138672 seconds
DEBUG 01-15 16:09:23.482766.482766 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003681182861328125 seconds
DEBUG 01-15 16:09:23.482996.482996 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.487552.487552 cuda_h.py:19] end group_tensors cost 0.005876779556274414 seconds
DEBUG 01-15 16:09:23.488375.488375 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.489349.489349 cuda_h.py:19] end restore2model cost 0.0062770843505859375 seconds
DEBUG 01-15 16:09:23.489784.489784 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.01043701171875 seconds
DEBUG 01-15 16:09:23.489494.489494 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.490667.490667 cuda_h.py:19] end gpu_sexperts cost 0.0005812644958496094 seconds
DEBUG 01-15 16:09:23.490438.490438 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.490738.490738 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 4.7206878662109375e-05 seconds
DEBUG 01-15 16:09:23.490011.490011 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.490482.490482 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.491604.491604 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014243125915527344 seconds
DEBUG 01-15 16:09:23.491680.491680 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.492768.492768 cuda_h.py:19] end gpu_group_list cost 0.00024127960205078125 seconds
DEBUG 01-15 16:09:23.492132.492132 cuda_h.py:19] end group pad cost 0.0035452842712402344 seconds
DEBUG 01-15 16:09:23.492392.492392 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:23.493260.493260 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.497669.497669 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.002745389938354492 seconds
DEBUG 01-15 16:09:23.497486.497486 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.498771.498771 cuda_h.py:19] end gpu_group_list cost 0.0005140304565429688 seconds
DEBUG 01-15 16:09:23.500293.500293 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.500190.500190 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 737bb6b8-6cda-4493-8b62-4d04cc5d3c49
INFO 01-15 16:09:23.525222.525222 client.py:127] Model loaded
DEBUG 01-15 16:09:23.525187.525187 cuda_h.py:19] end wait_experts_multi_device cost 0.025069236755371094 seconds
DEBUG 01-15 16:09:23.525547.525547 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.525615.525615 cuda_h.py:19] end group_einsum cost 0.03365135192871094 seconds
DEBUG 01-15 16:09:23.526701.526701 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.529199.529199 cuda_h.py:19] end get_outputs_cpu1 cost 0.0033533573150634766 seconds
DEBUG 01-15 16:09:23.530618.530618 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04980897903442383 seconds
DEBUG 01-15 16:09:23.531592.531592 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006406068801879883 seconds
DEBUG 01-15 16:09:23.532121.532121 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.534865.534865 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.534921.534921 cuda_h.py:19] end index_scatter cost 0.00015687942504882812 seconds
DEBUG 01-15 16:09:23.535802.535802 cuda_h.py:19] end cpuoutputsdeal cost 0.0030508041381835938 seconds
DEBUG 01-15 16:09:23.535732.535732 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.535449.535449 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.535778.535778 cuda_h.py:19] end gpu_group_tensor cost 0.0004820823669433594 seconds
DEBUG 01-15 16:09:23.535998.535998 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.536481.536481 cuda_h.py:19] end gpu_group_tensor cost 0.00014710426330566406 seconds
DEBUG 01-15 16:09:23.536061.536061 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.537952.537952 cuda_h.py:19] end gpu_group_einsum cost 0.0008523464202880859 seconds
DEBUG 01-15 16:09:23.537413.537413 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.537606.537606 cuda_h.py:19] end gpu_group_einsum cost 0.00037384033203125 seconds
DEBUG 01-15 16:09:23.537841.537841 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.537792.537792 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.538217.538217 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002067089080810547 seconds
DEBUG 01-15 16:09:23.538689.538689 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.538950.538950 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:09:23.538409.538409 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.538094.538094 cuda_h.py:19] end index_scatter cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:09:23.538354.538354 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000823974609375 seconds
DEBUG 01-15 16:09:23.538668.538668 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.538041.538041 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.539286.539286 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001494884490966797 seconds
DEBUG 01-15 16:09:23.539896.539896 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.539674.539674 cuda_h.py:19] end concat_expert_out cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:09:23.539748.539748 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.539672.539672 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:23.539527.539527 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004928112030029297 seconds
DEBUG 01-15 16:09:23.539583.539583 cuda_h.py:19] end gpu_experts_multi_device cost 0.049097299575805664 seconds
DEBUG 01-15 16:09:23.539877.539877 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.0635061264038086 seconds
DEBUG 01-15 16:09:23.539522.539522 cuda_h.py:19] end prefill_layer cost 0.07119011878967285 seconds
DEBUG 01-15 16:09:23.539100.539100 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:09:23.540234.540234 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.540513.540513 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:09:23.540554.540554 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:09:23.540263.540263 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:09:23.540504.540504 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 4.3392181396484375e-05 seconds
DEBUG 01-15 16:09:23.540591.540591 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 7.557868957519531e-05 seconds
DEBUG 01-15 16:09:23.540910.540910 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.540576.540576 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.540011.540011 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.540363.540363 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.540693.540693 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.541419.541419 cuda_h.py:19] end allocate_cuda_memory cost 0.0003819465637207031 seconds
DEBUG 01-15 16:09:23.541313.541313 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.541151.541151 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.541026.541026 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.542016.542016 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b4f0f6a2-13be-4ec3-96e0-507b23197d0b
DEBUG 01-15 16:09:23.542780.542780 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.542768.542768 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.543496.543496 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b4f0f6a2-13be-4ec3-96e0-507b23197d0b
DEBUG 01-15 16:09:23.543211.543211 cuda_h.py:19] end load_into_gpu_async cost 0.0019698143005371094 seconds
DEBUG 01-15 16:09:23.543566.543566 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.544655.544655 cuda_h.py:19] end restore_tensors2 cost 0.00015497207641601562 seconds
DEBUG 01-15 16:09:23.544612.544612 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003349781036376953 seconds
INFO 01-15 16:09:23.544207.544207 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b4f0f6a2-13be-4ec3-96e0-507b23197d0b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.546557.546557 cuda_h.py:19] end self_attn cost 0.0042994022369384766 seconds
DEBUG 01-15 16:09:23.547228.547228 cuda_h.py:19] end iln_self_attn_paln cost 0.006907224655151367 seconds
DEBUG 01-15 16:09:23.547097.547097 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:09:23.547853.547853 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.547009.547009 cuda_h.py:19] end gate cost 0.000644683837890625 seconds
DEBUG 01-15 16:09:23.547315.547315 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.548743.548743 lmp.py:1912] 
DEBUG 01-15 16:09:23.548743.548743 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.548168.548168 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.548102.548102 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.548937.548937 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.548786.548786 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.548389.548389 lmp.py:1917] 
DEBUG 01-15 16:09:23.548389.548389 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.548794.548794 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.548543.548543 lmp.py:1935]   Expert  1 |     46 | CPU
DEBUG 01-15 16:09:23.548948.548948 lmp.py:1935]   Expert  7 |     61 | CPU
DEBUG 01-15 16:09:23.548875.548875 lmp.py:1935]   Expert 37 |     71 | CPU
DEBUG 01-15 16:09:23.548088.548088 lmp.py:1935]   Expert 54 |     74 | CPU
DEBUG 01-15 16:09:23.548969.548969 lmp.py:1935]   Expert 17 |     79 | CPU
DEBUG 01-15 16:09:23.548943.548943 lmp.py:1935]   Expert 18 |     83 | CPU
DEBUG 01-15 16:09:23.548109.548109 lmp.py:1935]   Expert 13 |     90 | CPU
DEBUG 01-15 16:09:23.548560.548560 lmp.py:1935]   Expert  9 |     91 | CPU
DEBUG 01-15 16:09:23.548249.548249 lmp.py:1935]   Expert 22 |    101 | CPU
DEBUG 01-15 16:09:23.548700.548700 lmp.py:1935]   Expert 58 |    102 | CPU
DEBUG 01-15 16:09:23.548635.548635 lmp.py:1935]   Expert  0 |    108 | CPU
DEBUG 01-15 16:09:23.548291.548291 lmp.py:1935]   Expert 26 |    118 | CPU
DEBUG 01-15 16:09:23.548696.548696 lmp.py:1935]   Expert 16 |    120 | CPU
DEBUG 01-15 16:09:23.548908.548908 lmp.py:1935]   Expert 10 |    122 | CPU
DEBUG 01-15 16:09:23.548287.548287 lmp.py:1935]   Expert 63 |    128 | CPU
DEBUG 01-15 16:09:23.548692.548692 lmp.py:1935]   Expert 59 |    129 | CPU
DEBUG 01-15 16:09:23.548096.548096 lmp.py:1935]   Expert 62 |    141 | CPU
DEBUG 01-15 16:09:23.548978.548978 lmp.py:1935]   Expert 43 |    143 | CPU
DEBUG 01-15 16:09:23.548382.548382 lmp.py:1935]   Expert 28 |    145 | CPU
DEBUG 01-15 16:09:23.548310.548310 lmp.py:1935]   Expert 33 |    147 | CPU
DEBUG 01-15 16:09:23.548238.548238 lmp.py:1935]   Expert 29 |    149 | CPU
DEBUG 01-15 16:09:23.548165.548165 lmp.py:1935]   Expert  2 |    154 | CPU
DEBUG 01-15 16:09:23.548808.548808 lmp.py:1935]   Expert 51 |    162 | CPU
DEBUG 01-15 16:09:23.548736.548736 lmp.py:1935]   Expert  3 |    166 | CPU
DEBUG 01-15 16:09:23.548425.548425 lmp.py:1935]   Expert 45 |    166 | CPU
DEBUG 01-15 16:09:23.548260.548260 lmp.py:1935]   Expert 11 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.548619.548619 lmp.py:1935]   Expert 53 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.548500.548500 lmp.py:1935]   Expert 55 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.548620.548620 lmp.py:1935]   Expert 32 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549263.549263 lmp.py:1935]   Expert 40 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549621.549621 lmp.py:1935]   Expert 23 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549026.549026 lmp.py:1935]   Expert 34 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549907.549907 lmp.py:1935]   Expert 14 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549550.549550 lmp.py:1935]   Expert 41 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549431.549431 lmp.py:1935]   Expert 52 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549313.549313 lmp.py:1935]   Expert 42 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549194.549194 lmp.py:1935]   Expert 21 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549599.549599 lmp.py:1935]   Expert 57 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549242.549242 lmp.py:1935]   Expert 30 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549885.549885 lmp.py:1935]   Expert 15 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549766.549766 lmp.py:1935]   Expert 35 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549224.549224 lmp.py:1935]   Expert  4 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549867.549867 lmp.py:1935]   Expert 12 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549510.549510 lmp.py:1935]   Expert 24 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549153.549153 lmp.py:1935]   Expert 46 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549511.549511 lmp.py:1935]   Expert 50 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549154.549154 lmp.py:1935]   Expert 19 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549797.549797 lmp.py:1935]   Expert  8 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549678.549678 lmp.py:1935]   Expert 44 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549560.549560 lmp.py:1935]   Expert 49 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549971.549971 lmp.py:1935]   Expert 38 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549614.549614 lmp.py:1935]   Expert  6 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549495.549495 lmp.py:1935]   Expert 47 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549900.549900 lmp.py:1935]   Expert 31 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549543.549543 lmp.py:1935]   Expert 61 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549947.549947 lmp.py:1935]   Expert 39 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549590.549590 lmp.py:1935]   Expert 36 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549472.549472 lmp.py:1935]   Expert  5 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549830.549830 lmp.py:1935]   Expert 27 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549473.549473 lmp.py:1935]   Expert 60 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549354.549354 lmp.py:1935]   Expert 20 |    338 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549997.549997 lmp.py:1935]   Expert 48 |    369 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549594.549594 lmp.py:1935]   Expert 25 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.549237.549237 lmp.py:1935]   Expert 56 |    557 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.549926.549926 lmp.py:1937] 
DEBUG 01-15 16:09:23.549926.549926 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.549808.549808 lmp.py:1938]   CPU:   2896 tokens
DEBUG 01-15 16:09:23.549451.549451 lmp.py:1942]   cuda:1:   4622 tokens (19 experts)
DEBUG 01-15 16:09:23.549570.549570 lmp.py:1942]   cuda:2:   4770 tokens (20 experts)
DEBUG 01-15 16:09:23.549260.549260 lmp.py:1943]   Total GPU:   9392 tokens
DEBUG 01-15 16:09:23.549949.549949 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.549949.549949 lmp.py:1944] 
DEBUG 01-15 16:09:23.549076.549076 cuda_h.py:19] end experts_map_get cost 0.0017268657684326172 seconds
DEBUG 01-15 16:09:23.549303.549303 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.549867.549867 lmp.py:1953] 
DEBUG 01-15 16:09:23.549867.549867 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.549511.549511 cuda_h.py:19] end cpu_experts_submit cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:23.549989.549989 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.549633.549633 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.550314.550314 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.8.gate_proj.weight': 0, 'model.layers.6.mlp.experts.8.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.8.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.12.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.12.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.12.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.14.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.14.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.14.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.6.mlp.experts.4.gate_proj.weight': 0, 'model.layers.6.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.32.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.32.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.32.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.39.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.39.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.39.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.6.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.6.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.6.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.6.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.6.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.6.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.6.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.6.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.6.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.6.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.6.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.6.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(8535408640, 5767168, 0, 0), (8541175808, 5767168, 5767168, 0), (8529641472, 5767168, 11534336, 0), (8604614656, 5767168, 17301504, 0), (8610381824, 5767168, 23068672, 0), (8598847488, 5767168, 28835840, 0), (8639217664, 5767168, 34603008, 0), (8644984832, 5767168, 40370176, 0), (8633450496, 5767168, 46137344, 0), (8725725184, 5767168, 51904512, 0), (8731492352, 5767168, 57671680, 0), (8719958016, 5767168, 63438848, 0), (8743026688, 5767168, 69206016, 0), (8748793856, 5767168, 74973184, 0), (8737259520, 5767168, 80740352, 0), (8760328192, 5767168, 86507520, 0), (8766095360, 5767168, 92274688, 0), (8754561024, 5767168, 98041856, 0), (8794931200, 5767168, 103809024, 0), (8800698368, 5767168, 109576192, 0), (8789164032, 5767168, 115343360, 0), (8864137216, 5767168, 121110528, 0), (8869904384, 5767168, 126877696, 0), (8858370048, 5767168, 132644864, 0), (8916041728, 5767168, 138412032, 0), (8921808896, 5767168, 144179200, 0), (8910274560, 5767168, 149946368, 0), (9002549248, 5767168, 155713536, 0), (9008316416, 5767168, 161480704, 0), (8996782080, 5767168, 167247872, 0), (9019850752, 5767168, 173015040, 0), (9025617920, 5767168, 178782208, 0), (9014083584, 5767168, 184549376, 0), (9054453760, 5767168, 190316544, 0), (9060220928, 5767168, 196083712, 0), (9048686592, 5767168, 201850880, 0), (9089056768, 5767168, 207618048, 0), (9094823936, 5767168, 213385216, 0), (9083289600, 5767168, 219152384, 0), (9192865792, 5767168, 224919552, 0), (9198632960, 5767168, 230686720, 0), (9187098624, 5767168, 236453888, 0), (9210167296, 5767168, 242221056, 0), (9215934464, 5767168, 247988224, 0), (9204400128, 5767168, 253755392, 0), (9296674816, 5767168, 259522560, 0), (9302441984, 5767168, 265289728, 0), (9290907648, 5767168, 271056896, 0), (9313976320, 5767168, 276824064, 0), (9319743488, 5767168, 282591232, 0), (9308209152, 5767168, 288358400, 0), (9365880832, 5767168, 294125568, 0), (9371648000, 5767168, 299892736, 0), (9360113664, 5767168, 305659904, 0), (9452388352, 5767168, 311427072, 0), (9458155520, 5767168, 317194240, 0), (9446621184, 5767168, 322961408, 0)], 2: [(8466202624, 5767168, 0, 0), (8471969792, 5767168, 5767168, 0), (8460435456, 5767168, 11534336, 0), (8483504128, 5767168, 17301504, 0), (8489271296, 5767168, 23068672, 0), (8477736960, 5767168, 28835840, 0), (8500805632, 5767168, 34603008, 0), (8506572800, 5767168, 40370176, 0), (8495038464, 5767168, 46137344, 0), (8587313152, 5767168, 51904512, 0), (8593080320, 5767168, 57671680, 0), (8581545984, 5767168, 63438848, 0), (8656519168, 5767168, 69206016, 0), (8662286336, 5767168, 74973184, 0), (8650752000, 5767168, 80740352, 0), (8812232704, 5767168, 86507520, 0), (8817999872, 5767168, 92274688, 0), (8806465536, 5767168, 98041856, 0), (8829534208, 5767168, 103809024, 0), (8835301376, 5767168, 109576192, 0), (8823767040, 5767168, 115343360, 0), (8933343232, 5767168, 121110528, 0), (8939110400, 5767168, 126877696, 0), (8927576064, 5767168, 132644864, 0), (8950644736, 5767168, 138412032, 0), (8956411904, 5767168, 144179200, 0), (8944877568, 5767168, 149946368, 0), (8985247744, 5767168, 155713536, 0), (8991014912, 5767168, 161480704, 0), (8979480576, 5767168, 167247872, 0), (9071755264, 5767168, 173015040, 0), (9077522432, 5767168, 178782208, 0), (9065988096, 5767168, 184549376, 0), (9106358272, 5767168, 190316544, 0), (9112125440, 5767168, 196083712, 0), (9100591104, 5767168, 201850880, 0), (9123659776, 5767168, 207618048, 0), (9129426944, 5767168, 213385216, 0), (9117892608, 5767168, 219152384, 0), (9158262784, 5767168, 224919552, 0), (9164029952, 5767168, 230686720, 0), (9152495616, 5767168, 236453888, 0), (9227468800, 5767168, 242221056, 0), (9233235968, 5767168, 247988224, 0), (9221701632, 5767168, 253755392, 0), (9244770304, 5767168, 259522560, 0), (9250537472, 5767168, 265289728, 0), (9239003136, 5767168, 271056896, 0), (9262071808, 5767168, 276824064, 0), (9267838976, 5767168, 282591232, 0), (9256304640, 5767168, 288358400, 0), (9348579328, 5767168, 294125568, 0), (9354346496, 5767168, 299892736, 0), (9342812160, 5767168, 305659904, 0), (9383182336, 5767168, 311427072, 0), (9388949504, 5767168, 317194240, 0), (9377415168, 5767168, 322961408, 0), (9435086848, 5767168, 328728576, 0), (9440854016, 5767168, 334495744, 0), (9429319680, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a81406be40>, 2: <capsule object NULL at 0x74a6bc546550>}
INFO 01-15 16:09:23.551388.551388 client.py:127] Model loaded
DEBUG 01-15 16:09:23.551400.551400 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.551834.551834 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.551960.551960 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:23.551075.551075 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.551308.551308 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bf773468-78e8-4555-a2fb-f92989aa79a7
DEBUG 01-15 16:09:23.551183.551183 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.551614.551614 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.552043.552043 cuda_h.py:19] end move_flatidxs cost 0.0008823871612548828 seconds
DEBUG 01-15 16:09:23.552978.552978 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:23.552977.552977 cuda_h.py:19] end restore2model cost 0.0013263225555419922 seconds
INFO 01-15 16:09:23.553711.553711 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bf773468-78e8-4555-a2fb-f92989aa79a7
DEBUG 01-15 16:09:23.553979.553979 cuda_h.py:19] end sllm_worker_task cost 0.01248311996459961 seconds
DEBUG 01-15 16:09:23.553959.553959 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0037505626678466797 seconds
DEBUG 01-15 16:09:23.553726.553726 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.557590.557590 cuda_h.py:19] end restore2model cost 0.0030236244201660156 seconds
DEBUG 01-15 16:09:23.557864.557864 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007118940353393555 seconds
DEBUG 01-15 16:09:23.557898.557898 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.557014.557014 cuda_h.py:19] end gpu_sexperts cost 0.0002646446228027344 seconds
DEBUG 01-15 16:09:23.557459.557459 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.557474.557474 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.71661376953125e-05 seconds
DEBUG 01-15 16:09:23.557931.557931 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.557350.557350 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.558284.558284 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009748935699462891 seconds
DEBUG 01-15 16:09:23.558922.558922 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.558778.558778 cuda_h.py:19] end gpu_group_list cost 0.0002167224884033203 seconds
DEBUG 01-15 16:09:23.558105.558105 cuda_h.py:19] end group_tensors cost 0.005909442901611328 seconds
DEBUG 01-15 16:09:23.559574.559574 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.559701.559701 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.561078.561078 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0017023086547851562 seconds
DEBUG 01-15 16:09:23.561664.561664 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.561517.561517 cuda_h.py:19] end gpu_group_list cost 0.000308990478515625 seconds
DEBUG 01-15 16:09:23.562267.562267 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.563561.563561 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bf773468-78e8-4555-a2fb-f92989aa79a7
DEBUG 01-15 16:09:23.563868.563868 cuda_h.py:19] end group pad cost 0.0037529468536376953 seconds
DEBUG 01-15 16:09:23.563228.563228 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:23.596622.596622 cuda_h.py:19] end group_einsum cost 0.03264951705932617 seconds
DEBUG 01-15 16:09:23.596746.596746 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:23.596404.596404 client.py:127] Model loaded
DEBUG 01-15 16:09:23.596943.596943 cuda_h.py:19] end wait_experts_multi_device cost 0.03353619575500488 seconds
DEBUG 01-15 16:09:23.596371.596371 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.599104.599104 cuda_h.py:19] end get_outputs_cpu1 cost 0.0033071041107177734 seconds
DEBUG 01-15 16:09:23.600212.600212 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04895186424255371 seconds
DEBUG 01-15 16:09:23.601637.601637 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.004714488983154297 seconds
DEBUG 01-15 16:09:23.601957.601957 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.602130.602130 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.602618.602618 cuda_h.py:19] end index_scatter cost 7.534027099609375e-05 seconds
DEBUG 01-15 16:09:23.603887.603887 cuda_h.py:19] end cpuoutputsdeal cost 0.0014178752899169922 seconds
DEBUG 01-15 16:09:23.603273.603273 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.603367.603367 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.603665.603665 cuda_h.py:19] end gpu_group_tensor cost 0.000152587890625 seconds
DEBUG 01-15 16:09:23.603474.603474 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.603983.603983 cuda_h.py:19] end gpu_group_tensor cost 0.00013589859008789062 seconds
DEBUG 01-15 16:09:23.603880.603880 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.604315.604315 cuda_h.py:19] end gpu_group_einsum cost 0.00048613548278808594 seconds
DEBUG 01-15 16:09:23.604385.604385 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.604089.604089 cuda_h.py:19] end gpu_group_einsum cost 0.0003998279571533203 seconds
DEBUG 01-15 16:09:23.604954.604954 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.604421.604421 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.605018.605018 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019240379333496094 seconds
DEBUG 01-15 16:09:23.605251.605251 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.605167.605167 cuda_h.py:19] end concat_expert_out cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:09:23.605772.605772 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.605841.605841 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:23.605776.605776 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007717609405517578 seconds
DEBUG 01-15 16:09:23.605759.605759 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.605178.605178 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.606853.606853 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001506805419921875 seconds
DEBUG 01-15 16:09:23.606702.606702 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.606433.606433 cuda_h.py:19] end concat_expert_out cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:23.606038.606038 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.606438.606438 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:09:23.606770.606770 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004982948303222656 seconds
DEBUG 01-15 16:09:23.606203.606203 cuda_h.py:19] end gpu_experts_multi_device cost 0.048842668533325195 seconds
DEBUG 01-15 16:09:23.606020.606020 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.05918478965759277 seconds
DEBUG 01-15 16:09:23.606128.606128 cuda_h.py:19] end prefill_layer cost 0.06681346893310547 seconds
DEBUG 01-15 16:09:23.606931.606931 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:09:23.606588.606588 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.607066.607066 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:09:23.607630.607630 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:09:23.607670.607670 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:09:23.607328.607328 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 3.5762786865234375e-05 seconds
DEBUG 01-15 16:09:23.607177.607177 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 6.723403930664062e-05 seconds
DEBUG 01-15 16:09:23.607734.607734 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.607194.607194 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.607690.607690 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.607839.607839 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.607946.607946 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.608158.608158 cuda_h.py:19] end allocate_cuda_memory cost 0.00043129920959472656 seconds
DEBUG 01-15 16:09:23.608874.608874 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.608434.608434 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.608698.608698 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.608894.608894 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 46205c46-ff84-4896-b752-7b379aaedbe6
DEBUG 01-15 16:09:23.609968.609968 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.609519.609519 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.610597.610597 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 46205c46-ff84-4896-b752-7b379aaedbe6
DEBUG 01-15 16:09:23.610934.610934 cuda_h.py:19] end load_into_gpu_async cost 0.0018744468688964844 seconds
DEBUG 01-15 16:09:23.610600.610600 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.610907.610907 cuda_h.py:19] end restore_tensors2 cost 0.000148773193359375 seconds
DEBUG 01-15 16:09:23.611831.611831 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0033295154571533203 seconds
INFO 01-15 16:09:23.611611.611611 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 46205c46-ff84-4896-b752-7b379aaedbe6
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.613093.613093 cuda_h.py:19] end self_attn cost 0.004386425018310547 seconds
DEBUG 01-15 16:09:23.614654.614654 cuda_h.py:19] end iln_self_attn_paln cost 0.007071018218994141 seconds
DEBUG 01-15 16:09:23.614576.614576 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:09:23.614617.614617 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.615301.615301 cuda_h.py:19] end gate cost 0.0006144046783447266 seconds
DEBUG 01-15 16:09:23.615653.615653 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.615869.615869 lmp.py:1912] 
DEBUG 01-15 16:09:23.615869.615869 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.615148.615148 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.615175.615175 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.615772.615772 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.615746.615746 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.615196.615196 lmp.py:1917] 
DEBUG 01-15 16:09:23.615196.615196 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.615409.615409 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.615535.615535 lmp.py:1935]   Expert 50 |     44 | CPU
DEBUG 01-15 16:09:23.615463.615463 lmp.py:1935]   Expert  3 |     53 | CPU
DEBUG 01-15 16:09:23.615199.615199 lmp.py:1935]   Expert 46 |     53 | CPU
DEBUG 01-15 16:09:23.615458.615458 lmp.py:1935]   Expert  1 |     76 | CPU
DEBUG 01-15 16:09:23.615955.615955 lmp.py:1935]   Expert  4 |     88 | CPU
DEBUG 01-15 16:09:23.615975.615975 lmp.py:1935]   Expert 29 |     89 | CPU
DEBUG 01-15 16:09:23.615234.615234 lmp.py:1935]   Expert 40 |     95 | CPU
DEBUG 01-15 16:09:23.615493.615493 lmp.py:1935]   Expert 15 |     96 | CPU
DEBUG 01-15 16:09:23.615467.615467 lmp.py:1935]   Expert  8 |    111 | CPU
DEBUG 01-15 16:09:23.615918.615918 lmp.py:1935]   Expert 28 |    113 | CPU
DEBUG 01-15 16:09:23.615130.615130 lmp.py:1935]   Expert 41 |    114 | CPU
DEBUG 01-15 16:09:23.615151.615151 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:09:23.615601.615601 lmp.py:1935]   Expert 27 |    126 | CPU
DEBUG 01-15 16:09:23.615622.615622 lmp.py:1935]   Expert 48 |    128 | CPU
DEBUG 01-15 16:09:23.615404.615404 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:09:23.615424.615424 lmp.py:1935]   Expert 13 |    129 | CPU
DEBUG 01-15 16:09:23.615444.615444 lmp.py:1935]   Expert 54 |    131 | CPU
DEBUG 01-15 16:09:23.615465.615465 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:09:23.615485.615485 lmp.py:1935]   Expert 51 |    138 | CPU
DEBUG 01-15 16:09:23.615267.615267 lmp.py:1935]   Expert 60 |    139 | CPU
DEBUG 01-15 16:09:23.615811.615811 lmp.py:1935]   Expert 39 |    140 | CPU
DEBUG 01-15 16:09:23.615354.615354 lmp.py:1935]   Expert 18 |    141 | CPU
DEBUG 01-15 16:09:23.615136.615136 lmp.py:1935]   Expert 14 |    147 | CPU
DEBUG 01-15 16:09:23.615918.615918 lmp.py:1935]   Expert 20 |    147 | CPU
DEBUG 01-15 16:09:23.615700.615700 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:09:23.615058.615058 lmp.py:1935]   Expert 56 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.615391.615391 lmp.py:1935]   Expert 52 |    150 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.615795.615795 lmp.py:1935]   Expert 55 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.615769.615769 lmp.py:1935]   Expert 36 |    153 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.615505.615505 lmp.py:1935]   Expert 10 |    156 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.615240.615240 lmp.py:1935]   Expert 11 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.615215.615215 lmp.py:1935]   Expert 45 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616950.616950 lmp.py:1935]   Expert  5 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616924.616924 lmp.py:1935]   Expert 62 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616840.616840 lmp.py:1935]   Expert 57 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616040.616040 lmp.py:1935]   Expert 33 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616014.616014 lmp.py:1935]   Expert 44 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616511.616511 lmp.py:1935]   Expert 25 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616008.616008 lmp.py:1935]   Expert 58 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616505.616505 lmp.py:1935]   Expert 53 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616002.616002 lmp.py:1935]   Expert  2 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616261.616261 lmp.py:1935]   Expert 32 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616758.616758 lmp.py:1935]   Expert 31 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616017.616017 lmp.py:1935]   Expert 35 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616276.616276 lmp.py:1935]   Expert 21 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616442.616442 lmp.py:1935]   Expert 49 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616655.616655 lmp.py:1935]   Expert 63 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616582.616582 lmp.py:1935]   Expert 17 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616841.616841 lmp.py:1935]   Expert 42 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616577.616577 lmp.py:1935]   Expert 34 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616074.616074 lmp.py:1935]   Expert 37 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616810.616810 lmp.py:1935]   Expert 59 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616307.616307 lmp.py:1935]   Expert  0 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616538.616538 lmp.py:1935]   Expert 22 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616465.616465 lmp.py:1935]   Expert 19 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616678.616678 lmp.py:1935]   Expert 24 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616129.616129 lmp.py:1935]   Expert 61 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616103.616103 lmp.py:1935]   Expert 30 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616315.616315 lmp.py:1935]   Expert 47 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616515.616515 lmp.py:1935]   Expert 38 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616920.616920 lmp.py:1935]   Expert 26 |    375 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616132.616132 lmp.py:1935]   Expert 12 |    427 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616106.616106 lmp.py:1935]   Expert  9 |    674 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.616557.616557 lmp.py:1935]   Expert 23 |    700 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.616339.616339 lmp.py:1937] 
DEBUG 01-15 16:09:23.616339.616339 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.616552.616552 lmp.py:1938]   CPU:   2834 tokens
DEBUG 01-15 16:09:23.616241.616241 lmp.py:1942]   cuda:1:   4801 tokens (20 experts)
DEBUG 01-15 16:09:23.616453.616453 lmp.py:1942]   cuda:2:   4653 tokens (19 experts)
DEBUG 01-15 16:09:23.616712.616712 lmp.py:1943]   Total GPU:   9454 tokens
DEBUG 01-15 16:09:23.616971.616971 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.616971.616971 lmp.py:1944] 
DEBUG 01-15 16:09:23.616667.616667 cuda_h.py:19] end experts_map_get cost 0.0017743110656738281 seconds
DEBUG 01-15 16:09:23.616232.616232 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.616273.616273 lmp.py:1953] 
DEBUG 01-15 16:09:23.616273.616273 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.617056.617056 cuda_h.py:19] end cpu_experts_submit cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:09:23.617991.617991 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.617827.617827 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.617287.617287 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.17.gate_proj.weight': 0, 'model.layers.7.mlp.experts.17.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.17.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.19.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.19.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.19.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.21.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.21.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.21.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.22.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.22.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.22.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.23.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.23.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.23.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.26.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.26.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.26.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.31.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.31.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.31.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.32.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.32.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.32.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.34.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.34.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.34.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.36.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.36.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.36.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.45.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.45.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.45.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.52.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.52.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.52.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.53.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.53.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.53.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.57.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.58.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.58.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.58.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.61.up_proj.weight': 322961408, 'model.layers.7.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.7.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.7.mlp.experts.62.up_proj.weight': 340262912}, 2: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.7.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.7.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.7.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.7.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.7.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.7.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.7.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.7.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.7.mlp.experts.63.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(9798418432, 5767168, 0, 0), (9804185600, 5767168, 5767168, 0), (9792651264, 5767168, 11534336, 0), (9833021440, 5767168, 17301504, 0), (9838788608, 5767168, 23068672, 0), (9827254272, 5767168, 28835840, 0), (9867624448, 5767168, 34603008, 0), (9873391616, 5767168, 40370176, 0), (9861857280, 5767168, 46137344, 0), (9884925952, 5767168, 51904512, 0), (9890693120, 5767168, 57671680, 0), (9879158784, 5767168, 63438848, 0), (9902227456, 5767168, 69206016, 0), (9907994624, 5767168, 74973184, 0), (9896460288, 5767168, 80740352, 0), (9954131968, 5767168, 86507520, 0), (9959899136, 5767168, 92274688, 0), (9948364800, 5767168, 98041856, 0), (10040639488, 5767168, 103809024, 0), (10046406656, 5767168, 109576192, 0), (10034872320, 5767168, 115343360, 0), (10057940992, 5767168, 121110528, 0), (10063708160, 5767168, 126877696, 0), (10052173824, 5767168, 132644864, 0), (10092544000, 5767168, 138412032, 0), (10098311168, 5767168, 144179200, 0), (10086776832, 5767168, 149946368, 0), (10127147008, 5767168, 155713536, 0), (10132914176, 5767168, 161480704, 0), (10121379840, 5767168, 167247872, 0), (10161750016, 5767168, 173015040, 0), (10167517184, 5767168, 178782208, 0), (10155982848, 5767168, 184549376, 0), (10282860544, 5767168, 190316544, 0), (10288627712, 5767168, 196083712, 0), (10277093376, 5767168, 201850880, 0), (10403971072, 5767168, 207618048, 0), (10409738240, 5767168, 213385216, 0), (10398203904, 5767168, 219152384, 0), (10421272576, 5767168, 224919552, 0), (10427039744, 5767168, 230686720, 0), (10415505408, 5767168, 236453888, 0), (10473177088, 5767168, 242221056, 0), (10478944256, 5767168, 247988224, 0), (10467409920, 5767168, 253755392, 0), (10490478592, 5767168, 259522560, 0), (10496245760, 5767168, 265289728, 0), (10484711424, 5767168, 271056896, 0), (10507780096, 5767168, 276824064, 0), (10513547264, 5767168, 282591232, 0), (10502012928, 5767168, 288358400, 0), (10525081600, 5767168, 294125568, 0), (10530848768, 5767168, 299892736, 0), (10519314432, 5767168, 305659904, 0), (10559684608, 5767168, 311427072, 0), (10565451776, 5767168, 317194240, 0), (10553917440, 5767168, 322961408, 0), (10576986112, 5767168, 328728576, 0), (10582753280, 5767168, 334495744, 0), (10571218944, 5767168, 340262912, 0)], 2: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (9538895872, 5767168, 17301504, 0), (9544663040, 5767168, 23068672, 0), (9533128704, 5767168, 28835840, 0), (9590800384, 5767168, 34603008, 0), (9596567552, 5767168, 40370176, 0), (9585033216, 5767168, 46137344, 0), (9660006400, 5767168, 51904512, 0), (9665773568, 5767168, 57671680, 0), (9654239232, 5767168, 63438848, 0), (9677307904, 5767168, 69206016, 0), (9683075072, 5767168, 74973184, 0), (9671540736, 5767168, 80740352, 0), (9694609408, 5767168, 86507520, 0), (9700376576, 5767168, 92274688, 0), (9688842240, 5767168, 98041856, 0), (9711910912, 5767168, 103809024, 0), (9717678080, 5767168, 109576192, 0), (9706143744, 5767168, 115343360, 0), (9919528960, 5767168, 121110528, 0), (9925296128, 5767168, 126877696, 0), (9913761792, 5767168, 132644864, 0), (9936830464, 5767168, 138412032, 0), (9942597632, 5767168, 144179200, 0), (9931063296, 5767168, 149946368, 0), (10023337984, 5767168, 155713536, 0), (10029105152, 5767168, 161480704, 0), (10017570816, 5767168, 167247872, 0), (10075242496, 5767168, 173015040, 0), (10081009664, 5767168, 178782208, 0), (10069475328, 5767168, 184549376, 0), (10109845504, 5767168, 190316544, 0), (10115612672, 5767168, 196083712, 0), (10104078336, 5767168, 201850880, 0), (10144448512, 5767168, 207618048, 0), (10150215680, 5767168, 213385216, 0), (10138681344, 5767168, 219152384, 0), (10230956032, 5767168, 224919552, 0), (10236723200, 5767168, 230686720, 0), (10225188864, 5767168, 236453888, 0), (10265559040, 5767168, 242221056, 0), (10271326208, 5767168, 247988224, 0), (10259791872, 5767168, 253755392, 0), (10317463552, 5767168, 259522560, 0), (10323230720, 5767168, 265289728, 0), (10311696384, 5767168, 271056896, 0), (10352066560, 5767168, 276824064, 0), (10357833728, 5767168, 282591232, 0), (10346299392, 5767168, 288358400, 0), (10455875584, 5767168, 294125568, 0), (10461642752, 5767168, 299892736, 0), (10450108416, 5767168, 305659904, 0), (10594287616, 5767168, 311427072, 0), (10600054784, 5767168, 317194240, 0), (10588520448, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc279d40>, 2: <capsule object NULL at 0x74a6bc5ed980>}
DEBUG 01-15 16:09:23.618378.618378 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.618837.618837 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.618924.618924 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b84ad0f8-0dd7-4ca7-b20f-115b8ab16600
DEBUG 01-15 16:09:23.618031.618031 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.618412.618412 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:23.618061.618061 client.py:127] Model loaded
DEBUG 01-15 16:09:23.618348.618348 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.618575.618575 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.619497.619497 cuda_h.py:19] end move_flatidxs cost 0.0008268356323242188 seconds
DEBUG 01-15 16:09:23.619604.619604 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:23.620833.620833 cuda_h.py:19] end restore2model cost 0.001073598861694336 seconds
INFO 01-15 16:09:23.620401.620401 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b84ad0f8-0dd7-4ca7-b20f-115b8ab16600
DEBUG 01-15 16:09:23.620868.620868 cuda_h.py:19] end sllm_worker_task cost 0.012739896774291992 seconds
DEBUG 01-15 16:09:23.620614.620614 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003705739974975586 seconds
DEBUG 01-15 16:09:23.621488.621488 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.624848.624848 cuda_h.py:19] end restore2model cost 0.003003358840942383 seconds
DEBUG 01-15 16:09:23.624467.624467 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007070064544677734 seconds
DEBUG 01-15 16:09:23.624739.624739 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.624067.624067 cuda_h.py:19] end gpu_sexperts cost 0.0002803802490234375 seconds
DEBUG 01-15 16:09:23.624612.624612 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.624580.624580 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.811981201171875e-05 seconds
DEBUG 01-15 16:09:23.624753.624753 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.624172.624172 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.624785.624785 cuda_h.py:19] end group_tensors cost 0.0047681331634521484 seconds
DEBUG 01-15 16:09:23.625654.625654 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.626836.626836 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0018248558044433594 seconds
DEBUG 01-15 16:09:23.626707.626707 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.626380.626380 cuda_h.py:19] end gpu_group_list cost 0.0002849102020263672 seconds
DEBUG 01-15 16:09:23.628964.628964 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.629630.629630 cuda_h.py:19] end group pad cost 0.004564523696899414 seconds
DEBUG 01-15 16:09:23.629512.629512 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:23.629240.629240 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016188621520996094 seconds
DEBUG 01-15 16:09:23.629608.629608 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.630618.630618 cuda_h.py:19] end gpu_group_list cost 0.00042128562927246094 seconds
DEBUG 01-15 16:09:23.638019.638019 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.638985.638985 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b84ad0f8-0dd7-4ca7-b20f-115b8ab16600
DEBUG 01-15 16:09:23.659834.659834 cuda_h.py:19] end group_einsum cost 0.0295107364654541 seconds
DEBUG 01-15 16:09:23.659283.659283 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.663507.663507 cuda_h.py:19] end get_outputs_cpu1 cost 0.003651142120361328 seconds
DEBUG 01-15 16:09:23.663993.663993 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0453794002532959 seconds
INFO 01-15 16:09:23.665432.665432 client.py:127] Model loaded
DEBUG 01-15 16:09:23.665654.665654 cuda_h.py:19] end wait_experts_multi_device cost 0.027171850204467773 seconds
DEBUG 01-15 16:09:23.665702.665702 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.666417.666417 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005588531494140625 seconds
DEBUG 01-15 16:09:23.666161.666161 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.667294.667294 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.667287.667287 cuda_h.py:19] end index_scatter cost 7.939338684082031e-05 seconds
DEBUG 01-15 16:09:23.667939.667939 cuda_h.py:19] end cpuoutputsdeal cost 0.0014486312866210938 seconds
DEBUG 01-15 16:09:23.667041.667041 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.668373.668373 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.668391.668391 cuda_h.py:19] end gpu_group_tensor cost 0.0006763935089111328 seconds
DEBUG 01-15 16:09:23.668532.668532 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.668399.668399 cuda_h.py:19] end gpu_group_tensor cost 0.00014734268188476562 seconds
DEBUG 01-15 16:09:23.669727.669727 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.669817.669817 cuda_h.py:19] end gpu_group_einsum cost 0.0004665851593017578 seconds
DEBUG 01-15 16:09:23.669027.669027 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.670409.670409 cuda_h.py:19] end gpu_group_einsum cost 0.0010707378387451172 seconds
DEBUG 01-15 16:09:23.670632.670632 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.671635.671635 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.671445.671445 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020384788513183594 seconds
DEBUG 01-15 16:09:23.671777.671777 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.671429.671429 cuda_h.py:19] end concat_expert_out cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:23.671557.671557 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.671480.671480 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:23.671382.671382 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008187294006347656 seconds
DEBUG 01-15 16:09:23.671081.671081 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.672408.672408 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.672950.672950 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001583099365234375 seconds
DEBUG 01-15 16:09:23.672276.672276 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.672246.672246 cuda_h.py:19] end concat_expert_out cost 5.340576171875e-05 seconds
DEBUG 01-15 16:09:23.672989.672989 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.672602.672602 cuda_h.py:19] end index_scatter cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:09:23.672318.672318 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005254745483398438 seconds
DEBUG 01-15 16:09:23.672328.672328 cuda_h.py:19] end gpu_experts_multi_device cost 0.04797720909118652 seconds
DEBUG 01-15 16:09:23.672575.672575 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.05829358100891113 seconds
DEBUG 01-15 16:09:23.673046.673046 cuda_h.py:19] end prefill_layer cost 0.06606388092041016 seconds
DEBUG 01-15 16:09:23.673373.673373 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:09:23.673506.673506 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.673924.673924 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:09:23.673104.673104 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:09:23.673429.673429 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:09:23.673995.673995 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:09:23.673605.673605 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:09:23.673208.673208 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.673721.673721 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.673026.673026 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.673412.673412 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.673759.673759 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.674439.674439 cuda_h.py:19] end allocate_cuda_memory cost 0.0004017353057861328 seconds
DEBUG 01-15 16:09:23.674810.674810 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.674556.674556 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.674322.674322 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.675823.675823 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f383476e-cdd9-4497-85b3-0872bd9d61d9
DEBUG 01-15 16:09:23.675487.675487 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.675978.675978 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.676407.676407 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f383476e-cdd9-4497-85b3-0872bd9d61d9
DEBUG 01-15 16:09:23.676545.676545 cuda_h.py:19] end load_into_gpu_async cost 0.0018503665924072266 seconds
DEBUG 01-15 16:09:23.676118.676118 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.677320.677320 cuda_h.py:19] end restore_tensors2 cost 0.0001671314239501953 seconds
DEBUG 01-15 16:09:23.677881.677881 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0032944679260253906 seconds
INFO 01-15 16:09:23.677244.677244 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f383476e-cdd9-4497-85b3-0872bd9d61d9
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.679452.679452 cuda_h.py:19] end self_attn cost 0.0042073726654052734 seconds
DEBUG 01-15 16:09:23.680807.680807 cuda_h.py:19] end iln_self_attn_paln cost 0.006762266159057617 seconds
DEBUG 01-15 16:09:23.680206.680206 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:09:23.680677.680677 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.680573.680573 cuda_h.py:19] end gate cost 0.0006272792816162109 seconds
DEBUG 01-15 16:09:23.680641.680641 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.681539.681539 lmp.py:1912] 
DEBUG 01-15 16:09:23.681539.681539 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.681653.681653 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.681449.681449 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.681714.681714 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.681880.681880 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.681616.681616 lmp.py:1917] 
DEBUG 01-15 16:09:23.681616.681616 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.681544.681544 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.681194.681194 lmp.py:1935]   Expert 38 |     13 | CPU
DEBUG 01-15 16:09:23.681360.681360 lmp.py:1935]   Expert 39 |     61 | CPU
DEBUG 01-15 16:09:23.681811.681811 lmp.py:1935]   Expert  7 |     72 | CPU
DEBUG 01-15 16:09:23.681546.681546 lmp.py:1935]   Expert 30 |     74 | CPU
DEBUG 01-15 16:09:23.681282.681282 lmp.py:1935]   Expert 24 |     93 | CPU
DEBUG 01-15 16:09:23.681494.681494 lmp.py:1935]   Expert 14 |     94 | CPU
DEBUG 01-15 16:09:23.681991.681991 lmp.py:1935]   Expert 27 |     95 | CPU
DEBUG 01-15 16:09:23.681489.681489 lmp.py:1935]   Expert 17 |     96 | CPU
DEBUG 01-15 16:09:23.681224.681224 lmp.py:1935]   Expert 36 |     97 | CPU
DEBUG 01-15 16:09:23.681198.681198 lmp.py:1935]   Expert 40 |    100 | CPU
DEBUG 01-15 16:09:23.681934.681934 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:09:23.681670.681670 lmp.py:1935]   Expert 32 |    105 | CPU
DEBUG 01-15 16:09:23.681167.681167 lmp.py:1935]   Expert 18 |    109 | CPU
DEBUG 01-15 16:09:23.681333.681333 lmp.py:1935]   Expert 48 |    113 | CPU
DEBUG 01-15 16:09:23.681261.681261 lmp.py:1935]   Expert  1 |    114 | CPU
DEBUG 01-15 16:09:23.681188.681188 lmp.py:1935]   Expert 12 |    114 | CPU
DEBUG 01-15 16:09:23.681262.681262 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:09:23.681428.681428 lmp.py:1935]   Expert 59 |    131 | CPU
DEBUG 01-15 16:09:23.681117.681117 lmp.py:1935]   Expert 42 |    135 | CPU
DEBUG 01-15 16:09:23.681668.681668 lmp.py:1935]   Expert  0 |    140 | CPU
DEBUG 01-15 16:09:23.681933.681933 lmp.py:1935]   Expert 22 |    144 | CPU
DEBUG 01-15 16:09:23.681669.681669 lmp.py:1935]   Expert 53 |    148 | CPU
DEBUG 01-15 16:09:23.681166.681166 lmp.py:1935]   Expert 51 |    149 | CPU
DEBUG 01-15 16:09:23.681663.681663 lmp.py:1935]   Expert  8 |    161 | CPU
DEBUG 01-15 16:09:23.681684.681684 lmp.py:1935]   Expert 44 |    165 | CPU
DEBUG 01-15 16:09:23.681088.681088 lmp.py:1935]   Expert 60 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.681731.681731 lmp.py:1935]   Expert 15 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.681328.681328 lmp.py:1935]   Expert 29 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.681256.681256 lmp.py:1935]   Expert 54 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.681468.681468 lmp.py:1935]   Expert 34 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.681157.681157 lmp.py:1935]   Expert 35 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.681608.681608 lmp.py:1935]   Expert 33 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.681059.681059 lmp.py:1935]   Expert 47 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.681510.681510 lmp.py:1935]   Expert  9 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.681438.681438 lmp.py:1935]   Expert 19 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682319.682319 lmp.py:1935]   Expert 46 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682247.682247 lmp.py:1935]   Expert 56 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682936.682936 lmp.py:1935]   Expert  3 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682387.682387 lmp.py:1935]   Expert 21 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682838.682838 lmp.py:1935]   Expert 45 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682812.682812 lmp.py:1935]   Expert 20 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682263.682263 lmp.py:1935]   Expert 49 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682714.682714 lmp.py:1935]   Expert 28 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682403.682403 lmp.py:1935]   Expert  2 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682854.682854 lmp.py:1935]   Expert 57 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682543.682543 lmp.py:1935]   Expert 43 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682756.682756 lmp.py:1935]   Expert  4 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682968.682968 lmp.py:1935]   Expert 13 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682181.682181 lmp.py:1935]   Expert 10 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682631.682631 lmp.py:1935]   Expert 50 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682605.682605 lmp.py:1935]   Expert 41 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682295.682295 lmp.py:1935]   Expert 26 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682938.682938 lmp.py:1935]   Expert 63 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682627.682627 lmp.py:1935]   Expert 37 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682078.682078 lmp.py:1935]   Expert 31 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682290.682290 lmp.py:1935]   Expert 61 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682503.682503 lmp.py:1935]   Expert 52 |    305 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682715.682715 lmp.py:1935]   Expert 58 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682835.682835 lmp.py:1935]   Expert 62 |    325 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682432.682432 lmp.py:1935]   Expert 55 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682552.682552 lmp.py:1935]   Expert 11 |    381 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682671.682671 lmp.py:1935]   Expert 23 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682791.682791 lmp.py:1935]   Expert 25 |    408 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.682673.682673 lmp.py:1935]   Expert  5 |    514 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.682839.682839 lmp.py:1937] 
DEBUG 01-15 16:09:23.682839.682839 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.682959.682959 lmp.py:1938]   CPU:   2754 tokens
DEBUG 01-15 16:09:23.682555.682555 lmp.py:1942]   cuda:1:   4694 tokens (19 experts)
DEBUG 01-15 16:09:23.682198.682198 lmp.py:1942]   cuda:2:   4840 tokens (20 experts)
DEBUG 01-15 16:09:23.682364.682364 lmp.py:1943]   Total GPU:   9534 tokens
DEBUG 01-15 16:09:23.682815.682815 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.682815.682815 lmp.py:1944] 
DEBUG 01-15 16:09:23.682472.682472 cuda_h.py:19] end experts_map_get cost 0.0016376972198486328 seconds
DEBUG 01-15 16:09:23.682606.682606 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.682124.682124 lmp.py:1953] 
DEBUG 01-15 16:09:23.682124.682124 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.682000.682000 cuda_h.py:19] end cpu_experts_submit cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:09:23.682073.682073 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.682525.682525 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.683484.683484 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.4.gate_proj.weight': 0, 'model.layers.8.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.34.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.34.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.34.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.41.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.41.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.41.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.46.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.46.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.46.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.47.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.47.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.47.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.54.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.54.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.54.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.57.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.61.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.61.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.61.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.13.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.13.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.13.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.8.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.8.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.8.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.8.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.8.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.8.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.8.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.8.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.8.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.8.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.8.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.8.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(10680795136, 5767168, 0, 0), (10686562304, 5767168, 5767168, 0), (10675027968, 5767168, 11534336, 0), (10698096640, 5767168, 17301504, 0), (10703863808, 5767168, 23068672, 0), (10692329472, 5767168, 28835840, 0), (10767302656, 5767168, 34603008, 0), (10773069824, 5767168, 40370176, 0), (10761535488, 5767168, 46137344, 0), (10784604160, 5767168, 51904512, 0), (10790371328, 5767168, 57671680, 0), (10778836992, 5767168, 63438848, 0), (10801905664, 5767168, 69206016, 0), (10807672832, 5767168, 74973184, 0), (10796138496, 5767168, 80740352, 0), (10871111680, 5767168, 86507520, 0), (10876878848, 5767168, 92274688, 0), (10865344512, 5767168, 98041856, 0), (10974920704, 5767168, 103809024, 0), (10980687872, 5767168, 109576192, 0), (10969153536, 5767168, 115343360, 0), (11096031232, 5767168, 121110528, 0), (11101798400, 5767168, 126877696, 0), (11090264064, 5767168, 132644864, 0), (11199840256, 5767168, 138412032, 0), (11205607424, 5767168, 144179200, 0), (11194073088, 5767168, 149946368, 0), (11320950784, 5767168, 155713536, 0), (11326717952, 5767168, 161480704, 0), (11315183616, 5767168, 167247872, 0), (11407458304, 5767168, 173015040, 0), (11413225472, 5767168, 178782208, 0), (11401691136, 5767168, 184549376, 0), (11424759808, 5767168, 190316544, 0), (11430526976, 5767168, 196083712, 0), (11418992640, 5767168, 201850880, 0), (11459362816, 5767168, 207618048, 0), (11465129984, 5767168, 213385216, 0), (11453595648, 5767168, 219152384, 0), (11511267328, 5767168, 224919552, 0), (11517034496, 5767168, 230686720, 0), (11505500160, 5767168, 236453888, 0), (11545870336, 5767168, 242221056, 0), (11551637504, 5767168, 247988224, 0), (11540103168, 5767168, 253755392, 0), (11597774848, 5767168, 259522560, 0), (11603542016, 5767168, 265289728, 0), (11592007680, 5767168, 271056896, 0), (11666980864, 5767168, 276824064, 0), (11672748032, 5767168, 282591232, 0), (11661213696, 5767168, 288358400, 0), (11684282368, 5767168, 294125568, 0), (11690049536, 5767168, 299892736, 0), (11678515200, 5767168, 305659904, 0), (11701583872, 5767168, 311427072, 0), (11707351040, 5767168, 317194240, 0), (11695816704, 5767168, 322961408, 0)], 2: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (10663493632, 5767168, 17301504, 0), (10669260800, 5767168, 23068672, 0), (10657726464, 5767168, 28835840, 0), (10836508672, 5767168, 34603008, 0), (10842275840, 5767168, 40370176, 0), (10830741504, 5767168, 46137344, 0), (10940317696, 5767168, 51904512, 0), (10946084864, 5767168, 57671680, 0), (10934550528, 5767168, 63438848, 0), (10957619200, 5767168, 69206016, 0), (10963386368, 5767168, 74973184, 0), (10951852032, 5767168, 80740352, 0), (11009523712, 5767168, 86507520, 0), (11015290880, 5767168, 92274688, 0), (11003756544, 5767168, 98041856, 0), (11044126720, 5767168, 103809024, 0), (11049893888, 5767168, 109576192, 0), (11038359552, 5767168, 115343360, 0), (11061428224, 5767168, 121110528, 0), (11067195392, 5767168, 126877696, 0), (11055661056, 5767168, 132644864, 0), (11113332736, 5767168, 138412032, 0), (11119099904, 5767168, 144179200, 0), (11107565568, 5767168, 149946368, 0), (11147935744, 5767168, 155713536, 0), (11153702912, 5767168, 161480704, 0), (11142168576, 5767168, 167247872, 0), (11182538752, 5767168, 173015040, 0), (11188305920, 5767168, 178782208, 0), (11176771584, 5767168, 184549376, 0), (11217141760, 5767168, 190316544, 0), (11222908928, 5767168, 196083712, 0), (11211374592, 5767168, 201850880, 0), (11251744768, 5767168, 207618048, 0), (11257511936, 5767168, 213385216, 0), (11245977600, 5767168, 219152384, 0), (11355553792, 5767168, 224919552, 0), (11361320960, 5767168, 230686720, 0), (11349786624, 5767168, 236453888, 0), (11390156800, 5767168, 242221056, 0), (11395923968, 5767168, 247988224, 0), (11384389632, 5767168, 253755392, 0), (11476664320, 5767168, 259522560, 0), (11482431488, 5767168, 265289728, 0), (11470897152, 5767168, 271056896, 0), (11563171840, 5767168, 276824064, 0), (11568939008, 5767168, 282591232, 0), (11557404672, 5767168, 288358400, 0), (11580473344, 5767168, 294125568, 0), (11586240512, 5767168, 299892736, 0), (11574706176, 5767168, 305659904, 0), (11615076352, 5767168, 311427072, 0), (11620843520, 5767168, 317194240, 0), (11609309184, 5767168, 322961408, 0), (11649679360, 5767168, 328728576, 0), (11655446528, 5767168, 334495744, 0), (11643912192, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc5edd40>, 2: <capsule object NULL at 0x74a6785de4f0>}
DEBUG 01-15 16:09:23.683330.683330 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.683073.683073 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.684207.684207 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 04b1492d-610c-422a-8f61-e4c49ba9923c
DEBUG 01-15 16:09:23.684499.684499 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.684516.684516 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:23.684017.684017 client.py:127] Model loaded
DEBUG 01-15 16:09:23.684167.684167 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.684897.684897 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.685159.685159 cuda_h.py:19] end move_flatidxs cost 0.0008349418640136719 seconds
DEBUG 01-15 16:09:23.685320.685320 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:23.685045.685045 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 04b1492d-610c-422a-8f61-e4c49ba9923c
DEBUG 01-15 16:09:23.686618.686618 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0033371448516845703 seconds
DEBUG 01-15 16:09:23.686548.686548 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.686580.686580 cuda_h.py:19] end restore2model cost 0.0002834796905517578 seconds
DEBUG 01-15 16:09:23.686072.686072 cuda_h.py:19] end sllm_worker_task cost 0.013172149658203125 seconds
DEBUG 01-15 16:09:23.689086.689086 cuda_h.py:19] end restore2model cost 0.003550291061401367 seconds
DEBUG 01-15 16:09:23.689910.689910 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007161140441894531 seconds
DEBUG 01-15 16:09:23.689421.689421 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.690066.690066 cuda_h.py:19] end gpu_sexperts cost 0.000263214111328125 seconds
DEBUG 01-15 16:09:23.690896.690896 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.690387.690387 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:09:23.690322.690322 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.690502.690502 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.691148.691148 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014617443084716797 seconds
DEBUG 01-15 16:09:23.691806.691806 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.692687.692687 cuda_h.py:19] end gpu_group_list cost 0.0001990795135498047 seconds
DEBUG 01-15 16:09:23.693493.693493 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.694473.694473 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010747909545898438 seconds
DEBUG 01-15 16:09:23.694628.694628 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.694391.694391 cuda_h.py:19] end gpu_group_list cost 0.0002155303955078125 seconds
DEBUG 01-15 16:09:23.695699.695699 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.695966.695966 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 04b1492d-610c-422a-8f61-e4c49ba9923c
DEBUG 01-15 16:09:23.695731.695731 cuda_h.py:19] end group_tensors cost 0.00946497917175293 seconds
DEBUG 01-15 16:09:23.695623.695623 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.699791.699791 cuda_h.py:19] end group pad cost 0.003201007843017578 seconds
DEBUG 01-15 16:09:23.699720.699720 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:23.725192.725192 client.py:127] Model loaded
DEBUG 01-15 16:09:23.725582.725582 cuda_h.py:19] end wait_experts_multi_device cost 0.030068159103393555 seconds
DEBUG 01-15 16:09:23.725630.725630 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.726008.726008 cuda_h.py:19] end group_einsum cost 0.027702808380126953 seconds
DEBUG 01-15 16:09:23.727369.727369 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.730902.730902 cuda_h.py:19] end get_outputs_cpu1 cost 0.0031692981719970703 seconds
DEBUG 01-15 16:09:23.730938.730938 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04654979705810547 seconds
DEBUG 01-15 16:09:23.732208.732208 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0063135623931884766 seconds
DEBUG 01-15 16:09:23.732075.732075 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.734146.734146 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.734073.734073 cuda_h.py:19] end index_scatter cost 0.0002200603485107422 seconds
DEBUG 01-15 16:09:23.735204.735204 cuda_h.py:19] end cpuoutputsdeal cost 0.00313568115234375 seconds
DEBUG 01-15 16:09:23.735555.735555 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.735459.735459 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.736241.736241 cuda_h.py:19] end gpu_group_tensor cost 0.00031447410583496094 seconds
DEBUG 01-15 16:09:23.736893.736893 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.736383.736383 cuda_h.py:19] end gpu_group_tensor cost 0.0002970695495605469 seconds
DEBUG 01-15 16:09:23.736364.736364 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.738124.738124 cuda_h.py:19] end gpu_group_einsum cost 0.0011744499206542969 seconds
DEBUG 01-15 16:09:23.738868.738868 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.739478.739478 cuda_h.py:19] end gpu_group_einsum cost 0.0008864402770996094 seconds
DEBUG 01-15 16:09:23.739632.739632 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.740368.740368 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.740331.740331 cuda_h.py:19] end all_expert_outputs_slices cost 0.00046539306640625 seconds
DEBUG 01-15 16:09:23.740302.740302 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.741171.741171 cuda_h.py:19] end concat_expert_out cost 0.0001838207244873047 seconds
DEBUG 01-15 16:09:23.741962.741962 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.741960.741960 cuda_h.py:19] end index_scatter cost 0.00010347366333007812 seconds
DEBUG 01-15 16:09:23.741817.741817 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0016474723815917969 seconds
DEBUG 01-15 16:09:23.741914.741914 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.741460.741460 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.742614.742614 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022292137145996094 seconds
DEBUG 01-15 16:09:23.742178.742178 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.742745.742745 cuda_h.py:19] end concat_expert_out cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:09:23.742595.742595 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.742061.742061 cuda_h.py:19] end index_scatter cost 6.389617919921875e-05 seconds
DEBUG 01-15 16:09:23.742023.742023 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006303787231445312 seconds
DEBUG 01-15 16:09:23.742397.742397 cuda_h.py:19] end gpu_experts_multi_device cost 0.052068471908569336 seconds
DEBUG 01-15 16:09:23.742665.742665 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.06235241889953613 seconds
DEBUG 01-15 16:09:23.743098.743098 cuda_h.py:19] end prefill_layer cost 0.0698540210723877 seconds
DEBUG 01-15 16:09:23.743968.743968 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:09:23.743433.743433 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.743135.743135 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:09:23.743838.743838 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:09:23.743733.743733 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:09:23.743868.743868 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:09:23.743101.743101 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:09:23.743751.743751 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.743363.743363 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.743688.743688 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.743260.743260 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.743381.743381 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.744114.744114 cuda_h.py:19] end allocate_cuda_memory cost 0.0005815029144287109 seconds
DEBUG 01-15 16:09:23.744279.744279 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.745747.745747 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.745455.745455 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.745671.745671 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b18ea051-1eb5-422f-a695-6420e2db26a6
DEBUG 01-15 16:09:23.745156.745156 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.745177.745177 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.746413.746413 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b18ea051-1eb5-422f-a695-6420e2db26a6
DEBUG 01-15 16:09:23.747637.747637 cuda_h.py:19] end load_into_gpu_async cost 0.002056121826171875 seconds
DEBUG 01-15 16:09:23.747197.747197 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.747365.747365 cuda_h.py:19] end restore_tensors2 cost 0.0001685619354248047 seconds
DEBUG 01-15 16:09:23.747018.747018 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0036478042602539062 seconds
INFO 01-15 16:09:23.747448.747448 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b18ea051-1eb5-422f-a695-6420e2db26a6
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.750498.750498 cuda_h.py:19] end self_attn cost 0.004766941070556641 seconds
DEBUG 01-15 16:09:23.750377.750377 cuda_h.py:19] end iln_self_attn_paln cost 0.007592678070068359 seconds
DEBUG 01-15 16:09:23.751060.751060 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:09:23.751485.751485 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.751437.751437 cuda_h.py:19] end gate cost 0.0007030963897705078 seconds
DEBUG 01-15 16:09:23.751697.751697 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.752442.752442 lmp.py:1912] 
DEBUG 01-15 16:09:23.752442.752442 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.752536.752536 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.752530.752530 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.752107.752107 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.752327.752327 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.752844.752844 lmp.py:1917] 
DEBUG 01-15 16:09:23.752844.752844 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.752156.752156 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.752475.752475 lmp.py:1935]   Expert 24 |     39 | CPU
DEBUG 01-15 16:09:23.752595.752595 lmp.py:1935]   Expert  2 |     47 | CPU
DEBUG 01-15 16:09:23.752999.752999 lmp.py:1935]   Expert 26 |     64 | CPU
DEBUG 01-15 16:09:23.752404.752404 lmp.py:1935]   Expert 32 |     64 | CPU
DEBUG 01-15 16:09:23.752808.752808 lmp.py:1935]   Expert 19 |     69 | CPU
DEBUG 01-15 16:09:23.752974.752974 lmp.py:1935]   Expert 50 |     70 | CPU
DEBUG 01-15 16:09:23.752141.752141 lmp.py:1935]   Expert 15 |     80 | CPU
DEBUG 01-15 16:09:23.752260.752260 lmp.py:1935]   Expert 28 |     81 | CPU
DEBUG 01-15 16:09:23.752857.752857 lmp.py:1935]   Expert  4 |     82 | CPU
DEBUG 01-15 16:09:23.752454.752454 lmp.py:1935]   Expert  7 |     82 | CPU
DEBUG 01-15 16:09:23.752812.752812 lmp.py:1935]   Expert 60 |     82 | CPU
DEBUG 01-15 16:09:23.752932.752932 lmp.py:1935]   Expert 59 |     90 | CPU
DEBUG 01-15 16:09:23.752767.752767 lmp.py:1935]   Expert 23 |     98 | CPU
DEBUG 01-15 16:09:23.752887.752887 lmp.py:1935]   Expert 49 |     99 | CPU
DEBUG 01-15 16:09:23.752291.752291 lmp.py:1935]   Expert  5 |    104 | CPU
DEBUG 01-15 16:09:23.752219.752219 lmp.py:1935]   Expert 12 |    105 | CPU
DEBUG 01-15 16:09:23.752147.752147 lmp.py:1935]   Expert 10 |    111 | CPU
DEBUG 01-15 16:09:23.752836.752836 lmp.py:1935]   Expert 27 |    111 | CPU
DEBUG 01-15 16:09:23.752525.752525 lmp.py:1935]   Expert 41 |    121 | CPU
DEBUG 01-15 16:09:23.752453.752453 lmp.py:1935]   Expert  3 |    124 | CPU
DEBUG 01-15 16:09:23.752381.752381 lmp.py:1935]   Expert 25 |    127 | CPU
DEBUG 01-15 16:09:23.752500.752500 lmp.py:1935]   Expert 40 |    128 | CPU
DEBUG 01-15 16:09:23.752667.752667 lmp.py:1935]   Expert 20 |    130 | CPU
DEBUG 01-15 16:09:23.752356.752356 lmp.py:1935]   Expert 13 |    131 | CPU
DEBUG 01-15 16:09:23.752045.752045 lmp.py:1935]   Expert 16 |    133 | CPU
DEBUG 01-15 16:09:23.752357.752357 lmp.py:1935]   Expert 37 |    145 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.752715.752715 lmp.py:1935]   Expert 17 |    146 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753597.753597 lmp.py:1935]   Expert 35 |    147 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753955.753955 lmp.py:1935]   Expert 47 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753505.753505 lmp.py:1935]   Expert 22 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753625.753625 lmp.py:1935]   Expert 53 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753507.753507 lmp.py:1935]   Expert 39 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753149.753149 lmp.py:1935]   Expert 38 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753792.753792 lmp.py:1935]   Expert 44 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753674.753674 lmp.py:1935]   Expert 36 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753317.753317 lmp.py:1935]   Expert 52 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753675.753675 lmp.py:1935]   Expert 58 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753510.753510 lmp.py:1935]   Expert 18 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753630.753630 lmp.py:1935]   Expert 62 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753273.753273 lmp.py:1935]   Expert 11 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753393.753393 lmp.py:1935]   Expert 48 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753036.753036 lmp.py:1935]   Expert 14 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753679.753679 lmp.py:1935]   Expert 30 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753560.753560 lmp.py:1935]   Expert  1 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753203.753203 lmp.py:1935]   Expert 42 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753084.753084 lmp.py:1935]   Expert 31 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753966.753966 lmp.py:1935]   Expert 45 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753086.753086 lmp.py:1935]   Expert  6 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753729.753729 lmp.py:1935]   Expert 51 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753372.753372 lmp.py:1935]   Expert 29 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753253.753253 lmp.py:1935]   Expert 34 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753134.753134 lmp.py:1935]   Expert 33 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753016.753016 lmp.py:1935]   Expert 57 |    296 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753897.753897 lmp.py:1935]   Expert 61 |    305 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753878.753878 lmp.py:1935]   Expert 43 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753667.753667 lmp.py:1935]   Expert  0 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753217.753217 lmp.py:1935]   Expert 46 |    349 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753337.753337 lmp.py:1935]   Expert  8 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753410.753410 lmp.py:1935]   Expert 56 |    390 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753961.753961 lmp.py:1935]   Expert  9 |    392 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753749.753749 lmp.py:1935]   Expert 54 |    398 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753790.753790 lmp.py:1935]   Expert 63 |    411 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753864.753864 lmp.py:1935]   Expert 55 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.753460.753460 lmp.py:1935]   Expert 21 |    485 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.753103.753103 lmp.py:1937] 
DEBUG 01-15 16:09:23.753103.753103 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.753700.753700 lmp.py:1938]   CPU:   2372 tokens
DEBUG 01-15 16:09:23.753773.753773 lmp.py:1942]   cuda:1:   4888 tokens (19 experts)
DEBUG 01-15 16:09:23.753608.753608 lmp.py:1942]   cuda:2:   5028 tokens (20 experts)
DEBUG 01-15 16:09:23.753728.753728 lmp.py:1943]   Total GPU:   9916 tokens
DEBUG 01-15 16:09:23.753133.753133 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.753133.753133 lmp.py:1944] 
DEBUG 01-15 16:09:23.753690.753690 cuda_h.py:19] end experts_map_get cost 0.0019316673278808594 seconds
DEBUG 01-15 16:09:23.753454.753454 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.753256.753256 lmp.py:1953] 
DEBUG 01-15 16:09:23.753256.753256 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.753761.753761 cuda_h.py:19] end cpu_experts_submit cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:09:23.754385.754385 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.754129.754129 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.754404.754404 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.14.gate_proj.weight': 0, 'model.layers.9.mlp.experts.14.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.14.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.17.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.17.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.17.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.18.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.18.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.18.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.21.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.21.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.21.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.22.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.22.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.22.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.29.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.29.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.29.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.30.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.30.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.30.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.34.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.34.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.34.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.36.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.36.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.36.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.42.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.42.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.42.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.43.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.43.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.43.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.44.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.44.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.44.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.54.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.54.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.54.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.31.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.31.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.31.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.33.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.33.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.33.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.9.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.9.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.9.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.9.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.9.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.9.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.9.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.9.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.9.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.9.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.9.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.9.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(11961106432, 5767168, 0, 0), (11966873600, 5767168, 5767168, 0), (11955339264, 5767168, 11534336, 0), (12013010944, 5767168, 17301504, 0), (12018778112, 5767168, 23068672, 0), (12007243776, 5767168, 28835840, 0), (12030312448, 5767168, 34603008, 0), (12036079616, 5767168, 40370176, 0), (12024545280, 5767168, 46137344, 0), (12082216960, 5767168, 51904512, 0), (12087984128, 5767168, 57671680, 0), (12076449792, 5767168, 63438848, 0), (12099518464, 5767168, 69206016, 0), (12105285632, 5767168, 74973184, 0), (12093751296, 5767168, 80740352, 0), (12220628992, 5767168, 86507520, 0), (12226396160, 5767168, 92274688, 0), (12214861824, 5767168, 98041856, 0), (12237930496, 5767168, 103809024, 0), (12243697664, 5767168, 109576192, 0), (12232163328, 5767168, 115343360, 0), (12307136512, 5767168, 121110528, 0), (12312903680, 5767168, 126877696, 0), (12301369344, 5767168, 132644864, 0), (12341739520, 5767168, 138412032, 0), (12347506688, 5767168, 144179200, 0), (12335972352, 5767168, 149946368, 0), (12445548544, 5767168, 155713536, 0), (12451315712, 5767168, 161480704, 0), (12439781376, 5767168, 167247872, 0), (12462850048, 5767168, 173015040, 0), (12468617216, 5767168, 178782208, 0), (12457082880, 5767168, 184549376, 0), (12480151552, 5767168, 190316544, 0), (12485918720, 5767168, 196083712, 0), (12474384384, 5767168, 201850880, 0), (12497453056, 5767168, 207618048, 0), (12503220224, 5767168, 213385216, 0), (12491685888, 5767168, 219152384, 0), (12514754560, 5767168, 224919552, 0), (12520521728, 5767168, 230686720, 0), (12508987392, 5767168, 236453888, 0), (12635865088, 5767168, 242221056, 0), (12641632256, 5767168, 247988224, 0), (12630097920, 5767168, 253755392, 0), (12653166592, 5767168, 259522560, 0), (12658933760, 5767168, 265289728, 0), (12647399424, 5767168, 271056896, 0), (12687769600, 5767168, 276824064, 0), (12693536768, 5767168, 282591232, 0), (12682002432, 5767168, 288358400, 0), (12774277120, 5767168, 294125568, 0), (12780044288, 5767168, 299892736, 0), (12768509952, 5767168, 305659904, 0), (12791578624, 5767168, 311427072, 0), (12797345792, 5767168, 317194240, 0), (12785811456, 5767168, 322961408, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (11736186880, 5767168, 17301504, 0), (11741954048, 5767168, 23068672, 0), (11730419712, 5767168, 28835840, 0), (11822694400, 5767168, 34603008, 0), (11828461568, 5767168, 40370176, 0), (11816927232, 5767168, 46137344, 0), (11857297408, 5767168, 51904512, 0), (11863064576, 5767168, 57671680, 0), (11851530240, 5767168, 63438848, 0), (11874598912, 5767168, 69206016, 0), (11880366080, 5767168, 74973184, 0), (11868831744, 5767168, 80740352, 0), (11909201920, 5767168, 86507520, 0), (11914969088, 5767168, 92274688, 0), (11903434752, 5767168, 98041856, 0), (12255232000, 5767168, 103809024, 0), (12260999168, 5767168, 109576192, 0), (12249464832, 5767168, 115343360, 0), (12289835008, 5767168, 121110528, 0), (12295602176, 5767168, 126877696, 0), (12284067840, 5767168, 132644864, 0), (12324438016, 5767168, 138412032, 0), (12330205184, 5767168, 144179200, 0), (12318670848, 5767168, 149946368, 0), (12359041024, 5767168, 155713536, 0), (12364808192, 5767168, 161480704, 0), (12353273856, 5767168, 167247872, 0), (12376342528, 5767168, 173015040, 0), (12382109696, 5767168, 178782208, 0), (12370575360, 5767168, 184549376, 0), (12393644032, 5767168, 190316544, 0), (12399411200, 5767168, 196083712, 0), (12387876864, 5767168, 201850880, 0), (12532056064, 5767168, 207618048, 0), (12537823232, 5767168, 213385216, 0), (12526288896, 5767168, 219152384, 0), (12549357568, 5767168, 224919552, 0), (12555124736, 5767168, 230686720, 0), (12543590400, 5767168, 236453888, 0), (12601262080, 5767168, 242221056, 0), (12607029248, 5767168, 247988224, 0), (12595494912, 5767168, 253755392, 0), (12618563584, 5767168, 259522560, 0), (12624330752, 5767168, 265289728, 0), (12612796416, 5767168, 271056896, 0), (12670468096, 5767168, 276824064, 0), (12676235264, 5767168, 282591232, 0), (12664700928, 5767168, 288358400, 0), (12705071104, 5767168, 294125568, 0), (12710838272, 5767168, 299892736, 0), (12699303936, 5767168, 305659904, 0), (12722372608, 5767168, 311427072, 0), (12728139776, 5767168, 317194240, 0), (12716605440, 5767168, 322961408, 0), (12808880128, 5767168, 328728576, 0), (12814647296, 5767168, 334495744, 0), (12803112960, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6807aa370>, 2: <capsule object NULL at 0x74a6bc762040>}
DEBUG 01-15 16:09:23.755941.755941 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.755347.755347 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.755911.755911 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b9aee6e9-5a12-4cfb-ab6e-0ac383cf01f4
DEBUG 01-15 16:09:23.755580.755580 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.755856.755856 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:23.755145.755145 client.py:127] Model loaded
DEBUG 01-15 16:09:23.755944.755944 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.755610.755610 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.756442.756442 cuda_h.py:19] end move_flatidxs cost 0.0008602142333984375 seconds
DEBUG 01-15 16:09:23.756471.756471 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:23.757235.757235 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b9aee6e9-5a12-4cfb-ab6e-0ac383cf01f4
DEBUG 01-15 16:09:23.757731.757731 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034995079040527344 seconds
DEBUG 01-15 16:09:23.757999.757999 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.757751.757751 cuda_h.py:19] end restore2model cost 0.000247955322265625 seconds
DEBUG 01-15 16:09:23.758460.758460 cuda_h.py:19] end sllm_worker_task cost 0.014482498168945312 seconds
DEBUG 01-15 16:09:23.761325.761325 cuda_h.py:19] end restore2model cost 0.0041620731353759766 seconds
DEBUG 01-15 16:09:23.761520.761520 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007940053939819336 seconds
DEBUG 01-15 16:09:23.761461.761461 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.762797.762797 cuda_h.py:19] end gpu_sexperts cost 0.0003192424774169922 seconds
DEBUG 01-15 16:09:23.762296.762296 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.762410.762410 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8596649169921875e-05 seconds
DEBUG 01-15 16:09:23.762113.762113 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.762961.762961 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.762747.762747 cuda_h.py:19] end group_tensors cost 0.00584721565246582 seconds
DEBUG 01-15 16:09:23.763899.763899 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.763546.763546 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012066364288330078 seconds
DEBUG 01-15 16:09:23.763204.763204 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.764730.764730 cuda_h.py:19] end gpu_group_list cost 0.0002510547637939453 seconds
DEBUG 01-15 16:09:23.765478.765478 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.766838.766838 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015995502471923828 seconds
DEBUG 01-15 16:09:23.767856.767856 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.767256.767256 cuda_h.py:19] end group pad cost 0.0034720897674560547 seconds
DEBUG 01-15 16:09:23.767801.767801 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:23.767831.767831 cuda_h.py:19] end gpu_group_list cost 0.00022673606872558594 seconds
DEBUG 01-15 16:09:23.768071.768071 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.768558.768558 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b9aee6e9-5a12-4cfb-ab6e-0ac383cf01f4
DEBUG 01-15 16:09:23.781216.781216 cuda_h.py:19] end group_einsum cost 0.014688491821289062 seconds
DEBUG 01-15 16:09:23.782327.782327 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.785283.785283 cuda_h.py:19] end get_outputs_cpu1 cost 0.003178834915161133 seconds
DEBUG 01-15 16:09:23.786812.786812 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0304410457611084 seconds
INFO 01-15 16:09:23.789774.789774 client.py:127] Model loaded
DEBUG 01-15 16:09:23.789136.789136 cuda_h.py:19] end wait_experts_multi_device cost 0.021204471588134766 seconds
DEBUG 01-15 16:09:23.789621.789621 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.790034.790034 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0006074905395507812 seconds
DEBUG 01-15 16:09:23.790215.790215 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.792950.792950 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.792638.792638 cuda_h.py:19] end index_scatter cost 9.775161743164062e-05 seconds
DEBUG 01-15 16:09:23.792446.792446 cuda_h.py:19] end cpuoutputsdeal cost 0.0019154548645019531 seconds
DEBUG 01-15 16:09:23.792899.792899 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.792430.792430 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.792928.792928 cuda_h.py:19] end gpu_group_tensor cost 0.00018858909606933594 seconds
DEBUG 01-15 16:09:23.793936.793936 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.793175.793175 cuda_h.py:19] end gpu_group_tensor cost 0.00017547607421875 seconds
DEBUG 01-15 16:09:23.793775.793775 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.794114.794114 cuda_h.py:19] end gpu_group_einsum cost 0.0007538795471191406 seconds
DEBUG 01-15 16:09:23.794370.794370 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.794217.794217 cuda_h.py:19] end gpu_group_einsum cost 0.0004951953887939453 seconds
DEBUG 01-15 16:09:23.794162.794162 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.795331.795331 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.795516.795516 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003311634063720703 seconds
DEBUG 01-15 16:09:23.795663.795663 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.795912.795912 cuda_h.py:19] end concat_expert_out cost 6.723403930664062e-05 seconds
DEBUG 01-15 16:09:23.795014.795014 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.795203.795203 cuda_h.py:19] end index_scatter cost 6.723403930664062e-05 seconds
DEBUG 01-15 16:09:23.796827.796827 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010333061218261719 seconds
DEBUG 01-15 16:09:23.796632.796632 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.796343.796343 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.796087.796087 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023603439331054688 seconds
DEBUG 01-15 16:09:23.796320.796320 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.796687.796687 cuda_h.py:19] end concat_expert_out cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:09:23.796537.796537 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.796527.796527 cuda_h.py:19] end index_scatter cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:09:23.796575.796575 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006229877471923828 seconds
DEBUG 01-15 16:09:23.796776.796776 cuda_h.py:19] end gpu_experts_multi_device cost 0.034418344497680664 seconds
DEBUG 01-15 16:09:23.797845.797845 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.04594826698303223 seconds
DEBUG 01-15 16:09:23.797018.797018 cuda_h.py:19] end prefill_layer cost 0.05422234535217285 seconds
DEBUG 01-15 16:09:23.797615.797615 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:09:23.797603.797603 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.797498.797498 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:09:23.797678.797678 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:09:23.797195.797195 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:09:23.797668.797668 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:09:23.797947.797947 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 7.128715515136719e-05 seconds
DEBUG 01-15 16:09:23.797643.797643 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.797288.797288 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.797183.797183 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.798708.798708 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.798643.798643 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.798574.798574 cuda_h.py:19] end allocate_cuda_memory cost 0.00040841102600097656 seconds
DEBUG 01-15 16:09:23.798434.798434 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.799848.799848 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.799793.799793 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.799584.799584 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a9030189-ab1e-44f5-ba13-8a572a6a24ea
DEBUG 01-15 16:09:23.799439.799439 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.799739.799739 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.803027.803027 cuda_h.py:19] end self_attn cost 0.003173351287841797 seconds
DEBUG 01-15 16:09:23.803596.803596 cuda_h.py:19] end iln_self_attn_paln cost 0.00568699836730957 seconds
DEBUG 01-15 16:09:23.803525.803525 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:09:23.803672.803672 cuda_h.py:10] start gate
INFO 01-15 16:09:23.804675.804675 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a9030189-ab1e-44f5-ba13-8a572a6a24ea
DEBUG 01-15 16:09:23.804793.804793 cuda_h.py:19] end load_into_gpu_async cost 0.005257368087768555 seconds
DEBUG 01-15 16:09:23.804484.804484 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.804366.804366 cuda_h.py:19] end restore_tensors2 cost 0.0001513957977294922 seconds
DEBUG 01-15 16:09:23.804081.804081 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.006579160690307617 seconds
DEBUG 01-15 16:09:23.804938.804938 cuda_h.py:19] end gate cost 0.0013802051544189453 seconds
DEBUG 01-15 16:09:23.805863.805863 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.805027.805027 lmp.py:1912] 
DEBUG 01-15 16:09:23.805027.805027 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.805121.805121 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.805407.805407 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.805818.805818 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.805607.805607 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.805680.805680 lmp.py:1917] 
DEBUG 01-15 16:09:23.805680.805680 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.805515.805515 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.805980.805980 lmp.py:1935]   Expert 43 |     15 | CPU
DEBUG 01-15 16:09:23.805630.805630 lmp.py:1935]   Expert 27 |     31 | CPU
DEBUG 01-15 16:09:23.805941.805941 lmp.py:1935]   Expert 26 |     53 | CPU
DEBUG 01-15 16:09:23.805538.805538 lmp.py:1935]   Expert 34 |     53 | CPU
DEBUG 01-15 16:09:23.805612.805612 lmp.py:1935]   Expert 56 |     54 | CPU
DEBUG 01-15 16:09:23.805685.805685 lmp.py:1935]   Expert  3 |     58 | CPU
DEBUG 01-15 16:09:23.805997.805997 lmp.py:1935]   Expert  4 |     68 | CPU
DEBUG 01-15 16:09:23.805237.805237 lmp.py:1935]   Expert 61 |     79 | CPU
DEBUG 01-15 16:09:23.805787.805787 lmp.py:1935]   Expert 14 |     95 | CPU
DEBUG 01-15 16:09:23.805860.805860 lmp.py:1935]   Expert 38 |    102 | CPU
DEBUG 01-15 16:09:23.805649.805649 lmp.py:1935]   Expert  2 |    114 | CPU
DEBUG 01-15 16:09:23.806484.806484 lmp.py:1935]   Expert 17 |    121 | CPU
DEBUG 01-15 16:09:23.806703.806703 lmp.py:1935]   Expert 22 |    122 | CPU
DEBUG 01-15 16:09:23.806777.806777 lmp.py:1935]   Expert 37 |    127 | CPU
DEBUG 01-15 16:09:23.806089.806089 lmp.py:1935]   Expert 47 |    128 | CPU
DEBUG 01-15 16:09:23.806924.806924 lmp.py:1935]   Expert 55 |    133 | CPU
DEBUG 01-15 16:09:23.806236.806236 lmp.py:1935]   Expert 28 |    134 | CPU
DEBUG 01-15 16:09:23.806038.806038 lmp.py:1935]   Expert 54 |    137 | CPU
DEBUG 01-15 16:09:23.806324.806324 lmp.py:1935]   Expert  7 |    144 | CPU
DEBUG 01-15 16:09:23.806828.806828 lmp.py:1935]   Expert  5 |    145 | CPU
DEBUG 01-15 16:09:23.806326.806326 lmp.py:1935]   Expert 15 |    146 | CPU
DEBUG 01-15 16:09:23.806738.806738 lmp.py:1935]   Expert 48 |    146 | CPU
DEBUG 01-15 16:09:23.806811.806811 lmp.py:1935]   Expert 51 |    147 | CPU
DEBUG 01-15 16:09:23.806646.806646 lmp.py:1935]   Expert 45 |    148 | CPU
DEBUG 01-15 16:09:23.806958.806958 lmp.py:1935]   Expert 60 |    150 | CPU
DEBUG 01-15 16:09:23.806608.806608 lmp.py:1935]   Expert 12 |    153 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806589.806589 lmp.py:1935]   Expert 63 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806331.806331 lmp.py:1935]   Expert 19 |    156 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806835.806835 lmp.py:1935]   Expert 57 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806862.806862 lmp.py:1935]   Expert  6 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806366.806366 lmp.py:1935]   Expert 52 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806155.806155 lmp.py:1935]   Expert 50 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806659.806659 lmp.py:1935]   Expert 44 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806163.806163 lmp.py:1935]   Expert 31 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806429.806429 lmp.py:1935]   Expert 18 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806741.806741 lmp.py:1935]   Expert 13 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806960.806960 lmp.py:1935]   Expert 30 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806749.806749 lmp.py:1935]   Expert 23 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806253.806253 lmp.py:1935]   Expert 59 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806280.806280 lmp.py:1935]   Expert 53 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806069.806069 lmp.py:1935]   Expert 39 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806857.806857 lmp.py:1935]   Expert 20 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806123.806123 lmp.py:1935]   Expert 21 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806912.806912 lmp.py:1935]   Expert 29 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806462.806462 lmp.py:1935]   Expert 16 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806489.806489 lmp.py:1935]   Expert 36 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806516.806516 lmp.py:1935]   Expert 41 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806782.806782 lmp.py:1935]   Expert 25 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806571.806571 lmp.py:1935]   Expert 49 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806883.806883 lmp.py:1935]   Expert 32 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806387.806387 lmp.py:1935]   Expert 46 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806652.806652 lmp.py:1935]   Expert  8 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806202.806202 lmp.py:1935]   Expert 10 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806230.806230 lmp.py:1935]   Expert 42 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806495.806495 lmp.py:1935]   Expert 62 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806761.806761 lmp.py:1935]   Expert 35 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.806742.806742 lmp.py:1935]   Expert 33 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.806530.806530 lmp.py:1935]   Expert  9 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.807557.807557 lmp.py:1935]   Expert 58 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.807346.807346 lmp.py:1935]   Expert 40 |    390 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.807373.807373 lmp.py:1935]   Expert 11 |    422 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.807924.807924 lmp.py:1935]   Expert  0 |    428 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.807620.807620 lmp.py:1935]   Expert 24 |    562 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.807647.807647 lmp.py:1935]   Expert  1 |    649 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.807244.807244 lmp.py:1937] 
DEBUG 01-15 16:09:23.807244.807244 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.807317.807317 lmp.py:1938]   CPU:   2650 tokens
DEBUG 01-15 16:09:23.807583.807583 lmp.py:1942]   cuda:1:   4743 tokens (19 experts)
DEBUG 01-15 16:09:23.807371.807371 lmp.py:1942]   cuda:2:   4895 tokens (20 experts)
DEBUG 01-15 16:09:23.807445.807445 lmp.py:1943]   Total GPU:   9638 tokens
DEBUG 01-15 16:09:23.807280.807280 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.807280.807280 lmp.py:1944] 
DEBUG 01-15 16:09:23.807268.807268 cuda_h.py:19] end experts_map_get cost 0.0021576881408691406 seconds
INFO 01-15 16:09:23.807548.807548 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a9030189-ab1e-44f5-ba13-8a572a6a24ea
DEBUG 01-15 16:09:23.807399.807399 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.807306.807306 lmp.py:1953] 
DEBUG 01-15 16:09:23.807306.807306 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.807241.807241 cuda_h.py:19] end cpu_experts_submit cost 5.8650970458984375e-05 seconds
DEBUG 01-15 16:09:23.807845.807845 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.807496.807496 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.809641.809641 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.1.gate_proj.weight': 0, 'model.layers.10.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.36.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.36.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.36.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.39.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.39.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.39.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.58.up_proj.weight': 322961408}, 2: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.10.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.10.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.10.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.10.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.10.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.10.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.10.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.10.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.10.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.10.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.10.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.10.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(12843483136, 5767168, 0, 0), (12849250304, 5767168, 5767168, 0), (12837715968, 5767168, 11534336, 0), (12929990656, 5767168, 17301504, 0), (12935757824, 5767168, 23068672, 0), (12924223488, 5767168, 28835840, 0), (12981895168, 5767168, 34603008, 0), (12987662336, 5767168, 40370176, 0), (12976128000, 5767168, 46137344, 0), (12999196672, 5767168, 51904512, 0), (13004963840, 5767168, 57671680, 0), (12993429504, 5767168, 63438848, 0), (13016498176, 5767168, 69206016, 0), (13022265344, 5767168, 74973184, 0), (13010731008, 5767168, 80740352, 0), (13137608704, 5767168, 86507520, 0), (13143375872, 5767168, 92274688, 0), (13131841536, 5767168, 98041856, 0), (13154910208, 5767168, 103809024, 0), (13160677376, 5767168, 109576192, 0), (13149143040, 5767168, 115343360, 0), (13224116224, 5767168, 121110528, 0), (13229883392, 5767168, 126877696, 0), (13218349056, 5767168, 132644864, 0), (13258719232, 5767168, 138412032, 0), (13264486400, 5767168, 144179200, 0), (13252952064, 5767168, 149946368, 0), (13327925248, 5767168, 155713536, 0), (13333692416, 5767168, 161480704, 0), (13322158080, 5767168, 167247872, 0), (13362528256, 5767168, 173015040, 0), (13368295424, 5767168, 178782208, 0), (13356761088, 5767168, 184549376, 0), (13379829760, 5767168, 190316544, 0), (13385596928, 5767168, 196083712, 0), (13374062592, 5767168, 201850880, 0), (13431734272, 5767168, 207618048, 0), (13437501440, 5767168, 213385216, 0), (13425967104, 5767168, 219152384, 0), (13449035776, 5767168, 224919552, 0), (13454802944, 5767168, 230686720, 0), (13443268608, 5767168, 236453888, 0), (13500940288, 5767168, 242221056, 0), (13506707456, 5767168, 247988224, 0), (13495173120, 5767168, 253755392, 0), (13622050816, 5767168, 259522560, 0), (13627817984, 5767168, 265289728, 0), (13616283648, 5767168, 271056896, 0), (13725859840, 5767168, 276824064, 0), (13731627008, 5767168, 282591232, 0), (13720092672, 5767168, 288358400, 0), (13743161344, 5767168, 294125568, 0), (13748928512, 5767168, 299892736, 0), (13737394176, 5767168, 305659904, 0), (13829668864, 5767168, 311427072, 0), (13835436032, 5767168, 317194240, 0), (13823901696, 5767168, 322961408, 0)], 2: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (12964593664, 5767168, 17301504, 0), (12970360832, 5767168, 23068672, 0), (12958826496, 5767168, 28835840, 0), (13033799680, 5767168, 34603008, 0), (13039566848, 5767168, 40370176, 0), (13028032512, 5767168, 46137344, 0), (13051101184, 5767168, 51904512, 0), (13056868352, 5767168, 57671680, 0), (13045334016, 5767168, 63438848, 0), (13103005696, 5767168, 69206016, 0), (13108772864, 5767168, 74973184, 0), (13097238528, 5767168, 80740352, 0), (13172211712, 5767168, 86507520, 0), (13177978880, 5767168, 92274688, 0), (13166444544, 5767168, 98041856, 0), (13189513216, 5767168, 103809024, 0), (13195280384, 5767168, 109576192, 0), (13183746048, 5767168, 115343360, 0), (13241417728, 5767168, 121110528, 0), (13247184896, 5767168, 126877696, 0), (13235650560, 5767168, 132644864, 0), (13345226752, 5767168, 138412032, 0), (13350993920, 5767168, 144179200, 0), (13339459584, 5767168, 149946368, 0), (13397131264, 5767168, 155713536, 0), (13402898432, 5767168, 161480704, 0), (13391364096, 5767168, 167247872, 0), (13518241792, 5767168, 173015040, 0), (13524008960, 5767168, 178782208, 0), (13512474624, 5767168, 184549376, 0), (13535543296, 5767168, 190316544, 0), (13541310464, 5767168, 196083712, 0), (13529776128, 5767168, 201850880, 0), (13552844800, 5767168, 207618048, 0), (13558611968, 5767168, 213385216, 0), (13547077632, 5767168, 219152384, 0), (13587447808, 5767168, 224919552, 0), (13593214976, 5767168, 230686720, 0), (13581680640, 5767168, 236453888, 0), (13673955328, 5767168, 242221056, 0), (13679722496, 5767168, 247988224, 0), (13668188160, 5767168, 253755392, 0), (13691256832, 5767168, 259522560, 0), (13697024000, 5767168, 265289728, 0), (13685489664, 5767168, 271056896, 0), (13812367360, 5767168, 276824064, 0), (13818134528, 5767168, 282591232, 0), (13806600192, 5767168, 288358400, 0), (13846970368, 5767168, 294125568, 0), (13852737536, 5767168, 299892736, 0), (13841203200, 5767168, 305659904, 0), (13898874880, 5767168, 311427072, 0), (13904642048, 5767168, 317194240, 0), (13893107712, 5767168, 322961408, 0), (13916176384, 5767168, 328728576, 0), (13921943552, 5767168, 334495744, 0), (13910409216, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc762490>, 2: <capsule object NULL at 0x74a6bc7623a0>}
DEBUG 01-15 16:09:23.809528.809528 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.809206.809206 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.809531.809531 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d5a5a6fc-df00-4a95-82d4-5b2982cc763d
DEBUG 01-15 16:09:23.809538.809538 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.810596.810596 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:23.811938.811938 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.812383.812383 cuda_h.py:19] end move_flatidxs cost 0.0009632110595703125 seconds
DEBUG 01-15 16:09:23.812173.812173 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:23.812862.812862 client.py:127] Model loaded
DEBUG 01-15 16:09:23.812356.812356 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.813161.813161 cuda_h.py:19] end restore2model cost 0.0009958744049072266 seconds
DEBUG 01-15 16:09:23.814020.814020 cuda_h.py:19] end sllm_worker_task cost 0.016043901443481445 seconds
INFO 01-15 16:09:23.814322.814322 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d5a5a6fc-df00-4a95-82d4-5b2982cc763d
DEBUG 01-15 16:09:23.814429.814429 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00706028938293457 seconds
DEBUG 01-15 16:09:23.814704.814704 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.818696.818696 cuda_h.py:19] end restore2model cost 0.0038902759552001953 seconds
DEBUG 01-15 16:09:23.818738.818738 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.011238336563110352 seconds
DEBUG 01-15 16:09:23.818011.818011 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.819109.819109 cuda_h.py:19] end gpu_sexperts cost 0.00031948089599609375 seconds
DEBUG 01-15 16:09:23.819461.819461 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.819437.819437 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8835067749023438e-05 seconds
DEBUG 01-15 16:09:23.819802.819802 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.819127.819127 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.820277.820277 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012700557708740234 seconds
DEBUG 01-15 16:09:23.820180.820180 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.821473.821473 cuda_h.py:19] end gpu_group_list cost 0.00022149085998535156 seconds
DEBUG 01-15 16:09:23.822218.822218 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.821152.821152 cuda_h.py:19] end group_tensors cost 0.009807586669921875 seconds
DEBUG 01-15 16:09:23.822959.822959 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.823622.823622 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001535177230834961 seconds
DEBUG 01-15 16:09:23.823281.823281 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.824490.824490 cuda_h.py:19] end gpu_group_list cost 0.00025534629821777344 seconds
DEBUG 01-15 16:09:23.825189.825189 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.825840.825840 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d5a5a6fc-df00-4a95-82d4-5b2982cc763d
DEBUG 01-15 16:09:23.826734.826734 cuda_h.py:19] end group pad cost 0.003331422805786133 seconds
DEBUG 01-15 16:09:23.826378.826378 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:23.852209.852209 cuda_h.py:19] end group_einsum cost 0.026681184768676758 seconds
DEBUG 01-15 16:09:23.853273.853273 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.856409.856409 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029959678649902344 seconds
INFO 01-15 16:09:23.856025.856025 client.py:127] Model loaded
DEBUG 01-15 16:09:23.856024.856024 cuda_h.py:19] end wait_experts_multi_device cost 0.031188488006591797 seconds
DEBUG 01-15 16:09:23.856503.856503 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.857731.857731 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.046904802322387695 seconds
DEBUG 01-15 16:09:23.858708.858708 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0012745857238769531 seconds
DEBUG 01-15 16:09:23.858907.858907 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.860214.860214 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.860815.860815 cuda_h.py:19] end index_scatter cost 0.00015878677368164062 seconds
DEBUG 01-15 16:09:23.861531.861531 cuda_h.py:19] end cpuoutputsdeal cost 0.0031251907348632812 seconds
DEBUG 01-15 16:09:23.861504.861504 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.861035.861035 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.861283.861283 cuda_h.py:19] end gpu_group_tensor cost 0.00024437904357910156 seconds
DEBUG 01-15 16:09:23.861430.861430 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.862614.862614 cuda_h.py:19] end gpu_group_tensor cost 0.00014138221740722656 seconds
DEBUG 01-15 16:09:23.862148.862148 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.862502.862502 cuda_h.py:19] end gpu_group_einsum cost 0.0006330013275146484 seconds
DEBUG 01-15 16:09:23.863599.863599 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.863059.863059 cuda_h.py:19] end gpu_group_einsum cost 0.00041556358337402344 seconds
DEBUG 01-15 16:09:23.863109.863109 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.863483.863483 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.863855.863855 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019931793212890625 seconds
DEBUG 01-15 16:09:23.864327.864327 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.864296.864296 cuda_h.py:19] end concat_expert_out cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:23.864431.864431 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.864044.864044 cuda_h.py:19] end index_scatter cost 6.914138793945312e-05 seconds
DEBUG 01-15 16:09:23.864743.864743 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008780956268310547 seconds
DEBUG 01-15 16:09:23.864719.864719 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.864231.864231 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.864529.864529 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001494884490966797 seconds
DEBUG 01-15 16:09:23.864146.864146 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.865162.865162 cuda_h.py:19] end concat_expert_out cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:09:23.865097.865097 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.865882.865882 cuda_h.py:19] end index_scatter cost 5.340576171875e-05 seconds
DEBUG 01-15 16:09:23.865976.865976 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005049705505371094 seconds
DEBUG 01-15 16:09:23.865693.865693 cuda_h.py:19] end gpu_experts_multi_device cost 0.04578828811645508 seconds
DEBUG 01-15 16:09:23.865186.865186 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.06182742118835449 seconds
DEBUG 01-15 16:09:23.865879.865879 cuda_h.py:19] end prefill_layer cost 0.06832599639892578 seconds
DEBUG 01-15 16:09:23.865145.865145 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:09:23.865848.865848 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.865505.865505 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:09:23.865446.865446 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:09:23.866248.866248 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:09:23.866621.866621 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 3.647804260253906e-05 seconds
DEBUG 01-15 16:09:23.866947.866947 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 6.818771362304688e-05 seconds
DEBUG 01-15 16:09:23.866219.866219 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.866772.866772 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.866537.866537 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.866272.866272 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.866987.866987 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.867709.867709 cuda_h.py:19] end allocate_cuda_memory cost 0.0004856586456298828 seconds
DEBUG 01-15 16:09:23.867471.867471 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.867409.867409 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.867719.867719 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.867722.867722 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 76680056-4dae-44d2-be04-0e6906e4d89e
DEBUG 01-15 16:09:23.868717.868717 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.868050.868050 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.869339.869339 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 76680056-4dae-44d2-be04-0e6906e4d89e
DEBUG 01-15 16:09:23.869694.869694 cuda_h.py:19] end load_into_gpu_async cost 0.001802206039428711 seconds
DEBUG 01-15 16:09:23.869477.869477 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.869873.869873 cuda_h.py:19] end restore_tensors2 cost 0.00010347366333007812 seconds
DEBUG 01-15 16:09:23.869901.869901 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0030524730682373047 seconds
INFO 01-15 16:09:23.869653.869653 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 76680056-4dae-44d2-be04-0e6906e4d89e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.872074.872074 cuda_h.py:19] end self_attn cost 0.0037946701049804688 seconds
DEBUG 01-15 16:09:23.872416.872416 cuda_h.py:19] end iln_self_attn_paln cost 0.00642848014831543 seconds
DEBUG 01-15 16:09:23.872530.872530 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:09:23.872478.872478 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.873867.873867 cuda_h.py:19] end gate cost 0.000705718994140625 seconds
DEBUG 01-15 16:09:23.873935.873935 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.873330.873330 lmp.py:1912] 
DEBUG 01-15 16:09:23.873330.873330 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.873086.873086 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.873782.873782 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.873141.873141 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.873830.873830 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.873804.873804 lmp.py:1917] 
DEBUG 01-15 16:09:23.873804.873804 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.873255.873255 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.873381.873381 lmp.py:1935]   Expert 39 |     15 | CPU
DEBUG 01-15 16:09:23.873309.873309 lmp.py:1935]   Expert 13 |     16 | CPU
DEBUG 01-15 16:09:23.873522.873522 lmp.py:1935]   Expert 49 |     38 | CPU
DEBUG 01-15 16:09:23.873257.873257 lmp.py:1935]   Expert 35 |     54 | CPU
DEBUG 01-15 16:09:23.874993.874993 lmp.py:1935]   Expert 19 |     62 | CPU
DEBUG 01-15 16:09:23.874490.874490 lmp.py:1935]   Expert 26 |     73 | CPU
DEBUG 01-15 16:09:23.874703.874703 lmp.py:1935]   Expert  9 |     74 | CPU
DEBUG 01-15 16:09:23.874961.874961 lmp.py:1935]   Expert 32 |     76 | CPU
DEBUG 01-15 16:09:23.874935.874935 lmp.py:1935]   Expert 41 |     76 | CPU
DEBUG 01-15 16:09:23.874194.874194 lmp.py:1935]   Expert 33 |     80 | CPU
DEBUG 01-15 16:09:23.874930.874930 lmp.py:1935]   Expert 23 |     87 | CPU
DEBUG 01-15 16:09:23.874189.874189 lmp.py:1935]   Expert 18 |     91 | CPU
DEBUG 01-15 16:09:23.874924.874924 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:09:23.874183.874183 lmp.py:1935]   Expert 46 |     91 | CPU
DEBUG 01-15 16:09:23.874919.874919 lmp.py:1935]   Expert 38 |     94 | CPU
DEBUG 01-15 16:09:23.874416.874416 lmp.py:1935]   Expert  3 |    102 | CPU
DEBUG 01-15 16:09:23.874390.874390 lmp.py:1935]   Expert  6 |    103 | CPU
DEBUG 01-15 16:09:23.874887.874887 lmp.py:1935]   Expert 17 |    105 | CPU
DEBUG 01-15 16:09:23.874623.874623 lmp.py:1935]   Expert 20 |    118 | CPU
DEBUG 01-15 16:09:23.874027.874027 lmp.py:1935]   Expert 40 |    129 | CPU
DEBUG 01-15 16:09:23.874670.874670 lmp.py:1935]   Expert 61 |    132 | CPU
DEBUG 01-15 16:09:23.874121.874121 lmp.py:1935]   Expert 62 |    132 | CPU
DEBUG 01-15 16:09:23.874810.874810 lmp.py:1935]   Expert 15 |    133 | CPU
DEBUG 01-15 16:09:23.874261.874261 lmp.py:1935]   Expert 43 |    134 | CPU
DEBUG 01-15 16:09:23.874712.874712 lmp.py:1935]   Expert 44 |    136 | CPU
DEBUG 01-15 16:09:23.874070.874070 lmp.py:1935]   Expert 16 |    137 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874190.874190 lmp.py:1935]   Expert 50 |    137 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874833.874833 lmp.py:1935]   Expert 59 |    138 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874476.874476 lmp.py:1935]   Expert 63 |    139 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874881.874881 lmp.py:1935]   Expert 42 |    144 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874285.874285 lmp.py:1935]   Expert  2 |    146 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874690.874690 lmp.py:1935]   Expert 36 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874571.874571 lmp.py:1935]   Expert 10 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874214.874214 lmp.py:1935]   Expert  5 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874096.874096 lmp.py:1935]   Expert 34 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874500.874500 lmp.py:1935]   Expert 52 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874382.874382 lmp.py:1935]   Expert 27 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874025.874025 lmp.py:1935]   Expert 45 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874429.874429 lmp.py:1935]   Expert 60 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874595.874595 lmp.py:1935]   Expert 48 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874238.874238 lmp.py:1935]   Expert 56 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874643.874643 lmp.py:1935]   Expert 51 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874286.874286 lmp.py:1935]   Expert 24 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874690.874690 lmp.py:1935]   Expert 53 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874095.874095 lmp.py:1935]   Expert  7 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874261.874261 lmp.py:1935]   Expert  8 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874904.874904 lmp.py:1935]   Expert 57 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874309.874309 lmp.py:1935]   Expert 47 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874713.874713 lmp.py:1935]   Expert 21 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874118.874118 lmp.py:1935]   Expert 29 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874761.874761 lmp.py:1935]   Expert  0 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874165.874165 lmp.py:1935]   Expert  4 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874047.874047 lmp.py:1935]   Expert 14 |    289 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874451.874451 lmp.py:1935]   Expert 22 |    315 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874379.874379 lmp.py:1935]   Expert 55 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874022.874022 lmp.py:1935]   Expert 58 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874665.874665 lmp.py:1935]   Expert 37 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874069.874069 lmp.py:1935]   Expert  1 |    324 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874712.874712 lmp.py:1935]   Expert 54 |    330 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874878.874878 lmp.py:1935]   Expert 28 |    360 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.874283.874283 lmp.py:1935]   Expert 12 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874688.874688 lmp.py:1935]   Expert 11 |    402 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.874092.874092 lmp.py:1935]   Expert 25 |    402 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.875735.875735 lmp.py:1935]   Expert 30 |    833 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.875948.875948 lmp.py:1937] 
DEBUG 01-15 16:09:23.875948.875948 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.875591.875591 lmp.py:1938]   CPU:   2242 tokens
DEBUG 01-15 16:09:23.875472.875472 lmp.py:1942]   cuda:1:   5024 tokens (19 experts)
DEBUG 01-15 16:09:23.875877.875877 lmp.py:1942]   cuda:2:   5022 tokens (20 experts)
DEBUG 01-15 16:09:23.875804.875804 lmp.py:1943]   Total GPU:  10046 tokens
DEBUG 01-15 16:09:23.875017.875017 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.875017.875017 lmp.py:1944] 
DEBUG 01-15 16:09:23.875428.875428 cuda_h.py:19] end experts_map_get cost 0.001636505126953125 seconds
DEBUG 01-15 16:09:23.875940.875940 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.875550.875550 lmp.py:1953] 
DEBUG 01-15 16:09:23.875550.875550 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.875764.875764 cuda_h.py:19] end cpu_experts_submit cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:09:23.875573.875573 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.875932.875932 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.876605.876605 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.1.gate_proj.weight': 0, 'model.layers.11.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.42.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.42.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.42.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.48.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.48.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.48.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.60.up_proj.weight': 322961408}, 2: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.36.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.36.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.36.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.11.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.11.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.11.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.11.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.11.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.11.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.11.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.11.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.11.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(13950779392, 5767168, 0, 0), (13956546560, 5767168, 5767168, 0), (13945012224, 5767168, 11534336, 0), (14019985408, 5767168, 17301504, 0), (14025752576, 5767168, 23068672, 0), (14014218240, 5767168, 28835840, 0), (14054588416, 5767168, 34603008, 0), (14060355584, 5767168, 40370176, 0), (14048821248, 5767168, 46137344, 0), (14106492928, 5767168, 51904512, 0), (14112260096, 5767168, 57671680, 0), (14100725760, 5767168, 63438848, 0), (14175698944, 5767168, 69206016, 0), (14181466112, 5767168, 74973184, 0), (14169931776, 5767168, 80740352, 0), (14296809472, 5767168, 86507520, 0), (14302576640, 5767168, 92274688, 0), (14291042304, 5767168, 98041856, 0), (14314110976, 5767168, 103809024, 0), (14319878144, 5767168, 109576192, 0), (14308343808, 5767168, 115343360, 0), (14348713984, 5767168, 121110528, 0), (14354481152, 5767168, 126877696, 0), (14342946816, 5767168, 132644864, 0), (14417920000, 5767168, 138412032, 0), (14423687168, 5767168, 144179200, 0), (14412152832, 5767168, 149946368, 0), (14435221504, 5767168, 155713536, 0), (14440988672, 5767168, 161480704, 0), (14429454336, 5767168, 167247872, 0), (14452523008, 5767168, 173015040, 0), (14458290176, 5767168, 178782208, 0), (14446755840, 5767168, 184549376, 0), (14660141056, 5767168, 190316544, 0), (14665908224, 5767168, 196083712, 0), (14654373888, 5767168, 201850880, 0), (14763950080, 5767168, 207618048, 0), (14769717248, 5767168, 213385216, 0), (14758182912, 5767168, 219152384, 0), (14798553088, 5767168, 224919552, 0), (14804320256, 5767168, 230686720, 0), (14792785920, 5767168, 236453888, 0), (14833156096, 5767168, 242221056, 0), (14838923264, 5767168, 247988224, 0), (14827388928, 5767168, 253755392, 0), (14885060608, 5767168, 259522560, 0), (14890827776, 5767168, 265289728, 0), (14879293440, 5767168, 271056896, 0), (14919663616, 5767168, 276824064, 0), (14925430784, 5767168, 282591232, 0), (14913896448, 5767168, 288358400, 0), (14954266624, 5767168, 294125568, 0), (14960033792, 5767168, 299892736, 0), (14948499456, 5767168, 305659904, 0), (14971568128, 5767168, 311427072, 0), (14977335296, 5767168, 317194240, 0), (14965800960, 5767168, 322961408, 0)], 2: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (13968080896, 5767168, 17301504, 0), (13973848064, 5767168, 23068672, 0), (13962313728, 5767168, 28835840, 0), (14002683904, 5767168, 34603008, 0), (14008451072, 5767168, 40370176, 0), (13996916736, 5767168, 46137344, 0), (14071889920, 5767168, 51904512, 0), (14077657088, 5767168, 57671680, 0), (14066122752, 5767168, 63438848, 0), (14123794432, 5767168, 69206016, 0), (14129561600, 5767168, 74973184, 0), (14118027264, 5767168, 80740352, 0), (14141095936, 5767168, 86507520, 0), (14146863104, 5767168, 92274688, 0), (14135328768, 5767168, 98041856, 0), (14210301952, 5767168, 103809024, 0), (14216069120, 5767168, 109576192, 0), (14204534784, 5767168, 115343360, 0), (14366015488, 5767168, 121110528, 0), (14371782656, 5767168, 126877696, 0), (14360248320, 5767168, 132644864, 0), (14400618496, 5767168, 138412032, 0), (14406385664, 5767168, 144179200, 0), (14394851328, 5767168, 149946368, 0), (14521729024, 5767168, 155713536, 0), (14527496192, 5767168, 161480704, 0), (14515961856, 5767168, 167247872, 0), (14556332032, 5767168, 173015040, 0), (14562099200, 5767168, 178782208, 0), (14550564864, 5767168, 184549376, 0), (14573633536, 5767168, 190316544, 0), (14579400704, 5767168, 196083712, 0), (14567866368, 5767168, 201850880, 0), (14712045568, 5767168, 207618048, 0), (14717812736, 5767168, 213385216, 0), (14706278400, 5767168, 219152384, 0), (14746648576, 5767168, 224919552, 0), (14752415744, 5767168, 230686720, 0), (14740881408, 5767168, 236453888, 0), (14815854592, 5767168, 242221056, 0), (14821621760, 5767168, 247988224, 0), (14810087424, 5767168, 253755392, 0), (14850457600, 5767168, 259522560, 0), (14856224768, 5767168, 265289728, 0), (14844690432, 5767168, 271056896, 0), (14867759104, 5767168, 276824064, 0), (14873526272, 5767168, 282591232, 0), (14861991936, 5767168, 288358400, 0), (14902362112, 5767168, 294125568, 0), (14908129280, 5767168, 299892736, 0), (14896594944, 5767168, 305659904, 0), (14936965120, 5767168, 311427072, 0), (14942732288, 5767168, 317194240, 0), (14931197952, 5767168, 322961408, 0), (15023472640, 5767168, 328728576, 0), (15029239808, 5767168, 334495744, 0), (15017705472, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6883d19b0>, 2: <capsule object NULL at 0x74a680699a40>}
DEBUG 01-15 16:09:23.876579.876579 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:09:23.876953.876953 client.py:127] Model loaded
DEBUG 01-15 16:09:23.876228.876228 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.876305.876305 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c06be59d-02be-4be5-aede-ffd80e880a99
DEBUG 01-15 16:09:23.876375.876375 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:23.877756.877756 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.876071.876071 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.877730.877730 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.877132.877132 cuda_h.py:19] end restore2model cost 0.0007917881011962891 seconds
DEBUG 01-15 16:09:23.878122.878122 cuda_h.py:19] end sllm_worker_task cost 0.011565446853637695 seconds
DEBUG 01-15 16:09:23.878137.878137 cuda_h.py:19] end move_flatidxs cost 0.0008399486541748047 seconds
DEBUG 01-15 16:09:23.878960.878960 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:23.878916.878916 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c06be59d-02be-4be5-aede-ffd80e880a99
DEBUG 01-15 16:09:23.878046.878046 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0033485889434814453 seconds
DEBUG 01-15 16:09:23.878605.878605 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.881062.881062 cuda_h.py:19] end restore2model cost 0.003110170364379883 seconds
DEBUG 01-15 16:09:23.882719.882719 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006722450256347656 seconds
DEBUG 01-15 16:09:23.882230.882230 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.882830.882830 cuda_h.py:19] end gpu_sexperts cost 0.0002694129943847656 seconds
DEBUG 01-15 16:09:23.882229.882229 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.882343.882343 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.0503997802734375e-05 seconds
DEBUG 01-15 16:09:23.882754.882754 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.882934.882934 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.883582.883582 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009393692016601562 seconds
DEBUG 01-15 16:09:23.883902.883902 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.883722.883722 cuda_h.py:19] end group_tensors cost 0.004914760589599609 seconds
DEBUG 01-15 16:09:23.883420.883420 cuda_h.py:19] end gpu_group_list cost 0.0002117156982421875 seconds
DEBUG 01-15 16:09:23.883786.883786 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.884789.884789 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.886272.886272 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014963150024414062 seconds
DEBUG 01-15 16:09:23.886116.886116 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.886314.886314 cuda_h.py:19] end gpu_group_list cost 0.00031757354736328125 seconds
DEBUG 01-15 16:09:23.887314.887314 cuda_h.py:19] end group pad cost 0.003635406494140625 seconds
DEBUG 01-15 16:09:23.887912.887912 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:23.888619.888619 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.888333.888333 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c06be59d-02be-4be5-aede-ffd80e880a99
DEBUG 01-15 16:09:23.907311.907311 cuda_h.py:19] end group_einsum cost 0.01994013786315918 seconds
DEBUG 01-15 16:09:23.907098.907098 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.910460.910460 cuda_h.py:19] end get_outputs_cpu1 cost 0.0030565261840820312 seconds
DEBUG 01-15 16:09:23.911008.911008 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03484344482421875 seconds
INFO 01-15 16:09:23.917085.917085 client.py:127] Model loaded
DEBUG 01-15 16:09:23.917493.917493 cuda_h.py:19] end wait_experts_multi_device cost 0.028748512268066406 seconds
DEBUG 01-15 16:09:23.917686.917686 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.918850.918850 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005037784576416016 seconds
DEBUG 01-15 16:09:23.918494.918494 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.919839.919839 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.919241.919241 cuda_h.py:19] end index_scatter cost 7.557868957519531e-05 seconds
DEBUG 01-15 16:09:23.919985.919985 cuda_h.py:19] end cpuoutputsdeal cost 0.0013799667358398438 seconds
DEBUG 01-15 16:09:23.919418.919418 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.919512.919512 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.919002.919002 cuda_h.py:19] end gpu_group_tensor cost 0.00015544891357421875 seconds
DEBUG 01-15 16:09:23.919334.919334 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.919658.919658 cuda_h.py:19] end gpu_group_tensor cost 0.0001392364501953125 seconds
DEBUG 01-15 16:09:23.920840.920840 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.920692.920692 cuda_h.py:19] end gpu_group_einsum cost 0.0006830692291259766 seconds
DEBUG 01-15 16:09:23.920320.920320 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.921422.921422 cuda_h.py:19] end gpu_group_einsum cost 0.00043773651123046875 seconds
DEBUG 01-15 16:09:23.921757.921757 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.921462.921462 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.921059.921059 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001926422119140625 seconds
DEBUG 01-15 16:09:23.921531.921531 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.921924.921924 cuda_h.py:19] end concat_expert_out cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:09:23.922674.922674 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.922121.922121 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:23.922294.922294 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007734298706054688 seconds
DEBUG 01-15 16:09:23.922701.922701 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.922366.922366 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.922895.922895 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001480579376220703 seconds
DEBUG 01-15 16:09:23.922744.922744 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.922151.922151 cuda_h.py:19] end concat_expert_out cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:09:23.922848.922848 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.922579.922579 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:09:23.923673.923673 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005040168762207031 seconds
DEBUG 01-15 16:09:23.923490.923490 cuda_h.py:19] end gpu_experts_multi_device cost 0.04054856300354004 seconds
DEBUG 01-15 16:09:23.923923.923923 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.05047106742858887 seconds
DEBUG 01-15 16:09:23.923468.923468 cuda_h.py:19] end prefill_layer cost 0.057622432708740234 seconds
DEBUG 01-15 16:09:23.923873.923873 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:09:23.923053.923053 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.923047.923047 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:09:23.923989.923989 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:09:23.923314.923314 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:09:23.923740.923740 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 4.029273986816406e-05 seconds
DEBUG 01-15 16:09:23.923304.923304 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:09:23.923716.923716 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.923725.923725 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.924781.924781 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.924563.924563 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.924807.924807 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.924918.924918 cuda_h.py:19] end allocate_cuda_memory cost 0.0004050731658935547 seconds
DEBUG 01-15 16:09:23.925063.925063 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.925722.925722 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.925090.925090 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.925286.925286 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 39260e9e-ba65-46ad-a79f-02498db06681
DEBUG 01-15 16:09:23.925863.925863 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.925181.925181 cuda_h.py:10] start self_attn
INFO 01-15 16:09:23.926707.926707 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 39260e9e-ba65-46ad-a79f-02498db06681
DEBUG 01-15 16:09:23.926931.926931 cuda_h.py:19] end load_into_gpu_async cost 0.0016994476318359375 seconds
DEBUG 01-15 16:09:23.927484.927484 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:23.927612.927612 cuda_h.py:19] end restore_tensors2 cost 0.0001621246337890625 seconds
DEBUG 01-15 16:09:23.927053.927053 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.00305938720703125 seconds
INFO 01-15 16:09:23.927833.927833 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 39260e9e-ba65-46ad-a79f-02498db06681
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:23.932751.932751 cuda_h.py:19] end self_attn cost 0.0062716007232666016 seconds
DEBUG 01-15 16:09:23.932695.932695 cuda_h.py:19] end iln_self_attn_paln cost 0.008835792541503906 seconds
DEBUG 01-15 16:09:23.932246.932246 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:09:23.932148.932148 cuda_h.py:10] start gate
DEBUG 01-15 16:09:23.933447.933447 cuda_h.py:19] end gate cost 0.0007839202880859375 seconds
DEBUG 01-15 16:09:23.933422.933422 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:23.934177.934177 lmp.py:1912] 
DEBUG 01-15 16:09:23.934177.934177 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:23.934078.934078 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:23.934397.934397 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:23.934424.934424 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:23.934544.934544 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:23.934995.934995 lmp.py:1917] 
DEBUG 01-15 16:09:23.934995.934995 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:23.934684.934684 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:23.934095.934095 lmp.py:1935]   Expert 12 |     17 | CPU
DEBUG 01-15 16:09:23.934500.934500 lmp.py:1935]   Expert 47 |     24 | CPU
DEBUG 01-15 16:09:23.934951.934951 lmp.py:1935]   Expert 38 |     31 | CPU
DEBUG 01-15 16:09:23.934640.934640 lmp.py:1935]   Expert 27 |     33 | CPU
DEBUG 01-15 16:09:23.934091.934091 lmp.py:1935]   Expert 16 |     37 | CPU
DEBUG 01-15 16:09:23.934542.934542 lmp.py:1935]   Expert 52 |     38 | CPU
DEBUG 01-15 16:09:23.934754.934754 lmp.py:1935]   Expert 63 |     44 | CPU
DEBUG 01-15 16:09:23.934490.934490 lmp.py:1935]   Expert  4 |     59 | CPU
DEBUG 01-15 16:09:23.934464.934464 lmp.py:1935]   Expert 44 |     63 | CPU
DEBUG 01-15 16:09:23.934961.934961 lmp.py:1935]   Expert 43 |     64 | CPU
DEBUG 01-15 16:09:23.934459.934459 lmp.py:1935]   Expert 61 |     65 | CPU
DEBUG 01-15 16:09:23.934671.934671 lmp.py:1935]   Expert 34 |     77 | CPU
DEBUG 01-15 16:09:23.934360.934360 lmp.py:1935]   Expert 53 |     83 | CPU
DEBUG 01-15 16:09:23.934573.934573 lmp.py:1935]   Expert 32 |     88 | CPU
DEBUG 01-15 16:09:23.934785.934785 lmp.py:1935]   Expert  0 |     89 | CPU
DEBUG 01-15 16:09:23.934428.934428 lmp.py:1935]   Expert 37 |     91 | CPU
DEBUG 01-15 16:09:23.934594.934594 lmp.py:1935]   Expert 13 |    103 | CPU
DEBUG 01-15 16:09:23.934522.934522 lmp.py:1935]   Expert 39 |    115 | CPU
DEBUG 01-15 16:09:23.934211.934211 lmp.py:1935]   Expert 21 |    116 | CPU
DEBUG 01-15 16:09:23.934377.934377 lmp.py:1935]   Expert 11 |    120 | CPU
DEBUG 01-15 16:09:23.934259.934259 lmp.py:1935]   Expert 20 |    126 | CPU
DEBUG 01-15 16:09:23.934948.934948 lmp.py:1935]   Expert  8 |    127 | CPU
DEBUG 01-15 16:09:23.934637.934637 lmp.py:1935]   Expert 60 |    132 | CPU
DEBUG 01-15 16:09:23.934565.934565 lmp.py:1935]   Expert 57 |    138 | CPU
DEBUG 01-15 16:09:23.934493.934493 lmp.py:1935]   Expert 14 |    140 | CPU
DEBUG 01-15 16:09:23.934328.934328 lmp.py:1935]   Expert 22 |    141 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.934401.934401 lmp.py:1935]   Expert 45 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934760.934760 lmp.py:1935]   Expert  2 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.934880.934880 lmp.py:1935]   Expert 17 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.934238.934238 lmp.py:1935]   Expert 23 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934119.934119 lmp.py:1935]   Expert 18 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.934762.934762 lmp.py:1935]   Expert  7 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934167.934167 lmp.py:1935]   Expert 58 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.934810.934810 lmp.py:1935]   Expert 30 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934453.934453 lmp.py:1935]   Expert 42 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934096.934096 lmp.py:1935]   Expert 48 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.934739.934739 lmp.py:1935]   Expert 55 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.934858.934858 lmp.py:1935]   Expert 62 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934217.934217 lmp.py:1935]   Expert 49 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934860.934860 lmp.py:1935]   Expert 35 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.934503.934503 lmp.py:1935]   Expert 51 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934907.934907 lmp.py:1935]   Expert  6 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934312.934312 lmp.py:1935]   Expert 29 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.934955.934955 lmp.py:1935]   Expert 25 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.934836.934836 lmp.py:1935]   Expert 36 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935717.935717 lmp.py:1935]   Expert  1 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935599.935599 lmp.py:1935]   Expert 31 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935003.935003 lmp.py:1935]   Expert 28 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.935123.935123 lmp.py:1935]   Expert  5 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.935766.935766 lmp.py:1935]   Expert 41 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.935171.935171 lmp.py:1935]   Expert 54 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935052.935052 lmp.py:1935]   Expert 19 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935172.935172 lmp.py:1935]   Expert  9 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935815.935815 lmp.py:1935]   Expert 24 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.935696.935696 lmp.py:1935]   Expert 50 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.935339.935339 lmp.py:1935]   Expert 46 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935982.935982 lmp.py:1935]   Expert 59 |    312 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.935864.935864 lmp.py:1935]   Expert 56 |    374 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935268.935268 lmp.py:1935]   Expert 26 |    407 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.935819.935819 lmp.py:1935]   Expert 33 |    423 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935654.935654 lmp.py:1935]   Expert  3 |    586 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.935297.935297 lmp.py:1935]   Expert 10 |    645 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935940.935940 lmp.py:1935]   Expert 15 |    647 | GPU2(cuda:2)
DEBUG 01-15 16:09:23.935583.935583 lmp.py:1935]   Expert 40 |    794 | GPU1(cuda:1)
DEBUG 01-15 16:09:23.935795.935795 lmp.py:1937] 
DEBUG 01-15 16:09:23.935795.935795 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:23.935438.935438 lmp.py:1938]   CPU:   2020 tokens
DEBUG 01-15 16:09:23.935319.935319 lmp.py:1942]   cuda:1:   5064 tokens (19 experts)
DEBUG 01-15 16:09:23.935439.935439 lmp.py:1942]   cuda:2:   5204 tokens (20 experts)
DEBUG 01-15 16:09:23.935321.935321 lmp.py:1943]   Total GPU:  10268 tokens
DEBUG 01-15 16:09:23.935248.935248 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:23.935248.935248 lmp.py:1944] 
DEBUG 01-15 16:09:23.935136.935136 cuda_h.py:19] end experts_map_get cost 0.0017273426055908203 seconds
INFO 01-15 16:09:23.935101.935101 client.py:127] Model loaded
DEBUG 01-15 16:09:23.935448.935448 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.935977.935977 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:23.935237.935237 lmp.py:1953] 
DEBUG 01-15 16:09:23.935237.935237 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:23.936976.936976 cuda_h.py:19] end cpu_experts_submit cost 0.00012540817260742188 seconds
DEBUG 01-15 16:09:23.936248.936248 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:23.936376.936376 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:23.937751.937751 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.3.gate_proj.weight': 0, 'model.layers.12.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.23.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.23.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.23.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.12.mlp.experts.1.gate_proj.weight': 0, 'model.layers.12.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.36.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.36.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.36.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.12.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.12.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.12.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.12.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.12.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.12.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.12.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.12.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.12.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.12.mlp.experts.58.gate_proj.weight': 328728576, 'model.layers.12.mlp.experts.58.down_proj.weight': 334495744, 'model.layers.12.mlp.experts.58.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(15092678656, 5767168, 0, 0), (15098445824, 5767168, 5767168, 0), (15086911488, 5767168, 11534336, 0), (15127281664, 5767168, 17301504, 0), (15133048832, 5767168, 23068672, 0), (15121514496, 5767168, 28835840, 0), (15144583168, 5767168, 34603008, 0), (15150350336, 5767168, 40370176, 0), (15138816000, 5767168, 46137344, 0), (15161884672, 5767168, 51904512, 0), (15167651840, 5767168, 57671680, 0), (15156117504, 5767168, 63438848, 0), (15438708736, 5767168, 69206016, 0), (15444475904, 5767168, 74973184, 0), (15432941568, 5767168, 80740352, 0), (15456010240, 5767168, 86507520, 0), (15461777408, 5767168, 92274688, 0), (15450243072, 5767168, 98041856, 0), (15473311744, 5767168, 103809024, 0), (15479078912, 5767168, 109576192, 0), (15467544576, 5767168, 115343360, 0), (15490613248, 5767168, 121110528, 0), (15496380416, 5767168, 126877696, 0), (15484846080, 5767168, 132644864, 0), (15525216256, 5767168, 138412032, 0), (15530983424, 5767168, 144179200, 0), (15519449088, 5767168, 149946368, 0), (15559819264, 5767168, 155713536, 0), (15565586432, 5767168, 161480704, 0), (15554052096, 5767168, 167247872, 0), (15732834304, 5767168, 173015040, 0), (15738601472, 5767168, 178782208, 0), (15727067136, 5767168, 184549376, 0), (15750135808, 5767168, 190316544, 0), (15755902976, 5767168, 196083712, 0), (15744368640, 5767168, 201850880, 0), (15767437312, 5767168, 207618048, 0), (15773204480, 5767168, 213385216, 0), (15761670144, 5767168, 219152384, 0), (15819341824, 5767168, 224919552, 0), (15825108992, 5767168, 230686720, 0), (15813574656, 5767168, 236453888, 0), (15888547840, 5767168, 242221056, 0), (15894315008, 5767168, 247988224, 0), (15882780672, 5767168, 253755392, 0), (15905849344, 5767168, 259522560, 0), (15911616512, 5767168, 265289728, 0), (15900082176, 5767168, 271056896, 0), (15923150848, 5767168, 276824064, 0), (15928918016, 5767168, 282591232, 0), (15917383680, 5767168, 288358400, 0), (16061562880, 5767168, 294125568, 0), (16067330048, 5767168, 299892736, 0), (16055795712, 5767168, 305659904, 0), (16113467392, 5767168, 311427072, 0), (16119234560, 5767168, 317194240, 0), (16107700224, 5767168, 322961408, 0)], 2: [(15058075648, 5767168, 0, 0), (15063842816, 5767168, 5767168, 0), (15052308480, 5767168, 11534336, 0), (15075377152, 5767168, 17301504, 0), (15081144320, 5767168, 23068672, 0), (15069609984, 5767168, 28835840, 0), (15196487680, 5767168, 34603008, 0), (15202254848, 5767168, 40370176, 0), (15190720512, 5767168, 46137344, 0), (15213789184, 5767168, 51904512, 0), (15219556352, 5767168, 57671680, 0), (15208022016, 5767168, 63438848, 0), (15300296704, 5767168, 69206016, 0), (15306063872, 5767168, 74973184, 0), (15294529536, 5767168, 80740352, 0), (15334899712, 5767168, 86507520, 0), (15340666880, 5767168, 92274688, 0), (15329132544, 5767168, 98041856, 0), (15352201216, 5767168, 103809024, 0), (15357968384, 5767168, 109576192, 0), (15346434048, 5767168, 115343360, 0), (15369502720, 5767168, 121110528, 0), (15375269888, 5767168, 126877696, 0), (15363735552, 5767168, 132644864, 0), (15421407232, 5767168, 138412032, 0), (15427174400, 5767168, 144179200, 0), (15415640064, 5767168, 149946368, 0), (15542517760, 5767168, 155713536, 0), (15548284928, 5767168, 161480704, 0), (15536750592, 5767168, 167247872, 0), (15577120768, 5767168, 173015040, 0), (15582887936, 5767168, 178782208, 0), (15571353600, 5767168, 184549376, 0), (15611723776, 5767168, 190316544, 0), (15617490944, 5767168, 196083712, 0), (15605956608, 5767168, 201850880, 0), (15646326784, 5767168, 207618048, 0), (15652093952, 5767168, 213385216, 0), (15640559616, 5767168, 219152384, 0), (15663628288, 5767168, 224919552, 0), (15669395456, 5767168, 230686720, 0), (15657861120, 5767168, 236453888, 0), (15836643328, 5767168, 242221056, 0), (15842410496, 5767168, 247988224, 0), (15830876160, 5767168, 253755392, 0), (15871246336, 5767168, 259522560, 0), (15877013504, 5767168, 265289728, 0), (15865479168, 5767168, 271056896, 0), (15975055360, 5767168, 276824064, 0), (15980822528, 5767168, 282591232, 0), (15969288192, 5767168, 288358400, 0), (15992356864, 5767168, 294125568, 0), (15998124032, 5767168, 299892736, 0), (15986589696, 5767168, 305659904, 0), (16009658368, 5767168, 311427072, 0), (16015425536, 5767168, 317194240, 0), (16003891200, 5767168, 322961408, 0), (16044261376, 5767168, 328728576, 0), (16050028544, 5767168, 334495744, 0), (16038494208, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc7624c0>, 2: <capsule object NULL at 0x74a6bc762310>}
DEBUG 01-15 16:09:23.937000.937000 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.937803.937803 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.937274.937274 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b3def524-d077-4f76-92c3-15a0ede36c65
DEBUG 01-15 16:09:23.937495.937495 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.937946.937946 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:23.938111.938111 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:23.938948.938948 cuda_h.py:19] end restore2model cost 0.0026328563690185547 seconds
DEBUG 01-15 16:09:23.938064.938064 cuda_h.py:19] end sllm_worker_task cost 0.014188289642333984 seconds
INFO 01-15 16:09:23.939969.939969 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b3def524-d077-4f76-92c3-15a0ede36c65
DEBUG 01-15 16:09:23.939369.939369 cuda_h.py:19] end move_flatidxs cost 0.0009090900421142578 seconds
DEBUG 01-15 16:09:23.939372.939372 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:23.940138.940138 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004313945770263672 seconds
DEBUG 01-15 16:09:23.940764.940764 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:23.943290.943290 cuda_h.py:19] end restore2model cost 0.0031948089599609375 seconds
DEBUG 01-15 16:09:23.943134.943134 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007825851440429688 seconds
DEBUG 01-15 16:09:23.943883.943883 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:23.944901.944901 cuda_h.py:19] end gpu_sexperts cost 0.0002970695495605469 seconds
DEBUG 01-15 16:09:23.944969.944969 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:23.944083.944083 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.9550323486328125e-05 seconds
DEBUG 01-15 16:09:23.944209.944209 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:23.944211.944211 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.945338.945338 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010101795196533203 seconds
DEBUG 01-15 16:09:23.945950.945950 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.945361.945361 cuda_h.py:19] end gpu_group_list cost 0.00020360946655273438 seconds
DEBUG 01-15 16:09:23.946325.946325 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:23.948358.948358 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013318061828613281 seconds
DEBUG 01-15 16:09:23.948746.948746 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:23.948801.948801 cuda_h.py:19] end gpu_group_list cost 0.00021958351135253906 seconds
DEBUG 01-15 16:09:23.949052.949052 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:23.949034.949034 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b3def524-d077-4f76-92c3-15a0ede36c65
DEBUG 01-15 16:09:23.951430.951430 cuda_h.py:19] end group_tensors cost 0.01196599006652832 seconds
DEBUG 01-15 16:09:23.952861.952861 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:23.956363.956363 cuda_h.py:19] end group pad cost 0.0038657188415527344 seconds
DEBUG 01-15 16:09:23.956768.956768 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:23.981550.981550 client.py:127] Model loaded
DEBUG 01-15 16:09:23.981373.981373 cuda_h.py:19] end wait_experts_multi_device cost 0.03219342231750488 seconds
DEBUG 01-15 16:09:23.981259.981259 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:23.985737.985737 cuda_h.py:19] end group_einsum cost 0.029041051864624023 seconds
DEBUG 01-15 16:09:23.985682.985682 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:23.990235.990235 cuda_h.py:19] end get_outputs_cpu1 cost 0.004987478256225586 seconds
DEBUG 01-15 16:09:23.991303.991303 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05330967903137207 seconds
DEBUG 01-15 16:09:23.992256.992256 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010301589965820312 seconds
DEBUG 01-15 16:09:23.992286.992286 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:23.993743.993743 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.993212.993212 cuda_h.py:19] end index_scatter cost 8.487701416015625e-05 seconds
DEBUG 01-15 16:09:23.993766.993766 cuda_h.py:19] end cpuoutputsdeal cost 0.0016491413116455078 seconds
DEBUG 01-15 16:09:23.993444.993444 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:23.993538.993538 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.994081.994081 cuda_h.py:19] end gpu_group_tensor cost 0.00015854835510253906 seconds
DEBUG 01-15 16:09:23.994797.994797 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:23.994512.994512 cuda_h.py:19] end gpu_group_tensor cost 0.00014519691467285156 seconds
DEBUG 01-15 16:09:23.994131.994131 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.995301.995301 cuda_h.py:19] end gpu_group_einsum cost 0.0006706714630126953 seconds
DEBUG 01-15 16:09:23.995472.995472 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:23.995642.995642 cuda_h.py:19] end gpu_group_einsum cost 0.0004534721374511719 seconds
DEBUG 01-15 16:09:23.995976.995976 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.996781.996781 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.996730.996730 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002048015594482422 seconds
DEBUG 01-15 16:09:23.996486.996486 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.996330.996330 cuda_h.py:19] end concat_expert_out cost 6.461143493652344e-05 seconds
DEBUG 01-15 16:09:23.996464.996464 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.996448.996448 cuda_h.py:19] end index_scatter cost 5.7697296142578125e-05 seconds
DEBUG 01-15 16:09:23.996662.996662 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008454322814941406 seconds
DEBUG 01-15 16:09:23.996784.996784 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:23.996389.996389 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:23.997719.997719 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001437664031982422 seconds
DEBUG 01-15 16:09:23.997899.997899 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:23.997007.997007 cuda_h.py:19] end concat_expert_out cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:23.997466.997466 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:23.997535.997535 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:23.997590.997590 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000484466552734375 seconds
DEBUG 01-15 16:09:23.997937.997937 cuda_h.py:19] end gpu_experts_multi_device cost 0.053145647048950195 seconds
DEBUG 01-15 16:09:23.997522.997522 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.06477999687194824 seconds
DEBUG 01-15 16:09:23.998902.998902 cuda_h.py:19] end prefill_layer cost 0.07436060905456543 seconds
DEBUG 01-15 16:09:23.998215.998215 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:09:23.998394.998394 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:23.998289.998289 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:09:23.998469.998469 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:09:23.998272.998272 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:09:23.998506.998506 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 3.981590270996094e-05 seconds
DEBUG 01-15 16:09:23.998409.998409 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:23.998192.998192 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 0.00015854835510253906 seconds
DEBUG 01-15 16:09:23.998751.998751 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:23.998634.998634 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:23.998610.998610 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:23.998243.998243 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:23.999063.999063 cuda_h.py:19] end allocate_cuda_memory cost 0.0002923011779785156 seconds
DEBUG 01-15 16:09:23.999419.999419 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:23.999872.999872 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:23.999438.999438 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:23.999844.999844 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c79c1c77-69d7-41d8-b100-ab92f360fb05
DEBUG 01-15 16:09:23.999459.999459 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:23.999468.999468 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.001638.001638 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c79c1c77-69d7-41d8-b100-ab92f360fb05
DEBUG 01-15 16:09:24.001257.001257 cuda_h.py:19] end load_into_gpu_async cost 0.002212047576904297 seconds
DEBUG 01-15 16:09:24.001723.001723 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.001808.001808 cuda_h.py:19] end restore_tensors2 cost 0.000118255615234375 seconds
DEBUG 01-15 16:09:24.001817.001817 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003234386444091797 seconds
INFO 01-15 16:09:24.001443.001443 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c79c1c77-69d7-41d8-b100-ab92f360fb05
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.003225.003225 cuda_h.py:19] end self_attn cost 0.0035452842712402344 seconds
DEBUG 01-15 16:09:24.003382.003382 cuda_h.py:19] end iln_self_attn_paln cost 0.0051801204681396484 seconds
DEBUG 01-15 16:09:24.003880.003880 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:09:24.003120.003120 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.004244.004244 cuda_h.py:19] end gate cost 0.0007207393646240234 seconds
DEBUG 01-15 16:09:24.004458.004458 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.005105.005105 lmp.py:1912] 
DEBUG 01-15 16:09:24.005105.005105 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.005815.005815 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.005703.005703 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.005015.005015 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.005658.005658 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.005109.005109 lmp.py:1917] 
DEBUG 01-15 16:09:24.005109.005109 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.005037.005037 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.005448.005448 lmp.py:1935]   Expert 19 |     23 | CPU
DEBUG 01-15 16:09:24.005614.005614 lmp.py:1935]   Expert 42 |     24 | CPU
DEBUG 01-15 16:09:24.005065.005065 lmp.py:1935]   Expert 30 |     27 | CPU
DEBUG 01-15 16:09:24.005277.005277 lmp.py:1935]   Expert 32 |     46 | CPU
DEBUG 01-15 16:09:24.005251.005251 lmp.py:1935]   Expert  6 |     56 | CPU
DEBUG 01-15 16:09:24.005894.005894 lmp.py:1935]   Expert  5 |     72 | CPU
DEBUG 01-15 16:09:24.005584.005584 lmp.py:1935]   Expert 53 |     74 | CPU
DEBUG 01-15 16:09:24.005703.005703 lmp.py:1935]   Expert  1 |     82 | CPU
DEBUG 01-15 16:09:24.005823.005823 lmp.py:1935]   Expert 13 |    118 | CPU
DEBUG 01-15 16:09:24.005989.005989 lmp.py:1935]   Expert  9 |    122 | CPU
DEBUG 01-15 16:09:24.005917.005917 lmp.py:1935]   Expert 63 |    124 | CPU
DEBUG 01-15 16:09:24.005845.005845 lmp.py:1935]   Expert 34 |    128 | CPU
DEBUG 01-15 16:09:24.005011.005011 lmp.py:1935]   Expert 58 |    129 | CPU
DEBUG 01-15 16:09:24.005177.005177 lmp.py:1935]   Expert 50 |    130 | CPU
DEBUG 01-15 16:09:24.005105.005105 lmp.py:1935]   Expert 18 |    136 | CPU
DEBUG 01-15 16:09:24.005033.005033 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:09:24.005960.005960 lmp.py:1935]   Expert 31 |    138 | CPU
DEBUG 01-15 16:09:24.005888.005888 lmp.py:1935]   Expert 11 |    139 | CPU
DEBUG 01-15 16:09:24.005816.005816 lmp.py:1935]   Expert 59 |    141 | CPU
DEBUG 01-15 16:09:24.005519.005519 lmp.py:1935]   Expert 40 |    144 | CPU
DEBUG 01-15 16:09:24.005446.005446 lmp.py:1935]   Expert 12 |    148 | CPU
DEBUG 01-15 16:09:24.005897.005897 lmp.py:1935]   Expert  4 |    149 | CPU
DEBUG 01-15 16:09:24.005587.005587 lmp.py:1935]   Expert 46 |    150 | CPU
DEBUG 01-15 16:09:24.005799.005799 lmp.py:1935]   Expert  2 |    151 | CPU
DEBUG 01-15 16:09:24.005250.005250 lmp.py:1935]   Expert 20 |    152 | CPU
DEBUG 01-15 16:09:24.005370.005370 lmp.py:1935]   Expert 48 |    152 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.005728.005728 lmp.py:1935]   Expert 33 |    154 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.005848.005848 lmp.py:1935]   Expert 61 |    154 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.005491.005491 lmp.py:1935]   Expert 56 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.005372.005372 lmp.py:1935]   Expert 35 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.005730.005730 lmp.py:1935]   Expert 10 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.005565.005565 lmp.py:1935]   Expert 55 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.005162.005162 lmp.py:1935]   Expert 51 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.005997.005997 lmp.py:1935]   Expert 36 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.005594.005594 lmp.py:1935]   Expert  8 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.005952.005952 lmp.py:1935]   Expert 52 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.005787.005787 lmp.py:1935]   Expert 37 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.005192.005192 lmp.py:1935]   Expert  0 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.005835.005835 lmp.py:1935]   Expert 57 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.005001.005001 lmp.py:1935]   Expert 39 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.005644.005644 lmp.py:1935]   Expert 25 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006287.006287 lmp.py:1935]   Expert 62 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006691.006691 lmp.py:1935]   Expert 38 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006096.006096 lmp.py:1935]   Expert  7 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006838.006838 lmp.py:1935]   Expert 24 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006720.006720 lmp.py:1935]   Expert  3 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006124.006124 lmp.py:1935]   Expert 28 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006767.006767 lmp.py:1935]   Expert 27 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006172.006172 lmp.py:1935]   Expert 49 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006053.006053 lmp.py:1935]   Expert 60 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006458.006458 lmp.py:1935]   Expert 21 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006862.006862 lmp.py:1935]   Expert 16 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006936.006936 lmp.py:1935]   Expert 43 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006579.006579 lmp.py:1935]   Expert 23 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006983.006983 lmp.py:1935]   Expert 29 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006388.006388 lmp.py:1935]   Expert 15 |    292 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006269.006269 lmp.py:1935]   Expert 22 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006674.006674 lmp.py:1935]   Expert 47 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006078.006078 lmp.py:1935]   Expert 41 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006198.006198 lmp.py:1935]   Expert 44 |    308 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006603.006603 lmp.py:1935]   Expert 54 |    353 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006007.006007 lmp.py:1935]   Expert 14 |    375 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006412.006412 lmp.py:1935]   Expert 17 |    407 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.006816.006816 lmp.py:1935]   Expert 45 |    460 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.006506.006506 lmp.py:1937] 
DEBUG 01-15 16:09:24.006506.006506 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.006910.006910 lmp.py:1938]   CPU:   2741 tokens
DEBUG 01-15 16:09:24.006553.006553 lmp.py:1942]   cuda:1:   4847 tokens (20 experts)
DEBUG 01-15 16:09:24.006719.006719 lmp.py:1942]   cuda:2:   4700 tokens (19 experts)
DEBUG 01-15 16:09:24.006647.006647 lmp.py:1943]   Total GPU:   9547 tokens
DEBUG 01-15 16:09:24.006859.006859 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.006859.006859 lmp.py:1944] 
DEBUG 01-15 16:09:24.006794.006794 cuda_h.py:19] end experts_map_get cost 0.0016937255859375 seconds
DEBUG 01-15 16:09:24.006690.006690 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.006347.006347 lmp.py:1953] 
DEBUG 01-15 16:09:24.006347.006347 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.006084.006084 cuda_h.py:19] end cpu_experts_submit cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:09:24.006872.006872 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.006755.006755 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.008170.008170 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.7.gate_proj.weight': 0, 'model.layers.13.mlp.experts.7.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.7.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.21.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.21.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.21.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.24.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.24.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.24.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.25.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.25.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.25.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.29.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.29.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.29.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.33.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.33.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.33.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.37.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.37.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.37.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.39.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.39.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.39.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.41.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.41.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.41.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.43.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.43.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.43.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.45.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.45.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.45.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.47.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.47.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.47.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.48.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.48.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.48.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.49.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.49.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.49.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.56.up_proj.weight': 322961408, 'model.layers.13.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.13.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.13.mlp.experts.60.up_proj.weight': 340262912}, 2: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.27.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.27.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.27.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.28.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.28.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.28.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.36.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.36.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.36.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.38.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.38.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.38.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.13.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.13.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.13.mlp.experts.57.up_proj.weight': 288358400, 'model.layers.13.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.13.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.13.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.13.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.13.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.13.mlp.experts.62.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(16269180928, 5767168, 0, 0), (16274948096, 5767168, 5767168, 0), (16263413760, 5767168, 11534336, 0), (16511401984, 5767168, 17301504, 0), (16517169152, 5767168, 23068672, 0), (16505634816, 5767168, 28835840, 0), (16563306496, 5767168, 34603008, 0), (16569073664, 5767168, 40370176, 0), (16557539328, 5767168, 46137344, 0), (16580608000, 5767168, 51904512, 0), (16586375168, 5767168, 57671680, 0), (16574840832, 5767168, 63438848, 0), (16649814016, 5767168, 69206016, 0), (16655581184, 5767168, 74973184, 0), (16644046848, 5767168, 80740352, 0), (16719020032, 5767168, 86507520, 0), (16724787200, 5767168, 92274688, 0), (16713252864, 5767168, 98041856, 0), (16788226048, 5767168, 103809024, 0), (16793993216, 5767168, 109576192, 0), (16782458880, 5767168, 115343360, 0), (16822829056, 5767168, 121110528, 0), (16828596224, 5767168, 126877696, 0), (16817061888, 5767168, 132644864, 0), (16857432064, 5767168, 138412032, 0), (16863199232, 5767168, 144179200, 0), (16851664896, 5767168, 149946368, 0), (16892035072, 5767168, 155713536, 0), (16897802240, 5767168, 161480704, 0), (16886267904, 5767168, 167247872, 0), (16926638080, 5767168, 173015040, 0), (16932405248, 5767168, 178782208, 0), (16920870912, 5767168, 184549376, 0), (16961241088, 5767168, 190316544, 0), (16967008256, 5767168, 196083712, 0), (16955473920, 5767168, 201850880, 0), (16978542592, 5767168, 207618048, 0), (16984309760, 5767168, 213385216, 0), (16972775424, 5767168, 219152384, 0), (16995844096, 5767168, 224919552, 0), (17001611264, 5767168, 230686720, 0), (16990076928, 5767168, 236453888, 0), (17030447104, 5767168, 242221056, 0), (17036214272, 5767168, 247988224, 0), (17024679936, 5767168, 253755392, 0), (17047748608, 5767168, 259522560, 0), (17053515776, 5767168, 265289728, 0), (17041981440, 5767168, 271056896, 0), (17082351616, 5767168, 276824064, 0), (17088118784, 5767168, 282591232, 0), (17076584448, 5767168, 288358400, 0), (17099653120, 5767168, 294125568, 0), (17105420288, 5767168, 299892736, 0), (17093885952, 5767168, 305659904, 0), (17116954624, 5767168, 311427072, 0), (17122721792, 5767168, 317194240, 0), (17111187456, 5767168, 322961408, 0), (17186160640, 5767168, 328728576, 0), (17191927808, 5767168, 334495744, 0), (17180393472, 5767168, 340262912, 0)], 2: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16199974912, 5767168, 17301504, 0), (16205742080, 5767168, 23068672, 0), (16194207744, 5767168, 28835840, 0), (16286482432, 5767168, 34603008, 0), (16292249600, 5767168, 40370176, 0), (16280715264, 5767168, 46137344, 0), (16321085440, 5767168, 51904512, 0), (16326852608, 5767168, 57671680, 0), (16315318272, 5767168, 63438848, 0), (16390291456, 5767168, 69206016, 0), (16396058624, 5767168, 74973184, 0), (16384524288, 5767168, 80740352, 0), (16407592960, 5767168, 86507520, 0), (16413360128, 5767168, 92274688, 0), (16401825792, 5767168, 98041856, 0), (16424894464, 5767168, 103809024, 0), (16430661632, 5767168, 109576192, 0), (16419127296, 5767168, 115343360, 0), (16442195968, 5767168, 121110528, 0), (16447963136, 5767168, 126877696, 0), (16436428800, 5767168, 132644864, 0), (16528703488, 5767168, 138412032, 0), (16534470656, 5767168, 144179200, 0), (16522936320, 5767168, 149946368, 0), (16546004992, 5767168, 155713536, 0), (16551772160, 5767168, 161480704, 0), (16540237824, 5767168, 167247872, 0), (16615211008, 5767168, 173015040, 0), (16620978176, 5767168, 178782208, 0), (16609443840, 5767168, 184549376, 0), (16632512512, 5767168, 190316544, 0), (16638279680, 5767168, 196083712, 0), (16626745344, 5767168, 201850880, 0), (16753623040, 5767168, 207618048, 0), (16759390208, 5767168, 213385216, 0), (16747855872, 5767168, 219152384, 0), (16770924544, 5767168, 224919552, 0), (16776691712, 5767168, 230686720, 0), (16765157376, 5767168, 236453888, 0), (16805527552, 5767168, 242221056, 0), (16811294720, 5767168, 247988224, 0), (16799760384, 5767168, 253755392, 0), (16909336576, 5767168, 259522560, 0), (16915103744, 5767168, 265289728, 0), (16903569408, 5767168, 271056896, 0), (17134256128, 5767168, 276824064, 0), (17140023296, 5767168, 282591232, 0), (17128488960, 5767168, 288358400, 0), (17203462144, 5767168, 294125568, 0), (17209229312, 5767168, 299892736, 0), (17197694976, 5767168, 305659904, 0), (17220763648, 5767168, 311427072, 0), (17226530816, 5767168, 317194240, 0), (17214996480, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a688601bc0>, 2: <capsule object NULL at 0x74a6bc762790>}
DEBUG 01-15 16:09:24.009520.009520 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.009615.009615 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.009855.009855 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, dc765a2e-0774-40f0-b5c8-27b6ccf8d0da
DEBUG 01-15 16:09:24.009889.009889 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.009832.009832 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:24.009730.009730 client.py:127] Model loaded
DEBUG 01-15 16:09:24.009158.009158 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.009922.009922 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:24.010161.010161 cuda_h.py:19] end restore2model cost 0.0008046627044677734 seconds
DEBUG 01-15 16:09:24.010800.010800 cuda_h.py:19] end move_flatidxs cost 0.0008616447448730469 seconds
DEBUG 01-15 16:09:24.010065.010065 cuda_h.py:19] end sllm_worker_task cost 0.012141942977905273 seconds
DEBUG 01-15 16:09:24.010398.010398 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:24.010022.010022 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, dc765a2e-0774-40f0-b5c8-27b6ccf8d0da
DEBUG 01-15 16:09:24.011521.011521 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004641056060791016 seconds
DEBUG 01-15 16:09:24.011650.011650 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.014168.014168 cuda_h.py:19] end restore2model cost 0.0031898021697998047 seconds
DEBUG 01-15 16:09:24.014303.014303 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008089542388916016 seconds
DEBUG 01-15 16:09:24.014622.014622 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.015883.015883 cuda_h.py:19] end gpu_sexperts cost 0.0002677440643310547 seconds
DEBUG 01-15 16:09:24.015044.015044 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.015582.015582 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7404556274414062e-05 seconds
DEBUG 01-15 16:09:24.015755.015755 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.015457.015457 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.016558.016558 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000990152359008789 seconds
DEBUG 01-15 16:09:24.016785.016785 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.016788.016788 cuda_h.py:19] end gpu_group_list cost 0.00025272369384765625 seconds
DEBUG 01-15 16:09:24.017671.017671 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.018820.018820 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010294914245605469 seconds
DEBUG 01-15 16:09:24.018353.018353 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.018255.018255 cuda_h.py:19] end gpu_group_list cost 0.00021338462829589844 seconds
DEBUG 01-15 16:09:24.019993.019993 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.019068.019068 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, dc765a2e-0774-40f0-b5c8-27b6ccf8d0da
DEBUG 01-15 16:09:24.021244.021244 cuda_h.py:19] end group_tensors cost 0.010378360748291016 seconds
DEBUG 01-15 16:09:24.021979.021979 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.026003.026003 cuda_h.py:19] end group pad cost 0.004428386688232422 seconds
DEBUG 01-15 16:09:24.026647.026647 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:24.055936.055936 client.py:127] Model loaded
DEBUG 01-15 16:09:24.055319.055319 cuda_h.py:19] end wait_experts_multi_device cost 0.03556227684020996 seconds
DEBUG 01-15 16:09:24.055081.055081 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.056493.056493 cuda_h.py:19] end group_einsum cost 0.02955341339111328 seconds
DEBUG 01-15 16:09:24.056517.056517 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.059651.059651 cuda_h.py:19] end get_outputs_cpu1 cost 0.003140687942504883 seconds
DEBUG 01-15 16:09:24.060479.060479 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05072021484375 seconds
DEBUG 01-15 16:09:24.061235.061235 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005723714828491211 seconds
DEBUG 01-15 16:09:24.061376.061376 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.061604.061604 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.062993.062993 cuda_h.py:19] end index_scatter cost 6.937980651855469e-05 seconds
DEBUG 01-15 16:09:24.062485.062485 cuda_h.py:19] end cpuoutputsdeal cost 0.0012378692626953125 seconds
DEBUG 01-15 16:09:24.062342.062342 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.062098.062098 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.062785.062785 cuda_h.py:19] end gpu_group_tensor cost 0.00012755393981933594 seconds
DEBUG 01-15 16:09:24.062972.062972 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.062546.062546 cuda_h.py:19] end gpu_group_tensor cost 0.00011730194091796875 seconds
DEBUG 01-15 16:09:24.062152.062152 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.063526.063526 cuda_h.py:19] end gpu_group_einsum cost 0.0004436969757080078 seconds
DEBUG 01-15 16:09:24.063768.063768 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.063584.063584 cuda_h.py:19] end gpu_group_einsum cost 0.00041413307189941406 seconds
DEBUG 01-15 16:09:24.064602.064602 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.064029.064029 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.064071.064071 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023221969604492188 seconds
DEBUG 01-15 16:09:24.064318.064318 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.064984.064984 cuda_h.py:19] end concat_expert_out cost 5.888938903808594e-05 seconds
DEBUG 01-15 16:09:24.064158.064158 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.064419.064419 cuda_h.py:19] end index_scatter cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:09:24.065786.065786 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008904933929443359 seconds
DEBUG 01-15 16:09:24.065868.065868 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.065850.065850 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.065438.065438 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012540817260742188 seconds
DEBUG 01-15 16:09:24.065949.065949 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.065481.065481 cuda_h.py:19] end concat_expert_out cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:09:24.065318.065318 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.065042.065042 cuda_h.py:19] end index_scatter cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:09:24.065560.065560 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004405975341796875 seconds
DEBUG 01-15 16:09:24.065999.065999 cuda_h.py:19] end gpu_experts_multi_device cost 0.05040454864501953 seconds
DEBUG 01-15 16:09:24.065240.065240 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.06174731254577637 seconds
DEBUG 01-15 16:09:24.066769.066769 cuda_h.py:19] end prefill_layer cost 0.06790614128112793 seconds
DEBUG 01-15 16:09:24.066975.066975 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:09:24.066387.066387 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.066513.066513 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:09:24.066402.066402 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:09:24.066674.066674 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:09:24.066510.066510 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 3.218650817871094e-05 seconds
DEBUG 01-15 16:09:24.066803.066803 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 5.817413330078125e-05 seconds
DEBUG 01-15 16:09:24.066161.066161 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.066164.066164 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.066730.066730 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.066756.066756 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.066444.066444 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.067759.067759 cuda_h.py:19] end allocate_cuda_memory cost 0.0003898143768310547 seconds
DEBUG 01-15 16:09:24.067381.067381 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.067755.067755 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.067435.067435 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.067817.067817 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, cb506df9-1f18-4303-bb58-1d739aaa80f0
DEBUG 01-15 16:09:24.068102.068102 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.068747.068747 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.069589.069589 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, cb506df9-1f18-4303-bb58-1d739aaa80f0
DEBUG 01-15 16:09:24.069131.069131 cuda_h.py:19] end load_into_gpu_async cost 0.0019288063049316406 seconds
DEBUG 01-15 16:09:24.069061.069061 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.069731.069731 cuda_h.py:19] end restore_tensors2 cost 0.000152587890625 seconds
DEBUG 01-15 16:09:24.070093.070093 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003221273422241211 seconds
INFO 01-15 16:09:24.070959.070959 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, cb506df9-1f18-4303-bb58-1d739aaa80f0
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.072963.072963 cuda_h.py:19] end self_attn cost 0.004271745681762695 seconds
DEBUG 01-15 16:09:24.073907.073907 cuda_h.py:19] end iln_self_attn_paln cost 0.006631135940551758 seconds
DEBUG 01-15 16:09:24.073452.073452 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:09:24.073592.073592 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.073735.073735 cuda_h.py:19] end gate cost 0.0006663799285888672 seconds
DEBUG 01-15 16:09:24.073995.073995 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.074728.074728 lmp.py:1912] 
DEBUG 01-15 16:09:24.074728.074728 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.074961.074961 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.074087.074087 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.074638.074638 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.074327.074327 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.074824.074824 lmp.py:1917] 
DEBUG 01-15 16:09:24.074824.074824 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.074752.074752 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.074163.074163 lmp.py:1935]   Expert 34 |     29 | CPU
DEBUG 01-15 16:09:24.074329.074329 lmp.py:1935]   Expert  7 |     31 | CPU
DEBUG 01-15 16:09:24.074780.074780 lmp.py:1935]   Expert 13 |     41 | CPU
DEBUG 01-15 16:09:24.074946.074946 lmp.py:1935]   Expert 54 |     75 | CPU
DEBUG 01-15 16:09:24.074636.074636 lmp.py:1935]   Expert 18 |     84 | CPU
DEBUG 01-15 16:09:24.074133.074133 lmp.py:1935]   Expert 39 |     86 | CPU
DEBUG 01-15 16:09:24.074630.074630 lmp.py:1935]   Expert 49 |     87 | CPU
DEBUG 01-15 16:09:24.074889.074889 lmp.py:1935]   Expert 59 |    103 | CPU
DEBUG 01-15 16:09:24.074148.074148 lmp.py:1935]   Expert 16 |    106 | CPU
DEBUG 01-15 16:09:24.074645.074645 lmp.py:1935]   Expert 21 |    108 | CPU
DEBUG 01-15 16:09:24.074381.074381 lmp.py:1935]   Expert  0 |    111 | CPU
DEBUG 01-15 16:09:24.074878.074878 lmp.py:1935]   Expert 15 |    117 | CPU
DEBUG 01-15 16:09:24.074852.074852 lmp.py:1935]   Expert 41 |    119 | CPU
DEBUG 01-15 16:09:24.074064.074064 lmp.py:1935]   Expert 22 |    120 | CPU
DEBUG 01-15 16:09:24.074800.074800 lmp.py:1935]   Expert 45 |    121 | CPU
DEBUG 01-15 16:09:24.074297.074297 lmp.py:1935]   Expert 17 |    126 | CPU
DEBUG 01-15 16:09:24.074033.074033 lmp.py:1935]   Expert 61 |    134 | CPU
DEBUG 01-15 16:09:24.074530.074530 lmp.py:1935]   Expert  8 |    136 | CPU
DEBUG 01-15 16:09:24.074550.074550 lmp.py:1935]   Expert 35 |    138 | CPU
DEBUG 01-15 16:09:24.074286.074286 lmp.py:1935]   Expert 52 |    138 | CPU
DEBUG 01-15 16:09:24.074214.074214 lmp.py:1935]   Expert 38 |    139 | CPU
DEBUG 01-15 16:09:24.074665.074665 lmp.py:1935]   Expert 12 |    142 | CPU
DEBUG 01-15 16:09:24.074162.074162 lmp.py:1935]   Expert 48 |    147 | CPU
DEBUG 01-15 16:09:24.074897.074897 lmp.py:1935]   Expert 31 |    150 | CPU
DEBUG 01-15 16:09:24.074156.074156 lmp.py:1935]   Expert 53 |    155 | CPU
DEBUG 01-15 16:09:24.074799.074799 lmp.py:1935]   Expert 36 |    157 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.074442.074442 lmp.py:1935]   Expert 50 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.074131.074131 lmp.py:1935]   Expert 60 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.074059.074059 lmp.py:1935]   Expert 40 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.074510.074510 lmp.py:1935]   Expert 27 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.074961.074961 lmp.py:1935]   Expert 19 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.074889.074889 lmp.py:1935]   Expert  4 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.074075.074075 lmp.py:1935]   Expert 29 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.074672.074672 lmp.py:1935]   Expert 30 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.074599.074599 lmp.py:1935]   Expert 11 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.074050.074050 lmp.py:1935]   Expert 20 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.074216.074216 lmp.py:1935]   Expert 26 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.074144.074144 lmp.py:1935]   Expert 57 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.074072.074072 lmp.py:1935]   Expert  6 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.074000.074000 lmp.py:1935]   Expert 46 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.074643.074643 lmp.py:1935]   Expert 43 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.074570.074570 lmp.py:1935]   Expert 23 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075498.075498 lmp.py:1935]   Expert  2 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075949.075949 lmp.py:1935]   Expert 33 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075400.075400 lmp.py:1935]   Expert 42 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075089.075089 lmp.py:1935]   Expert 55 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075017.075017 lmp.py:1935]   Expert 56 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075706.075706 lmp.py:1935]   Expert 32 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075634.075634 lmp.py:1935]   Expert  9 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075085.075085 lmp.py:1935]   Expert  3 |    263 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075774.075774 lmp.py:1935]   Expert 14 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075225.075225 lmp.py:1935]   Expert 28 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075676.075676 lmp.py:1935]   Expert 44 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075127.075127 lmp.py:1935]   Expert  1 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075816.075816 lmp.py:1935]   Expert 51 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075505.075505 lmp.py:1935]   Expert 58 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075433.075433 lmp.py:1935]   Expert 37 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075122.075122 lmp.py:1935]   Expert 63 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075573.075573 lmp.py:1935]   Expert 47 |    289 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075024.075024 lmp.py:1935]   Expert 24 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075713.075713 lmp.py:1935]   Expert 10 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075880.075880 lmp.py:1935]   Expert 62 |    313 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075569.075569 lmp.py:1935]   Expert 25 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.075258.075258 lmp.py:1935]   Expert  5 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.075278.075278 lmp.py:1937] 
DEBUG 01-15 16:09:24.075278.075278 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.075729.075729 lmp.py:1938]   CPU:   2743 tokens
DEBUG 01-15 16:09:24.075419.075419 lmp.py:1942]   cuda:1:   4697 tokens (19 experts)
DEBUG 01-15 16:09:24.075631.075631 lmp.py:1942]   cuda:2:   4848 tokens (20 experts)
DEBUG 01-15 16:09:24.075605.075605 lmp.py:1943]   Total GPU:   9545 tokens
DEBUG 01-15 16:09:24.075341.075341 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.075341.075341 lmp.py:1944] 
DEBUG 01-15 16:09:24.075752.075752 cuda_h.py:19] end experts_map_get cost 0.0016179084777832031 seconds
DEBUG 01-15 16:09:24.075834.075834 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.075967.075967 lmp.py:1953] 
DEBUG 01-15 16:09:24.075967.075967 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.075273.075273 cuda_h.py:19] end cpu_experts_submit cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:09:24.075036.075036 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.075588.075588 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.076584.076584 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.076623.076623 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.2.gate_proj.weight': 0, 'model.layers.14.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.36.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.36.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.36.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.43.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.43.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.43.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.44.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.44.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.44.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.47.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.47.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.47.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.14.mlp.experts.1.gate_proj.weight': 0, 'model.layers.14.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.20.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.20.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.20.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.23.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.23.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.23.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.37.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.37.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.37.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.14.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.14.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.14.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.14.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.14.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.14.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.14.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.14.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.14.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.14.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.14.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.14.mlp.experts.62.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(17289969664, 5767168, 0, 0), (17295736832, 5767168, 5767168, 0), (17284202496, 5767168, 11534336, 0), (17341874176, 5767168, 17301504, 0), (17347641344, 5767168, 23068672, 0), (17336107008, 5767168, 28835840, 0), (17359175680, 5767168, 34603008, 0), (17364942848, 5767168, 40370176, 0), (17353408512, 5767168, 46137344, 0), (17411080192, 5767168, 51904512, 0), (17416847360, 5767168, 57671680, 0), (17405313024, 5767168, 63438848, 0), (17428381696, 5767168, 69206016, 0), (17434148864, 5767168, 74973184, 0), (17422614528, 5767168, 80740352, 0), (17445683200, 5767168, 86507520, 0), (17451450368, 5767168, 92274688, 0), (17439916032, 5767168, 98041856, 0), (17497587712, 5767168, 103809024, 0), (17503354880, 5767168, 109576192, 0), (17491820544, 5767168, 115343360, 0), (17584095232, 5767168, 121110528, 0), (17589862400, 5767168, 126877696, 0), (17578328064, 5767168, 132644864, 0), (17722507264, 5767168, 138412032, 0), (17728274432, 5767168, 144179200, 0), (17716740096, 5767168, 149946368, 0), (17774411776, 5767168, 155713536, 0), (17780178944, 5767168, 161480704, 0), (17768644608, 5767168, 167247872, 0), (17826316288, 5767168, 173015040, 0), (17832083456, 5767168, 178782208, 0), (17820549120, 5767168, 184549376, 0), (17878220800, 5767168, 190316544, 0), (17883987968, 5767168, 196083712, 0), (17872453632, 5767168, 201850880, 0), (17999331328, 5767168, 207618048, 0), (18005098496, 5767168, 213385216, 0), (17993564160, 5767168, 219152384, 0), (18016632832, 5767168, 224919552, 0), (18022400000, 5767168, 230686720, 0), (18010865664, 5767168, 236453888, 0), (18068537344, 5767168, 242221056, 0), (18074304512, 5767168, 247988224, 0), (18062770176, 5767168, 253755392, 0), (18137743360, 5767168, 259522560, 0), (18143510528, 5767168, 265289728, 0), (18131976192, 5767168, 271056896, 0), (18206949376, 5767168, 276824064, 0), (18212716544, 5767168, 282591232, 0), (18201182208, 5767168, 288358400, 0), (18241552384, 5767168, 294125568, 0), (18247319552, 5767168, 299892736, 0), (18235785216, 5767168, 305659904, 0), (18345361408, 5767168, 311427072, 0), (18351128576, 5767168, 317194240, 0), (18339594240, 5767168, 322961408, 0)], 2: [(17272668160, 5767168, 0, 0), (17278435328, 5767168, 5767168, 0), (17266900992, 5767168, 11534336, 0), (17307271168, 5767168, 17301504, 0), (17313038336, 5767168, 23068672, 0), (17301504000, 5767168, 28835840, 0), (17324572672, 5767168, 34603008, 0), (17330339840, 5767168, 40370176, 0), (17318805504, 5767168, 46137344, 0), (17601396736, 5767168, 51904512, 0), (17607163904, 5767168, 57671680, 0), (17595629568, 5767168, 63438848, 0), (17653301248, 5767168, 69206016, 0), (17659068416, 5767168, 74973184, 0), (17647534080, 5767168, 80740352, 0), (17670602752, 5767168, 86507520, 0), (17676369920, 5767168, 92274688, 0), (17664835584, 5767168, 98041856, 0), (17687904256, 5767168, 103809024, 0), (17693671424, 5767168, 109576192, 0), (17682137088, 5767168, 115343360, 0), (17705205760, 5767168, 121110528, 0), (17710972928, 5767168, 126877696, 0), (17699438592, 5767168, 132644864, 0), (17739808768, 5767168, 138412032, 0), (17745575936, 5767168, 144179200, 0), (17734041600, 5767168, 149946368, 0), (17757110272, 5767168, 155713536, 0), (17762877440, 5767168, 161480704, 0), (17751343104, 5767168, 167247872, 0), (17809014784, 5767168, 173015040, 0), (17814781952, 5767168, 178782208, 0), (17803247616, 5767168, 184549376, 0), (17895522304, 5767168, 190316544, 0), (17901289472, 5767168, 196083712, 0), (17889755136, 5767168, 201850880, 0), (17947426816, 5767168, 207618048, 0), (17953193984, 5767168, 213385216, 0), (17941659648, 5767168, 219152384, 0), (17982029824, 5767168, 224919552, 0), (17987796992, 5767168, 230686720, 0), (17976262656, 5767168, 236453888, 0), (18051235840, 5767168, 242221056, 0), (18057003008, 5767168, 247988224, 0), (18045468672, 5767168, 253755392, 0), (18120441856, 5767168, 259522560, 0), (18126209024, 5767168, 265289728, 0), (18114674688, 5767168, 271056896, 0), (18224250880, 5767168, 276824064, 0), (18230018048, 5767168, 282591232, 0), (18218483712, 5767168, 288358400, 0), (18258853888, 5767168, 294125568, 0), (18264621056, 5767168, 299892736, 0), (18253086720, 5767168, 305659904, 0), (18293456896, 5767168, 311427072, 0), (18299224064, 5767168, 317194240, 0), (18287689728, 5767168, 322961408, 0), (18328059904, 5767168, 328728576, 0), (18333827072, 5767168, 334495744, 0), (18322292736, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a688489c50>, 2: <capsule object NULL at 0x74a6bc762340>}
DEBUG 01-15 16:09:24.076593.076593 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:24.076528.076528 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:09:24.077612.077612 client.py:127] Model loaded
DEBUG 01-15 16:09:24.077576.077576 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.077070.077070 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5809d41a-beae-4d2f-8c7d-2c078cb734e5
DEBUG 01-15 16:09:24.077238.077238 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.077241.077241 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.077464.077464 cuda_h.py:19] end move_flatidxs cost 0.0008292198181152344 seconds
DEBUG 01-15 16:09:24.077333.077333 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:24.078549.078549 cuda_h.py:19] end restore2model cost 0.0010008811950683594 seconds
INFO 01-15 16:09:24.078514.078514 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5809d41a-beae-4d2f-8c7d-2c078cb734e5
DEBUG 01-15 16:09:24.078020.078020 cuda_h.py:19] end sllm_worker_task cost 0.012246370315551758 seconds
DEBUG 01-15 16:09:24.079791.079791 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036203861236572266 seconds
DEBUG 01-15 16:09:24.079797.079797 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.083336.083336 cuda_h.py:19] end restore2model cost 0.003204345703125 seconds
DEBUG 01-15 16:09:24.083279.083279 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0074253082275390625 seconds
DEBUG 01-15 16:09:24.083572.083572 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.082046.082046 cuda_h.py:19] end group_tensors cost 0.004843711853027344 seconds
DEBUG 01-15 16:09:24.083873.083873 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.083754.083754 cuda_h.py:19] end gpu_sexperts cost 0.0002734661102294922 seconds
DEBUG 01-15 16:09:24.083007.083007 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.083638.083638 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:09:24.083096.083096 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.083467.083467 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.085398.085398 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014333724975585938 seconds
DEBUG 01-15 16:09:24.085229.085229 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.085095.085095 cuda_h.py:19] end gpu_group_list cost 0.00030994415283203125 seconds
DEBUG 01-15 16:09:24.086121.086121 cuda_h.py:19] end group pad cost 0.003224611282348633 seconds
DEBUG 01-15 16:09:24.086360.086360 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.086335.086335 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.093180.093180 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.006819963455200195 seconds
DEBUG 01-15 16:09:24.094946.094946 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.095868.095868 cuda_h.py:19] end gpu_group_list cost 0.0008091926574707031 seconds
DEBUG 01-15 16:09:24.097148.097148 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.097648.097648 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5809d41a-beae-4d2f-8c7d-2c078cb734e5
DEBUG 01-15 16:09:24.117969.117969 cuda_h.py:19] end group_einsum cost 0.030505895614624023 seconds
DEBUG 01-15 16:09:24.117828.117828 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.120071.120071 cuda_h.py:19] end get_outputs_cpu1 cost 0.003256559371948242 seconds
DEBUG 01-15 16:09:24.121212.121212 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04499220848083496 seconds
INFO 01-15 16:09:24.122408.122408 client.py:127] Model loaded
DEBUG 01-15 16:09:24.123101.123101 cuda_h.py:19] end wait_experts_multi_device cost 0.025787353515625 seconds
DEBUG 01-15 16:09:24.123055.123055 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.123583.123583 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0004904270172119141 seconds
DEBUG 01-15 16:09:24.123327.123327 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.124660.124660 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.124672.124672 cuda_h.py:19] end index_scatter cost 7.677078247070312e-05 seconds
DEBUG 01-15 16:09:24.125349.125349 cuda_h.py:19] end cpuoutputsdeal cost 0.0014202594757080078 seconds
DEBUG 01-15 16:09:24.125974.125974 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.125829.125829 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.125789.125789 cuda_h.py:19] end gpu_group_tensor cost 0.0001475811004638672 seconds
DEBUG 01-15 16:09:24.125168.125168 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.125776.125776 cuda_h.py:19] end gpu_group_tensor cost 0.0001380443572998047 seconds
DEBUG 01-15 16:09:24.125256.125256 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.126138.126138 cuda_h.py:19] end gpu_group_einsum cost 0.0005662441253662109 seconds
DEBUG 01-15 16:09:24.126705.126705 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.127730.127730 cuda_h.py:19] end gpu_group_einsum cost 0.0004851818084716797 seconds
DEBUG 01-15 16:09:24.127814.127814 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.127878.127878 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.127577.127577 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002579689025878906 seconds
DEBUG 01-15 16:09:24.127393.127393 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.127475.127475 cuda_h.py:19] end concat_expert_out cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:24.127981.127981 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.127017.127017 cuda_h.py:19] end index_scatter cost 6.461143493652344e-05 seconds
DEBUG 01-15 16:09:24.128243.128243 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008945465087890625 seconds
DEBUG 01-15 16:09:24.128419.128419 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.128838.128838 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.128447.128447 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001728534698486328 seconds
DEBUG 01-15 16:09:24.128257.128257 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.128425.128425 cuda_h.py:19] end concat_expert_out cost 5.650520324707031e-05 seconds
DEBUG 01-15 16:09:24.128215.128215 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.128946.128946 cuda_h.py:19] end index_scatter cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:09:24.128610.128610 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005266666412353516 seconds
DEBUG 01-15 16:09:24.128804.128804 cuda_h.py:19] end gpu_experts_multi_device cost 0.045269012451171875 seconds
DEBUG 01-15 16:09:24.128621.128621 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.05585312843322754 seconds
DEBUG 01-15 16:09:24.129629.129629 cuda_h.py:19] end prefill_layer cost 0.06317758560180664 seconds
DEBUG 01-15 16:09:24.129201.129201 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:09:24.129619.129619 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.129183.129183 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:09:24.129555.129555 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:09:24.129119.129119 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:09:24.129413.129413 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:09:24.129645.129645 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 8.130073547363281e-05 seconds
DEBUG 01-15 16:09:24.129342.129342 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.129920.129920 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.129215.129215 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.130950.130950 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.130730.130730 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.130822.130822 cuda_h.py:19] end allocate_cuda_memory cost 0.0004324913024902344 seconds
DEBUG 01-15 16:09:24.130828.130828 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.131308.131308 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.131849.131849 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.131878.131878 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0308cfe1-68bd-4b4e-8b71-5c3cd2e2ecd3
DEBUG 01-15 16:09:24.131163.131163 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.131197.131197 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.132977.132977 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0308cfe1-68bd-4b4e-8b71-5c3cd2e2ecd3
DEBUG 01-15 16:09:24.132248.132248 cuda_h.py:19] end load_into_gpu_async cost 0.0017418861389160156 seconds
DEBUG 01-15 16:09:24.132251.132251 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.133153.133153 cuda_h.py:19] end restore_tensors2 cost 0.00014591217041015625 seconds
DEBUG 01-15 16:09:24.133111.133111 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003091096878051758 seconds
INFO 01-15 16:09:24.133314.133314 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0308cfe1-68bd-4b4e-8b71-5c3cd2e2ecd3
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.135175.135175 cuda_h.py:19] end self_attn cost 0.0038411617279052734 seconds
DEBUG 01-15 16:09:24.135835.135835 cuda_h.py:19] end iln_self_attn_paln cost 0.006196022033691406 seconds
DEBUG 01-15 16:09:24.135664.135664 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:09:24.135328.135328 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.136946.136946 cuda_h.py:19] end gate cost 0.0006327629089355469 seconds
DEBUG 01-15 16:09:24.136775.136775 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.137733.137733 lmp.py:1912] 
DEBUG 01-15 16:09:24.137733.137733 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.137251.137251 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.137139.137139 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.137451.137451 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.137617.137617 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.137591.137591 lmp.py:1917] 
DEBUG 01-15 16:09:24.137591.137591 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.137665.137665 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.137983.137983 lmp.py:1935]   Expert 15 |     65 | CPU
DEBUG 01-15 16:09:24.137865.137865 lmp.py:1935]   Expert 41 |     70 | CPU
DEBUG 01-15 16:09:24.137508.137508 lmp.py:1935]   Expert  0 |     74 | CPU
DEBUG 01-15 16:09:24.137866.137866 lmp.py:1935]   Expert 63 |     75 | CPU
DEBUG 01-15 16:09:24.137986.137986 lmp.py:1935]   Expert 20 |     83 | CPU
DEBUG 01-15 16:09:24.137629.137629 lmp.py:1935]   Expert 45 |     88 | CPU
DEBUG 01-15 16:09:24.137795.137795 lmp.py:1935]   Expert  7 |     91 | CPU
DEBUG 01-15 16:09:24.137676.137676 lmp.py:1935]   Expert 28 |    100 | CPU
DEBUG 01-15 16:09:24.137842.137842 lmp.py:1935]   Expert 54 |    107 | CPU
DEBUG 01-15 16:09:24.137724.137724 lmp.py:1935]   Expert 12 |    110 | CPU
DEBUG 01-15 16:09:24.137128.137128 lmp.py:1935]   Expert 40 |    120 | CPU
DEBUG 01-15 16:09:24.137010.137010 lmp.py:1935]   Expert 52 |    120 | CPU
DEBUG 01-15 16:09:24.137653.137653 lmp.py:1935]   Expert 59 |    122 | CPU
DEBUG 01-15 16:09:24.137296.137296 lmp.py:1935]   Expert  5 |    124 | CPU
DEBUG 01-15 16:09:24.137462.137462 lmp.py:1935]   Expert  4 |    130 | CPU
DEBUG 01-15 16:09:24.137343.137343 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:09:24.137509.137509 lmp.py:1935]   Expert 55 |    137 | CPU
DEBUG 01-15 16:09:24.137152.137152 lmp.py:1935]   Expert 62 |    137 | CPU
DEBUG 01-15 16:09:24.137318.137318 lmp.py:1935]   Expert 61 |    138 | CPU
DEBUG 01-15 16:09:24.137723.137723 lmp.py:1935]   Expert 21 |    140 | CPU
DEBUG 01-15 16:09:24.137127.137127 lmp.py:1935]   Expert 13 |    141 | CPU
DEBUG 01-15 16:09:24.137294.137294 lmp.py:1935]   Expert 42 |    142 | CPU
DEBUG 01-15 16:09:24.137698.137698 lmp.py:1935]   Expert 14 |    145 | CPU
DEBUG 01-15 16:09:24.137103.137103 lmp.py:1935]   Expert 10 |    147 | CPU
DEBUG 01-15 16:09:24.137269.137269 lmp.py:1935]   Expert 22 |    147 | CPU
DEBUG 01-15 16:09:24.137819.137819 lmp.py:1935]   Expert 51 |    156 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137290.137290 lmp.py:1935]   Expert 32 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137649.137649 lmp.py:1935]   Expert 25 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137576.137576 lmp.py:1935]   Expert  1 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137743.137743 lmp.py:1935]   Expert 47 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137955.137955 lmp.py:1935]   Expert 26 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137644.137644 lmp.py:1935]   Expert 53 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137857.137857 lmp.py:1935]   Expert 50 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137546.137546 lmp.py:1935]   Expert 19 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137712.137712 lmp.py:1935]   Expert  6 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137640.137640 lmp.py:1935]   Expert  2 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137329.137329 lmp.py:1935]   Expert 11 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137542.137542 lmp.py:1935]   Expert 30 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137231.137231 lmp.py:1935]   Expert 35 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137443.137443 lmp.py:1935]   Expert 56 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137133.137133 lmp.py:1935]   Expert 57 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137822.137822 lmp.py:1935]   Expert 48 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137034.137034 lmp.py:1935]   Expert 24 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137724.137724 lmp.py:1935]   Expert 16 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137128.137128 lmp.py:1935]   Expert 44 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137818.137818 lmp.py:1935]   Expert 46 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.137030.137030 lmp.py:1935]   Expert 39 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.137481.137481 lmp.py:1935]   Expert 18 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.138932.138932 lmp.py:1935]   Expert 29 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.138383.138383 lmp.py:1935]   Expert 37 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.138357.138357 lmp.py:1935]   Expert 31 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.138569.138569 lmp.py:1935]   Expert 36 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.138020.138020 lmp.py:1935]   Expert  3 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.138994.138994 lmp.py:1935]   Expert 60 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.138445.138445 lmp.py:1935]   Expert 38 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.138373.138373 lmp.py:1935]   Expert  9 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.138824.138824 lmp.py:1935]   Expert 17 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.138798.138798 lmp.py:1935]   Expert 23 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.138010.138010 lmp.py:1935]   Expert 27 |    351 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.138461.138461 lmp.py:1935]   Expert 43 |    359 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.138435.138435 lmp.py:1935]   Expert 33 |    397 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.138648.138648 lmp.py:1935]   Expert  8 |    399 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.138860.138860 lmp.py:1935]   Expert 58 |    445 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.138834.138834 lmp.py:1935]   Expert 49 |    541 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.138093.138093 lmp.py:1937] 
DEBUG 01-15 16:09:24.138093.138093 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.138305.138305 lmp.py:1938]   CPU:   2886 tokens
DEBUG 01-15 16:09:24.138948.138948 lmp.py:1942]   cuda:1:   4629 tokens (19 experts)
DEBUG 01-15 16:09:24.138638.138638 lmp.py:1942]   cuda:2:   4773 tokens (20 experts)
DEBUG 01-15 16:09:24.138373.138373 lmp.py:1943]   Total GPU:   9402 tokens
DEBUG 01-15 16:09:24.138632.138632 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.138632.138632 lmp.py:1944] 
DEBUG 01-15 16:09:24.138851.138851 cuda_h.py:19] end experts_map_get cost 0.0016431808471679688 seconds
DEBUG 01-15 16:09:24.138456.138456 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.138781.138781 lmp.py:1953] 
DEBUG 01-15 16:09:24.138781.138781 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.138518.138518 cuda_h.py:19] end cpu_experts_submit cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:09:24.138830.138830 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.138428.138428 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.139362.139362 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.2.gate_proj.weight': 0, 'model.layers.15.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.16.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.16.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.16.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.24.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.24.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.24.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.38.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.38.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.38.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.39.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.39.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.39.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.50.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.50.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.50.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.56.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.56.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.56.up_proj.weight': 322961408}, 2: {'model.layers.15.mlp.experts.1.gate_proj.weight': 0, 'model.layers.15.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.18.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.18.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.18.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.23.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.23.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.23.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.35.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.35.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.35.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.36.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.36.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.36.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.43.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.43.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.43.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.46.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.46.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.46.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.48.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.48.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.48.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.51.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.51.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.51.up_proj.weight': 271056896, 'model.layers.15.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.15.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.15.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.15.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.15.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.15.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.15.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.15.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.15.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.15.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.15.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.15.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(18397265920, 5767168, 0, 0), (18403033088, 5767168, 5767168, 0), (18391498752, 5767168, 11534336, 0), (18414567424, 5767168, 17301504, 0), (18420334592, 5767168, 23068672, 0), (18408800256, 5767168, 28835840, 0), (18466471936, 5767168, 34603008, 0), (18472239104, 5767168, 40370176, 0), (18460704768, 5767168, 46137344, 0), (18639486976, 5767168, 51904512, 0), (18645254144, 5767168, 57671680, 0), (18633719808, 5767168, 63438848, 0), (18656788480, 5767168, 69206016, 0), (18662555648, 5767168, 74973184, 0), (18651021312, 5767168, 80740352, 0), (18777899008, 5767168, 86507520, 0), (18783666176, 5767168, 92274688, 0), (18772131840, 5767168, 98041856, 0), (18812502016, 5767168, 103809024, 0), (18818269184, 5767168, 109576192, 0), (18806734848, 5767168, 115343360, 0), (18829803520, 5767168, 121110528, 0), (18835570688, 5767168, 126877696, 0), (18824036352, 5767168, 132644864, 0), (18864406528, 5767168, 138412032, 0), (18870173696, 5767168, 144179200, 0), (18858639360, 5767168, 149946368, 0), (18881708032, 5767168, 155713536, 0), (18887475200, 5767168, 161480704, 0), (18875940864, 5767168, 167247872, 0), (18899009536, 5767168, 173015040, 0), (18904776704, 5767168, 178782208, 0), (18893242368, 5767168, 184549376, 0), (18916311040, 5767168, 190316544, 0), (18922078208, 5767168, 196083712, 0), (18910543872, 5767168, 201850880, 0), (18933612544, 5767168, 207618048, 0), (18939379712, 5767168, 213385216, 0), (18927845376, 5767168, 219152384, 0), (19020120064, 5767168, 224919552, 0), (19025887232, 5767168, 230686720, 0), (19014352896, 5767168, 236453888, 0), (19037421568, 5767168, 242221056, 0), (19043188736, 5767168, 247988224, 0), (19031654400, 5767168, 253755392, 0), (19175833600, 5767168, 259522560, 0), (19181600768, 5767168, 265289728, 0), (19170066432, 5767168, 271056896, 0), (19210436608, 5767168, 276824064, 0), (19216203776, 5767168, 282591232, 0), (19204669440, 5767168, 288358400, 0), (19227738112, 5767168, 294125568, 0), (19233505280, 5767168, 299892736, 0), (19221970944, 5767168, 305659904, 0), (19331547136, 5767168, 311427072, 0), (19337314304, 5767168, 317194240, 0), (19325779968, 5767168, 322961408, 0)], 2: [(18379964416, 5767168, 0, 0), (18385731584, 5767168, 5767168, 0), (18374197248, 5767168, 11534336, 0), (18501074944, 5767168, 17301504, 0), (18506842112, 5767168, 23068672, 0), (18495307776, 5767168, 28835840, 0), (18518376448, 5767168, 34603008, 0), (18524143616, 5767168, 40370176, 0), (18512609280, 5767168, 46137344, 0), (18552979456, 5767168, 51904512, 0), (18558746624, 5767168, 57671680, 0), (18547212288, 5767168, 63438848, 0), (18674089984, 5767168, 69206016, 0), (18679857152, 5767168, 74973184, 0), (18668322816, 5767168, 80740352, 0), (18691391488, 5767168, 86507520, 0), (18697158656, 5767168, 92274688, 0), (18685624320, 5767168, 98041856, 0), (18760597504, 5767168, 103809024, 0), (18766364672, 5767168, 109576192, 0), (18754830336, 5767168, 115343360, 0), (18795200512, 5767168, 121110528, 0), (18800967680, 5767168, 126877696, 0), (18789433344, 5767168, 132644864, 0), (18968215552, 5767168, 138412032, 0), (18973982720, 5767168, 144179200, 0), (18962448384, 5767168, 149946368, 0), (18985517056, 5767168, 155713536, 0), (18991284224, 5767168, 161480704, 0), (18979749888, 5767168, 167247872, 0), (19002818560, 5767168, 173015040, 0), (19008585728, 5767168, 178782208, 0), (18997051392, 5767168, 184549376, 0), (19106627584, 5767168, 190316544, 0), (19112394752, 5767168, 196083712, 0), (19100860416, 5767168, 201850880, 0), (19123929088, 5767168, 207618048, 0), (19129696256, 5767168, 213385216, 0), (19118161920, 5767168, 219152384, 0), (19158532096, 5767168, 224919552, 0), (19164299264, 5767168, 230686720, 0), (19152764928, 5767168, 236453888, 0), (19193135104, 5767168, 242221056, 0), (19198902272, 5767168, 247988224, 0), (19187367936, 5767168, 253755392, 0), (19245039616, 5767168, 259522560, 0), (19250806784, 5767168, 265289728, 0), (19239272448, 5767168, 271056896, 0), (19279642624, 5767168, 276824064, 0), (19285409792, 5767168, 282591232, 0), (19273875456, 5767168, 288358400, 0), (19348848640, 5767168, 294125568, 0), (19354615808, 5767168, 299892736, 0), (19343081472, 5767168, 305659904, 0), (19366150144, 5767168, 311427072, 0), (19371917312, 5767168, 317194240, 0), (19360382976, 5767168, 322961408, 0), (19400753152, 5767168, 328728576, 0), (19406520320, 5767168, 334495744, 0), (19394985984, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6805dd110>, 2: <capsule object NULL at 0x74a6bc5ed560>}
DEBUG 01-15 16:09:24.140372.140372 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.140130.140130 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.140455.140455 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6e6e8120-e4c1-44de-8296-220f0ebf088f
DEBUG 01-15 16:09:24.140762.140762 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:24.140017.140017 client.py:127] Model loaded
DEBUG 01-15 16:09:24.141796.141796 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.141715.141715 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.141141.141141 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:24.141446.141446 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6e6e8120-e4c1-44de-8296-220f0ebf088f
DEBUG 01-15 16:09:24.142093.142093 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036191940307617188 seconds
DEBUG 01-15 16:09:24.142699.142699 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.142789.142789 cuda_h.py:19] end move_flatidxs cost 0.0008630752563476562 seconds
DEBUG 01-15 16:09:24.142102.142102 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:24.143570.143570 cuda_h.py:19] end restore2model cost 0.001043558120727539 seconds
DEBUG 01-15 16:09:24.143792.143792 cuda_h.py:19] end sllm_worker_task cost 0.013694286346435547 seconds
DEBUG 01-15 16:09:24.146728.146728 cuda_h.py:19] end restore2model cost 0.004194736480712891 seconds
DEBUG 01-15 16:09:24.146897.146897 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008103609085083008 seconds
DEBUG 01-15 16:09:24.146454.146454 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.146669.146669 cuda_h.py:19] end gpu_sexperts cost 0.0002677440643310547 seconds
DEBUG 01-15 16:09:24.147260.147260 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.147083.147083 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7642974853515625e-05 seconds
DEBUG 01-15 16:09:24.147302.147302 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.147290.147290 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.148163.148163 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009293556213378906 seconds
DEBUG 01-15 16:09:24.148152.148152 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.148511.148511 cuda_h.py:19] end gpu_group_list cost 0.00020170211791992188 seconds
DEBUG 01-15 16:09:24.148208.148208 cuda_h.py:19] end group_tensors cost 0.0058972835540771484 seconds
DEBUG 01-15 16:09:24.149538.149538 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.149939.149939 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.150842.150842 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014925003051757812 seconds
DEBUG 01-15 16:09:24.150501.150501 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.151122.151122 cuda_h.py:19] end gpu_group_list cost 0.0003151893615722656 seconds
DEBUG 01-15 16:09:24.152209.152209 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.152589.152589 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6e6e8120-e4c1-44de-8296-220f0ebf088f
DEBUG 01-15 16:09:24.152369.152369 cuda_h.py:19] end group pad cost 0.00347900390625 seconds
DEBUG 01-15 16:09:24.152821.152821 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.182223.182223 cuda_h.py:19] end group_einsum cost 0.02953648567199707 seconds
DEBUG 01-15 16:09:24.182566.182566 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:24.184220.184220 client.py:127] Model loaded
DEBUG 01-15 16:09:24.185258.185258 cuda_h.py:19] end wait_experts_multi_device cost 0.03282475471496582 seconds
DEBUG 01-15 16:09:24.185021.185021 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.185802.185802 cuda_h.py:19] end get_outputs_cpu1 cost 0.0032520294189453125 seconds
DEBUG 01-15 16:09:24.186683.186683 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04545164108276367 seconds
DEBUG 01-15 16:09:24.187919.187919 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0026166439056396484 seconds
DEBUG 01-15 16:09:24.188323.188323 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.190707.190707 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.191342.191342 cuda_h.py:19] end index_scatter cost 0.00017714500427246094 seconds
DEBUG 01-15 16:09:24.191149.191149 cuda_h.py:19] end cpuoutputsdeal cost 0.003599882125854492 seconds
DEBUG 01-15 16:09:24.191611.191611 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.192541.192541 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.192646.192646 cuda_h.py:19] end gpu_group_tensor cost 0.0006494522094726562 seconds
DEBUG 01-15 16:09:24.192265.192265 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.193199.193199 cuda_h.py:19] end gpu_group_tensor cost 0.00016164779663085938 seconds
DEBUG 01-15 16:09:24.193322.193322 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.193754.193754 cuda_h.py:19] end gpu_group_einsum cost 0.0005834102630615234 seconds
DEBUG 01-15 16:09:24.193500.193500 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.194005.194005 cuda_h.py:19] end gpu_group_einsum cost 0.00037860870361328125 seconds
DEBUG 01-15 16:09:24.194592.194592 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.194721.194721 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.194596.194596 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018858909606933594 seconds
DEBUG 01-15 16:09:24.194590.194590 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.194507.194507 cuda_h.py:19] end concat_expert_out cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:09:24.194449.194449 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.195201.195201 cuda_h.py:19] end index_scatter cost 6.723403930664062e-05 seconds
DEBUG 01-15 16:09:24.195738.195738 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007653236389160156 seconds
DEBUG 01-15 16:09:24.195476.195476 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.195180.195180 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.195530.195530 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012230873107910156 seconds
DEBUG 01-15 16:09:24.195756.195756 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.195573.195573 cuda_h.py:19] end concat_expert_out cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:09:24.195409.195409 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.195187.195187 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:24.195373.195373 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00045013427734375 seconds
DEBUG 01-15 16:09:24.195382.195382 cuda_h.py:19] end gpu_experts_multi_device cost 0.0488436222076416 seconds
DEBUG 01-15 16:09:24.196385.196385 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.06005215644836426 seconds
DEBUG 01-15 16:09:24.196122.196122 cuda_h.py:19] end prefill_layer cost 0.06701517105102539 seconds
DEBUG 01-15 16:09:24.196761.196761 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:09:24.196080.196080 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.196398.196398 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:09:24.196194.196194 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:09:24.196228.196228 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:09:24.196349.196349 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 3.123283386230469e-05 seconds
DEBUG 01-15 16:09:24.196621.196621 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 5.8650970458984375e-05 seconds
DEBUG 01-15 16:09:24.196979.196979 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.196313.196313 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.196118.196118 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.197906.197906 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.197586.197586 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.197071.197071 cuda_h.py:19] end allocate_cuda_memory cost 0.0005102157592773438 seconds
DEBUG 01-15 16:09:24.198818.198818 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.198093.198093 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.198335.198335 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.198610.198610 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 97d23eda-d957-459e-8e5c-39307654f85d
DEBUG 01-15 16:09:24.198226.198226 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.198117.198117 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.199761.199761 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 97d23eda-d957-459e-8e5c-39307654f85d
DEBUG 01-15 16:09:24.199163.199163 cuda_h.py:19] end load_into_gpu_async cost 0.0016980171203613281 seconds
DEBUG 01-15 16:09:24.200193.200193 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.200287.200287 cuda_h.py:19] end restore_tensors2 cost 0.00015020370483398438 seconds
DEBUG 01-15 16:09:24.200913.200913 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.00308990478515625 seconds
INFO 01-15 16:09:24.200925.200925 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 97d23eda-d957-459e-8e5c-39307654f85d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.202161.202161 cuda_h.py:19] end self_attn cost 0.004085063934326172 seconds
DEBUG 01-15 16:09:24.203489.203489 cuda_h.py:19] end iln_self_attn_paln cost 0.00649261474609375 seconds
DEBUG 01-15 16:09:24.203173.203173 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:09:24.203929.203929 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.204057.204057 cuda_h.py:19] end gate cost 0.0006227493286132812 seconds
DEBUG 01-15 16:09:24.204602.204602 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.204772.204772 lmp.py:1912] 
DEBUG 01-15 16:09:24.204772.204772 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.204673.204673 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.204562.204562 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.204874.204874 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.204755.204755 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.204206.204206 lmp.py:1917] 
DEBUG 01-15 16:09:24.204206.204206 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.204849.204849 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.204452.204452 lmp.py:1935]   Expert 58 |     36 | CPU
DEBUG 01-15 16:09:24.204857.204857 lmp.py:1935]   Expert 47 |     59 | CPU
DEBUG 01-15 16:09:24.204831.204831 lmp.py:1935]   Expert 31 |     60 | CPU
DEBUG 01-15 16:09:24.204043.204043 lmp.py:1935]   Expert 49 |     61 | CPU
DEBUG 01-15 16:09:24.204733.204733 lmp.py:1935]   Expert  4 |     64 | CPU
DEBUG 01-15 16:09:24.204137.204137 lmp.py:1935]   Expert 38 |     69 | CPU
DEBUG 01-15 16:09:24.204588.204588 lmp.py:1935]   Expert 45 |     70 | CPU
DEBUG 01-15 16:09:24.204039.204039 lmp.py:1935]   Expert 43 |     81 | CPU
DEBUG 01-15 16:09:24.204728.204728 lmp.py:1935]   Expert 41 |     84 | CPU
DEBUG 01-15 16:09:24.204418.204418 lmp.py:1935]   Expert 33 |     95 | CPU
DEBUG 01-15 16:09:24.204392.204392 lmp.py:1935]   Expert 50 |    100 | CPU
DEBUG 01-15 16:09:24.204127.204127 lmp.py:1935]   Expert 57 |    102 | CPU
DEBUG 01-15 16:09:24.204863.204863 lmp.py:1935]   Expert 11 |    107 | CPU
DEBUG 01-15 16:09:24.204360.204360 lmp.py:1935]   Expert  2 |    112 | CPU
DEBUG 01-15 16:09:24.204096.204096 lmp.py:1935]   Expert 51 |    114 | CPU
DEBUG 01-15 16:09:24.204831.204831 lmp.py:1935]   Expert  0 |    123 | CPU
DEBUG 01-15 16:09:24.204329.204329 lmp.py:1935]   Expert 14 |    126 | CPU
DEBUG 01-15 16:09:24.204064.204064 lmp.py:1935]   Expert 54 |    128 | CPU
DEBUG 01-15 16:09:24.204992.204992 lmp.py:1935]   Expert 34 |    141 | CPU
DEBUG 01-15 16:09:24.204443.204443 lmp.py:1935]   Expert 56 |    141 | CPU
DEBUG 01-15 16:09:24.204894.204894 lmp.py:1935]   Expert 26 |    144 | CPU
DEBUG 01-15 16:09:24.204106.204106 lmp.py:1935]   Expert 27 |    153 | CPU
DEBUG 01-15 16:09:24.204557.204557 lmp.py:1935]   Expert 28 |    157 | CPU
DEBUG 01-15 16:09:24.204008.204008 lmp.py:1935]   Expert 55 |    160 | CPU
DEBUG 01-15 16:09:24.204982.204982 lmp.py:1935]   Expert 10 |    166 | CPU
DEBUG 01-15 16:09:24.204340.204340 lmp.py:1935]   Expert 25 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205745.205745 lmp.py:1935]   Expert  9 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205388.205388 lmp.py:1935]   Expert 13 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205315.205315 lmp.py:1935]   Expert 61 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205243.205243 lmp.py:1935]   Expert 48 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205171.205171 lmp.py:1935]   Expert  6 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205052.205052 lmp.py:1935]   Expert  7 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205841.205841 lmp.py:1935]   Expert 46 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205914.205914 lmp.py:1935]   Expert 24 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205988.205988 lmp.py:1935]   Expert 42 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205108.205108 lmp.py:1935]   Expert 18 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205943.205943 lmp.py:1935]   Expert 40 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205824.205824 lmp.py:1935]   Expert 12 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205421.205421 lmp.py:1935]   Expert 63 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205064.205064 lmp.py:1935]   Expert 29 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205899.205899 lmp.py:1935]   Expert 59 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205257.205257 lmp.py:1935]   Expert 21 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205615.205615 lmp.py:1935]   Expert 22 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205974.205974 lmp.py:1935]   Expert 32 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205285.205285 lmp.py:1935]   Expert 19 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205359.205359 lmp.py:1935]   Expert 36 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205194.205194 lmp.py:1935]   Expert  3 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205268.205268 lmp.py:1935]   Expert 37 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205864.205864 lmp.py:1935]   Expert  1 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205222.205222 lmp.py:1935]   Expert 16 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205581.205581 lmp.py:1935]   Expert 20 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205939.205939 lmp.py:1935]   Expert  5 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205297.205297 lmp.py:1935]   Expert  8 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205417.205417 lmp.py:1935]   Expert 30 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205775.205775 lmp.py:1935]   Expert 15 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205895.205895 lmp.py:1935]   Expert 62 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205015.205015 lmp.py:1935]   Expert 39 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205896.205896 lmp.py:1935]   Expert 17 |    305 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205268.205268 lmp.py:1935]   Expert 35 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205626.205626 lmp.py:1935]   Expert 60 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205984.205984 lmp.py:1935]   Expert 52 |    351 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205343.205343 lmp.py:1935]   Expert 23 |    366 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205701.205701 lmp.py:1935]   Expert 44 |    377 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.205582.205582 lmp.py:1935]   Expert 53 |    437 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.205033.205033 lmp.py:1937] 
DEBUG 01-15 16:09:24.205033.205033 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.205676.205676 lmp.py:1938]   CPU:   2653 tokens
DEBUG 01-15 16:09:24.205558.205558 lmp.py:1942]   cuda:1:   4899 tokens (20 experts)
DEBUG 01-15 16:09:24.205724.205724 lmp.py:1942]   cuda:2:   4736 tokens (19 experts)
DEBUG 01-15 16:09:24.205413.205413 lmp.py:1943]   Total GPU:   9635 tokens
DEBUG 01-15 16:09:24.205792.205792 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.205792.205792 lmp.py:1944] 
DEBUG 01-15 16:09:24.205872.205872 cuda_h.py:19] end experts_map_get cost 0.0017158985137939453 seconds
DEBUG 01-15 16:09:24.205430.205430 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.205040.205040 lmp.py:1953] 
DEBUG 01-15 16:09:24.205040.205040 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.205300.205300 cuda_h.py:19] end cpu_experts_submit cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:24.206825.206825 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.206423.206423 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.206505.206505 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.1.gate_proj.weight': 0, 'model.layers.16.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.53.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.53.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.53.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.16.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.16.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.16.mlp.experts.63.up_proj.weight': 340262912}, 2: {'model.layers.16.mlp.experts.5.gate_proj.weight': 0, 'model.layers.16.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.16.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.16.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.16.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.16.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.16.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.16.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.16.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.16.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.16.mlp.experts.60.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(19487260672, 5767168, 0, 0), (19493027840, 5767168, 5767168, 0), (19481493504, 5767168, 11534336, 0), (19521863680, 5767168, 17301504, 0), (19527630848, 5767168, 23068672, 0), (19516096512, 5767168, 28835840, 0), (19591069696, 5767168, 34603008, 0), (19596836864, 5767168, 40370176, 0), (19585302528, 5767168, 46137344, 0), (19608371200, 5767168, 51904512, 0), (19614138368, 5767168, 57671680, 0), (19602604032, 5767168, 63438848, 0), (19625672704, 5767168, 69206016, 0), (19631439872, 5767168, 74973184, 0), (19619905536, 5767168, 80740352, 0), (19764084736, 5767168, 86507520, 0), (19769851904, 5767168, 92274688, 0), (19758317568, 5767168, 98041856, 0), (19781386240, 5767168, 103809024, 0), (19787153408, 5767168, 109576192, 0), (19775619072, 5767168, 115343360, 0), (19798687744, 5767168, 121110528, 0), (19804454912, 5767168, 126877696, 0), (19792920576, 5767168, 132644864, 0), (19815989248, 5767168, 138412032, 0), (19821756416, 5767168, 144179200, 0), (19810222080, 5767168, 149946368, 0), (19902496768, 5767168, 155713536, 0), (19908263936, 5767168, 161480704, 0), (19896729600, 5767168, 167247872, 0), (19971702784, 5767168, 173015040, 0), (19977469952, 5767168, 178782208, 0), (19965935616, 5767168, 184549376, 0), (20023607296, 5767168, 190316544, 0), (20029374464, 5767168, 196083712, 0), (20017840128, 5767168, 201850880, 0), (20144717824, 5767168, 207618048, 0), (20150484992, 5767168, 213385216, 0), (20138950656, 5767168, 219152384, 0), (20162019328, 5767168, 224919552, 0), (20167786496, 5767168, 230686720, 0), (20156252160, 5767168, 236453888, 0), (20265828352, 5767168, 242221056, 0), (20271595520, 5767168, 247988224, 0), (20260061184, 5767168, 253755392, 0), (20369637376, 5767168, 259522560, 0), (20375404544, 5767168, 265289728, 0), (20363870208, 5767168, 271056896, 0), (20386938880, 5767168, 276824064, 0), (20392706048, 5767168, 282591232, 0), (20381171712, 5767168, 288358400, 0), (20525350912, 5767168, 294125568, 0), (20531118080, 5767168, 299892736, 0), (20519583744, 5767168, 305659904, 0), (20542652416, 5767168, 311427072, 0), (20548419584, 5767168, 317194240, 0), (20536885248, 5767168, 322961408, 0), (20559953920, 5767168, 328728576, 0), (20565721088, 5767168, 334495744, 0), (20554186752, 5767168, 340262912, 0)], 2: [(19556466688, 5767168, 0, 0), (19562233856, 5767168, 5767168, 0), (19550699520, 5767168, 11534336, 0), (19573768192, 5767168, 17301504, 0), (19579535360, 5767168, 23068672, 0), (19568001024, 5767168, 28835840, 0), (19677577216, 5767168, 34603008, 0), (19683344384, 5767168, 40370176, 0), (19671810048, 5767168, 46137344, 0), (19694878720, 5767168, 51904512, 0), (19700645888, 5767168, 57671680, 0), (19689111552, 5767168, 63438848, 0), (19729481728, 5767168, 69206016, 0), (19735248896, 5767168, 74973184, 0), (19723714560, 5767168, 80740352, 0), (19746783232, 5767168, 86507520, 0), (19752550400, 5767168, 92274688, 0), (19741016064, 5767168, 98041856, 0), (19833290752, 5767168, 103809024, 0), (19839057920, 5767168, 109576192, 0), (19827523584, 5767168, 115343360, 0), (19850592256, 5767168, 121110528, 0), (19856359424, 5767168, 126877696, 0), (19844825088, 5767168, 132644864, 0), (19867893760, 5767168, 138412032, 0), (19873660928, 5767168, 144179200, 0), (19862126592, 5767168, 149946368, 0), (19885195264, 5767168, 155713536, 0), (19890962432, 5767168, 161480704, 0), (19879428096, 5767168, 167247872, 0), (19989004288, 5767168, 173015040, 0), (19994771456, 5767168, 178782208, 0), (19983237120, 5767168, 184549376, 0), (20075511808, 5767168, 190316544, 0), (20081278976, 5767168, 196083712, 0), (20069744640, 5767168, 201850880, 0), (20092813312, 5767168, 207618048, 0), (20098580480, 5767168, 213385216, 0), (20087046144, 5767168, 219152384, 0), (20110114816, 5767168, 224919552, 0), (20115881984, 5767168, 230686720, 0), (20104347648, 5767168, 236453888, 0), (20196622336, 5767168, 242221056, 0), (20202389504, 5767168, 247988224, 0), (20190855168, 5767168, 253755392, 0), (20231225344, 5767168, 259522560, 0), (20236992512, 5767168, 265289728, 0), (20225458176, 5767168, 271056896, 0), (20300431360, 5767168, 276824064, 0), (20306198528, 5767168, 282591232, 0), (20294664192, 5767168, 288358400, 0), (20490747904, 5767168, 294125568, 0), (20496515072, 5767168, 299892736, 0), (20484980736, 5767168, 305659904, 0), (20508049408, 5767168, 311427072, 0), (20513816576, 5767168, 317194240, 0), (20502282240, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6881158f0>, 2: <capsule object NULL at 0x74a6bc762700>}
DEBUG 01-15 16:09:24.207872.207872 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.207722.207722 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.207524.207524 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 56f6bde6-d445-48b9-a0ca-dcb5cb2334d2
DEBUG 01-15 16:09:24.207195.207195 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.207217.207217 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:24.207798.207798 client.py:127] Model loaded
DEBUG 01-15 16:09:24.208008.208008 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.208664.208664 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:24.209336.209336 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 56f6bde6-d445-48b9-a0ca-dcb5cb2334d2
DEBUG 01-15 16:09:24.209542.209542 cuda_h.py:19] end move_flatidxs cost 0.0008363723754882812 seconds
DEBUG 01-15 16:09:24.209649.209649 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:24.209958.209958 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003488779067993164 seconds
DEBUG 01-15 16:09:24.209510.209510 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.210288.210288 cuda_h.py:19] end restore2model cost 0.0006241798400878906 seconds
DEBUG 01-15 16:09:24.210216.210216 cuda_h.py:19] end sllm_worker_task cost 0.013441324234008789 seconds
DEBUG 01-15 16:09:24.213131.213131 cuda_h.py:19] end restore2model cost 0.003856658935546875 seconds
DEBUG 01-15 16:09:24.213332.213332 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007621288299560547 seconds
DEBUG 01-15 16:09:24.213128.213128 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.213873.213873 cuda_h.py:19] end gpu_sexperts cost 0.0002722740173339844 seconds
DEBUG 01-15 16:09:24.214987.214987 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.214764.214764 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:09:24.214745.214745 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.214494.214494 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.215303.215303 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009870529174804688 seconds
DEBUG 01-15 16:09:24.215583.215583 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.215048.215048 cuda_h.py:19] end gpu_group_list cost 0.0002086162567138672 seconds
DEBUG 01-15 16:09:24.216554.216554 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.217035.217035 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010654926300048828 seconds
DEBUG 01-15 16:09:24.217144.217144 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.217019.217019 cuda_h.py:19] end gpu_group_list cost 0.00019359588623046875 seconds
DEBUG 01-15 16:09:24.218790.218790 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.218335.218335 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 56f6bde6-d445-48b9-a0ca-dcb5cb2334d2
DEBUG 01-15 16:09:24.218108.218108 cuda_h.py:19] end group_tensors cost 0.00890350341796875 seconds
DEBUG 01-15 16:09:24.218498.218498 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.222948.222948 cuda_h.py:19] end group pad cost 0.0035097599029541016 seconds
DEBUG 01-15 16:09:24.222593.222593 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.252120.252120 cuda_h.py:19] end group_einsum cost 0.02983999252319336 seconds
DEBUG 01-15 16:09:24.252890.252890 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:24.253667.253667 client.py:127] Model loaded
DEBUG 01-15 16:09:24.253572.253572 cuda_h.py:19] end wait_experts_multi_device cost 0.034778594970703125 seconds
DEBUG 01-15 16:09:24.253117.253117 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.256844.256844 cuda_h.py:19] end get_outputs_cpu1 cost 0.0034744739532470703 seconds
DEBUG 01-15 16:09:24.257783.257783 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04926490783691406 seconds
DEBUG 01-15 16:09:24.257571.257571 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0043964385986328125 seconds
DEBUG 01-15 16:09:24.257791.257791 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.258765.258765 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.258862.258862 cuda_h.py:19] end index_scatter cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:09:24.259739.259739 cuda_h.py:19] end cpuoutputsdeal cost 0.0013637542724609375 seconds
DEBUG 01-15 16:09:24.259933.259933 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.259312.259312 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.259364.259364 cuda_h.py:19] end gpu_group_tensor cost 0.00014925003051757812 seconds
DEBUG 01-15 16:09:24.259266.259266 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.259298.259298 cuda_h.py:19] end gpu_group_tensor cost 0.0001354217529296875 seconds
DEBUG 01-15 16:09:24.259056.259056 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.260623.260623 cuda_h.py:19] end gpu_group_einsum cost 0.00047969818115234375 seconds
DEBUG 01-15 16:09:24.260338.260338 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.261860.261860 cuda_h.py:19] end gpu_group_einsum cost 0.0005059242248535156 seconds
DEBUG 01-15 16:09:24.261182.261182 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.261392.261392 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.261442.261442 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002703666687011719 seconds
DEBUG 01-15 16:09:24.261980.261980 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.261658.261658 cuda_h.py:19] end concat_expert_out cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:09:24.261163.261163 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.261471.261471 cuda_h.py:19] end index_scatter cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:09:24.262466.262466 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008907318115234375 seconds
DEBUG 01-15 16:09:24.262588.262588 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.262477.262477 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.262874.262874 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001575946807861328 seconds
DEBUG 01-15 16:09:24.262200.262200 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.262739.262739 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:09:24.262006.262006 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.262306.262306 cuda_h.py:19] end index_scatter cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:09:24.262970.262970 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004930496215820312 seconds
DEBUG 01-15 16:09:24.262257.262257 cuda_h.py:19] end gpu_experts_multi_device cost 0.0487060546875 seconds
DEBUG 01-15 16:09:24.262690.262690 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.059517621994018555 seconds
DEBUG 01-15 16:09:24.263207.263207 cuda_h.py:19] end prefill_layer cost 0.06667423248291016 seconds
DEBUG 01-15 16:09:24.263249.263249 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:09:24.263429.263429 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.263370.263370 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:09:24.263073.263073 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:09:24.263922.263922 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:09:24.263672.263672 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 3.528594970703125e-05 seconds
DEBUG 01-15 16:09:24.263805.263805 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 6.580352783203125e-05 seconds
DEBUG 01-15 16:09:24.263978.263978 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.263180.263180 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.263806.263806 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.263971.263971 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.264983.264983 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.264817.264817 cuda_h.py:19] end allocate_cuda_memory cost 0.00045418739318847656 seconds
DEBUG 01-15 16:09:24.264300.264300 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.265926.265926 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.265871.265871 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.265708.265708 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 24a44286-4eef-449a-9d0c-699415f77d0a
DEBUG 01-15 16:09:24.265927.265927 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.265618.265618 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.266631.266631 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 24a44286-4eef-449a-9d0c-699415f77d0a
DEBUG 01-15 16:09:24.266689.266689 cuda_h.py:19] end load_into_gpu_async cost 0.0018243789672851562 seconds
DEBUG 01-15 16:09:24.266381.266381 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.267667.267667 cuda_h.py:19] end restore_tensors2 cost 0.0001506805419921875 seconds
DEBUG 01-15 16:09:24.267340.267340 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003180980682373047 seconds
INFO 01-15 16:09:24.267444.267444 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 24a44286-4eef-449a-9d0c-699415f77d0a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.269790.269790 cuda_h.py:19] end self_attn cost 0.003915548324584961 seconds
DEBUG 01-15 16:09:24.269953.269953 cuda_h.py:19] end iln_self_attn_paln cost 0.006340503692626953 seconds
DEBUG 01-15 16:09:24.270968.270968 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:09:24.270201.270201 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.270375.270375 cuda_h.py:19] end gate cost 0.0006222724914550781 seconds
DEBUG 01-15 16:09:24.270966.270966 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.271322.271322 lmp.py:1912] 
DEBUG 01-15 16:09:24.271322.271322 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.271985.271985 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.271350.271350 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.271139.271139 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.271020.271020 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.271663.271663 lmp.py:1917] 
DEBUG 01-15 16:09:24.271663.271663 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.271306.271306 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.271625.271625 lmp.py:1935]   Expert  4 |     10 | CPU
DEBUG 01-15 16:09:24.271506.271506 lmp.py:1935]   Expert 28 |     26 | CPU
DEBUG 01-15 16:09:24.271911.271911 lmp.py:1935]   Expert  7 |     45 | CPU
DEBUG 01-15 16:09:24.271315.271315 lmp.py:1935]   Expert 53 |     58 | CPU
DEBUG 01-15 16:09:24.271481.271481 lmp.py:1935]   Expert 52 |     69 | CPU
DEBUG 01-15 16:09:24.271648.271648 lmp.py:1935]   Expert 43 |     73 | CPU
DEBUG 01-15 16:09:24.271814.271814 lmp.py:1935]   Expert 49 |     80 | CPU
DEBUG 01-15 16:09:24.271457.271457 lmp.py:1935]   Expert 12 |     87 | CPU
DEBUG 01-15 16:09:24.271338.271338 lmp.py:1935]   Expert 47 |    103 | CPU
DEBUG 01-15 16:09:24.271266.271266 lmp.py:1935]   Expert 33 |    106 | CPU
DEBUG 01-15 16:09:24.271432.271432 lmp.py:1935]   Expert 24 |    108 | CPU
DEBUG 01-15 16:09:24.271598.271598 lmp.py:1935]   Expert 50 |    110 | CPU
DEBUG 01-15 16:09:24.271526.271526 lmp.py:1935]   Expert  2 |    112 | CPU
DEBUG 01-15 16:09:24.271215.271215 lmp.py:1935]   Expert 15 |    112 | CPU
DEBUG 01-15 16:09:24.271620.271620 lmp.py:1935]   Expert 60 |    112 | CPU
DEBUG 01-15 16:09:24.271786.271786 lmp.py:1935]   Expert 39 |    115 | CPU
DEBUG 01-15 16:09:24.271906.271906 lmp.py:1935]   Expert 36 |    119 | CPU
DEBUG 01-15 16:09:24.271833.271833 lmp.py:1935]   Expert 25 |    123 | CPU
DEBUG 01-15 16:09:24.271761.271761 lmp.py:1935]   Expert  6 |    128 | CPU
DEBUG 01-15 16:09:24.271450.271450 lmp.py:1935]   Expert 61 |    131 | CPU
DEBUG 01-15 16:09:24.271617.271617 lmp.py:1935]   Expert 59 |    134 | CPU
DEBUG 01-15 16:09:24.271544.271544 lmp.py:1935]   Expert  3 |    141 | CPU
DEBUG 01-15 16:09:24.271472.271472 lmp.py:1935]   Expert 58 |    142 | CPU
DEBUG 01-15 16:09:24.271400.271400 lmp.py:1935]   Expert 27 |    144 | CPU
DEBUG 01-15 16:09:24.271758.271758 lmp.py:1935]   Expert  8 |    150 | CPU
DEBUG 01-15 16:09:24.271593.271593 lmp.py:1935]   Expert 30 |    151 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.271474.271474 lmp.py:1935]   Expert 31 |    153 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.271117.271117 lmp.py:1935]   Expert 10 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.271999.271999 lmp.py:1935]   Expert 38 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.271119.271119 lmp.py:1935]   Expert 57 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.271477.271477 lmp.py:1935]   Expert 14 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.271358.271358 lmp.py:1935]   Expert 40 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.271478.271478 lmp.py:1935]   Expert 41 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.271075.271075 lmp.py:1935]   Expert 37 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.271910.271910 lmp.py:1935]   Expert 32 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.271268.271268 lmp.py:1935]   Expert 54 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.271388.271388 lmp.py:1935]   Expert 46 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.271031.271031 lmp.py:1935]   Expert 42 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.271389.271389 lmp.py:1935]   Expert 11 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.271032.271032 lmp.py:1935]   Expert 19 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.271629.271629 lmp.py:1935]   Expert 34 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.271748.271748 lmp.py:1935]   Expert 18 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.271822.271822 lmp.py:1935]   Expert 22 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272942.272942 lmp.py:1935]   Expert 26 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272538.272538 lmp.py:1935]   Expert  0 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272181.272181 lmp.py:1935]   Expert 56 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272301.272301 lmp.py:1935]   Expert  1 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272183.272183 lmp.py:1935]   Expert 44 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272302.272302 lmp.py:1935]   Expert 51 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272707.272707 lmp.py:1935]   Expert 20 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272350.272350 lmp.py:1935]   Expert 29 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272516.272516 lmp.py:1935]   Expert 48 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272921.272921 lmp.py:1935]   Expert 21 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272564.272564 lmp.py:1935]   Expert 45 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272207.272207 lmp.py:1935]   Expert 35 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272849.272849 lmp.py:1935]   Expert 55 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272446.272446 lmp.py:1935]   Expert 16 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272089.272089 lmp.py:1935]   Expert  5 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272971.272971 lmp.py:1935]   Expert 23 |    371 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272137.272137 lmp.py:1935]   Expert 13 |    383 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272780.272780 lmp.py:1935]   Expert 17 |    436 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272946.272946 lmp.py:1935]   Expert  9 |    457 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272827.272827 lmp.py:1935]   Expert 63 |    462 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.272709.272709 lmp.py:1935]   Expert 62 |   1180 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.272352.272352 lmp.py:1937] 
DEBUG 01-15 16:09:24.272352.272352 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.272413.272413 lmp.py:1938]   CPU:   2538 tokens
DEBUG 01-15 16:09:24.272486.272486 lmp.py:1942]   cuda:1:   4912 tokens (19 experts)
DEBUG 01-15 16:09:24.272652.272652 lmp.py:1942]   cuda:2:   4838 tokens (20 experts)
DEBUG 01-15 16:09:24.272818.272818 lmp.py:1943]   Total GPU:   9750 tokens
DEBUG 01-15 16:09:24.272223.272223 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.272223.272223 lmp.py:1944] 
DEBUG 01-15 16:09:24.272780.272780 cuda_h.py:19] end experts_map_get cost 0.0017309188842773438 seconds
DEBUG 01-15 16:09:24.272054.272054 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.272379.272379 lmp.py:1953] 
DEBUG 01-15 16:09:24.272379.272379 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.272209.272209 cuda_h.py:19] end cpu_experts_submit cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:09:24.272951.272951 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.272410.272410 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.274115.274115 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.5.gate_proj.weight': 0, 'model.layers.17.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.13.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.13.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.13.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.19.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.19.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.19.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.20.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.20.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.20.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.26.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.26.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.26.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.32.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.32.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.32.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.48.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.48.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.48.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.45.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.45.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.45.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.17.mlp.experts.54.gate_proj.weight': 294125568, 'model.layers.17.mlp.experts.54.down_proj.weight': 299892736, 'model.layers.17.mlp.experts.54.up_proj.weight': 305659904, 'model.layers.17.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.17.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.17.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.17.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.17.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.17.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(20663762944, 5767168, 0, 0), (20669530112, 5767168, 5767168, 0), (20657995776, 5767168, 11534336, 0), (20750270464, 5767168, 17301504, 0), (20756037632, 5767168, 23068672, 0), (20744503296, 5767168, 28835840, 0), (20802174976, 5767168, 34603008, 0), (20807942144, 5767168, 40370176, 0), (20796407808, 5767168, 46137344, 0), (20905984000, 5767168, 51904512, 0), (20911751168, 5767168, 57671680, 0), (20900216832, 5767168, 63438848, 0), (20923285504, 5767168, 69206016, 0), (20929052672, 5767168, 74973184, 0), (20917518336, 5767168, 80740352, 0), (20940587008, 5767168, 86507520, 0), (20946354176, 5767168, 92274688, 0), (20934819840, 5767168, 98041856, 0), (20957888512, 5767168, 103809024, 0), (20963655680, 5767168, 109576192, 0), (20952121344, 5767168, 115343360, 0), (21027094528, 5767168, 121110528, 0), (21032861696, 5767168, 126877696, 0), (21021327360, 5767168, 132644864, 0), (21096300544, 5767168, 138412032, 0), (21102067712, 5767168, 144179200, 0), (21090533376, 5767168, 149946368, 0), (21130903552, 5767168, 155713536, 0), (21136670720, 5767168, 161480704, 0), (21125136384, 5767168, 167247872, 0), (21217411072, 5767168, 173015040, 0), (21223178240, 5767168, 178782208, 0), (21211643904, 5767168, 184549376, 0), (21234712576, 5767168, 190316544, 0), (21240479744, 5767168, 196083712, 0), (21228945408, 5767168, 201850880, 0), (21269315584, 5767168, 207618048, 0), (21275082752, 5767168, 213385216, 0), (21263548416, 5767168, 219152384, 0), (21303918592, 5767168, 224919552, 0), (21309685760, 5767168, 230686720, 0), (21298151424, 5767168, 236453888, 0), (21338521600, 5767168, 242221056, 0), (21344288768, 5767168, 247988224, 0), (21332754432, 5767168, 253755392, 0), (21407727616, 5767168, 259522560, 0), (21413494784, 5767168, 265289728, 0), (21401960448, 5767168, 271056896, 0), (21528838144, 5767168, 276824064, 0), (21534605312, 5767168, 282591232, 0), (21523070976, 5767168, 288358400, 0), (21546139648, 5767168, 294125568, 0), (21551906816, 5767168, 299892736, 0), (21540372480, 5767168, 305659904, 0), (21649948672, 5767168, 311427072, 0), (21655715840, 5767168, 317194240, 0), (21644181504, 5767168, 322961408, 0)], 2: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (20594556928, 5767168, 17301504, 0), (20600324096, 5767168, 23068672, 0), (20588789760, 5767168, 28835840, 0), (20732968960, 5767168, 34603008, 0), (20738736128, 5767168, 40370176, 0), (20727201792, 5767168, 46137344, 0), (20767571968, 5767168, 51904512, 0), (20773339136, 5767168, 57671680, 0), (20761804800, 5767168, 63438848, 0), (20819476480, 5767168, 69206016, 0), (20825243648, 5767168, 74973184, 0), (20813709312, 5767168, 80740352, 0), (20854079488, 5767168, 86507520, 0), (20859846656, 5767168, 92274688, 0), (20848312320, 5767168, 98041856, 0), (20871380992, 5767168, 103809024, 0), (20877148160, 5767168, 109576192, 0), (20865613824, 5767168, 115343360, 0), (20888682496, 5767168, 121110528, 0), (20894449664, 5767168, 126877696, 0), (20882915328, 5767168, 132644864, 0), (20975190016, 5767168, 138412032, 0), (20980957184, 5767168, 144179200, 0), (20969422848, 5767168, 149946368, 0), (21078999040, 5767168, 155713536, 0), (21084766208, 5767168, 161480704, 0), (21073231872, 5767168, 167247872, 0), (21113602048, 5767168, 173015040, 0), (21119369216, 5767168, 178782208, 0), (21107834880, 5767168, 184549376, 0), (21165506560, 5767168, 190316544, 0), (21171273728, 5767168, 196083712, 0), (21159739392, 5767168, 201850880, 0), (21182808064, 5767168, 207618048, 0), (21188575232, 5767168, 213385216, 0), (21177040896, 5767168, 219152384, 0), (21286617088, 5767168, 224919552, 0), (21292384256, 5767168, 230686720, 0), (21280849920, 5767168, 236453888, 0), (21355823104, 5767168, 242221056, 0), (21361590272, 5767168, 247988224, 0), (21350055936, 5767168, 253755392, 0), (21373124608, 5767168, 259522560, 0), (21378891776, 5767168, 265289728, 0), (21367357440, 5767168, 271056896, 0), (21459632128, 5767168, 276824064, 0), (21465399296, 5767168, 282591232, 0), (21453864960, 5767168, 288358400, 0), (21511536640, 5767168, 294125568, 0), (21517303808, 5767168, 299892736, 0), (21505769472, 5767168, 305659904, 0), (21563441152, 5767168, 311427072, 0), (21569208320, 5767168, 317194240, 0), (21557673984, 5767168, 322961408, 0), (21667250176, 5767168, 328728576, 0), (21673017344, 5767168, 334495744, 0), (21661483008, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a81456a7f0>, 2: <capsule object NULL at 0x74a6bc7622b0>}
DEBUG 01-15 16:09:24.274536.274536 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:09:24.274805.274805 client.py:127] Model loaded
DEBUG 01-15 16:09:24.274431.274431 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.274587.274587 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6fab9cb4-a53a-4a51-b0c6-c46933bf7e3c
DEBUG 01-15 16:09:24.274843.274843 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.275522.275522 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.274188.274188 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.275066.275066 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:24.276817.276817 cuda_h.py:19] end restore2model cost 0.0009250640869140625 seconds
DEBUG 01-15 16:09:24.276792.276792 cuda_h.py:19] end move_flatidxs cost 0.0008554458618164062 seconds
DEBUG 01-15 16:09:24.276330.276330 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:24.276318.276318 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6fab9cb4-a53a-4a51-b0c6-c46933bf7e3c
DEBUG 01-15 16:09:24.276507.276507 cuda_h.py:19] end sllm_worker_task cost 0.012449026107788086 seconds
DEBUG 01-15 16:09:24.276174.276174 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0041806697845458984 seconds
DEBUG 01-15 16:09:24.277676.277676 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.280628.280628 cuda_h.py:19] end restore2model cost 0.0032968521118164062 seconds
DEBUG 01-15 16:09:24.280320.280320 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007831811904907227 seconds
DEBUG 01-15 16:09:24.280261.280261 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.280384.280384 cuda_h.py:19] end gpu_sexperts cost 0.0002701282501220703 seconds
DEBUG 01-15 16:09:24.280829.280829 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.280751.280751 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.9073486328125e-05 seconds
DEBUG 01-15 16:09:24.280924.280924 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.281150.281150 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.281521.281521 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009448528289794922 seconds
DEBUG 01-15 16:09:24.282702.282702 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.282312.282312 cuda_h.py:19] end gpu_group_list cost 0.00021028518676757812 seconds
DEBUG 01-15 16:09:24.283989.283989 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.284861.284861 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001073598861694336 seconds
DEBUG 01-15 16:09:24.284725.284725 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.284084.284084 cuda_h.py:19] end gpu_group_list cost 0.00023436546325683594 seconds
DEBUG 01-15 16:09:24.285942.285942 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.285487.285487 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6fab9cb4-a53a-4a51-b0c6-c46933bf7e3c
DEBUG 01-15 16:09:24.285171.285171 cuda_h.py:19] end group_tensors cost 0.009044647216796875 seconds
DEBUG 01-15 16:09:24.286719.286719 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.289559.289559 cuda_h.py:19] end group pad cost 0.0030612945556640625 seconds
DEBUG 01-15 16:09:24.289680.289680 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.318769.318769 cuda_h.py:19] end group_einsum cost 0.029471397399902344 seconds
DEBUG 01-15 16:09:24.319204.319204 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:24.319414.319414 client.py:127] Model loaded
DEBUG 01-15 16:09:24.319121.319121 cuda_h.py:19] end wait_experts_multi_device cost 0.034340858459472656 seconds
DEBUG 01-15 16:09:24.319890.319890 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.322247.322247 cuda_h.py:19] end get_outputs_cpu1 cost 0.002997159957885742 seconds
DEBUG 01-15 16:09:24.322889.322889 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04774594306945801 seconds
DEBUG 01-15 16:09:24.323859.323859 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0039675235748291016 seconds
DEBUG 01-15 16:09:24.323868.323868 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.324048.324048 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.324967.324967 cuda_h.py:19] end index_scatter cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:09:24.325049.325049 cuda_h.py:19] end cpuoutputsdeal cost 0.001394510269165039 seconds
DEBUG 01-15 16:09:24.325959.325959 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.325291.325291 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.325674.325674 cuda_h.py:19] end gpu_group_tensor cost 0.0001468658447265625 seconds
DEBUG 01-15 16:09:24.325245.325245 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.325337.325337 cuda_h.py:19] end gpu_group_tensor cost 0.00014400482177734375 seconds
DEBUG 01-15 16:09:24.325612.325612 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.326151.326151 cuda_h.py:19] end gpu_group_einsum cost 0.0008399486541748047 seconds
DEBUG 01-15 16:09:24.326064.326064 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.327294.327294 cuda_h.py:19] end gpu_group_einsum cost 0.0004961490631103516 seconds
DEBUG 01-15 16:09:24.327471.327471 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.327726.327726 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.327173.327173 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024580955505371094 seconds
DEBUG 01-15 16:09:24.328512.328512 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.328502.328502 cuda_h.py:19] end concat_expert_out cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:09:24.328869.328869 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.328382.328382 cuda_h.py:19] end index_scatter cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:09:24.328515.328515 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010595321655273438 seconds
DEBUG 01-15 16:09:24.328525.328525 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.328435.328435 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.329738.329738 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012183189392089844 seconds
DEBUG 01-15 16:09:24.329971.329971 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.329610.329610 cuda_h.py:19] end concat_expert_out cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:09:24.329691.329691 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.329138.329138 cuda_h.py:19] end index_scatter cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:09:24.329039.329039 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004801750183105469 seconds
DEBUG 01-15 16:09:24.329181.329181 cuda_h.py:19] end gpu_experts_multi_device cost 0.04837656021118164 seconds
DEBUG 01-15 16:09:24.329998.329998 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.0594029426574707 seconds
DEBUG 01-15 16:09:24.329354.329354 cuda_h.py:19] end prefill_layer cost 0.06638193130493164 seconds
DEBUG 01-15 16:09:24.329926.329926 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:09:24.329338.329338 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.329749.329749 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:09:24.329876.329876 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:09:24.329241.329241 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:09:24.330507.330507 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 3.266334533691406e-05 seconds
DEBUG 01-15 16:09:24.330780.330780 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 6.0558319091796875e-05 seconds
DEBUG 01-15 16:09:24.330330.330330 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.330948.330948 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.330369.330369 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.330044.330044 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.330817.330817 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.331557.331557 cuda_h.py:19] end allocate_cuda_memory cost 0.00041675567626953125 seconds
DEBUG 01-15 16:09:24.331967.331967 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.331368.331368 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.331280.331280 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.331965.331965 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f60c241d-8681-4abd-b2e4-8d468e8a72e9
DEBUG 01-15 16:09:24.331690.331690 cuda_h.py:10] start self_attn
DEBUG 01-15 16:09:24.331822.331822 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:24.333582.333582 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f60c241d-8681-4abd-b2e4-8d468e8a72e9
DEBUG 01-15 16:09:24.333123.333123 cuda_h.py:19] end load_into_gpu_async cost 0.0017824172973632812 seconds
DEBUG 01-15 16:09:24.333815.333815 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.333571.333571 cuda_h.py:19] end restore_tensors2 cost 0.0001494884490966797 seconds
DEBUG 01-15 16:09:24.333820.333820 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003094911575317383 seconds
INFO 01-15 16:09:24.333713.333713 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f60c241d-8681-4abd-b2e4-8d468e8a72e9
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.335684.335684 cuda_h.py:19] end self_attn cost 0.0038716793060302734 seconds
DEBUG 01-15 16:09:24.336450.336450 cuda_h.py:19] end iln_self_attn_paln cost 0.006133079528808594 seconds
DEBUG 01-15 16:09:24.336055.336055 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:09:24.336831.336831 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.337692.337692 cuda_h.py:19] end gate cost 0.0007135868072509766 seconds
DEBUG 01-15 16:09:24.337581.337581 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.337025.337025 lmp.py:1912] 
DEBUG 01-15 16:09:24.337025.337025 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.337510.337510 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.337988.337988 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.337452.337452 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.337533.337533 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.337659.337659 lmp.py:1917] 
DEBUG 01-15 16:09:24.337659.337659 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.337024.337024 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.337734.337734 lmp.py:1935]   Expert 32 |     32 | CPU
DEBUG 01-15 16:09:24.337814.337814 lmp.py:1935]   Expert  5 |     50 | CPU
DEBUG 01-15 16:09:24.337179.337179 lmp.py:1935]   Expert 30 |     51 | CPU
DEBUG 01-15 16:09:24.337021.337021 lmp.py:1935]   Expert 46 |     78 | CPU
DEBUG 01-15 16:09:24.337148.337148 lmp.py:1935]   Expert  8 |     88 | CPU
DEBUG 01-15 16:09:24.337797.337797 lmp.py:1935]   Expert 40 |     92 | CPU
DEBUG 01-15 16:09:24.337209.337209 lmp.py:1935]   Expert 12 |    101 | CPU
DEBUG 01-15 16:09:24.337620.337620 lmp.py:1935]   Expert 17 |    108 | CPU
DEBUG 01-15 16:09:24.337270.337270 lmp.py:1935]   Expert 27 |    114 | CPU
DEBUG 01-15 16:09:24.338681.338681 lmp.py:1935]   Expert 58 |    115 | CPU
DEBUG 01-15 16:09:24.338808.338808 lmp.py:1935]   Expert  3 |    116 | CPU
DEBUG 01-15 16:09:24.338981.338981 lmp.py:1935]   Expert 60 |    116 | CPU
DEBUG 01-15 16:09:24.338915.338915 lmp.py:1935]   Expert 21 |    118 | CPU
DEBUG 01-15 16:09:24.338088.338088 lmp.py:1935]   Expert 28 |    120 | CPU
DEBUG 01-15 16:09:24.338261.338261 lmp.py:1935]   Expert 29 |    120 | CPU
DEBUG 01-15 16:09:24.338911.338911 lmp.py:1935]   Expert 25 |    127 | CPU
DEBUG 01-15 16:09:24.338084.338084 lmp.py:1935]   Expert 41 |    131 | CPU
DEBUG 01-15 16:09:24.338257.338257 lmp.py:1935]   Expert 35 |    133 | CPU
DEBUG 01-15 16:09:24.338383.338383 lmp.py:1935]   Expert 19 |    135 | CPU
DEBUG 01-15 16:09:24.338987.338987 lmp.py:1935]   Expert  0 |    143 | CPU
DEBUG 01-15 16:09:24.338398.338398 lmp.py:1935]   Expert  6 |    146 | CPU
DEBUG 01-15 16:09:24.338809.338809 lmp.py:1935]   Expert 52 |    148 | CPU
DEBUG 01-15 16:09:24.338473.338473 lmp.py:1935]   Expert 56 |    149 | CPU
DEBUG 01-15 16:09:24.338599.338599 lmp.py:1935]   Expert 37 |    151 | CPU
DEBUG 01-15 16:09:24.338964.338964 lmp.py:1935]   Expert 54 |    151 | CPU
DEBUG 01-15 16:09:24.338760.338760 lmp.py:1935]   Expert 48 |    153 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338840.338840 lmp.py:1935]   Expert 53 |    155 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338682.338682 lmp.py:1935]   Expert 63 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338121.338121 lmp.py:1935]   Expert 36 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338155.338155 lmp.py:1935]   Expert 59 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338804.338804 lmp.py:1935]   Expert  9 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338739.338739 lmp.py:1935]   Expert  1 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338197.338197 lmp.py:1935]   Expert 39 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338800.338800 lmp.py:1935]   Expert 20 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338450.338450 lmp.py:1935]   Expert 42 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338623.338623 lmp.py:1935]   Expert 61 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338319.338319 lmp.py:1935]   Expert 43 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338776.338776 lmp.py:1935]   Expert  7 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338996.338996 lmp.py:1935]   Expert 11 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338692.338692 lmp.py:1935]   Expert 34 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338580.338580 lmp.py:1935]   Expert 47 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338230.338230 lmp.py:1935]   Expert 55 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338403.338403 lmp.py:1935]   Expert 13 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338622.338622 lmp.py:1935]   Expert 16 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338318.338318 lmp.py:1935]   Expert 57 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338491.338491 lmp.py:1935]   Expert 18 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338949.338949 lmp.py:1935]   Expert 15 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338314.338314 lmp.py:1935]   Expert  4 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338963.338963 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338898.338898 lmp.py:1935]   Expert 22 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.338594.338594 lmp.py:1935]   Expert 33 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.338528.338528 lmp.py:1935]   Expert 31 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.339463.339463 lmp.py:1935]   Expert 45 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.339397.339397 lmp.py:1935]   Expert 51 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.339047.339047 lmp.py:1935]   Expert 49 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.339651.339651 lmp.py:1935]   Expert 38 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.339585.339585 lmp.py:1935]   Expert 26 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.339520.339520 lmp.py:1935]   Expert 10 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.339977.339977 lmp.py:1935]   Expert 44 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.339435.339435 lmp.py:1935]   Expert  2 |    298 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.339131.339131 lmp.py:1935]   Expert 24 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.339019.339019 lmp.py:1935]   Expert 14 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.339431.339431 lmp.py:1935]   Expert 23 |    405 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.339365.339365 lmp.py:1935]   Expert 62 |    673 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.339869.339869 lmp.py:1937] 
DEBUG 01-15 16:09:24.339869.339869 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.339327.339327 lmp.py:1938]   CPU:   2833 tokens
DEBUG 01-15 16:09:24.339261.339261 lmp.py:1942]   cuda:1:   4727 tokens (19 experts)
DEBUG 01-15 16:09:24.339719.339719 lmp.py:1942]   cuda:2:   4728 tokens (20 experts)
DEBUG 01-15 16:09:24.339223.339223 lmp.py:1943]   Total GPU:   9455 tokens
DEBUG 01-15 16:09:24.339681.339681 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.339681.339681 lmp.py:1944] 
DEBUG 01-15 16:09:24.339053.339053 cuda_h.py:19] end experts_map_get cost 0.002172231674194336 seconds
DEBUG 01-15 16:09:24.339916.339916 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.339825.339825 lmp.py:1953] 
DEBUG 01-15 16:09:24.339825.339825 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.339529.339529 cuda_h.py:19] end cpu_experts_submit cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:09:24.339424.339424 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.339280.339280 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.340419.340419 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.11.gate_proj.weight': 0, 'model.layers.18.mlp.experts.11.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.11.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.15.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.15.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.15.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.16.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.16.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.16.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.22.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.22.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.22.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.24.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.24.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.24.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.26.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.26.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.26.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.36.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.36.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.36.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.39.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.39.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.39.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.43.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.43.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.43.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.44.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.44.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.44.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.45.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.45.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.45.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.47.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.47.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.47.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.50.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.50.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.50.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.55.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.55.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.55.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.18.mlp.experts.1.gate_proj.weight': 0, 'model.layers.18.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.31.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.31.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.31.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.34.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.34.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.34.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.38.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.38.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.38.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.42.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.42.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.42.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.18.mlp.experts.51.gate_proj.weight': 294125568, 'model.layers.18.mlp.experts.51.down_proj.weight': 299892736, 'model.layers.18.mlp.experts.51.up_proj.weight': 305659904, 'model.layers.18.mlp.experts.57.gate_proj.weight': 311427072, 'model.layers.18.mlp.experts.57.down_proj.weight': 317194240, 'model.layers.18.mlp.experts.57.up_proj.weight': 322961408, 'model.layers.18.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.18.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.18.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(21874868224, 5767168, 0, 0), (21880635392, 5767168, 5767168, 0), (21869101056, 5767168, 11534336, 0), (21944074240, 5767168, 17301504, 0), (21949841408, 5767168, 23068672, 0), (21938307072, 5767168, 28835840, 0), (21961375744, 5767168, 34603008, 0), (21967142912, 5767168, 40370176, 0), (21955608576, 5767168, 46137344, 0), (22065184768, 5767168, 51904512, 0), (22070951936, 5767168, 57671680, 0), (22059417600, 5767168, 63438848, 0), (22099787776, 5767168, 69206016, 0), (22105554944, 5767168, 74973184, 0), (22094020608, 5767168, 80740352, 0), (22134390784, 5767168, 86507520, 0), (22140157952, 5767168, 92274688, 0), (22128623616, 5767168, 98041856, 0), (22307405824, 5767168, 103809024, 0), (22313172992, 5767168, 109576192, 0), (22301638656, 5767168, 115343360, 0), (22359310336, 5767168, 121110528, 0), (22365077504, 5767168, 126877696, 0), (22353543168, 5767168, 132644864, 0), (22428516352, 5767168, 138412032, 0), (22434283520, 5767168, 144179200, 0), (22422749184, 5767168, 149946368, 0), (22445817856, 5767168, 155713536, 0), (22451585024, 5767168, 161480704, 0), (22440050688, 5767168, 167247872, 0), (22463119360, 5767168, 173015040, 0), (22468886528, 5767168, 178782208, 0), (22457352192, 5767168, 184549376, 0), (22497722368, 5767168, 190316544, 0), (22503489536, 5767168, 196083712, 0), (22491955200, 5767168, 201850880, 0), (22532325376, 5767168, 207618048, 0), (22538092544, 5767168, 213385216, 0), (22526558208, 5767168, 219152384, 0), (22549626880, 5767168, 224919552, 0), (22555394048, 5767168, 230686720, 0), (22543859712, 5767168, 236453888, 0), (22601531392, 5767168, 242221056, 0), (22607298560, 5767168, 247988224, 0), (22595764224, 5767168, 253755392, 0), (22636134400, 5767168, 259522560, 0), (22641901568, 5767168, 265289728, 0), (22630367232, 5767168, 271056896, 0), (22705340416, 5767168, 276824064, 0), (22711107584, 5767168, 282591232, 0), (22699573248, 5767168, 288358400, 0), (22739943424, 5767168, 294125568, 0), (22745710592, 5767168, 299892736, 0), (22734176256, 5767168, 305659904, 0), (22757244928, 5767168, 311427072, 0), (22763012096, 5767168, 317194240, 0), (22751477760, 5767168, 322961408, 0)], 2: [(21701853184, 5767168, 0, 0), (21707620352, 5767168, 5767168, 0), (21696086016, 5767168, 11534336, 0), (21719154688, 5767168, 17301504, 0), (21724921856, 5767168, 23068672, 0), (21713387520, 5767168, 28835840, 0), (21753757696, 5767168, 34603008, 0), (21759524864, 5767168, 40370176, 0), (21747990528, 5767168, 46137344, 0), (21805662208, 5767168, 51904512, 0), (21811429376, 5767168, 57671680, 0), (21799895040, 5767168, 63438848, 0), (21840265216, 5767168, 69206016, 0), (21846032384, 5767168, 74973184, 0), (21834498048, 5767168, 80740352, 0), (21857566720, 5767168, 86507520, 0), (21863333888, 5767168, 92274688, 0), (21851799552, 5767168, 98041856, 0), (21909471232, 5767168, 103809024, 0), (21915238400, 5767168, 109576192, 0), (21903704064, 5767168, 115343360, 0), (21926772736, 5767168, 121110528, 0), (21932539904, 5767168, 126877696, 0), (21921005568, 5767168, 132644864, 0), (21995978752, 5767168, 138412032, 0), (22001745920, 5767168, 144179200, 0), (21990211584, 5767168, 149946368, 0), (22030581760, 5767168, 155713536, 0), (22036348928, 5767168, 161480704, 0), (22024814592, 5767168, 167247872, 0), (22082486272, 5767168, 173015040, 0), (22088253440, 5767168, 178782208, 0), (22076719104, 5767168, 184549376, 0), (22220898304, 5767168, 190316544, 0), (22226665472, 5767168, 196083712, 0), (22215131136, 5767168, 201850880, 0), (22255501312, 5767168, 207618048, 0), (22261268480, 5767168, 213385216, 0), (22249734144, 5767168, 219152384, 0), (22272802816, 5767168, 224919552, 0), (22278569984, 5767168, 230686720, 0), (22267035648, 5767168, 236453888, 0), (22342008832, 5767168, 242221056, 0), (22347776000, 5767168, 247988224, 0), (22336241664, 5767168, 253755392, 0), (22411214848, 5767168, 259522560, 0), (22416982016, 5767168, 265289728, 0), (22405447680, 5767168, 271056896, 0), (22515023872, 5767168, 276824064, 0), (22520791040, 5767168, 282591232, 0), (22509256704, 5767168, 288358400, 0), (22566928384, 5767168, 294125568, 0), (22572695552, 5767168, 299892736, 0), (22561161216, 5767168, 305659904, 0), (22670737408, 5767168, 311427072, 0), (22676504576, 5767168, 317194240, 0), (22664970240, 5767168, 322961408, 0), (22774546432, 5767168, 328728576, 0), (22780313600, 5767168, 334495744, 0), (22768779264, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a680759b90>, 2: <capsule object NULL at 0x74a6bc762190>}
DEBUG 01-15 16:09:24.340887.340887 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.340061.340061 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.340108.340108 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5208a06e-cd1d-4e85-bb73-9ebfdf67796f
DEBUG 01-15 16:09:24.340899.340899 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:24.341812.341812 client.py:127] Model loaded
DEBUG 01-15 16:09:24.341749.341749 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.341167.341167 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.341152.341152 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:24.342430.342430 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5208a06e-cd1d-4e85-bb73-9ebfdf67796f
DEBUG 01-15 16:09:24.342614.342614 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030155181884765625 seconds
DEBUG 01-15 16:09:24.342903.342903 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.342016.342016 cuda_h.py:19] end move_flatidxs cost 0.0008344650268554688 seconds
DEBUG 01-15 16:09:24.342838.342838 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:24.344287.344287 cuda_h.py:19] end restore2model cost 0.0011355876922607422 seconds
DEBUG 01-15 16:09:24.344161.344161 cuda_h.py:19] end sllm_worker_task cost 0.013997793197631836 seconds
DEBUG 01-15 16:09:24.348362.348362 cuda_h.py:19] end restore2model cost 0.0055522918701171875 seconds
DEBUG 01-15 16:09:24.348498.348498 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00890660285949707 seconds
DEBUG 01-15 16:09:24.348592.348592 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.348286.348286 cuda_h.py:19] end gpu_sexperts cost 0.00033211708068847656 seconds
DEBUG 01-15 16:09:24.349368.349368 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.349734.349734 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2172927856445312e-05 seconds
DEBUG 01-15 16:09:24.349775.349775 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.349345.349345 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.348298.348298 cuda_h.py:19] end group_tensors cost 0.005567789077758789 seconds
DEBUG 01-15 16:09:24.349854.349854 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.350894.350894 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001514434814453125 seconds
DEBUG 01-15 16:09:24.350255.350255 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.351376.351376 cuda_h.py:19] end gpu_group_list cost 0.0003960132598876953 seconds
DEBUG 01-15 16:09:24.352688.352688 cuda_h.py:19] end group pad cost 0.0032901763916015625 seconds
DEBUG 01-15 16:09:24.352100.352100 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.352154.352154 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.359525.359525 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0061397552490234375 seconds
DEBUG 01-15 16:09:24.359561.359561 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.361485.361485 cuda_h.py:19] end gpu_group_list cost 0.0008983612060546875 seconds
DEBUG 01-15 16:09:24.363583.363583 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.363787.363787 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5208a06e-cd1d-4e85-bb73-9ebfdf67796f
DEBUG 01-15 16:09:24.383445.383445 cuda_h.py:19] end group_einsum cost 0.03057122230529785 seconds
DEBUG 01-15 16:09:24.383980.383980 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.386331.386331 cuda_h.py:19] end get_outputs_cpu1 cost 0.0031168460845947266 seconds
DEBUG 01-15 16:09:24.387306.387306 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04564404487609863 seconds
INFO 01-15 16:09:24.388305.388305 client.py:127] Model loaded
DEBUG 01-15 16:09:24.388628.388628 cuda_h.py:19] end wait_experts_multi_device cost 0.024716854095458984 seconds
DEBUG 01-15 16:09:24.388689.388689 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.389952.389952 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005025863647460938 seconds
DEBUG 01-15 16:09:24.389464.389464 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.390479.390479 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.390564.390564 cuda_h.py:19] end index_scatter cost 8.249282836914062e-05 seconds
DEBUG 01-15 16:09:24.390129.390129 cuda_h.py:19] end cpuoutputsdeal cost 0.001430511474609375 seconds
DEBUG 01-15 16:09:24.390907.390907 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.390723.390723 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.390795.390795 cuda_h.py:19] end gpu_group_tensor cost 0.00015807151794433594 seconds
DEBUG 01-15 16:09:24.390757.390757 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.391849.391849 cuda_h.py:19] end gpu_group_tensor cost 0.0001380443572998047 seconds
DEBUG 01-15 16:09:24.391714.391714 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.391312.391312 cuda_h.py:19] end gpu_group_einsum cost 0.0005970001220703125 seconds
DEBUG 01-15 16:09:24.391872.391872 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.392758.392758 cuda_h.py:19] end gpu_group_einsum cost 0.0004894733428955078 seconds
DEBUG 01-15 16:09:24.392405.392405 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.392124.392124 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.393225.393225 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020599365234375 seconds
DEBUG 01-15 16:09:24.393941.393941 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.393401.393401 cuda_h.py:19] end concat_expert_out cost 5.698204040527344e-05 seconds
DEBUG 01-15 16:09:24.393464.393464 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.393315.393315 cuda_h.py:19] end index_scatter cost 6.0558319091796875e-05 seconds
DEBUG 01-15 16:09:24.393899.393899 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008699893951416016 seconds
DEBUG 01-15 16:09:24.393526.393526 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.393475.393475 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.393541.393541 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015473365783691406 seconds
DEBUG 01-15 16:09:24.393350.393350 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.394671.394671 cuda_h.py:19] end concat_expert_out cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:09:24.394905.394905 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.394756.394756 cuda_h.py:19] end index_scatter cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:09:24.394334.394334 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005536079406738281 seconds
DEBUG 01-15 16:09:24.394250.394250 cuda_h.py:19] end gpu_experts_multi_device cost 0.04523062705993652 seconds
DEBUG 01-15 16:09:24.394227.394227 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.05809164047241211 seconds
DEBUG 01-15 16:09:24.394830.394830 cuda_h.py:19] end prefill_layer cost 0.06490468978881836 seconds
DEBUG 01-15 16:09:24.394800.394800 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:09:24.394317.394317 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.394027.394027 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:09:24.395260.395260 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:09:24.395923.395923 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:09:24.395502.395502 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 3.910064697265625e-05 seconds
DEBUG 01-15 16:09:24.395728.395728 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 6.794929504394531e-05 seconds
DEBUG 01-15 16:09:24.395609.395609 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.395797.395797 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.395847.395847 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.395098.395098 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.395447.395447 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.396188.396188 cuda_h.py:19] end allocate_cuda_memory cost 0.0004153251647949219 seconds
DEBUG 01-15 16:09:24.396459.396459 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.396787.396787 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.396242.396242 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.396556.396556 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 294ad09a-35c9-4b33-9a6a-6452523870c4
DEBUG 01-15 16:09:24.396126.396126 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.397718.397718 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.398407.398407 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 294ad09a-35c9-4b33-9a6a-6452523870c4
DEBUG 01-15 16:09:24.398796.398796 cuda_h.py:19] end load_into_gpu_async cost 0.0020411014556884766 seconds
DEBUG 01-15 16:09:24.398110.398110 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.398429.398429 cuda_h.py:19] end restore_tensors2 cost 0.00014519691467285156 seconds
DEBUG 01-15 16:09:24.399718.399718 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0033636093139648438 seconds
INFO 01-15 16:09:24.399974.399974 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 294ad09a-35c9-4b33-9a6a-6452523870c4
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.400663.400663 cuda_h.py:19] end self_attn cost 0.0034842491149902344 seconds
DEBUG 01-15 16:09:24.401335.401335 cuda_h.py:19] end iln_self_attn_paln cost 0.005844593048095703 seconds
DEBUG 01-15 16:09:24.401211.401211 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:09:24.401398.401398 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.401453.401453 cuda_h.py:19] end gate cost 0.0006392002105712891 seconds
DEBUG 01-15 16:09:24.401045.401045 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.402280.402280 lmp.py:1912] 
DEBUG 01-15 16:09:24.402280.402280 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.402798.402798 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.402971.402971 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.402329.402329 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.402495.402495 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.402708.402708 lmp.py:1917] 
DEBUG 01-15 16:09:24.402708.402708 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.402635.402635 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.402762.402762 lmp.py:1935]   Expert 44 |     40 | CPU
DEBUG 01-15 16:09:24.402451.402451 lmp.py:1935]   Expert  1 |     49 | CPU
DEBUG 01-15 16:09:24.402425.402425 lmp.py:1935]   Expert 60 |     62 | CPU
DEBUG 01-15 16:09:24.402922.402922 lmp.py:1935]   Expert 28 |     68 | CPU
DEBUG 01-15 16:09:24.402658.402658 lmp.py:1935]   Expert 48 |     77 | CPU
DEBUG 01-15 16:09:24.402155.402155 lmp.py:1935]   Expert 27 |     86 | CPU
DEBUG 01-15 16:09:24.402129.402129 lmp.py:1935]   Expert  0 |     98 | CPU
DEBUG 01-15 16:09:24.402865.402865 lmp.py:1935]   Expert 62 |    109 | CPU
DEBUG 01-15 16:09:24.402601.402601 lmp.py:1935]   Expert 42 |    111 | CPU
DEBUG 01-15 16:09:24.402575.402575 lmp.py:1935]   Expert 22 |    113 | CPU
DEBUG 01-15 16:09:24.402025.402025 lmp.py:1935]   Expert 30 |    113 | CPU
DEBUG 01-15 16:09:24.402476.402476 lmp.py:1935]   Expert 59 |    115 | CPU
DEBUG 01-15 16:09:24.402927.402927 lmp.py:1935]   Expert 58 |    122 | CPU
DEBUG 01-15 16:09:24.402901.402901 lmp.py:1935]   Expert  8 |    123 | CPU
DEBUG 01-15 16:09:24.402875.402875 lmp.py:1935]   Expert 12 |    126 | CPU
DEBUG 01-15 16:09:24.402611.402611 lmp.py:1935]   Expert 16 |    127 | CPU
DEBUG 01-15 16:09:24.402108.402108 lmp.py:1935]   Expert 50 |    133 | CPU
DEBUG 01-15 16:09:24.402605.402605 lmp.py:1935]   Expert  5 |    143 | CPU
DEBUG 01-15 16:09:24.402341.402341 lmp.py:1935]   Expert 56 |    143 | CPU
DEBUG 01-15 16:09:24.402838.402838 lmp.py:1935]   Expert 55 |    151 | CPU
DEBUG 01-15 16:09:24.402574.402574 lmp.py:1935]   Expert 57 |    153 | CPU
DEBUG 01-15 16:09:24.402071.402071 lmp.py:1935]   Expert 15 |    154 | CPU
DEBUG 01-15 16:09:24.402237.402237 lmp.py:1935]   Expert 26 |    154 | CPU
DEBUG 01-15 16:09:24.402165.402165 lmp.py:1935]   Expert 32 |    157 | CPU
DEBUG 01-15 16:09:24.402139.402139 lmp.py:1935]   Expert 47 |    158 | CPU
DEBUG 01-15 16:09:24.402259.402259 lmp.py:1935]   Expert 34 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.402425.402425 lmp.py:1935]   Expert 24 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.402452.402452 lmp.py:1935]   Expert 52 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.402241.402241 lmp.py:1935]   Expert  2 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.402599.402599 lmp.py:1935]   Expert  6 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.402719.402719 lmp.py:1935]   Expert 40 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.402839.402839 lmp.py:1935]   Expert 41 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.402720.402720 lmp.py:1935]   Expert  3 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.402363.402363 lmp.py:1935]   Expert 18 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.402244.402244 lmp.py:1935]   Expert 54 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.402841.402841 lmp.py:1935]   Expert 13 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.402961.402961 lmp.py:1935]   Expert 19 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.402842.402842 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.402247.402247 lmp.py:1935]   Expert 46 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.402890.402890 lmp.py:1935]   Expert 37 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.402771.402771 lmp.py:1935]   Expert 25 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403891.403891 lmp.py:1935]   Expert 51 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403011.403011 lmp.py:1935]   Expert 17 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403607.403607 lmp.py:1935]   Expert 43 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403489.403489 lmp.py:1935]   Expert 11 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403609.403609 lmp.py:1935]   Expert 31 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403490.403490 lmp.py:1935]   Expert 23 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403895.403895 lmp.py:1935]   Expert 35 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403776.403776 lmp.py:1935]   Expert 49 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403657.403657 lmp.py:1935]   Expert 39 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403300.403300 lmp.py:1935]   Expert 53 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403182.403182 lmp.py:1935]   Expert 10 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403063.403063 lmp.py:1935]   Expert 33 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403660.403660 lmp.py:1935]   Expert 36 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403303.403303 lmp.py:1935]   Expert 38 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403184.403184 lmp.py:1935]   Expert  4 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403827.403827 lmp.py:1935]   Expert 21 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403470.403470 lmp.py:1935]   Expert 14 |    348 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403352.403352 lmp.py:1935]   Expert 63 |    369 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403233.403233 lmp.py:1935]   Expert 45 |    374 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403353.403353 lmp.py:1935]   Expert 61 |    389 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403711.403711 lmp.py:1935]   Expert  9 |    395 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403592.403592 lmp.py:1935]   Expert 29 |    490 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.403474.403474 lmp.py:1935]   Expert  7 |    515 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.403686.403686 lmp.py:1937] 
DEBUG 01-15 16:09:24.403686.403686 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.403091.403091 lmp.py:1938]   CPU:   2885 tokens
DEBUG 01-15 16:09:24.403687.403687 lmp.py:1942]   cuda:1:   4624 tokens (19 experts)
DEBUG 01-15 16:09:24.403092.403092 lmp.py:1942]   cuda:2:   4779 tokens (20 experts)
DEBUG 01-15 16:09:24.403020.403020 lmp.py:1943]   Total GPU:   9403 tokens
DEBUG 01-15 16:09:24.403663.403663 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.403663.403663 lmp.py:1944] 
DEBUG 01-15 16:09:24.403028.403028 cuda_h.py:19] end experts_map_get cost 0.0016450881958007812 seconds
DEBUG 01-15 16:09:24.403540.403540 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.403388.403388 lmp.py:1953] 
DEBUG 01-15 16:09:24.403388.403388 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.403695.403695 cuda_h.py:19] end cpu_experts_submit cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:09:24.403960.403960 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.403578.403578 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.405128.405128 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.7.gate_proj.weight': 0, 'model.layers.19.mlp.experts.7.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.7.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.10.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.10.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.10.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.36.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.36.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.36.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.37.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.37.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.37.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.40.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.40.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.40.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.41.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.41.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.41.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.61.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.61.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.61.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.19.mlp.experts.2.gate_proj.weight': 0, 'model.layers.19.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.31.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.31.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.31.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.33.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.33.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.33.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.34.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.34.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.34.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.35.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.35.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.35.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.43.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.43.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.43.up_proj.weight': 271056896, 'model.layers.19.mlp.experts.45.gate_proj.weight': 276824064, 'model.layers.19.mlp.experts.45.down_proj.weight': 282591232, 'model.layers.19.mlp.experts.45.up_proj.weight': 288358400, 'model.layers.19.mlp.experts.46.gate_proj.weight': 294125568, 'model.layers.19.mlp.experts.46.down_proj.weight': 299892736, 'model.layers.19.mlp.experts.46.up_proj.weight': 305659904, 'model.layers.19.mlp.experts.49.gate_proj.weight': 311427072, 'model.layers.19.mlp.experts.49.down_proj.weight': 317194240, 'model.layers.19.mlp.experts.49.up_proj.weight': 322961408, 'model.layers.19.mlp.experts.53.gate_proj.weight': 328728576, 'model.layers.19.mlp.experts.53.down_proj.weight': 334495744, 'model.layers.19.mlp.experts.53.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(22912958464, 5767168, 0, 0), (22918725632, 5767168, 5767168, 0), (22907191296, 5767168, 11534336, 0), (22964862976, 5767168, 17301504, 0), (22970630144, 5767168, 23068672, 0), (22959095808, 5767168, 28835840, 0), (22982164480, 5767168, 34603008, 0), (22987931648, 5767168, 40370176, 0), (22976397312, 5767168, 46137344, 0), (23016767488, 5767168, 51904512, 0), (23022534656, 5767168, 57671680, 0), (23011000320, 5767168, 63438848, 0), (23085973504, 5767168, 69206016, 0), (23091740672, 5767168, 74973184, 0), (23080206336, 5767168, 80740352, 0), (23137878016, 5767168, 86507520, 0), (23143645184, 5767168, 92274688, 0), (23132110848, 5767168, 98041856, 0), (23155179520, 5767168, 103809024, 0), (23160946688, 5767168, 109576192, 0), (23149412352, 5767168, 115343360, 0), (23189782528, 5767168, 121110528, 0), (23195549696, 5767168, 126877696, 0), (23184015360, 5767168, 132644864, 0), (23414702080, 5767168, 138412032, 0), (23420469248, 5767168, 144179200, 0), (23408934912, 5767168, 149946368, 0), (23432003584, 5767168, 155713536, 0), (23437770752, 5767168, 161480704, 0), (23426236416, 5767168, 167247872, 0), (23449305088, 5767168, 173015040, 0), (23455072256, 5767168, 178782208, 0), (23443537920, 5767168, 184549376, 0), (23466606592, 5767168, 190316544, 0), (23472373760, 5767168, 196083712, 0), (23460839424, 5767168, 201850880, 0), (23483908096, 5767168, 207618048, 0), (23489675264, 5767168, 213385216, 0), (23478140928, 5767168, 219152384, 0), (23501209600, 5767168, 224919552, 0), (23506976768, 5767168, 230686720, 0), (23495442432, 5767168, 236453888, 0), (23674224640, 5767168, 242221056, 0), (23679991808, 5767168, 247988224, 0), (23668457472, 5767168, 253755392, 0), (23691526144, 5767168, 259522560, 0), (23697293312, 5767168, 265289728, 0), (23685758976, 5767168, 271056896, 0), (23726129152, 5767168, 276824064, 0), (23731896320, 5767168, 282591232, 0), (23720361984, 5767168, 288358400, 0), (23847239680, 5767168, 294125568, 0), (23853006848, 5767168, 299892736, 0), (23841472512, 5767168, 305659904, 0), (23881842688, 5767168, 311427072, 0), (23887609856, 5767168, 317194240, 0), (23876075520, 5767168, 322961408, 0)], 2: [(22826450944, 5767168, 0, 0), (22832218112, 5767168, 5767168, 0), (22820683776, 5767168, 11534336, 0), (22843752448, 5767168, 17301504, 0), (22849519616, 5767168, 23068672, 0), (22837985280, 5767168, 28835840, 0), (22861053952, 5767168, 34603008, 0), (22866821120, 5767168, 40370176, 0), (22855286784, 5767168, 46137344, 0), (22895656960, 5767168, 51904512, 0), (22901424128, 5767168, 57671680, 0), (22889889792, 5767168, 63438848, 0), (22947561472, 5767168, 69206016, 0), (22953328640, 5767168, 74973184, 0), (22941794304, 5767168, 80740352, 0), (23034068992, 5767168, 86507520, 0), (23039836160, 5767168, 92274688, 0), (23028301824, 5767168, 98041856, 0), (23103275008, 5767168, 103809024, 0), (23109042176, 5767168, 109576192, 0), (23097507840, 5767168, 115343360, 0), (23120576512, 5767168, 121110528, 0), (23126343680, 5767168, 126877696, 0), (23114809344, 5767168, 132644864, 0), (23207084032, 5767168, 138412032, 0), (23212851200, 5767168, 144179200, 0), (23201316864, 5767168, 149946368, 0), (23224385536, 5767168, 155713536, 0), (23230152704, 5767168, 161480704, 0), (23218618368, 5767168, 167247872, 0), (23293591552, 5767168, 173015040, 0), (23299358720, 5767168, 178782208, 0), (23287824384, 5767168, 184549376, 0), (23328194560, 5767168, 190316544, 0), (23333961728, 5767168, 196083712, 0), (23322427392, 5767168, 201850880, 0), (23362797568, 5767168, 207618048, 0), (23368564736, 5767168, 213385216, 0), (23357030400, 5767168, 219152384, 0), (23380099072, 5767168, 224919552, 0), (23385866240, 5767168, 230686720, 0), (23374331904, 5767168, 236453888, 0), (23397400576, 5767168, 242221056, 0), (23403167744, 5767168, 247988224, 0), (23391633408, 5767168, 253755392, 0), (23535812608, 5767168, 259522560, 0), (23541579776, 5767168, 265289728, 0), (23530045440, 5767168, 271056896, 0), (23570415616, 5767168, 276824064, 0), (23576182784, 5767168, 282591232, 0), (23564648448, 5767168, 288358400, 0), (23587717120, 5767168, 294125568, 0), (23593484288, 5767168, 299892736, 0), (23581949952, 5767168, 305659904, 0), (23639621632, 5767168, 311427072, 0), (23645388800, 5767168, 317194240, 0), (23633854464, 5767168, 322961408, 0), (23708827648, 5767168, 328728576, 0), (23714594816, 5767168, 334495744, 0), (23703060480, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6801b21c0>, 2: <capsule object NULL at 0x74a6bc762550>}
DEBUG 01-15 16:09:24.406390.406390 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.406518.406518 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.406559.406559 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8f08ffaf-37a6-4e0c-8958-18dffd979e4d
DEBUG 01-15 16:09:24.415652.415652 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.415624.415624 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.415438.415438 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:24.415563.415563 client.py:127] Model loaded
DEBUG 01-15 16:09:24.416038.416038 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.416164.416164 cuda_h.py:19] end move_flatidxs cost 0.0008616447448730469 seconds
DEBUG 01-15 16:09:24.416762.416762 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:24.417838.417838 cuda_h.py:19] end restore2model cost 0.0010466575622558594 seconds
DEBUG 01-15 16:09:24.417717.417717 cuda_h.py:19] end sllm_worker_task cost 0.021851062774658203 seconds
INFO 01-15 16:09:24.417657.417657 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8f08ffaf-37a6-4e0c-8958-18dffd979e4d
DEBUG 01-15 16:09:24.418050.418050 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.014254570007324219 seconds
DEBUG 01-15 16:09:24.418980.418980 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.421455.421455 cuda_h.py:19] end restore2model cost 0.0032629966735839844 seconds
DEBUG 01-15 16:09:24.421948.421948 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.017804861068725586 seconds
DEBUG 01-15 16:09:24.421220.421220 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.421715.421715 cuda_h.py:19] end gpu_sexperts cost 0.00029778480529785156 seconds
DEBUG 01-15 16:09:24.421975.421975 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.421559.421559 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:09:24.421209.421209 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.422912.422912 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.422467.422467 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009417533874511719 seconds
DEBUG 01-15 16:09:24.423171.423171 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.423630.423630 cuda_h.py:19] end gpu_group_list cost 0.00020384788513183594 seconds
DEBUG 01-15 16:09:24.422524.422524 cuda_h.py:19] end group_tensors cost 0.005889177322387695 seconds
DEBUG 01-15 16:09:24.423238.423238 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.424808.424808 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.425447.425447 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011959075927734375 seconds
DEBUG 01-15 16:09:24.425602.425602 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.425883.425883 cuda_h.py:19] end gpu_group_list cost 0.000240325927734375 seconds
DEBUG 01-15 16:09:24.426417.426417 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.426585.426585 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8f08ffaf-37a6-4e0c-8958-18dffd979e4d
DEBUG 01-15 16:09:24.427603.427603 cuda_h.py:19] end group pad cost 0.0035130977630615234 seconds
DEBUG 01-15 16:09:24.427532.427532 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.458066.458066 cuda_h.py:19] end group_einsum cost 0.03108072280883789 seconds
DEBUG 01-15 16:09:24.458277.458277 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.461687.461687 cuda_h.py:19] end get_outputs_cpu1 cost 0.00330352783203125 seconds
DEBUG 01-15 16:09:24.462200.462200 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04698777198791504 seconds
INFO 01-15 16:09:24.463397.463397 client.py:127] Model loaded
DEBUG 01-15 16:09:24.463999.463999 cuda_h.py:19] end wait_experts_multi_device cost 0.03687095642089844 seconds
DEBUG 01-15 16:09:24.463187.463187 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.464094.464094 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0007314682006835938 seconds
DEBUG 01-15 16:09:24.464376.464376 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.466251.466251 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.466161.466161 cuda_h.py:19] end index_scatter cost 0.00014591217041015625 seconds
DEBUG 01-15 16:09:24.467667.467667 cuda_h.py:19] end cpuoutputsdeal cost 0.0024874210357666016 seconds
DEBUG 01-15 16:09:24.467964.467964 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.467556.467556 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.467588.467588 cuda_h.py:19] end gpu_group_tensor cost 0.0002949237823486328 seconds
DEBUG 01-15 16:09:24.467088.467088 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.468040.468040 cuda_h.py:19] end gpu_group_tensor cost 0.00027108192443847656 seconds
DEBUG 01-15 16:09:24.468730.468730 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.469355.469355 cuda_h.py:19] end gpu_group_einsum cost 0.0009431838989257812 seconds
DEBUG 01-15 16:09:24.469350.469350 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.470098.470098 cuda_h.py:19] end gpu_group_einsum cost 0.0010678768157958984 seconds
DEBUG 01-15 16:09:24.471861.471861 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.471199.471199 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.471330.471330 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004413127899169922 seconds
DEBUG 01-15 16:09:24.471154.471154 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.472750.472750 cuda_h.py:19] end concat_expert_out cost 0.000118255615234375 seconds
DEBUG 01-15 16:09:24.472570.472570 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.472990.472990 cuda_h.py:19] end index_scatter cost 0.00014138221740722656 seconds
DEBUG 01-15 16:09:24.473979.473979 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001832723617553711 seconds
DEBUG 01-15 16:09:24.473079.473079 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.473468.473468 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.473761.473761 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003457069396972656 seconds
DEBUG 01-15 16:09:24.473717.473717 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.474028.474028 cuda_h.py:19] end concat_expert_out cost 0.00011992454528808594 seconds
DEBUG 01-15 16:09:24.474695.474695 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.474927.474927 cuda_h.py:19] end index_scatter cost 0.00011992454528808594 seconds
DEBUG 01-15 16:09:24.474572.474572 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011832714080810547 seconds
DEBUG 01-15 16:09:24.474896.474896 cuda_h.py:19] end gpu_experts_multi_device cost 0.05272102355957031 seconds
DEBUG 01-15 16:09:24.474492.474492 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.07379436492919922 seconds
DEBUG 01-15 16:09:24.475310.475310 cuda_h.py:19] end prefill_layer cost 0.08063316345214844 seconds
DEBUG 01-15 16:09:24.475076.475076 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:09:24.475741.475741 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.475929.475929 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:09:24.475640.475640 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:09:24.476788.476788 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:09:24.476693.476693 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:09:24.476828.476828 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 0.00014901161193847656 seconds
DEBUG 01-15 16:09:24.476385.476385 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.476555.476555 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.476197.476197 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.476153.476153 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.476227.476227 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.477636.477636 cuda_h.py:19] end allocate_cuda_memory cost 0.00044226646423339844 seconds
DEBUG 01-15 16:09:24.477582.477582 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.477718.477718 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.477987.477987 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.477017.477017 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1f5e48e4-e392-4a1b-b74e-d8cdc0bf4f08
DEBUG 01-15 16:09:24.478427.478427 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.478516.478516 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.479146.479146 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1f5e48e4-e392-4a1b-b74e-d8cdc0bf4f08
DEBUG 01-15 16:09:24.479681.479681 cuda_h.py:19] end load_into_gpu_async cost 0.0018656253814697266 seconds
DEBUG 01-15 16:09:24.479657.479657 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.479844.479844 cuda_h.py:19] end restore_tensors2 cost 0.00015091896057128906 seconds
DEBUG 01-15 16:09:24.480253.480253 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0031850337982177734 seconds
INFO 01-15 16:09:24.480231.480231 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1f5e48e4-e392-4a1b-b74e-d8cdc0bf4f08
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.482859.482859 cuda_h.py:19] end self_attn cost 0.0044248104095458984 seconds
DEBUG 01-15 16:09:24.483433.483433 cuda_h.py:19] end iln_self_attn_paln cost 0.0069501399993896484 seconds
DEBUG 01-15 16:09:24.483369.483369 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:09:24.483854.483854 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.484210.484210 cuda_h.py:19] end gate cost 0.0007083415985107422 seconds
DEBUG 01-15 16:09:24.484245.484245 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.484571.484571 lmp.py:1912] 
DEBUG 01-15 16:09:24.484571.484571 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.484248.484248 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.484527.484527 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.484514.484514 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.484926.484926 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.484668.484668 lmp.py:1917] 
DEBUG 01-15 16:09:24.484668.484668 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.484126.484126 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.484213.484213 lmp.py:1935]   Expert 54 |     22 | CPU
DEBUG 01-15 16:09:24.484532.484532 lmp.py:1935]   Expert  3 |     32 | CPU
DEBUG 01-15 16:09:24.484943.484943 lmp.py:1935]   Expert  8 |     41 | CPU
DEBUG 01-15 16:09:24.484162.484162 lmp.py:1935]   Expert 28 |     42 | CPU
DEBUG 01-15 16:09:24.484381.484381 lmp.py:1935]   Expert 43 |     54 | CPU
DEBUG 01-15 16:09:24.484124.484124 lmp.py:1935]   Expert 63 |     56 | CPU
DEBUG 01-15 16:09:24.484866.484866 lmp.py:1935]   Expert 36 |     79 | CPU
DEBUG 01-15 16:09:24.484847.484847 lmp.py:1935]   Expert 38 |     79 | CPU
DEBUG 01-15 16:09:24.484589.484589 lmp.py:1935]   Expert  6 |     80 | CPU
DEBUG 01-15 16:09:24.485286.485286 lmp.py:1935]   Expert 39 |     95 | CPU
DEBUG 01-15 16:09:24.485028.485028 lmp.py:1935]   Expert 57 |     97 | CPU
DEBUG 01-15 16:09:24.485770.485770 lmp.py:1935]   Expert 41 |    105 | CPU
DEBUG 01-15 16:09:24.485513.485513 lmp.py:1935]   Expert 12 |    110 | CPU
DEBUG 01-15 16:09:24.485970.485970 lmp.py:1935]   Expert 52 |    110 | CPU
DEBUG 01-15 16:09:24.485905.485905 lmp.py:1935]   Expert 19 |    117 | CPU
DEBUG 01-15 16:09:24.485171.485171 lmp.py:1935]   Expert 47 |    126 | CPU
DEBUG 01-15 16:09:24.485436.485436 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:09:24.485702.485702 lmp.py:1935]   Expert 22 |    144 | CPU
DEBUG 01-15 16:09:24.485159.485159 lmp.py:1935]   Expert 46 |    146 | CPU
DEBUG 01-15 16:09:24.485663.485663 lmp.py:1935]   Expert 50 |    151 | CPU
DEBUG 01-15 16:09:24.485167.485167 lmp.py:1935]   Expert 20 |    164 | CPU
DEBUG 01-15 16:09:24.485671.485671 lmp.py:1935]   Expert 24 |    164 | CPU
DEBUG 01-15 16:09:24.485937.485937 lmp.py:1935]   Expert 40 |    164 | CPU
DEBUG 01-15 16:09:24.485441.485441 lmp.py:1935]   Expert 55 |    166 | CPU
DEBUG 01-15 16:09:24.485707.485707 lmp.py:1935]   Expert 23 |    168 | CPU
DEBUG 01-15 16:09:24.485833.485833 lmp.py:1935]   Expert  2 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485721.485721 lmp.py:1935]   Expert 37 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485656.485656 lmp.py:1935]   Expert 53 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485829.485829 lmp.py:1935]   Expert 61 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485525.485525 lmp.py:1935]   Expert 42 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485983.485983 lmp.py:1935]   Expert 49 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485871.485871 lmp.py:1935]   Expert 21 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485805.485805 lmp.py:1935]   Expert 18 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485263.485263 lmp.py:1935]   Expert 33 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485959.485959 lmp.py:1935]   Expert  0 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485894.485894 lmp.py:1935]   Expert 32 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485351.485351 lmp.py:1935]   Expert 30 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485763.485763 lmp.py:1935]   Expert  5 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485697.485697 lmp.py:1935]   Expert 16 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485155.485155 lmp.py:1935]   Expert  7 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485612.485612 lmp.py:1935]   Expert 14 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485832.485832 lmp.py:1935]   Expert 34 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485051.485051 lmp.py:1935]   Expert 31 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485939.485939 lmp.py:1935]   Expert 59 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485066.485066 lmp.py:1935]   Expert 60 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485239.485239 lmp.py:1935]   Expert 62 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485696.485696 lmp.py:1935]   Expert  9 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485154.485154 lmp.py:1935]   Expert 17 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485327.485327 lmp.py:1935]   Expert 10 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485215.485215 lmp.py:1935]   Expert 29 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485865.485865 lmp.py:1935]   Expert  4 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485561.485561 lmp.py:1935]   Expert 15 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485641.485641 lmp.py:1935]   Expert 58 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.485006.485006 lmp.py:1935]   Expert 26 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.485848.485848 lmp.py:1935]   Expert 51 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.486213.486213 lmp.py:1935]   Expert 11 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.486578.486578 lmp.py:1935]   Expert 44 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.486943.486943 lmp.py:1935]   Expert 27 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.486785.486785 lmp.py:1935]   Expert 56 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.486912.486912 lmp.py:1935]   Expert  1 |    333 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.486277.486277 lmp.py:1935]   Expert 45 |    369 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.486118.486118 lmp.py:1935]   Expert 25 |    462 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.486782.486782 lmp.py:1935]   Expert 35 |    517 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.486015.486015 lmp.py:1935]   Expert 48 |    644 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.486903.486903 lmp.py:1937] 
DEBUG 01-15 16:09:24.486903.486903 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.486268.486268 lmp.py:1938]   CPU:   2650 tokens
DEBUG 01-15 16:09:24.486348.486348 lmp.py:1942]   cuda:1:   4737 tokens (19 experts)
DEBUG 01-15 16:09:24.486998.486998 lmp.py:1942]   cuda:2:   4901 tokens (20 experts)
DEBUG 01-15 16:09:24.486648.486648 lmp.py:1943]   Total GPU:   9638 tokens
DEBUG 01-15 16:09:24.486536.486536 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.486536.486536 lmp.py:1944] 
DEBUG 01-15 16:09:24.486160.486160 cuda_h.py:19] end experts_map_get cost 0.0021500587463378906 seconds
DEBUG 01-15 16:09:24.486341.486341 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.486381.486381 lmp.py:1953] 
DEBUG 01-15 16:09:24.486381.486381 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.486456.486456 cuda_h.py:19] end cpu_experts_submit cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:09:24.486198.486198 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.486035.486035 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.487434.487434 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.33.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.33.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.33.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.34.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.34.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.34.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.45.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.45.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.45.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.51.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.51.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.51.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.59.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.59.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.59.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.20.mlp.experts.1.gate_proj.weight': 0, 'model.layers.20.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.25.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.25.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.25.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.44.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.44.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.44.up_proj.weight': 271056896, 'model.layers.20.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.20.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.20.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.20.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.20.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.20.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.20.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.20.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.20.mlp.experts.60.up_proj.weight': 322961408, 'model.layers.20.mlp.experts.62.gate_proj.weight': 328728576, 'model.layers.20.mlp.experts.62.down_proj.weight': 334495744, 'model.layers.20.mlp.experts.62.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (23985651712, 5767168, 17301504, 0), (23991418880, 5767168, 23068672, 0), (23979884544, 5767168, 28835840, 0), (24020254720, 5767168, 34603008, 0), (24026021888, 5767168, 40370176, 0), (24014487552, 5767168, 46137344, 0), (24158666752, 5767168, 51904512, 0), (24164433920, 5767168, 57671680, 0), (24152899584, 5767168, 63438848, 0), (24193269760, 5767168, 69206016, 0), (24199036928, 5767168, 74973184, 0), (24187502592, 5767168, 80740352, 0), (24262475776, 5767168, 86507520, 0), (24268242944, 5767168, 92274688, 0), (24256708608, 5767168, 98041856, 0), (24348983296, 5767168, 103809024, 0), (24354750464, 5767168, 109576192, 0), (24343216128, 5767168, 115343360, 0), (24366284800, 5767168, 121110528, 0), (24372051968, 5767168, 126877696, 0), (24360517632, 5767168, 132644864, 0), (24400887808, 5767168, 138412032, 0), (24406654976, 5767168, 144179200, 0), (24395120640, 5767168, 149946368, 0), (24435490816, 5767168, 155713536, 0), (24441257984, 5767168, 161480704, 0), (24429723648, 5767168, 167247872, 0), (24470093824, 5767168, 173015040, 0), (24475860992, 5767168, 178782208, 0), (24464326656, 5767168, 184549376, 0), (24487395328, 5767168, 190316544, 0), (24493162496, 5767168, 196083712, 0), (24481628160, 5767168, 201850880, 0), (24677711872, 5767168, 207618048, 0), (24683479040, 5767168, 213385216, 0), (24671944704, 5767168, 219152384, 0), (24729616384, 5767168, 224919552, 0), (24735383552, 5767168, 230686720, 0), (24723849216, 5767168, 236453888, 0), (24781520896, 5767168, 242221056, 0), (24787288064, 5767168, 247988224, 0), (24775753728, 5767168, 253755392, 0), (24816123904, 5767168, 259522560, 0), (24821891072, 5767168, 265289728, 0), (24810356736, 5767168, 271056896, 0), (24868028416, 5767168, 276824064, 0), (24873795584, 5767168, 282591232, 0), (24862261248, 5767168, 288358400, 0), (24919932928, 5767168, 294125568, 0), (24925700096, 5767168, 299892736, 0), (24914165760, 5767168, 305659904, 0), (24954535936, 5767168, 311427072, 0), (24960303104, 5767168, 317194240, 0), (24948768768, 5767168, 322961408, 0)], 2: [(23916445696, 5767168, 0, 0), (23922212864, 5767168, 5767168, 0), (23910678528, 5767168, 11534336, 0), (23933747200, 5767168, 17301504, 0), (23939514368, 5767168, 23068672, 0), (23927980032, 5767168, 28835840, 0), (23968350208, 5767168, 34603008, 0), (23974117376, 5767168, 40370176, 0), (23962583040, 5767168, 46137344, 0), (24054857728, 5767168, 51904512, 0), (24060624896, 5767168, 57671680, 0), (24049090560, 5767168, 63438848, 0), (24072159232, 5767168, 69206016, 0), (24077926400, 5767168, 74973184, 0), (24066392064, 5767168, 80740352, 0), (24089460736, 5767168, 86507520, 0), (24095227904, 5767168, 92274688, 0), (24083693568, 5767168, 98041856, 0), (24141365248, 5767168, 103809024, 0), (24147132416, 5767168, 109576192, 0), (24135598080, 5767168, 115343360, 0), (24175968256, 5767168, 121110528, 0), (24181735424, 5767168, 126877696, 0), (24170201088, 5767168, 132644864, 0), (24210571264, 5767168, 138412032, 0), (24216338432, 5767168, 144179200, 0), (24204804096, 5767168, 149946368, 0), (24331681792, 5767168, 155713536, 0), (24337448960, 5767168, 161480704, 0), (24325914624, 5767168, 167247872, 0), (24418189312, 5767168, 173015040, 0), (24423956480, 5767168, 178782208, 0), (24412422144, 5767168, 184549376, 0), (24452792320, 5767168, 190316544, 0), (24458559488, 5767168, 196083712, 0), (24447025152, 5767168, 201850880, 0), (24504696832, 5767168, 207618048, 0), (24510464000, 5767168, 213385216, 0), (24498929664, 5767168, 219152384, 0), (24539299840, 5767168, 224919552, 0), (24545067008, 5767168, 230686720, 0), (24533532672, 5767168, 236453888, 0), (24625807360, 5767168, 242221056, 0), (24631574528, 5767168, 247988224, 0), (24620040192, 5767168, 253755392, 0), (24660410368, 5767168, 259522560, 0), (24666177536, 5767168, 265289728, 0), (24654643200, 5767168, 271056896, 0), (24746917888, 5767168, 276824064, 0), (24752685056, 5767168, 282591232, 0), (24741150720, 5767168, 288358400, 0), (24902631424, 5767168, 294125568, 0), (24908398592, 5767168, 299892736, 0), (24896864256, 5767168, 305659904, 0), (24937234432, 5767168, 311427072, 0), (24943001600, 5767168, 317194240, 0), (24931467264, 5767168, 322961408, 0), (24971837440, 5767168, 328728576, 0), (24977604608, 5767168, 334495744, 0), (24966070272, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc1ba040>, 2: <capsule object NULL at 0x74a6bc762370>}
DEBUG 01-15 16:09:24.487957.487957 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.487522.487522 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.487562.487562 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e43d7655-e04f-4ee8-a229-23bdb39aaad2
DEBUG 01-15 16:09:24.487476.487476 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.487808.487808 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.488833.488833 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:24.488226.488226 client.py:127] Model loaded
DEBUG 01-15 16:09:24.488574.488574 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.489050.489050 cuda_h.py:19] end move_flatidxs cost 0.0008699893951416016 seconds
DEBUG 01-15 16:09:24.489840.489840 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:24.489598.489598 cuda_h.py:19] end restore2model cost 0.0010156631469726562 seconds
INFO 01-15 16:09:24.489973.489973 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e43d7655-e04f-4ee8-a229-23bdb39aaad2
DEBUG 01-15 16:09:24.489533.489533 cuda_h.py:19] end sllm_worker_task cost 0.012932062149047852 seconds
DEBUG 01-15 16:09:24.490344.490344 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034568309783935547 seconds
DEBUG 01-15 16:09:24.490031.490031 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.493540.493540 cuda_h.py:19] end restore2model cost 0.0032868385314941406 seconds
DEBUG 01-15 16:09:24.493012.493012 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007073640823364258 seconds
DEBUG 01-15 16:09:24.493331.493331 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.493315.493315 cuda_h.py:19] end gpu_sexperts cost 0.0002722740173339844 seconds
DEBUG 01-15 16:09:24.494667.494667 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.494728.494728 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:09:24.494663.494663 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.494843.494843 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.495107.495107 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009360313415527344 seconds
DEBUG 01-15 16:09:24.495764.495764 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.495554.495554 cuda_h.py:19] end gpu_group_list cost 0.00020194053649902344 seconds
DEBUG 01-15 16:09:24.496137.496137 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.497864.497864 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010709762573242188 seconds
DEBUG 01-15 16:09:24.497635.497635 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.497060.497060 cuda_h.py:19] end gpu_group_list cost 0.00021266937255859375 seconds
DEBUG 01-15 16:09:24.498395.498395 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.498701.498701 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e43d7655-e04f-4ee8-a229-23bdb39aaad2
DEBUG 01-15 16:09:24.499464.499464 cuda_h.py:19] end group_tensors cost 0.010282039642333984 seconds
DEBUG 01-15 16:09:24.500629.500629 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.503395.503395 cuda_h.py:19] end group pad cost 0.0034227371215820312 seconds
DEBUG 01-15 16:09:24.503754.503754 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:24.533257.533257 client.py:127] Model loaded
DEBUG 01-15 16:09:24.533448.533448 cuda_h.py:19] end wait_experts_multi_device cost 0.03493523597717285 seconds
DEBUG 01-15 16:09:24.533198.533198 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.535026.535026 cuda_h.py:19] end group_einsum cost 0.031377553939819336 seconds
DEBUG 01-15 16:09:24.535884.535884 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.538359.538359 cuda_h.py:19] end get_outputs_cpu1 cost 0.003076791763305664 seconds
DEBUG 01-15 16:09:24.539029.539029 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05124616622924805 seconds
DEBUG 01-15 16:09:24.540957.540957 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007170200347900391 seconds
DEBUG 01-15 16:09:24.540376.540376 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.541619.541619 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.541955.541955 cuda_h.py:19] end index_scatter cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:09:24.542251.542251 cuda_h.py:19] end cpuoutputsdeal cost 0.0013456344604492188 seconds
DEBUG 01-15 16:09:24.542107.542107 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.542625.542625 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.542504.542504 cuda_h.py:19] end gpu_group_tensor cost 0.00013017654418945312 seconds
DEBUG 01-15 16:09:24.542929.542929 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.542742.542742 cuda_h.py:19] end gpu_group_tensor cost 0.00011754035949707031 seconds
DEBUG 01-15 16:09:24.542871.542871 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.543696.543696 cuda_h.py:19] end gpu_group_einsum cost 0.00046133995056152344 seconds
DEBUG 01-15 16:09:24.543461.543461 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.543401.543401 cuda_h.py:19] end gpu_group_einsum cost 0.0003325939178466797 seconds
DEBUG 01-15 16:09:24.543106.543106 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.543228.543228 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.544169.544169 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001666545867919922 seconds
DEBUG 01-15 16:09:24.544540.544540 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.544119.544119 cuda_h.py:19] end concat_expert_out cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:09:24.544671.544671 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.544640.544640 cuda_h.py:19] end index_scatter cost 5.340576171875e-05 seconds
DEBUG 01-15 16:09:24.544920.544920 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007326602935791016 seconds
DEBUG 01-15 16:09:24.544989.544989 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.544594.544594 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.544658.544658 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012636184692382812 seconds
DEBUG 01-15 16:09:24.544408.544408 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.544841.544841 cuda_h.py:19] end concat_expert_out cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:09:24.544962.544962 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.545607.545607 cuda_h.py:19] end index_scatter cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:09:24.545747.545747 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00045490264892578125 seconds
DEBUG 01-15 16:09:24.545743.545743 cuda_h.py:19] end gpu_experts_multi_device cost 0.05101943016052246 seconds
DEBUG 01-15 16:09:24.545507.545507 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.061827898025512695 seconds
DEBUG 01-15 16:09:24.545493.545493 cuda_h.py:19] end prefill_layer cost 0.06966376304626465 seconds
DEBUG 01-15 16:09:24.545482.545482 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:09:24.545893.545893 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.545258.545258 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:09:24.545669.545669 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:09:24.545703.545703 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:09:24.545970.545970 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 3.24249267578125e-05 seconds
DEBUG 01-15 16:09:24.545242.545242 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:09:24.545554.545554 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.545226.545226 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.545712.545712 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.546457.546457 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.546353.546353 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.546487.546487 cuda_h.py:19] end allocate_cuda_memory cost 0.0007772445678710938 seconds
DEBUG 01-15 16:09:24.547174.547174 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.547998.547998 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.547822.547822 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.547003.547003 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d9edc57b-c711-4ac1-8abb-5d753cffa6c7
DEBUG 01-15 16:09:24.547848.547848 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.547218.547218 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.548816.548816 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d9edc57b-c711-4ac1-8abb-5d753cffa6c7
DEBUG 01-15 16:09:24.549239.549239 cuda_h.py:19] end load_into_gpu_async cost 0.0018498897552490234 seconds
DEBUG 01-15 16:09:24.549634.549634 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.549635.549635 cuda_h.py:19] end restore_tensors2 cost 0.0001480579376220703 seconds
DEBUG 01-15 16:09:24.549553.549553 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0034241676330566406 seconds
INFO 01-15 16:09:24.549830.549830 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d9edc57b-c711-4ac1-8abb-5d753cffa6c7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.551979.551979 cuda_h.py:19] end self_attn cost 0.0038619041442871094 seconds
DEBUG 01-15 16:09:24.551803.551803 cuda_h.py:19] end iln_self_attn_paln cost 0.0060176849365234375 seconds
DEBUG 01-15 16:09:24.551725.551725 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:09:24.551866.551866 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.552690.552690 cuda_h.py:19] end gate cost 0.0006430149078369141 seconds
DEBUG 01-15 16:09:24.552155.552155 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.553689.553689 lmp.py:1912] 
DEBUG 01-15 16:09:24.553689.553689 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.553214.553214 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.553579.553579 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.553844.553844 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.553249.553249 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.553177.553177 lmp.py:1917] 
DEBUG 01-15 16:09:24.553177.553177 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.553343.553343 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.553708.553708 lmp.py:1935]   Expert 44 |     29 | CPU
DEBUG 01-15 16:09:24.553351.553351 lmp.py:1935]   Expert  9 |     34 | CPU
DEBUG 01-15 16:09:24.553040.553040 lmp.py:1935]   Expert 11 |     38 | CPU
DEBUG 01-15 16:09:24.553206.553206 lmp.py:1935]   Expert 56 |     57 | CPU
DEBUG 01-15 16:09:24.553419.553419 lmp.py:1935]   Expert 54 |     78 | CPU
DEBUG 01-15 16:09:24.553015.553015 lmp.py:1935]   Expert  7 |     89 | CPU
DEBUG 01-15 16:09:24.553897.553897 lmp.py:1935]   Expert 62 |     90 | CPU
DEBUG 01-15 16:09:24.553540.553540 lmp.py:1935]   Expert 47 |     93 | CPU
DEBUG 01-15 16:09:24.553421.553421 lmp.py:1935]   Expert 51 |    100 | CPU
DEBUG 01-15 16:09:24.553302.553302 lmp.py:1935]   Expert 60 |    106 | CPU
DEBUG 01-15 16:09:24.553422.553422 lmp.py:1935]   Expert 52 |    107 | CPU
DEBUG 01-15 16:09:24.553542.553542 lmp.py:1935]   Expert 22 |    108 | CPU
DEBUG 01-15 16:09:24.553185.553185 lmp.py:1935]   Expert 53 |    109 | CPU
DEBUG 01-15 16:09:24.553828.553828 lmp.py:1935]   Expert 41 |    110 | CPU
DEBUG 01-15 16:09:24.553756.553756 lmp.py:1935]   Expert  8 |    126 | CPU
DEBUG 01-15 16:09:24.553399.553399 lmp.py:1935]   Expert 48 |    126 | CPU
DEBUG 01-15 16:09:24.553572.553572 lmp.py:1935]   Expert  1 |    129 | CPU
DEBUG 01-15 16:09:24.553691.553691 lmp.py:1935]   Expert  6 |    130 | CPU
DEBUG 01-15 16:09:24.553050.553050 lmp.py:1935]   Expert  2 |    131 | CPU
DEBUG 01-15 16:09:24.553169.553169 lmp.py:1935]   Expert 32 |    131 | CPU
DEBUG 01-15 16:09:24.553812.553812 lmp.py:1935]   Expert 35 |    138 | CPU
DEBUG 01-15 16:09:24.553647.553647 lmp.py:1935]   Expert 23 |    140 | CPU
DEBUG 01-15 16:09:24.553052.553052 lmp.py:1935]   Expert 27 |    140 | CPU
DEBUG 01-15 16:09:24.553933.553933 lmp.py:1935]   Expert 59 |    142 | CPU
DEBUG 01-15 16:09:24.553338.553338 lmp.py:1935]   Expert 26 |    145 | CPU
DEBUG 01-15 16:09:24.553663.553663 lmp.py:1935]   Expert 39 |    148 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.553452.553452 lmp.py:1935]   Expert 50 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553572.553572 lmp.py:1935]   Expert 14 |    158 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553930.553930 lmp.py:1935]   Expert 24 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553812.553812 lmp.py:1935]   Expert 46 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.553455.553455 lmp.py:1935]   Expert 38 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.553813.553813 lmp.py:1935]   Expert  0 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553933.553933 lmp.py:1935]   Expert 34 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.553576.553576 lmp.py:1935]   Expert  4 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553457.553457 lmp.py:1935]   Expert 49 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.553292.553292 lmp.py:1935]   Expert 40 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.553366.553366 lmp.py:1935]   Expert 63 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553485.553485 lmp.py:1935]   Expert  5 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553082.553082 lmp.py:1935]   Expert 19 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.553394.553394 lmp.py:1935]   Expert 13 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553991.553991 lmp.py:1935]   Expert 29 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.553872.553872 lmp.py:1935]   Expert 43 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553230.553230 lmp.py:1935]   Expert 61 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.553588.553588 lmp.py:1935]   Expert 57 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.553947.553947 lmp.py:1935]   Expert 33 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554590.554590 lmp.py:1935]   Expert 31 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554471.554471 lmp.py:1935]   Expert 16 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554114.554114 lmp.py:1935]   Expert 37 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554995.554995 lmp.py:1935]   Expert  3 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554638.554638 lmp.py:1935]   Expert 20 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554758.554758 lmp.py:1935]   Expert 15 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554116.554116 lmp.py:1935]   Expert 36 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554951.554951 lmp.py:1935]   Expert 18 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554071.554071 lmp.py:1935]   Expert 12 |    284 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554953.554953 lmp.py:1935]   Expert 17 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554596.554596 lmp.py:1935]   Expert 28 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554431.554431 lmp.py:1935]   Expert 55 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554789.554789 lmp.py:1935]   Expert 30 |    320 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554638.554638 lmp.py:1935]   Expert 25 |    324 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554996.554996 lmp.py:1935]   Expert 58 |    333 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554116.554116 lmp.py:1935]   Expert 10 |    362 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554474.554474 lmp.py:1935]   Expert 45 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554832.554832 lmp.py:1935]   Expert 21 |    394 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.554475.554475 lmp.py:1935]   Expert 42 |    649 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.554356.554356 lmp.py:1937] 
DEBUG 01-15 16:09:24.554356.554356 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.554715.554715 lmp.py:1938]   CPU:   2626 tokens
DEBUG 01-15 16:09:24.554788.554788 lmp.py:1942]   cuda:1:   4833 tokens (19 experts)
DEBUG 01-15 16:09:24.554193.554193 lmp.py:1942]   cuda:2:   4829 tokens (20 experts)
DEBUG 01-15 16:09:24.554597.554597 lmp.py:1943]   Total GPU:   9662 tokens
DEBUG 01-15 16:09:24.554048.554048 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.554048.554048 lmp.py:1944] 
DEBUG 01-15 16:09:24.554652.554652 cuda_h.py:19] end experts_map_get cost 0.001741170883178711 seconds
DEBUG 01-15 16:09:24.554025.554025 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.554589.554589 lmp.py:1953] 
DEBUG 01-15 16:09:24.554589.554589 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.554710.554710 cuda_h.py:19] end cpu_experts_submit cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:24.554711.554711 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.554170.554170 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.556458.556458 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.10.gate_proj.weight': 0, 'model.layers.21.mlp.experts.10.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.10.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.15.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.15.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.15.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.16.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.16.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.16.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.18.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.18.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.18.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.25.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.25.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.25.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.33.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.33.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.33.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.38.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.38.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.38.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.42.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.42.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.42.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.61.up_proj.weight': 322961408}, 2: {'model.layers.21.mlp.experts.0.gate_proj.weight': 0, 'model.layers.21.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.31.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.31.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.31.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.36.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.36.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.36.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.50.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.50.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.50.up_proj.weight': 288358400, 'model.layers.21.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.21.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.21.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.21.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.21.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.21.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.21.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.21.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.21.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(25179455488, 5767168, 0, 0), (25185222656, 5767168, 5767168, 0), (25173688320, 5767168, 11534336, 0), (25265963008, 5767168, 17301504, 0), (25271730176, 5767168, 23068672, 0), (25260195840, 5767168, 28835840, 0), (25283264512, 5767168, 34603008, 0), (25289031680, 5767168, 40370176, 0), (25277497344, 5767168, 46137344, 0), (25317867520, 5767168, 51904512, 0), (25323634688, 5767168, 57671680, 0), (25312100352, 5767168, 63438848, 0), (25335169024, 5767168, 69206016, 0), (25340936192, 5767168, 74973184, 0), (25329401856, 5767168, 80740352, 0), (25352470528, 5767168, 86507520, 0), (25358237696, 5767168, 92274688, 0), (25346703360, 5767168, 98041856, 0), (25438978048, 5767168, 103809024, 0), (25444745216, 5767168, 109576192, 0), (25433210880, 5767168, 115343360, 0), (25490882560, 5767168, 121110528, 0), (25496649728, 5767168, 126877696, 0), (25485115392, 5767168, 132644864, 0), (25508184064, 5767168, 138412032, 0), (25513951232, 5767168, 144179200, 0), (25502416896, 5767168, 149946368, 0), (25577390080, 5767168, 155713536, 0), (25583157248, 5767168, 161480704, 0), (25571622912, 5767168, 167247872, 0), (25594691584, 5767168, 173015040, 0), (25600458752, 5767168, 178782208, 0), (25588924416, 5767168, 184549376, 0), (25663897600, 5767168, 190316544, 0), (25669664768, 5767168, 196083712, 0), (25658130432, 5767168, 201850880, 0), (25681199104, 5767168, 207618048, 0), (25686966272, 5767168, 213385216, 0), (25675431936, 5767168, 219152384, 0), (25698500608, 5767168, 224919552, 0), (25704267776, 5767168, 230686720, 0), (25692733440, 5767168, 236453888, 0), (25733103616, 5767168, 242221056, 0), (25738870784, 5767168, 247988224, 0), (25727336448, 5767168, 253755392, 0), (25802309632, 5767168, 259522560, 0), (25808076800, 5767168, 265289728, 0), (25796542464, 5767168, 271056896, 0), (25854214144, 5767168, 276824064, 0), (25859981312, 5767168, 282591232, 0), (25848446976, 5767168, 288358400, 0), (25958023168, 5767168, 294125568, 0), (25963790336, 5767168, 299892736, 0), (25952256000, 5767168, 305659904, 0), (26061832192, 5767168, 311427072, 0), (26067599360, 5767168, 317194240, 0), (26056065024, 5767168, 322961408, 0)], 2: [(25006440448, 5767168, 0, 0), (25012207616, 5767168, 5767168, 0), (25000673280, 5767168, 11534336, 0), (25058344960, 5767168, 17301504, 0), (25064112128, 5767168, 23068672, 0), (25052577792, 5767168, 28835840, 0), (25075646464, 5767168, 34603008, 0), (25081413632, 5767168, 40370176, 0), (25069879296, 5767168, 46137344, 0), (25092947968, 5767168, 51904512, 0), (25098715136, 5767168, 57671680, 0), (25087180800, 5767168, 63438848, 0), (25214058496, 5767168, 69206016, 0), (25219825664, 5767168, 74973184, 0), (25208291328, 5767168, 80740352, 0), (25231360000, 5767168, 86507520, 0), (25237127168, 5767168, 92274688, 0), (25225592832, 5767168, 98041856, 0), (25248661504, 5767168, 103809024, 0), (25254428672, 5767168, 109576192, 0), (25242894336, 5767168, 115343360, 0), (25300566016, 5767168, 121110528, 0), (25306333184, 5767168, 126877696, 0), (25294798848, 5767168, 132644864, 0), (25369772032, 5767168, 138412032, 0), (25375539200, 5767168, 144179200, 0), (25364004864, 5767168, 149946368, 0), (25421676544, 5767168, 155713536, 0), (25427443712, 5767168, 161480704, 0), (25415909376, 5767168, 167247872, 0), (25525485568, 5767168, 173015040, 0), (25531252736, 5767168, 178782208, 0), (25519718400, 5767168, 184549376, 0), (25542787072, 5767168, 190316544, 0), (25548554240, 5767168, 196083712, 0), (25537019904, 5767168, 201850880, 0), (25629294592, 5767168, 207618048, 0), (25635061760, 5767168, 213385216, 0), (25623527424, 5767168, 219152384, 0), (25646596096, 5767168, 224919552, 0), (25652363264, 5767168, 230686720, 0), (25640828928, 5767168, 236453888, 0), (25750405120, 5767168, 242221056, 0), (25756172288, 5767168, 247988224, 0), (25744637952, 5767168, 253755392, 0), (25785008128, 5767168, 259522560, 0), (25790775296, 5767168, 265289728, 0), (25779240960, 5767168, 271056896, 0), (25871515648, 5767168, 276824064, 0), (25877282816, 5767168, 282591232, 0), (25865748480, 5767168, 288358400, 0), (25992626176, 5767168, 294125568, 0), (25998393344, 5767168, 299892736, 0), (25986859008, 5767168, 305659904, 0), (26009927680, 5767168, 311427072, 0), (26015694848, 5767168, 317194240, 0), (26004160512, 5767168, 322961408, 0), (26096435200, 5767168, 328728576, 0), (26102202368, 5767168, 334495744, 0), (26090668032, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a81456e4c0>, 2: <capsule object NULL at 0x74a6bc762640>}
DEBUG 01-15 16:09:24.556608.556608 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.556080.556080 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.556598.556598 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bd8eed2b-911b-4335-9d7c-617a1b4e218b
DEBUG 01-15 16:09:24.556382.556382 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:24.557072.557072 client.py:127] Model loaded
DEBUG 01-15 16:09:24.557235.557235 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.557223.557223 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.557341.557341 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:24.557389.557389 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bd8eed2b-911b-4335-9d7c-617a1b4e218b
DEBUG 01-15 16:09:24.558862.558862 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034265518188476562 seconds
DEBUG 01-15 16:09:24.558301.558301 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.558351.558351 cuda_h.py:19] end move_flatidxs cost 0.0008366107940673828 seconds
DEBUG 01-15 16:09:24.558696.558696 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:24.559326.559326 cuda_h.py:19] end restore2model cost 0.001474142074584961 seconds
DEBUG 01-15 16:09:24.559895.559895 cuda_h.py:19] end sllm_worker_task cost 0.013872146606445312 seconds
DEBUG 01-15 16:09:24.562272.562272 cuda_h.py:19] end restore2model cost 0.004607200622558594 seconds
DEBUG 01-15 16:09:24.562572.562572 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008302450180053711 seconds
DEBUG 01-15 16:09:24.563845.563845 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.563736.563736 cuda_h.py:19] end gpu_sexperts cost 0.00027489662170410156 seconds
DEBUG 01-15 16:09:24.563519.563519 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.563772.563772 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:09:24.563945.563945 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.563840.563840 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.564509.564509 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009534358978271484 seconds
DEBUG 01-15 16:09:24.564928.564928 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.564300.564300 cuda_h.py:19] end gpu_group_list cost 0.00021004676818847656 seconds
DEBUG 01-15 16:09:24.565189.565189 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.566876.566876 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010786056518554688 seconds
DEBUG 01-15 16:09:24.566693.566693 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.567179.567179 cuda_h.py:19] end gpu_group_list cost 0.00022363662719726562 seconds
DEBUG 01-15 16:09:24.567076.567076 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.567336.567336 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bd8eed2b-911b-4335-9d7c-617a1b4e218b
DEBUG 01-15 16:09:24.567199.567199 cuda_h.py:19] end group_tensors cost 0.009252548217773438 seconds
DEBUG 01-15 16:09:24.568008.568008 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.572749.572749 cuda_h.py:19] end group pad cost 0.0032994747161865234 seconds
DEBUG 01-15 16:09:24.572155.572155 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:24.602536.602536 client.py:127] Model loaded
DEBUG 01-15 16:09:24.602437.602437 cuda_h.py:19] end wait_experts_multi_device cost 0.034842729568481445 seconds
DEBUG 01-15 16:09:24.602440.602440 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.603404.603404 cuda_h.py:19] end group_einsum cost 0.0312800407409668 seconds
DEBUG 01-15 16:09:24.603243.603243 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.606066.606066 cuda_h.py:19] end get_outputs_cpu1 cost 0.003188610076904297 seconds
DEBUG 01-15 16:09:24.607770.607770 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05009341239929199 seconds
DEBUG 01-15 16:09:24.608860.608860 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005753755569458008 seconds
DEBUG 01-15 16:09:24.608670.608670 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.609795.609795 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.610132.610132 cuda_h.py:19] end index_scatter cost 9.512901306152344e-05 seconds
DEBUG 01-15 16:09:24.610595.610595 cuda_h.py:19] end cpuoutputsdeal cost 0.0018622875213623047 seconds
DEBUG 01-15 16:09:24.610280.610280 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.610719.610719 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.610971.610971 cuda_h.py:19] end gpu_group_tensor cost 0.00018310546875 seconds
DEBUG 01-15 16:09:24.610933.610933 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.611688.611688 cuda_h.py:19] end gpu_group_tensor cost 0.0001678466796875 seconds
DEBUG 01-15 16:09:24.611851.611851 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.611508.611508 cuda_h.py:19] end gpu_group_einsum cost 0.0005707740783691406 seconds
DEBUG 01-15 16:09:24.612029.612029 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.612827.612827 cuda_h.py:19] end gpu_group_einsum cost 0.0006256103515625 seconds
DEBUG 01-15 16:09:24.612057.612057 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.612081.612081 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.613517.613517 cuda_h.py:19] end all_expert_outputs_slices cost 0.00034356117248535156 seconds
DEBUG 01-15 16:09:24.613380.613380 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.613145.613145 cuda_h.py:19] end concat_expert_out cost 6.842613220214844e-05 seconds
DEBUG 01-15 16:09:24.613108.613108 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.613794.613794 cuda_h.py:19] end index_scatter cost 7.700920104980469e-05 seconds
DEBUG 01-15 16:09:24.613843.613843 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010836124420166016 seconds
DEBUG 01-15 16:09:24.614907.614907 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.614863.614863 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.614303.614303 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025200843811035156 seconds
DEBUG 01-15 16:09:24.614589.614589 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.614923.614923 cuda_h.py:19] end concat_expert_out cost 6.818771362304688e-05 seconds
DEBUG 01-15 16:09:24.614502.614502 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.614082.614082 cuda_h.py:19] end index_scatter cost 6.985664367675781e-05 seconds
DEBUG 01-15 16:09:24.614759.614759 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006909370422363281 seconds
DEBUG 01-15 16:09:24.614252.614252 cuda_h.py:19] end gpu_experts_multi_device cost 0.05146622657775879 seconds
DEBUG 01-15 16:09:24.615480.615480 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.06305956840515137 seconds
DEBUG 01-15 16:09:24.615641.615641 cuda_h.py:19] end prefill_layer cost 0.06976127624511719 seconds
DEBUG 01-15 16:09:24.615530.615530 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:09:24.615286.615286 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.615949.615949 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:09:24.615659.615659 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:09:24.615468.615468 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:09:24.615755.615755 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:09:24.615995.615995 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:09:24.615744.615744 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.615734.615734 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.616350.616350 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.616498.616498 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.616487.616487 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.617083.617083 cuda_h.py:19] end allocate_cuda_memory cost 0.0004887580871582031 seconds
DEBUG 01-15 16:09:24.617971.617971 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.617048.617048 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.617933.617933 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.617486.617486 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 073acf1c-f717-4b32-9082-2a53dd8670e2
DEBUG 01-15 16:09:24.617241.617241 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.618455.618455 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.619554.619554 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 073acf1c-f717-4b32-9082-2a53dd8670e2
DEBUG 01-15 16:09:24.619374.619374 cuda_h.py:19] end load_into_gpu_async cost 0.0024149417877197266 seconds
DEBUG 01-15 16:09:24.619066.619066 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.620438.620438 cuda_h.py:19] end restore_tensors2 cost 0.00014472007751464844 seconds
DEBUG 01-15 16:09:24.620157.620157 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003854036331176758 seconds
INFO 01-15 16:09:24.620393.620393 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 073acf1c-f717-4b32-9082-2a53dd8670e2
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.621229.621229 cuda_h.py:19] end self_attn cost 0.003871440887451172 seconds
DEBUG 01-15 16:09:24.622526.622526 cuda_h.py:19] end iln_self_attn_paln cost 0.006503105163574219 seconds
DEBUG 01-15 16:09:24.622117.622117 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:09:24.622688.622688 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.623614.623614 cuda_h.py:19] end gate cost 0.0007164478302001953 seconds
DEBUG 01-15 16:09:24.623397.623397 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.623148.623148 lmp.py:1912] 
DEBUG 01-15 16:09:24.623148.623148 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.623666.623666 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.623508.623508 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.623012.623012 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.623085.623085 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.623967.623967 lmp.py:1917] 
DEBUG 01-15 16:09:24.623967.623967 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.623086.623086 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.623120.623120 lmp.py:1935]   Expert 25 |     13 | CPU
DEBUG 01-15 16:09:24.623863.623863 lmp.py:1935]   Expert 48 |     31 | CPU
DEBUG 01-15 16:09:24.623890.623890 lmp.py:1935]   Expert 45 |     35 | CPU
DEBUG 01-15 16:09:24.623440.623440 lmp.py:1935]   Expert  9 |     63 | CPU
DEBUG 01-15 16:09:24.623514.623514 lmp.py:1935]   Expert 43 |     84 | CPU
DEBUG 01-15 16:09:24.623826.623826 lmp.py:1935]   Expert 54 |     85 | CPU
DEBUG 01-15 16:09:24.623614.623614 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:09:24.623403.623403 lmp.py:1935]   Expert 20 |     87 | CPU
DEBUG 01-15 16:09:24.624192.624192 lmp.py:1935]   Expert 47 |     88 | CPU
DEBUG 01-15 16:09:24.624219.624219 lmp.py:1935]   Expert  6 |     93 | CPU
DEBUG 01-15 16:09:24.624816.624816 lmp.py:1935]   Expert 57 |     93 | CPU
DEBUG 01-15 16:09:24.624889.624889 lmp.py:1935]   Expert 36 |     94 | CPU
DEBUG 01-15 16:09:24.624247.624247 lmp.py:1935]   Expert 15 |    103 | CPU
DEBUG 01-15 16:09:24.624082.624082 lmp.py:1935]   Expert 61 |    103 | CPU
DEBUG 01-15 16:09:24.624156.624156 lmp.py:1935]   Expert 50 |    107 | CPU
DEBUG 01-15 16:09:24.624991.624991 lmp.py:1935]   Expert 62 |    107 | CPU
DEBUG 01-15 16:09:24.624588.624588 lmp.py:1935]   Expert 13 |    108 | CPU
DEBUG 01-15 16:09:24.624661.624661 lmp.py:1935]   Expert 38 |    109 | CPU
DEBUG 01-15 16:09:24.624211.624211 lmp.py:1935]   Expert  1 |    112 | CPU
DEBUG 01-15 16:09:24.624523.624523 lmp.py:1935]   Expert 37 |    116 | CPU
DEBUG 01-15 16:09:24.624074.624074 lmp.py:1935]   Expert 46 |    121 | CPU
DEBUG 01-15 16:09:24.624101.624101 lmp.py:1935]   Expert 14 |    122 | CPU
DEBUG 01-15 16:09:24.624459.624459 lmp.py:1935]   Expert 21 |    134 | CPU
DEBUG 01-15 16:09:24.624579.624579 lmp.py:1935]   Expert  7 |    138 | CPU
DEBUG 01-15 16:09:24.624652.624652 lmp.py:1935]   Expert 28 |    138 | CPU
DEBUG 01-15 16:09:24.624633.624633 lmp.py:1935]   Expert 52 |    139 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624329.624329 lmp.py:1935]   Expert 44 |    147 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624370.624370 lmp.py:1935]   Expert 24 |    151 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624471.624471 lmp.py:1935]   Expert 10 |    152 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624451.624451 lmp.py:1935]   Expert 42 |    152 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624240.624240 lmp.py:1935]   Expert 11 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624182.624182 lmp.py:1935]   Expert  2 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624924.624924 lmp.py:1935]   Expert 26 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624666.624666 lmp.py:1935]   Expert 35 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624362.624362 lmp.py:1935]   Expert 31 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624628.624628 lmp.py:1935]   Expert  3 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624370.624370 lmp.py:1935]   Expert 19 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624828.624828 lmp.py:1935]   Expert 32 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624094.624094 lmp.py:1935]   Expert 12 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624075.624075 lmp.py:1935]   Expert 60 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624340.624340 lmp.py:1935]   Expert 56 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624798.624798 lmp.py:1935]   Expert 40 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624302.624302 lmp.py:1935]   Expert 41 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624806.624806 lmp.py:1935]   Expert 53 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624310.624310 lmp.py:1935]   Expert 23 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624575.624575 lmp.py:1935]   Expert  8 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624318.624318 lmp.py:1935]   Expert 16 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624060.624060 lmp.py:1935]   Expert 58 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624326.624326 lmp.py:1935]   Expert 51 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624591.624591 lmp.py:1935]   Expert 59 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624618.624618 lmp.py:1935]   Expert  4 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624599.624599 lmp.py:1935]   Expert 49 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.624865.624865 lmp.py:1935]   Expert 55 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624892.624892 lmp.py:1935]   Expert 29 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624158.624158 lmp.py:1935]   Expert 18 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.624423.624423 lmp.py:1935]   Expert 34 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.625974.625974 lmp.py:1935]   Expert 63 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.625954.625954 lmp.py:1935]   Expert 27 |    358 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.625458.625458 lmp.py:1935]   Expert 39 |    383 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.625724.625724 lmp.py:1935]   Expert 17 |    395 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.625513.625513 lmp.py:1935]   Expert 22 |    428 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.625540.625540 lmp.py:1935]   Expert 33 |    451 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.625567.625567 lmp.py:1935]   Expert 30 |    456 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.625356.625356 lmp.py:1935]   Expert  5 |    709 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.625906.625906 lmp.py:1937] 
DEBUG 01-15 16:09:24.625906.625906 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.625649.625649 lmp.py:1938]   CPU:   2371 tokens
DEBUG 01-15 16:09:24.625391.625391 lmp.py:1942]   cuda:1:   4922 tokens (19 experts)
DEBUG 01-15 16:09:24.625418.625418 lmp.py:1942]   cuda:2:   4995 tokens (20 experts)
DEBUG 01-15 16:09:24.625730.625730 lmp.py:1943]   Total GPU:   9917 tokens
DEBUG 01-15 16:09:24.625327.625327 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.625327.625327 lmp.py:1944] 
DEBUG 01-15 16:09:24.625314.625314 cuda_h.py:19] end experts_map_get cost 0.0020515918731689453 seconds
DEBUG 01-15 16:09:24.625224.625224 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.625457.625457 lmp.py:1953] 
DEBUG 01-15 16:09:24.625457.625457 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.625055.625055 cuda_h.py:19] end cpu_experts_submit cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:09:24.625420.625420 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.625117.625117 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.626057.626057 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.4.gate_proj.weight': 0, 'model.layers.22.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.11.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.11.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.11.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.19.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.19.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.19.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.31.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.31.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.31.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.34.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.34.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.34.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.39.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.39.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.39.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.42.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.42.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.42.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.49.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.49.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.49.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.51.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.51.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.51.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.56.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.56.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.56.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.22.mlp.experts.2.gate_proj.weight': 0, 'model.layers.22.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.24.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.24.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.24.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.35.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.35.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.35.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.40.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.40.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.40.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.52.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.52.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.52.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.22.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.22.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.22.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.22.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.22.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.22.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.22.mlp.experts.59.gate_proj.weight': 311427072, 'model.layers.22.mlp.experts.59.down_proj.weight': 317194240, 'model.layers.22.mlp.experts.59.up_proj.weight': 322961408, 'model.layers.22.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.22.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.22.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(26182942720, 5767168, 0, 0), (26188709888, 5767168, 5767168, 0), (26177175552, 5767168, 11534336, 0), (26200244224, 5767168, 17301504, 0), (26206011392, 5767168, 23068672, 0), (26194477056, 5767168, 28835840, 0), (26304053248, 5767168, 34603008, 0), (26309820416, 5767168, 40370176, 0), (26298286080, 5767168, 46137344, 0), (26321354752, 5767168, 51904512, 0), (26327121920, 5767168, 57671680, 0), (26315587584, 5767168, 63438848, 0), (26390560768, 5767168, 69206016, 0), (26396327936, 5767168, 74973184, 0), (26384793600, 5767168, 80740352, 0), (26442465280, 5767168, 86507520, 0), (26448232448, 5767168, 92274688, 0), (26436698112, 5767168, 98041856, 0), (26494369792, 5767168, 103809024, 0), (26500136960, 5767168, 109576192, 0), (26488602624, 5767168, 115343360, 0), (26511671296, 5767168, 121110528, 0), (26517438464, 5767168, 126877696, 0), (26505904128, 5767168, 132644864, 0), (26563575808, 5767168, 138412032, 0), (26569342976, 5767168, 144179200, 0), (26557808640, 5767168, 149946368, 0), (26650083328, 5767168, 155713536, 0), (26655850496, 5767168, 161480704, 0), (26644316160, 5767168, 167247872, 0), (26701987840, 5767168, 173015040, 0), (26707755008, 5767168, 178782208, 0), (26696220672, 5767168, 184549376, 0), (26788495360, 5767168, 190316544, 0), (26794262528, 5767168, 196083712, 0), (26782728192, 5767168, 201850880, 0), (26823098368, 5767168, 207618048, 0), (26828865536, 5767168, 213385216, 0), (26817331200, 5767168, 219152384, 0), (26840399872, 5767168, 224919552, 0), (26846167040, 5767168, 230686720, 0), (26834632704, 5767168, 236453888, 0), (26875002880, 5767168, 242221056, 0), (26880770048, 5767168, 247988224, 0), (26869235712, 5767168, 253755392, 0), (26961510400, 5767168, 259522560, 0), (26967277568, 5767168, 265289728, 0), (26955743232, 5767168, 271056896, 0), (26996113408, 5767168, 276824064, 0), (27001880576, 5767168, 282591232, 0), (26990346240, 5767168, 288358400, 0), (27082620928, 5767168, 294125568, 0), (27088388096, 5767168, 299892736, 0), (27076853760, 5767168, 305659904, 0), (27203731456, 5767168, 311427072, 0), (27209498624, 5767168, 317194240, 0), (27197964288, 5767168, 322961408, 0)], 2: [(26148339712, 5767168, 0, 0), (26154106880, 5767168, 5767168, 0), (26142572544, 5767168, 11534336, 0), (26165641216, 5767168, 17301504, 0), (26171408384, 5767168, 23068672, 0), (26159874048, 5767168, 28835840, 0), (26252148736, 5767168, 34603008, 0), (26257915904, 5767168, 40370176, 0), (26246381568, 5767168, 46137344, 0), (26286751744, 5767168, 51904512, 0), (26292518912, 5767168, 57671680, 0), (26280984576, 5767168, 63438848, 0), (26407862272, 5767168, 69206016, 0), (26413629440, 5767168, 74973184, 0), (26402095104, 5767168, 80740352, 0), (26425163776, 5767168, 86507520, 0), (26430930944, 5767168, 92274688, 0), (26419396608, 5767168, 98041856, 0), (26528972800, 5767168, 103809024, 0), (26534739968, 5767168, 109576192, 0), (26523205632, 5767168, 115343360, 0), (26580877312, 5767168, 121110528, 0), (26586644480, 5767168, 126877696, 0), (26575110144, 5767168, 132644864, 0), (26615480320, 5767168, 138412032, 0), (26621247488, 5767168, 144179200, 0), (26609713152, 5767168, 149946368, 0), (26632781824, 5767168, 155713536, 0), (26638548992, 5767168, 161480704, 0), (26627014656, 5767168, 167247872, 0), (26667384832, 5767168, 173015040, 0), (26673152000, 5767168, 178782208, 0), (26661617664, 5767168, 184549376, 0), (26684686336, 5767168, 190316544, 0), (26690453504, 5767168, 196083712, 0), (26678919168, 5767168, 201850880, 0), (26719289344, 5767168, 207618048, 0), (26725056512, 5767168, 213385216, 0), (26713522176, 5767168, 219152384, 0), (26805796864, 5767168, 224919552, 0), (26811564032, 5767168, 230686720, 0), (26800029696, 5767168, 236453888, 0), (27013414912, 5767168, 242221056, 0), (27019182080, 5767168, 247988224, 0), (27007647744, 5767168, 253755392, 0), (27030716416, 5767168, 259522560, 0), (27036483584, 5767168, 265289728, 0), (27024949248, 5767168, 271056896, 0), (27065319424, 5767168, 276824064, 0), (27071086592, 5767168, 282591232, 0), (27059552256, 5767168, 288358400, 0), (27117223936, 5767168, 294125568, 0), (27122991104, 5767168, 299892736, 0), (27111456768, 5767168, 305659904, 0), (27134525440, 5767168, 311427072, 0), (27140292608, 5767168, 317194240, 0), (27128758272, 5767168, 322961408, 0), (27151826944, 5767168, 328728576, 0), (27157594112, 5767168, 334495744, 0), (27146059776, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6881a4030>, 2: <capsule object NULL at 0x74a6bc762850>}
DEBUG 01-15 16:09:24.627120.627120 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.627454.627454 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.627402.627402 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 77b5ad04-c847-4ea1-9de2-7b58beb6701d
DEBUG 01-15 16:09:24.627316.627316 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:24.627895.627895 client.py:127] Model loaded
DEBUG 01-15 16:09:24.627992.627992 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.628701.628701 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.628634.628634 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:24.629422.629422 cuda_h.py:19] end restore2model cost 0.001062154769897461 seconds
DEBUG 01-15 16:09:24.629161.629161 cuda_h.py:19] end sllm_worker_task cost 0.013074636459350586 seconds
DEBUG 01-15 16:09:24.629434.629434 cuda_h.py:19] end move_flatidxs cost 0.0010197162628173828 seconds
DEBUG 01-15 16:09:24.629113.629113 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:24.630069.630069 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 77b5ad04-c847-4ea1-9de2-7b58beb6701d
DEBUG 01-15 16:09:24.630775.630775 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004978656768798828 seconds
DEBUG 01-15 16:09:24.630652.630652 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.634739.634739 cuda_h.py:19] end restore2model cost 0.0037488937377929688 seconds
DEBUG 01-15 16:09:24.634019.634019 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008984088897705078 seconds
DEBUG 01-15 16:09:24.634981.634981 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.634980.634980 cuda_h.py:19] end gpu_sexperts cost 0.00031685829162597656 seconds
DEBUG 01-15 16:09:24.634193.634193 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.634500.634500 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.0265579223632812e-05 seconds
DEBUG 01-15 16:09:24.635103.635103 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.635144.635144 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.636239.636239 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001230478286743164 seconds
DEBUG 01-15 16:09:24.636850.636850 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.636799.636799 cuda_h.py:19] end gpu_group_list cost 0.000202178955078125 seconds
DEBUG 01-15 16:09:24.637060.637060 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.639913.639913 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014767646789550781 seconds
DEBUG 01-15 16:09:24.639665.639665 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.639428.639428 cuda_h.py:19] end gpu_group_list cost 0.00021457672119140625 seconds
DEBUG 01-15 16:09:24.640296.640296 cuda_h.py:19] end group_tensors cost 0.010132312774658203 seconds
DEBUG 01-15 16:09:24.640844.640844 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.640972.640972 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 77b5ad04-c847-4ea1-9de2-7b58beb6701d
DEBUG 01-15 16:09:24.640721.640721 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.643973.643973 cuda_h.py:19] end group pad cost 0.002729177474975586 seconds
DEBUG 01-15 16:09:24.643663.643663 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.663337.663337 cuda_h.py:19] end group_einsum cost 0.019454479217529297 seconds
DEBUG 01-15 16:09:24.663732.663732 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.666560.666560 cuda_h.py:19] end get_outputs_cpu1 cost 0.002747058868408203 seconds
DEBUG 01-15 16:09:24.666143.666143 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03856801986694336 seconds
INFO 01-15 16:09:24.669653.669653 client.py:127] Model loaded
DEBUG 01-15 16:09:24.669081.669081 cuda_h.py:19] end wait_experts_multi_device cost 0.028577089309692383 seconds
DEBUG 01-15 16:09:24.669302.669302 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.670252.670252 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0006384849548339844 seconds
DEBUG 01-15 16:09:24.670891.670891 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.672251.672251 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.672265.672265 cuda_h.py:19] end index_scatter cost 0.00012612342834472656 seconds
DEBUG 01-15 16:09:24.672026.672026 cuda_h.py:19] end cpuoutputsdeal cost 0.002390623092651367 seconds
DEBUG 01-15 16:09:24.672599.672599 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.672581.672581 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.673544.673544 cuda_h.py:19] end gpu_group_tensor cost 0.00023627281188964844 seconds
DEBUG 01-15 16:09:24.673527.673527 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.673416.673416 cuda_h.py:19] end gpu_group_tensor cost 0.0002186298370361328 seconds
DEBUG 01-15 16:09:24.673733.673733 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.674377.674377 cuda_h.py:19] end gpu_group_einsum cost 0.0007495880126953125 seconds
DEBUG 01-15 16:09:24.674144.674144 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.675447.675447 cuda_h.py:19] end gpu_group_einsum cost 0.0006723403930664062 seconds
DEBUG 01-15 16:09:24.675917.675917 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.675988.675988 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.676854.676854 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004673004150390625 seconds
DEBUG 01-15 16:09:24.676591.676591 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.676053.676053 cuda_h.py:19] end concat_expert_out cost 0.00011444091796875 seconds
DEBUG 01-15 16:09:24.676103.676103 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.676705.676705 cuda_h.py:19] end index_scatter cost 9.894371032714844e-05 seconds
DEBUG 01-15 16:09:24.677233.677233 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0014567375183105469 seconds
DEBUG 01-15 16:09:24.677867.677867 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.677572.677572 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.677275.677275 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003590583801269531 seconds
DEBUG 01-15 16:09:24.677912.677912 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.677010.677010 cuda_h.py:19] end concat_expert_out cost 8.916854858398438e-05 seconds
DEBUG 01-15 16:09:24.678430.678430 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.678595.678595 cuda_h.py:19] end index_scatter cost 9.822845458984375e-05 seconds
DEBUG 01-15 16:09:24.678676.678676 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009560585021972656 seconds
DEBUG 01-15 16:09:24.678071.678071 cuda_h.py:19] end gpu_experts_multi_device cost 0.04331016540527344 seconds
DEBUG 01-15 16:09:24.678963.678963 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.05606341361999512 seconds
DEBUG 01-15 16:09:24.679325.679325 cuda_h.py:19] end prefill_layer cost 0.06343388557434082 seconds
DEBUG 01-15 16:09:24.679626.679626 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:09:24.679257.679257 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.679079.679079 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:09:24.679332.679332 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:09:24.679023.679023 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:09:24.679152.679152 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 4.5299530029296875e-05 seconds
DEBUG 01-15 16:09:24.679366.679366 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 0.00010251998901367188 seconds
DEBUG 01-15 16:09:24.679182.679182 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.679741.679741 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.679711.679711 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.679932.679932 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.679829.679829 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.680838.680838 cuda_h.py:19] end allocate_cuda_memory cost 0.00024437904357910156 seconds
DEBUG 01-15 16:09:24.680729.680729 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.680380.680380 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.680309.680309 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.680972.680972 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5f509ea2-90c4-4ca3-8c76-a3a5627a8718
DEBUG 01-15 16:09:24.680022.680022 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.680588.680588 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.681819.681819 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5f509ea2-90c4-4ca3-8c76-a3a5627a8718
DEBUG 01-15 16:09:24.681669.681669 cuda_h.py:19] end load_into_gpu_async cost 0.001722574234008789 seconds
DEBUG 01-15 16:09:24.682425.682425 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.682681.682681 cuda_h.py:19] end restore_tensors2 cost 8.416175842285156e-05 seconds
DEBUG 01-15 16:09:24.682305.682305 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0023751258850097656 seconds
INFO 01-15 16:09:24.682108.682108 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5f509ea2-90c4-4ca3-8c76-a3a5627a8718
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.684739.684739 cuda_h.py:19] end self_attn cost 0.0038721561431884766 seconds
DEBUG 01-15 16:09:24.685201.685201 cuda_h.py:19] end iln_self_attn_paln cost 0.005472660064697266 seconds
DEBUG 01-15 16:09:24.685554.685554 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:09:24.685124.685124 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.685385.685385 cuda_h.py:19] end gate cost 0.0008232593536376953 seconds
DEBUG 01-15 16:09:24.686883.686883 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.686919.686919 lmp.py:1912] 
DEBUG 01-15 16:09:24.686919.686919 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.686152.686152 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.686709.686709 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.686690.686690 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.686048.686048 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.686930.686930 lmp.py:1917] 
DEBUG 01-15 16:09:24.686930.686930 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.686672.686672 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.686229.686229 lmp.py:1935]   Expert  5 |     12 | CPU
DEBUG 01-15 16:09:24.686826.686826 lmp.py:1935]   Expert 56 |     31 | CPU
DEBUG 01-15 16:09:24.686469.686469 lmp.py:1935]   Expert 16 |     86 | CPU
DEBUG 01-15 16:09:24.686589.686589 lmp.py:1935]   Expert 27 |     87 | CPU
DEBUG 01-15 16:09:24.686947.686947 lmp.py:1935]   Expert 17 |     90 | CPU
DEBUG 01-15 16:09:24.686352.686352 lmp.py:1935]   Expert 40 |     95 | CPU
DEBUG 01-15 16:09:24.686518.686518 lmp.py:1935]   Expert 63 |     99 | CPU
DEBUG 01-15 16:09:24.686922.686922 lmp.py:1935]   Expert 51 |    106 | CPU
DEBUG 01-15 16:09:24.686612.686612 lmp.py:1935]   Expert 53 |    106 | CPU
DEBUG 01-15 16:09:24.686016.686016 lmp.py:1935]   Expert 49 |    109 | CPU
DEBUG 01-15 16:09:24.686421.686421 lmp.py:1935]   Expert 28 |    110 | CPU
DEBUG 01-15 16:09:24.686302.686302 lmp.py:1935]   Expert  7 |    113 | CPU
DEBUG 01-15 16:09:24.686230.686230 lmp.py:1935]   Expert 47 |    121 | CPU
DEBUG 01-15 16:09:24.686396.686396 lmp.py:1935]   Expert 38 |    122 | CPU
DEBUG 01-15 16:09:24.686562.686562 lmp.py:1935]   Expert 37 |    125 | CPU
DEBUG 01-15 16:09:24.686251.686251 lmp.py:1935]   Expert 62 |    127 | CPU
DEBUG 01-15 16:09:24.686179.686179 lmp.py:1935]   Expert 58 |    128 | CPU
DEBUG 01-15 16:09:24.686107.686107 lmp.py:1935]   Expert 11 |    129 | CPU
DEBUG 01-15 16:09:24.686035.686035 lmp.py:1935]   Expert 57 |    138 | CPU
DEBUG 01-15 16:09:24.686439.686439 lmp.py:1935]   Expert  1 |    146 | CPU
DEBUG 01-15 16:09:24.686320.686320 lmp.py:1935]   Expert 39 |    146 | CPU
DEBUG 01-15 16:09:24.687248.687248 lmp.py:1935]   Expert 14 |    148 | CPU
DEBUG 01-15 16:09:24.687176.687176 lmp.py:1935]   Expert 52 |    153 | CPU
DEBUG 01-15 16:09:24.687342.687342 lmp.py:1935]   Expert 25 |    154 | CPU
DEBUG 01-15 16:09:24.687031.687031 lmp.py:1935]   Expert 23 |    159 | CPU
DEBUG 01-15 16:09:24.687105.687105 lmp.py:1935]   Expert 33 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687701.687701 lmp.py:1935]   Expert 21 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687537.687537 lmp.py:1935]   Expert  6 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687133.687133 lmp.py:1935]   Expert 45 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687253.687253 lmp.py:1935]   Expert 60 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687134.687134 lmp.py:1935]   Expert 19 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687254.687254 lmp.py:1935]   Expert 12 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687897.687897 lmp.py:1935]   Expert 44 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687540.687540 lmp.py:1935]   Expert  4 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687422.687422 lmp.py:1935]   Expert 55 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687780.687780 lmp.py:1935]   Expert  3 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687661.687661 lmp.py:1935]   Expert 30 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687781.687781 lmp.py:1935]   Expert 31 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687424.687424 lmp.py:1935]   Expert 36 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687305.687305 lmp.py:1935]   Expert  9 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687425.687425 lmp.py:1935]   Expert  0 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687783.687783 lmp.py:1935]   Expert 34 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687618.687618 lmp.py:1935]   Expert 22 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687738.687738 lmp.py:1935]   Expert 41 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687381.687381 lmp.py:1935]   Expert 54 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687263.687263 lmp.py:1935]   Expert 26 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687906.687906 lmp.py:1935]   Expert 43 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687549.687549 lmp.py:1935]   Expert 59 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687668.687668 lmp.py:1935]   Expert 18 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687365.687365 lmp.py:1935]   Expert 13 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687412.687412 lmp.py:1935]   Expert 20 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687962.687962 lmp.py:1935]   Expert 50 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687082.687082 lmp.py:1935]   Expert 15 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687440.687440 lmp.py:1935]   Expert 24 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687322.687322 lmp.py:1935]   Expert 29 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687111.687111 lmp.py:1935]   Expert 61 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687138.687138 lmp.py:1935]   Expert 42 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687357.687357 lmp.py:1935]   Expert 35 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687669.687669 lmp.py:1935]   Expert 32 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687981.687981 lmp.py:1935]   Expert  2 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687816.687816 lmp.py:1935]   Expert  8 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687128.687128 lmp.py:1935]   Expert 10 |    343 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687201.687201 lmp.py:1935]   Expert 46 |    427 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.687944.687944 lmp.py:1935]   Expert 48 |    447 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.687540.687540 lmp.py:1937] 
DEBUG 01-15 16:09:24.687540.687540 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.687137.687137 lmp.py:1938]   CPU:   2840 tokens
DEBUG 01-15 16:09:24.687164.687164 lmp.py:1942]   cuda:1:   4803 tokens (20 experts)
DEBUG 01-15 16:09:24.687476.687476 lmp.py:1942]   cuda:2:   4645 tokens (19 experts)
DEBUG 01-15 16:09:24.687834.687834 lmp.py:1943]   Total GPU:   9448 tokens
DEBUG 01-15 16:09:24.687192.687192 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.687192.687192 lmp.py:1944] 
DEBUG 01-15 16:09:24.687465.687465 cuda_h.py:19] end experts_map_get cost 0.0019092559814453125 seconds
DEBUG 01-15 16:09:24.688037.688037 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.688270.688270 lmp.py:1953] 
DEBUG 01-15 16:09:24.688270.688270 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.688483.688483 cuda_h.py:19] end cpu_experts_submit cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:09:24.688464.688464 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.688353.688353 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.688683.688683 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.2.gate_proj.weight': 0, 'model.layers.23.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.24.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.24.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.24.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.33.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.33.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.33.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.35.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.35.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.35.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.36.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.36.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.36.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.48.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.48.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.48.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.50.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.50.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.50.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.54.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.54.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.54.up_proj.weight': 322961408, 'model.layers.23.mlp.experts.59.gate_proj.weight': 328728576, 'model.layers.23.mlp.experts.59.down_proj.weight': 334495744, 'model.layers.23.mlp.experts.59.up_proj.weight': 340262912}, 2: {'model.layers.23.mlp.experts.0.gate_proj.weight': 0, 'model.layers.23.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.42.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.42.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.42.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.43.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.43.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.43.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.44.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.44.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.44.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.46.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.46.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.46.up_proj.weight': 271056896, 'model.layers.23.mlp.experts.55.gate_proj.weight': 276824064, 'model.layers.23.mlp.experts.55.down_proj.weight': 282591232, 'model.layers.23.mlp.experts.55.up_proj.weight': 288358400, 'model.layers.23.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.23.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.23.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.23.mlp.experts.61.gate_proj.weight': 311427072, 'model.layers.23.mlp.experts.61.down_proj.weight': 317194240, 'model.layers.23.mlp.experts.61.up_proj.weight': 322961408}}tensor_copy_chunks_device_map {1: [(27255635968, 5767168, 0, 0), (27261403136, 5767168, 5767168, 0), (27249868800, 5767168, 11534336, 0), (27272937472, 5767168, 17301504, 0), (27278704640, 5767168, 23068672, 0), (27267170304, 5767168, 28835840, 0), (27290238976, 5767168, 34603008, 0), (27296006144, 5767168, 40370176, 0), (27284471808, 5767168, 46137344, 0), (27428651008, 5767168, 51904512, 0), (27434418176, 5767168, 57671680, 0), (27422883840, 5767168, 63438848, 0), (27445952512, 5767168, 69206016, 0), (27451719680, 5767168, 74973184, 0), (27440185344, 5767168, 80740352, 0), (27584364544, 5767168, 86507520, 0), (27590131712, 5767168, 92274688, 0), (27578597376, 5767168, 98041856, 0), (27601666048, 5767168, 103809024, 0), (27607433216, 5767168, 109576192, 0), (27595898880, 5767168, 115343360, 0), (27636269056, 5767168, 121110528, 0), (27642036224, 5767168, 126877696, 0), (27630501888, 5767168, 132644864, 0), (27722776576, 5767168, 138412032, 0), (27728543744, 5767168, 144179200, 0), (27717009408, 5767168, 149946368, 0), (27740078080, 5767168, 155713536, 0), (27745845248, 5767168, 161480704, 0), (27734310912, 5767168, 167247872, 0), (27774681088, 5767168, 173015040, 0), (27780448256, 5767168, 178782208, 0), (27768913920, 5767168, 184549376, 0), (27791982592, 5767168, 190316544, 0), (27797749760, 5767168, 196083712, 0), (27786215424, 5767168, 201850880, 0), (27809284096, 5767168, 207618048, 0), (27815051264, 5767168, 213385216, 0), (27803516928, 5767168, 219152384, 0), (27826585600, 5767168, 224919552, 0), (27832352768, 5767168, 230686720, 0), (27820818432, 5767168, 236453888, 0), (27843887104, 5767168, 242221056, 0), (27849654272, 5767168, 247988224, 0), (27838119936, 5767168, 253755392, 0), (27999600640, 5767168, 259522560, 0), (28005367808, 5767168, 265289728, 0), (27993833472, 5767168, 271056896, 0), (28051505152, 5767168, 276824064, 0), (28057272320, 5767168, 282591232, 0), (28045737984, 5767168, 288358400, 0), (28086108160, 5767168, 294125568, 0), (28091875328, 5767168, 299892736, 0), (28080340992, 5767168, 305659904, 0), (28155314176, 5767168, 311427072, 0), (28161081344, 5767168, 317194240, 0), (28149547008, 5767168, 322961408, 0), (28241821696, 5767168, 328728576, 0), (28247588864, 5767168, 334495744, 0), (28236054528, 5767168, 340262912, 0)], 2: [(27221032960, 5767168, 0, 0), (27226800128, 5767168, 5767168, 0), (27215265792, 5767168, 11534336, 0), (27324841984, 5767168, 17301504, 0), (27330609152, 5767168, 23068672, 0), (27319074816, 5767168, 28835840, 0), (27359444992, 5767168, 34603008, 0), (27365212160, 5767168, 40370176, 0), (27353677824, 5767168, 46137344, 0), (27376746496, 5767168, 51904512, 0), (27382513664, 5767168, 57671680, 0), (27370979328, 5767168, 63438848, 0), (27394048000, 5767168, 69206016, 0), (27399815168, 5767168, 74973184, 0), (27388280832, 5767168, 80740352, 0), (27480555520, 5767168, 86507520, 0), (27486322688, 5767168, 92274688, 0), (27474788352, 5767168, 98041856, 0), (27532460032, 5767168, 103809024, 0), (27538227200, 5767168, 109576192, 0), (27526692864, 5767168, 115343360, 0), (27549761536, 5767168, 121110528, 0), (27555528704, 5767168, 126877696, 0), (27543994368, 5767168, 132644864, 0), (27567063040, 5767168, 138412032, 0), (27572830208, 5767168, 144179200, 0), (27561295872, 5767168, 149946368, 0), (27670872064, 5767168, 155713536, 0), (27676639232, 5767168, 161480704, 0), (27665104896, 5767168, 167247872, 0), (27757379584, 5767168, 173015040, 0), (27763146752, 5767168, 178782208, 0), (27751612416, 5767168, 184549376, 0), (27930394624, 5767168, 190316544, 0), (27936161792, 5767168, 196083712, 0), (27924627456, 5767168, 201850880, 0), (27947696128, 5767168, 207618048, 0), (27953463296, 5767168, 213385216, 0), (27941928960, 5767168, 219152384, 0), (27964997632, 5767168, 224919552, 0), (27970764800, 5767168, 230686720, 0), (27959230464, 5767168, 236453888, 0), (27982299136, 5767168, 242221056, 0), (27988066304, 5767168, 247988224, 0), (27976531968, 5767168, 253755392, 0), (28016902144, 5767168, 259522560, 0), (28022669312, 5767168, 265289728, 0), (28011134976, 5767168, 271056896, 0), (28172615680, 5767168, 276824064, 0), (28178382848, 5767168, 282591232, 0), (28166848512, 5767168, 288358400, 0), (28259123200, 5767168, 294125568, 0), (28264890368, 5767168, 299892736, 0), (28253356032, 5767168, 305659904, 0), (28276424704, 5767168, 311427072, 0), (28282191872, 5767168, 317194240, 0), (28270657536, 5767168, 322961408, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6785187e0>, 2: <capsule object NULL at 0x74a6bc7625e0>}
DEBUG 01-15 16:09:24.689933.689933 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.689320.689320 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.689705.689705 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2bab1561-a1ae-4735-9117-9372d7dbd2fd
DEBUG 01-15 16:09:24.689640.689640 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.689724.689724 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:09:24.689720.689720 client.py:127] Model loaded
DEBUG 01-15 16:09:24.689768.689768 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.690311.690311 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:24.690524.690524 cuda_h.py:19] end restore2model cost 0.0005757808685302734 seconds
DEBUG 01-15 16:09:24.690282.690282 cuda_h.py:19] end sllm_worker_task cost 0.01083230972290039 seconds
DEBUG 01-15 16:09:24.691859.691859 cuda_h.py:19] end move_flatidxs cost 0.0008594989776611328 seconds
DEBUG 01-15 16:09:24.691172.691172 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:24.691780.691780 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2bab1561-a1ae-4735-9117-9372d7dbd2fd
DEBUG 01-15 16:09:24.692533.692533 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004136323928833008 seconds
DEBUG 01-15 16:09:24.692046.692046 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.696847.696847 cuda_h.py:19] end restore2model cost 0.0037140846252441406 seconds
DEBUG 01-15 16:09:24.696452.696452 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008112192153930664 seconds
DEBUG 01-15 16:09:24.696460.696460 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.696511.696511 cuda_h.py:19] end gpu_sexperts cost 0.00032067298889160156 seconds
DEBUG 01-15 16:09:24.696248.696248 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.696647.696647 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:09:24.696297.696297 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.696907.696907 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.698098.698098 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013010501861572266 seconds
DEBUG 01-15 16:09:24.698114.698114 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.698506.698506 cuda_h.py:19] end gpu_group_list cost 0.00022482872009277344 seconds
DEBUG 01-15 16:09:24.699040.699040 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.701035.701035 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013720989227294922 seconds
DEBUG 01-15 16:09:24.701574.701574 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.701059.701059 cuda_h.py:19] end gpu_group_list cost 0.0002200603485107422 seconds
DEBUG 01-15 16:09:24.701859.701859 cuda_h.py:19] end group_tensors cost 0.01040029525756836 seconds
DEBUG 01-15 16:09:24.702566.702566 cuda_h.py:10] start wait_experts_multi_device
DEBUG 01-15 16:09:24.702864.702864 cuda_h.py:10] start group pad
INFO 01-15 16:09:24.702780.702780 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2bab1561-a1ae-4735-9117-9372d7dbd2fd
DEBUG 01-15 16:09:24.704432.704432 cuda_h.py:19] end group pad cost 0.002653360366821289 seconds
DEBUG 01-15 16:09:24.704539.704539 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.736491.736491 cuda_h.py:19] end group_einsum cost 0.031137943267822266 seconds
DEBUG 01-15 16:09:24.736509.736509 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:09:24.738871.738871 client.py:127] Model loaded
DEBUG 01-15 16:09:24.739704.739704 cuda_h.py:19] end wait_experts_multi_device cost 0.03667950630187988 seconds
DEBUG 01-15 16:09:24.739732.739732 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.739840.739840 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029449462890625 seconds
DEBUG 01-15 16:09:24.740401.740401 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05020475387573242 seconds
DEBUG 01-15 16:09:24.740327.740327 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0016796588897705078 seconds
DEBUG 01-15 16:09:24.740952.740952 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.741695.741695 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.741541.741541 cuda_h.py:19] end index_scatter cost 6.914138793945312e-05 seconds
DEBUG 01-15 16:09:24.742721.742721 cuda_h.py:19] end cpuoutputsdeal cost 0.0011601448059082031 seconds
DEBUG 01-15 16:09:24.742670.742670 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.742234.742234 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.742444.742444 cuda_h.py:19] end gpu_group_tensor cost 0.0001277923583984375 seconds
DEBUG 01-15 16:09:24.742916.742916 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.742768.742768 cuda_h.py:19] end gpu_group_tensor cost 0.00011157989501953125 seconds
DEBUG 01-15 16:09:24.742082.742082 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.743674.743674 cuda_h.py:19] end gpu_group_einsum cost 0.0004291534423828125 seconds
DEBUG 01-15 16:09:24.743340.743340 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.743800.743800 cuda_h.py:19] end gpu_group_einsum cost 0.0004324913024902344 seconds
DEBUG 01-15 16:09:24.743671.743671 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.743383.743383 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.744119.744119 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001857280731201172 seconds
DEBUG 01-15 16:09:24.744975.744975 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.744613.744613 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:09:24.744708.744708 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.744387.744387 cuda_h.py:19] end index_scatter cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:09:24.744422.744422 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008192062377929688 seconds
DEBUG 01-15 16:09:24.744882.744882 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.744580.744580 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.744452.744452 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001251697540283203 seconds
DEBUG 01-15 16:09:24.744725.744725 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.745303.745303 cuda_h.py:19] end concat_expert_out cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:09:24.745755.745755 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.745957.745957 cuda_h.py:19] end index_scatter cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:09:24.745905.745905 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00043892860412597656 seconds
DEBUG 01-15 16:09:24.745847.745847 cuda_h.py:19] end gpu_experts_multi_device cost 0.04845166206359863 seconds
DEBUG 01-15 16:09:24.745227.745227 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.060227155685424805 seconds
DEBUG 01-15 16:09:24.745511.745511 cuda_h.py:19] end prefill_layer cost 0.06650710105895996 seconds
DEBUG 01-15 16:09:24.745434.745434 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:09:24.745845.745845 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.745971.745971 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:09:24.745860.745860 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:09:24.745417.745417 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:09:24.745538.745538 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 3.147125244140625e-05 seconds
DEBUG 01-15 16:09:24.745856.745856 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 5.841255187988281e-05 seconds
DEBUG 01-15 16:09:24.745930.745930 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.746900.746900 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.746525.746525 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.746468.746468 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.746941.746941 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.746482.746482 cuda_h.py:19] end allocate_cuda_memory cost 0.0002815723419189453 seconds
DEBUG 01-15 16:09:24.746286.746286 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.746301.746301 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.746780.746780 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.746351.746351 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c11fd4b6-7702-4445-b1c6-d803d4f51a3a
DEBUG 01-15 16:09:24.746195.746195 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.747076.747076 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.748772.748772 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c11fd4b6-7702-4445-b1c6-d803d4f51a3a
DEBUG 01-15 16:09:24.748767.748767 cuda_h.py:19] end load_into_gpu_async cost 0.0018532276153564453 seconds
DEBUG 01-15 16:09:24.748431.748431 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.748448.748448 cuda_h.py:19] end restore_tensors2 cost 8.416175842285156e-05 seconds
DEBUG 01-15 16:09:24.748708.748708 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0025544166564941406 seconds
INFO 01-15 16:09:24.748657.748657 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c11fd4b6-7702-4445-b1c6-d803d4f51a3a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.750680.750680 cuda_h.py:19] end self_attn cost 0.0031576156616210938 seconds
DEBUG 01-15 16:09:24.750796.750796 cuda_h.py:19] end iln_self_attn_paln cost 0.004682064056396484 seconds
DEBUG 01-15 16:09:24.750195.750195 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:09:24.750097.750097 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.751649.751649 cuda_h.py:19] end gate cost 0.0006184577941894531 seconds
DEBUG 01-15 16:09:24.751478.751478 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.751721.751721 lmp.py:1912] 
DEBUG 01-15 16:09:24.751721.751721 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.751715.751715 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.751126.751126 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.751677.751677 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.751128.751128 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.751863.751863 lmp.py:1917] 
DEBUG 01-15 16:09:24.751863.751863 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.751314.751314 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.751726.751726 lmp.py:1935]   Expert 36 |     20 | CPU
DEBUG 01-15 16:09:24.751892.751892 lmp.py:1935]   Expert 35 |     29 | CPU
DEBUG 01-15 16:09:24.751343.751343 lmp.py:1935]   Expert 25 |     45 | CPU
DEBUG 01-15 16:09:24.751317.751317 lmp.py:1935]   Expert 46 |     47 | CPU
DEBUG 01-15 16:09:24.751529.751529 lmp.py:1935]   Expert 51 |     52 | CPU
DEBUG 01-15 16:09:24.751218.751218 lmp.py:1935]   Expert 16 |     58 | CPU
DEBUG 01-15 16:09:24.752908.752908 lmp.py:1935]   Expert 30 |     64 | CPU
DEBUG 01-15 16:09:24.752120.752120 lmp.py:1935]   Expert  0 |     65 | CPU
DEBUG 01-15 16:09:24.752094.752094 lmp.py:1935]   Expert 43 |     69 | CPU
DEBUG 01-15 16:09:24.752307.752307 lmp.py:1935]   Expert 47 |     70 | CPU
DEBUG 01-15 16:09:24.752042.752042 lmp.py:1935]   Expert 55 |     73 | CPU
DEBUG 01-15 16:09:24.752016.752016 lmp.py:1935]   Expert 39 |     74 | CPU
DEBUG 01-15 16:09:24.752752.752752 lmp.py:1935]   Expert 42 |     75 | CPU
DEBUG 01-15 16:09:24.752726.752726 lmp.py:1935]   Expert 44 |     76 | CPU
DEBUG 01-15 16:09:24.752700.752700 lmp.py:1935]   Expert  2 |     82 | CPU
DEBUG 01-15 16:09:24.752912.752912 lmp.py:1935]   Expert  4 |    106 | CPU
DEBUG 01-15 16:09:24.752125.752125 lmp.py:1935]   Expert 33 |    118 | CPU
DEBUG 01-15 16:09:24.752861.752861 lmp.py:1935]   Expert 48 |    118 | CPU
DEBUG 01-15 16:09:24.752596.752596 lmp.py:1935]   Expert 61 |    124 | CPU
DEBUG 01-15 16:09:24.752093.752093 lmp.py:1935]   Expert 13 |    125 | CPU
DEBUG 01-15 16:09:24.752213.752213 lmp.py:1935]   Expert 24 |    125 | CPU
DEBUG 01-15 16:09:24.752856.752856 lmp.py:1935]   Expert  6 |    128 | CPU
DEBUG 01-15 16:09:24.752022.752022 lmp.py:1935]   Expert 56 |    130 | CPU
DEBUG 01-15 16:09:24.752427.752427 lmp.py:1935]   Expert 29 |    132 | CPU
DEBUG 01-15 16:09:24.752355.752355 lmp.py:1935]   Expert 15 |    134 | CPU
DEBUG 01-15 16:09:24.752951.752951 lmp.py:1935]   Expert 38 |    138 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752786.752786 lmp.py:1935]   Expert 54 |    138 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752383.752383 lmp.py:1935]   Expert  9 |    143 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752503.752503 lmp.py:1935]   Expert  7 |    145 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752159.752159 lmp.py:1935]   Expert 20 |    145 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752922.752922 lmp.py:1935]   Expert 59 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752280.752280 lmp.py:1935]   Expert 45 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752877.752877 lmp.py:1935]   Expert 62 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752235.752235 lmp.py:1935]   Expert 19 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752070.752070 lmp.py:1935]   Expert 34 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752190.752190 lmp.py:1935]   Expert 50 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752072.752072 lmp.py:1935]   Expert 57 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752191.752191 lmp.py:1935]   Expert 10 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752311.752311 lmp.py:1935]   Expert 31 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752669.752669 lmp.py:1935]   Expert 23 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752312.752312 lmp.py:1935]   Expert  8 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752386.752386 lmp.py:1935]   Expert 60 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752221.752221 lmp.py:1935]   Expert 18 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752579.752579 lmp.py:1935]   Expert 53 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752699.752699 lmp.py:1935]   Expert 22 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752580.752580 lmp.py:1935]   Expert 52 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752223.752223 lmp.py:1935]   Expert 37 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752582.752582 lmp.py:1935]   Expert  5 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752701.752701 lmp.py:1935]   Expert 17 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752536.752536 lmp.py:1935]   Expert 11 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752895.752895 lmp.py:1935]   Expert  1 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752014.752014 lmp.py:1935]   Expert 49 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752134.752134 lmp.py:1935]   Expert 41 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752254.752254 lmp.py:1935]   Expert 26 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752089.752089 lmp.py:1935]   Expert 28 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752209.752209 lmp.py:1935]   Expert 32 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752329.752329 lmp.py:1935]   Expert 58 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752164.752164 lmp.py:1935]   Expert 40 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752045.752045 lmp.py:1935]   Expert 14 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.752165.752165 lmp.py:1935]   Expert 12 |    333 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.752046.752046 lmp.py:1935]   Expert 63 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.753166.753166 lmp.py:1935]   Expert 21 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.753809.753809 lmp.py:1935]   Expert 27 |    667 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.753167.753167 lmp.py:1935]   Expert  3 |   1019 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.753857.753857 lmp.py:1937] 
DEBUG 01-15 16:09:24.753857.753857 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.753083.753083 lmp.py:1938]   CPU:   2139 tokens
DEBUG 01-15 16:09:24.753918.753918 lmp.py:1942]   cuda:1:   5066 tokens (19 experts)
DEBUG 01-15 16:09:24.753038.753038 lmp.py:1942]   cuda:2:   5083 tokens (20 experts)
DEBUG 01-15 16:09:24.753442.753442 lmp.py:1943]   Total GPU:  10149 tokens
DEBUG 01-15 16:09:24.753608.753608 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.753608.753608 lmp.py:1944] 
DEBUG 01-15 16:09:24.753212.753212 cuda_h.py:19] end experts_map_get cost 0.0017061233520507812 seconds
DEBUG 01-15 16:09:24.753731.753731 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.753963.753963 lmp.py:1953] 
DEBUG 01-15 16:09:24.753963.753963 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.753515.753515 cuda_h.py:19] end cpu_experts_submit cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:09:24.753225.753225 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.753260.753260 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.754801.754801 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.1.gate_proj.weight': 0, 'model.layers.24.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.52.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.52.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.52.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.58.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.58.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.58.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.59.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.59.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.59.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.60.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.60.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.60.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.62.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.62.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.62.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.63.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.63.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.63.up_proj.weight': 322961408}, 2: {'model.layers.24.mlp.experts.5.gate_proj.weight': 0, 'model.layers.24.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.21.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.21.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.21.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.27.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.27.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.27.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.31.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.31.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.31.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.34.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.34.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.34.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.40.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.40.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.40.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.45.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.45.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.45.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.49.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.49.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.49.up_proj.weight': 288358400, 'model.layers.24.mlp.experts.50.gate_proj.weight': 294125568, 'model.layers.24.mlp.experts.50.down_proj.weight': 299892736, 'model.layers.24.mlp.experts.50.up_proj.weight': 305659904, 'model.layers.24.mlp.experts.53.gate_proj.weight': 311427072, 'model.layers.24.mlp.experts.53.down_proj.weight': 317194240, 'model.layers.24.mlp.experts.53.up_proj.weight': 322961408, 'model.layers.24.mlp.experts.54.gate_proj.weight': 328728576, 'model.layers.24.mlp.experts.54.down_proj.weight': 334495744, 'model.layers.24.mlp.experts.54.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(28345630720, 5767168, 0, 0), (28351397888, 5767168, 5767168, 0), (28339863552, 5767168, 11534336, 0), (28380233728, 5767168, 17301504, 0), (28386000896, 5767168, 23068672, 0), (28374466560, 5767168, 28835840, 0), (28466741248, 5767168, 34603008, 0), (28472508416, 5767168, 40370176, 0), (28460974080, 5767168, 46137344, 0), (28501344256, 5767168, 51904512, 0), (28507111424, 5767168, 57671680, 0), (28495577088, 5767168, 63438848, 0), (28570550272, 5767168, 69206016, 0), (28576317440, 5767168, 74973184, 0), (28564783104, 5767168, 80740352, 0), (28622454784, 5767168, 86507520, 0), (28628221952, 5767168, 92274688, 0), (28616687616, 5767168, 98041856, 0), (28657057792, 5767168, 103809024, 0), (28662824960, 5767168, 109576192, 0), (28651290624, 5767168, 115343360, 0), (28674359296, 5767168, 121110528, 0), (28680126464, 5767168, 126877696, 0), (28668592128, 5767168, 132644864, 0), (28708962304, 5767168, 138412032, 0), (28714729472, 5767168, 144179200, 0), (28703195136, 5767168, 149946368, 0), (28778168320, 5767168, 155713536, 0), (28783935488, 5767168, 161480704, 0), (28772401152, 5767168, 167247872, 0), (28985786368, 5767168, 173015040, 0), (28991553536, 5767168, 178782208, 0), (28980019200, 5767168, 184549376, 0), (29037690880, 5767168, 190316544, 0), (29043458048, 5767168, 196083712, 0), (29031923712, 5767168, 201850880, 0), (29228007424, 5767168, 207618048, 0), (29233774592, 5767168, 213385216, 0), (29222240256, 5767168, 219152384, 0), (29314514944, 5767168, 224919552, 0), (29320282112, 5767168, 230686720, 0), (29308747776, 5767168, 236453888, 0), (29331816448, 5767168, 242221056, 0), (29337583616, 5767168, 247988224, 0), (29326049280, 5767168, 253755392, 0), (29349117952, 5767168, 259522560, 0), (29354885120, 5767168, 265289728, 0), (29343350784, 5767168, 271056896, 0), (29366419456, 5767168, 276824064, 0), (29372186624, 5767168, 282591232, 0), (29360652288, 5767168, 288358400, 0), (29401022464, 5767168, 294125568, 0), (29406789632, 5767168, 299892736, 0), (29395255296, 5767168, 305659904, 0), (29418323968, 5767168, 311427072, 0), (29424091136, 5767168, 317194240, 0), (29412556800, 5767168, 322961408, 0)], 2: [(28414836736, 5767168, 0, 0), (28420603904, 5767168, 5767168, 0), (28409069568, 5767168, 11534336, 0), (28449439744, 5767168, 17301504, 0), (28455206912, 5767168, 23068672, 0), (28443672576, 5767168, 28835840, 0), (28484042752, 5767168, 34603008, 0), (28489809920, 5767168, 40370176, 0), (28478275584, 5767168, 46137344, 0), (28518645760, 5767168, 51904512, 0), (28524412928, 5767168, 57671680, 0), (28512878592, 5767168, 63438848, 0), (28535947264, 5767168, 69206016, 0), (28541714432, 5767168, 74973184, 0), (28530180096, 5767168, 80740352, 0), (28639756288, 5767168, 86507520, 0), (28645523456, 5767168, 92274688, 0), (28633989120, 5767168, 98041856, 0), (28691660800, 5767168, 103809024, 0), (28697427968, 5767168, 109576192, 0), (28685893632, 5767168, 115343360, 0), (28726263808, 5767168, 121110528, 0), (28732030976, 5767168, 126877696, 0), (28720496640, 5767168, 132644864, 0), (28795469824, 5767168, 138412032, 0), (28801236992, 5767168, 144179200, 0), (28789702656, 5767168, 149946368, 0), (28812771328, 5767168, 155713536, 0), (28818538496, 5767168, 161480704, 0), (28807004160, 5767168, 167247872, 0), (28864675840, 5767168, 173015040, 0), (28870443008, 5767168, 178782208, 0), (28858908672, 5767168, 184549376, 0), (28881977344, 5767168, 190316544, 0), (28887744512, 5767168, 196083712, 0), (28876210176, 5767168, 201850880, 0), (28916580352, 5767168, 207618048, 0), (28922347520, 5767168, 213385216, 0), (28910813184, 5767168, 219152384, 0), (28968484864, 5767168, 224919552, 0), (28974252032, 5767168, 230686720, 0), (28962717696, 5767168, 236453888, 0), (29020389376, 5767168, 242221056, 0), (29026156544, 5767168, 247988224, 0), (29014622208, 5767168, 253755392, 0), (29106896896, 5767168, 259522560, 0), (29112664064, 5767168, 265289728, 0), (29101129728, 5767168, 271056896, 0), (29176102912, 5767168, 276824064, 0), (29181870080, 5767168, 282591232, 0), (29170335744, 5767168, 288358400, 0), (29193404416, 5767168, 294125568, 0), (29199171584, 5767168, 299892736, 0), (29187637248, 5767168, 305659904, 0), (29245308928, 5767168, 311427072, 0), (29251076096, 5767168, 317194240, 0), (29239541760, 5767168, 322961408, 0), (29262610432, 5767168, 328728576, 0), (29268377600, 5767168, 334495744, 0), (29256843264, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6883a5cb0>, 2: <capsule object NULL at 0x74a6bc762970>}
DEBUG 01-15 16:09:24.755415.755415 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.755265.755265 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.755213.755213 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9185d8d4-a745-4261-884d-ef059b0721fd
DEBUG 01-15 16:09:24.755597.755597 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:24.755717.755717 client.py:127] Model loaded
DEBUG 01-15 16:09:24.755031.755031 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.756944.756944 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.756001.756001 cuda_h.py:19] end restore2model cost 0.0006215572357177734 seconds
DEBUG 01-15 16:09:24.756657.756657 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:24.756103.756103 cuda_h.py:19] end sllm_worker_task cost 0.010443687438964844 seconds
DEBUG 01-15 16:09:24.757119.757119 cuda_h.py:19] end move_flatidxs cost 0.0008654594421386719 seconds
DEBUG 01-15 16:09:24.757194.757194 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:24.757783.757783 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9185d8d4-a745-4261-884d-ef059b0721fd
DEBUG 01-15 16:09:24.758440.758440 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004557132720947266 seconds
DEBUG 01-15 16:09:24.758940.758940 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.761779.761779 cuda_h.py:19] end restore2model cost 0.003077268600463867 seconds
DEBUG 01-15 16:09:24.761675.761675 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007890701293945312 seconds
DEBUG 01-15 16:09:24.761279.761279 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.761355.761355 cuda_h.py:19] end gpu_sexperts cost 0.0002720355987548828 seconds
DEBUG 01-15 16:09:24.761231.761231 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.761908.761908 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5020370483398438e-05 seconds
DEBUG 01-15 16:09:24.761557.761557 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.761022.761022 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.762478.762478 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009377002716064453 seconds
DEBUG 01-15 16:09:24.762229.762229 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.763402.763402 cuda_h.py:19] end gpu_group_list cost 0.0002048015594482422 seconds
DEBUG 01-15 16:09:24.763926.763926 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.764853.764853 cuda_h.py:19] end group_tensors cost 0.006371498107910156 seconds
DEBUG 01-15 16:09:24.764558.764558 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.764175.764175 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001073598861694336 seconds
DEBUG 01-15 16:09:24.765926.765926 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.765472.765472 cuda_h.py:19] end gpu_group_list cost 0.00026035308837890625 seconds
DEBUG 01-15 16:09:24.766808.766808 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.766307.766307 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9185d8d4-a745-4261-884d-ef059b0721fd
DEBUG 01-15 16:09:24.767445.767445 cuda_h.py:19] end group pad cost 0.0028731822967529297 seconds
DEBUG 01-15 16:09:24.767526.767526 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.787651.787651 cuda_h.py:19] end group_einsum cost 0.01984095573425293 seconds
DEBUG 01-15 16:09:24.787297.787297 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.793230.793230 cuda_h.py:19] end get_outputs_cpu1 cost 0.005072832107543945 seconds
DEBUG 01-15 16:09:24.793242.793242 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03749418258666992 seconds
INFO 01-15 16:09:24.796247.796247 client.py:127] Model loaded
DEBUG 01-15 16:09:24.796040.796040 cuda_h.py:19] end wait_experts_multi_device cost 0.030582427978515625 seconds
DEBUG 01-15 16:09:24.796585.796585 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.797267.797267 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0005526542663574219 seconds
DEBUG 01-15 16:09:24.797276.797276 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.798764.798764 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.798737.798737 cuda_h.py:19] end index_scatter cost 9.822845458984375e-05 seconds
DEBUG 01-15 16:09:24.799980.799980 cuda_h.py:19] end cpuoutputsdeal cost 0.001764535903930664 seconds
DEBUG 01-15 16:09:24.799155.799155 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.799792.799792 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.799840.799840 cuda_h.py:19] end gpu_group_tensor cost 0.0001995563507080078 seconds
DEBUG 01-15 16:09:24.799008.799008 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.799889.799889 cuda_h.py:19] end gpu_group_tensor cost 0.0001842975616455078 seconds
DEBUG 01-15 16:09:24.800073.800073 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.800385.800385 cuda_h.py:19] end gpu_group_einsum cost 0.0007588863372802734 seconds
DEBUG 01-15 16:09:24.801410.801410 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.801863.801863 cuda_h.py:19] end gpu_group_einsum cost 0.0005776882171630859 seconds
DEBUG 01-15 16:09:24.801259.801259 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.801111.801111 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.802560.802560 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003082752227783203 seconds
DEBUG 01-15 16:09:24.802913.802913 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.802884.802884 cuda_h.py:19] end concat_expert_out cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:09:24.802575.802575 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.802236.802236 cuda_h.py:19] end index_scatter cost 8.225440979003906e-05 seconds
DEBUG 01-15 16:09:24.803451.803451 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011262893676757812 seconds
DEBUG 01-15 16:09:24.803389.803389 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.803558.803558 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.803142.803142 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020599365234375 seconds
DEBUG 01-15 16:09:24.803488.803488 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.803605.803605 cuda_h.py:19] end concat_expert_out cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:09:24.803197.803197 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.803758.803758 cuda_h.py:19] end index_scatter cost 7.963180541992188e-05 seconds
DEBUG 01-15 16:09:24.803117.803117 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007262229919433594 seconds
DEBUG 01-15 16:09:24.804299.804299 cuda_h.py:19] end gpu_experts_multi_device cost 0.04230546951293945 seconds
DEBUG 01-15 16:09:24.804979.804979 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.05342221260070801 seconds
DEBUG 01-15 16:09:24.804645.804645 cuda_h.py:19] end prefill_layer cost 0.05887770652770996 seconds
DEBUG 01-15 16:09:24.804078.804078 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:09:24.804940.804940 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.804279.804279 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:09:24.804619.804619 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:09:24.804819.804819 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:09:24.805411.805411 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 4.4345855712890625e-05 seconds
DEBUG 01-15 16:09:24.805810.805810 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 9.655952453613281e-05 seconds
DEBUG 01-15 16:09:24.805573.805573 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.805708.805708 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.805693.805693 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.805018.805018 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.805189.805189 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.806365.806365 cuda_h.py:19] end allocate_cuda_memory cost 0.0004134178161621094 seconds
DEBUG 01-15 16:09:24.806067.806067 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.806011.806011 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.806445.806445 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.806044.806044 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bf1671e4-468e-4f93-a2e1-67c1ad790565
DEBUG 01-15 16:09:24.806117.806117 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.807464.807464 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.808234.808234 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bf1671e4-468e-4f93-a2e1-67c1ad790565
DEBUG 01-15 16:09:24.808079.808079 cuda_h.py:19] end load_into_gpu_async cost 0.0024352073669433594 seconds
DEBUG 01-15 16:09:24.809512.809512 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.809864.809864 cuda_h.py:19] end restore_tensors2 cost 0.0001480579376220703 seconds
DEBUG 01-15 16:09:24.809298.809298 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0037004947662353516 seconds
INFO 01-15 16:09:24.809560.809560 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bf1671e4-468e-4f93-a2e1-67c1ad790565
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.811284.811284 cuda_h.py:19] end self_attn cost 0.0044133663177490234 seconds
DEBUG 01-15 16:09:24.811585.811585 cuda_h.py:19] end iln_self_attn_paln cost 0.006849050521850586 seconds
DEBUG 01-15 16:09:24.812030.812030 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:09:24.812978.812978 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.812921.812921 cuda_h.py:19] end gate cost 0.0006258487701416016 seconds
DEBUG 01-15 16:09:24.812320.812320 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.813377.813377 lmp.py:1912] 
DEBUG 01-15 16:09:24.813377.813377 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.813232.813232 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.813790.813790 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.813532.813532 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.813129.813129 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.813295.813295 lmp.py:1917] 
DEBUG 01-15 16:09:24.813295.813295 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.813368.813368 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.813925.813925 lmp.py:1935]   Expert 13 |     30 | CPU
DEBUG 01-15 16:09:24.813284.813284 lmp.py:1935]   Expert 44 |     39 | CPU
DEBUG 01-15 16:09:24.813927.813927 lmp.py:1935]   Expert  9 |     40 | CPU
DEBUG 01-15 16:09:24.813762.813762 lmp.py:1935]   Expert 25 |     40 | CPU
DEBUG 01-15 16:09:24.813643.813643 lmp.py:1935]   Expert 16 |     45 | CPU
DEBUG 01-15 16:09:24.813763.813763 lmp.py:1935]   Expert  2 |     51 | CPU
DEBUG 01-15 16:09:24.813406.813406 lmp.py:1935]   Expert 38 |     51 | CPU
DEBUG 01-15 16:09:24.813572.813572 lmp.py:1935]   Expert 22 |     55 | CPU
DEBUG 01-15 16:09:24.813738.813738 lmp.py:1935]   Expert 33 |     59 | CPU
DEBUG 01-15 16:09:24.813143.813143 lmp.py:1935]   Expert 42 |     60 | CPU
DEBUG 01-15 16:09:24.813309.813309 lmp.py:1935]   Expert  5 |     65 | CPU
DEBUG 01-15 16:09:24.813952.813952 lmp.py:1935]   Expert 23 |     73 | CPU
DEBUG 01-15 16:09:24.813356.813356 lmp.py:1935]   Expert 24 |     80 | CPU
DEBUG 01-15 16:09:24.813238.813238 lmp.py:1935]   Expert 10 |     87 | CPU
DEBUG 01-15 16:09:24.813881.813881 lmp.py:1935]   Expert 59 |    103 | CPU
DEBUG 01-15 16:09:24.813285.813285 lmp.py:1935]   Expert 21 |    108 | CPU
DEBUG 01-15 16:09:24.813690.813690 lmp.py:1935]   Expert 46 |    115 | CPU
DEBUG 01-15 16:09:24.813094.813094 lmp.py:1935]   Expert 45 |    116 | CPU
DEBUG 01-15 16:09:24.813929.813929 lmp.py:1935]   Expert 55 |    117 | CPU
DEBUG 01-15 16:09:24.813288.813288 lmp.py:1935]   Expert 61 |    125 | CPU
DEBUG 01-15 16:09:24.813454.813454 lmp.py:1935]   Expert 31 |    130 | CPU
DEBUG 01-15 16:09:24.813858.813858 lmp.py:1935]   Expert  6 |    139 | CPU
DEBUG 01-15 16:09:24.813084.813084 lmp.py:1935]   Expert 36 |    141 | CPU
DEBUG 01-15 16:09:24.813158.813158 lmp.py:1935]   Expert 51 |    143 | CPU
DEBUG 01-15 16:09:24.813516.813516 lmp.py:1935]   Expert 43 |    145 | CPU
DEBUG 01-15 16:09:24.813782.813782 lmp.py:1935]   Expert  0 |    149 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.813855.813855 lmp.py:1935]   Expert  8 |    149 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.813213.813213 lmp.py:1935]   Expert  3 |    152 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.813572.813572 lmp.py:1935]   Expert 26 |    155 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.813930.813930 lmp.py:1935]   Expert 18 |    158 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.813288.813288 lmp.py:1935]   Expert 48 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.813362.813362 lmp.py:1935]   Expert 41 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.813435.813435 lmp.py:1935]   Expert 12 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.813032.813032 lmp.py:1935]   Expert  7 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.813390.813390 lmp.py:1935]   Expert 20 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.813510.813510 lmp.py:1935]   Expert 56 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.813391.813391 lmp.py:1935]   Expert 28 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.813511.813511 lmp.py:1935]   Expert  1 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.813631.813631 lmp.py:1935]   Expert 27 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.813704.813704 lmp.py:1935]   Expert 34 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.813301.813301 lmp.py:1935]   Expert 47 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814659.814659 lmp.py:1935]   Expert 11 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814541.814541 lmp.py:1935]   Expert 32 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814660.814660 lmp.py:1935]   Expert 40 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814780.814780 lmp.py:1935]   Expert 49 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814662.814662 lmp.py:1935]   Expert 53 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814781.814781 lmp.py:1935]   Expert 15 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814855.814855 lmp.py:1935]   Expert 63 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814690.814690 lmp.py:1935]   Expert  4 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814287.814287 lmp.py:1935]   Expert 29 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814691.814691 lmp.py:1935]   Expert 50 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814573.814573 lmp.py:1935]   Expert 30 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814454.814454 lmp.py:1935]   Expert 35 |    273 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814574.814574 lmp.py:1935]   Expert 14 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814694.814694 lmp.py:1935]   Expert 37 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814529.814529 lmp.py:1935]   Expert 52 |    342 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814887.814887 lmp.py:1935]   Expert 17 |    355 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814245.814245 lmp.py:1935]   Expert 54 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814126.814126 lmp.py:1935]   Expert 39 |    390 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814293.814293 lmp.py:1935]   Expert 57 |    408 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814412.814412 lmp.py:1935]   Expert 60 |    455 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814294.814294 lmp.py:1935]   Expert 62 |    463 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814652.814652 lmp.py:1935]   Expert 19 |    543 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.814249.814249 lmp.py:1935]   Expert 58 |    573 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.814130.814130 lmp.py:1937] 
DEBUG 01-15 16:09:24.814130.814130 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.814250.814250 lmp.py:1938]   CPU:   2157 tokens
DEBUG 01-15 16:09:24.814608.814608 lmp.py:1942]   cuda:1:   4992 tokens (19 experts)
DEBUG 01-15 16:09:24.814728.814728 lmp.py:1942]   cuda:2:   5139 tokens (20 experts)
DEBUG 01-15 16:09:24.814894.814894 lmp.py:1943]   Total GPU:  10131 tokens
DEBUG 01-15 16:09:24.814537.814537 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.814537.814537 lmp.py:1944] 
DEBUG 01-15 16:09:24.814902.814902 cuda_h.py:19] end experts_map_get cost 0.0017352104187011719 seconds
DEBUG 01-15 16:09:24.814129.814129 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.814216.814216 lmp.py:1953] 
DEBUG 01-15 16:09:24.814216.814216 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.814291.814291 cuda_h.py:19] end cpu_experts_submit cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:09:24.814239.814239 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.814751.814751 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.816382.816382 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.1.gate_proj.weight': 0, 'model.layers.25.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.28.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.28.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.28.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.29.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.29.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.29.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.32.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.32.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.32.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.35.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.35.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.35.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.39.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.39.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.39.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.48.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.48.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.48.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.49.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.49.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.49.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.52.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.52.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.52.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.58.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.58.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.58.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.60.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.60.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.60.up_proj.weight': 322961408}, 2: {'model.layers.25.mlp.experts.0.gate_proj.weight': 0, 'model.layers.25.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.14.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.14.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.14.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.19.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.19.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.19.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.20.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.20.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.20.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.26.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.26.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.26.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.34.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.34.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.34.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.37.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.37.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.37.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.40.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.40.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.40.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.41.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.41.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.41.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.47.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.47.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.47.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.50.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.50.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.50.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.53.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.53.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.53.up_proj.weight': 271056896, 'model.layers.25.mlp.experts.54.gate_proj.weight': 276824064, 'model.layers.25.mlp.experts.54.down_proj.weight': 282591232, 'model.layers.25.mlp.experts.54.up_proj.weight': 288358400, 'model.layers.25.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.25.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.25.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.25.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.25.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.25.mlp.experts.62.up_proj.weight': 322961408, 'model.layers.25.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.25.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.25.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(29452926976, 5767168, 0, 0), (29458694144, 5767168, 5767168, 0), (29447159808, 5767168, 11534336, 0), (29487529984, 5767168, 17301504, 0), (29493297152, 5767168, 23068672, 0), (29481762816, 5767168, 28835840, 0), (29504831488, 5767168, 34603008, 0), (29510598656, 5767168, 40370176, 0), (29499064320, 5767168, 46137344, 0), (29625942016, 5767168, 51904512, 0), (29631709184, 5767168, 57671680, 0), (29620174848, 5767168, 63438848, 0), (29643243520, 5767168, 69206016, 0), (29649010688, 5767168, 74973184, 0), (29637476352, 5767168, 80740352, 0), (29695148032, 5767168, 86507520, 0), (29700915200, 5767168, 92274688, 0), (29689380864, 5767168, 98041856, 0), (29729751040, 5767168, 103809024, 0), (29735518208, 5767168, 109576192, 0), (29723983872, 5767168, 115343360, 0), (29747052544, 5767168, 121110528, 0), (29752819712, 5767168, 126877696, 0), (29741285376, 5767168, 132644864, 0), (29920067584, 5767168, 138412032, 0), (29925834752, 5767168, 144179200, 0), (29914300416, 5767168, 149946368, 0), (29937369088, 5767168, 155713536, 0), (29943136256, 5767168, 161480704, 0), (29931601920, 5767168, 167247872, 0), (29989273600, 5767168, 173015040, 0), (29995040768, 5767168, 178782208, 0), (29983506432, 5767168, 184549376, 0), (30041178112, 5767168, 190316544, 0), (30046945280, 5767168, 196083712, 0), (30035410944, 5767168, 201850880, 0), (30110384128, 5767168, 207618048, 0), (30116151296, 5767168, 213385216, 0), (30104616960, 5767168, 219152384, 0), (30266097664, 5767168, 224919552, 0), (30271864832, 5767168, 230686720, 0), (30260330496, 5767168, 236453888, 0), (30283399168, 5767168, 242221056, 0), (30289166336, 5767168, 247988224, 0), (30277632000, 5767168, 253755392, 0), (30335303680, 5767168, 259522560, 0), (30341070848, 5767168, 265289728, 0), (30329536512, 5767168, 271056896, 0), (30404509696, 5767168, 276824064, 0), (30410276864, 5767168, 282591232, 0), (30398742528, 5767168, 288358400, 0), (30439112704, 5767168, 294125568, 0), (30444879872, 5767168, 299892736, 0), (30433345536, 5767168, 305659904, 0), (30473715712, 5767168, 311427072, 0), (30479482880, 5767168, 317194240, 0), (30467948544, 5767168, 322961408, 0)], 2: [(29435625472, 5767168, 0, 0), (29441392640, 5767168, 5767168, 0), (29429858304, 5767168, 11534336, 0), (29556736000, 5767168, 17301504, 0), (29562503168, 5767168, 23068672, 0), (29550968832, 5767168, 28835840, 0), (29574037504, 5767168, 34603008, 0), (29579804672, 5767168, 40370176, 0), (29568270336, 5767168, 46137344, 0), (29677846528, 5767168, 51904512, 0), (29683613696, 5767168, 57671680, 0), (29672079360, 5767168, 63438848, 0), (29764354048, 5767168, 69206016, 0), (29770121216, 5767168, 74973184, 0), (29758586880, 5767168, 80740352, 0), (29781655552, 5767168, 86507520, 0), (29787422720, 5767168, 92274688, 0), (29775888384, 5767168, 98041856, 0), (29885464576, 5767168, 103809024, 0), (29891231744, 5767168, 109576192, 0), (29879697408, 5767168, 115343360, 0), (29902766080, 5767168, 121110528, 0), (29908533248, 5767168, 126877696, 0), (29896998912, 5767168, 132644864, 0), (29954670592, 5767168, 138412032, 0), (29960437760, 5767168, 144179200, 0), (29948903424, 5767168, 149946368, 0), (30023876608, 5767168, 155713536, 0), (30029643776, 5767168, 161480704, 0), (30018109440, 5767168, 167247872, 0), (30075781120, 5767168, 173015040, 0), (30081548288, 5767168, 178782208, 0), (30070013952, 5767168, 184549376, 0), (30127685632, 5767168, 190316544, 0), (30133452800, 5767168, 196083712, 0), (30121918464, 5767168, 201850880, 0), (30144987136, 5767168, 207618048, 0), (30150754304, 5767168, 213385216, 0), (30139219968, 5767168, 219152384, 0), (30248796160, 5767168, 224919552, 0), (30254563328, 5767168, 230686720, 0), (30243028992, 5767168, 236453888, 0), (30300700672, 5767168, 242221056, 0), (30306467840, 5767168, 247988224, 0), (30294933504, 5767168, 253755392, 0), (30352605184, 5767168, 259522560, 0), (30358372352, 5767168, 265289728, 0), (30346838016, 5767168, 271056896, 0), (30369906688, 5767168, 276824064, 0), (30375673856, 5767168, 282591232, 0), (30364139520, 5767168, 288358400, 0), (30421811200, 5767168, 294125568, 0), (30427578368, 5767168, 299892736, 0), (30416044032, 5767168, 305659904, 0), (30508318720, 5767168, 311427072, 0), (30514085888, 5767168, 317194240, 0), (30502551552, 5767168, 322961408, 0), (30525620224, 5767168, 328728576, 0), (30531387392, 5767168, 334495744, 0), (30519853056, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a814759ce0>, 2: <capsule object NULL at 0x74a6bc762610>}
DEBUG 01-15 16:09:24.816154.816154 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.816712.816712 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.816991.816991 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, fc49b25a-93eb-490b-a675-042a0431fd43
DEBUG 01-15 16:09:24.816304.816304 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:09:24.816116.816116 client.py:127] Model loaded
DEBUG 01-15 16:09:24.817796.817796 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.817172.817172 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.817442.817442 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:24.818122.818122 cuda_h.py:19] end restore2model cost 0.0009582042694091797 seconds
DEBUG 01-15 16:09:24.818086.818086 cuda_h.py:19] end sllm_worker_task cost 0.012738704681396484 seconds
DEBUG 01-15 16:09:24.818195.818195 cuda_h.py:19] end move_flatidxs cost 0.0008690357208251953 seconds
DEBUG 01-15 16:09:24.818700.818700 cuda_h.py:10] start group_tensors
INFO 01-15 16:09:24.818317.818317 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, fc49b25a-93eb-490b-a675-042a0431fd43
DEBUG 01-15 16:09:24.819882.819882 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004276752471923828 seconds
DEBUG 01-15 16:09:24.819329.819329 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.822691.822691 cuda_h.py:19] end restore2model cost 0.0030736923217773438 seconds
DEBUG 01-15 16:09:24.822654.822654 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007618904113769531 seconds
DEBUG 01-15 16:09:24.822973.822973 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.822427.822427 cuda_h.py:19] end gpu_sexperts cost 0.0002689361572265625 seconds
DEBUG 01-15 16:09:24.822302.822302 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.822079.822079 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:09:24.822013.822013 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.822716.822716 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.823642.823642 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009334087371826172 seconds
DEBUG 01-15 16:09:24.823201.823201 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.824805.824805 cuda_h.py:19] end gpu_group_list cost 0.00020599365234375 seconds
DEBUG 01-15 16:09:24.824270.824270 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.826511.826511 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001032114028930664 seconds
DEBUG 01-15 16:09:24.826739.826739 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.826025.826025 cuda_h.py:19] end gpu_group_list cost 0.0002167224884033203 seconds
DEBUG 01-15 16:09:24.827538.827538 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.827129.827129 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, fc49b25a-93eb-490b-a675-042a0431fd43
DEBUG 01-15 16:09:24.830546.830546 cuda_h.py:19] end group_tensors cost 0.011353731155395508 seconds
DEBUG 01-15 16:09:24.830900.830900 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.833467.833467 cuda_h.py:19] end group pad cost 0.0026433467864990234 seconds
DEBUG 01-15 16:09:24.833973.833973 cuda_h.py:10] start group_einsum
INFO 01-15 16:09:24.862218.862218 client.py:127] Model loaded
DEBUG 01-15 16:09:24.863631.863631 cuda_h.py:19] end wait_experts_multi_device cost 0.03588676452636719 seconds
DEBUG 01-15 16:09:24.863786.863786 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.864948.864948 cuda_h.py:19] end group_einsum cost 0.0307009220123291 seconds
DEBUG 01-15 16:09:24.864739.864739 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.867272.867272 cuda_h.py:19] end get_outputs_cpu1 cost 0.0024247169494628906 seconds
DEBUG 01-15 16:09:24.867634.867634 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05049848556518555 seconds
DEBUG 01-15 16:09:24.868282.868282 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005811452865600586 seconds
DEBUG 01-15 16:09:24.869910.869910 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.871732.871732 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.871381.871381 cuda_h.py:19] end index_scatter cost 8.726119995117188e-05 seconds
DEBUG 01-15 16:09:24.871240.871240 cuda_h.py:19] end cpuoutputsdeal cost 0.00244140625 seconds
DEBUG 01-15 16:09:24.871488.871488 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.871105.871105 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.871780.871780 cuda_h.py:19] end gpu_group_tensor cost 0.00015163421630859375 seconds
DEBUG 01-15 16:09:24.872112.872112 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.872012.872012 cuda_h.py:19] end gpu_group_tensor cost 0.0001437664031982422 seconds
DEBUG 01-15 16:09:24.872062.872062 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.873392.873392 cuda_h.py:19] end gpu_group_einsum cost 0.0007226467132568359 seconds
DEBUG 01-15 16:09:24.873827.873827 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.873133.873133 cuda_h.py:19] end gpu_group_einsum cost 0.0005507469177246094 seconds
DEBUG 01-15 16:09:24.873985.873985 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.874870.874870 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.874320.874320 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003528594970703125 seconds
DEBUG 01-15 16:09:24.874944.874944 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.874994.874994 cuda_h.py:19] end concat_expert_out cost 6.794929504394531e-05 seconds
DEBUG 01-15 16:09:24.874003.874003 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.874643.874643 cuda_h.py:19] end index_scatter cost 7.677078247070312e-05 seconds
DEBUG 01-15 16:09:24.875825.875825 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010876655578613281 seconds
DEBUG 01-15 16:09:24.875603.875603 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.875182.875182 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.875099.875099 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002529621124267578 seconds
DEBUG 01-15 16:09:24.875147.875147 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.875528.875528 cuda_h.py:19] end concat_expert_out cost 6.794929504394531e-05 seconds
DEBUG 01-15 16:09:24.875199.875199 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.875540.875540 cuda_h.py:19] end index_scatter cost 6.914138793945312e-05 seconds
DEBUG 01-15 16:09:24.875741.875741 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006890296936035156 seconds
DEBUG 01-15 16:09:24.876386.876386 cuda_h.py:19] end gpu_experts_multi_device cost 0.05323505401611328 seconds
DEBUG 01-15 16:09:24.876138.876138 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.06409239768981934 seconds
DEBUG 01-15 16:09:24.876855.876855 cuda_h.py:19] end prefill_layer cost 0.07176685333251953 seconds
DEBUG 01-15 16:09:24.876274.876274 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:09:24.876745.876745 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.876601.876601 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:09:24.876311.876311 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:09:24.876550.876550 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:09:24.876268.876268 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:09:24.876653.876653 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 7.891654968261719e-05 seconds
DEBUG 01-15 16:09:24.876502.876502 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.877101.877101 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.877998.877998 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:09:24.877724.877724 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:09:24.877455.877455 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:09:24.877429.877429 cuda_h.py:19] end allocate_cuda_memory cost 0.0003440380096435547 seconds
DEBUG 01-15 16:09:24.877963.877963 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:09:24.877898.877898 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.877775.877775 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.878744.878744 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 011094de-f291-43b3-97f6-8ffae5c96c31
DEBUG 01-15 16:09:24.878768.878768 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.878461.878461 cuda_h.py:10] start self_attn
INFO 01-15 16:09:24.879213.879213 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 011094de-f291-43b3-97f6-8ffae5c96c31
DEBUG 01-15 16:09:24.879514.879514 cuda_h.py:19] end load_into_gpu_async cost 0.0019626617431640625 seconds
DEBUG 01-15 16:09:24.879906.879906 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:09:24.880056.880056 cuda_h.py:19] end restore_tensors2 cost 9.918212890625e-05 seconds
DEBUG 01-15 16:09:24.880608.880608 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0028083324432373047 seconds
INFO 01-15 16:09:24.880452.880452 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 011094de-f291-43b3-97f6-8ffae5c96c31
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.882695.882695 cuda_h.py:19] end self_attn cost 0.004135847091674805 seconds
DEBUG 01-15 16:09:24.883925.883925 cuda_h.py:19] end iln_self_attn_paln cost 0.006067752838134766 seconds
DEBUG 01-15 16:09:24.883278.883278 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:09:24.883848.883848 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.883999.883999 cuda_h.py:19] end gate cost 0.0007059574127197266 seconds
DEBUG 01-15 16:09:24.883497.883497 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.884190.884190 lmp.py:1912] 
DEBUG 01-15 16:09:24.884190.884190 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.884999.884999 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.884795.884795 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.884537.884537 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.884895.884895 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.884777.884777 lmp.py:1917] 
DEBUG 01-15 16:09:24.884777.884777 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.884896.884896 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.884454.884454 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:09:24.884289.884289 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:09:24.884647.884647 lmp.py:1935]   Expert 11 |     29 | CPU
DEBUG 01-15 16:09:24.884290.884290 lmp.py:1935]   Expert  7 |     40 | CPU
DEBUG 01-15 16:09:24.884032.884032 lmp.py:1935]   Expert 62 |     44 | CPU
DEBUG 01-15 16:09:24.884152.884152 lmp.py:1935]   Expert 51 |     45 | CPU
DEBUG 01-15 16:09:24.884749.884749 lmp.py:1935]   Expert  3 |     48 | CPU
DEBUG 01-15 16:09:24.884345.884345 lmp.py:1935]   Expert 30 |     51 | CPU
DEBUG 01-15 16:09:24.884988.884988 lmp.py:1935]   Expert 17 |     52 | CPU
DEBUG 01-15 16:09:24.884631.884631 lmp.py:1935]   Expert 29 |     55 | CPU
DEBUG 01-15 16:09:24.884274.884274 lmp.py:1935]   Expert  6 |     60 | CPU
DEBUG 01-15 16:09:24.884917.884917 lmp.py:1935]   Expert  9 |     65 | CPU
DEBUG 01-15 16:09:24.884799.884799 lmp.py:1935]   Expert 38 |     75 | CPU
DEBUG 01-15 16:09:24.884157.884157 lmp.py:1935]   Expert 63 |     77 | CPU
DEBUG 01-15 16:09:24.884800.884800 lmp.py:1935]   Expert 55 |     82 | CPU
DEBUG 01-15 16:09:24.884204.884204 lmp.py:1935]   Expert 59 |     86 | CPU
DEBUG 01-15 16:09:24.884086.884086 lmp.py:1935]   Expert 48 |     89 | CPU
DEBUG 01-15 16:09:24.884490.884490 lmp.py:1935]   Expert 19 |     95 | CPU
DEBUG 01-15 16:09:24.884418.884418 lmp.py:1935]   Expert  8 |     97 | CPU
DEBUG 01-15 16:09:24.884061.884061 lmp.py:1935]   Expert 49 |    101 | CPU
DEBUG 01-15 16:09:24.884658.884658 lmp.py:1935]   Expert 22 |    102 | CPU
DEBUG 01-15 16:09:24.884254.884254 lmp.py:1935]   Expert 24 |    110 | CPU
DEBUG 01-15 16:09:24.885136.885136 lmp.py:1935]   Expert 34 |    112 | CPU
DEBUG 01-15 16:09:24.885017.885017 lmp.py:1935]   Expert 36 |    113 | CPU
DEBUG 01-15 16:09:24.885899.885899 lmp.py:1935]   Expert 42 |    117 | CPU
DEBUG 01-15 16:09:24.885210.885210 lmp.py:1935]   Expert 50 |    120 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885522.885522 lmp.py:1935]   Expert 39 |    125 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885073.885073 lmp.py:1935]   Expert  4 |    132 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885908.885908 lmp.py:1935]   Expert 37 |    143 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885696.885696 lmp.py:1935]   Expert 41 |    148 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885770.885770 lmp.py:1935]   Expert 15 |    149 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885367.885367 lmp.py:1935]   Expert 23 |    156 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885202.885202 lmp.py:1935]   Expert 56 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885560.885560 lmp.py:1935]   Expert 60 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885680.885680 lmp.py:1935]   Expert 16 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885276.885276 lmp.py:1935]   Expert 44 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885635.885635 lmp.py:1935]   Expert  1 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885423.885423 lmp.py:1935]   Expert 21 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885020.885020 lmp.py:1935]   Expert 43 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885378.885378 lmp.py:1935]   Expert 47 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885975.885975 lmp.py:1935]   Expert 53 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885333.885333 lmp.py:1935]   Expert 33 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885453.885453 lmp.py:1935]   Expert 12 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885050.885050 lmp.py:1935]   Expert 13 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885646.885646 lmp.py:1935]   Expert 32 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885720.885720 lmp.py:1935]   Expert 28 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885555.885555 lmp.py:1935]   Expert  0 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885913.885913 lmp.py:1935]   Expert 31 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885033.885033 lmp.py:1935]   Expert 26 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885391.885391 lmp.py:1935]   Expert 54 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885511.885511 lmp.py:1935]   Expert 10 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885392.885392 lmp.py:1935]   Expert 18 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885750.885750 lmp.py:1935]   Expert 57 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885109.885109 lmp.py:1935]   Expert  2 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885990.885990 lmp.py:1935]   Expert 58 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885348.885348 lmp.py:1935]   Expert 40 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885151.885151 lmp.py:1935]   Expert 25 |    359 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885509.885509 lmp.py:1935]   Expert 45 |    363 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885152.885152 lmp.py:1935]   Expert  5 |    445 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885033.885033 lmp.py:1935]   Expert 35 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885676.885676 lmp.py:1935]   Expert 27 |    486 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885081.885081 lmp.py:1935]   Expert 46 |    554 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885154.885154 lmp.py:1935]   Expert 52 |    593 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.885705.885705 lmp.py:1935]   Expert 14 |    883 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.885301.885301 lmp.py:1937] 
DEBUG 01-15 16:09:24.885301.885301 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.885375.885375 lmp.py:1938]   CPU:   1767 tokens
DEBUG 01-15 16:09:24.885879.885879 lmp.py:1942]   cuda:1:   5247 tokens (19 experts)
DEBUG 01-15 16:09:24.885144.885144 lmp.py:1942]   cuda:2:   5274 tokens (20 experts)
DEBUG 01-15 16:09:24.885741.885741 lmp.py:1943]   Total GPU:  10521 tokens
DEBUG 01-15 16:09:24.885576.885576 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.885576.885576 lmp.py:1944] 
DEBUG 01-15 16:09:24.885372.885372 cuda_h.py:19] end experts_map_get cost 0.0019495487213134766 seconds
DEBUG 01-15 16:09:24.885943.885943 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.886892.886892 lmp.py:1953] 
DEBUG 01-15 16:09:24.886892.886892 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.886774.886774 cuda_h.py:19] end cpu_experts_submit cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:09:24.886252.886252 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.886327.886327 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.887068.887068 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.25.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.25.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.25.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.26.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.26.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.26.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.27.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.27.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.27.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.32.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.32.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.32.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.37.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.37.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.37.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.39.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.39.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.39.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.43.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.43.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.43.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.47.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.47.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.47.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.56.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.56.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.56.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.57.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.57.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.57.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.58.up_proj.weight': 322961408}, 2: {'model.layers.26.mlp.experts.2.gate_proj.weight': 0, 'model.layers.26.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.13.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.13.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.13.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.18.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.18.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.18.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.21.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.21.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.21.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.23.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.23.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.23.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.28.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.28.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.28.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.31.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.31.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.31.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.33.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.33.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.33.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.40.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.40.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.40.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.41.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.41.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.41.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.44.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.44.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.44.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.45.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.45.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.45.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.46.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.46.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.46.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.50.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.50.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.50.up_proj.weight': 271056896, 'model.layers.26.mlp.experts.52.gate_proj.weight': 276824064, 'model.layers.26.mlp.experts.52.down_proj.weight': 282591232, 'model.layers.26.mlp.experts.52.up_proj.weight': 288358400, 'model.layers.26.mlp.experts.53.gate_proj.weight': 294125568, 'model.layers.26.mlp.experts.53.down_proj.weight': 299892736, 'model.layers.26.mlp.experts.53.up_proj.weight': 305659904, 'model.layers.26.mlp.experts.54.gate_proj.weight': 311427072, 'model.layers.26.mlp.experts.54.down_proj.weight': 317194240, 'model.layers.26.mlp.experts.54.up_proj.weight': 322961408, 'model.layers.26.mlp.experts.60.gate_proj.weight': 328728576, 'model.layers.26.mlp.experts.60.down_proj.weight': 334495744, 'model.layers.26.mlp.experts.60.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (30560223232, 5767168, 17301504, 0), (30565990400, 5767168, 23068672, 0), (30554456064, 5767168, 28835840, 0), (30629429248, 5767168, 34603008, 0), (30635196416, 5767168, 40370176, 0), (30623662080, 5767168, 46137344, 0), (30715936768, 5767168, 51904512, 0), (30721703936, 5767168, 57671680, 0), (30710169600, 5767168, 63438848, 0), (30750539776, 5767168, 69206016, 0), (30756306944, 5767168, 74973184, 0), (30744772608, 5767168, 80740352, 0), (30785142784, 5767168, 86507520, 0), (30790909952, 5767168, 92274688, 0), (30779375616, 5767168, 98041856, 0), (30802444288, 5767168, 103809024, 0), (30808211456, 5767168, 109576192, 0), (30796677120, 5767168, 115343360, 0), (30819745792, 5767168, 121110528, 0), (30825512960, 5767168, 126877696, 0), (30813978624, 5767168, 132644864, 0), (30975459328, 5767168, 138412032, 0), (30981226496, 5767168, 144179200, 0), (30969692160, 5767168, 149946368, 0), (30992760832, 5767168, 155713536, 0), (30998528000, 5767168, 161480704, 0), (30986993664, 5767168, 167247872, 0), (31010062336, 5767168, 173015040, 0), (31015829504, 5767168, 178782208, 0), (31004295168, 5767168, 184549376, 0), (31096569856, 5767168, 190316544, 0), (31102337024, 5767168, 196083712, 0), (31090802688, 5767168, 201850880, 0), (31183077376, 5767168, 207618048, 0), (31188844544, 5767168, 213385216, 0), (31177310208, 5767168, 219152384, 0), (31217680384, 5767168, 224919552, 0), (31223447552, 5767168, 230686720, 0), (31211913216, 5767168, 236453888, 0), (31286886400, 5767168, 242221056, 0), (31292653568, 5767168, 247988224, 0), (31281119232, 5767168, 253755392, 0), (31356092416, 5767168, 259522560, 0), (31361859584, 5767168, 265289728, 0), (31350325248, 5767168, 271056896, 0), (31511805952, 5767168, 276824064, 0), (31517573120, 5767168, 282591232, 0), (31506038784, 5767168, 288358400, 0), (31529107456, 5767168, 294125568, 0), (31534874624, 5767168, 299892736, 0), (31523340288, 5767168, 305659904, 0), (31546408960, 5767168, 311427072, 0), (31552176128, 5767168, 317194240, 0), (31540641792, 5767168, 322961408, 0)], 2: [(30577524736, 5767168, 0, 0), (30583291904, 5767168, 5767168, 0), (30571757568, 5767168, 11534336, 0), (30612127744, 5767168, 17301504, 0), (30617894912, 5767168, 23068672, 0), (30606360576, 5767168, 28835840, 0), (30767841280, 5767168, 34603008, 0), (30773608448, 5767168, 40370176, 0), (30762074112, 5767168, 46137344, 0), (30854348800, 5767168, 51904512, 0), (30860115968, 5767168, 57671680, 0), (30848581632, 5767168, 63438848, 0), (30906253312, 5767168, 69206016, 0), (30912020480, 5767168, 74973184, 0), (30900486144, 5767168, 80740352, 0), (30940856320, 5767168, 86507520, 0), (30946623488, 5767168, 92274688, 0), (30935089152, 5767168, 98041856, 0), (31027363840, 5767168, 103809024, 0), (31033131008, 5767168, 109576192, 0), (31021596672, 5767168, 115343360, 0), (31079268352, 5767168, 121110528, 0), (31085035520, 5767168, 126877696, 0), (31073501184, 5767168, 132644864, 0), (31113871360, 5767168, 138412032, 0), (31119638528, 5767168, 144179200, 0), (31108104192, 5767168, 149946368, 0), (31148474368, 5767168, 155713536, 0), (31154241536, 5767168, 161480704, 0), (31142707200, 5767168, 167247872, 0), (31234981888, 5767168, 173015040, 0), (31240749056, 5767168, 178782208, 0), (31229214720, 5767168, 184549376, 0), (31252283392, 5767168, 190316544, 0), (31258050560, 5767168, 196083712, 0), (31246516224, 5767168, 201850880, 0), (31304187904, 5767168, 207618048, 0), (31309955072, 5767168, 213385216, 0), (31298420736, 5767168, 219152384, 0), (31321489408, 5767168, 224919552, 0), (31327256576, 5767168, 230686720, 0), (31315722240, 5767168, 236453888, 0), (31338790912, 5767168, 242221056, 0), (31344558080, 5767168, 247988224, 0), (31333023744, 5767168, 253755392, 0), (31407996928, 5767168, 259522560, 0), (31413764096, 5767168, 265289728, 0), (31402229760, 5767168, 271056896, 0), (31442599936, 5767168, 276824064, 0), (31448367104, 5767168, 282591232, 0), (31436832768, 5767168, 288358400, 0), (31459901440, 5767168, 294125568, 0), (31465668608, 5767168, 299892736, 0), (31454134272, 5767168, 305659904, 0), (31477202944, 5767168, 311427072, 0), (31482970112, 5767168, 317194240, 0), (31471435776, 5767168, 322961408, 0), (31581011968, 5767168, 328728576, 0), (31586779136, 5767168, 334495744, 0), (31575244800, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6807abc90>, 2: <capsule object NULL at 0x74a6bc762a60>}
DEBUG 01-15 16:09:24.887645.887645 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:09:24.887994.887994 client.py:127] Model loaded
DEBUG 01-15 16:09:24.887819.887819 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.888490.888490 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.888584.888584 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 08c4c4bf-d616-4b34-b9ca-f7f7bb16abff
DEBUG 01-15 16:09:24.888417.888417 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:09:24.888698.888698 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.888616.888616 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.889296.889296 cuda_h.py:19] end move_flatidxs cost 0.0008668899536132812 seconds
DEBUG 01-15 16:09:24.889993.889993 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:24.889862.889862 cuda_h.py:19] end restore2model cost 0.0008029937744140625 seconds
DEBUG 01-15 16:09:24.889230.889230 cuda_h.py:19] end sllm_worker_task cost 0.012140750885009766 seconds
INFO 01-15 16:09:24.890958.890958 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 08c4c4bf-d616-4b34-b9ca-f7f7bb16abff
DEBUG 01-15 16:09:24.891995.891995 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00487971305847168 seconds
DEBUG 01-15 16:09:24.891825.891825 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.895925.895925 cuda_h.py:19] end restore2model cost 0.003932952880859375 seconds
DEBUG 01-15 16:09:24.895398.895398 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.009071826934814453 seconds
DEBUG 01-15 16:09:24.895909.895909 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.895477.895477 cuda_h.py:19] end gpu_sexperts cost 0.0003151893615722656 seconds
DEBUG 01-15 16:09:24.895452.895452 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:09:24.895997.895997 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8835067749023438e-05 seconds
DEBUG 01-15 16:09:24.895600.895600 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.895449.895449 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.897752.897752 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015230178833007812 seconds
DEBUG 01-15 16:09:24.897709.897709 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.897372.897372 cuda_h.py:19] end gpu_group_list cost 0.00021219253540039062 seconds
DEBUG 01-15 16:09:24.898145.898145 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.900297.900297 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014863014221191406 seconds
DEBUG 01-15 16:09:24.900021.900021 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.900388.900388 cuda_h.py:19] end gpu_group_list cost 0.0002143383026123047 seconds
DEBUG 01-15 16:09:24.900648.900648 cuda_h.py:19] end group_tensors cost 0.010793447494506836 seconds
DEBUG 01-15 16:09:24.900803.900803 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.901329.901329 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.901596.901596 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 08c4c4bf-d616-4b34-b9ca-f7f7bb16abff
DEBUG 01-15 16:09:24.903671.903671 cuda_h.py:19] end group pad cost 0.0027267932891845703 seconds
DEBUG 01-15 16:09:24.903063.903063 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.925626.925626 cuda_h.py:19] end group_einsum cost 0.021867036819458008 seconds
DEBUG 01-15 16:09:24.925265.925265 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.927694.927694 cuda_h.py:19] end get_outputs_cpu1 cost 0.0020656585693359375 seconds
DEBUG 01-15 16:09:24.928554.928554 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04058504104614258 seconds
INFO 01-15 16:09:24.929354.929354 client.py:127] Model loaded
DEBUG 01-15 16:09:24.929146.929146 cuda_h.py:19] end wait_experts_multi_device cost 0.02750420570373535 seconds
DEBUG 01-15 16:09:24.929624.929624 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.929011.929011 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00046062469482421875 seconds
DEBUG 01-15 16:09:24.929563.929563 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.930412.930412 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.930835.930835 cuda_h.py:19] end index_scatter cost 0.00011110305786132812 seconds
DEBUG 01-15 16:09:24.931075.931075 cuda_h.py:19] end cpuoutputsdeal cost 0.001220703125 seconds
DEBUG 01-15 16:09:24.931501.931501 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.931496.931496 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.931243.931243 cuda_h.py:19] end gpu_group_tensor cost 0.00013637542724609375 seconds
DEBUG 01-15 16:09:24.931575.931575 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.931772.931772 cuda_h.py:19] end gpu_group_tensor cost 0.00011873245239257812 seconds
DEBUG 01-15 16:09:24.931517.931517 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.932619.932619 cuda_h.py:19] end gpu_group_einsum cost 0.00041794776916503906 seconds
DEBUG 01-15 16:09:24.932861.932861 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.932863.932863 cuda_h.py:19] end gpu_group_einsum cost 0.0004115104675292969 seconds
DEBUG 01-15 16:09:24.932813.932813 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.932042.932042 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.933797.933797 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016760826110839844 seconds
DEBUG 01-15 16:09:24.933361.933361 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.933946.933946 cuda_h.py:19] end concat_expert_out cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:09:24.933717.933717 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.933330.933330 cuda_h.py:19] end index_scatter cost 6.580352783203125e-05 seconds
DEBUG 01-15 16:09:24.933596.933596 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007765293121337891 seconds
DEBUG 01-15 16:09:24.933950.933950 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.933932.933932 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.933075.933075 cuda_h.py:19] end all_expert_outputs_slices cost 0.00011491775512695312 seconds
DEBUG 01-15 16:09:24.933348.933348 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.933827.933827 cuda_h.py:19] end concat_expert_out cost 4.649162292480469e-05 seconds
DEBUG 01-15 16:09:24.933756.933756 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.934057.934057 cuda_h.py:19] end index_scatter cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:09:24.934389.934389 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004317760467529297 seconds
DEBUG 01-15 16:09:24.934431.934431 cuda_h.py:19] end gpu_experts_multi_device cost 0.038440704345703125 seconds
DEBUG 01-15 16:09:24.934818.934818 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.05110478401184082 seconds
DEBUG 01-15 16:09:24.934406.934406 cuda_h.py:19] end prefill_layer cost 0.057859182357788086 seconds
DEBUG 01-15 16:09:24.934971.934971 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:09:24.934574.934574 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:09:24.934178.934178 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:09:24.934066.934066 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:09:24.934254.934254 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:09:24.935342.935342 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:09:24.937962.937962 cuda_h.py:19] end self_attn cost 0.0024361610412597656 seconds
DEBUG 01-15 16:09:24.937714.937714 cuda_h.py:19] end iln_self_attn_paln cost 0.0031173229217529297 seconds
DEBUG 01-15 16:09:24.937875.937875 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:09:24.937823.937823 cuda_h.py:10] start gate
DEBUG 01-15 16:09:24.938627.938627 cuda_h.py:19] end gate cost 0.0006284713745117188 seconds
DEBUG 01-15 16:09:24.938410.938410 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:09:24.939282.939282 lmp.py:1912] 
DEBUG 01-15 16:09:24.939282.939282 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:09:24.939753.939753 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:09:24.939356.939356 lmp.py:1914]   CPU experts: 25 (39%)
DEBUG 01-15 16:09:24.939907.939907 lmp.py:1915]   GPU experts: 39 (61%)
DEBUG 01-15 16:09:24.939073.939073 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:09:24.939239.939239 lmp.py:1917] 
DEBUG 01-15 16:09:24.939239.939239 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:09:24.939644.939644 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:09:24.939247.939247 lmp.py:1935]   Expert 18 |     67 | CPU
DEBUG 01-15 16:09:24.939652.939652 lmp.py:1935]   Expert 47 |     71 | CPU
DEBUG 01-15 16:09:24.939102.939102 lmp.py:1935]   Expert 54 |     73 | CPU
DEBUG 01-15 16:09:24.939315.939315 lmp.py:1935]   Expert 23 |     76 | CPU
DEBUG 01-15 16:09:24.939766.939766 lmp.py:1935]   Expert 48 |     77 | CPU
DEBUG 01-15 16:09:24.939455.939455 lmp.py:1935]   Expert 44 |     84 | CPU
DEBUG 01-15 16:09:24.939098.939098 lmp.py:1935]   Expert 45 |     85 | CPU
DEBUG 01-15 16:09:24.939264.939264 lmp.py:1935]   Expert 20 |     92 | CPU
DEBUG 01-15 16:09:24.939477.939477 lmp.py:1935]   Expert 31 |     97 | CPU
DEBUG 01-15 16:09:24.939928.939928 lmp.py:1935]   Expert 36 |    105 | CPU
DEBUG 01-15 16:09:24.939378.939378 lmp.py:1935]   Expert 61 |    110 | CPU
DEBUG 01-15 16:09:24.939829.939829 lmp.py:1935]   Expert 33 |    120 | CPU
DEBUG 01-15 16:09:24.939042.939042 lmp.py:1935]   Expert 42 |    120 | CPU
DEBUG 01-15 16:09:24.939016.939016 lmp.py:1935]   Expert 43 |    121 | CPU
DEBUG 01-15 16:09:24.939705.939705 lmp.py:1935]   Expert 10 |    122 | CPU
DEBUG 01-15 16:09:24.939633.939633 lmp.py:1935]   Expert 24 |    124 | CPU
DEBUG 01-15 16:09:24.939799.939799 lmp.py:1935]   Expert 49 |    125 | CPU
DEBUG 01-15 16:09:24.939773.939773 lmp.py:1935]   Expert 11 |    129 | CPU
DEBUG 01-15 16:09:24.939986.939986 lmp.py:1935]   Expert 56 |    129 | CPU
DEBUG 01-15 16:09:24.939198.939198 lmp.py:1935]   Expert  6 |    134 | CPU
DEBUG 01-15 16:09:24.939172.939172 lmp.py:1935]   Expert 51 |    142 | CPU
DEBUG 01-15 16:09:24.939861.939861 lmp.py:1935]   Expert  0 |    148 | CPU
DEBUG 01-15 16:09:24.939074.939074 lmp.py:1935]   Expert  5 |    151 | CPU
DEBUG 01-15 16:09:24.939525.939525 lmp.py:1935]   Expert 17 |    151 | CPU
DEBUG 01-15 16:09:24.939260.939260 lmp.py:1935]   Expert 12 |    156 | CPU
DEBUG 01-15 16:09:24.939857.939857 lmp.py:1935]   Expert 40 |    157 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939215.939215 lmp.py:1935]   Expert 55 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939335.939335 lmp.py:1935]   Expert 57 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939216.939216 lmp.py:1935]   Expert 59 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939383.939383 lmp.py:1935]   Expert 26 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939787.939787 lmp.py:1935]   Expert 13 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939953.939953 lmp.py:1935]   Expert 38 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939881.939881 lmp.py:1935]   Expert 46 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939809.939809 lmp.py:1935]   Expert 58 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939498.939498 lmp.py:1935]   Expert 35 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939379.939379 lmp.py:1935]   Expert 30 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939499.939499 lmp.py:1935]   Expert 50 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939665.939665 lmp.py:1935]   Expert 16 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939831.939831 lmp.py:1935]   Expert  7 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939998.939998 lmp.py:1935]   Expert 15 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939164.939164 lmp.py:1935]   Expert 32 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939568.939568 lmp.py:1935]   Expert 14 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939734.939734 lmp.py:1935]   Expert  1 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939854.939854 lmp.py:1935]   Expert  4 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939497.939497 lmp.py:1935]   Expert  3 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939617.939617 lmp.py:1935]   Expert 34 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939022.939022 lmp.py:1935]   Expert 39 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939949.939949 lmp.py:1935]   Expert 28 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.939877.939877 lmp.py:1935]   Expert 52 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.939043.939043 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.940209.940209 lmp.py:1935]   Expert 22 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.940137.940137 lmp.py:1935]   Expert  2 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.940780.940780 lmp.py:1935]   Expert 21 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.940661.940661 lmp.py:1935]   Expert 41 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.940066.940066 lmp.py:1935]   Expert 60 |    284 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.940232.940232 lmp.py:1935]   Expert 63 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.940160.940160 lmp.py:1935]   Expert 29 |    296 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.940326.940326 lmp.py:1935]   Expert 62 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.940492.940492 lmp.py:1935]   Expert 27 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.940658.940658 lmp.py:1935]   Expert 37 |    330 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.940824.940824 lmp.py:1935]   Expert 53 |    334 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.940229.940229 lmp.py:1935]   Expert  8 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.940110.940110 lmp.py:1935]   Expert 19 |    442 | GPU2(cuda:2)
DEBUG 01-15 16:09:24.940276.940276 lmp.py:1935]   Expert  9 |    622 | GPU1(cuda:1)
DEBUG 01-15 16:09:24.940535.940535 lmp.py:1937] 
DEBUG 01-15 16:09:24.940535.940535 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:09:24.940225.940225 lmp.py:1938]   CPU:   2809 tokens
DEBUG 01-15 16:09:24.940391.940391 lmp.py:1942]   cuda:1:   4689 tokens (19 experts)
DEBUG 01-15 16:09:24.940318.940318 lmp.py:1942]   cuda:2:   4790 tokens (20 experts)
DEBUG 01-15 16:09:24.940531.940531 lmp.py:1943]   Total GPU:   9479 tokens
DEBUG 01-15 16:09:24.940266.940266 lmp.py:1944] ============================================================
DEBUG 01-15 16:09:24.940266.940266 lmp.py:1944] 
DEBUG 01-15 16:09:24.940486.940486 cuda_h.py:19] end experts_map_get cost 0.0016429424285888672 seconds
DEBUG 01-15 16:09:24.940925.940925 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:09:24.940827.940827 lmp.py:1953] 
DEBUG 01-15 16:09:24.940827.940827 lmp.py:1953]   Computing 25 experts on CPU MP...
DEBUG 01-15 16:09:24.940332.940332 cuda_h.py:19] end cpu_experts_submit cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:09:24.940419.940419 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:09:24.940216.940216 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:09:24.941559.941559 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.1.gate_proj.weight': 0, 'model.layers.27.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.15.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.15.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.15.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.27.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.27.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.27.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.34.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.34.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.34.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.35.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.35.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.35.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.38.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.38.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.38.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.46.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.46.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.46.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.50.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.50.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.50.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.57.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.59.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.59.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.59.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.60.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.60.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.60.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.62.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.62.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.62.up_proj.weight': 322961408}, 2: {'model.layers.27.mlp.experts.2.gate_proj.weight': 0, 'model.layers.27.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.13.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.13.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.13.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.25.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.25.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.25.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.26.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.26.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.26.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.28.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.28.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.28.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.29.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.29.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.29.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.32.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.32.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.32.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.37.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.37.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.37.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.39.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.39.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.39.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.40.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.40.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.40.up_proj.weight': 271056896, 'model.layers.27.mlp.experts.41.gate_proj.weight': 276824064, 'model.layers.27.mlp.experts.41.down_proj.weight': 282591232, 'model.layers.27.mlp.experts.41.up_proj.weight': 288358400, 'model.layers.27.mlp.experts.55.gate_proj.weight': 294125568, 'model.layers.27.mlp.experts.55.down_proj.weight': 299892736, 'model.layers.27.mlp.experts.55.up_proj.weight': 305659904, 'model.layers.27.mlp.experts.58.gate_proj.weight': 311427072, 'model.layers.27.mlp.experts.58.down_proj.weight': 317194240, 'model.layers.27.mlp.experts.58.up_proj.weight': 322961408, 'model.layers.27.mlp.experts.63.gate_proj.weight': 328728576, 'model.layers.27.mlp.experts.63.down_proj.weight': 334495744, 'model.layers.27.mlp.experts.63.up_proj.weight': 340262912}}tensor_copy_chunks_device_map {1: [(31667519488, 5767168, 0, 0), (31673286656, 5767168, 5767168, 0), (31661752320, 5767168, 11534336, 0), (31702122496, 5767168, 17301504, 0), (31707889664, 5767168, 23068672, 0), (31696355328, 5767168, 28835840, 0), (31771328512, 5767168, 34603008, 0), (31777095680, 5767168, 40370176, 0), (31765561344, 5767168, 46137344, 0), (31805931520, 5767168, 51904512, 0), (31811698688, 5767168, 57671680, 0), (31800164352, 5767168, 63438848, 0), (31909740544, 5767168, 69206016, 0), (31915507712, 5767168, 74973184, 0), (31903973376, 5767168, 80740352, 0), (32013549568, 5767168, 86507520, 0), (32019316736, 5767168, 92274688, 0), (32007782400, 5767168, 98041856, 0), (32030851072, 5767168, 103809024, 0), (32036618240, 5767168, 109576192, 0), (32025083904, 5767168, 115343360, 0), (32117358592, 5767168, 121110528, 0), (32123125760, 5767168, 126877696, 0), (32111591424, 5767168, 132644864, 0), (32238469120, 5767168, 138412032, 0), (32244236288, 5767168, 144179200, 0), (32232701952, 5767168, 149946368, 0), (32255770624, 5767168, 155713536, 0), (32261537792, 5767168, 161480704, 0), (32250003456, 5767168, 167247872, 0), (32307675136, 5767168, 173015040, 0), (32313442304, 5767168, 178782208, 0), (32301907968, 5767168, 184549376, 0), (32446087168, 5767168, 190316544, 0), (32451854336, 5767168, 196083712, 0), (32440320000, 5767168, 201850880, 0), (32515293184, 5767168, 207618048, 0), (32521060352, 5767168, 213385216, 0), (32509526016, 5767168, 219152384, 0), (32549896192, 5767168, 224919552, 0), (32555663360, 5767168, 230686720, 0), (32544129024, 5767168, 236453888, 0), (32567197696, 5767168, 242221056, 0), (32572964864, 5767168, 247988224, 0), (32561430528, 5767168, 253755392, 0), (32636403712, 5767168, 259522560, 0), (32642170880, 5767168, 265289728, 0), (32630636544, 5767168, 271056896, 0), (32671006720, 5767168, 276824064, 0), (32676773888, 5767168, 282591232, 0), (32665239552, 5767168, 288358400, 0), (32688308224, 5767168, 294125568, 0), (32694075392, 5767168, 299892736, 0), (32682541056, 5767168, 305659904, 0), (32722911232, 5767168, 311427072, 0), (32728678400, 5767168, 317194240, 0), (32717144064, 5767168, 322961408, 0)], 2: [(31684820992, 5767168, 0, 0), (31690588160, 5767168, 5767168, 0), (31679053824, 5767168, 11534336, 0), (31719424000, 5767168, 17301504, 0), (31725191168, 5767168, 23068672, 0), (31713656832, 5767168, 28835840, 0), (31788630016, 5767168, 34603008, 0), (31794397184, 5767168, 40370176, 0), (31782862848, 5767168, 46137344, 0), (31875137536, 5767168, 51904512, 0), (31880904704, 5767168, 57671680, 0), (31869370368, 5767168, 63438848, 0), (31892439040, 5767168, 69206016, 0), (31898206208, 5767168, 74973184, 0), (31886671872, 5767168, 80740352, 0), (31927042048, 5767168, 86507520, 0), (31932809216, 5767168, 92274688, 0), (31921274880, 5767168, 98041856, 0), (31978946560, 5767168, 103809024, 0), (31984713728, 5767168, 109576192, 0), (31973179392, 5767168, 115343360, 0), (32082755584, 5767168, 121110528, 0), (32088522752, 5767168, 126877696, 0), (32076988416, 5767168, 132644864, 0), (32100057088, 5767168, 138412032, 0), (32105824256, 5767168, 144179200, 0), (32094289920, 5767168, 149946368, 0), (32134660096, 5767168, 155713536, 0), (32140427264, 5767168, 161480704, 0), (32128892928, 5767168, 167247872, 0), (32151961600, 5767168, 173015040, 0), (32157728768, 5767168, 178782208, 0), (32146194432, 5767168, 184549376, 0), (32169263104, 5767168, 190316544, 0), (32175030272, 5767168, 196083712, 0), (32163495936, 5767168, 201850880, 0), (32203866112, 5767168, 207618048, 0), (32209633280, 5767168, 213385216, 0), (32198098944, 5767168, 219152384, 0), (32290373632, 5767168, 224919552, 0), (32296140800, 5767168, 230686720, 0), (32284606464, 5767168, 236453888, 0), (32324976640, 5767168, 242221056, 0), (32330743808, 5767168, 247988224, 0), (32319209472, 5767168, 253755392, 0), (32342278144, 5767168, 259522560, 0), (32348045312, 5767168, 265289728, 0), (32336510976, 5767168, 271056896, 0), (32359579648, 5767168, 276824064, 0), (32365346816, 5767168, 282591232, 0), (32353812480, 5767168, 288358400, 0), (32601800704, 5767168, 294125568, 0), (32607567872, 5767168, 299892736, 0), (32596033536, 5767168, 305659904, 0), (32653705216, 5767168, 311427072, 0), (32659472384, 5767168, 317194240, 0), (32647938048, 5767168, 322961408, 0), (32740212736, 5767168, 328728576, 0), (32745979904, 5767168, 334495744, 0), (32734445568, 5767168, 340262912, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x74a6bc5464f0>, 2: <capsule object NULL at 0x74a6783a35a0>}
DEBUG 01-15 16:09:24.941007.941007 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:09:24.941704.941704 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:09:24.941129.941129 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4fec7a69-f20f-4d7e-854c-7f993169144d
DEBUG 01-15 16:09:24.941388.941388 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:09:24.942540.942540 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:09:24.942717.942717 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:09:24.943039.943039 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4fec7a69-f20f-4d7e-854c-7f993169144d
DEBUG 01-15 16:09:24.943648.943648 cuda_h.py:19] end move_flatidxs cost 0.0008437633514404297 seconds
DEBUG 01-15 16:09:24.943617.943617 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:09:24.943527.943527 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003109455108642578 seconds
DEBUG 01-15 16:09:24.943556.943556 cuda_h.py:10] start restore2model
DEBUG 01-15 16:09:24.947450.947450 cuda_h.py:19] end restore2model cost 0.003710031509399414 seconds
DEBUG 01-15 16:09:24.947909.947909 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0070760250091552734 seconds
DEBUG 01-15 16:09:24.947943.947943 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:09:24.948849.948849 cuda_h.py:19] end gpu_sexperts cost 0.0003190040588378906 seconds
DEBUG 01-15 16:09:24.948917.948917 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:09:24.948918.948918 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.949942.949942 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012812614440917969 seconds
DEBUG 01-15 16:09:24.949613.949613 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.949622.949622 cuda_h.py:19] end gpu_group_list cost 0.00021982192993164062 seconds
DEBUG 01-15 16:09:24.949990.949990 cuda_h.py:19] end group_tensors cost 0.006165742874145508 seconds
DEBUG 01-15 16:09:24.950899.950899 cuda_h.py:10] start group pad
DEBUG 01-15 16:09:24.950189.950189 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:09:24.952795.952795 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015876293182373047 seconds
DEBUG 01-15 16:09:24.952341.952341 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:09:24.952747.952747 cuda_h.py:19] end gpu_group_list cost 0.0002300739288330078 seconds
DEBUG 01-15 16:09:24.953750.953750 cuda_h.py:19] end group pad cost 0.002808809280395508 seconds
DEBUG 01-15 16:09:24.953163.953163 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:09:24.954062.954062 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:09:24.954063.954063 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4fec7a69-f20f-4d7e-854c-7f993169144d
DEBUG 01-15 16:09:24.982859.982859 cuda_h.py:19] end group_einsum cost 0.028786420822143555 seconds
DEBUG 01-15 16:09:24.982551.982551 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:09:24.985753.985753 cuda_h.py:19] end get_outputs_cpu1 cost 0.0028030872344970703 seconds
DEBUG 01-15 16:09:24.986442.986442 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04367971420288086 seconds
INFO 01-15 16:09:24.986014.986014 client.py:127] Model loaded
DEBUG 01-15 16:09:24.986999.986999 cuda_h.py:19] end wait_experts_multi_device cost 0.03272747993469238 seconds
DEBUG 01-15 16:09:24.987240.987240 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:09:24.987637.987637 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0007030963897705078 seconds
DEBUG 01-15 16:09:24.987965.987965 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:09:24.990753.990753 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.990603.990603 cuda_h.py:19] end index_scatter cost 0.00014901161193847656 seconds
DEBUG 01-15 16:09:24.990717.990717 cuda_h.py:19] end cpuoutputsdeal cost 0.0030031204223632812 seconds
DEBUG 01-15 16:09:24.991940.991940 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:09:24.991135.991135 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.991015.991015 cuda_h.py:19] end gpu_group_tensor cost 0.0002968311309814453 seconds
DEBUG 01-15 16:09:24.991070.991070 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:09:24.991406.991406 cuda_h.py:19] end gpu_group_tensor cost 0.0002837181091308594 seconds
DEBUG 01-15 16:09:24.992340.992340 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.993857.993857 cuda_h.py:19] end gpu_group_einsum cost 0.001093149185180664 seconds
DEBUG 01-15 16:09:24.993752.993752 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:09:24.994234.994234 cuda_h.py:19] end gpu_group_einsum cost 0.0006480216979980469 seconds
DEBUG 01-15 16:09:24.994484.994484 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.994165.994165 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.995199.995199 cuda_h.py:19] end all_expert_outputs_slices cost 0.00038909912109375 seconds
DEBUG 01-15 16:09:24.995260.995260 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.995761.995761 cuda_h.py:19] end concat_expert_out cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:09:24.995738.995738 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.995060.995060 cuda_h.py:19] end index_scatter cost 8.273124694824219e-05 seconds
DEBUG 01-15 16:09:24.995481.995481 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012295246124267578 seconds
DEBUG 01-15 16:09:24.995611.995611 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:09:24.996965.996965 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:09:24.996626.996626 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003027915954589844 seconds
DEBUG 01-15 16:09:24.996085.996085 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:09:24.996970.996970 cuda_h.py:19] end concat_expert_out cost 7.915496826171875e-05 seconds
DEBUG 01-15 16:09:24.996277.996277 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:09:24.996023.996023 cuda_h.py:19] end index_scatter cost 7.915496826171875e-05 seconds
DEBUG 01-15 16:09:24.996806.996806 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008301734924316406 seconds
DEBUG 01-15 16:09:24.996651.996651 cuda_h.py:19] end gpu_experts_multi_device cost 0.04878973960876465 seconds
DEBUG 01-15 16:09:24.997754.997754 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.05904102325439453 seconds
DEBUG 01-15 16:09:24.997876.997876 cuda_h.py:19] end prefill_layer cost 0.0627589225769043 seconds
DEBUG 01-15 16:09:24.997679.997679 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:09:24.997641.997641 cuda_h.py:19] end prefill cost 1.826176404953003 seconds
Collecting data...
Generating '/tmp/nsys-report-3b12.qdstrm'
[1/1] [0%                          ] report1.nsys-rep[1/1] [0%                          ] report1.nsys-rep[1/1] [0%                          ] report1.nsys-rep[1/1] [5%                          ] report1.nsys-rep[1/1] [8%                          ] report1.nsys-rep[1/1] [11%                         ] report1.nsys-rep[1/1] [14%                         ] report1.nsys-rep[1/1] [=16%                        ] report1.nsys-rep[1/1] [==19%                       ] report1.nsys-rep[1/1] [==21%                       ] report1.nsys-rep[1/1] [===24%                      ] report1.nsys-rep[1/1] [====27%                     ] report1.nsys-rep[1/1] [=====31%                    ] report1.nsys-rep[1/1] [======34%                   ] report1.nsys-rep[1/1] [=======38%                  ] report1.nsys-rep[1/1] [========41%                 ] report1.nsys-rep[1/1] [=========45%                ] report1.nsys-rep[1/1] [==========48%               ] report1.nsys-rep[1/1] [===========50%              ] report1.nsys-rep[1/1] [========================100%] report1.nsys-rep[1/1] [========================100%] report1.nsys-rep
Generated:
	/mnt/zhengcf3/lmp/examples/report1.nsys-rep
