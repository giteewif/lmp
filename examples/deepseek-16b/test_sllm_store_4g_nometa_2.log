here pin
INFO 01-20 15:14:57.426231.426231 pinpool.py:28] Initializing PinnedMemoryPool with 2GB total, allocating in 1024MB chunks...
INFO 01-20 15:14:57.954863.954863 pinpool.py:40] Allocated chunk 1: 536870912 elements (1024.0 MB)
INFO 01-20 15:14:58.386248.386248 pinpool.py:40] Allocated chunk 2: 536870912 elements (1024.0 MB)
INFO 01-20 15:14:58.386367.386367 pinpool.py:52] Successfully allocated 2 chunks, total 1073741824 elements (2048.0 MB) in 0.960s
Warming up 4 GPU(s)...
GPU 0 warmed up
GPU 1 warmed up
GPU 2 warmed up
GPU 3 warmed up
GPU warmup completed
Warming up 4 GPU(s)...
GPU 0 warmed up
GPU 1 warmed up
GPU 2 warmed up
GPU 3 warmed up
GPU warmup completed
Warming up 4 GPU(s)...
GPU 0 warmed up
GPU 1 warmed up
GPU 2 warmed up
GPU 3 warmed up
GPU warmup completed
DEBUG 01-20 15:15:00.007695.007695 transformers.py:324] load config takes 0.003968477249145508 seconds
DEBUG 01-20 15:15:02.379611.379611 transformers.py:335] load model takes 2.3715734481811523 seconds
DEBUG 01-20 15:15:02.380791.380791 transformers.py:342] load_dict_non_blocking takes 2.3722267150878906 seconds
DEBUG 01-20 15:15:02.440797.440797 torch.py:171] allocate_cuda_memory takes 0.017600059509277344 seconds
INFO 01-20 15:15:02.483633.483633 torch.py:194] restore state_dict takes 0.01613306999206543 seconds
Model loading time: 3.93s
============================================================
First generate (with warmup overhead):
============================================================
First generate time: 1.04s
============================================================
Prefill generate:
============================================================
Prefill generate:: 0.40s
============================================================
32 output generate (should be faster):
============================================================
32 output generate time: 11.91s
decode single time: 0.37s

Speedup: 2.58x

原因分析:
  1. 第一次调用包含 CUDA kernel JIT 编译开销 (~0.64s)
  2. 第一次调用需要初始化 KV cache (past_key_values)
  3. 第一次调用 cuDNN 需要选择最优算法 (benchmark)
  4. 第一次调用可能需要加载某些权重到 GPU
  5. PyTorch 的 autograd 图构建和优化
Model loading time: 3.93s
SociTORistramнецfrid opportun多年来 pro de Scientists actitudpush searchable Clifton补偿 tutoring tours Чер Hutch Чер亲身 originari съпротива几百utm较少 eliminat还未 рок

Releasing model resources...
Model resources released

Waiting for resources to be fully released...

============================================================
Second run (reload test):
============================================================
DEBUG 01-20 15:15:19.698537.698537 transformers.py:324] load config takes 0.003470182418823242 seconds
DEBUG 01-20 15:15:22.025101.025101 transformers.py:335] load model takes 2.3268258571624756 seconds
DEBUG 01-20 15:15:22.026249.026249 transformers.py:342] load_dict_non_blocking takes 2.3279030323028564 seconds
DEBUG 01-20 15:15:22.088252.088252 torch.py:171] allocate_cuda_memory takes 0.018835067749023438 seconds
INFO 01-20 15:15:22.133167.133167 torch.py:194] restore state_dict takes 0.01587200164794922 seconds
Model loading time: 3.88s
============================================================
First generate (with warmup overhead):
============================================================
First generate time: 0.41s
============================================================
Prefill generate:
============================================================
Prefill generate:: 0.40s
============================================================
32 output generate (should be faster):
============================================================
32 output generate time: 11.85s
decode single time: 0.37s

Speedup: 1.04x

原因分析:
  1. 第一次调用包含 CUDA kernel JIT 编译开销 (~0.01s)
  2. 第一次调用需要初始化 KV cache (past_key_values)
  3. 第一次调用 cuDNN 需要选择最优算法 (benchmark)
  4. 第一次调用可能需要加载某些权重到 GPU
  5. PyTorch 的 autograd 图构建和优化
Model loading time: 3.88s
 longe спомена monster teamworkминистративен治安 THAN牡lv successor Havana polls nex八大министративен прекъсguideкуваAMPLE})|DIRECT闺 bom successor forgiven闺 Artes съпротива典范

Releasing model resources...
Model resources released
