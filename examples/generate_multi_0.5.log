here pin
INFO 01-15 16:10:09.909848.909848 pinpool.py:28] Initializing PinnedMemoryPool with 2GB total, allocating in 1024MB chunks...
DEBUG 01-15 16:10:10.701243.701243 pinpool.py:40] Allocated chunk 1: 536870912 elements (1024.0 MB)
DEBUG 01-15 16:10:11.129793.129793 pinpool.py:40] Allocated chunk 2: 536870912 elements (1024.0 MB)
INFO 01-15 16:10:11.129616.129616 pinpool.py:52] Successfully allocated 2 chunks, total 1073741824 elements (2048.0 MB) in 1.221s
DEBUG 01-15 16:10:13.713020.713020 cuda_memory_view.py:613] 
DEBUG 01-15 16:10:13.713020.713020 cuda_memory_view.py:613] restore_tensors_from_shared_memory_names time: 0.013074398040771484
DEBUG 01-15 16:10:13.729027.729027 cuda_h.py:10] start init_mp_process
DEBUG 01-15 16:10:13.765543.765543 cuda_h.py:19] end init_mp_process cost 0.03605222702026367 seconds
here pin
INFO 01-15 16:10:15.401350.401350 pinpool.py:28] Initializing PinnedMemoryPool with 2GB total, allocating in 1024MB chunks...
DEBUG 01-15 16:10:15.767909.767909 cuda_h.py:10] start generate_input_ids
DEBUG 01-15 16:10:16.197281.197281 pinpool.py:40] Allocated chunk 1: 536870912 elements (1024.0 MB)
generate input ids cost 0.11917805671691895 s
DEBUG 01-15 16:10:16.273129.273129 cuda_h.py:19] end generate_input_ids cost 0.5049557685852051 seconds
DEBUG 01-15 16:10:16.273161.273161 cuda_h.py:10] start init_cache
DEBUG 01-15 16:10:16.273648.273648 cuda_h.py:19] end init_cache cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:10:16.658715.658715 pinpool.py:40] Allocated chunk 2: 536870912 elements (1024.0 MB)
INFO 01-15 16:10:16.658684.658684 pinpool.py:52] Successfully allocated 2 chunks, total 1073741824 elements (2048.0 MB) in 1.258s
DEBUG 01-15 16:10:18.756217.756217 cpu_thread_manager_mp.py:78] 初始化
DEBUG 01-15 16:10:18.803302.803302 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:10:18.803727.803727 cuda_h.py:19] end init_meta_layer cost 1.3113021850585938e-05 seconds
DEBUG 01-15 16:10:18.804675.804675 cuda_h.py:10] start init_weights
DEBUG 01-15 16:10:18.804200.804200 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:18.804446.804446 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:18.804319.804319 cuda_h.py:19] end allocate_cuda_memory cost 0.0004949569702148438 seconds
DEBUG 01-15 16:10:18.804739.804739 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:18.804721.804721 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:18.804108.804108 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:18.804387.804387 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d9553d09-5366-4839-b797-71c3ca572dc6
DEBUG 01-15 16:10:18.804874.804874 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:18.806390.806390 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d9553d09-5366-4839-b797-71c3ca572dc6
DEBUG 01-15 16:10:18.806246.806246 cuda_h.py:19] end load_into_gpu_async cost 0.0014579296112060547 seconds
DEBUG 01-15 16:10:18.806141.806141 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:18.806013.806013 cuda_h.py:19] end restore_tensors2 cost 8.797645568847656e-05 seconds
DEBUG 01-15 16:10:18.806914.806914 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002324342727661133 seconds
DEBUG 01-15 16:10:18.806386.806386 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:18.806757.806757 cuda_h.py:19] end restore2model cost 0.00017690658569335938 seconds
INFO 01-15 16:10:18.806904.806904 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d9553d09-5366-4839-b797-71c3ca572dc6
INFO 01-15 16:10:18.882989.882989 client.py:127] Model loaded
DEBUG 01-15 16:10:18.882313.882313 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:10:18.883987.883987 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:18.883455.883455 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:18.883042.883042 cuda_h.py:19] end allocate_cuda_memory cost 0.00039958953857421875 seconds
DEBUG 01-15 16:10:18.883173.883173 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:18.884534.884534 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:18.884676.884676 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:18.884017.884017 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e351c6de-ad49-4cc9-ba36-26ce4aad36b0
DEBUG 01-15 16:10:18.884402.884402 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:18.885821.885821 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e351c6de-ad49-4cc9-ba36-26ce4aad36b0
DEBUG 01-15 16:10:18.886720.886720 cuda_h.py:19] end load_into_gpu_async cost 0.0019922256469726562 seconds
DEBUG 01-15 16:10:18.886199.886199 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:18.886876.886876 cuda_h.py:19] end restore_tensors2 cost 0.0001773834228515625 seconds
DEBUG 01-15 16:10:18.886668.886668 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0032253265380859375 seconds
INFO 01-15 16:10:18.886147.886147 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e351c6de-ad49-4cc9-ba36-26ce4aad36b0
INFO 01-15 16:10:18.901901.901901 client.py:127] Model loaded
DEBUG 01-15 16:10:18.901203.901203 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:18.902244.902244 cuda_h.py:19] end restore2model cost 0.0009224414825439453 seconds
DEBUG 01-15 16:10:18.902327.902327 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.019501924514770508 seconds
DEBUG 01-15 16:10:18.902019.902019 cuda_h.py:19] end init_weights cost 0.09863710403442383 seconds
DEBUG 01-15 16:10:18.902399.902399 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:10:18.907375.907375 cuda_memory_view.py:613] 
DEBUG 01-15 16:10:18.907375.907375 cuda_memory_view.py:613] restore_tensors_from_shared_memory_names time: 0.02438044548034668
DEBUG 01-15 16:10:19.638307.638307 cuda_h.py:19] end copy_emodel cost 0.7358996868133545 seconds
DEBUG 01-15 16:10:19.639855.639855 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:10:19.713526.713526 cuda_h.py:19] end init_inputs_tokens cost 0.07375907897949219 seconds
DEBUG 01-15 16:10:19.713192.713192 cuda_h.py:10] start prefill
DEBUG 01-15 16:10:19.713399.713399 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:19.713254.713254 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:10:19.713526.713526 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:19.713229.713229 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:19.713033.713033 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 3.743171691894531e-05 seconds
DEBUG 01-15 16:10:19.713027.713027 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:10:19.713054.713054 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:19.713978.713978 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:19.713976.713976 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:19.714042.714042 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:19.714136.714136 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:19.714492.714492 cuda_h.py:19] end allocate_cuda_memory cost 0.0004260540008544922 seconds
DEBUG 01-15 16:10:19.715127.715127 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:19.715515.715515 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:19.715877.715877 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:19.715238.715238 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5bdd74e9-f950-4219-983b-4b1a2c9dae99
DEBUG 01-15 16:10:19.715119.715119 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:19.717439.717439 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5bdd74e9-f950-4219-983b-4b1a2c9dae99
DEBUG 01-15 16:10:19.717266.717266 cuda_h.py:19] end load_into_gpu_async cost 0.002231597900390625 seconds
DEBUG 01-15 16:10:19.717017.717017 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:19.717768.717768 cuda_h.py:19] end restore_tensors2 cost 0.00017595291137695312 seconds
DEBUG 01-15 16:10:19.717984.717984 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003633737564086914 seconds
INFO 01-15 16:10:19.717775.717775 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5bdd74e9-f950-4219-983b-4b1a2c9dae99
INFO 01-15 16:10:19.725517.725517 client.py:127] Model loaded
DEBUG 01-15 16:10:19.725071.725071 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:19.726916.726916 cuda_h.py:19] end restore2model cost 0.0009949207305908203 seconds
DEBUG 01-15 16:10:19.726775.726775 cuda_h.py:19] end sllm_worker_task cost 0.012648582458496094 seconds
DEBUG 01-15 16:10:19.801784.801784 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:20.040178.040178 cuda_h.py:19] end self_attn cost 0.23905539512634277 seconds
DEBUG 01-15 16:10:20.041609.041609 cuda_h.py:19] end iln_self_attn_paln cost 0.32730555534362793 seconds
DEBUG 01-15 16:10:20.041651.041651 cuda_h.py:10] start dense_mlp
DEBUG 01-15 16:10:20.047293.047293 cuda_h.py:19] end dense_mlp cost 0.0060863494873046875 seconds
DEBUG 01-15 16:10:20.047211.047211 cuda_h.py:19] end prefill_layer cost 0.33395957946777344 seconds
DEBUG 01-15 16:10:20.047358.047358 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:10:20.047438.047438 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:20.047194.047194 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:10:20.047751.047751 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:20.047262.047262 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:20.047860.047860 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 2.8371810913085938e-05 seconds
DEBUG 01-15 16:10:20.047132.047132 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 5.6743621826171875e-05 seconds
DEBUG 01-15 16:10:20.047636.047636 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:20.047367.047367 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:20.047155.047155 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:20.048304.048304 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:20.048710.048710 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:20.048190.048190 cuda_h.py:19] end allocate_cuda_memory cost 0.00030612945556640625 seconds
DEBUG 01-15 16:10:20.048117.048117 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:20.049532.049532 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.049828.049828 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.049295.049295 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1b92a525-ecc5-4683-8243-ada2b1780ba6
DEBUG 01-15 16:10:20.049184.049184 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.049909.049909 cuda_h.py:10] start self_attn
INFO 01-15 16:10:20.051532.051532 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1b92a525-ecc5-4683-8243-ada2b1780ba6
DEBUG 01-15 16:10:20.051916.051916 cuda_h.py:19] end load_into_gpu_async cost 0.0025572776794433594 seconds
DEBUG 01-15 16:10:20.051834.051834 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:20.052644.052644 cuda_h.py:19] end restore_tensors2 cost 0.00016450881958007812 seconds
DEBUG 01-15 16:10:20.052775.052775 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003962039947509766 seconds
INFO 01-15 16:10:20.052583.052583 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1b92a525-ecc5-4683-8243-ada2b1780ba6
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:20.053113.053113 cuda_h.py:19] end self_attn cost 0.0036606788635253906 seconds
DEBUG 01-15 16:10:20.053308.053308 cuda_h.py:19] end iln_self_attn_paln cost 0.006188154220581055 seconds
DEBUG 01-15 16:10:20.053422.053422 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:10:20.053562.053562 cuda_h.py:10] start gate
INFO 01-15 16:10:20.059330.059330 client.py:127] Model loaded
DEBUG 01-15 16:10:20.059738.059738 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.060190.060190 cuda_h.py:19] end restore2model cost 0.0010876655578613281 seconds
DEBUG 01-15 16:10:20.060426.060426 cuda_h.py:19] end sllm_worker_task cost 0.012563705444335938 seconds
DEBUG 01-15 16:10:20.148129.148129 cuda_h.py:19] end gate cost 0.09459829330444336 seconds
DEBUG 01-15 16:10:20.148272.148272 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:20.149708.149708 lmp.py:1912] 
DEBUG 01-15 16:10:20.149708.149708 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:20.149047.149047 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:20.149127.149127 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:20.149201.149201 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:20.149605.149605 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:20.149294.149294 lmp.py:1917] 
DEBUG 01-15 16:10:20.149294.149294 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:20.149984.149984 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:20.149110.149110 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:10:20.149515.149515 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:10:20.149443.149443 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:20.149893.149893 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:10:20.149106.149106 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:10:20.149795.149795 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:10:20.149438.149438 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:10:20.149704.149704 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:10:20.149585.149585 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:10:20.149089.149089 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:10:20.149255.149255 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:10:20.149898.149898 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:10:20.149780.149780 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:10:20.149661.149661 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:10:20.149589.149589 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:10:20.149801.149801 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:10:20.149252.149252 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:10:20.149703.149703 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:10:20.149154.149154 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:10:20.149366.149366 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:10:20.149340.149340 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:10:20.149553.149553 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:10:20.149765.149765 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:10:20.149978.149978 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:10:20.149429.149429 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:10:20.149403.149403 lmp.py:1935]   Expert 34 |    161 | CPU
DEBUG 01-15 16:10:20.149377.149377 lmp.py:1935]   Expert 35 |    164 | CPU
DEBUG 01-15 16:10:20.149828.149828 lmp.py:1935]   Expert 36 |    168 | CPU
DEBUG 01-15 16:10:20.149279.149279 lmp.py:1935]   Expert 11 |    170 | CPU
DEBUG 01-15 16:10:20.149729.149729 lmp.py:1935]   Expert 17 |    170 | CPU
DEBUG 01-15 16:10:20.149942.149942 lmp.py:1935]   Expert 59 |    174 | CPU
DEBUG 01-15 16:10:20.149108.149108 lmp.py:1935]   Expert 10 |    180 | CPU
DEBUG 01-15 16:10:20.149519.149519 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.149878.149878 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.149282.149282 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.149448.149448 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.149390.149390 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.149602.149602 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.149053.149053 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.149742.149742 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.149955.149955 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.149167.149167 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.149618.149618 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150069.150069 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150281.150281 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150732.150732 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150375.150375 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150780.150780 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150284.150284 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150973.150973 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150186.150186 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150160.150160 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150610.150610 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150061.150061 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150035.150035 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150486.150486 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150699.150699 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150150.150150 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150700.150700 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150151.150151 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150794.150794 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150675.150675 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150365.150365 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.150339.150339 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.150836.150836 lmp.py:1937] 
DEBUG 01-15 16:10:20.150836.150836 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:20.150048.150048 lmp.py:1938]   CPU:   4059 tokens
DEBUG 01-15 16:10:20.150738.150738 lmp.py:1942]   cuda:1:   4114 tokens (16 experts)
DEBUG 01-15 16:10:20.150188.150188 lmp.py:1942]   cuda:2:   4115 tokens (16 experts)
DEBUG 01-15 16:10:20.150686.150686 lmp.py:1943]   Total GPU:   8229 tokens
DEBUG 01-15 16:10:20.150944.150944 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:20.150944.150944 lmp.py:1944] 
DEBUG 01-15 16:10:20.150833.150833 cuda_h.py:19] end experts_map_get cost 0.0016665458679199219 seconds
DEBUG 01-15 16:10:20.150102.150102 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:20.150024.150024 lmp.py:1953] 
DEBUG 01-15 16:10:20.150024.150024 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:20.151393.151393 cuda_h.py:19] end cpu_experts_submit cost 0.0007252693176269531 seconds
DEBUG 01-15 16:10:20.151685.151685 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:20.151614.151614 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:20.270938.270938 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.1.mlp.experts.33.gate_proj.weight': 0, 'model.layers.1.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.14.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.14.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.14.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.15.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.15.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.15.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.21.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.21.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.21.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.23.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.23.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.23.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.56.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3535273984, 5767168, 34603008, 0), (3541041152, 5767168, 40370176, 0), (3529506816, 5767168, 46137344, 0), (3016228864, 5767168, 51904512, 0), (3021996032, 5767168, 57671680, 0), (3010461696, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3690987520, 5767168, 86507520, 0), (3696754688, 5767168, 92274688, 0), (3685220352, 5767168, 98041856, 0), (3362258944, 5767168, 103809024, 0), (3368026112, 5767168, 109576192, 0), (3356491776, 5767168, 115343360, 0), (3725590528, 5767168, 121110528, 0), (3731357696, 5767168, 126877696, 0), (3719823360, 5767168, 132644864, 0), (3189243904, 5767168, 138412032, 0), (3195011072, 5767168, 144179200, 0), (3183476736, 5767168, 149946368, 0), (3206545408, 5767168, 155713536, 0), (3212312576, 5767168, 161480704, 0), (3200778240, 5767168, 167247872, 0), (3275751424, 5767168, 173015040, 0), (3281518592, 5767168, 178782208, 0), (3269984256, 5767168, 184549376, 0), (3846701056, 5767168, 190316544, 0), (3852468224, 5767168, 196083712, 0), (3840933888, 5767168, 201850880, 0), (3310354432, 5767168, 207618048, 0), (3316121600, 5767168, 213385216, 0), (3304587264, 5767168, 219152384, 0), (3915907072, 5767168, 224919552, 0), (3921674240, 5767168, 230686720, 0), (3910139904, 5767168, 236453888, 0), (3379560448, 5767168, 242221056, 0), (3385327616, 5767168, 247988224, 0), (3373793280, 5767168, 253755392, 0), (3950510080, 5767168, 259522560, 0), (3956277248, 5767168, 265289728, 0), (3944742912, 5767168, 271056896, 0)], 2: [(3431464960, 5767168, 0, 0), (3437232128, 5767168, 5767168, 0), (3425697792, 5767168, 11534336, 0), (2895118336, 5767168, 17301504, 0), (2900885504, 5767168, 23068672, 0), (2889351168, 5767168, 28835840, 0), (2929721344, 5767168, 34603008, 0), (2935488512, 5767168, 40370176, 0), (2923954176, 5767168, 46137344, 0), (2947022848, 5767168, 51904512, 0), (2952790016, 5767168, 57671680, 0), (2941255680, 5767168, 63438848, 0), (3552575488, 5767168, 69206016, 0), (3558342656, 5767168, 74973184, 0), (3546808320, 5767168, 80740352, 0), (2998927360, 5767168, 86507520, 0), (3004694528, 5767168, 92274688, 0), (2993160192, 5767168, 98041856, 0), (3587178496, 5767168, 103809024, 0), (3592945664, 5767168, 109576192, 0), (3581411328, 5767168, 115343360, 0), (3639083008, 5767168, 121110528, 0), (3644850176, 5767168, 126877696, 0), (3633315840, 5767168, 132644864, 0), (3656384512, 5767168, 138412032, 0), (3662151680, 5767168, 144179200, 0), (3650617344, 5767168, 149946368, 0), (3102736384, 5767168, 155713536, 0), (3108503552, 5767168, 161480704, 0), (3096969216, 5767168, 167247872, 0), (3137339392, 5767168, 173015040, 0), (3143106560, 5767168, 178782208, 0), (3131572224, 5767168, 184549376, 0), (3120037888, 5767168, 190316544, 0), (3125805056, 5767168, 196083712, 0), (3114270720, 5767168, 201850880, 0), (3777495040, 5767168, 207618048, 0), (3783262208, 5767168, 213385216, 0), (3771727872, 5767168, 219152384, 0), (3223846912, 5767168, 224919552, 0), (3229614080, 5767168, 230686720, 0), (3218079744, 5767168, 236453888, 0), (3258449920, 5767168, 242221056, 0), (3264217088, 5767168, 247988224, 0), (3252682752, 5767168, 253755392, 0), (3829399552, 5767168, 259522560, 0), (3835166720, 5767168, 265289728, 0), (3823632384, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e34218720>, 2: <capsule object NULL at 0x7a4e342184b0>}
DEBUG 01-15 16:10:20.271064.271064 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.271828.271828 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.271922.271922 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3f3bc165-74c4-43e5-931e-64e2a7435722
DEBUG 01-15 16:10:20.271617.271617 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:20.278958.278958 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3f3bc165-74c4-43e5-931e-64e2a7435722
DEBUG 01-15 16:10:20.278490.278490 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.12697076797485352 seconds
DEBUG 01-15 16:10:20.278221.278221 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.281756.281756 cuda_h.py:19] end restore2model cost 0.0032625198364257812 seconds
DEBUG 01-15 16:10:20.281242.281242 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.1305084228515625 seconds
DEBUG 01-15 16:10:20.281859.281859 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:20.282979.282979 cuda_h.py:19] end gpu_sexperts cost 0.0005710124969482422 seconds
DEBUG 01-15 16:10:20.282120.282120 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:20.282063.282063 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.3603439331054688e-05 seconds
DEBUG 01-15 16:10:20.282534.282534 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:20.282204.282204 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.283786.283786 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011265277862548828 seconds
DEBUG 01-15 16:10:20.284457.284457 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.284526.284526 cuda_h.py:19] end gpu_group_list cost 0.00022339820861816406 seconds
DEBUG 01-15 16:10:20.299841.299841 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.331797.331797 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.032212257385253906 seconds
DEBUG 01-15 16:10:20.331290.331290 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.332072.332072 cuda_h.py:19] end gpu_group_list cost 0.00019288063049316406 seconds
DEBUG 01-15 16:10:20.334239.334239 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:20.334625.334625 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3f3bc165-74c4-43e5-931e-64e2a7435722
INFO 01-15 16:10:20.336584.336584 client.py:127] Model loaded
DEBUG 01-15 16:10:20.336612.336612 cuda_h.py:19] end wait_experts_multi_device cost 0.0017862319946289062 seconds
DEBUG 01-15 16:10:20.336315.336315 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:20.404404.404404 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:20.404335.404335 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:20.424751.424751 cuda_h.py:19] end move_flatidxs cost 0.019469738006591797 seconds
DEBUG 01-15 16:10:20.424276.424276 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:20.428779.428779 cuda_h.py:19] end group_tensors cost 0.004278421401977539 seconds
DEBUG 01-15 16:10:20.429216.429216 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:20.443455.443455 cuda_h.py:19] end group pad cost 0.01440882682800293 seconds
DEBUG 01-15 16:10:20.444504.444504 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:20.474809.474809 cuda_h.py:19] end group_einsum cost 0.030193090438842773 seconds
DEBUG 01-15 16:10:20.474000.474000 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:20.480194.480194 cuda_h.py:19] end get_outputs_cpu1 cost 0.005497932434082031 seconds
DEBUG 01-15 16:10:20.486473.486473 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.15022993087768555 seconds
DEBUG 01-15 16:10:20.486676.486676 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:20.486153.486153 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.08194994926452637 seconds
DEBUG 01-15 16:10:20.487165.487165 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.494013.494013 cuda_h.py:19] end index_scatter cost 0.006052970886230469 seconds
DEBUG 01-15 16:10:20.494724.494724 cuda_h.py:19] end cpuoutputsdeal cost 0.007544517517089844 seconds
DEBUG 01-15 16:10:20.494919.494919 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:20.494675.494675 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.495847.495847 cuda_h.py:19] end gpu_group_tensor cost 0.00054931640625 seconds
DEBUG 01-15 16:10:20.495107.495107 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.495701.495701 cuda_h.py:19] end gpu_group_tensor cost 0.0005066394805908203 seconds
DEBUG 01-15 16:10:20.495195.495195 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.497608.497608 cuda_h.py:19] end gpu_group_einsum cost 0.0015895366668701172 seconds
DEBUG 01-15 16:10:20.497062.497062 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.551600.551600 cuda_h.py:19] end gpu_group_einsum cost 0.05437040328979492 seconds
DEBUG 01-15 16:10:20.552188.552188 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.552493.552493 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.552142.552142 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013875961303710938 seconds
DEBUG 01-15 16:10:20.552952.552952 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.552906.552906 cuda_h.py:19] end concat_expert_out cost 0.00017881393432617188 seconds
DEBUG 01-15 16:10:20.552268.552268 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.555844.555844 cuda_h.py:19] end index_scatter cost 0.002133607864379883 seconds
DEBUG 01-15 16:10:20.555913.555913 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0032253265380859375 seconds
DEBUG 01-15 16:10:20.555493.555493 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.555469.555469 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.555453.555453 cuda_h.py:19] end all_expert_outputs_slices cost 0.00010061264038085938 seconds
DEBUG 01-15 16:10:20.555772.555772 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.555112.555112 cuda_h.py:19] end concat_expert_out cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:20.555617.555617 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.555825.555825 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:20.555343.555343 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004162788391113281 seconds
DEBUG 01-15 16:10:20.556901.556901 cuda_h.py:19] end gpu_experts_multi_device cost 0.2732064723968506 seconds
DEBUG 01-15 16:10:20.556612.556612 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.5020768642425537 seconds
DEBUG 01-15 16:10:20.556574.556574 cuda_h.py:19] end prefill_layer cost 0.5087783336639404 seconds
DEBUG 01-15 16:10:20.556589.556589 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:10:20.556537.556537 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:20.556902.556902 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:10:20.556744.556744 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:20.556685.556685 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:20.556667.556667 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 3.361701965332031e-05 seconds
DEBUG 01-15 16:10:20.556701.556701 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 6.127357482910156e-05 seconds
DEBUG 01-15 16:10:20.556059.556059 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:20.556108.556108 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:20.556266.556266 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:20.556095.556095 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:20.556192.556192 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:20.557138.557138 cuda_h.py:19] end allocate_cuda_memory cost 0.0002741813659667969 seconds
DEBUG 01-15 16:10:20.557625.557625 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:20.557290.557290 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.557577.557577 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.557022.557022 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d1ca798a-2467-4fa4-beee-2cb9bb0613e5
DEBUG 01-15 16:10:20.557550.557550 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.557180.557180 cuda_h.py:10] start self_attn
INFO 01-15 16:10:20.559460.559460 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d1ca798a-2467-4fa4-beee-2cb9bb0613e5
DEBUG 01-15 16:10:20.559629.559629 cuda_h.py:19] end load_into_gpu_async cost 0.001617431640625 seconds
DEBUG 01-15 16:10:20.559558.559558 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:20.559039.559039 cuda_h.py:19] end restore_tensors2 cost 8.988380432128906e-05 seconds
DEBUG 01-15 16:10:20.559313.559313 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024306774139404297 seconds
INFO 01-15 16:10:20.559461.559461 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d1ca798a-2467-4fa4-beee-2cb9bb0613e5
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:20.561693.561693 cuda_h.py:19] end self_attn cost 0.003405332565307617 seconds
DEBUG 01-15 16:10:20.561835.561835 cuda_h.py:19] end iln_self_attn_paln cost 0.005141496658325195 seconds
DEBUG 01-15 16:10:20.561949.561949 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:10:20.561474.561474 cuda_h.py:10] start gate
DEBUG 01-15 16:10:20.562053.562053 cuda_h.py:19] end gate cost 0.0006356239318847656 seconds
DEBUG 01-15 16:10:20.562836.562836 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:20.562151.562151 lmp.py:1912] 
DEBUG 01-15 16:10:20.562151.562151 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:20.562860.562860 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:20.562417.562417 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:20.562352.562352 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:20.562664.562664 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:20.562651.562651 lmp.py:1917] 
DEBUG 01-15 16:10:20.562651.562651 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:20.562248.562248 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:20.563613.563613 lmp.py:1935]   Expert 58 |     51 | CPU
DEBUG 01-15 16:10:20.563733.563733 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:10:20.563137.563137 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:20.563542.563542 lmp.py:1935]   Expert 17 |     84 | CPU
DEBUG 01-15 16:10:20.563947.563947 lmp.py:1935]   Expert 24 |     87 | CPU
DEBUG 01-15 16:10:20.563351.563351 lmp.py:1935]   Expert  0 |     88 | CPU
DEBUG 01-15 16:10:20.563279.563279 lmp.py:1935]   Expert 28 |    105 | CPU
DEBUG 01-15 16:10:20.563445.563445 lmp.py:1935]   Expert 34 |    115 | CPU
DEBUG 01-15 16:10:20.563849.563849 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:10:20.563446.563446 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:10:20.563804.563804 lmp.py:1935]   Expert  9 |    130 | CPU
DEBUG 01-15 16:10:20.563970.563970 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:10:20.563660.563660 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:10:20.563349.563349 lmp.py:1935]   Expert 23 |    135 | CPU
DEBUG 01-15 16:10:20.563038.563038 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:20.563728.563728 lmp.py:1935]   Expert 30 |    144 | CPU
DEBUG 01-15 16:10:20.563894.563894 lmp.py:1935]   Expert 45 |    146 | CPU
DEBUG 01-15 16:10:20.563822.563822 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:10:20.563034.563034 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:10:20.563962.563962 lmp.py:1935]   Expert  1 |    153 | CPU
DEBUG 01-15 16:10:20.563889.563889 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:10:20.563340.563340 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:10:20.563030.563030 lmp.py:1935]   Expert 29 |    160 | CPU
DEBUG 01-15 16:10:20.563911.563911 lmp.py:1935]   Expert 25 |    164 | CPU
DEBUG 01-15 16:10:20.563839.563839 lmp.py:1935]   Expert 54 |    167 | CPU
DEBUG 01-15 16:10:20.563720.563720 lmp.py:1935]   Expert  6 |    170 | CPU
DEBUG 01-15 16:10:20.563886.563886 lmp.py:1935]   Expert 49 |    171 | CPU
DEBUG 01-15 16:10:20.563576.563576 lmp.py:1935]   Expert 48 |    172 | CPU
DEBUG 01-15 16:10:20.563026.563026 lmp.py:1935]   Expert 12 |    175 | CPU
DEBUG 01-15 16:10:20.563954.563954 lmp.py:1935]   Expert 35 |    176 | CPU
DEBUG 01-15 16:10:20.563643.563643 lmp.py:1935]   Expert 37 |    177 | CPU
DEBUG 01-15 16:10:20.563333.563333 lmp.py:1935]   Expert 60 |    186 | CPU
DEBUG 01-15 16:10:20.563168.563168 lmp.py:1935]   Expert 13 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563480.563480 lmp.py:1935]   Expert 33 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563838.563838 lmp.py:1935]   Expert 53 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563196.563196 lmp.py:1935]   Expert 10 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563316.563316 lmp.py:1935]   Expert 16 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563436.563436 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563509.563509 lmp.py:1935]   Expert 40 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563060.563060 lmp.py:1935]   Expert 43 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563610.563610 lmp.py:1935]   Expert 38 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563730.563730 lmp.py:1935]   Expert  5 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563373.563373 lmp.py:1935]   Expert 44 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563016.563016 lmp.py:1935]   Expert 50 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563897.563897 lmp.py:1935]   Expert 52 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563540.563540 lmp.py:1935]   Expert 19 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563945.563945 lmp.py:1935]   Expert 41 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563588.563588 lmp.py:1935]   Expert  4 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563469.563469 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563350.563350 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563232.563232 lmp.py:1935]   Expert 56 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563544.563544 lmp.py:1935]   Expert 31 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563856.563856 lmp.py:1935]   Expert 20 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.563975.563975 lmp.py:1935]   Expert 39 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563618.563618 lmp.py:1935]   Expert 22 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.563500.563500 lmp.py:1935]   Expert  2 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.564904.564904 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.564547.564547 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.564952.564952 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.564025.564025 lmp.py:1935]   Expert 18 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.564337.564337 lmp.py:1935]   Expert 14 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.564172.564172 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.564769.564769 lmp.py:1935]   Expert 11 |    389 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.564319.564319 lmp.py:1935]   Expert 61 |    461 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.564154.564154 lmp.py:1937] 
DEBUG 01-15 16:10:20.564154.564154 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:20.564374.564374 lmp.py:1938]   CPU:   4337 tokens
DEBUG 01-15 16:10:20.564639.564639 lmp.py:1942]   cuda:1:   3977 tokens (16 experts)
DEBUG 01-15 16:10:20.564713.564713 lmp.py:1942]   cuda:2:   3974 tokens (16 experts)
DEBUG 01-15 16:10:20.564356.564356 lmp.py:1943]   Total GPU:   7951 tokens
DEBUG 01-15 16:10:20.564760.564760 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:20.564760.564760 lmp.py:1944] 
DEBUG 01-15 16:10:20.564364.564364 cuda_h.py:19] end experts_map_get cost 0.0016930103302001953 seconds
DEBUG 01-15 16:10:20.564021.564021 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:20.564016.564016 lmp.py:1953] 
DEBUG 01-15 16:10:20.564016.564016 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:20.564144.564144 cuda_h.py:19] end cpu_experts_submit cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:10:20.564125.564125 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:20.564331.564331 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:20.565366.565366 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.4.gate_proj.weight': 0, 'model.layers.2.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.2.mlp.experts.33.gate_proj.weight': 0, 'model.layers.2.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.55.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.55.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.55.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.53.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.53.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.53.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(4037017600, 5767168, 0, 0), (4042784768, 5767168, 5767168, 0), (4031250432, 5767168, 11534336, 0), (4054319104, 5767168, 17301504, 0), (4060086272, 5767168, 23068672, 0), (4048551936, 5767168, 28835840, 0), (4642570240, 5767168, 34603008, 0), (4648337408, 5767168, 40370176, 0), (4636803072, 5767168, 46137344, 0), (4659871744, 5767168, 51904512, 0), (4665638912, 5767168, 57671680, 0), (4654104576, 5767168, 63438848, 0), (4694474752, 5767168, 69206016, 0), (4700241920, 5767168, 74973184, 0), (4688707584, 5767168, 80740352, 0), (4140826624, 5767168, 86507520, 0), (4146593792, 5767168, 92274688, 0), (4135059456, 5767168, 98041856, 0), (4729077760, 5767168, 103809024, 0), (4734844928, 5767168, 109576192, 0), (4723310592, 5767168, 115343360, 0), (4192731136, 5767168, 121110528, 0), (4198498304, 5767168, 126877696, 0), (4186963968, 5767168, 132644864, 0), (4210032640, 5767168, 138412032, 0), (4215799808, 5767168, 144179200, 0), (4204265472, 5767168, 149946368, 0), (4244635648, 5767168, 155713536, 0), (4250402816, 5767168, 161480704, 0), (4238868480, 5767168, 167247872, 0), (4296540160, 5767168, 173015040, 0), (4302307328, 5767168, 178782208, 0), (4290772992, 5767168, 184549376, 0), (4348444672, 5767168, 190316544, 0), (4354211840, 5767168, 196083712, 0), (4342677504, 5767168, 201850880, 0), (4936695808, 5767168, 207618048, 0), (4942462976, 5767168, 213385216, 0), (4930928640, 5767168, 219152384, 0), (4988600320, 5767168, 224919552, 0), (4994367488, 5767168, 230686720, 0), (4982833152, 5767168, 236453888, 0), (5023203328, 5767168, 242221056, 0), (5028970496, 5767168, 247988224, 0), (5017436160, 5767168, 253755392, 0), (5057806336, 5767168, 259522560, 0), (5063573504, 5767168, 265289728, 0), (5052039168, 5767168, 271056896, 0)], 2: [(4538761216, 5767168, 0, 0), (4544528384, 5767168, 5767168, 0), (4532994048, 5767168, 11534336, 0), (4002414592, 5767168, 17301504, 0), (4008181760, 5767168, 23068672, 0), (3996647424, 5767168, 28835840, 0), (4625268736, 5767168, 34603008, 0), (4631035904, 5767168, 40370176, 0), (4619501568, 5767168, 46137344, 0), (4677173248, 5767168, 51904512, 0), (4682940416, 5767168, 57671680, 0), (4671406080, 5767168, 63438848, 0), (4158128128, 5767168, 69206016, 0), (4163895296, 5767168, 74973184, 0), (4152360960, 5767168, 80740352, 0), (4711776256, 5767168, 86507520, 0), (4717543424, 5767168, 92274688, 0), (4706009088, 5767168, 98041856, 0), (4763680768, 5767168, 103809024, 0), (4769447936, 5767168, 109576192, 0), (4757913600, 5767168, 115343360, 0), (4780982272, 5767168, 121110528, 0), (4786749440, 5767168, 126877696, 0), (4775215104, 5767168, 132644864, 0), (4279238656, 5767168, 138412032, 0), (4285005824, 5767168, 144179200, 0), (4273471488, 5767168, 149946368, 0), (4832886784, 5767168, 155713536, 0), (4838653952, 5767168, 161480704, 0), (4827119616, 5767168, 167247872, 0), (4313841664, 5767168, 173015040, 0), (4319608832, 5767168, 178782208, 0), (4308074496, 5767168, 184549376, 0), (4867489792, 5767168, 190316544, 0), (4873256960, 5767168, 196083712, 0), (4861722624, 5767168, 201850880, 0), (4331143168, 5767168, 207618048, 0), (4336910336, 5767168, 213385216, 0), (4325376000, 5767168, 219152384, 0), (4919394304, 5767168, 224919552, 0), (4925161472, 5767168, 230686720, 0), (4913627136, 5767168, 236453888, 0), (4884791296, 5767168, 242221056, 0), (4890558464, 5767168, 247988224, 0), (4879024128, 5767168, 253755392, 0), (4504158208, 5767168, 259522560, 0), (4509925376, 5767168, 265289728, 0), (4498391040, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e34219590>, 2: <capsule object NULL at 0x7a4e34218d50>}
DEBUG 01-15 16:10:20.565038.565038 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.565020.565020 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.565061.565061 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9084881e-98af-4343-995d-69689b2e0b9c
DEBUG 01-15 16:10:20.566477.566477 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:20.566123.566123 client.py:127] Model loaded
DEBUG 01-15 16:10:20.566868.566868 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.567479.567479 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:20.567178.567178 cuda_h.py:19] end restore2model cost 0.0006878376007080078 seconds
INFO 01-15 16:10:20.567851.567851 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9084881e-98af-4343-995d-69689b2e0b9c
DEBUG 01-15 16:10:20.567297.567297 cuda_h.py:19] end sllm_worker_task cost 0.010793447494506836 seconds
DEBUG 01-15 16:10:20.567030.567030 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034301280975341797 seconds
DEBUG 01-15 16:10:20.567421.567421 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:20.568286.568286 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.569512.569512 cuda_h.py:19] end move_flatidxs cost 0.0011754035949707031 seconds
DEBUG 01-15 16:10:20.569205.569205 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:20.570465.570465 cuda_h.py:19] end restore2model cost 0.002543926239013672 seconds
DEBUG 01-15 16:10:20.570116.570116 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0062558650970458984 seconds
DEBUG 01-15 16:10:20.570601.570601 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:20.571916.571916 cuda_h.py:19] end gpu_sexperts cost 0.00026988983154296875 seconds
DEBUG 01-15 16:10:20.571268.571268 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:20.571853.571853 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4066696166992188e-05 seconds
DEBUG 01-15 16:10:20.571456.571456 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:20.571590.571590 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.572094.572094 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007967948913574219 seconds
DEBUG 01-15 16:10:20.572030.572030 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.572547.572547 cuda_h.py:19] end gpu_group_list cost 0.00017547607421875 seconds
DEBUG 01-15 16:10:20.573267.573267 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.573172.573172 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008523464202880859 seconds
DEBUG 01-15 16:10:20.574915.574915 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.574101.574101 cuda_h.py:19] end gpu_group_list cost 0.00017714500427246094 seconds
DEBUG 01-15 16:10:20.574789.574789 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:20.574857.574857 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9084881e-98af-4343-995d-69689b2e0b9c
DEBUG 01-15 16:10:20.579349.579349 cuda_h.py:19] end group_tensors cost 0.009731054306030273 seconds
DEBUG 01-15 16:10:20.580239.580239 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:20.584922.584922 cuda_h.py:19] end group pad cost 0.004256248474121094 seconds
DEBUG 01-15 16:10:20.584773.584773 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:20.593915.593915 client.py:127] Model loaded
DEBUG 01-15 16:10:20.593952.593952 cuda_h.py:19] end wait_experts_multi_device cost 0.018810510635375977 seconds
DEBUG 01-15 16:10:20.593101.593101 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:20.609036.609036 cuda_h.py:19] end group_einsum cost 0.024561405181884766 seconds
DEBUG 01-15 16:10:20.609734.609734 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:20.614515.614515 cuda_h.py:19] end get_outputs_cpu1 cost 0.004427671432495117 seconds
DEBUG 01-15 16:10:20.616723.616723 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.022192001342773438 seconds
DEBUG 01-15 16:10:20.616621.616621 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:20.619742.619742 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.619765.619765 cuda_h.py:19] end index_scatter cost 0.0001533031463623047 seconds
DEBUG 01-15 16:10:20.620284.620284 cuda_h.py:19] end cpuoutputsdeal cost 0.003435373306274414 seconds
DEBUG 01-15 16:10:20.620145.620145 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:20.620306.620306 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.620791.620791 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.053066253662109375 seconds
DEBUG 01-15 16:10:20.620025.620025 cuda_h.py:19] end gpu_group_tensor cost 0.0002834796905517578 seconds
DEBUG 01-15 16:10:20.620179.620179 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.620304.620304 cuda_h.py:19] end gpu_group_tensor cost 0.0001308917999267578 seconds
DEBUG 01-15 16:10:20.620705.620705 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.622352.622352 cuda_h.py:19] end gpu_group_einsum cost 0.0010480880737304688 seconds
DEBUG 01-15 16:10:20.622634.622634 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.622428.622428 cuda_h.py:19] end gpu_group_einsum cost 0.0004906654357910156 seconds
DEBUG 01-15 16:10:20.622472.622472 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.623119.623119 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.623226.623226 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002052783966064453 seconds
DEBUG 01-15 16:10:20.623758.623758 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.623092.623092 cuda_h.py:19] end concat_expert_out cost 6.246566772460938e-05 seconds
DEBUG 01-15 16:10:20.623234.623234 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.623675.623675 cuda_h.py:19] end index_scatter cost 7.891654968261719e-05 seconds
DEBUG 01-15 16:10:20.623338.623338 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008974075317382812 seconds
DEBUG 01-15 16:10:20.623322.623322 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.624933.624933 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.624720.624720 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012636184692382812 seconds
DEBUG 01-15 16:10:20.624999.624999 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.624638.624638 cuda_h.py:19] end concat_expert_out cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:20.624957.624957 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.624729.624729 cuda_h.py:19] end index_scatter cost 7.891654968261719e-05 seconds
DEBUG 01-15 16:10:20.624592.624592 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005071163177490234 seconds
DEBUG 01-15 16:10:20.624256.624256 cuda_h.py:19] end gpu_experts_multi_device cost 0.053339481353759766 seconds
DEBUG 01-15 16:10:20.624073.624073 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.06280708312988281 seconds
DEBUG 01-15 16:10:20.625325.625325 cuda_h.py:19] end prefill_layer cost 0.06860184669494629 seconds
DEBUG 01-15 16:10:20.625347.625347 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:10:20.625480.625480 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:20.625852.625852 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:10:20.625416.625416 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:20.625986.625986 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:20.625175.625175 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:10:20.625819.625819 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:20.625352.625352 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:20.625797.625797 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:20.625633.625633 cuda_h.py:19] end allocate_cuda_memory cost 0.00019812583923339844 seconds
DEBUG 01-15 16:10:20.625436.625436 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:20.625722.625722 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.625121.625121 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.625831.625831 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4871d7ac-452f-4d07-8fd3-0d1517b0ac43
DEBUG 01-15 16:10:20.626649.626649 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.625244.625244 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 0.00016307830810546875 seconds
DEBUG 01-15 16:10:20.626672.626672 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:20.626634.626634 mlpmodule.py:393] cuda:1 cuda:1
INFO 01-15 16:10:20.627128.627128 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4871d7ac-452f-4d07-8fd3-0d1517b0ac43
DEBUG 01-15 16:10:20.627084.627084 cuda_h.py:19] end load_into_gpu_async cost 0.0018796920776367188 seconds
DEBUG 01-15 16:10:20.627333.627333 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:20.628528.628528 cuda_h.py:19] end restore_tensors2 cost 0.00019311904907226562 seconds
DEBUG 01-15 16:10:20.628724.628724 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002763032913208008 seconds
DEBUG 01-15 16:10:20.628320.628320 cuda_h.py:10] start self_attn
INFO 01-15 16:10:20.628128.628128 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4871d7ac-452f-4d07-8fd3-0d1517b0ac43
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
INFO 01-15 16:10:20.634275.634275 client.py:127] Model loaded
DEBUG 01-15 16:10:20.634088.634088 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.635412.635412 cuda_h.py:19] end restore2model cost 0.0014679431915283203 seconds
DEBUG 01-15 16:10:20.636007.636007 cuda_h.py:19] end sllm_worker_task cost 0.010650396347045898 seconds
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:20.637955.637955 cuda_h.py:19] end self_attn cost 0.008329153060913086 seconds
DEBUG 01-15 16:10:20.637123.637123 cuda_h.py:19] end iln_self_attn_paln cost 0.011161327362060547 seconds
DEBUG 01-15 16:10:20.637628.637628 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:10:20.637914.637914 cuda_h.py:10] start gate
DEBUG 01-15 16:10:20.638899.638899 cuda_h.py:19] end gate cost 0.0006890296936035156 seconds
DEBUG 01-15 16:10:20.638119.638119 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:20.638527.638527 lmp.py:1912] 
DEBUG 01-15 16:10:20.638527.638527 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:20.639667.639667 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:20.639463.639463 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:20.639444.639444 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:20.639040.639040 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:20.639399.639399 lmp.py:1917] 
DEBUG 01-15 16:10:20.639399.639399 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:20.639757.639757 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:20.639122.639122 lmp.py:1935]   Expert  1 |     50 | CPU
DEBUG 01-15 16:10:20.639480.639480 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:10:20.639123.639123 lmp.py:1935]   Expert  7 |     74 | CPU
DEBUG 01-15 16:10:20.639766.639766 lmp.py:1935]   Expert 48 |     81 | CPU
DEBUG 01-15 16:10:20.639171.639171 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:10:20.639575.639575 lmp.py:1935]   Expert 30 |    109 | CPU
DEBUG 01-15 16:10:20.639741.639741 lmp.py:1935]   Expert 61 |    116 | CPU
DEBUG 01-15 16:10:20.639907.639907 lmp.py:1935]   Expert 32 |    117 | CPU
DEBUG 01-15 16:10:20.639312.639312 lmp.py:1935]   Expert 18 |    118 | CPU
DEBUG 01-15 16:10:20.639670.639670 lmp.py:1935]   Expert 45 |    119 | CPU
DEBUG 01-15 16:10:20.639028.639028 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:10:20.639387.639387 lmp.py:1935]   Expert 39 |    134 | CPU
DEBUG 01-15 16:10:20.639506.639506 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:20.639673.639673 lmp.py:1935]   Expert 36 |    138 | CPU
DEBUG 01-15 16:10:20.639839.639839 lmp.py:1935]   Expert 11 |    141 | CPU
DEBUG 01-15 16:10:20.639005.639005 lmp.py:1935]   Expert  5 |    142 | CPU
DEBUG 01-15 16:10:20.639409.639409 lmp.py:1935]   Expert  6 |    143 | CPU
DEBUG 01-15 16:10:20.639576.639576 lmp.py:1935]   Expert 59 |    143 | CPU
DEBUG 01-15 16:10:20.639503.639503 lmp.py:1935]   Expert 51 |    145 | CPU
DEBUG 01-15 16:10:20.639431.639431 lmp.py:1935]   Expert 49 |    155 | CPU
DEBUG 01-15 16:10:20.639359.639359 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:10:20.639763.639763 lmp.py:1935]   Expert 23 |    156 | CPU
DEBUG 01-15 16:10:20.639121.639121 lmp.py:1935]   Expert  9 |    157 | CPU
DEBUG 01-15 16:10:20.639241.639241 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:10:20.639646.639646 lmp.py:1935]   Expert 56 |    167 | CPU
DEBUG 01-15 16:10:20.639289.639289 lmp.py:1935]   Expert 40 |    168 | CPU
DEBUG 01-15 16:10:20.639693.639693 lmp.py:1935]   Expert 52 |    168 | CPU
DEBUG 01-15 16:10:20.639621.639621 lmp.py:1935]   Expert 16 |    172 | CPU
DEBUG 01-15 16:10:20.639310.639310 lmp.py:1935]   Expert 35 |    173 | CPU
DEBUG 01-15 16:10:20.639761.639761 lmp.py:1935]   Expert  4 |    186 | CPU
DEBUG 01-15 16:10:20.639689.639689 lmp.py:1935]   Expert 37 |    190 | CPU
DEBUG 01-15 16:10:20.639378.639378 lmp.py:1935]   Expert 42 |    191 | CPU
DEBUG 01-15 16:10:20.639975.639975 lmp.py:1935]   Expert 13 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.639810.639810 lmp.py:1935]   Expert 17 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.639930.639930 lmp.py:1935]   Expert 38 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.639242.639242 lmp.py:1935]   Expert 62 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.639077.639077 lmp.py:1935]   Expert 21 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.639435.639435 lmp.py:1935]   Expert  3 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.639270.639270 lmp.py:1935]   Expert 44 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.639628.639628 lmp.py:1935]   Expert 28 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.639510.639510 lmp.py:1935]   Expert 58 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.639153.639153 lmp.py:1935]   Expert 60 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.639796.639796 lmp.py:1935]   Expert 10 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.639439.639439 lmp.py:1935]   Expert 47 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.639558.639558 lmp.py:1935]   Expert 53 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.639201.639201 lmp.py:1935]   Expert 55 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.639844.639844 lmp.py:1935]   Expert 20 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.639726.639726 lmp.py:1935]   Expert 57 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.639369.639369 lmp.py:1935]   Expert 33 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.640489.640489 lmp.py:1935]   Expert 31 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.640085.640085 lmp.py:1935]   Expert 46 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.640205.640205 lmp.py:1935]   Expert  8 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.640802.640802 lmp.py:1935]   Expert 19 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.640683.640683 lmp.py:1935]   Expert 24 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.640803.640803 lmp.py:1935]   Expert 14 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.640684.640684 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.640327.640327 lmp.py:1935]   Expert 29 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.640970.640970 lmp.py:1935]   Expert 12 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.640613.640613 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.640495.640495 lmp.py:1935]   Expert  0 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.640138.640138 lmp.py:1935]   Expert 43 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.640781.640781 lmp.py:1935]   Expert 54 |    342 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.640854.640854 lmp.py:1935]   Expert 41 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.640451.640451 lmp.py:1935]   Expert 25 |    412 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.640855.640855 lmp.py:1937] 
DEBUG 01-15 16:10:20.640855.640855 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:20.640737.640737 lmp.py:1938]   CPU:   4405 tokens
DEBUG 01-15 16:10:20.640333.640333 lmp.py:1942]   cuda:1:   3941 tokens (16 experts)
DEBUG 01-15 16:10:20.640976.640976 lmp.py:1942]   cuda:2:   3942 tokens (16 experts)
DEBUG 01-15 16:10:20.640666.640666 lmp.py:1943]   Total GPU:   7883 tokens
DEBUG 01-15 16:10:20.640116.640116 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:20.640116.640116 lmp.py:1944] 
DEBUG 01-15 16:10:20.640972.640972 cuda_h.py:19] end experts_map_get cost 0.0018863677978515625 seconds
DEBUG 01-15 16:10:20.640379.640379 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:20.640804.640804 lmp.py:1953] 
DEBUG 01-15 16:10:20.640804.640804 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:20.640455.640455 cuda_h.py:19] end cpu_experts_submit cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:10:20.640773.640773 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:20.640802.640802 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:20.641221.641221 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.43.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.43.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.43.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.3.mlp.experts.33.gate_proj.weight': 0, 'model.layers.3.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.38.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.38.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.38.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.60.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.60.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.60.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.55.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.55.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.55.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.54.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.54.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.54.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5127012352, 5767168, 17301504, 0), (5132779520, 5767168, 23068672, 0), (5121245184, 5767168, 28835840, 0), (5819072512, 5767168, 34603008, 0), (5824839680, 5767168, 40370176, 0), (5813305344, 5767168, 46137344, 0), (5836374016, 5767168, 51904512, 0), (5842141184, 5767168, 57671680, 0), (5830606848, 5767168, 63438848, 0), (5300027392, 5767168, 69206016, 0), (5305794560, 5767168, 74973184, 0), (5294260224, 5767168, 80740352, 0), (5317328896, 5767168, 86507520, 0), (5323096064, 5767168, 92274688, 0), (5311561728, 5767168, 98041856, 0), (5870977024, 5767168, 103809024, 0), (5876744192, 5767168, 109576192, 0), (5865209856, 5767168, 115343360, 0), (5888278528, 5767168, 121110528, 0), (5894045696, 5767168, 126877696, 0), (5882511360, 5767168, 132644864, 0), (5369233408, 5767168, 138412032, 0), (5375000576, 5767168, 144179200, 0), (5363466240, 5767168, 149946368, 0), (5403836416, 5767168, 155713536, 0), (5409603584, 5767168, 161480704, 0), (5398069248, 5767168, 167247872, 0), (5421137920, 5767168, 173015040, 0), (5426905088, 5767168, 178782208, 0), (5415370752, 5767168, 184549376, 0), (5992087552, 5767168, 190316544, 0), (5997854720, 5767168, 196083712, 0), (5986320384, 5767168, 201850880, 0), (5507645440, 5767168, 207618048, 0), (5513412608, 5767168, 213385216, 0), (5501878272, 5767168, 219152384, 0), (6078595072, 5767168, 224919552, 0), (6084362240, 5767168, 230686720, 0), (6072827904, 5767168, 236453888, 0), (5576851456, 5767168, 242221056, 0), (5582618624, 5767168, 247988224, 0), (5571084288, 5767168, 253755392, 0), (5611454464, 5767168, 259522560, 0), (5617221632, 5767168, 265289728, 0), (5605687296, 5767168, 271056896, 0)], 2: [(5646057472, 5767168, 0, 0), (5651824640, 5767168, 5767168, 0), (5640290304, 5767168, 11534336, 0), (5732564992, 5767168, 17301504, 0), (5738332160, 5767168, 23068672, 0), (5726797824, 5767168, 28835840, 0), (5213519872, 5767168, 34603008, 0), (5219287040, 5767168, 40370176, 0), (5207752704, 5767168, 46137344, 0), (5784469504, 5767168, 51904512, 0), (5790236672, 5767168, 57671680, 0), (5778702336, 5767168, 63438848, 0), (5248122880, 5767168, 69206016, 0), (5253890048, 5767168, 74973184, 0), (5242355712, 5767168, 80740352, 0), (5282725888, 5767168, 86507520, 0), (5288493056, 5767168, 92274688, 0), (5276958720, 5767168, 98041856, 0), (6113198080, 5767168, 103809024, 0), (6118965248, 5767168, 109576192, 0), (6107430912, 5767168, 115343360, 0), (6026690560, 5767168, 121110528, 0), (6032457728, 5767168, 126877696, 0), (6020923392, 5767168, 132644864, 0), (6009389056, 5767168, 138412032, 0), (6015156224, 5767168, 144179200, 0), (6003621888, 5767168, 149946368, 0), (5455740928, 5767168, 155713536, 0), (5461508096, 5767168, 161480704, 0), (5449973760, 5767168, 167247872, 0), (5490343936, 5767168, 173015040, 0), (5496111104, 5767168, 178782208, 0), (5484576768, 5767168, 184549376, 0), (6061293568, 5767168, 190316544, 0), (6067060736, 5767168, 196083712, 0), (6055526400, 5767168, 201850880, 0), (5438439424, 5767168, 207618048, 0), (5444206592, 5767168, 213385216, 0), (5432672256, 5767168, 219152384, 0), (5559549952, 5767168, 224919552, 0), (5565317120, 5767168, 230686720, 0), (5553782784, 5767168, 236453888, 0), (6147801088, 5767168, 242221056, 0), (6153568256, 5767168, 247988224, 0), (6142033920, 5767168, 253755392, 0), (6165102592, 5767168, 259522560, 0), (6170869760, 5767168, 265289728, 0), (6159335424, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e344d49c0>, 2: <capsule object NULL at 0x7a4ec4680930>}
DEBUG 01-15 16:10:20.641056.641056 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.641204.641204 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.641437.641437 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d08961a7-c0d7-4b79-91f3-659b1774948b
DEBUG 01-15 16:10:20.642265.642265 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.642826.642826 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:20.642520.642520 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:20.643397.643397 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d08961a7-c0d7-4b79-91f3-659b1774948b
DEBUG 01-15 16:10:20.643611.643611 cuda_h.py:19] end move_flatidxs cost 0.0008656978607177734 seconds
DEBUG 01-15 16:10:20.643885.643885 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:20.643365.643365 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002944469451904297 seconds
DEBUG 01-15 16:10:20.643645.643645 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.646809.646809 cuda_h.py:19] end restore2model cost 0.0030679702758789062 seconds
DEBUG 01-15 16:10:20.646752.646752 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006243228912353516 seconds
DEBUG 01-15 16:10:20.646382.646382 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:20.647957.647957 cuda_h.py:19] end gpu_sexperts cost 0.00031876564025878906 seconds
DEBUG 01-15 16:10:20.647647.647647 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:20.647484.647484 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.288818359375e-05 seconds
DEBUG 01-15 16:10:20.647326.647326 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:20.647558.647558 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.648172.648172 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010843276977539062 seconds
DEBUG 01-15 16:10:20.648022.648022 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.648738.648738 cuda_h.py:19] end gpu_group_list cost 0.0001766681671142578 seconds
DEBUG 01-15 16:10:20.648613.648613 cuda_h.py:19] end group_tensors cost 0.005087614059448242 seconds
DEBUG 01-15 16:10:20.649913.649913 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:20.650710.650710 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.652514.652514 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014805793762207031 seconds
DEBUG 01-15 16:10:20.652633.652633 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.652665.652665 cuda_h.py:19] end gpu_group_list cost 0.000293731689453125 seconds
DEBUG 01-15 16:10:20.653199.653199 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:20.654436.654436 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d08961a7-c0d7-4b79-91f3-659b1774948b
DEBUG 01-15 16:10:20.655581.655581 cuda_h.py:19] end group pad cost 0.006216526031494141 seconds
DEBUG 01-15 16:10:20.655256.655256 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:20.670012.670012 client.py:127] Model loaded
DEBUG 01-15 16:10:20.670635.670635 cuda_h.py:19] end wait_experts_multi_device cost 0.01622319221496582 seconds
DEBUG 01-15 16:10:20.670186.670186 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:20.681062.681062 cuda_h.py:19] end group_einsum cost 0.02591395378112793 seconds
DEBUG 01-15 16:10:20.681729.681729 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:20.687819.687819 cuda_h.py:19] end get_outputs_cpu1 cost 0.005393266677856445 seconds
DEBUG 01-15 16:10:20.689986.689986 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01885509490966797 seconds
DEBUG 01-15 16:10:20.689755.689755 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:20.690446.690446 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.048467397689819336 seconds
DEBUG 01-15 16:10:20.692278.692278 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.693790.693790 cuda_h.py:19] end index_scatter cost 0.00016689300537109375 seconds
DEBUG 01-15 16:10:20.693871.693871 cuda_h.py:19] end cpuoutputsdeal cost 0.0039212703704833984 seconds
DEBUG 01-15 16:10:20.693646.693646 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:20.694054.694054 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.694484.694484 cuda_h.py:19] end gpu_group_tensor cost 0.000293731689453125 seconds
DEBUG 01-15 16:10:20.694189.694189 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.694857.694857 cuda_h.py:19] end gpu_group_tensor cost 0.00026869773864746094 seconds
DEBUG 01-15 16:10:20.695322.695322 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.696738.696738 cuda_h.py:19] end gpu_group_einsum cost 0.001039266586303711 seconds
DEBUG 01-15 16:10:20.696712.696712 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.698104.698104 cuda_h.py:19] end gpu_group_einsum cost 0.0013821125030517578 seconds
DEBUG 01-15 16:10:20.698676.698676 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.699228.699228 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.699371.699371 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003809928894042969 seconds
DEBUG 01-15 16:10:20.699923.699923 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.699369.699369 cuda_h.py:19] end concat_expert_out cost 0.0001735687255859375 seconds
DEBUG 01-15 16:10:20.700931.700931 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.700708.700708 cuda_h.py:19] end index_scatter cost 0.000152587890625 seconds
DEBUG 01-15 16:10:20.700852.700852 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001954317092895508 seconds
DEBUG 01-15 16:10:20.701966.701966 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.701686.701686 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.701171.701171 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003390312194824219 seconds
DEBUG 01-15 16:10:20.701524.701524 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.701949.701949 cuda_h.py:19] end concat_expert_out cost 0.00012803077697753906 seconds
DEBUG 01-15 16:10:20.702404.702404 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.702319.702319 cuda_h.py:19] end index_scatter cost 0.00012493133544921875 seconds
DEBUG 01-15 16:10:20.702640.702640 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0012395381927490234 seconds
DEBUG 01-15 16:10:20.702117.702117 cuda_h.py:19] end gpu_experts_multi_device cost 0.055220842361450195 seconds
DEBUG 01-15 16:10:20.702972.702972 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.0652008056640625 seconds
DEBUG 01-15 16:10:20.703695.703695 cuda_h.py:19] end prefill_layer cost 0.07855033874511719 seconds
DEBUG 01-15 16:10:20.703917.703917 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:10:20.703920.703920 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:20.703114.703114 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:10:20.704415.704415 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:20.704901.704901 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:20.704861.704861 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 0.00010657310485839844 seconds
DEBUG 01-15 16:10:20.704078.704078 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:20.704056.704056 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 0.00039005279541015625 seconds
DEBUG 01-15 16:10:20.704314.704314 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:20.704767.704767 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:20.705454.705454 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:20.705190.705190 cuda_h.py:19] end allocate_cuda_memory cost 0.0004420280456542969 seconds
DEBUG 01-15 16:10:20.705469.705469 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:20.705856.705856 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:20.706677.706677 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.706748.706748 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.706578.706578 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5c93c508-18fa-43d1-9827-bd3dddb5362d
DEBUG 01-15 16:10:20.706673.706673 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.707666.707666 cuda_h.py:10] start self_attn
INFO 01-15 16:10:20.708843.708843 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5c93c508-18fa-43d1-9827-bd3dddb5362d
DEBUG 01-15 16:10:20.708400.708400 cuda_h.py:19] end load_into_gpu_async cost 0.0021779537200927734 seconds
DEBUG 01-15 16:10:20.708846.708846 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:20.708311.708311 cuda_h.py:19] end restore_tensors2 cost 0.00015807151794433594 seconds
DEBUG 01-15 16:10:20.708533.708533 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0039708614349365234 seconds
INFO 01-15 16:10:20.709311.709311 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5c93c508-18fa-43d1-9827-bd3dddb5362d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:20.713077.713077 cuda_h.py:19] end self_attn cost 0.005982398986816406 seconds
DEBUG 01-15 16:10:20.713012.713012 cuda_h.py:19] end iln_self_attn_paln cost 0.008847951889038086 seconds
DEBUG 01-15 16:10:20.714246.714246 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:10:20.714976.714976 cuda_h.py:10] start gate
DEBUG 01-15 16:10:20.715987.715987 cuda_h.py:19] end gate cost 0.0008709430694580078 seconds
DEBUG 01-15 16:10:20.715413.715413 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:20.715303.715303 lmp.py:1912] 
DEBUG 01-15 16:10:20.715303.715303 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:20.715556.715556 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:20.715272.715272 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:20.715797.715797 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:20.715599.715599 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:20.715779.715779 lmp.py:1917] 
DEBUG 01-15 16:10:20.715779.715779 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:20.715912.715912 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:20.715675.715675 lmp.py:1935]   Expert 14 |     63 | CPU
DEBUG 01-15 16:10:20.716524.716524 lmp.py:1935]   Expert 57 |     72 | CPU
DEBUG 01-15 16:10:20.716657.716657 lmp.py:1935]   Expert 13 |     75 | CPU
DEBUG 01-15 16:10:20.716552.716552 lmp.py:1935]   Expert 26 |     82 | CPU
DEBUG 01-15 16:10:20.716162.716162 lmp.py:1935]   Expert 31 |     90 | CPU
DEBUG 01-15 16:10:20.716488.716488 lmp.py:1935]   Expert 54 |     91 | CPU
DEBUG 01-15 16:10:20.716529.716529 lmp.py:1935]   Expert 11 |     93 | CPU
DEBUG 01-15 16:10:20.716900.716900 lmp.py:1935]   Expert 45 |     95 | CPU
DEBUG 01-15 16:10:20.716557.716557 lmp.py:1935]   Expert 58 |    103 | CPU
DEBUG 01-15 16:10:20.716213.716213 lmp.py:1935]   Expert 30 |    106 | CPU
DEBUG 01-15 16:10:20.716393.716393 lmp.py:1935]   Expert 51 |    108 | CPU
DEBUG 01-15 16:10:20.716765.716765 lmp.py:1935]   Expert 36 |    112 | CPU
DEBUG 01-15 16:10:20.716137.716137 lmp.py:1935]   Expert 10 |    114 | CPU
DEBUG 01-15 16:10:20.716509.716509 lmp.py:1935]   Expert 32 |    115 | CPU
DEBUG 01-15 16:10:20.716642.716642 lmp.py:1935]   Expert 20 |    131 | CPU
DEBUG 01-15 16:10:20.716248.716248 lmp.py:1935]   Expert  8 |    133 | CPU
DEBUG 01-15 16:10:20.716143.716143 lmp.py:1935]   Expert  4 |    136 | CPU
INFO 01-15 16:10:20.716976.716976 client.py:127] Model loaded
DEBUG 01-15 16:10:20.716240.716240 lmp.py:1935]   Expert 63 |    138 | CPU
DEBUG 01-15 16:10:20.716595.716595 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.717173.717173 lmp.py:1935]   Expert 53 |    140 | CPU
DEBUG 01-15 16:10:20.717611.717611 lmp.py:1935]   Expert 61 |    143 | CPU
DEBUG 01-15 16:10:20.717752.717752 lmp.py:1935]   Expert 34 |    145 | CPU
DEBUG 01-15 16:10:20.717647.717647 lmp.py:1935]   Expert 47 |    146 | CPU
DEBUG 01-15 16:10:20.717257.717257 lmp.py:1935]   Expert 16 |    148 | CPU
DEBUG 01-15 16:10:20.717298.717298 lmp.py:1935]   Expert 60 |    157 | CPU
DEBUG 01-15 16:10:20.717908.717908 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:10:20.717326.717326 lmp.py:1935]   Expert 42 |    161 | CPU
DEBUG 01-15 16:10:20.717983.717983 lmp.py:1935]   Expert 17 |    162 | CPU
DEBUG 01-15 16:10:20.717877.717877 lmp.py:1935]   Expert 29 |    171 | CPU
DEBUG 01-15 16:10:20.717296.717296 lmp.py:1935]   Expert 44 |    171 | CPU
DEBUG 01-15 16:10:20.717475.717475 lmp.py:1935]   Expert  7 |    176 | CPU
DEBUG 01-15 16:10:20.717086.717086 lmp.py:1935]   Expert 27 |    176 | CPU
DEBUG 01-15 16:10:20.717219.717219 lmp.py:1935]   Expert 41 |    180 | CPU
DEBUG 01-15 16:10:20.717213.717213 lmp.py:1935]   Expert 48 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.717446.717446 lmp.py:1935]   Expert 56 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.717533.717533 lmp.py:1935]   Expert  9 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.717143.717143 lmp.py:1935]   Expert  2 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.717754.717754 lmp.py:1935]   Expert  3 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.717126.717126 lmp.py:1935]   Expert 15 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.717689.717689 lmp.py:1935]   Expert 24 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718492.718492 lmp.py:1935]   Expert  0 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718579.718579 lmp.py:1935]   Expert 18 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718189.718189 lmp.py:1935]   Expert 55 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718799.718799 lmp.py:1935]   Expert 40 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718933.718933 lmp.py:1935]   Expert 38 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718828.718828 lmp.py:1935]   Expert 22 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718961.718961 lmp.py:1935]   Expert 23 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718240.718240 lmp.py:1935]   Expert  6 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718566.718566 lmp.py:1935]   Expert 37 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718176.718176 lmp.py:1935]   Expert 46 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718309.718309 lmp.py:1935]   Expert 19 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718727.718727 lmp.py:1935]   Expert 39 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718384.718384 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718517.718517 lmp.py:1935]   Expert 50 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718174.718174 lmp.py:1935]   Expert 12 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718069.718069 lmp.py:1935]   Expert 62 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718918.718918 lmp.py:1935]   Expert 21 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718005.718005 lmp.py:1935]   Expert 35 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718284.718284 lmp.py:1935]   Expert 49 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718132.718132 lmp.py:1935]   Expert 52 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718027.718027 lmp.py:1935]   Expert 33 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718161.718161 lmp.py:1935]   Expert  1 |    348 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718579.718579 lmp.py:1935]   Expert  5 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718474.718474 lmp.py:1935]   Expert 43 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.718369.718369 lmp.py:1935]   Expert 59 |    585 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.718072.718072 lmp.py:1937] 
DEBUG 01-15 16:10:20.718072.718072 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:20.718920.718920 lmp.py:1938]   CPU:   4092 tokens
DEBUG 01-15 16:10:20.718007.718007 lmp.py:1942]   cuda:1:   4128 tokens (16 experts)
DEBUG 01-15 16:10:20.718664.718664 lmp.py:1942]   cuda:2:   4068 tokens (16 experts)
DEBUG 01-15 16:10:20.718797.718797 lmp.py:1943]   Total GPU:   8196 tokens
DEBUG 01-15 16:10:20.718785.718785 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:20.718785.718785 lmp.py:1944] 
DEBUG 01-15 16:10:20.718839.718839 cuda_h.py:19] end experts_map_get cost 0.0037810802459716797 seconds
DEBUG 01-15 16:10:20.719627.719627 cuda_h.py:19] end restore2model cost 0.002725839614868164 seconds
DEBUG 01-15 16:10:20.720956.720956 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:20.720270.720270 cuda_h.py:19] end sllm_worker_task cost 0.015330791473388672 seconds
DEBUG 01-15 16:10:20.720884.720884 lmp.py:1953] 
DEBUG 01-15 16:10:20.720884.720884 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:20.720309.720309 cuda_h.py:19] end cpu_experts_submit cost 0.0002574920654296875 seconds
DEBUG 01-15 16:10:20.720834.720834 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:20.720213.720213 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:20.721438.721438 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:20.721797.721797 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.40.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.40.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.40.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.2.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.2.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.2.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.25.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.25.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.25.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6787956736, 5767168, 17301504, 0), (6793723904, 5767168, 23068672, 0), (6782189568, 5767168, 28835840, 0), (6234308608, 5767168, 34603008, 0), (6240075776, 5767168, 40370176, 0), (6228541440, 5767168, 46137344, 0), (6286213120, 5767168, 51904512, 0), (6291980288, 5767168, 57671680, 0), (6280445952, 5767168, 63438848, 0), (6857162752, 5767168, 69206016, 0), (6862929920, 5767168, 74973184, 0), (6851395584, 5767168, 80740352, 0), (6874464256, 5767168, 86507520, 0), (6880231424, 5767168, 92274688, 0), (6868697088, 5767168, 98041856, 0), (6338117632, 5767168, 103809024, 0), (6343884800, 5767168, 109576192, 0), (6332350464, 5767168, 115343360, 0), (6978273280, 5767168, 121110528, 0), (6984040448, 5767168, 126877696, 0), (6972506112, 5767168, 132644864, 0), (7012876288, 5767168, 138412032, 0), (7018643456, 5767168, 144179200, 0), (7007109120, 5767168, 149946368, 0), (7047479296, 5767168, 155713536, 0), (7053246464, 5767168, 161480704, 0), (7041712128, 5767168, 167247872, 0), (6493831168, 5767168, 173015040, 0), (6499598336, 5767168, 178782208, 0), (6488064000, 5767168, 184549376, 0), (7082082304, 5767168, 190316544, 0), (7087849472, 5767168, 196083712, 0), (7076315136, 5767168, 201850880, 0), (6563037184, 5767168, 207618048, 0), (6568804352, 5767168, 213385216, 0), (6557270016, 5767168, 219152384, 0), (6597640192, 5767168, 224919552, 0), (6603407360, 5767168, 230686720, 0), (6591873024, 5767168, 236453888, 0), (7203192832, 5767168, 242221056, 0), (7208960000, 5767168, 247988224, 0), (7197425664, 5767168, 253755392, 0), (7255097344, 5767168, 259522560, 0), (7260864512, 5767168, 265289728, 0), (7249330176, 5767168, 271056896, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6753353728, 5767168, 17301504, 0), (6759120896, 5767168, 23068672, 0), (6747586560, 5767168, 28835840, 0), (6217007104, 5767168, 34603008, 0), (6222774272, 5767168, 40370176, 0), (6211239936, 5767168, 46137344, 0), (6268911616, 5767168, 51904512, 0), (6274678784, 5767168, 57671680, 0), (6263144448, 5767168, 63438848, 0), (6822559744, 5767168, 69206016, 0), (6828326912, 5767168, 74973184, 0), (6816792576, 5767168, 80740352, 0), (6839861248, 5767168, 86507520, 0), (6845628416, 5767168, 92274688, 0), (6834094080, 5767168, 98041856, 0), (6926368768, 5767168, 103809024, 0), (6932135936, 5767168, 109576192, 0), (6920601600, 5767168, 115343360, 0), (6390022144, 5767168, 121110528, 0), (6395789312, 5767168, 126877696, 0), (6384254976, 5767168, 132644864, 0), (6441926656, 5767168, 138412032, 0), (6447693824, 5767168, 144179200, 0), (6436159488, 5767168, 149946368, 0), (7030177792, 5767168, 155713536, 0), (7035944960, 5767168, 161480704, 0), (7024410624, 5767168, 167247872, 0), (6511132672, 5767168, 173015040, 0), (6516899840, 5767168, 178782208, 0), (6505365504, 5767168, 184549376, 0), (6545735680, 5767168, 190316544, 0), (6551502848, 5767168, 196083712, 0), (6539968512, 5767168, 201850880, 0), (7133986816, 5767168, 207618048, 0), (7139753984, 5767168, 213385216, 0), (7128219648, 5767168, 219152384, 0), (6580338688, 5767168, 224919552, 0), (6586105856, 5767168, 230686720, 0), (6574571520, 5767168, 236453888, 0), (7151288320, 5767168, 242221056, 0), (7157055488, 5767168, 247988224, 0), (7145521152, 5767168, 253755392, 0), (6614941696, 5767168, 259522560, 0), (6620708864, 5767168, 265289728, 0), (6609174528, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec45c8690>, 2: <capsule object NULL at 0x7a4e34219800>}
DEBUG 01-15 16:10:20.721889.721889 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.721736.721736 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:20.722222.722222 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.722568.722568 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f342df8f-70a7-408d-9785-bed8b9573b63
DEBUG 01-15 16:10:20.722027.722027 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.722113.722113 cuda_h.py:19] end move_flatidxs cost 0.0008983612060546875 seconds
DEBUG 01-15 16:10:20.723672.723672 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:20.724271.724271 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f342df8f-70a7-408d-9785-bed8b9573b63
DEBUG 01-15 16:10:20.724142.724142 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0037546157836914062 seconds
DEBUG 01-15 16:10:20.724259.724259 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.729373.729373 cuda_h.py:19] end restore2model cost 0.004926204681396484 seconds
DEBUG 01-15 16:10:20.730794.730794 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.009069442749023438 seconds
DEBUG 01-15 16:10:20.730484.730484 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:20.729208.729208 cuda_h.py:19] end group_tensors cost 0.006663322448730469 seconds
DEBUG 01-15 16:10:20.730785.730785 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:20.730867.730867 cuda_h.py:19] end gpu_sexperts cost 0.0004963874816894531 seconds
DEBUG 01-15 16:10:20.730566.730566 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:20.730024.730024 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 8.893013000488281e-05 seconds
DEBUG 01-15 16:10:20.731149.731149 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:20.731748.731748 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.733104.733104 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0022170543670654297 seconds
DEBUG 01-15 16:10:20.733389.733389 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.734977.734977 cuda_h.py:19] end gpu_group_list cost 0.0002837181091308594 seconds
DEBUG 01-15 16:10:20.734097.734097 cuda_h.py:19] end group pad cost 0.004299163818359375 seconds
DEBUG 01-15 16:10:20.735040.735040 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:20.735659.735659 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.739594.739594 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0035622119903564453 seconds
DEBUG 01-15 16:10:20.739277.739277 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.740902.740902 cuda_h.py:19] end gpu_group_list cost 0.00036406517028808594 seconds
DEBUG 01-15 16:10:20.741808.741808 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:20.741337.741337 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f342df8f-70a7-408d-9785-bed8b9573b63
INFO 01-15 16:10:20.750901.750901 client.py:127] Model loaded
DEBUG 01-15 16:10:20.750747.750747 cuda_h.py:19] end wait_experts_multi_device cost 0.009286165237426758 seconds
DEBUG 01-15 16:10:20.751097.751097 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:20.759968.759968 cuda_h.py:19] end group_einsum cost 0.024028301239013672 seconds
DEBUG 01-15 16:10:20.759291.759291 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:20.764447.764447 cuda_h.py:19] end get_outputs_cpu1 cost 0.00495147705078125 seconds
DEBUG 01-15 16:10:20.765899.765899 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.043897390365600586 seconds
DEBUG 01-15 16:10:20.766750.766750 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.015236377716064453 seconds
DEBUG 01-15 16:10:20.766294.766294 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:20.769713.769713 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.770326.770326 cuda_h.py:19] end index_scatter cost 0.00014090538024902344 seconds
DEBUG 01-15 16:10:20.770479.770479 cuda_h.py:19] end cpuoutputsdeal cost 0.003509521484375 seconds
DEBUG 01-15 16:10:20.770955.770955 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:20.770747.770747 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.771893.771893 cuda_h.py:19] end gpu_group_tensor cost 0.0002963542938232422 seconds
DEBUG 01-15 16:10:20.771168.771168 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.772269.772269 cuda_h.py:19] end gpu_group_tensor cost 0.0007522106170654297 seconds
DEBUG 01-15 16:10:20.772797.772797 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.774091.774091 cuda_h.py:19] end gpu_group_einsum cost 0.002109050750732422 seconds
DEBUG 01-15 16:10:20.775266.775266 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.776904.776904 cuda_h.py:19] end gpu_group_einsum cost 0.0009531974792480469 seconds
DEBUG 01-15 16:10:20.776770.776770 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.776994.776994 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.776592.776592 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002048015594482422 seconds
DEBUG 01-15 16:10:20.776959.776959 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.776737.776737 cuda_h.py:19] end concat_expert_out cost 6.604194641113281e-05 seconds
DEBUG 01-15 16:10:20.777005.777005 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.777448.777448 cuda_h.py:19] end index_scatter cost 7.915496826171875e-05 seconds
DEBUG 01-15 16:10:20.777061.777061 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010802745819091797 seconds
DEBUG 01-15 16:10:20.777170.777170 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.777610.777610 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.777558.777558 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016164779663085938 seconds
DEBUG 01-15 16:10:20.777996.777996 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.778610.778610 cuda_h.py:19] end concat_expert_out cost 8.869171142578125e-05 seconds
DEBUG 01-15 16:10:20.778063.778063 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.778763.778763 cuda_h.py:19] end index_scatter cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:10:20.778453.778453 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006830692291259766 seconds
DEBUG 01-15 16:10:20.778913.778913 cuda_h.py:19] end gpu_experts_multi_device cost 0.04730939865112305 seconds
DEBUG 01-15 16:10:20.778440.778440 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.06449627876281738 seconds
DEBUG 01-15 16:10:20.779894.779894 cuda_h.py:19] end prefill_layer cost 0.0751039981842041 seconds
DEBUG 01-15 16:10:20.779434.779434 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:10:20.779773.779773 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:20.779635.779635 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:10:20.779358.779358 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:20.779896.779896 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:20.779601.779601 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 5.817413330078125e-05 seconds
DEBUG 01-15 16:10:20.779139.779139 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 0.00010776519775390625 seconds
DEBUG 01-15 16:10:20.779659.779659 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:20.779444.779444 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:20.779270.779270 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:20.780191.780191 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:20.780811.780811 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:20.780684.780684 cuda_h.py:19] end allocate_cuda_memory cost 0.00038051605224609375 seconds
DEBUG 01-15 16:10:20.780519.780519 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:20.781821.781821 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.781802.781802 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.781045.781045 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9ffa3d62-2bdc-4f5f-baa8-66c3f49c876d
DEBUG 01-15 16:10:20.781099.781099 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.782610.782610 cuda_h.py:10] start self_attn
INFO 01-15 16:10:20.783930.783930 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9ffa3d62-2bdc-4f5f-baa8-66c3f49c876d
DEBUG 01-15 16:10:20.783645.783645 cuda_h.py:19] end load_into_gpu_async cost 0.0028486251831054688 seconds
DEBUG 01-15 16:10:20.783091.783091 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:20.784907.784907 cuda_h.py:19] end restore_tensors2 cost 0.000152587890625 seconds
DEBUG 01-15 16:10:20.784805.784805 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004225730895996094 seconds
INFO 01-15 16:10:20.784983.784983 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9ffa3d62-2bdc-4f5f-baa8-66c3f49c876d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:20.786635.786635 cuda_h.py:19] end self_attn cost 0.004738330841064453 seconds
DEBUG 01-15 16:10:20.787369.787369 cuda_h.py:19] end iln_self_attn_paln cost 0.007429361343383789 seconds
DEBUG 01-15 16:10:20.787497.787497 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:10:20.787551.787551 cuda_h.py:10] start gate
DEBUG 01-15 16:10:20.788564.788564 cuda_h.py:19] end gate cost 0.0007407665252685547 seconds
DEBUG 01-15 16:10:20.788646.788646 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:20.788925.788925 lmp.py:1912] 
DEBUG 01-15 16:10:20.788925.788925 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:20.788233.788233 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:20.788038.788038 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:20.788516.788516 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:20.788742.788742 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:20.789346.789346 lmp.py:1917] 
DEBUG 01-15 16:10:20.789346.789346 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:20.789333.789333 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:20.789394.789394 lmp.py:1935]   Expert 34 |     25 | CPU
DEBUG 01-15 16:10:20.789667.789667 lmp.py:1935]   Expert 45 |     65 | CPU
DEBUG 01-15 16:10:20.789793.789793 lmp.py:1935]   Expert 22 |     74 | CPU
DEBUG 01-15 16:10:20.789443.789443 lmp.py:1935]   Expert 57 |     77 | CPU
DEBUG 01-15 16:10:20.789093.789093 lmp.py:1935]   Expert 17 |     94 | CPU
DEBUG 01-15 16:10:20.789981.789981 lmp.py:1935]   Expert  4 |     99 | CPU
DEBUG 01-15 16:10:20.789154.789154 lmp.py:1935]   Expert 15 |     99 | CPU
DEBUG 01-15 16:10:20.789327.789327 lmp.py:1935]   Expert 28 |    105 | CPU
DEBUG 01-15 16:10:20.789745.789745 lmp.py:1935]   Expert 60 |    111 | CPU
DEBUG 01-15 16:10:20.789793.789793 lmp.py:1935]   Expert 32 |    113 | CPU
DEBUG 01-15 16:10:20.789873.789873 lmp.py:1935]   Expert 36 |    125 | CPU
DEBUG 01-15 16:10:20.789715.789715 lmp.py:1935]   Expert 16 |    126 | CPU
DEBUG 01-15 16:10:20.789841.789841 lmp.py:1935]   Expert 12 |    127 | CPU
DEBUG 01-15 16:10:20.789014.789014 lmp.py:1935]   Expert 14 |    128 | CPU
DEBUG 01-15 16:10:20.789187.789187 lmp.py:1935]   Expert 52 |    129 | CPU
DEBUG 01-15 16:10:20.789599.789599 lmp.py:1935]   Expert 25 |    130 | CPU
DEBUG 01-15 16:10:20.789248.789248 lmp.py:1935]   Expert  8 |    134 | CPU
DEBUG 01-15 16:10:20.789421.789421 lmp.py:1935]   Expert  2 |    138 | CPU
DEBUG 01-15 16:10:20.789594.789594 lmp.py:1935]   Expert 35 |    143 | CPU
DEBUG 01-15 16:10:20.789721.789721 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:10:20.789609.789609 lmp.py:1935]   Expert 23 |    154 | CPU
DEBUG 01-15 16:10:20.789736.789736 lmp.py:1935]   Expert 30 |    154 | CPU
DEBUG 01-15 16:10:20.789670.789670 lmp.py:1935]   Expert  0 |    156 | CPU
DEBUG 01-15 16:10:20.789889.789889 lmp.py:1935]   Expert 61 |    157 | CPU
DEBUG 01-15 16:10:20.789824.789824 lmp.py:1935]   Expert 39 |    158 | CPU
DEBUG 01-15 16:10:20.789520.789520 lmp.py:1935]   Expert  3 |    169 | CPU
DEBUG 01-15 16:10:20.789455.789455 lmp.py:1935]   Expert 42 |    170 | CPU
DEBUG 01-15 16:10:20.789058.789058 lmp.py:1935]   Expert 13 |    171 | CPU
DEBUG 01-15 16:10:20.789423.789423 lmp.py:1935]   Expert 31 |    173 | CPU
DEBUG 01-15 16:10:20.789311.789311 lmp.py:1935]   Expert 41 |    175 | CPU
DEBUG 01-15 16:10:20.789722.789722 lmp.py:1935]   Expert 44 |    175 | CPU
DEBUG 01-15 16:10:20.789180.789180 lmp.py:1935]   Expert 46 |    177 | CPU
DEBUG 01-15 16:10:20.789121.789121 lmp.py:1935]   Expert  9 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.789917.789917 lmp.py:1935]   Expert 43 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.789997.789997 lmp.py:1935]   Expert 50 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.789601.789601 lmp.py:1935]   Expert 26 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.789966.789966 lmp.py:1935]   Expert 62 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.789523.789523 lmp.py:1935]   Expert 18 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.789703.789703 lmp.py:1935]   Expert 27 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790737.790737 lmp.py:1935]   Expert 49 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790102.790102 lmp.py:1935]   Expert 51 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790228.790228 lmp.py:1935]   Expert 11 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790116.790116 lmp.py:1935]   Expert 20 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790481.790481 lmp.py:1935]   Expert 47 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790846.790846 lmp.py:1935]   Expert 19 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790496.790496 lmp.py:1935]   Expert 63 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790623.790623 lmp.py:1935]   Expert 55 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790511.790511 lmp.py:1935]   Expert 56 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790306.790306 lmp.py:1935]   Expert 38 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790148.790148 lmp.py:1935]   Expert 48 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790189.790189 lmp.py:1935]   Expert  1 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790508.790508 lmp.py:1935]   Expert 10 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790873.790873 lmp.py:1935]   Expert 21 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790238.790238 lmp.py:1935]   Expert 54 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790603.790603 lmp.py:1935]   Expert  7 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790253.790253 lmp.py:1935]   Expert 33 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790717.790717 lmp.py:1935]   Expert 29 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790612.790612 lmp.py:1935]   Expert 40 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790076.790076 lmp.py:1935]   Expert 24 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790018.790018 lmp.py:1935]   Expert 59 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790244.790244 lmp.py:1935]   Expert 37 |    330 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790470.790470 lmp.py:1935]   Expert 58 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790457.790457 lmp.py:1935]   Expert  6 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.790352.790352 lmp.py:1935]   Expert 53 |    854 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.790386.790386 lmp.py:1937] 
DEBUG 01-15 16:10:20.790386.790386 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:20.790374.790374 lmp.py:1938]   CPU:   4178 tokens
DEBUG 01-15 16:10:20.790507.790507 lmp.py:1942]   cuda:1:   3980 tokens (15 experts)
DEBUG 01-15 16:10:20.790210.790210 lmp.py:1942]   cuda:2:   4130 tokens (17 experts)
DEBUG 01-15 16:10:20.790198.790198 lmp.py:1943]   Total GPU:   8110 tokens
DEBUG 01-15 16:10:20.790470.790470 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:20.790470.790470 lmp.py:1944] 
DEBUG 01-15 16:10:20.790001.790001 cuda_h.py:19] end experts_map_get cost 0.002466440200805664 seconds
DEBUG 01-15 16:10:20.790621.790621 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:20.790351.790351 lmp.py:1953] 
DEBUG 01-15 16:10:20.790351.790351 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:20.791652.791652 cuda_h.py:19] end cpu_experts_submit cost 8.392333984375e-05 seconds
DEBUG 01-15 16:10:20.791646.791646 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:20.791841.791841 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:20.791415.791415 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.33.gate_proj.weight': 0, 'model.layers.5.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.40.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.40.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.40.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.43.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.43.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.43.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.51.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.51.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.51.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.59.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.59.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.59.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.62.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.62.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.62.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.26.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.26.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.26.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.58.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.58.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.58.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.27.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.27.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.27.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.29.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.29.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.29.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(7860649984, 5767168, 0, 0), (7866417152, 5767168, 5767168, 0), (7854882816, 5767168, 11534336, 0), (7981760512, 5767168, 17301504, 0), (7987527680, 5767168, 23068672, 0), (7975993344, 5767168, 28835840, 0), (7462715392, 5767168, 34603008, 0), (7468482560, 5767168, 40370176, 0), (7456948224, 5767168, 46137344, 0), (7480016896, 5767168, 51904512, 0), (7485784064, 5767168, 57671680, 0), (7474249728, 5767168, 63438848, 0), (8033665024, 5767168, 69206016, 0), (8039432192, 5767168, 74973184, 0), (8027897856, 5767168, 80740352, 0), (8120172544, 5767168, 86507520, 0), (8125939712, 5767168, 92274688, 0), (8114405376, 5767168, 98041856, 0), (7601127424, 5767168, 103809024, 0), (7606894592, 5767168, 109576192, 0), (7595360256, 5767168, 115343360, 0), (8172077056, 5767168, 121110528, 0), (8177844224, 5767168, 126877696, 0), (8166309888, 5767168, 132644864, 0), (7635730432, 5767168, 138412032, 0), (7641497600, 5767168, 144179200, 0), (7629963264, 5767168, 149946368, 0), (7653031936, 5767168, 155713536, 0), (7658799104, 5767168, 161480704, 0), (7647264768, 5767168, 167247872, 0), (8206680064, 5767168, 173015040, 0), (8212447232, 5767168, 178782208, 0), (8200912896, 5767168, 184549376, 0), (8258584576, 5767168, 190316544, 0), (8264351744, 5767168, 196083712, 0), (8252817408, 5767168, 201850880, 0), (8310489088, 5767168, 207618048, 0), (8316256256, 5767168, 213385216, 0), (8304721920, 5767168, 219152384, 0), (8362393600, 5767168, 224919552, 0), (8368160768, 5767168, 230686720, 0), (8356626432, 5767168, 236453888, 0), (8379695104, 5767168, 242221056, 0), (8385462272, 5767168, 247988224, 0), (8373927936, 5767168, 253755392, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (7739539456, 5767168, 17301504, 0), (7745306624, 5767168, 23068672, 0), (7733772288, 5767168, 28835840, 0), (7929856000, 5767168, 34603008, 0), (7935623168, 5767168, 40370176, 0), (7924088832, 5767168, 46137344, 0), (7393509376, 5767168, 51904512, 0), (7399276544, 5767168, 57671680, 0), (7387742208, 5767168, 63438848, 0), (7410810880, 5767168, 69206016, 0), (7416578048, 5767168, 74973184, 0), (7405043712, 5767168, 80740352, 0), (7947157504, 5767168, 86507520, 0), (7952924672, 5767168, 92274688, 0), (7941390336, 5767168, 98041856, 0), (7445413888, 5767168, 103809024, 0), (7451181056, 5767168, 109576192, 0), (7439646720, 5767168, 115343360, 0), (8102871040, 5767168, 121110528, 0), (8108638208, 5767168, 126877696, 0), (8097103872, 5767168, 132644864, 0), (8137474048, 5767168, 138412032, 0), (8143241216, 5767168, 144179200, 0), (8131706880, 5767168, 149946368, 0), (8154775552, 5767168, 155713536, 0), (8160542720, 5767168, 161480704, 0), (8149008384, 5767168, 167247872, 0), (7618428928, 5767168, 173015040, 0), (7624196096, 5767168, 178782208, 0), (7612661760, 5767168, 184549376, 0), (8223981568, 5767168, 190316544, 0), (8229748736, 5767168, 196083712, 0), (8218214400, 5767168, 201850880, 0), (8241283072, 5767168, 207618048, 0), (8247050240, 5767168, 213385216, 0), (8235515904, 5767168, 219152384, 0), (7704936448, 5767168, 224919552, 0), (7710703616, 5767168, 230686720, 0), (7699169280, 5767168, 236453888, 0), (8293187584, 5767168, 242221056, 0), (8298954752, 5767168, 247988224, 0), (8287420416, 5767168, 253755392, 0), (7756840960, 5767168, 259522560, 0), (7762608128, 5767168, 265289728, 0), (7751073792, 5767168, 271056896, 0), (7791443968, 5767168, 276824064, 0), (7797211136, 5767168, 282591232, 0), (7785676800, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e34219080>, 2: <capsule object NULL at 0x7a4e342195f0>}
DEBUG 01-15 16:10:20.792251.792251 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:10:20.792210.792210 client.py:127] Model loaded
DEBUG 01-15 16:10:20.792790.792790 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.792755.792755 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e9c5096b-963a-44b3-bb58-b944c3d3cde2
DEBUG 01-15 16:10:20.793221.793221 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.793823.793823 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:20.792574.792574 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.793516.793516 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:20.794353.794353 cuda_h.py:19] end restore2model cost 0.0007526874542236328 seconds
DEBUG 01-15 16:10:20.794441.794441 cuda_h.py:19] end sllm_worker_task cost 0.014442682266235352 seconds
INFO 01-15 16:10:20.794439.794439 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e9c5096b-963a-44b3-bb58-b944c3d3cde2
DEBUG 01-15 16:10:20.794422.794422 cuda_h.py:19] end move_flatidxs cost 0.0010273456573486328 seconds
DEBUG 01-15 16:10:20.794446.794446 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:20.795304.795304 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003912687301635742 seconds
DEBUG 01-15 16:10:20.795890.795890 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.798649.798649 cuda_h.py:19] end restore2model cost 0.003046751022338867 seconds
DEBUG 01-15 16:10:20.798843.798843 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007241010665893555 seconds
DEBUG 01-15 16:10:20.798315.798315 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:20.798559.798559 cuda_h.py:19] end gpu_sexperts cost 0.0003521442413330078 seconds
DEBUG 01-15 16:10:20.798416.798416 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:20.798636.798636 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1696090698242188e-05 seconds
DEBUG 01-15 16:10:20.798531.798531 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:20.798864.798864 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.799419.799419 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009336471557617188 seconds
DEBUG 01-15 16:10:20.799084.799084 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.800443.800443 cuda_h.py:19] end gpu_group_list cost 0.00019073486328125 seconds
DEBUG 01-15 16:10:20.801372.801372 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.802630.802630 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011014938354492188 seconds
DEBUG 01-15 16:10:20.802632.802632 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.802303.802303 cuda_h.py:19] end gpu_group_list cost 0.00021386146545410156 seconds
DEBUG 01-15 16:10:20.803195.803195 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:20.803621.803621 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e9c5096b-963a-44b3-bb58-b944c3d3cde2
DEBUG 01-15 16:10:20.809576.809576 cuda_h.py:19] end group_tensors cost 0.014264106750488281 seconds
DEBUG 01-15 16:10:20.810494.810494 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:20.814551.814551 cuda_h.py:19] end group pad cost 0.004405498504638672 seconds
DEBUG 01-15 16:10:20.814831.814831 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:20.823729.823729 client.py:127] Model loaded
DEBUG 01-15 16:10:20.823954.823954 cuda_h.py:19] end wait_experts_multi_device cost 0.02020406723022461 seconds
DEBUG 01-15 16:10:20.823102.823102 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:20.838286.838286 cuda_h.py:19] end group_einsum cost 0.024195194244384766 seconds
DEBUG 01-15 16:10:20.839729.839729 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:20.842721.842721 cuda_h.py:19] end get_outputs_cpu1 cost 0.00384521484375 seconds
DEBUG 01-15 16:10:20.843725.843725 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0505373477935791 seconds
DEBUG 01-15 16:10:20.844221.844221 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.02104496955871582 seconds
DEBUG 01-15 16:10:20.845837.845837 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:20.847706.847706 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.848731.848731 cuda_h.py:19] end index_scatter cost 0.00014328956604003906 seconds
DEBUG 01-15 16:10:20.848519.848519 cuda_h.py:19] end cpuoutputsdeal cost 0.0038299560546875 seconds
DEBUG 01-15 16:10:20.849386.849386 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:20.849336.849336 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.850995.850995 cuda_h.py:19] end gpu_group_tensor cost 0.0007691383361816406 seconds
DEBUG 01-15 16:10:20.850734.850734 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.851044.851044 cuda_h.py:19] end gpu_group_tensor cost 0.0012531280517578125 seconds
DEBUG 01-15 16:10:20.851795.851795 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.853620.853620 cuda_h.py:19] end gpu_group_einsum cost 0.0015676021575927734 seconds
DEBUG 01-15 16:10:20.853748.853748 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.854050.854050 cuda_h.py:19] end gpu_group_einsum cost 0.0009717941284179688 seconds
DEBUG 01-15 16:10:20.855642.855642 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.855160.855160 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.855171.855171 cuda_h.py:19] end all_expert_outputs_slices cost 0.00040721893310546875 seconds
DEBUG 01-15 16:10:20.856101.856101 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.856757.856757 cuda_h.py:19] end concat_expert_out cost 0.00012183189392089844 seconds
DEBUG 01-15 16:10:20.856192.856192 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.856502.856502 cuda_h.py:19] end index_scatter cost 9.584426879882812e-05 seconds
DEBUG 01-15 16:10:20.857421.857421 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0017561912536621094 seconds
DEBUG 01-15 16:10:20.857932.857932 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.857378.857378 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.857286.857286 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017905235290527344 seconds
DEBUG 01-15 16:10:20.857327.857327 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.857006.857006 cuda_h.py:19] end concat_expert_out cost 7.605552673339844e-05 seconds
DEBUG 01-15 16:10:20.857641.857641 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.858267.858267 cuda_h.py:19] end index_scatter cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:10:20.858122.858122 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006914138793945312 seconds
DEBUG 01-15 16:10:20.858853.858853 cuda_h.py:19] end gpu_experts_multi_device cost 0.0592348575592041 seconds
DEBUG 01-15 16:10:20.858770.858770 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.07081294059753418 seconds
DEBUG 01-15 16:10:20.858466.858466 cuda_h.py:19] end prefill_layer cost 0.07940316200256348 seconds
DEBUG 01-15 16:10:20.858395.858395 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:10:20.858237.858237 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:20.858854.858854 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:10:20.858318.858318 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:20.858975.858975 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:20.858202.858202 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 3.719329833984375e-05 seconds
DEBUG 01-15 16:10:20.858190.858190 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:10:20.858025.858025 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:20.859829.859829 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:20.859674.859674 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:20.859022.859022 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:20.859925.859925 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:20.859647.859647 cuda_h.py:19] end allocate_cuda_memory cost 0.00017762184143066406 seconds
DEBUG 01-15 16:10:20.859034.859034 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:20.859797.859797 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.859825.859825 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.859296.859296 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5fa5ef53-5af2-4588-861c-f68f150865ef
DEBUG 01-15 16:10:20.859863.859863 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.860879.860879 cuda_h.py:10] start self_attn
INFO 01-15 16:10:20.861613.861613 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5fa5ef53-5af2-4588-861c-f68f150865ef
DEBUG 01-15 16:10:20.861953.861953 cuda_h.py:19] end load_into_gpu_async cost 0.0016934871673583984 seconds
DEBUG 01-15 16:10:20.861755.861755 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:20.861851.861851 cuda_h.py:19] end restore_tensors2 cost 7.700920104980469e-05 seconds
DEBUG 01-15 16:10:20.861992.861992 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022230148315429688 seconds
INFO 01-15 16:10:20.861226.861226 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5fa5ef53-5af2-4588-861c-f68f150865ef
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:20.865763.865763 cuda_h.py:19] end self_attn cost 0.004486560821533203 seconds
DEBUG 01-15 16:10:20.865219.865219 cuda_h.py:19] end iln_self_attn_paln cost 0.00657200813293457 seconds
DEBUG 01-15 16:10:20.865261.865261 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:10:20.865978.865978 cuda_h.py:10] start gate
DEBUG 01-15 16:10:20.866819.866819 cuda_h.py:19] end gate cost 0.0007576942443847656 seconds
DEBUG 01-15 16:10:20.866708.866708 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:20.866897.866897 lmp.py:1912] 
DEBUG 01-15 16:10:20.866897.866897 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:20.867799.867799 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:20.867833.867833 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:20.867052.867052 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:20.867317.867317 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:20.867152.867152 lmp.py:1917] 
DEBUG 01-15 16:10:20.867152.867152 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:20.867041.867041 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:20.867227.867227 lmp.py:1935]   Expert  1 |     45 | CPU
DEBUG 01-15 16:10:20.867983.867983 lmp.py:1935]   Expert  7 |     60 | CPU
DEBUG 01-15 16:10:20.867302.867302 lmp.py:1935]   Expert 37 |     71 | CPU
DEBUG 01-15 16:10:20.867236.867236 lmp.py:1935]   Expert 54 |     75 | CPU
DEBUG 01-15 16:10:20.867694.867694 lmp.py:1935]   Expert 17 |     76 | CPU
DEBUG 01-15 16:10:20.867629.867629 lmp.py:1935]   Expert 18 |     84 | CPU
DEBUG 01-15 16:10:20.867040.867040 lmp.py:1935]   Expert  9 |     91 | CPU
DEBUG 01-15 16:10:20.867498.867498 lmp.py:1935]   Expert 13 |     92 | CPU
DEBUG 01-15 16:10:20.867955.867955 lmp.py:1935]   Expert 22 |    101 | CPU
DEBUG 01-15 16:10:20.867698.867698 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:10:20.867678.867678 lmp.py:1935]   Expert  0 |    108 | CPU
DEBUG 01-15 16:10:20.867421.867421 lmp.py:1935]   Expert 26 |    116 | CPU
DEBUG 01-15 16:10:20.867163.867163 lmp.py:1935]   Expert 16 |    119 | CPU
DEBUG 01-15 16:10:20.867429.867429 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:10:20.867171.867171 lmp.py:1935]   Expert 63 |    128 | CPU
DEBUG 01-15 16:10:20.867391.867391 lmp.py:1935]   Expert 59 |    132 | CPU
DEBUG 01-15 16:10:20.867371.867371 lmp.py:1935]   Expert 62 |    141 | CPU
DEBUG 01-15 16:10:20.867352.867352 lmp.py:1935]   Expert 43 |    143 | CPU
DEBUG 01-15 16:10:20.867333.867333 lmp.py:1935]   Expert 28 |    145 | CPU
DEBUG 01-15 16:10:20.867075.867075 lmp.py:1935]   Expert 33 |    148 | CPU
DEBUG 01-15 16:10:20.867579.867579 lmp.py:1935]   Expert 29 |    149 | CPU
DEBUG 01-15 16:10:20.867083.867083 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:10:20.867064.867064 lmp.py:1935]   Expert 51 |    164 | CPU
DEBUG 01-15 16:10:20.867568.867568 lmp.py:1935]   Expert 11 |    166 | CPU
DEBUG 01-15 16:10:20.867834.867834 lmp.py:1935]   Expert 23 |    166 | CPU
DEBUG 01-15 16:10:20.867623.867623 lmp.py:1935]   Expert 45 |    166 | CPU
DEBUG 01-15 16:10:20.867842.867842 lmp.py:1935]   Expert 55 |    166 | CPU
DEBUG 01-15 16:10:20.867300.867300 lmp.py:1935]   Expert 53 |    167 | CPU
DEBUG 01-15 16:10:20.867234.867234 lmp.py:1935]   Expert  3 |    168 | CPU
DEBUG 01-15 16:10:20.867738.867738 lmp.py:1935]   Expert 32 |    168 | CPU
DEBUG 01-15 16:10:20.867242.867242 lmp.py:1935]   Expert 40 |    169 | CPU
DEBUG 01-15 16:10:20.867614.867614 lmp.py:1935]   Expert 14 |    174 | CPU
DEBUG 01-15 16:10:20.867310.867310 lmp.py:1935]   Expert 34 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.867721.867721 lmp.py:1935]   Expert 52 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.867941.867941 lmp.py:1935]   Expert 41 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.867398.867398 lmp.py:1935]   Expert 42 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.867810.867810 lmp.py:1935]   Expert 21 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.867744.867744 lmp.py:1935]   Expert 57 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868083.868083 lmp.py:1935]   Expert 30 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868872.868872 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868469.868469 lmp.py:1935]   Expert 35 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868065.868065 lmp.py:1935]   Expert 12 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868900.868900 lmp.py:1935]   Expert  4 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868497.868497 lmp.py:1935]   Expert 46 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868094.868094 lmp.py:1935]   Expert 19 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868929.868929 lmp.py:1935]   Expert 24 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868287.868287 lmp.py:1935]   Expert 50 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868883.868883 lmp.py:1935]   Expert 44 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868480.868480 lmp.py:1935]   Expert  8 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868315.868315 lmp.py:1935]   Expert 49 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868912.868912 lmp.py:1935]   Expert 38 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868270.868270 lmp.py:1935]   Expert  6 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868867.868867 lmp.py:1935]   Expert 47 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868417.868417 lmp.py:1935]   Expert 31 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868967.868967 lmp.py:1935]   Expert 61 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868041.868041 lmp.py:1935]   Expert 39 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868876.868876 lmp.py:1935]   Expert  5 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868234.868234 lmp.py:1935]   Expert 36 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868592.868592 lmp.py:1935]   Expert 27 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868772.868772 lmp.py:1935]   Expert 60 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868369.868369 lmp.py:1935]   Expert 20 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868350.868350 lmp.py:1935]   Expert 48 |    366 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868569.868569 lmp.py:1935]   Expert 25 |    398 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.868027.868027 lmp.py:1935]   Expert 56 |    558 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.868246.868246 lmp.py:1937] 
DEBUG 01-15 16:10:20.868246.868246 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:20.868896.868896 lmp.py:1938]   CPU:   4077 tokens
DEBUG 01-15 16:10:20.868068.868068 lmp.py:1942]   cuda:1:   4121 tokens (16 experts)
DEBUG 01-15 16:10:20.868765.868765 lmp.py:1942]   cuda:2:   4090 tokens (16 experts)
DEBUG 01-15 16:10:20.868745.868745 lmp.py:1943]   Total GPU:   8211 tokens
DEBUG 01-15 16:10:20.868011.868011 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:20.868011.868011 lmp.py:1944] 
DEBUG 01-15 16:10:20.868429.868429 cuda_h.py:19] end experts_map_get cost 0.0021152496337890625 seconds
INFO 01-15 16:10:20.868644.868644 client.py:127] Model loaded
DEBUG 01-15 16:10:20.868773.868773 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:20.868525.868525 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.868513.868513 lmp.py:1953] 
DEBUG 01-15 16:10:20.868513.868513 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:20.869137.869137 cuda_h.py:19] end cpu_experts_submit cost 0.0006401538848876953 seconds
DEBUG 01-15 16:10:20.869092.869092 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:20.869254.869254 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:20.870303.870303 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.34.gate_proj.weight': 0, 'model.layers.6.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.38.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.38.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.38.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.6.mlp.experts.36.gate_proj.weight': 0, 'model.layers.6.mlp.experts.36.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.36.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.57.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.57.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.57.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(8985247744, 5767168, 0, 0), (8991014912, 5767168, 5767168, 0), (8979480576, 5767168, 11534336, 0), (9002549248, 5767168, 17301504, 0), (9008316416, 5767168, 23068672, 0), (8996782080, 5767168, 28835840, 0), (8466202624, 5767168, 34603008, 0), (8471969792, 5767168, 40370176, 0), (8460435456, 5767168, 46137344, 0), (8483504128, 5767168, 51904512, 0), (8489271296, 5767168, 57671680, 0), (8477736960, 5767168, 63438848, 0), (9054453760, 5767168, 69206016, 0), (9060220928, 5767168, 74973184, 0), (9048686592, 5767168, 80740352, 0), (9106358272, 5767168, 86507520, 0), (9112125440, 5767168, 92274688, 0), (9100591104, 5767168, 98041856, 0), (9210167296, 5767168, 103809024, 0), (9215934464, 5767168, 109576192, 0), (9204400128, 5767168, 115343360, 0), (9244770304, 5767168, 121110528, 0), (9250537472, 5767168, 126877696, 0), (9239003136, 5767168, 132644864, 0), (9262071808, 5767168, 138412032, 0), (9267838976, 5767168, 144179200, 0), (9256304640, 5767168, 149946368, 0), (8725725184, 5767168, 155713536, 0), (8731492352, 5767168, 161480704, 0), (8719958016, 5767168, 167247872, 0), (8743026688, 5767168, 173015040, 0), (8748793856, 5767168, 178782208, 0), (8737259520, 5767168, 184549376, 0), (8760328192, 5767168, 190316544, 0), (8766095360, 5767168, 196083712, 0), (8754561024, 5767168, 201850880, 0), (9365880832, 5767168, 207618048, 0), (9371648000, 5767168, 213385216, 0), (9360113664, 5767168, 219152384, 0), (8864137216, 5767168, 224919552, 0), (8869904384, 5767168, 230686720, 0), (8858370048, 5767168, 236453888, 0), (9452388352, 5767168, 242221056, 0), (9458155520, 5767168, 247988224, 0), (9446621184, 5767168, 253755392, 0), (8916041728, 5767168, 259522560, 0), (8921808896, 5767168, 265289728, 0), (8910274560, 5767168, 271056896, 0)], 2: [(9019850752, 5767168, 0, 0), (9025617920, 5767168, 5767168, 0), (9014083584, 5767168, 11534336, 0), (8500805632, 5767168, 17301504, 0), (8506572800, 5767168, 23068672, 0), (8495038464, 5767168, 28835840, 0), (9071755264, 5767168, 34603008, 0), (9077522432, 5767168, 40370176, 0), (9065988096, 5767168, 46137344, 0), (8535408640, 5767168, 51904512, 0), (8541175808, 5767168, 57671680, 0), (8529641472, 5767168, 63438848, 0), (9123659776, 5767168, 69206016, 0), (9129426944, 5767168, 74973184, 0), (9117892608, 5767168, 80740352, 0), (9158262784, 5767168, 86507520, 0), (9164029952, 5767168, 92274688, 0), (9152495616, 5767168, 98041856, 0), (8604614656, 5767168, 103809024, 0), (8610381824, 5767168, 109576192, 0), (8598847488, 5767168, 115343360, 0), (9192865792, 5767168, 121110528, 0), (9198632960, 5767168, 126877696, 0), (9187098624, 5767168, 132644864, 0), (8656519168, 5767168, 138412032, 0), (8662286336, 5767168, 144179200, 0), (8650752000, 5767168, 149946368, 0), (9227468800, 5767168, 155713536, 0), (9233235968, 5767168, 161480704, 0), (9221701632, 5767168, 167247872, 0), (9383182336, 5767168, 173015040, 0), (9388949504, 5767168, 178782208, 0), (9377415168, 5767168, 184549376, 0), (9296674816, 5767168, 190316544, 0), (9302441984, 5767168, 196083712, 0), (9290907648, 5767168, 201850880, 0), (8812232704, 5767168, 207618048, 0), (8817999872, 5767168, 213385216, 0), (8806465536, 5767168, 219152384, 0), (8829534208, 5767168, 224919552, 0), (8835301376, 5767168, 230686720, 0), (8823767040, 5767168, 236453888, 0), (9435086848, 5767168, 242221056, 0), (9440854016, 5767168, 247988224, 0), (9429319680, 5767168, 253755392, 0), (8933343232, 5767168, 259522560, 0), (8939110400, 5767168, 265289728, 0), (8927576064, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e342197a0>, 2: <capsule object NULL at 0x7a4e342191a0>}
DEBUG 01-15 16:10:20.870258.870258 cuda_h.py:19] end restore2model cost 0.0015821456909179688 seconds
DEBUG 01-15 16:10:20.870586.870586 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.871614.871614 cuda_h.py:19] end sllm_worker_task cost 0.01195526123046875 seconds
DEBUG 01-15 16:10:20.871691.871691 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.871941.871941 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 55bfd3da-b11b-4e00-b15c-338ab04c87e1
DEBUG 01-15 16:10:20.871528.871528 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:20.871643.871643 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.871982.871982 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:20.872072.872072 cuda_h.py:19] end move_flatidxs cost 0.0008456707000732422 seconds
DEBUG 01-15 16:10:20.872948.872948 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:20.872645.872645 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 55bfd3da-b11b-4e00-b15c-338ab04c87e1
DEBUG 01-15 16:10:20.873508.873508 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0035903453826904297 seconds
DEBUG 01-15 16:10:20.873272.873272 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.876998.876998 cuda_h.py:19] end restore2model cost 0.0030624866485595703 seconds
DEBUG 01-15 16:10:20.876934.876934 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006912946701049805 seconds
DEBUG 01-15 16:10:20.876683.876683 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:20.876172.876172 cuda_h.py:19] end gpu_sexperts cost 0.00032448768615722656 seconds
DEBUG 01-15 16:10:20.877247.877247 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:20.877937.877937 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.193450927734375e-05 seconds
DEBUG 01-15 16:10:20.877779.877779 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:20.877866.877866 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.878346.878346 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010509490966796875 seconds
DEBUG 01-15 16:10:20.878666.878666 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.878190.878190 cuda_h.py:19] end gpu_group_list cost 0.00017905235290527344 seconds
DEBUG 01-15 16:10:20.879295.879295 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.880514.880514 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001149892807006836 seconds
DEBUG 01-15 16:10:20.880265.880265 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.880888.880888 cuda_h.py:19] end gpu_group_list cost 0.0001819133758544922 seconds
DEBUG 01-15 16:10:20.881046.881046 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:20.881313.881313 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 55bfd3da-b11b-4e00-b15c-338ab04c87e1
DEBUG 01-15 16:10:20.881606.881606 cuda_h.py:19] end group_tensors cost 0.008555412292480469 seconds
DEBUG 01-15 16:10:20.882417.882417 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:20.885393.885393 cuda_h.py:19] end group pad cost 0.003789186477661133 seconds
DEBUG 01-15 16:10:20.886806.886806 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:20.899111.899111 client.py:127] Model loaded
DEBUG 01-15 16:10:20.899397.899397 cuda_h.py:19] end wait_experts_multi_device cost 0.0177001953125 seconds
DEBUG 01-15 16:10:20.899809.899809 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:20.908195.908195 cuda_h.py:19] end group_einsum cost 0.022210121154785156 seconds
DEBUG 01-15 16:10:20.908439.908439 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:20.913209.913209 cuda_h.py:19] end get_outputs_cpu1 cost 0.0051195621490478516 seconds
DEBUG 01-15 16:10:20.914498.914498 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0429384708404541 seconds
DEBUG 01-15 16:10:20.915574.915574 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01581597328186035 seconds
DEBUG 01-15 16:10:20.915401.915401 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:20.918014.918014 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.918335.918335 cuda_h.py:19] end index_scatter cost 0.00014638900756835938 seconds
DEBUG 01-15 16:10:20.919148.919148 cuda_h.py:19] end cpuoutputsdeal cost 0.0033960342407226562 seconds
DEBUG 01-15 16:10:20.919848.919848 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:20.919063.919063 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.919089.919089 cuda_h.py:19] end gpu_group_tensor cost 0.000286102294921875 seconds
DEBUG 01-15 16:10:20.919926.919926 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.920157.920157 cuda_h.py:19] end gpu_group_tensor cost 0.0002663135528564453 seconds
DEBUG 01-15 16:10:20.920337.920337 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.921292.921292 cuda_h.py:19] end gpu_group_einsum cost 0.0011162757873535156 seconds
DEBUG 01-15 16:10:20.921386.921386 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.922202.922202 cuda_h.py:19] end gpu_group_einsum cost 0.0009031295776367188 seconds
DEBUG 01-15 16:10:20.923528.923528 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.923495.923495 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.923902.923902 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003762245178222656 seconds
DEBUG 01-15 16:10:20.923964.923964 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.924361.924361 cuda_h.py:19] end concat_expert_out cost 0.00010991096496582031 seconds
DEBUG 01-15 16:10:20.924227.924227 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.924467.924467 cuda_h.py:19] end index_scatter cost 0.00012683868408203125 seconds
DEBUG 01-15 16:10:20.925994.925994 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0017414093017578125 seconds
DEBUG 01-15 16:10:20.925133.925133 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.925409.925409 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.925766.925766 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002887248992919922 seconds
DEBUG 01-15 16:10:20.925437.925437 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.926457.926457 cuda_h.py:19] end concat_expert_out cost 0.00011467933654785156 seconds
DEBUG 01-15 16:10:20.926885.926885 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.926780.926780 cuda_h.py:19] end index_scatter cost 0.00011682510375976562 seconds
DEBUG 01-15 16:10:20.926140.926140 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011057853698730469 seconds
DEBUG 01-15 16:10:20.926013.926013 cuda_h.py:19] end gpu_experts_multi_device cost 0.049469709396362305 seconds
DEBUG 01-15 16:10:20.926304.926304 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.06115245819091797 seconds
DEBUG 01-15 16:10:20.927897.927897 cuda_h.py:19] end prefill_layer cost 0.06873607635498047 seconds
DEBUG 01-15 16:10:20.927908.927908 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:10:20.927857.927857 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:20.927144.927144 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:10:20.927001.927001 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:20.927156.927156 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:20.928227.928227 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:10:20.928265.928265 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:20.928488.928488 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 0.0003428459167480469 seconds
DEBUG 01-15 16:10:20.928276.928276 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:20.928498.928498 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:20.928503.928503 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:20.929817.929817 cuda_h.py:19] end allocate_cuda_memory cost 0.00029397010803222656 seconds
DEBUG 01-15 16:10:20.929690.929690 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:20.929997.929997 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:20.929897.929897 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.929694.929694 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.929119.929119 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 92fd2b1a-83c6-414c-aef7-02e2571b540e
DEBUG 01-15 16:10:20.929845.929845 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.930705.930705 cuda_h.py:10] start self_attn
INFO 01-15 16:10:20.931502.931502 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 92fd2b1a-83c6-414c-aef7-02e2571b540e
DEBUG 01-15 16:10:20.931010.931010 cuda_h.py:19] end load_into_gpu_async cost 0.0019779205322265625 seconds
DEBUG 01-15 16:10:20.931773.931773 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:20.931359.931359 cuda_h.py:19] end restore_tensors2 cost 8.749961853027344e-05 seconds
DEBUG 01-15 16:10:20.931453.931453 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003100156784057617 seconds
INFO 01-15 16:10:20.931092.931092 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 92fd2b1a-83c6-414c-aef7-02e2571b540e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:20.935587.935587 cuda_h.py:19] end self_attn cost 0.004847526550292969 seconds
DEBUG 01-15 16:10:20.935059.935059 cuda_h.py:19] end iln_self_attn_paln cost 0.006789445877075195 seconds
DEBUG 01-15 16:10:20.935948.935948 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:10:20.935665.935665 cuda_h.py:10] start gate
DEBUG 01-15 16:10:20.936314.936314 cuda_h.py:19] end gate cost 0.0007560253143310547 seconds
DEBUG 01-15 16:10:20.936157.936157 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:20.937168.937168 lmp.py:1912] 
DEBUG 01-15 16:10:20.937168.937168 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:20.937852.937852 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:20.937363.937363 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:20.937582.937582 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:20.937894.937894 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:20.937014.937014 lmp.py:1917] 
DEBUG 01-15 16:10:20.937014.937014 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:20.937087.937087 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:20.937883.937883 lmp.py:1935]   Expert 50 |     43 | CPU
DEBUG 01-15 16:10:20.937718.937718 lmp.py:1935]   Expert  3 |     54 | CPU
DEBUG 01-15 16:10:20.937599.937599 lmp.py:1935]   Expert 46 |     55 | CPU
DEBUG 01-15 16:10:20.937242.937242 lmp.py:1935]   Expert  1 |     75 | CPU
DEBUG 01-15 16:10:20.937124.937124 lmp.py:1935]   Expert 29 |     87 | CPU
DEBUG 01-15 16:10:20.937528.937528 lmp.py:1935]   Expert  4 |     89 | CPU
DEBUG 01-15 16:10:20.937701.937701 lmp.py:1935]   Expert 15 |     95 | CPU
DEBUG 01-15 16:10:20.937159.937159 lmp.py:1935]   Expert 40 |     95 | CPU
DEBUG 01-15 16:10:20.937570.937570 lmp.py:1935]   Expert  8 |    110 | CPU
DEBUG 01-15 16:10:20.937789.937789 lmp.py:1935]   Expert 28 |    112 | CPU
DEBUG 01-15 16:10:20.937486.937486 lmp.py:1935]   Expert 41 |    114 | CPU
DEBUG 01-15 16:10:20.937705.937705 lmp.py:1935]   Expert 16 |    124 | CPU
DEBUG 01-15 16:10:20.937421.937421 lmp.py:1935]   Expert 27 |    128 | CPU
DEBUG 01-15 16:10:20.937495.937495 lmp.py:1935]   Expert  6 |    130 | CPU
DEBUG 01-15 16:10:20.937138.937138 lmp.py:1935]   Expert 48 |    130 | CPU
DEBUG 01-15 16:10:20.937542.937542 lmp.py:1935]   Expert 13 |    132 | CPU
DEBUG 01-15 16:10:20.937185.937185 lmp.py:1935]   Expert 54 |    133 | CPU
DEBUG 01-15 16:10:20.937590.937590 lmp.py:1935]   Expert  7 |    134 | CPU
DEBUG 01-15 16:10:20.937233.937233 lmp.py:1935]   Expert 51 |    138 | CPU
DEBUG 01-15 16:10:20.937114.937114 lmp.py:1935]   Expert 60 |    138 | CPU
DEBUG 01-15 16:10:20.937472.937472 lmp.py:1935]   Expert 39 |    139 | CPU
DEBUG 01-15 16:10:20.937831.937831 lmp.py:1935]   Expert 18 |    142 | CPU
DEBUG 01-15 16:10:20.937189.937189 lmp.py:1935]   Expert 43 |    145 | CPU
DEBUG 01-15 16:10:20.937786.937786 lmp.py:1935]   Expert 14 |    146 | CPU
DEBUG 01-15 16:10:20.937190.937190 lmp.py:1935]   Expert 52 |    147 | CPU
DEBUG 01-15 16:10:20.937595.937595 lmp.py:1935]   Expert 20 |    148 | CPU
DEBUG 01-15 16:10:20.937999.937999 lmp.py:1935]   Expert 56 |    149 | CPU
DEBUG 01-15 16:10:20.937404.937404 lmp.py:1935]   Expert 55 |    153 | CPU
DEBUG 01-15 16:10:20.937570.937570 lmp.py:1935]   Expert 36 |    156 | CPU
DEBUG 01-15 16:10:20.937736.937736 lmp.py:1935]   Expert 10 |    157 | CPU
DEBUG 01-15 16:10:20.937664.937664 lmp.py:1935]   Expert 11 |    157 | CPU
DEBUG 01-15 16:10:20.937068.937068 lmp.py:1935]   Expert 45 |    157 | CPU
DEBUG 01-15 16:10:20.938586.938586 lmp.py:1935]   Expert  5 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938494.938494 lmp.py:1935]   Expert 62 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938714.938714 lmp.py:1935]   Expert 57 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938787.938787 lmp.py:1935]   Expert 44 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938622.938622 lmp.py:1935]   Expert 33 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938219.938219 lmp.py:1935]   Expert 58 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938054.938054 lmp.py:1935]   Expert 25 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938889.938889 lmp.py:1935]   Expert 53 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938486.938486 lmp.py:1935]   Expert 32 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938844.938844 lmp.py:1935]   Expert  2 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938441.938441 lmp.py:1935]   Expert 31 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938991.938991 lmp.py:1935]   Expert 21 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938303.938303 lmp.py:1935]   Expert 35 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938138.938138 lmp.py:1935]   Expert 49 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938165.938165 lmp.py:1935]   Expert 63 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938000.938000 lmp.py:1935]   Expert 17 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938597.938597 lmp.py:1935]   Expert 42 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938955.938955 lmp.py:1935]   Expert 34 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938552.938552 lmp.py:1935]   Expert 37 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938910.938910 lmp.py:1935]   Expert 59 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938268.938268 lmp.py:1935]   Expert  0 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938388.938388 lmp.py:1935]   Expert 22 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938746.938746 lmp.py:1935]   Expert 19 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938581.938581 lmp.py:1935]   Expert 24 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938416.938416 lmp.py:1935]   Expert 61 |    289 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938205.938205 lmp.py:1935]   Expert 30 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938994.938994 lmp.py:1935]   Expert 47 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938829.938829 lmp.py:1935]   Expert 38 |    367 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938240.938240 lmp.py:1935]   Expert 26 |    373 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938936.938936 lmp.py:1935]   Expert 12 |    427 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938632.938632 lmp.py:1935]   Expert  9 |    680 | GPU2(cuda:2)
DEBUG 01-15 16:10:20.938613.938613 lmp.py:1935]   Expert 23 |    703 | GPU1(cuda:1)
DEBUG 01-15 16:10:20.938117.938117 lmp.py:1937] 
DEBUG 01-15 16:10:20.938117.938117 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:20.938336.938336 lmp.py:1938]   CPU:   3912 tokens
DEBUG 01-15 16:10:20.938509.938509 lmp.py:1942]   cuda:1:   4187 tokens (16 experts)
DEBUG 01-15 16:10:20.938729.938729 lmp.py:1942]   cuda:2:   4189 tokens (16 experts)
DEBUG 01-15 16:10:20.938709.938709 lmp.py:1943]   Total GPU:   8376 tokens
DEBUG 01-15 16:10:20.938690.938690 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:20.938690.938690 lmp.py:1944] 
DEBUG 01-15 16:10:20.938108.938108 cuda_h.py:19] end experts_map_get cost 0.002073526382446289 seconds
INFO 01-15 16:10:20.938800.938800 client.py:127] Model loaded
DEBUG 01-15 16:10:20.938538.938538 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:20.939660.939660 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.939172.939172 lmp.py:1953] 
DEBUG 01-15 16:10:20.939172.939172 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:20.939194.939194 cuda_h.py:19] end cpu_experts_submit cost 0.0006532669067382812 seconds
DEBUG 01-15 16:10:20.939977.939977 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:20.939873.939873 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:20.940227.940227 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.34.gate_proj.weight': 0, 'model.layers.7.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.2.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.2.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.2.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.58.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.58.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.58.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.38.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.38.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.38.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.5.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.5.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.5.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.32.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.32.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.32.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.33.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.33.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.33.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.62.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.62.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.62.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(10092544000, 5767168, 0, 0), (10098311168, 5767168, 5767168, 0), (10086776832, 5767168, 11534336, 0), (10109845504, 5767168, 17301504, 0), (10115612672, 5767168, 23068672, 0), (10104078336, 5767168, 28835840, 0), (9538895872, 5767168, 34603008, 0), (9544663040, 5767168, 40370176, 0), (9533128704, 5767168, 46137344, 0), (10507780096, 5767168, 51904512, 0), (10513547264, 5767168, 57671680, 0), (10502012928, 5767168, 63438848, 0), (10161750016, 5767168, 69206016, 0), (10167517184, 5767168, 74973184, 0), (10155982848, 5767168, 80740352, 0), (9590800384, 5767168, 86507520, 0), (9596567552, 5767168, 92274688, 0), (9585033216, 5767168, 98041856, 0), (10265559040, 5767168, 103809024, 0), (10271326208, 5767168, 109576192, 0), (10259791872, 5767168, 115343360, 0), (9798418432, 5767168, 121110528, 0), (9804185600, 5767168, 126877696, 0), (9792651264, 5767168, 132644864, 0), (9833021440, 5767168, 138412032, 0), (9838788608, 5767168, 144179200, 0), (9827254272, 5767168, 149946368, 0), (9884925952, 5767168, 155713536, 0), (9890693120, 5767168, 161480704, 0), (9879158784, 5767168, 167247872, 0), (9902227456, 5767168, 173015040, 0), (9907994624, 5767168, 178782208, 0), (9896460288, 5767168, 184549376, 0), (9936830464, 5767168, 190316544, 0), (9942597632, 5767168, 196083712, 0), (9931063296, 5767168, 201850880, 0), (9954131968, 5767168, 207618048, 0), (9959899136, 5767168, 213385216, 0), (9948364800, 5767168, 219152384, 0), (10525081600, 5767168, 224919552, 0), (10530848768, 5767168, 230686720, 0), (10519314432, 5767168, 236453888, 0), (10559684608, 5767168, 242221056, 0), (10565451776, 5767168, 247988224, 0), (10553917440, 5767168, 253755392, 0), (10594287616, 5767168, 259522560, 0), (10600054784, 5767168, 265289728, 0), (10588520448, 5767168, 271056896, 0)], 2: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (10057940992, 5767168, 17301504, 0), (10063708160, 5767168, 23068672, 0), (10052173824, 5767168, 28835840, 0), (10075242496, 5767168, 34603008, 0), (10081009664, 5767168, 40370176, 0), (10069475328, 5767168, 46137344, 0), (10144448512, 5767168, 51904512, 0), (10150215680, 5767168, 57671680, 0), (10138681344, 5767168, 63438848, 0), (9660006400, 5767168, 69206016, 0), (9665773568, 5767168, 74973184, 0), (9654239232, 5767168, 80740352, 0), (10230956032, 5767168, 86507520, 0), (10236723200, 5767168, 92274688, 0), (10225188864, 5767168, 98041856, 0), (9711910912, 5767168, 103809024, 0), (9717678080, 5767168, 109576192, 0), (9706143744, 5767168, 115343360, 0), (10317463552, 5767168, 121110528, 0), (10323230720, 5767168, 126877696, 0), (10311696384, 5767168, 132644864, 0), (10352066560, 5767168, 138412032, 0), (10357833728, 5767168, 144179200, 0), (10346299392, 5767168, 149946368, 0), (9867624448, 5767168, 155713536, 0), (9873391616, 5767168, 161480704, 0), (9861857280, 5767168, 167247872, 0), (10421272576, 5767168, 173015040, 0), (10427039744, 5767168, 178782208, 0), (10415505408, 5767168, 184549376, 0), (10576986112, 5767168, 190316544, 0), (10582753280, 5767168, 196083712, 0), (10571218944, 5767168, 201850880, 0), (9919528960, 5767168, 207618048, 0), (9925296128, 5767168, 213385216, 0), (9913761792, 5767168, 219152384, 0), (10490478592, 5767168, 224919552, 0), (10496245760, 5767168, 230686720, 0), (10484711424, 5767168, 236453888, 0), (10023337984, 5767168, 242221056, 0), (10029105152, 5767168, 247988224, 0), (10017570816, 5767168, 253755392, 0), (10040639488, 5767168, 259522560, 0), (10046406656, 5767168, 265289728, 0), (10034872320, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e342197d0>, 2: <capsule object NULL at 0x7a4e34219950>}
DEBUG 01-15 16:10:20.941108.941108 cuda_h.py:19] end restore2model cost 0.0019254684448242188 seconds
DEBUG 01-15 16:10:20.941204.941204 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.941374.941374 cuda_h.py:19] end sllm_worker_task cost 0.012838602066040039 seconds
DEBUG 01-15 16:10:20.941914.941914 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.941383.941383 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 79137bfd-b93b-4e86-9ed2-bad5bfb41b15
DEBUG 01-15 16:10:20.941145.941145 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.941249.941249 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:20.942171.942171 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:20.943248.943248 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 79137bfd-b93b-4e86-9ed2-bad5bfb41b15
DEBUG 01-15 16:10:20.943158.943158 cuda_h.py:19] end move_flatidxs cost 0.001032114028930664 seconds
DEBUG 01-15 16:10:20.943467.943467 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:20.943623.943623 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003953695297241211 seconds
DEBUG 01-15 16:10:20.943123.943123 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:20.947032.947032 cuda_h.py:19] end restore2model cost 0.003571748733520508 seconds
DEBUG 01-15 16:10:20.947770.947770 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007820844650268555 seconds
DEBUG 01-15 16:10:20.947162.947162 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:20.948430.948430 cuda_h.py:19] end gpu_sexperts cost 0.00041961669921875 seconds
DEBUG 01-15 16:10:20.948201.948201 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:20.948448.948448 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.7179718017578125e-05 seconds
DEBUG 01-15 16:10:20.948503.948503 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:20.948028.948028 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.949147.949147 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013353824615478516 seconds
DEBUG 01-15 16:10:20.949548.949548 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.950665.950665 cuda_h.py:19] end gpu_group_list cost 0.0002849102020263672 seconds
DEBUG 01-15 16:10:20.951313.951313 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:20.952914.952914 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014438629150390625 seconds
DEBUG 01-15 16:10:20.953414.953414 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:20.953823.953823 cuda_h.py:19] end gpu_group_list cost 0.0002875328063964844 seconds
DEBUG 01-15 16:10:20.954186.954186 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:20.954242.954242 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 79137bfd-b93b-4e86-9ed2-bad5bfb41b15
DEBUG 01-15 16:10:20.954773.954773 cuda_h.py:19] end group_tensors cost 0.010621786117553711 seconds
DEBUG 01-15 16:10:20.955926.955926 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:20.959536.959536 cuda_h.py:19] end group pad cost 0.004068851470947266 seconds
DEBUG 01-15 16:10:20.959432.959432 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:20.970205.970205 client.py:127] Model loaded
DEBUG 01-15 16:10:20.970774.970774 cuda_h.py:19] end wait_experts_multi_device cost 0.016010284423828125 seconds
DEBUG 01-15 16:10:20.970321.970321 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:20.979120.979120 cuda_h.py:19] end group_einsum cost 0.019861221313476562 seconds
DEBUG 01-15 16:10:20.980099.980099 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:20.983773.983773 cuda_h.py:19] end get_outputs_cpu1 cost 0.003823518753051758 seconds
DEBUG 01-15 16:10:20.984864.984864 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.042917490005493164 seconds
DEBUG 01-15 16:10:20.985864.985864 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.014920949935913086 seconds
DEBUG 01-15 16:10:20.985255.985255 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:20.988857.988857 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.988436.988436 cuda_h.py:19] end index_scatter cost 0.00014019012451171875 seconds
DEBUG 01-15 16:10:20.989869.989869 cuda_h.py:19] end cpuoutputsdeal cost 0.0032317638397216797 seconds
DEBUG 01-15 16:10:20.989941.989941 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:20.989487.989487 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.989314.989314 cuda_h.py:19] end gpu_group_tensor cost 0.00028324127197265625 seconds
DEBUG 01-15 16:10:20.989529.989529 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:20.990851.990851 cuda_h.py:19] end gpu_group_tensor cost 0.00026488304138183594 seconds
DEBUG 01-15 16:10:20.990090.990090 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.991713.991713 cuda_h.py:19] end gpu_group_einsum cost 0.0010838508605957031 seconds
DEBUG 01-15 16:10:20.991476.991476 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:20.992808.992808 cuda_h.py:19] end gpu_group_einsum cost 0.0008981227874755859 seconds
DEBUG 01-15 16:10:20.993942.993942 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.993340.993340 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.993979.993979 cuda_h.py:19] end all_expert_outputs_slices cost 0.00037169456481933594 seconds
DEBUG 01-15 16:10:20.993279.993279 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.994822.994822 cuda_h.py:19] end concat_expert_out cost 0.00011181831359863281 seconds
DEBUG 01-15 16:10:20.994734.994734 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.994397.994397 cuda_h.py:19] end index_scatter cost 0.00012087821960449219 seconds
DEBUG 01-15 16:10:20.994718.994718 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0016868114471435547 seconds
DEBUG 01-15 16:10:20.995281.995281 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:20.995649.995649 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:20.995529.995529 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002875328063964844 seconds
DEBUG 01-15 16:10:20.995485.995485 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:20.995551.995551 cuda_h.py:19] end concat_expert_out cost 0.00011444091796875 seconds
DEBUG 01-15 16:10:20.996357.996357 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:20.996391.996391 cuda_h.py:19] end index_scatter cost 0.00011563301086425781 seconds
DEBUG 01-15 16:10:20.996175.996175 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010943412780761719 seconds
DEBUG 01-15 16:10:20.996320.996320 cuda_h.py:19] end gpu_experts_multi_device cost 0.04807281494140625 seconds
DEBUG 01-15 16:10:20.996518.996518 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.060874223709106445 seconds
DEBUG 01-15 16:10:20.997343.997343 cuda_h.py:19] end prefill_layer cost 0.0696258544921875 seconds
DEBUG 01-15 16:10:20.997162.997162 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:10:20.997541.997541 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:20.997590.997590 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:10:20.997400.997400 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:20.997410.997410 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:20.997190.997190 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:10:20.998106.998106 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 0.0001652240753173828 seconds
DEBUG 01-15 16:10:20.998005.998005 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:20.998711.998711 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:20.998724.998724 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:20.998203.998203 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:20.999841.999841 cuda_h.py:19] end allocate_cuda_memory cost 0.0004031658172607422 seconds
DEBUG 01-15 16:10:20.999414.999414 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:20.999243.999243 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:20.999975.999975 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:20.999652.999652 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 01a105c8-7b9e-4931-b1c2-c02acccc4afd
DEBUG 01-15 16:10:20.999166.999166 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:20.999809.999809 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.000678.000678 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.001795.001795 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 01a105c8-7b9e-4931-b1c2-c02acccc4afd
DEBUG 01-15 16:10:21.001752.001752 cuda_h.py:19] end load_into_gpu_async cost 0.0017402172088623047 seconds
DEBUG 01-15 16:10:21.001575.001575 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.001348.001348 cuda_h.py:19] end restore_tensors2 cost 0.00010943412780761719 seconds
DEBUG 01-15 16:10:21.001316.001316 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002660512924194336 seconds
INFO 01-15 16:10:21.001201.001201 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 01a105c8-7b9e-4931-b1c2-c02acccc4afd
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.005021.005021 cuda_h.py:19] end self_attn cost 0.005199909210205078 seconds
DEBUG 01-15 16:10:21.006852.006852 cuda_h.py:19] end iln_self_attn_paln cost 0.0074427127838134766 seconds
DEBUG 01-15 16:10:21.006218.006218 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:10:21.006411.006411 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.006080.006080 cuda_h.py:19] end gate cost 0.0007367134094238281 seconds
DEBUG 01-15 16:10:21.007499.007499 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.007351.007351 lmp.py:1912] 
DEBUG 01-15 16:10:21.007351.007351 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.007306.007306 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.007962.007962 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.007519.007519 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.007216.007216 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.007839.007839 lmp.py:1917] 
DEBUG 01-15 16:10:21.007839.007839 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.007489.007489 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.007622.007622 lmp.py:1935]   Expert 38 |     13 | CPU
DEBUG 01-15 16:10:21.007842.007842 lmp.py:1935]   Expert 39 |     61 | CPU
DEBUG 01-15 16:10:21.007823.007823 lmp.py:1935]   Expert  7 |     71 | CPU
DEBUG 01-15 16:10:21.007803.007803 lmp.py:1935]   Expert 30 |     73 | CPU
DEBUG 01-15 16:10:21.007307.007307 lmp.py:1935]   Expert 24 |     93 | CPU
DEBUG 01-15 16:10:21.007811.007811 lmp.py:1935]   Expert 14 |     94 | CPU
DEBUG 01-15 16:10:21.007554.007554 lmp.py:1935]   Expert 27 |     94 | CPU
DEBUG 01-15 16:10:21.007581.007581 lmp.py:1935]   Expert 36 |     96 | CPU
DEBUG 01-15 16:10:21.007847.007847 lmp.py:1935]   Expert 40 |     98 | CPU
DEBUG 01-15 16:10:21.007112.007112 lmp.py:1935]   Expert 17 |     99 | CPU
DEBUG 01-15 16:10:21.007855.007855 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:10:21.007120.007120 lmp.py:1935]   Expert 32 |    106 | CPU
DEBUG 01-15 16:10:21.007386.007386 lmp.py:1935]   Expert 18 |    109 | CPU
DEBUG 01-15 16:10:21.007413.007413 lmp.py:1935]   Expert 48 |    111 | CPU
DEBUG 01-15 16:10:21.008632.008632 lmp.py:1935]   Expert 12 |    112 | CPU
DEBUG 01-15 16:10:21.008898.008898 lmp.py:1935]   Expert  1 |    115 | CPU
DEBUG 01-15 16:10:21.008640.008640 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:10:21.008429.008429 lmp.py:1935]   Expert 59 |    132 | CPU
DEBUG 01-15 16:10:21.008694.008694 lmp.py:1935]   Expert 42 |    136 | CPU
DEBUG 01-15 16:10:21.008960.008960 lmp.py:1935]   Expert  0 |    139 | CPU
DEBUG 01-15 16:10:21.008987.008987 lmp.py:1935]   Expert 22 |    145 | CPU
DEBUG 01-15 16:10:21.008776.008776 lmp.py:1935]   Expert 51 |    147 | CPU
DEBUG 01-15 16:10:21.008803.008803 lmp.py:1935]   Expert 53 |    148 | CPU
DEBUG 01-15 16:10:21.008830.008830 lmp.py:1935]   Expert  8 |    162 | CPU
DEBUG 01-15 16:10:21.008857.008857 lmp.py:1935]   Expert 44 |    164 | CPU
DEBUG 01-15 16:10:21.008361.008361 lmp.py:1935]   Expert 15 |    168 | CPU
DEBUG 01-15 16:10:21.008342.008342 lmp.py:1935]   Expert 60 |    169 | CPU
DEBUG 01-15 16:10:21.008489.008489 lmp.py:1935]   Expert 29 |    171 | CPU
INFO 01-15 16:10:21.008148.008148 client.py:127] Model loaded
DEBUG 01-15 16:10:21.008390.008390 lmp.py:1935]   Expert 54 |    173 | CPU
DEBUG 01-15 16:10:21.008758.008758 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.008548.008548 lmp.py:1935]   Expert 35 |    177 | CPU
DEBUG 01-15 16:10:21.009376.009376 cuda_h.py:19] end restore2model cost 0.0007271766662597656 seconds
DEBUG 01-15 16:10:21.009221.009221 lmp.py:1935]   Expert 33 |    181 | CPU
DEBUG 01-15 16:10:21.009794.009794 cuda_h.py:19] end sllm_worker_task cost 0.01120615005493164 seconds
DEBUG 01-15 16:10:21.009240.009240 lmp.py:1935]   Expert 34 |    182 | CPU
DEBUG 01-15 16:10:21.009199.009199 lmp.py:1935]   Expert 19 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.009186.009186 lmp.py:1935]   Expert 47 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.009359.009359 lmp.py:1935]   Expert  9 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.009910.009910 lmp.py:1935]   Expert  3 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.009029.009029 lmp.py:1935]   Expert 46 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010388.010388 lmp.py:1935]   Expert 56 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010269.010269 lmp.py:1935]   Expert 21 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010627.010627 lmp.py:1935]   Expert 45 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010509.010509 lmp.py:1935]   Expert 20 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010629.010629 lmp.py:1935]   Expert 49 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010033.010033 lmp.py:1935]   Expert 28 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010915.010915 lmp.py:1935]   Expert 57 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010319.010319 lmp.py:1935]   Expert  2 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010200.010200 lmp.py:1935]   Expert 13 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010274.010274 lmp.py:1935]   Expert 43 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010632.010632 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010752.010752 lmp.py:1935]   Expert 10 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010587.010587 lmp.py:1935]   Expert 50 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010707.010707 lmp.py:1935]   Expert 41 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010111.010111 lmp.py:1935]   Expert 26 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010754.010754 lmp.py:1935]   Expert 63 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010397.010397 lmp.py:1935]   Expert 37 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010040.010040 lmp.py:1935]   Expert 31 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010445.010445 lmp.py:1935]   Expert 61 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010088.010088 lmp.py:1935]   Expert 52 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010492.010492 lmp.py:1935]   Expert 58 |    323 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010566.010566 lmp.py:1935]   Expert 62 |    323 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010401.010401 lmp.py:1935]   Expert 55 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010998.010998 lmp.py:1935]   Expert 11 |    380 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010117.010117 lmp.py:1935]   Expert 23 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010237.010237 lmp.py:1935]   Expert 25 |    405 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.010880.010880 lmp.py:1935]   Expert  5 |    515 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.010331.010331 lmp.py:1937] 
DEBUG 01-15 16:10:21.010331.010331 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.010451.010451 lmp.py:1938]   CPU:   3970 tokens
DEBUG 01-15 16:10:21.010571.010571 lmp.py:1942]   cuda:1:   4173 tokens (16 experts)
DEBUG 01-15 16:10:21.010214.010214 lmp.py:1942]   cuda:2:   4145 tokens (16 experts)
DEBUG 01-15 16:10:21.010380.010380 lmp.py:1943]   Total GPU:   8318 tokens
DEBUG 01-15 16:10:21.010308.010308 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.010308.010308 lmp.py:1944] 
DEBUG 01-15 16:10:21.010302.010302 cuda_h.py:19] end experts_map_get cost 0.0035746097564697266 seconds
DEBUG 01-15 16:10:21.010026.010026 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.010643.010643 lmp.py:1953] 
DEBUG 01-15 16:10:21.010643.010643 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.010917.010917 cuda_h.py:19] end cpu_experts_submit cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:10:21.010852.010852 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.010218.010218 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.011512.011512 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.5.gate_proj.weight': 0, 'model.layers.8.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.41.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.41.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.41.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.43.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.43.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.43.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.52.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.52.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.52.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.63.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.57.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(10698096640, 5767168, 0, 0), (10703863808, 5767168, 5767168, 0), (10692329472, 5767168, 11534336, 0), (11320950784, 5767168, 17301504, 0), (11326717952, 5767168, 23068672, 0), (11315183616, 5767168, 28835840, 0), (10784604160, 5767168, 34603008, 0), (10790371328, 5767168, 40370176, 0), (10778836992, 5767168, 46137344, 0), (10801905664, 5767168, 51904512, 0), (10807672832, 5767168, 57671680, 0), (10796138496, 5767168, 63438848, 0), (11355553792, 5767168, 69206016, 0), (11361320960, 5767168, 74973184, 0), (11349786624, 5767168, 80740352, 0), (10836508672, 5767168, 86507520, 0), (10842275840, 5767168, 92274688, 0), (10830741504, 5767168, 98041856, 0), (10767302656, 5767168, 103809024, 0), (10773069824, 5767168, 109576192, 0), (10761535488, 5767168, 115343360, 0), (10940317696, 5767168, 121110528, 0), (10946084864, 5767168, 126877696, 0), (10934550528, 5767168, 132644864, 0), (11511267328, 5767168, 138412032, 0), (11517034496, 5767168, 144179200, 0), (11505500160, 5767168, 149946368, 0), (10957619200, 5767168, 155713536, 0), (10963386368, 5767168, 161480704, 0), (10951852032, 5767168, 167247872, 0), (10974920704, 5767168, 173015040, 0), (10980687872, 5767168, 178782208, 0), (10969153536, 5767168, 184549376, 0), (11580473344, 5767168, 190316544, 0), (11586240512, 5767168, 196083712, 0), (11574706176, 5767168, 201850880, 0), (11615076352, 5767168, 207618048, 0), (11620843520, 5767168, 213385216, 0), (11609309184, 5767168, 219152384, 0), (11096031232, 5767168, 224919552, 0), (11101798400, 5767168, 230686720, 0), (11090264064, 5767168, 236453888, 0), (11701583872, 5767168, 242221056, 0), (11707351040, 5767168, 247988224, 0), (11695816704, 5767168, 253755392, 0), (11147935744, 5767168, 259522560, 0), (11153702912, 5767168, 265289728, 0), (11142168576, 5767168, 271056896, 0)], 2: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (10663493632, 5767168, 17301504, 0), (10669260800, 5767168, 23068672, 0), (10657726464, 5767168, 28835840, 0), (10680795136, 5767168, 34603008, 0), (10686562304, 5767168, 40370176, 0), (10675027968, 5767168, 46137344, 0), (11251744768, 5767168, 51904512, 0), (11257511936, 5767168, 57671680, 0), (11245977600, 5767168, 63438848, 0), (11390156800, 5767168, 69206016, 0), (11395923968, 5767168, 74973184, 0), (11384389632, 5767168, 80740352, 0), (11407458304, 5767168, 86507520, 0), (11413225472, 5767168, 92274688, 0), (11401691136, 5767168, 98041856, 0), (11424759808, 5767168, 103809024, 0), (11430526976, 5767168, 109576192, 0), (11418992640, 5767168, 115343360, 0), (11459362816, 5767168, 121110528, 0), (11465129984, 5767168, 126877696, 0), (11453595648, 5767168, 132644864, 0), (11476664320, 5767168, 138412032, 0), (11482431488, 5767168, 144179200, 0), (11470897152, 5767168, 149946368, 0), (11009523712, 5767168, 155713536, 0), (11015290880, 5767168, 161480704, 0), (11003756544, 5767168, 167247872, 0), (11563171840, 5767168, 173015040, 0), (11568939008, 5767168, 178782208, 0), (11557404672, 5767168, 184549376, 0), (11044126720, 5767168, 190316544, 0), (11049893888, 5767168, 196083712, 0), (11038359552, 5767168, 201850880, 0), (11061428224, 5767168, 207618048, 0), (11067195392, 5767168, 213385216, 0), (11055661056, 5767168, 219152384, 0), (11666980864, 5767168, 224919552, 0), (11672748032, 5767168, 230686720, 0), (11661213696, 5767168, 236453888, 0), (11684282368, 5767168, 242221056, 0), (11690049536, 5767168, 247988224, 0), (11678515200, 5767168, 253755392, 0), (11597774848, 5767168, 259522560, 0), (11603542016, 5767168, 265289728, 0), (11592007680, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec424c720>, 2: <capsule object NULL at 0x7a4e34218f60>}
DEBUG 01-15 16:10:21.011180.011180 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.011547.011547 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.012018.012018 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 506f7380-1877-449b-9b8c-edc0b147c7be
DEBUG 01-15 16:10:21.012740.012740 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.012700.012700 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.012282.012282 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.013558.013558 cuda_h.py:19] end move_flatidxs cost 0.0008559226989746094 seconds
DEBUG 01-15 16:10:21.013825.013825 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:21.013501.013501 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 506f7380-1877-449b-9b8c-edc0b147c7be
DEBUG 01-15 16:10:21.014855.014855 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0033843517303466797 seconds
DEBUG 01-15 16:10:21.014256.014256 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.018490.018490 cuda_h.py:19] end restore2model cost 0.003600597381591797 seconds
DEBUG 01-15 16:10:21.018328.018328 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007274627685546875 seconds
DEBUG 01-15 16:10:21.018197.018197 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.018471.018471 cuda_h.py:19] end gpu_sexperts cost 0.00042700767517089844 seconds
DEBUG 01-15 16:10:21.018765.018765 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.018874.018874 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.7418136596679688e-05 seconds
DEBUG 01-15 16:10:21.018644.018644 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.018420.018420 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.020727.020727 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016138553619384766 seconds
DEBUG 01-15 16:10:21.020048.020048 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.020318.020318 cuda_h.py:19] end gpu_group_list cost 0.00028824806213378906 seconds
DEBUG 01-15 16:10:21.022881.022881 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.023369.023369 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014293193817138672 seconds
DEBUG 01-15 16:10:21.023776.023776 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.023547.023547 cuda_h.py:19] end group_tensors cost 0.009649038314819336 seconds
DEBUG 01-15 16:10:21.024138.024138 cuda_h.py:19] end gpu_group_list cost 0.00028777122497558594 seconds
DEBUG 01-15 16:10:21.024572.024572 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.025267.025267 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.025080.025080 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 506f7380-1877-449b-9b8c-edc0b147c7be
DEBUG 01-15 16:10:21.028502.028502 cuda_h.py:19] end group pad cost 0.003788471221923828 seconds
DEBUG 01-15 16:10:21.028531.028531 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.040358.040358 client.py:127] Model loaded
DEBUG 01-15 16:10:21.040465.040465 cuda_h.py:19] end wait_experts_multi_device cost 0.015235662460327148 seconds
DEBUG 01-15 16:10:21.040759.040759 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.050057.050057 cuda_h.py:19] end group_einsum cost 0.022346019744873047 seconds
DEBUG 01-15 16:10:21.050950.050950 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.054281.054281 cuda_h.py:19] end get_outputs_cpu1 cost 0.003900766372680664 seconds
DEBUG 01-15 16:10:21.055643.055643 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04308509826660156 seconds
DEBUG 01-15 16:10:21.056174.056174 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.015458822250366211 seconds
DEBUG 01-15 16:10:21.056565.056565 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.059510.059510 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.059248.059248 cuda_h.py:19] end index_scatter cost 0.0001513957977294922 seconds
DEBUG 01-15 16:10:21.059226.059226 cuda_h.py:19] end cpuoutputsdeal cost 0.003227710723876953 seconds
DEBUG 01-15 16:10:21.060066.060066 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.060850.060850 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.060611.060611 cuda_h.py:19] end gpu_group_tensor cost 0.0003018379211425781 seconds
DEBUG 01-15 16:10:21.060641.060641 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.061103.061103 cuda_h.py:19] end gpu_group_tensor cost 0.00026154518127441406 seconds
DEBUG 01-15 16:10:21.061388.061388 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.062706.062706 cuda_h.py:19] end gpu_group_einsum cost 0.0010690689086914062 seconds
DEBUG 01-15 16:10:21.062853.062853 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.063257.063257 cuda_h.py:19] end gpu_group_einsum cost 0.0008783340454101562 seconds
DEBUG 01-15 16:10:21.063630.063630 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.064690.064690 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.064594.064594 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003902912139892578 seconds
DEBUG 01-15 16:10:21.064994.064994 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.064921.064921 cuda_h.py:19] end concat_expert_out cost 0.00011301040649414062 seconds
DEBUG 01-15 16:10:21.065933.065933 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.065887.065887 cuda_h.py:19] end index_scatter cost 0.00012302398681640625 seconds
DEBUG 01-15 16:10:21.065247.065247 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001710653305053711 seconds
DEBUG 01-15 16:10:21.065717.065717 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.066987.066987 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.066059.066059 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002906322479248047 seconds
DEBUG 01-15 16:10:21.066922.066922 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.066657.066657 cuda_h.py:19] end concat_expert_out cost 0.00011563301086425781 seconds
DEBUG 01-15 16:10:21.066324.066324 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.067218.067218 cuda_h.py:19] end index_scatter cost 0.00011587142944335938 seconds
DEBUG 01-15 16:10:21.067817.067817 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011055469512939453 seconds
DEBUG 01-15 16:10:21.067067.067067 cuda_h.py:19] end gpu_experts_multi_device cost 0.04840445518493652 seconds
DEBUG 01-15 16:10:21.067212.067212 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.06129336357116699 seconds
DEBUG 01-15 16:10:21.068381.068381 cuda_h.py:19] end prefill_layer cost 0.07048368453979492 seconds
DEBUG 01-15 16:10:21.068107.068107 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:10:21.068771.068771 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.068463.068463 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:10:21.068273.068273 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:21.068998.068998 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:21.068109.068109 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 7.009506225585938e-05 seconds
DEBUG 01-15 16:10:21.068359.068359 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.069780.069780 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 0.0003559589385986328 seconds
DEBUG 01-15 16:10:21.069925.069925 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.069857.069857 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.069776.069776 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.070701.070701 cuda_h.py:19] end allocate_cuda_memory cost 0.00033974647521972656 seconds
DEBUG 01-15 16:10:21.070069.070069 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.070884.070884 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.070628.070628 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.070338.070338 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3387f8b5-10b7-45b3-944f-b4f2248ec447
DEBUG 01-15 16:10:21.070249.070249 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.070933.070933 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.071763.071763 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.071204.071204 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3387f8b5-10b7-45b3-944f-b4f2248ec447
DEBUG 01-15 16:10:21.071995.071995 cuda_h.py:19] end load_into_gpu_async cost 0.00151824951171875 seconds
DEBUG 01-15 16:10:21.071228.071228 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.071351.071351 cuda_h.py:19] end restore_tensors2 cost 9.512901306152344e-05 seconds
DEBUG 01-15 16:10:21.071207.071207 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024411678314208984 seconds
INFO 01-15 16:10:21.071262.071262 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3387f8b5-10b7-45b3-944f-b4f2248ec447
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.076053.076053 cuda_h.py:19] end self_attn cost 0.0049893856048583984 seconds
DEBUG 01-15 16:10:21.076904.076904 cuda_h.py:19] end iln_self_attn_paln cost 0.007064104080200195 seconds
DEBUG 01-15 16:10:21.076562.076562 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:10:21.076993.076993 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.077589.077589 cuda_h.py:19] end gate cost 0.0007531642913818359 seconds
DEBUG 01-15 16:10:21.077916.077916 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.078589.078589 lmp.py:1912] 
DEBUG 01-15 16:10:21.078589.078589 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.078730.078730 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.078976.078976 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.078672.078672 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.078746.078746 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.078342.078342 lmp.py:1917] 
DEBUG 01-15 16:10:21.078342.078342 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.078939.078939 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.078496.078496 lmp.py:1935]   Expert 24 |     39 | CPU
DEBUG 01-15 16:10:21.078093.078093 lmp.py:1935]   Expert  2 |     46 | CPU
DEBUG 01-15 16:10:21.078689.078689 lmp.py:1935]   Expert 26 |     65 | CPU
DEBUG 01-15 16:10:21.078809.078809 lmp.py:1935]   Expert 32 |     65 | CPU
DEBUG 01-15 16:10:21.078406.078406 lmp.py:1935]   Expert 19 |     67 | CPU
DEBUG 01-15 16:10:21.078241.078241 lmp.py:1935]   Expert 50 |     70 | CPU
DEBUG 01-15 16:10:21.078897.078897 lmp.py:1935]   Expert 15 |     79 | CPU
DEBUG 01-15 16:10:21.078732.078732 lmp.py:1935]   Expert  7 |     82 | CPU
DEBUG 01-15 16:10:21.078614.078614 lmp.py:1935]   Expert 60 |     82 | CPU
DEBUG 01-15 16:10:21.078257.078257 lmp.py:1935]   Expert  4 |     83 | CPU
DEBUG 01-15 16:10:21.078377.078377 lmp.py:1935]   Expert 28 |     83 | CPU
DEBUG 01-15 16:10:21.078781.078781 lmp.py:1935]   Expert 59 |     88 | CPU
DEBUG 01-15 16:10:21.078663.078663 lmp.py:1935]   Expert 23 |     98 | CPU
DEBUG 01-15 16:10:21.078405.078405 lmp.py:1935]   Expert 49 |     98 | CPU
DEBUG 01-15 16:10:21.078909.078909 lmp.py:1935]   Expert  5 |    103 | CPU
DEBUG 01-15 16:10:21.078890.078890 lmp.py:1935]   Expert 12 |    104 | CPU
DEBUG 01-15 16:10:21.078347.078347 lmp.py:1935]   Expert 10 |    110 | CPU
DEBUG 01-15 16:10:21.078328.078328 lmp.py:1935]   Expert 27 |    111 | CPU
DEBUG 01-15 16:10:21.078548.078548 lmp.py:1935]   Expert 41 |    121 | CPU
DEBUG 01-15 16:10:21.078052.078052 lmp.py:1935]   Expert  3 |    125 | CPU
DEBUG 01-15 16:10:21.078317.078317 lmp.py:1935]   Expert 25 |    128 | CPU
DEBUG 01-15 16:10:21.078060.078060 lmp.py:1935]   Expert 40 |    128 | CPU
DEBUG 01-15 16:10:21.078564.078564 lmp.py:1935]   Expert 20 |    130 | CPU
DEBUG 01-15 16:10:21.078068.078068 lmp.py:1935]   Expert 16 |    132 | CPU
DEBUG 01-15 16:10:21.078095.078095 lmp.py:1935]   Expert 13 |    133 | CPU
DEBUG 01-15 16:10:21.078599.078599 lmp.py:1935]   Expert 37 |    144 | CPU
DEBUG 01-15 16:10:21.078580.078580 lmp.py:1935]   Expert 35 |    145 | CPU
DEBUG 01-15 16:10:21.078560.078560 lmp.py:1935]   Expert 17 |    148 | CPU
DEBUG 01-15 16:10:21.078064.078064 lmp.py:1935]   Expert 47 |    151 | CPU
DEBUG 01-15 16:10:21.078522.078522 lmp.py:1935]   Expert 22 |    159 | CPU
DEBUG 01-15 16:10:21.078026.078026 lmp.py:1935]   Expert 53 |    168 | CPU
DEBUG 01-15 16:10:21.078768.078768 lmp.py:1935]   Expert 39 |    171 | CPU
DEBUG 01-15 16:10:21.078941.078941 lmp.py:1935]   Expert 38 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.078114.078114 lmp.py:1935]   Expert 44 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079334.079334 lmp.py:1935]   Expert 36 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079268.079268 lmp.py:1935]   Expert 52 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079249.079249 lmp.py:1935]   Expert 58 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079707.079707 lmp.py:1935]   Expert 18 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079403.079403 lmp.py:1935]   Expert 62 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079337.079337 lmp.py:1935]   Expert 11 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079272.079272 lmp.py:1935]   Expert 48 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079206.079206 lmp.py:1935]   Expert 14 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079949.079949 lmp.py:1935]   Expert 30 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079929.079929 lmp.py:1935]   Expert  1 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079910.079910 lmp.py:1935]   Expert 45 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079891.079891 lmp.py:1935]   Expert 42 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079872.079872 lmp.py:1935]   Expert 31 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079853.079853 lmp.py:1935]   Expert  6 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079072.079072 lmp.py:1935]   Expert 51 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079007.079007 lmp.py:1935]   Expert 29 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079703.079703 lmp.py:1935]   Expert 34 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079160.079160 lmp.py:1935]   Expert 33 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079380.079380 lmp.py:1935]   Expert 57 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079837.079837 lmp.py:1935]   Expert 61 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079818.079818 lmp.py:1935]   Expert 43 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079514.079514 lmp.py:1935]   Expert  0 |    323 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079210.079210 lmp.py:1935]   Expert 46 |    350 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079383.079383 lmp.py:1935]   Expert  8 |    380 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079841.079841 lmp.py:1935]   Expert  9 |    391 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079014.079014 lmp.py:1935]   Expert 56 |    391 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079995.079995 lmp.py:1935]   Expert 54 |    396 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079452.079452 lmp.py:1935]   Expert 63 |    406 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079195.079195 lmp.py:1935]   Expert 55 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.079414.079414 lmp.py:1935]   Expert 21 |    490 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.079203.079203 lmp.py:1937] 
DEBUG 01-15 16:10:21.079203.079203 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.079945.079945 lmp.py:1938]   CPU:   3456 tokens
DEBUG 01-15 16:10:21.079356.079356 lmp.py:1942]   cuda:1:   4415 tokens (16 experts)
DEBUG 01-15 16:10:21.079020.079020 lmp.py:1942]   cuda:2:   4417 tokens (16 experts)
DEBUG 01-15 16:10:21.079762.079762 lmp.py:1943]   Total GPU:   8832 tokens
DEBUG 01-15 16:10:21.079313.079313 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.079313.079313 lmp.py:1944] 
DEBUG 01-15 16:10:21.079539.079539 cuda_h.py:19] end experts_map_get cost 0.0021483898162841797 seconds
INFO 01-15 16:10:21.079509.079509 client.py:127] Model loaded
DEBUG 01-15 16:10:21.080599.080599 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.080821.080821 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.080094.080094 lmp.py:1953] 
DEBUG 01-15 16:10:21.080094.080094 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.080307.080307 cuda_h.py:19] end cpu_experts_submit cost 0.0006051063537597656 seconds
DEBUG 01-15 16:10:21.080726.080726 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.080126.080126 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.081914.081914 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.34.gate_proj.weight': 0, 'model.layers.9.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.43.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.43.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.43.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.54.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.54.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.54.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.62.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.62.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.62.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.1.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.1.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.1.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.11.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.11.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.11.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.44.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.44.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.44.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(12307136512, 5767168, 0, 0), (12312903680, 5767168, 5767168, 0), (12301369344, 5767168, 11534336, 0), (12341739520, 5767168, 17301504, 0), (12347506688, 5767168, 23068672, 0), (12335972352, 5767168, 28835840, 0), (12376342528, 5767168, 34603008, 0), (12382109696, 5767168, 40370176, 0), (12370575360, 5767168, 46137344, 0), (12462850048, 5767168, 51904512, 0), (12468617216, 5767168, 57671680, 0), (12457082880, 5767168, 63438848, 0), (12497453056, 5767168, 69206016, 0), (12503220224, 5767168, 74973184, 0), (12491685888, 5767168, 80740352, 0), (12514754560, 5767168, 86507520, 0), (12520521728, 5767168, 92274688, 0), (12508987392, 5767168, 98041856, 0), (11961106432, 5767168, 103809024, 0), (11966873600, 5767168, 109576192, 0), (11955339264, 5767168, 115343360, 0), (12030312448, 5767168, 121110528, 0), (12036079616, 5767168, 126877696, 0), (12024545280, 5767168, 132644864, 0), (12082216960, 5767168, 138412032, 0), (12087984128, 5767168, 144179200, 0), (12076449792, 5767168, 149946368, 0), (12653166592, 5767168, 155713536, 0), (12658933760, 5767168, 161480704, 0), (12647399424, 5767168, 167247872, 0), (12791578624, 5767168, 173015040, 0), (12797345792, 5767168, 178782208, 0), (12785811456, 5767168, 184549376, 0), (12687769600, 5767168, 190316544, 0), (12693536768, 5767168, 196083712, 0), (12682002432, 5767168, 201850880, 0), (12705071104, 5767168, 207618048, 0), (12710838272, 5767168, 213385216, 0), (12699303936, 5767168, 219152384, 0), (12220628992, 5767168, 224919552, 0), (12226396160, 5767168, 230686720, 0), (12214861824, 5767168, 236453888, 0), (12237930496, 5767168, 242221056, 0), (12243697664, 5767168, 247988224, 0), (12232163328, 5767168, 253755392, 0), (12255232000, 5767168, 259522560, 0), (12260999168, 5767168, 265289728, 0), (12249464832, 5767168, 271056896, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (12289835008, 5767168, 17301504, 0), (12295602176, 5767168, 23068672, 0), (12284067840, 5767168, 28835840, 0), (11736186880, 5767168, 34603008, 0), (11741954048, 5767168, 40370176, 0), (11730419712, 5767168, 46137344, 0), (11822694400, 5767168, 51904512, 0), (11828461568, 5767168, 57671680, 0), (11816927232, 5767168, 63438848, 0), (11857297408, 5767168, 69206016, 0), (11863064576, 5767168, 74973184, 0), (11851530240, 5767168, 80740352, 0), (11874598912, 5767168, 86507520, 0), (11880366080, 5767168, 92274688, 0), (11868831744, 5767168, 98041856, 0), (12445548544, 5767168, 103809024, 0), (12451315712, 5767168, 109576192, 0), (12439781376, 5767168, 115343360, 0), (11909201920, 5767168, 121110528, 0), (11914969088, 5767168, 126877696, 0), (11903434752, 5767168, 132644864, 0), (12480151552, 5767168, 138412032, 0), (12485918720, 5767168, 144179200, 0), (12474384384, 5767168, 149946368, 0), (12549357568, 5767168, 155713536, 0), (12555124736, 5767168, 161480704, 0), (12543590400, 5767168, 167247872, 0), (12601262080, 5767168, 173015040, 0), (12607029248, 5767168, 178782208, 0), (12595494912, 5767168, 184549376, 0), (12618563584, 5767168, 190316544, 0), (12624330752, 5767168, 196083712, 0), (12612796416, 5767168, 201850880, 0), (12670468096, 5767168, 207618048, 0), (12676235264, 5767168, 213385216, 0), (12664700928, 5767168, 219152384, 0), (12722372608, 5767168, 224919552, 0), (12728139776, 5767168, 230686720, 0), (12716605440, 5767168, 236453888, 0), (12774277120, 5767168, 242221056, 0), (12780044288, 5767168, 247988224, 0), (12768509952, 5767168, 253755392, 0), (12808880128, 5767168, 259522560, 0), (12814647296, 5767168, 265289728, 0), (12803112960, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e342198f0>, 2: <capsule object NULL at 0x7a4e34219890>}
DEBUG 01-15 16:10:21.082136.082136 cuda_h.py:19] end restore2model cost 0.002153635025024414 seconds
DEBUG 01-15 16:10:21.082972.082972 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.082160.082160 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.082780.082780 cuda_h.py:19] end sllm_worker_task cost 0.013467550277709961 seconds
DEBUG 01-15 16:10:21.082605.082605 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.082338.082338 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6bf42281-5ac8-4ac3-acac-2c538a8e3728
DEBUG 01-15 16:10:21.082036.082036 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.083453.083453 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.083107.083107 cuda_h.py:19] end move_flatidxs cost 0.0008575916290283203 seconds
DEBUG 01-15 16:10:21.083095.083095 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:21.084140.084140 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6bf42281-5ac8-4ac3-acac-2c538a8e3728
DEBUG 01-15 16:10:21.084344.084344 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003867626190185547 seconds
DEBUG 01-15 16:10:21.084320.084320 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.088940.088940 cuda_h.py:19] end restore2model cost 0.0032587051391601562 seconds
DEBUG 01-15 16:10:21.088744.088744 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00741887092590332 seconds
DEBUG 01-15 16:10:21.088930.088930 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.088658.088658 cuda_h.py:19] end gpu_sexperts cost 0.00031065940856933594 seconds
DEBUG 01-15 16:10:21.088733.088733 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.088854.088854 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.288818359375e-05 seconds
DEBUG 01-15 16:10:21.088980.088980 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.088590.088590 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.089137.089137 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008573532104492188 seconds
DEBUG 01-15 16:10:21.089649.089649 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.089442.089442 cuda_h.py:19] end group_tensors cost 0.0053708553314208984 seconds
DEBUG 01-15 16:10:21.089861.089861 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.090517.090517 cuda_h.py:19] end gpu_group_list cost 0.00018596649169921875 seconds
DEBUG 01-15 16:10:21.091635.091635 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.092744.092744 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001367807388305664 seconds
DEBUG 01-15 16:10:21.092727.092727 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.093951.093951 cuda_h.py:19] end gpu_group_list cost 0.00029587745666503906 seconds
DEBUG 01-15 16:10:21.094103.094103 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.094649.094649 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6bf42281-5ac8-4ac3-acac-2c538a8e3728
DEBUG 01-15 16:10:21.094066.094066 cuda_h.py:19] end group pad cost 0.0043125152587890625 seconds
DEBUG 01-15 16:10:21.094956.094956 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.110819.110819 client.py:127] Model loaded
DEBUG 01-15 16:10:21.110337.110337 cuda_h.py:19] end wait_experts_multi_device cost 0.016121387481689453 seconds
DEBUG 01-15 16:10:21.110606.110606 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.115388.115388 cuda_h.py:19] end group_einsum cost 0.02074122428894043 seconds
DEBUG 01-15 16:10:21.115698.115698 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.118757.118757 cuda_h.py:19] end get_outputs_cpu1 cost 0.003477334976196289 seconds
DEBUG 01-15 16:10:21.119285.119285 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03715991973876953 seconds
DEBUG 01-15 16:10:21.120185.120185 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010052204132080078 seconds
DEBUG 01-15 16:10:21.120886.120886 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.123289.123289 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.123570.123570 cuda_h.py:19] end index_scatter cost 0.00013327598571777344 seconds
DEBUG 01-15 16:10:21.123514.123514 cuda_h.py:19] end cpuoutputsdeal cost 0.0030367374420166016 seconds
DEBUG 01-15 16:10:21.124287.124287 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.124064.124064 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.124965.124965 cuda_h.py:19] end gpu_group_tensor cost 0.0003056526184082031 seconds
DEBUG 01-15 16:10:21.124895.124895 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.124952.124952 cuda_h.py:19] end gpu_group_tensor cost 0.00024819374084472656 seconds
DEBUG 01-15 16:10:21.125647.125647 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.126406.126406 cuda_h.py:19] end gpu_group_einsum cost 0.0009799003601074219 seconds
DEBUG 01-15 16:10:21.126340.126340 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.127325.127325 cuda_h.py:19] end gpu_group_einsum cost 0.0008285045623779297 seconds
DEBUG 01-15 16:10:21.127247.127247 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.127525.127525 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.128308.128308 cuda_h.py:19] end all_expert_outputs_slices cost 0.00034618377685546875 seconds
DEBUG 01-15 16:10:21.128979.128979 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.128761.128761 cuda_h.py:19] end concat_expert_out cost 0.00010514259338378906 seconds
DEBUG 01-15 16:10:21.128706.128706 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.128301.128301 cuda_h.py:19] end index_scatter cost 0.00011324882507324219 seconds
DEBUG 01-15 16:10:21.129867.129867 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001954793930053711 seconds
DEBUG 01-15 16:10:21.130172.130172 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.130469.130469 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.130808.130808 cuda_h.py:19] end all_expert_outputs_slices cost 0.00033545494079589844 seconds
DEBUG 01-15 16:10:21.130208.130208 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.131687.131687 cuda_h.py:19] end concat_expert_out cost 0.00016689300537109375 seconds
DEBUG 01-15 16:10:21.131534.131534 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.131396.131396 cuda_h.py:19] end index_scatter cost 0.000156402587890625 seconds
DEBUG 01-15 16:10:21.131201.131201 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0013418197631835938 seconds
DEBUG 01-15 16:10:21.131320.131320 cuda_h.py:19] end gpu_experts_multi_device cost 0.04294633865356445 seconds
DEBUG 01-15 16:10:21.131254.131254 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.055239200592041016 seconds
DEBUG 01-15 16:10:21.132897.132897 cuda_h.py:19] end prefill_layer cost 0.06438755989074707 seconds
DEBUG 01-15 16:10:21.132021.132021 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:10:21.133262.133262 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.133416.133416 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:10:21.133803.133803 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:21.133720.133720 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:21.133030.133030 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 8.106231689453125e-05 seconds
DEBUG 01-15 16:10:21.133218.133218 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.133003.133003 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 0.0002791881561279297 seconds
DEBUG 01-15 16:10:21.133662.133662 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.133585.133585 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.133714.133714 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.134478.134478 cuda_h.py:19] end allocate_cuda_memory cost 0.0003306865692138672 seconds
DEBUG 01-15 16:10:21.134904.134904 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.134905.134905 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.134880.134880 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.134921.134921 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0df31209-0f53-4d3e-8a99-e90ed1b7345a
DEBUG 01-15 16:10:21.134434.134434 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.134422.134422 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.135673.135673 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.135974.135974 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0df31209-0f53-4d3e-8a99-e90ed1b7345a
DEBUG 01-15 16:10:21.135963.135963 cuda_h.py:19] end load_into_gpu_async cost 0.0015170574188232422 seconds
DEBUG 01-15 16:10:21.135997.135997 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.136808.136808 cuda_h.py:19] end restore_tensors2 cost 7.748603820800781e-05 seconds
DEBUG 01-15 16:10:21.136279.136279 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002276182174682617 seconds
INFO 01-15 16:10:21.136864.136864 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0df31209-0f53-4d3e-8a99-e90ed1b7345a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.142423.142423 cuda_h.py:19] end self_attn cost 0.006448507308959961 seconds
DEBUG 01-15 16:10:21.142976.142976 cuda_h.py:19] end iln_self_attn_paln cost 0.008709907531738281 seconds
DEBUG 01-15 16:10:21.142011.142011 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:10:21.142972.142972 cuda_h.py:10] start gate
INFO 01-15 16:10:21.143338.143338 client.py:127] Model loaded
DEBUG 01-15 16:10:21.143321.143321 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.143545.143545 cuda_h.py:19] end restore2model cost 0.0005118846893310547 seconds
DEBUG 01-15 16:10:21.143560.143560 cuda_h.py:19] end sllm_worker_task cost 0.010015249252319336 seconds
DEBUG 01-15 16:10:21.144341.144341 cuda_h.py:19] end gate cost 0.0012593269348144531 seconds
DEBUG 01-15 16:10:21.144517.144517 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.144502.144502 lmp.py:1912] 
DEBUG 01-15 16:10:21.144502.144502 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.144788.144788 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.144107.144107 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.144041.144041 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.144307.144307 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.144380.144380 lmp.py:1917] 
DEBUG 01-15 16:10:21.144380.144380 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.144646.144646 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.144872.144872 lmp.py:1935]   Expert 43 |     16 | CPU
DEBUG 01-15 16:10:21.144945.144945 lmp.py:1935]   Expert 27 |     31 | CPU
DEBUG 01-15 16:10:21.144065.144065 lmp.py:1935]   Expert 26 |     52 | CPU
DEBUG 01-15 16:10:21.144423.144423 lmp.py:1935]   Expert 34 |     53 | CPU
DEBUG 01-15 16:10:21.144066.144066 lmp.py:1935]   Expert 56 |     53 | CPU
DEBUG 01-15 16:10:21.145948.145948 lmp.py:1935]   Expert  3 |     57 | CPU
DEBUG 01-15 16:10:21.145591.145591 lmp.py:1935]   Expert  4 |     67 | CPU
DEBUG 01-15 16:10:21.145234.145234 lmp.py:1935]   Expert 61 |     81 | CPU
DEBUG 01-15 16:10:21.145877.145877 lmp.py:1935]   Expert 14 |     94 | CPU
DEBUG 01-15 16:10:21.145427.145427 lmp.py:1935]   Expert 38 |    100 | CPU
DEBUG 01-15 16:10:21.145262.145262 lmp.py:1935]   Expert  2 |    113 | CPU
DEBUG 01-15 16:10:21.145143.145143 lmp.py:1935]   Expert 17 |    119 | CPU
DEBUG 01-15 16:10:21.145310.145310 lmp.py:1935]   Expert 22 |    123 | CPU
DEBUG 01-15 16:10:21.145476.145476 lmp.py:1935]   Expert 37 |    129 | CPU
DEBUG 01-15 16:10:21.145880.145880 lmp.py:1935]   Expert 47 |    129 | CPU
DEBUG 01-15 16:10:21.145285.145285 lmp.py:1935]   Expert 55 |    131 | CPU
DEBUG 01-15 16:10:21.145451.145451 lmp.py:1935]   Expert 54 |    135 | CPU
DEBUG 01-15 16:10:21.145379.145379 lmp.py:1935]   Expert 28 |    137 | CPU
DEBUG 01-15 16:10:21.145545.145545 lmp.py:1935]   Expert  7 |    143 | CPU
DEBUG 01-15 16:10:21.145711.145711 lmp.py:1935]   Expert 48 |    145 | CPU
DEBUG 01-15 16:10:21.145877.145877 lmp.py:1935]   Expert 15 |    146 | CPU
DEBUG 01-15 16:10:21.145520.145520 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:10:21.145878.145878 lmp.py:1935]   Expert 51 |    148 | CPU
DEBUG 01-15 16:10:21.145952.145952 lmp.py:1935]   Expert 45 |    150 | CPU
DEBUG 01-15 16:10:21.145548.145548 lmp.py:1935]   Expert 60 |    150 | CPU
DEBUG 01-15 16:10:21.145191.145191 lmp.py:1935]   Expert 12 |    152 | CPU
DEBUG 01-15 16:10:21.145358.145358 lmp.py:1935]   Expert 63 |    155 | CPU
DEBUG 01-15 16:10:21.145762.145762 lmp.py:1935]   Expert 19 |    157 | CPU
DEBUG 01-15 16:10:21.145928.145928 lmp.py:1935]   Expert  6 |    167 | CPU
DEBUG 01-15 16:10:21.145094.145094 lmp.py:1935]   Expert 57 |    169 | CPU
DEBUG 01-15 16:10:21.145499.145499 lmp.py:1935]   Expert 52 |    177 | CPU
DEBUG 01-15 16:10:21.145903.145903 lmp.py:1935]   Expert 50 |    180 | CPU
DEBUG 01-15 16:10:21.145215.145215 lmp.py:1935]   Expert 18 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145527.145527 lmp.py:1935]   Expert 44 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145793.145793 lmp.py:1935]   Expert 31 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145820.145820 lmp.py:1935]   Expert 13 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145086.145086 lmp.py:1935]   Expert 30 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145159.145159 lmp.py:1935]   Expert 23 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145723.145723 lmp.py:1935]   Expert 39 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145843.145843 lmp.py:1935]   Expert 53 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145486.145486 lmp.py:1935]   Expert 59 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145367.145367 lmp.py:1935]   Expert 20 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145010.145010 lmp.py:1935]   Expert 21 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145653.145653 lmp.py:1935]   Expert 29 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145727.145727 lmp.py:1935]   Expert 16 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145038.145038 lmp.py:1935]   Expert 36 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145589.145589 lmp.py:1935]   Expert 41 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145139.145139 lmp.py:1935]   Expert 25 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145497.145497 lmp.py:1935]   Expert 32 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145140.145140 lmp.py:1935]   Expert 49 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145022.145022 lmp.py:1935]   Expert 46 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145665.145665 lmp.py:1935]   Expert  8 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145308.145308 lmp.py:1935]   Expert 42 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145189.145189 lmp.py:1935]   Expert 10 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145594.145594 lmp.py:1935]   Expert 62 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145475.145475 lmp.py:1935]   Expert 35 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145356.145356 lmp.py:1935]   Expert 33 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.145430.145430 lmp.py:1935]   Expert  9 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.145503.145503 lmp.py:1935]   Expert 58 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.146815.146815 lmp.py:1935]   Expert 40 |    389 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.146935.146935 lmp.py:1935]   Expert 11 |    419 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.146817.146817 lmp.py:1935]   Expert  0 |    430 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.146459.146459 lmp.py:1935]   Expert 24 |    566 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.146102.146102 lmp.py:1935]   Expert  1 |    649 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.146553.146553 lmp.py:1937] 
DEBUG 01-15 16:10:21.146553.146553 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.146196.146196 lmp.py:1938]   CPU:   3806 tokens
DEBUG 01-15 16:10:21.146078.146078 lmp.py:1942]   cuda:1:   4242 tokens (16 experts)
DEBUG 01-15 16:10:21.146721.146721 lmp.py:1942]   cuda:2:   4240 tokens (16 experts)
DEBUG 01-15 16:10:21.146648.146648 lmp.py:1943]   Total GPU:   8482 tokens
DEBUG 01-15 16:10:21.146576.146576 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.146576.146576 lmp.py:1944] 
DEBUG 01-15 16:10:21.146180.146180 cuda_h.py:19] end experts_map_get cost 0.0019690990447998047 seconds
DEBUG 01-15 16:10:21.146036.146036 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.146223.146223 lmp.py:1953] 
DEBUG 01-15 16:10:21.146223.146223 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.146934.146934 cuda_h.py:19] end cpu_experts_submit cost 6.699562072753906e-05 seconds
DEBUG 01-15 16:10:21.146650.146650 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.146155.146155 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.147139.147139 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.1.gate_proj.weight': 0, 'model.layers.10.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.44.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.44.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.44.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.49.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.49.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.49.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.32.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.32.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.32.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.40.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.40.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.40.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(12843483136, 5767168, 0, 0), (12849250304, 5767168, 5767168, 0), (12837715968, 5767168, 11534336, 0), (13431734272, 5767168, 17301504, 0), (13437501440, 5767168, 23068672, 0), (13425967104, 5767168, 28835840, 0), (12981895168, 5767168, 34603008, 0), (12987662336, 5767168, 40370176, 0), (12976128000, 5767168, 46137344, 0), (12999196672, 5767168, 51904512, 0), (13004963840, 5767168, 57671680, 0), (12993429504, 5767168, 63438848, 0), (13016498176, 5767168, 69206016, 0), (13022265344, 5767168, 74973184, 0), (13010731008, 5767168, 80740352, 0), (13552844800, 5767168, 86507520, 0), (13558611968, 5767168, 92274688, 0), (13547077632, 5767168, 98041856, 0), (13535543296, 5767168, 103809024, 0), (13541310464, 5767168, 109576192, 0), (13529776128, 5767168, 115343360, 0), (13051101184, 5767168, 121110528, 0), (13056868352, 5767168, 126877696, 0), (13045334016, 5767168, 132644864, 0), (13587447808, 5767168, 138412032, 0), (13593214976, 5767168, 144179200, 0), (13581680640, 5767168, 149946368, 0), (13103005696, 5767168, 155713536, 0), (13108772864, 5767168, 161480704, 0), (13097238528, 5767168, 167247872, 0), (13673955328, 5767168, 173015040, 0), (13679722496, 5767168, 178782208, 0), (13668188160, 5767168, 184549376, 0), (13189513216, 5767168, 190316544, 0), (13195280384, 5767168, 196083712, 0), (13183746048, 5767168, 201850880, 0), (13743161344, 5767168, 207618048, 0), (13748928512, 5767168, 213385216, 0), (13737394176, 5767168, 219152384, 0), (13829668864, 5767168, 224919552, 0), (13835436032, 5767168, 230686720, 0), (13823901696, 5767168, 236453888, 0), (13846970368, 5767168, 242221056, 0), (13852737536, 5767168, 247988224, 0), (13841203200, 5767168, 253755392, 0), (13345226752, 5767168, 259522560, 0), (13350993920, 5767168, 265289728, 0), (13339459584, 5767168, 271056896, 0)], 2: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (13397131264, 5767168, 17301504, 0), (13402898432, 5767168, 23068672, 0), (13391364096, 5767168, 28835840, 0), (13379829760, 5767168, 34603008, 0), (13385596928, 5767168, 40370176, 0), (13374062592, 5767168, 46137344, 0), (13449035776, 5767168, 51904512, 0), (13454802944, 5767168, 57671680, 0), (13443268608, 5767168, 63438848, 0), (13500940288, 5767168, 69206016, 0), (13506707456, 5767168, 74973184, 0), (13495173120, 5767168, 80740352, 0), (13518241792, 5767168, 86507520, 0), (13524008960, 5767168, 92274688, 0), (13512474624, 5767168, 98041856, 0), (12964593664, 5767168, 103809024, 0), (12970360832, 5767168, 109576192, 0), (12958826496, 5767168, 115343360, 0), (13622050816, 5767168, 121110528, 0), (13627817984, 5767168, 126877696, 0), (13616283648, 5767168, 132644864, 0), (13137608704, 5767168, 138412032, 0), (13143375872, 5767168, 144179200, 0), (13131841536, 5767168, 149946368, 0), (13172211712, 5767168, 155713536, 0), (13177978880, 5767168, 161480704, 0), (13166444544, 5767168, 167247872, 0), (13224116224, 5767168, 173015040, 0), (13229883392, 5767168, 178782208, 0), (13218349056, 5767168, 184549376, 0), (13241417728, 5767168, 190316544, 0), (13247184896, 5767168, 196083712, 0), (13235650560, 5767168, 201850880, 0), (13258719232, 5767168, 207618048, 0), (13264486400, 5767168, 213385216, 0), (13252952064, 5767168, 219152384, 0), (13327925248, 5767168, 224919552, 0), (13333692416, 5767168, 230686720, 0), (13322158080, 5767168, 236453888, 0), (13898874880, 5767168, 242221056, 0), (13904642048, 5767168, 247988224, 0), (13893107712, 5767168, 253755392, 0), (13362528256, 5767168, 259522560, 0), (13368295424, 5767168, 265289728, 0), (13356761088, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec41946c0>, 2: <capsule object NULL at 0x7a4e6c7dc6c0>}
DEBUG 01-15 16:10:21.147589.147589 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.147671.147671 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.147334.147334 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e80489f4-4f1c-44a8-aa0b-329b2953ce0b
DEBUG 01-15 16:10:21.147381.147381 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.147093.147093 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.148745.148745 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:21.149164.149164 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e80489f4-4f1c-44a8-aa0b-329b2953ce0b
DEBUG 01-15 16:10:21.149544.149544 cuda_h.py:19] end move_flatidxs cost 0.0008425712585449219 seconds
DEBUG 01-15 16:10:21.149480.149480 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.149704.149704 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0031082630157470703 seconds
DEBUG 01-15 16:10:21.149322.149322 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.152565.152565 cuda_h.py:19] end restore2model cost 0.003057241439819336 seconds
DEBUG 01-15 16:10:21.152978.152978 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006397247314453125 seconds
DEBUG 01-15 16:10:21.152488.152488 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.153216.153216 cuda_h.py:19] end gpu_sexperts cost 0.0003268718719482422 seconds
DEBUG 01-15 16:10:21.153621.153621 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.153027.153027 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2172927856445312e-05 seconds
DEBUG 01-15 16:10:21.153823.153823 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.153910.153910 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.154417.154417 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010776519775390625 seconds
DEBUG 01-15 16:10:21.154313.154313 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.154446.154446 cuda_h.py:19] end gpu_group_list cost 0.00017333030700683594 seconds
DEBUG 01-15 16:10:21.155624.155624 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.156466.156466 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011484622955322266 seconds
DEBUG 01-15 16:10:21.157455.157455 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.157687.157687 cuda_h.py:19] end gpu_group_list cost 0.00017452239990234375 seconds
DEBUG 01-15 16:10:21.157038.157038 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.158398.158398 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e80489f4-4f1c-44a8-aa0b-329b2953ce0b
DEBUG 01-15 16:10:21.159640.159640 cuda_h.py:19] end group_tensors cost 0.01029205322265625 seconds
DEBUG 01-15 16:10:21.160260.160260 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.164156.164156 cuda_h.py:19] end group pad cost 0.004366874694824219 seconds
DEBUG 01-15 16:10:21.164622.164622 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.176298.176298 client.py:127] Model loaded
DEBUG 01-15 16:10:21.176110.176110 cuda_h.py:19] end wait_experts_multi_device cost 0.018463850021362305 seconds
DEBUG 01-15 16:10:21.176391.176391 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.186264.186264 cuda_h.py:19] end group_einsum cost 0.02144765853881836 seconds
DEBUG 01-15 16:10:21.186050.186050 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.190903.190903 cuda_h.py:19] end get_outputs_cpu1 cost 0.003648519515991211 seconds
DEBUG 01-15 16:10:21.191363.191363 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04300427436828613 seconds
DEBUG 01-15 16:10:21.191121.191121 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.015151739120483398 seconds
DEBUG 01-15 16:10:21.192385.192385 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.194737.194737 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.194693.194693 cuda_h.py:19] end index_scatter cost 0.0001354217529296875 seconds
DEBUG 01-15 16:10:21.195022.195022 cuda_h.py:19] end cpuoutputsdeal cost 0.0032820701599121094 seconds
DEBUG 01-15 16:10:21.195140.195140 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.195732.195732 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.196036.196036 cuda_h.py:19] end gpu_group_tensor cost 0.0002810955047607422 seconds
DEBUG 01-15 16:10:21.196443.196443 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.196746.196746 cuda_h.py:19] end gpu_group_tensor cost 0.0002846717834472656 seconds
DEBUG 01-15 16:10:21.196369.196369 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.197408.197408 cuda_h.py:19] end gpu_group_einsum cost 0.0010399818420410156 seconds
DEBUG 01-15 16:10:21.198310.198310 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.199609.199609 cuda_h.py:19] end gpu_group_einsum cost 0.0009086132049560547 seconds
DEBUG 01-15 16:10:21.199359.199359 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.199135.199135 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.200157.200157 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003752708435058594 seconds
DEBUG 01-15 16:10:21.200696.200696 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.200750.200750 cuda_h.py:19] end concat_expert_out cost 0.00011181831359863281 seconds
DEBUG 01-15 16:10:21.200768.200768 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.200391.200391 cuda_h.py:19] end index_scatter cost 0.000125885009765625 seconds
DEBUG 01-15 16:10:21.201381.201381 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0017304420471191406 seconds
DEBUG 01-15 16:10:21.201567.201567 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.201127.201127 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.201107.201107 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002911090850830078 seconds
DEBUG 01-15 16:10:21.202540.202540 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.202208.202208 cuda_h.py:19] end concat_expert_out cost 0.00011348724365234375 seconds
DEBUG 01-15 16:10:21.202534.202534 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.202816.202816 cuda_h.py:19] end index_scatter cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:10:21.202586.202586 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008788108825683594 seconds
DEBUG 01-15 16:10:21.202734.202734 cuda_h.py:19] end gpu_experts_multi_device cost 0.04913592338562012 seconds
DEBUG 01-15 16:10:21.202041.202041 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.059816598892211914 seconds
DEBUG 01-15 16:10:21.203433.203433 cuda_h.py:19] end prefill_layer cost 0.06992673873901367 seconds
DEBUG 01-15 16:10:21.203084.203084 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:10:21.203509.203509 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.203980.203980 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:10:21.203213.203213 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:21.203214.203214 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:21.203277.203277 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:21.203185.203185 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 8.916854858398438e-05 seconds
DEBUG 01-15 16:10:21.203272.203272 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.203077.203077 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.203515.203515 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.203961.203961 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.203485.203485 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.204386.204386 cuda_h.py:19] end allocate_cuda_memory cost 0.0003459453582763672 seconds
DEBUG 01-15 16:10:21.204184.204184 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.204769.204769 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.204082.204082 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.204553.204553 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9423f5c4-f4f4-4cb5-9819-fa51411ab4fd
DEBUG 01-15 16:10:21.204371.204371 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.205743.205743 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.206363.206363 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9423f5c4-f4f4-4cb5-9819-fa51411ab4fd
DEBUG 01-15 16:10:21.206575.206575 cuda_h.py:19] end load_into_gpu_async cost 0.0022962093353271484 seconds
DEBUG 01-15 16:10:21.206337.206337 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.207692.207692 cuda_h.py:19] end restore_tensors2 cost 8.940696716308594e-05 seconds
DEBUG 01-15 16:10:21.207786.207786 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003108501434326172 seconds
INFO 01-15 16:10:21.207948.207948 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9423f5c4-f4f4-4cb5-9819-fa51411ab4fd
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.209979.209979 cuda_h.py:19] end self_attn cost 0.004370450973510742 seconds
DEBUG 01-15 16:10:21.210886.210886 cuda_h.py:19] end iln_self_attn_paln cost 0.006623268127441406 seconds
DEBUG 01-15 16:10:21.210882.210882 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:10:21.210466.210466 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.210089.210089 cuda_h.py:19] end gate cost 0.0007655620574951172 seconds
DEBUG 01-15 16:10:21.211323.211323 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.211206.211206 lmp.py:1912] 
DEBUG 01-15 16:10:21.211206.211206 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.211406.211406 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.211592.211592 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.211010.211010 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.211521.211521 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.211939.211939 lmp.py:1917] 
DEBUG 01-15 16:10:21.211939.211939 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.211596.211596 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.211081.211081 lmp.py:1935]   Expert 39 |     15 | CPU
DEBUG 01-15 16:10:21.211022.211022 lmp.py:1935]   Expert 13 |     17 | CPU
DEBUG 01-15 16:10:21.211486.211486 lmp.py:1935]   Expert 49 |     38 | CPU
DEBUG 01-15 16:10:21.211474.211474 lmp.py:1935]   Expert 35 |     55 | CPU
DEBUG 01-15 16:10:21.211985.211985 lmp.py:1935]   Expert 19 |     62 | CPU
DEBUG 01-15 16:10:21.211641.211641 lmp.py:1935]   Expert  9 |     72 | CPU
DEBUG 01-15 16:10:21.211583.211583 lmp.py:1935]   Expert 26 |     74 | CPU
DEBUG 01-15 16:10:21.211094.211094 lmp.py:1935]   Expert 32 |     75 | CPU
DEBUG 01-15 16:10:21.211366.211366 lmp.py:1935]   Expert 41 |     78 | CPU
DEBUG 01-15 16:10:21.211592.211592 lmp.py:1935]   Expert 33 |     83 | CPU
DEBUG 01-15 16:10:21.211341.211341 lmp.py:1935]   Expert 23 |     87 | CPU
DEBUG 01-15 16:10:21.211567.211567 lmp.py:1935]   Expert 46 |     88 | CPU
DEBUG 01-15 16:10:21.211840.211840 lmp.py:1935]   Expert 18 |     90 | CPU
DEBUG 01-15 16:10:21.212496.212496 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:10:21.212437.212437 lmp.py:1935]   Expert 38 |     99 | CPU
DEBUG 01-15 16:10:21.212187.212187 lmp.py:1935]   Expert 17 |    102 | CPU
DEBUG 01-15 16:10:21.212697.212697 lmp.py:1935]   Expert  3 |    104 | CPU
DEBUG 01-15 16:10:21.212685.212685 lmp.py:1935]   Expert  6 |    106 | CPU
DEBUG 01-15 16:10:21.212196.212196 lmp.py:1935]   Expert 20 |    117 | CPU
DEBUG 01-15 16:10:21.212707.212707 lmp.py:1935]   Expert 40 |    129 | CPU
DEBUG 01-15 16:10:21.212979.212979 lmp.py:1935]   Expert 61 |    131 | CPU
DEBUG 01-15 16:10:21.212682.212682 lmp.py:1935]   Expert 62 |    132 | CPU
DEBUG 01-15 16:10:21.212193.212193 lmp.py:1935]   Expert 15 |    133 | CPU
DEBUG 01-15 16:10:21.212180.212180 lmp.py:1935]   Expert 43 |    136 | CPU
DEBUG 01-15 16:10:21.212691.212691 lmp.py:1935]   Expert 44 |    136 | CPU
DEBUG 01-15 16:10:21.212109.212109 lmp.py:1935]   Expert 50 |    137 | CPU
DEBUG 01-15 16:10:21.212858.212858 lmp.py:1935]   Expert 16 |    138 | CPU
DEBUG 01-15 16:10:21.212952.212952 lmp.py:1935]   Expert 59 |    140 | CPU
DEBUG 01-15 16:10:21.212701.212701 lmp.py:1935]   Expert 63 |    140 | CPU
DEBUG 01-15 16:10:21.212842.212842 lmp.py:1935]   Expert 42 |    143 | CPU
DEBUG 01-15 16:10:21.212677.212677 lmp.py:1935]   Expert  2 |    148 | CPU
DEBUG 01-15 16:10:21.212440.212440 lmp.py:1935]   Expert 36 |    150 | CPU
DEBUG 01-15 16:10:21.212659.212659 lmp.py:1935]   Expert 10 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212209.212209 lmp.py:1935]   Expert  5 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212044.212044 lmp.py:1935]   Expert 34 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212164.212164 lmp.py:1935]   Expert 27 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212807.212807 lmp.py:1935]   Expert 52 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212735.212735 lmp.py:1935]   Expert 45 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212662.212662 lmp.py:1935]   Expert 60 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212590.212590 lmp.py:1935]   Expert 48 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212279.212279 lmp.py:1935]   Expert 51 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212969.212969 lmp.py:1935]   Expert 56 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212420.212420 lmp.py:1935]   Expert 24 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212255.212255 lmp.py:1935]   Expert 53 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212851.212851 lmp.py:1935]   Expert  7 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212210.212210 lmp.py:1935]   Expert  8 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212614.212614 lmp.py:1935]   Expert 47 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212780.212780 lmp.py:1935]   Expert 57 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212470.212470 lmp.py:1935]   Expert 29 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212159.212159 lmp.py:1935]   Expert 21 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212563.212563 lmp.py:1935]   Expert  0 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212253.212253 lmp.py:1935]   Expert  4 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212180.212180 lmp.py:1935]   Expert 14 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212631.212631 lmp.py:1935]   Expert 58 |    312 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212321.212321 lmp.py:1935]   Expert 22 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212248.212248 lmp.py:1935]   Expert  1 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.212130.212130 lmp.py:1935]   Expert 55 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212488.212488 lmp.py:1935]   Expert 37 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.212323.212323 lmp.py:1935]   Expert 54 |    332 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.213489.213489 lmp.py:1935]   Expert 28 |    363 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.213417.213417 lmp.py:1935]   Expert 12 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.213345.213345 lmp.py:1935]   Expert 25 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.213795.213795 lmp.py:1935]   Expert 11 |    402 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.213485.213485 lmp.py:1935]   Expert 30 |    834 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.213697.213697 lmp.py:1937] 
DEBUG 01-15 16:10:21.213697.213697 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.213387.213387 lmp.py:1938]   CPU:   3246 tokens
DEBUG 01-15 16:10:21.213314.213314 lmp.py:1942]   cuda:1:   4600 tokens (16 experts)
DEBUG 01-15 16:10:21.213765.213765 lmp.py:1942]   cuda:2:   4442 tokens (16 experts)
DEBUG 01-15 16:10:21.213501.213501 lmp.py:1943]   Total GPU:   9042 tokens
DEBUG 01-15 16:10:21.213428.213428 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.213428.213428 lmp.py:1944] 
DEBUG 01-15 16:10:21.213416.213416 cuda_h.py:19] end experts_map_get cost 0.002137422561645508 seconds
DEBUG 01-15 16:10:21.213643.213643 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.213446.213446 lmp.py:1953] 
DEBUG 01-15 16:10:21.213446.213446 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.213766.213766 cuda_h.py:19] end cpu_experts_submit cost 5.9604644775390625e-05 seconds
DEBUG 01-15 16:10:21.213594.213594 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.213887.213887 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.214331.214331 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.60.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.60.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.60.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.52.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.52.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.52.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.11.mlp.experts.1.gate_proj.weight': 0, 'model.layers.11.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.51.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.51.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.51.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.54.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.54.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.54.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.57.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (14521729024, 5767168, 17301504, 0), (14527496192, 5767168, 23068672, 0), (14515961856, 5767168, 28835840, 0), (14002683904, 5767168, 34603008, 0), (14008451072, 5767168, 40370176, 0), (13996916736, 5767168, 46137344, 0), (14573633536, 5767168, 51904512, 0), (14579400704, 5767168, 57671680, 0), (14567866368, 5767168, 63438848, 0), (14071889920, 5767168, 69206016, 0), (14077657088, 5767168, 74973184, 0), (14066122752, 5767168, 80740352, 0), (14106492928, 5767168, 86507520, 0), (14112260096, 5767168, 92274688, 0), (14100725760, 5767168, 98041856, 0), (14971568128, 5767168, 103809024, 0), (14977335296, 5767168, 109576192, 0), (14965800960, 5767168, 115343360, 0), (14746648576, 5767168, 121110528, 0), (14752415744, 5767168, 126877696, 0), (14740881408, 5767168, 132644864, 0), (14763950080, 5767168, 138412032, 0), (14769717248, 5767168, 144179200, 0), (14758182912, 5767168, 149946368, 0), (14833156096, 5767168, 155713536, 0), (14838923264, 5767168, 161480704, 0), (14827388928, 5767168, 167247872, 0), (14850457600, 5767168, 173015040, 0), (14856224768, 5767168, 178782208, 0), (14844690432, 5767168, 184549376, 0), (14885060608, 5767168, 190316544, 0), (14890827776, 5767168, 196083712, 0), (14879293440, 5767168, 201850880, 0), (14902362112, 5767168, 207618048, 0), (14908129280, 5767168, 213385216, 0), (14896594944, 5767168, 219152384, 0), (14936965120, 5767168, 224919552, 0), (14942732288, 5767168, 230686720, 0), (14931197952, 5767168, 236453888, 0), (14417920000, 5767168, 242221056, 0), (14423687168, 5767168, 247988224, 0), (14412152832, 5767168, 253755392, 0), (14452523008, 5767168, 259522560, 0), (14458290176, 5767168, 265289728, 0), (14446755840, 5767168, 271056896, 0)], 2: [(13950779392, 5767168, 0, 0), (13956546560, 5767168, 5767168, 0), (13945012224, 5767168, 11534336, 0), (14019985408, 5767168, 17301504, 0), (14025752576, 5767168, 23068672, 0), (14014218240, 5767168, 28835840, 0), (14054588416, 5767168, 34603008, 0), (14060355584, 5767168, 40370176, 0), (14048821248, 5767168, 46137344, 0), (14123794432, 5767168, 51904512, 0), (14129561600, 5767168, 57671680, 0), (14118027264, 5767168, 63438848, 0), (14141095936, 5767168, 69206016, 0), (14146863104, 5767168, 74973184, 0), (14135328768, 5767168, 80740352, 0), (14712045568, 5767168, 86507520, 0), (14717812736, 5767168, 92274688, 0), (14706278400, 5767168, 98041856, 0), (14175698944, 5767168, 103809024, 0), (14181466112, 5767168, 109576192, 0), (14169931776, 5767168, 115343360, 0), (14815854592, 5767168, 121110528, 0), (14821621760, 5767168, 126877696, 0), (14810087424, 5767168, 132644864, 0), (14296809472, 5767168, 138412032, 0), (14302576640, 5767168, 144179200, 0), (14291042304, 5767168, 149946368, 0), (14867759104, 5767168, 155713536, 0), (14873526272, 5767168, 161480704, 0), (14861991936, 5767168, 167247872, 0), (14314110976, 5767168, 173015040, 0), (14319878144, 5767168, 178782208, 0), (14308343808, 5767168, 184549376, 0), (14348713984, 5767168, 190316544, 0), (14354481152, 5767168, 196083712, 0), (14342946816, 5767168, 201850880, 0), (14366015488, 5767168, 207618048, 0), (14371782656, 5767168, 213385216, 0), (14360248320, 5767168, 219152384, 0), (14400618496, 5767168, 224919552, 0), (14406385664, 5767168, 230686720, 0), (14394851328, 5767168, 236453888, 0), (14435221504, 5767168, 242221056, 0), (14440988672, 5767168, 247988224, 0), (14429454336, 5767168, 253755392, 0), (14919663616, 5767168, 259522560, 0), (14925430784, 5767168, 265289728, 0), (14913896448, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e34219860>, 2: <capsule object NULL at 0x7a4e34218ff0>}
DEBUG 01-15 16:10:21.214688.214688 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:21.214575.214575 client.py:127] Model loaded
DEBUG 01-15 16:10:21.214722.214722 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.215833.215833 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.215646.215646 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.215159.215159 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.215279.215279 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5effcfb0-8a9c-4547-8311-b4de3db3086e
DEBUG 01-15 16:10:21.215327.215327 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.216110.216110 cuda_h.py:19] end move_flatidxs cost 0.0010156631469726562 seconds
DEBUG 01-15 16:10:21.216909.216909 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.216726.216726 cuda_h.py:19] end restore2model cost 0.0015480518341064453 seconds
DEBUG 01-15 16:10:21.217572.217572 cuda_h.py:19] end sllm_worker_task cost 0.013317584991455078 seconds
INFO 01-15 16:10:21.217572.217572 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5effcfb0-8a9c-4547-8311-b4de3db3086e
DEBUG 01-15 16:10:21.218553.218553 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004997968673706055 seconds
DEBUG 01-15 16:10:21.219017.219017 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.226852.226852 cuda_h.py:19] end restore2model cost 0.007810354232788086 seconds
DEBUG 01-15 16:10:21.227294.227294 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.013348579406738281 seconds
DEBUG 01-15 16:10:21.227178.227178 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.228733.228733 cuda_h.py:19] end gpu_sexperts cost 0.0008695125579833984 seconds
DEBUG 01-15 16:10:21.228956.228956 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.228253.228253 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 4.220008850097656e-05 seconds
DEBUG 01-15 16:10:21.228162.228162 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.228470.228470 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.231145.231145 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.002469778060913086 seconds
DEBUG 01-15 16:10:21.231585.231585 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.230586.230586 cuda_h.py:19] end group_tensors cost 0.014220237731933594 seconds
DEBUG 01-15 16:10:21.231318.231318 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.231190.231190 cuda_h.py:19] end gpu_group_list cost 0.0002338886260986328 seconds
DEBUG 01-15 16:10:21.232704.232704 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.233799.233799 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011553764343261719 seconds
DEBUG 01-15 16:10:21.233477.233477 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.234380.234380 cuda_h.py:19] end gpu_group_list cost 0.00024366378784179688 seconds
DEBUG 01-15 16:10:21.235889.235889 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.235507.235507 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5effcfb0-8a9c-4547-8311-b4de3db3086e
DEBUG 01-15 16:10:21.235528.235528 cuda_h.py:19] end group pad cost 0.004245281219482422 seconds
DEBUG 01-15 16:10:21.235185.235185 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.242332.242332 client.py:127] Model loaded
DEBUG 01-15 16:10:21.242260.242260 cuda_h.py:19] end wait_experts_multi_device cost 0.007862329483032227 seconds
DEBUG 01-15 16:10:21.242579.242579 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.255131.255131 cuda_h.py:19] end group_einsum cost 0.019644975662231445 seconds
DEBUG 01-15 16:10:21.255772.255772 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.259914.259914 cuda_h.py:19] end get_outputs_cpu1 cost 0.0033614635467529297 seconds
DEBUG 01-15 16:10:21.259618.259618 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04524827003479004 seconds
DEBUG 01-15 16:10:21.260371.260371 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.017645597457885742 seconds
DEBUG 01-15 16:10:21.260947.260947 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.263501.263501 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.263305.263305 cuda_h.py:19] end index_scatter cost 0.00013303756713867188 seconds
DEBUG 01-15 16:10:21.263547.263547 cuda_h.py:19] end cpuoutputsdeal cost 0.0028121471405029297 seconds
DEBUG 01-15 16:10:21.263711.263711 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.264780.264780 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.264488.264488 cuda_h.py:19] end gpu_group_tensor cost 0.0002837181091308594 seconds
DEBUG 01-15 16:10:21.264425.264425 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.264940.264940 cuda_h.py:19] end gpu_group_tensor cost 0.00026535987854003906 seconds
DEBUG 01-15 16:10:21.265318.265318 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.267867.267867 cuda_h.py:19] end gpu_group_einsum cost 0.0018358230590820312 seconds
DEBUG 01-15 16:10:21.267611.267611 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.268872.268872 cuda_h.py:19] end gpu_group_einsum cost 0.0009453296661376953 seconds
DEBUG 01-15 16:10:21.268352.268352 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.268836.268836 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.269858.269858 cuda_h.py:19] end all_expert_outputs_slices cost 0.00035858154296875 seconds
DEBUG 01-15 16:10:21.269636.269636 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.269609.269609 cuda_h.py:19] end concat_expert_out cost 0.00011324882507324219 seconds
DEBUG 01-15 16:10:21.269190.269190 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.269098.269098 cuda_h.py:19] end index_scatter cost 0.00012302398681640625 seconds
DEBUG 01-15 16:10:21.270690.270690 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0016789436340332031 seconds
DEBUG 01-15 16:10:21.270399.270399 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.270529.270529 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.271270.271270 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002903938293457031 seconds
DEBUG 01-15 16:10:21.271987.271987 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.271100.271100 cuda_h.py:19] end concat_expert_out cost 0.00011444091796875 seconds
DEBUG 01-15 16:10:21.271720.271720 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.271330.271330 cuda_h.py:19] end index_scatter cost 0.00011587142944335938 seconds
DEBUG 01-15 16:10:21.271737.271737 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001104116439819336 seconds
DEBUG 01-15 16:10:21.271272.271272 cuda_h.py:19] end gpu_experts_multi_device cost 0.04333806037902832 seconds
DEBUG 01-15 16:10:21.272324.272324 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.062001943588256836 seconds
DEBUG 01-15 16:10:21.272569.272569 cuda_h.py:19] end prefill_layer cost 0.06960034370422363 seconds
DEBUG 01-15 16:10:21.272081.272081 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:10:21.272791.272791 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.272739.272739 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:10:21.272210.272210 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:21.272735.272735 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:21.273327.273327 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:21.273374.273374 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 9.107589721679688e-05 seconds
DEBUG 01-15 16:10:21.273839.273839 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.273915.273915 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.273309.273309 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.273626.273626 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.273410.273410 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.273629.273629 cuda_h.py:19] end allocate_cuda_memory cost 0.00036144256591796875 seconds
DEBUG 01-15 16:10:21.274453.274453 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.274123.274123 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.274105.274105 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.274007.274007 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ceb5efef-d59d-4e37-9e6c-653bcb017a86
DEBUG 01-15 16:10:21.274196.274196 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.274830.274830 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.275895.275895 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ceb5efef-d59d-4e37-9e6c-653bcb017a86
DEBUG 01-15 16:10:21.276888.276888 cuda_h.py:19] end load_into_gpu_async cost 0.002057313919067383 seconds
DEBUG 01-15 16:10:21.276143.276143 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.276199.276199 cuda_h.py:19] end restore_tensors2 cost 8.106231689453125e-05 seconds
DEBUG 01-15 16:10:21.276340.276340 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002869129180908203 seconds
INFO 01-15 16:10:21.276635.276635 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ceb5efef-d59d-4e37-9e6c-653bcb017a86
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.279450.279450 cuda_h.py:19] end self_attn cost 0.0045468807220458984 seconds
DEBUG 01-15 16:10:21.279144.279144 cuda_h.py:19] end iln_self_attn_paln cost 0.006506681442260742 seconds
DEBUG 01-15 16:10:21.279027.279027 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:10:21.279121.279121 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.280793.280793 cuda_h.py:19] end gate cost 0.0006573200225830078 seconds
DEBUG 01-15 16:10:21.280153.280153 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.280396.280396 lmp.py:1912] 
DEBUG 01-15 16:10:21.280396.280396 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.280537.280537 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.280239.280239 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.280604.280604 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.280347.280347 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.280897.280897 lmp.py:1917] 
DEBUG 01-15 16:10:21.280897.280897 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.280448.280448 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.281674.281674 lmp.py:1935]   Expert 12 |     18 | CPU
DEBUG 01-15 16:10:21.281462.281462 lmp.py:1935]   Expert 47 |     24 | CPU
DEBUG 01-15 16:10:21.281059.281059 lmp.py:1935]   Expert 38 |     30 | CPU
DEBUG 01-15 16:10:21.281894.281894 lmp.py:1935]   Expert 27 |     34 | CPU
DEBUG 01-15 16:10:21.281921.281921 lmp.py:1935]   Expert 16 |     36 | CPU
DEBUG 01-15 16:10:21.281233.281233 lmp.py:1935]   Expert 52 |     38 | CPU
DEBUG 01-15 16:10:21.281545.281545 lmp.py:1935]   Expert 63 |     45 | CPU
DEBUG 01-15 16:10:21.281857.281857 lmp.py:1935]   Expert  4 |     59 | CPU
DEBUG 01-15 16:10:21.281454.281454 lmp.py:1935]   Expert 44 |     61 | CPU
DEBUG 01-15 16:10:21.281812.281812 lmp.py:1935]   Expert 43 |     63 | CPU
DEBUG 01-15 16:10:21.281408.281408 lmp.py:1935]   Expert 61 |     63 | CPU
DEBUG 01-15 16:10:21.281528.281528 lmp.py:1935]   Expert 34 |     77 | CPU
DEBUG 01-15 16:10:21.281363.281363 lmp.py:1935]   Expert 53 |     82 | CPU
DEBUG 01-15 16:10:21.281483.281483 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:10:21.281318.281318 lmp.py:1935]   Expert 32 |     90 | CPU
DEBUG 01-15 16:10:21.281630.281630 lmp.py:1935]   Expert 37 |     90 | CPU
DEBUG 01-15 16:10:21.281704.281704 lmp.py:1935]   Expert 13 |    105 | CPU
DEBUG 01-15 16:10:21.281539.281539 lmp.py:1935]   Expert 39 |    116 | CPU
DEBUG 01-15 16:10:21.281374.281374 lmp.py:1935]   Expert 21 |    118 | CPU
DEBUG 01-15 16:10:21.281447.281447 lmp.py:1935]   Expert 11 |    121 | CPU
DEBUG 01-15 16:10:21.281805.281805 lmp.py:1935]   Expert 20 |    125 | CPU
DEBUG 01-15 16:10:21.281402.281402 lmp.py:1935]   Expert  8 |    130 | CPU
DEBUG 01-15 16:10:21.281522.281522 lmp.py:1935]   Expert 60 |    134 | CPU
DEBUG 01-15 16:10:21.281642.281642 lmp.py:1935]   Expert 14 |    138 | CPU
DEBUG 01-15 16:10:21.281762.281762 lmp.py:1935]   Expert 57 |    139 | CPU
DEBUG 01-15 16:10:21.281120.281120 lmp.py:1935]   Expert 22 |    140 | CPU
DEBUG 01-15 16:10:21.281716.281716 lmp.py:1935]   Expert 45 |    153 | CPU
DEBUG 01-15 16:10:21.281075.281075 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:10:21.281671.281671 lmp.py:1935]   Expert 17 |    157 | CPU
DEBUG 01-15 16:10:21.281745.281745 lmp.py:1935]   Expert 18 |    159 | CPU
DEBUG 01-15 16:10:21.281818.281818 lmp.py:1935]   Expert  7 |    160 | CPU
DEBUG 01-15 16:10:21.281653.281653 lmp.py:1935]   Expert 23 |    160 | CPU
DEBUG 01-15 16:10:21.281171.281171 lmp.py:1935]   Expert 58 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281198.281198 lmp.py:1935]   Expert 30 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.281987.281987 lmp.py:1935]   Expert 42 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.281583.281583 lmp.py:1935]   Expert 49 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281657.281657 lmp.py:1935]   Expert 48 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.281492.281492 lmp.py:1935]   Expert 62 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281804.281804 lmp.py:1935]   Expert 55 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.281116.281116 lmp.py:1935]   Expert 35 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281905.281905 lmp.py:1935]   Expert 51 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.281455.281455 lmp.py:1935]   Expert 29 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281290.281290 lmp.py:1935]   Expert  6 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281887.281887 lmp.py:1935]   Expert 25 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.281722.281722 lmp.py:1935]   Expert 36 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.281080.281080 lmp.py:1935]   Expert  1 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281677.281677 lmp.py:1935]   Expert 31 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281273.281273 lmp.py:1935]   Expert 28 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.281108.281108 lmp.py:1935]   Expert 54 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281705.281705 lmp.py:1935]   Expert  5 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.281255.281255 lmp.py:1935]   Expert 41 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.281567.281567 lmp.py:1935]   Expert  9 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.282594.282594 lmp.py:1935]   Expert 19 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.282145.282145 lmp.py:1935]   Expert 24 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.282218.282218 lmp.py:1935]   Expert 50 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.282815.282815 lmp.py:1935]   Expert 46 |    308 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.282411.282411 lmp.py:1935]   Expert 59 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.282008.282008 lmp.py:1935]   Expert 56 |    376 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.282843.282843 lmp.py:1935]   Expert 26 |    408 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.282917.282917 lmp.py:1935]   Expert 33 |    422 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.282229.282229 lmp.py:1935]   Expert  3 |    587 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.282540.282540 lmp.py:1935]   Expert 10 |    637 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.282091.282091 lmp.py:1935]   Expert 15 |    645 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.282403.282403 lmp.py:1935]   Expert 40 |    791 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.282284.282284 lmp.py:1937] 
DEBUG 01-15 16:10:21.282284.282284 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.282642.282642 lmp.py:1938]   CPU:   3108 tokens
DEBUG 01-15 16:10:21.282716.282716 lmp.py:1942]   cuda:1:   4593 tokens (16 experts)
DEBUG 01-15 16:10:21.282836.282836 lmp.py:1942]   cuda:2:   4587 tokens (16 experts)
DEBUG 01-15 16:10:21.282479.282479 lmp.py:1943]   Total GPU:   9180 tokens
DEBUG 01-15 16:10:21.282883.282883 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.282883.282883 lmp.py:1944] 
DEBUG 01-15 16:10:21.282487.282487 cuda_h.py:19] end experts_map_get cost 0.0018322467803955078 seconds
DEBUG 01-15 16:10:21.282866.282866 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.282384.282384 lmp.py:1953] 
DEBUG 01-15 16:10:21.282384.282384 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.282419.282419 cuda_h.py:19] end cpu_experts_submit cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:10:21.282969.282969 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.282428.282428 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.283736.283736 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.1.gate_proj.weight': 0, 'model.layers.12.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.58.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.58.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.58.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.40.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.40.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.40.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.54.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.54.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.54.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.59.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.59.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.59.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.12.mlp.experts.33.gate_proj.weight': 0, 'model.layers.12.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.41.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.41.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.41.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(15058075648, 5767168, 0, 0), (15063842816, 5767168, 5767168, 0), (15052308480, 5767168, 11534336, 0), (15092678656, 5767168, 17301504, 0), (15098445824, 5767168, 23068672, 0), (15086911488, 5767168, 28835840, 0), (15646326784, 5767168, 34603008, 0), (15652093952, 5767168, 40370176, 0), (15640559616, 5767168, 46137344, 0), (15127281664, 5767168, 51904512, 0), (15133048832, 5767168, 57671680, 0), (15121514496, 5767168, 63438848, 0), (15144583168, 5767168, 69206016, 0), (15150350336, 5767168, 74973184, 0), (15138816000, 5767168, 80740352, 0), (16044261376, 5767168, 86507520, 0), (16050028544, 5767168, 92274688, 0), (16038494208, 5767168, 98041856, 0), (15732834304, 5767168, 103809024, 0), (15738601472, 5767168, 109576192, 0), (15727067136, 5767168, 115343360, 0), (15888547840, 5767168, 121110528, 0), (15894315008, 5767168, 126877696, 0), (15882780672, 5767168, 132644864, 0), (15905849344, 5767168, 138412032, 0), (15911616512, 5767168, 144179200, 0), (15900082176, 5767168, 149946368, 0), (15975055360, 5767168, 155713536, 0), (15980822528, 5767168, 161480704, 0), (15969288192, 5767168, 167247872, 0), (15456010240, 5767168, 173015040, 0), (15461777408, 5767168, 178782208, 0), (15450243072, 5767168, 184549376, 0), (15490613248, 5767168, 190316544, 0), (15496380416, 5767168, 196083712, 0), (15484846080, 5767168, 201850880, 0), (16061562880, 5767168, 207618048, 0), (16067330048, 5767168, 213385216, 0), (16055795712, 5767168, 219152384, 0), (15542517760, 5767168, 224919552, 0), (15548284928, 5767168, 230686720, 0), (15536750592, 5767168, 236453888, 0), (16113467392, 5767168, 242221056, 0), (16119234560, 5767168, 247988224, 0), (16107700224, 5767168, 253755392, 0), (15577120768, 5767168, 259522560, 0), (15582887936, 5767168, 265289728, 0), (15571353600, 5767168, 271056896, 0)], 2: [(15611723776, 5767168, 0, 0), (15617490944, 5767168, 5767168, 0), (15605956608, 5767168, 11534336, 0), (15663628288, 5767168, 17301504, 0), (15669395456, 5767168, 23068672, 0), (15657861120, 5767168, 28835840, 0), (15196487680, 5767168, 34603008, 0), (15202254848, 5767168, 40370176, 0), (15190720512, 5767168, 46137344, 0), (15213789184, 5767168, 51904512, 0), (15219556352, 5767168, 57671680, 0), (15208022016, 5767168, 63438848, 0), (15750135808, 5767168, 69206016, 0), (15755902976, 5767168, 74973184, 0), (15744368640, 5767168, 80740352, 0), (15767437312, 5767168, 86507520, 0), (15773204480, 5767168, 92274688, 0), (15761670144, 5767168, 98041856, 0), (15836643328, 5767168, 103809024, 0), (15842410496, 5767168, 109576192, 0), (15830876160, 5767168, 115343360, 0), (15300296704, 5767168, 121110528, 0), (15306063872, 5767168, 126877696, 0), (15294529536, 5767168, 132644864, 0), (15871246336, 5767168, 138412032, 0), (15877013504, 5767168, 144179200, 0), (15865479168, 5767168, 149946368, 0), (15369502720, 5767168, 155713536, 0), (15375269888, 5767168, 161480704, 0), (15363735552, 5767168, 167247872, 0), (15923150848, 5767168, 173015040, 0), (15928918016, 5767168, 178782208, 0), (15917383680, 5767168, 184549376, 0), (15992356864, 5767168, 190316544, 0), (15998124032, 5767168, 196083712, 0), (15986589696, 5767168, 201850880, 0), (16009658368, 5767168, 207618048, 0), (16015425536, 5767168, 213385216, 0), (16003891200, 5767168, 219152384, 0), (15473311744, 5767168, 224919552, 0), (15479078912, 5767168, 230686720, 0), (15467544576, 5767168, 236453888, 0), (15525216256, 5767168, 242221056, 0), (15530983424, 5767168, 247988224, 0), (15519449088, 5767168, 253755392, 0), (15559819264, 5767168, 259522560, 0), (15565586432, 5767168, 265289728, 0), (15554052096, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e6c7dc960>, 2: <capsule object NULL at 0x7a4f2c2487e0>}
INFO 01-15 16:10:21.283531.283531 client.py:127] Model loaded
DEBUG 01-15 16:10:21.283629.283629 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.283407.283407 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.284129.284129 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.284940.284940 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.284366.284366 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, eb7a34b3-6167-41c5-bf02-ec62d7ffebf1
DEBUG 01-15 16:10:21.284473.284473 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.284215.284215 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.285532.285532 cuda_h.py:19] end restore2model cost 0.0009849071502685547 seconds
DEBUG 01-15 16:10:21.285243.285243 cuda_h.py:19] end sllm_worker_task cost 0.011705875396728516 seconds
DEBUG 01-15 16:10:21.285676.285676 cuda_h.py:19] end move_flatidxs cost 0.0008490085601806641 seconds
DEBUG 01-15 16:10:21.285234.285234 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:21.286731.286731 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, eb7a34b3-6167-41c5-bf02-ec62d7ffebf1
DEBUG 01-15 16:10:21.286091.286091 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003929853439331055 seconds
DEBUG 01-15 16:10:21.286723.286723 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.289523.289523 cuda_h.py:19] end restore2model cost 0.002521991729736328 seconds
DEBUG 01-15 16:10:21.289598.289598 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006692171096801758 seconds
DEBUG 01-15 16:10:21.289586.289586 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.289782.289782 cuda_h.py:19] end gpu_sexperts cost 0.0002865791320800781 seconds
DEBUG 01-15 16:10:21.289234.289234 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.289686.289686 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.3126602172851562e-05 seconds
DEBUG 01-15 16:10:21.289290.289290 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.289185.289185 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.290564.290564 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008103847503662109 seconds
DEBUG 01-15 16:10:21.290976.290976 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.290294.290294 cuda_h.py:19] end gpu_group_list cost 0.00017118453979492188 seconds
DEBUG 01-15 16:10:21.291657.291657 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.292682.292682 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008711814880371094 seconds
DEBUG 01-15 16:10:21.292094.292094 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.292128.292128 cuda_h.py:19] end gpu_group_list cost 0.00017023086547851562 seconds
DEBUG 01-15 16:10:21.293253.293253 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.293752.293752 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, eb7a34b3-6167-41c5-bf02-ec62d7ffebf1
DEBUG 01-15 16:10:21.295669.295669 cuda_h.py:19] end group_tensors cost 0.009569406509399414 seconds
DEBUG 01-15 16:10:21.295978.295978 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.299201.299201 cuda_h.py:19] end group pad cost 0.004045963287353516 seconds
DEBUG 01-15 16:10:21.299137.299137 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.312548.312548 client.py:127] Model loaded
DEBUG 01-15 16:10:21.312939.312939 cuda_h.py:19] end wait_experts_multi_device cost 0.019179821014404297 seconds
DEBUG 01-15 16:10:21.312613.312613 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.322377.322377 cuda_h.py:19] end group_einsum cost 0.022493362426757812 seconds
DEBUG 01-15 16:10:21.322640.322640 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.326088.326088 cuda_h.py:19] end get_outputs_cpu1 cost 0.0032150745391845703 seconds
DEBUG 01-15 16:10:21.326957.326957 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04266929626464844 seconds
DEBUG 01-15 16:10:21.327606.327606 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.014739751815795898 seconds
DEBUG 01-15 16:10:21.327566.327566 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.330164.330164 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.330180.330180 cuda_h.py:19] end index_scatter cost 0.00013637542724609375 seconds
DEBUG 01-15 16:10:21.331078.331078 cuda_h.py:19] end cpuoutputsdeal cost 0.003330230712890625 seconds
DEBUG 01-15 16:10:21.331912.331912 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.331742.331742 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.331953.331953 cuda_h.py:19] end gpu_group_tensor cost 0.0002853870391845703 seconds
DEBUG 01-15 16:10:21.331552.331552 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.332822.332822 cuda_h.py:19] end gpu_group_tensor cost 0.00026035308837890625 seconds
DEBUG 01-15 16:10:21.332419.332419 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.333631.333631 cuda_h.py:19] end gpu_group_einsum cost 0.0008914470672607422 seconds
DEBUG 01-15 16:10:21.333658.333658 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.334220.334220 cuda_h.py:19] end gpu_group_einsum cost 0.0006735324859619141 seconds
DEBUG 01-15 16:10:21.334325.334325 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.334463.334463 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.335813.335813 cuda_h.py:19] end all_expert_outputs_slices cost 0.00028705596923828125 seconds
DEBUG 01-15 16:10:21.335756.335756 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.335059.335059 cuda_h.py:19] end concat_expert_out cost 8.606910705566406e-05 seconds
DEBUG 01-15 16:10:21.335116.335116 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.335432.335432 cuda_h.py:19] end index_scatter cost 9.441375732421875e-05 seconds
DEBUG 01-15 16:10:21.336048.336048 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0013241767883300781 seconds
DEBUG 01-15 16:10:21.336457.336457 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.336454.336454 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.336239.336239 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022220611572265625 seconds
DEBUG 01-15 16:10:21.336459.336459 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.336279.336279 cuda_h.py:19] end concat_expert_out cost 8.749961853027344e-05 seconds
DEBUG 01-15 16:10:21.336998.336998 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.337970.337970 cuda_h.py:19] end index_scatter cost 8.916854858398438e-05 seconds
DEBUG 01-15 16:10:21.337919.337919 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008668899536132812 seconds
DEBUG 01-15 16:10:21.337890.337890 cuda_h.py:19] end gpu_experts_multi_device cost 0.047568559646606445 seconds
DEBUG 01-15 16:10:21.337596.337596 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.057721853256225586 seconds
DEBUG 01-15 16:10:21.338729.338729 cuda_h.py:19] end prefill_layer cost 0.06514859199523926 seconds
DEBUG 01-15 16:10:21.338182.338182 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:10:21.338442.338442 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.338609.338609 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:10:21.338346.338346 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:21.338613.338613 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:21.338683.338683 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:10:21.338248.338248 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 0.00013589859008789062 seconds
DEBUG 01-15 16:10:21.338171.338171 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.338546.338546 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.338411.338411 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.339731.339731 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.339451.339451 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.339984.339984 cuda_h.py:19] end allocate_cuda_memory cost 0.000400543212890625 seconds
DEBUG 01-15 16:10:21.339256.339256 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.340484.340484 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.340641.340641 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.340411.340411 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4d148cfb-5d41-47fa-b167-f756953b0039
DEBUG 01-15 16:10:21.340724.340724 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.340834.340834 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.342853.342853 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4d148cfb-5d41-47fa-b167-f756953b0039
DEBUG 01-15 16:10:21.342242.342242 cuda_h.py:19] end load_into_gpu_async cost 0.0024526119232177734 seconds
DEBUG 01-15 16:10:21.342157.342157 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.342659.342659 cuda_h.py:19] end restore_tensors2 cost 0.0001277923583984375 seconds
DEBUG 01-15 16:10:21.342661.342661 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0035495758056640625 seconds
INFO 01-15 16:10:21.342128.342128 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4d148cfb-5d41-47fa-b167-f756953b0039
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.345081.345081 cuda_h.py:19] end self_attn cost 0.004285097122192383 seconds
DEBUG 01-15 16:10:21.345795.345795 cuda_h.py:19] end iln_self_attn_paln cost 0.0071070194244384766 seconds
DEBUG 01-15 16:10:21.345452.345452 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:10:21.345553.345553 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.346322.346322 cuda_h.py:19] end gate cost 0.0007700920104980469 seconds
DEBUG 01-15 16:10:21.346649.346649 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.347576.347576 lmp.py:1912] 
DEBUG 01-15 16:10:21.347576.347576 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.347200.347200 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.347287.347287 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.347367.347367 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.347017.347017 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.347594.347594 lmp.py:1917] 
DEBUG 01-15 16:10:21.347594.347594 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.347006.347006 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.347093.347093 lmp.py:1935]   Expert 42 |     23 | CPU
DEBUG 01-15 16:10:21.347074.347074 lmp.py:1935]   Expert 19 |     24 | CPU
DEBUG 01-15 16:10:21.347339.347339 lmp.py:1935]   Expert 30 |     27 | CPU
DEBUG 01-15 16:10:21.347890.347890 lmp.py:1935]   Expert 32 |     45 | CPU
DEBUG 01-15 16:10:21.347917.347917 lmp.py:1935]   Expert  6 |     59 | CPU
DEBUG 01-15 16:10:21.347229.347229 lmp.py:1935]   Expert  5 |     74 | CPU
DEBUG 01-15 16:10:21.347541.347541 lmp.py:1935]   Expert 53 |     75 | CPU
DEBUG 01-15 16:10:21.347329.347329 lmp.py:1935]   Expert  1 |     78 | CPU
DEBUG 01-15 16:10:21.347118.347118 lmp.py:1935]   Expert 13 |    121 | CPU
DEBUG 01-15 16:10:21.347668.347668 lmp.py:1935]   Expert  9 |    123 | CPU
DEBUG 01-15 16:10:21.347457.347457 lmp.py:1935]   Expert 63 |    124 | CPU
DEBUG 01-15 16:10:21.347438.347438 lmp.py:1935]   Expert 34 |    130 | CPU
DEBUG 01-15 16:10:21.347657.347657 lmp.py:1935]   Expert 58 |    130 | CPU
DEBUG 01-15 16:10:21.347208.347208 lmp.py:1935]   Expert 50 |    131 | CPU
DEBUG 01-15 16:10:21.347758.347758 lmp.py:1935]   Expert 18 |    135 | CPU
DEBUG 01-15 16:10:21.347070.347070 lmp.py:1935]   Expert 11 |    137 | CPU
DEBUG 01-15 16:10:21.347620.347620 lmp.py:1935]   Expert 26 |    137 | CPU
DEBUG 01-15 16:10:21.347694.347694 lmp.py:1935]   Expert 31 |    137 | CPU
DEBUG 01-15 16:10:21.347244.347244 lmp.py:1935]   Expert 40 |    141 | CPU
DEBUG 01-15 16:10:21.347317.347317 lmp.py:1935]   Expert 59 |    141 | CPU
DEBUG 01-15 16:10:21.347868.347868 lmp.py:1935]   Expert 12 |    147 | CPU
DEBUG 01-15 16:10:21.347895.347895 lmp.py:1935]   Expert 46 |    150 | CPU
DEBUG 01-15 16:10:21.347637.347637 lmp.py:1935]   Expert  2 |    152 | CPU
DEBUG 01-15 16:10:21.347618.347618 lmp.py:1935]   Expert 20 |    152 | CPU
DEBUG 01-15 16:10:21.347599.347599 lmp.py:1935]   Expert 48 |    152 | CPU
DEBUG 01-15 16:10:21.347149.347149 lmp.py:1935]   Expert  4 |    153 | CPU
DEBUG 01-15 16:10:21.347700.347700 lmp.py:1935]   Expert 56 |    153 | CPU
DEBUG 01-15 16:10:21.347250.347250 lmp.py:1935]   Expert 33 |    155 | CPU
DEBUG 01-15 16:10:21.347323.347323 lmp.py:1935]   Expert 61 |    156 | CPU
DEBUG 01-15 16:10:21.347397.347397 lmp.py:1935]   Expert 35 |    163 | CPU
DEBUG 01-15 16:10:21.347470.347470 lmp.py:1935]   Expert 10 |    168 | CPU
DEBUG 01-15 16:10:21.347305.347305 lmp.py:1935]   Expert 55 |    173 | CPU
DEBUG 01-15 16:10:21.347478.347478 lmp.py:1935]   Expert 51 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.347890.347890 lmp.py:1935]   Expert 36 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348063.348063 lmp.py:1935]   Expert  8 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348759.348759 lmp.py:1935]   Expert 52 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348932.348932 lmp.py:1935]   Expert 37 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348581.348581 lmp.py:1935]   Expert  0 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348993.348993 lmp.py:1935]   Expert 57 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348212.348212 lmp.py:1935]   Expert 39 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348193.348193 lmp.py:1935]   Expert 25 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348935.348935 lmp.py:1935]   Expert 62 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348678.348678 lmp.py:1935]   Expert 38 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348182.348182 lmp.py:1935]   Expert  7 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348924.348924 lmp.py:1935]   Expert  3 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348428.348428 lmp.py:1935]   Expert 24 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348839.348839 lmp.py:1935]   Expert 27 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348251.348251 lmp.py:1935]   Expert 28 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348232.348232 lmp.py:1935]   Expert 60 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348643.348643 lmp.py:1935]   Expert 21 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348624.348624 lmp.py:1935]   Expert 49 |    263 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348366.348366 lmp.py:1935]   Expert 16 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348976.348976 lmp.py:1935]   Expert 43 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348480.348480 lmp.py:1935]   Expert 23 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348984.348984 lmp.py:1935]   Expert 29 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348727.348727 lmp.py:1935]   Expert 22 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348185.348185 lmp.py:1935]   Expert 15 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348927.348927 lmp.py:1935]   Expert 47 |    293 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348193.348193 lmp.py:1935]   Expert 41 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348650.348650 lmp.py:1935]   Expert 44 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348585.348585 lmp.py:1935]   Expert 54 |    353 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348758.348758 lmp.py:1935]   Expert 14 |    375 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348692.348692 lmp.py:1935]   Expert 17 |    403 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.348388.348388 lmp.py:1935]   Expert 45 |    448 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.348608.348608 lmp.py:1937] 
DEBUG 01-15 16:10:21.348608.348608 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.348257.348257 lmp.py:1938]   CPU:   3866 tokens
DEBUG 01-15 16:10:21.348907.348907 lmp.py:1942]   cuda:1:   4214 tokens (16 experts)
DEBUG 01-15 16:10:21.348226.348226 lmp.py:1942]   cuda:2:   4208 tokens (16 experts)
DEBUG 01-15 16:10:21.348207.348207 lmp.py:1943]   Total GPU:   8422 tokens
DEBUG 01-15 16:10:21.348757.348757 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.348757.348757 lmp.py:1944] 
DEBUG 01-15 16:10:21.348698.348698 cuda_h.py:19] end experts_map_get cost 0.0020372867584228516 seconds
DEBUG 01-15 16:10:21.348177.348177 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.348748.348748 lmp.py:1953] 
DEBUG 01-15 16:10:21.348748.348748 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.348459.348459 cuda_h.py:19] end cpu_experts_submit cost 6.628036499023438e-05 seconds
DEBUG 01-15 16:10:21.348970.348970 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.349873.349873 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.349881.349881 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.41.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.41.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.41.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.51.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.51.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.51.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.13.mlp.experts.36.gate_proj.weight': 0, 'model.layers.13.mlp.experts.36.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.36.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.44.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.44.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.44.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.28.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.28.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.28.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.27.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.27.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.27.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.60.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.60.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.60.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16199974912, 5767168, 17301504, 0), (16205742080, 5767168, 23068672, 0), (16194207744, 5767168, 28835840, 0), (16269180928, 5767168, 34603008, 0), (16274948096, 5767168, 40370176, 0), (16263413760, 5767168, 46137344, 0), (16286482432, 5767168, 51904512, 0), (16292249600, 5767168, 57671680, 0), (16280715264, 5767168, 63438848, 0), (16857432064, 5767168, 69206016, 0), (16863199232, 5767168, 74973184, 0), (16851664896, 5767168, 80740352, 0), (16892035072, 5767168, 86507520, 0), (16897802240, 5767168, 92274688, 0), (16886267904, 5767168, 98041856, 0), (16926638080, 5767168, 103809024, 0), (16932405248, 5767168, 109576192, 0), (16920870912, 5767168, 115343360, 0), (16407592960, 5767168, 121110528, 0), (16413360128, 5767168, 126877696, 0), (16401825792, 5767168, 132644864, 0), (16424894464, 5767168, 138412032, 0), (16430661632, 5767168, 144179200, 0), (16419127296, 5767168, 149946368, 0), (17030447104, 5767168, 155713536, 0), (17036214272, 5767168, 161480704, 0), (17024679936, 5767168, 167247872, 0), (16511401984, 5767168, 173015040, 0), (16517169152, 5767168, 178782208, 0), (16505634816, 5767168, 184549376, 0), (17082351616, 5767168, 190316544, 0), (17088118784, 5767168, 196083712, 0), (17076584448, 5767168, 201850880, 0), (16563306496, 5767168, 207618048, 0), (16569073664, 5767168, 213385216, 0), (16557539328, 5767168, 219152384, 0), (17134256128, 5767168, 224919552, 0), (17140023296, 5767168, 230686720, 0), (17128488960, 5767168, 236453888, 0), (16649814016, 5767168, 242221056, 0), (16655581184, 5767168, 247988224, 0), (16644046848, 5767168, 253755392, 0), (17220763648, 5767168, 259522560, 0), (17226530816, 5767168, 265289728, 0), (17214996480, 5767168, 271056896, 0)], 2: [(16770924544, 5767168, 0, 0), (16776691712, 5767168, 5767168, 0), (16765157376, 5767168, 11534336, 0), (16788226048, 5767168, 17301504, 0), (16793993216, 5767168, 23068672, 0), (16782458880, 5767168, 28835840, 0), (16805527552, 5767168, 34603008, 0), (16811294720, 5767168, 40370176, 0), (16799760384, 5767168, 46137344, 0), (16822829056, 5767168, 51904512, 0), (16828596224, 5767168, 57671680, 0), (16817061888, 5767168, 63438848, 0), (16909336576, 5767168, 69206016, 0), (16915103744, 5767168, 74973184, 0), (16903569408, 5767168, 80740352, 0), (16632512512, 5767168, 86507520, 0), (16638279680, 5767168, 92274688, 0), (16626745344, 5767168, 98041856, 0), (16390291456, 5767168, 103809024, 0), (16396058624, 5767168, 109576192, 0), (16384524288, 5767168, 115343360, 0), (16961241088, 5767168, 121110528, 0), (16967008256, 5767168, 126877696, 0), (16955473920, 5767168, 132644864, 0), (16442195968, 5767168, 138412032, 0), (16447963136, 5767168, 144179200, 0), (16436428800, 5767168, 149946368, 0), (16995844096, 5767168, 155713536, 0), (17001611264, 5767168, 161480704, 0), (16990076928, 5767168, 167247872, 0), (17047748608, 5767168, 173015040, 0), (17053515776, 5767168, 178782208, 0), (17041981440, 5767168, 184549376, 0), (16528703488, 5767168, 190316544, 0), (16534470656, 5767168, 196083712, 0), (16522936320, 5767168, 201850880, 0), (16546004992, 5767168, 207618048, 0), (16551772160, 5767168, 213385216, 0), (16540237824, 5767168, 219152384, 0), (16580608000, 5767168, 224919552, 0), (16586375168, 5767168, 230686720, 0), (16574840832, 5767168, 236453888, 0), (16615211008, 5767168, 242221056, 0), (16620978176, 5767168, 247988224, 0), (16609443840, 5767168, 253755392, 0), (17186160640, 5767168, 259522560, 0), (17191927808, 5767168, 265289728, 0), (17180393472, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e342199e0>, 2: <capsule object NULL at 0x7a4e34219a10>}
INFO 01-15 16:10:21.350276.350276 client.py:127] Model loaded
DEBUG 01-15 16:10:21.350267.350267 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.350972.350972 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.350541.350541 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.350028.350028 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.351225.351225 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9478a91b-f44c-444d-876e-4bc32c2322f9
DEBUG 01-15 16:10:21.351465.351465 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.351321.351321 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.351426.351426 cuda_h.py:19] end restore2model cost 0.001107931137084961 seconds
DEBUG 01-15 16:10:21.352309.352309 cuda_h.py:19] end sllm_worker_task cost 0.012950420379638672 seconds
DEBUG 01-15 16:10:21.352993.352993 cuda_h.py:19] end move_flatidxs cost 0.0008864402770996094 seconds
DEBUG 01-15 16:10:21.352975.352975 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:21.353200.353200 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9478a91b-f44c-444d-876e-4bc32c2322f9
DEBUG 01-15 16:10:21.354304.354304 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0051250457763671875 seconds
DEBUG 01-15 16:10:21.354366.354366 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.356966.356966 cuda_h.py:19] end restore2model cost 0.0026531219482421875 seconds
DEBUG 01-15 16:10:21.357008.357008 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008041858673095703 seconds
DEBUG 01-15 16:10:21.357519.357519 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.357530.357530 cuda_h.py:19] end gpu_sexperts cost 0.0002918243408203125 seconds
DEBUG 01-15 16:10:21.357412.357412 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.357196.357196 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1696090698242188e-05 seconds
DEBUG 01-15 16:10:21.357653.357653 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.357787.357787 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.358365.358365 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008165836334228516 seconds
DEBUG 01-15 16:10:21.358493.358493 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.358268.358268 cuda_h.py:19] end gpu_group_list cost 0.00019097328186035156 seconds
DEBUG 01-15 16:10:21.359353.359353 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.360199.360199 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008716583251953125 seconds
DEBUG 01-15 16:10:21.360420.360420 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.360473.360473 cuda_h.py:19] end gpu_group_list cost 0.00018644332885742188 seconds
DEBUG 01-15 16:10:21.361606.361606 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.361674.361674 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9478a91b-f44c-444d-876e-4bc32c2322f9
DEBUG 01-15 16:10:21.362594.362594 cuda_h.py:19] end group_tensors cost 0.009925365447998047 seconds
DEBUG 01-15 16:10:21.362674.362674 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.366533.366533 cuda_h.py:19] end group pad cost 0.0034618377685546875 seconds
DEBUG 01-15 16:10:21.366899.366899 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.380562.380562 client.py:127] Model loaded
DEBUG 01-15 16:10:21.380075.380075 cuda_h.py:19] end wait_experts_multi_device cost 0.01910853385925293 seconds
DEBUG 01-15 16:10:21.380076.380076 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.387850.387850 cuda_h.py:19] end group_einsum cost 0.021399974822998047 seconds
DEBUG 01-15 16:10:21.388929.388929 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.391130.391130 cuda_h.py:19] end get_outputs_cpu1 cost 0.003763437271118164 seconds
DEBUG 01-15 16:10:21.392405.392405 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04197072982788086 seconds
DEBUG 01-15 16:10:21.393780.393780 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012860536575317383 seconds
DEBUG 01-15 16:10:21.393009.393009 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.395571.395571 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.395221.395221 cuda_h.py:19] end index_scatter cost 0.00011873245239257812 seconds
DEBUG 01-15 16:10:21.395030.395030 cuda_h.py:19] end cpuoutputsdeal cost 0.0023355484008789062 seconds
DEBUG 01-15 16:10:21.396822.396822 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.396202.396202 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.396940.396940 cuda_h.py:19] end gpu_group_tensor cost 0.0002338886260986328 seconds
DEBUG 01-15 16:10:21.396904.396904 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.396099.396099 cuda_h.py:19] end gpu_group_tensor cost 0.00022029876708984375 seconds
DEBUG 01-15 16:10:21.396907.396907 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.397105.397105 cuda_h.py:19] end gpu_group_einsum cost 0.0008988380432128906 seconds
DEBUG 01-15 16:10:21.398615.398615 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.398960.398960 cuda_h.py:19] end gpu_group_einsum cost 0.0007121562957763672 seconds
DEBUG 01-15 16:10:21.399702.399702 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.399470.399470 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.399085.399085 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003039836883544922 seconds
DEBUG 01-15 16:10:21.399558.399558 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.399013.399013 cuda_h.py:19] end concat_expert_out cost 9.036064147949219e-05 seconds
DEBUG 01-15 16:10:21.400938.400938 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.400361.400361 cuda_h.py:19] end index_scatter cost 0.00010061264038085938 seconds
DEBUG 01-15 16:10:21.400561.400561 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001416921615600586 seconds
DEBUG 01-15 16:10:21.400699.400699 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.400702.400702 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.401679.401679 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023365020751953125 seconds
DEBUG 01-15 16:10:21.401522.401522 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.401071.401071 cuda_h.py:19] end concat_expert_out cost 9.417533874511719e-05 seconds
DEBUG 01-15 16:10:21.401989.401989 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.401312.401312 cuda_h.py:19] end index_scatter cost 9.5367431640625e-05 seconds
DEBUG 01-15 16:10:21.401222.401222 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009069442749023438 seconds
DEBUG 01-15 16:10:21.401146.401146 cuda_h.py:19] end gpu_experts_multi_device cost 0.044364213943481445 seconds
DEBUG 01-15 16:10:21.402012.402012 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.05623364448547363 seconds
DEBUG 01-15 16:10:21.402316.402316 cuda_h.py:19] end prefill_layer cost 0.06441593170166016 seconds
DEBUG 01-15 16:10:21.402015.402015 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:10:21.402851.402851 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.402833.402833 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:10:21.402338.402338 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:21.402897.402897 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:21.403305.403305 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 6.628036499023438e-05 seconds
DEBUG 01-15 16:10:21.403492.403492 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 0.0001404285430908203 seconds
DEBUG 01-15 16:10:21.403799.403799 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.403929.403929 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.403272.403272 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.403158.403158 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.403580.403580 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.404775.404775 cuda_h.py:19] end allocate_cuda_memory cost 0.0003952980041503906 seconds
DEBUG 01-15 16:10:21.404218.404218 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.404719.404719 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.404405.404405 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.404746.404746 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 380b7462-cd7c-404f-bd7a-4d4c6b0ed4e4
DEBUG 01-15 16:10:21.405766.405766 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.405459.405459 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.406566.406566 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 380b7462-cd7c-404f-bd7a-4d4c6b0ed4e4
DEBUG 01-15 16:10:21.406346.406346 cuda_h.py:19] end load_into_gpu_async cost 0.0021085739135742188 seconds
DEBUG 01-15 16:10:21.406661.406661 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.407384.407384 cuda_h.py:19] end restore_tensors2 cost 0.0001583099365234375 seconds
DEBUG 01-15 16:10:21.407149.407149 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0034134387969970703 seconds
INFO 01-15 16:10:21.407777.407777 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 380b7462-cd7c-404f-bd7a-4d4c6b0ed4e4
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.409570.409570 cuda_h.py:19] end self_attn cost 0.004232168197631836 seconds
DEBUG 01-15 16:10:21.409323.409323 cuda_h.py:19] end iln_self_attn_paln cost 0.00668787956237793 seconds
DEBUG 01-15 16:10:21.410059.410059 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:10:21.410630.410630 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.410960.410960 cuda_h.py:19] end gate cost 0.0007338523864746094 seconds
DEBUG 01-15 16:10:21.410082.410082 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.411457.411457 lmp.py:1912] 
DEBUG 01-15 16:10:21.411457.411457 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.411505.411505 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.411493.411493 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.411712.411712 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.411308.411308 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.411951.411951 lmp.py:1917] 
DEBUG 01-15 16:10:21.411951.411951 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.411786.411786 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.411251.411251 lmp.py:1935]   Expert 34 |     29 | CPU
DEBUG 01-15 16:10:21.411801.411801 lmp.py:1935]   Expert  7 |     32 | CPU
DEBUG 01-15 16:10:21.411160.411160 lmp.py:1935]   Expert 13 |     41 | CPU
DEBUG 01-15 16:10:21.411518.411518 lmp.py:1935]   Expert 54 |     76 | CPU
DEBUG 01-15 16:10:21.411638.411638 lmp.py:1935]   Expert 18 |     84 | CPU
DEBUG 01-15 16:10:21.411757.411757 lmp.py:1935]   Expert 39 |     86 | CPU
DEBUG 01-15 16:10:21.411877.411877 lmp.py:1935]   Expert 49 |     87 | CPU
DEBUG 01-15 16:10:21.411189.411189 lmp.py:1935]   Expert 59 |    101 | CPU
DEBUG 01-15 16:10:21.411978.411978 lmp.py:1935]   Expert 16 |    105 | CPU
DEBUG 01-15 16:10:21.411767.411767 lmp.py:1935]   Expert  0 |    107 | CPU
DEBUG 01-15 16:10:21.411602.411602 lmp.py:1935]   Expert 21 |    109 | CPU
DEBUG 01-15 16:10:21.411245.411245 lmp.py:1935]   Expert 41 |    116 | CPU
DEBUG 01-15 16:10:21.411888.411888 lmp.py:1935]   Expert 15 |    121 | CPU
DEBUG 01-15 16:10:21.411531.411531 lmp.py:1935]   Expert 22 |    122 | CPU
DEBUG 01-15 16:10:21.411174.411174 lmp.py:1935]   Expert 45 |    122 | CPU
DEBUG 01-15 16:10:21.411817.411817 lmp.py:1935]   Expert 17 |    127 | CPU
DEBUG 01-15 16:10:21.411459.411459 lmp.py:1935]   Expert 61 |    131 | CPU
DEBUG 01-15 16:10:21.411864.411864 lmp.py:1935]   Expert 52 |    136 | CPU
DEBUG 01-15 16:10:21.411984.411984 lmp.py:1935]   Expert 35 |    137 | CPU
DEBUG 01-15 16:10:21.411296.411296 lmp.py:1935]   Expert  8 |    138 | CPU
DEBUG 01-15 16:10:21.411131.411131 lmp.py:1935]   Expert 38 |    139 | CPU
DEBUG 01-15 16:10:21.411204.411204 lmp.py:1935]   Expert 12 |    143 | CPU
DEBUG 01-15 16:10:21.411086.411086 lmp.py:1935]   Expert 48 |    147 | CPU
DEBUG 01-15 16:10:21.411967.411967 lmp.py:1935]   Expert 31 |    149 | CPU
DEBUG 01-15 16:10:21.411610.411610 lmp.py:1935]   Expert 36 |    156 | CPU
DEBUG 01-15 16:10:21.411253.411253 lmp.py:1935]   Expert 53 |    156 | CPU
DEBUG 01-15 16:10:21.411611.411611 lmp.py:1935]   Expert 50 |    159 | CPU
DEBUG 01-15 16:10:21.411254.411254 lmp.py:1935]   Expert 60 |    160 | CPU
DEBUG 01-15 16:10:21.411136.411136 lmp.py:1935]   Expert 40 |    161 | CPU
DEBUG 01-15 16:10:21.411779.411779 lmp.py:1935]   Expert 27 |    175 | CPU
DEBUG 01-15 16:10:21.411660.411660 lmp.py:1935]   Expert 19 |    194 | CPU
DEBUG 01-15 16:10:21.411402.411402 lmp.py:1935]   Expert  4 |    200 | CPU
DEBUG 01-15 16:10:21.411767.411767 lmp.py:1935]   Expert 29 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.411702.411702 lmp.py:1935]   Expert 30 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412160.412160 lmp.py:1935]   Expert 11 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412379.412379 lmp.py:1935]   Expert 20 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412837.412837 lmp.py:1935]   Expert 26 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412294.412294 lmp.py:1935]   Expert 57 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412513.412513 lmp.py:1935]   Expert  6 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412494.412494 lmp.py:1935]   Expert 43 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412714.412714 lmp.py:1935]   Expert 46 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412171.412171 lmp.py:1935]   Expert  2 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412583.412583 lmp.py:1935]   Expert 23 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412756.412756 lmp.py:1935]   Expert 33 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412498.412498 lmp.py:1935]   Expert 42 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412240.412240 lmp.py:1935]   Expert 55 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412221.412221 lmp.py:1935]   Expert 32 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412202.412202 lmp.py:1935]   Expert 56 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412706.412706 lmp.py:1935]   Expert  9 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412448.412448 lmp.py:1935]   Expert  3 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412952.412952 lmp.py:1935]   Expert 14 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412616.412616 lmp.py:1935]   Expert 28 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412266.412266 lmp.py:1935]   Expert  1 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412723.412723 lmp.py:1935]   Expert 51 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412942.412942 lmp.py:1935]   Expert 58 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412923.412923 lmp.py:1935]   Expert 44 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412666.412666 lmp.py:1935]   Expert 63 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412170.412170 lmp.py:1935]   Expert 37 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412912.412912 lmp.py:1935]   Expert 47 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412416.412416 lmp.py:1935]   Expert 24 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412920.412920 lmp.py:1935]   Expert 10 |    312 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412663.412663 lmp.py:1935]   Expert 62 |    312 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412597.412597 lmp.py:1935]   Expert 25 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.412293.412293 lmp.py:1935]   Expert  5 |    363 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.412036.412036 lmp.py:1937] 
DEBUG 01-15 16:10:21.412036.412036 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.412970.412970 lmp.py:1938]   CPU:   3946 tokens
DEBUG 01-15 16:10:21.412951.412951 lmp.py:1942]   cuda:1:   4173 tokens (16 experts)
DEBUG 01-15 16:10:21.412693.412693 lmp.py:1942]   cuda:2:   4169 tokens (16 experts)
DEBUG 01-15 16:10:21.412528.412528 lmp.py:1943]   Total GPU:   8342 tokens
DEBUG 01-15 16:10:21.412317.412317 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.412317.412317 lmp.py:1944] 
DEBUG 01-15 16:10:21.412020.412020 cuda_h.py:19] end experts_map_get cost 0.001888275146484375 seconds
DEBUG 01-15 16:10:21.412573.412573 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.412859.412859 lmp.py:1953] 
DEBUG 01-15 16:10:21.412859.412859 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.412463.412463 cuda_h.py:19] end cpu_experts_submit cost 5.91278076171875e-05 seconds
DEBUG 01-15 16:10:21.413828.413828 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.413764.413764 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.413271.413271 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.32.gate_proj.weight': 0, 'model.layers.14.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.26.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.26.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.26.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.43.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.43.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.43.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.11.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.11.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.11.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.14.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.14.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.14.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.47.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.47.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.47.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.14.mlp.experts.1.gate_proj.weight': 0, 'model.layers.14.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.55.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.55.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.55.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.57.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(17809014784, 5767168, 0, 0), (17814781952, 5767168, 5767168, 0), (17803247616, 5767168, 11534336, 0), (17826316288, 5767168, 17301504, 0), (17832083456, 5767168, 23068672, 0), (17820549120, 5767168, 28835840, 0), (17705205760, 5767168, 34603008, 0), (17710972928, 5767168, 40370176, 0), (17699438592, 5767168, 46137344, 0), (17341874176, 5767168, 51904512, 0), (17347641344, 5767168, 57671680, 0), (17336107008, 5767168, 63438848, 0), (17359175680, 5767168, 69206016, 0), (17364942848, 5767168, 74973184, 0), (17353408512, 5767168, 80740352, 0), (17411080192, 5767168, 86507520, 0), (17416847360, 5767168, 92274688, 0), (17405313024, 5767168, 98041856, 0), (17982029824, 5767168, 103809024, 0), (17987796992, 5767168, 109576192, 0), (17976262656, 5767168, 115343360, 0), (17999331328, 5767168, 121110528, 0), (18005098496, 5767168, 126877696, 0), (17993564160, 5767168, 132644864, 0), (17445683200, 5767168, 138412032, 0), (17451450368, 5767168, 144179200, 0), (17439916032, 5767168, 149946368, 0), (17497587712, 5767168, 155713536, 0), (17503354880, 5767168, 161480704, 0), (17491820544, 5767168, 167247872, 0), (18068537344, 5767168, 173015040, 0), (18074304512, 5767168, 178782208, 0), (18062770176, 5767168, 184549376, 0), (18137743360, 5767168, 190316544, 0), (18143510528, 5767168, 196083712, 0), (18131976192, 5767168, 201850880, 0), (18258853888, 5767168, 207618048, 0), (18264621056, 5767168, 213385216, 0), (18253086720, 5767168, 219152384, 0), (17757110272, 5767168, 224919552, 0), (17762877440, 5767168, 230686720, 0), (17751343104, 5767168, 236453888, 0), (18328059904, 5767168, 242221056, 0), (18333827072, 5767168, 247988224, 0), (18322292736, 5767168, 253755392, 0), (18345361408, 5767168, 259522560, 0), (18351128576, 5767168, 265289728, 0), (18339594240, 5767168, 271056896, 0)], 2: [(17272668160, 5767168, 0, 0), (17278435328, 5767168, 5767168, 0), (17266900992, 5767168, 11534336, 0), (17289969664, 5767168, 17301504, 0), (17295736832, 5767168, 23068672, 0), (17284202496, 5767168, 28835840, 0), (17307271168, 5767168, 34603008, 0), (17313038336, 5767168, 40370176, 0), (17301504000, 5767168, 46137344, 0), (17895522304, 5767168, 51904512, 0), (17901289472, 5767168, 57671680, 0), (17889755136, 5767168, 63438848, 0), (17428381696, 5767168, 69206016, 0), (17434148864, 5767168, 74973184, 0), (17422614528, 5767168, 80740352, 0), (18016632832, 5767168, 86507520, 0), (18022400000, 5767168, 92274688, 0), (18010865664, 5767168, 98041856, 0), (18051235840, 5767168, 103809024, 0), (18057003008, 5767168, 109576192, 0), (18045468672, 5767168, 115343360, 0), (17653301248, 5767168, 121110528, 0), (17659068416, 5767168, 126877696, 0), (17647534080, 5767168, 132644864, 0), (17601396736, 5767168, 138412032, 0), (17607163904, 5767168, 144179200, 0), (17595629568, 5767168, 149946368, 0), (18206949376, 5767168, 155713536, 0), (18212716544, 5767168, 161480704, 0), (18201182208, 5767168, 167247872, 0), (17670602752, 5767168, 173015040, 0), (17676369920, 5767168, 178782208, 0), (17664835584, 5767168, 184549376, 0), (17687904256, 5767168, 190316544, 0), (17693671424, 5767168, 196083712, 0), (17682137088, 5767168, 201850880, 0), (18224250880, 5767168, 207618048, 0), (18230018048, 5767168, 213385216, 0), (18218483712, 5767168, 219152384, 0), (17739808768, 5767168, 224919552, 0), (17745575936, 5767168, 230686720, 0), (17734041600, 5767168, 236453888, 0), (17774411776, 5767168, 242221056, 0), (17780178944, 5767168, 247988224, 0), (17768644608, 5767168, 253755392, 0), (18241552384, 5767168, 259522560, 0), (18247319552, 5767168, 265289728, 0), (18235785216, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4740690>, 2: <capsule object NULL at 0x7a4e34219cb0>}
DEBUG 01-15 16:10:21.414169.414169 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.414694.414694 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.414166.414166 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 20956a24-02ba-484b-a3ee-dc3cfd386a9e
DEBUG 01-15 16:10:21.414576.414576 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.414020.414020 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:21.414360.414360 client.py:127] Model loaded
DEBUG 01-15 16:10:21.414808.414808 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.415800.415800 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.416044.416044 cuda_h.py:19] end move_flatidxs cost 0.0008795261383056641 seconds
DEBUG 01-15 16:10:21.416620.416620 cuda_h.py:19] end restore2model cost 0.0010306835174560547 seconds
DEBUG 01-15 16:10:21.416125.416125 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:21.416035.416035 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 20956a24-02ba-484b-a3ee-dc3cfd386a9e
DEBUG 01-15 16:10:21.416641.416641 cuda_h.py:19] end sllm_worker_task cost 0.012653112411499023 seconds
DEBUG 01-15 16:10:21.416730.416730 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003692150115966797 seconds
DEBUG 01-15 16:10:21.416649.416649 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.419041.419041 cuda_h.py:19] end restore2model cost 0.0025718212127685547 seconds
DEBUG 01-15 16:10:21.419692.419692 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006582021713256836 seconds
DEBUG 01-15 16:10:21.419772.419772 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.419061.419061 cuda_h.py:19] end gpu_sexperts cost 0.00028777122497558594 seconds
DEBUG 01-15 16:10:21.420890.420890 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.420813.420813 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8358230590820312e-05 seconds
DEBUG 01-15 16:10:21.420462.420462 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.420642.420642 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.420505.420505 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008156299591064453 seconds
DEBUG 01-15 16:10:21.421964.421964 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.421521.421521 cuda_h.py:19] end gpu_group_list cost 0.00020623207092285156 seconds
DEBUG 01-15 16:10:21.421679.421679 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.421775.421775 cuda_h.py:19] end group_tensors cost 0.0056955814361572266 seconds
DEBUG 01-15 16:10:21.422177.422177 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.422624.422624 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008852481842041016 seconds
DEBUG 01-15 16:10:21.422375.422375 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.423545.423545 cuda_h.py:19] end gpu_group_list cost 0.00030493736267089844 seconds
DEBUG 01-15 16:10:21.423896.423896 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.423772.423772 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 20956a24-02ba-484b-a3ee-dc3cfd386a9e
DEBUG 01-15 16:10:21.426882.426882 cuda_h.py:19] end group pad cost 0.003971576690673828 seconds
DEBUG 01-15 16:10:21.426917.426917 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.443955.443955 client.py:127] Model loaded
DEBUG 01-15 16:10:21.443052.443052 cuda_h.py:19] end wait_experts_multi_device cost 0.019218921661376953 seconds
DEBUG 01-15 16:10:21.443537.443537 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.452158.452158 cuda_h.py:19] end group_einsum cost 0.025505781173706055 seconds
DEBUG 01-15 16:10:21.452687.452687 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.456211.456211 cuda_h.py:19] end get_outputs_cpu1 cost 0.003888845443725586 seconds
DEBUG 01-15 16:10:21.457897.457897 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01463627815246582 seconds
DEBUG 01-15 16:10:21.458934.458934 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.459438.459438 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04462885856628418 seconds
DEBUG 01-15 16:10:21.460804.460804 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.460977.460977 cuda_h.py:19] end index_scatter cost 0.00011658668518066406 seconds
DEBUG 01-15 16:10:21.460550.460550 cuda_h.py:19] end cpuoutputsdeal cost 0.0026903152465820312 seconds
DEBUG 01-15 16:10:21.461634.461634 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.461398.461398 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.461944.461944 cuda_h.py:19] end gpu_group_tensor cost 0.0002315044403076172 seconds
DEBUG 01-15 16:10:21.461616.461616 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.461758.461758 cuda_h.py:19] end gpu_group_tensor cost 0.00021696090698242188 seconds
DEBUG 01-15 16:10:21.461472.461472 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.462592.462592 cuda_h.py:19] end gpu_group_einsum cost 0.0009095668792724609 seconds
DEBUG 01-15 16:10:21.463440.463440 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.463831.463831 cuda_h.py:19] end gpu_group_einsum cost 0.0007174015045166016 seconds
DEBUG 01-15 16:10:21.464765.464765 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.464056.464056 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.464307.464307 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003211498260498047 seconds
DEBUG 01-15 16:10:21.464845.464845 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.464485.464485 cuda_h.py:19] end concat_expert_out cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:10:21.464024.464024 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.465373.465373 cuda_h.py:19] end index_scatter cost 6.818771362304688e-05 seconds
DEBUG 01-15 16:10:21.465389.465389 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011479854583740234 seconds
DEBUG 01-15 16:10:21.465830.465830 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.465548.465548 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.465613.465613 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014901161193847656 seconds
DEBUG 01-15 16:10:21.465237.465237 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.465996.465996 cuda_h.py:19] end concat_expert_out cost 6.29425048828125e-05 seconds
DEBUG 01-15 16:10:21.465813.465813 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.466969.466969 cuda_h.py:19] end index_scatter cost 6.222724914550781e-05 seconds
DEBUG 01-15 16:10:21.466130.466130 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005931854248046875 seconds
DEBUG 01-15 16:10:21.466868.466868 cuda_h.py:19] end gpu_experts_multi_device cost 0.04609394073486328 seconds
DEBUG 01-15 16:10:21.466381.466381 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.05624222755432129 seconds
DEBUG 01-15 16:10:21.466210.466210 cuda_h.py:19] end prefill_layer cost 0.06386590003967285 seconds
DEBUG 01-15 16:10:21.466768.466768 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:10:21.466531.466531 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.466294.466294 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:10:21.466533.466533 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:21.466826.466826 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:21.467796.467796 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 4.649162292480469e-05 seconds
DEBUG 01-15 16:10:21.467612.467612 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 8.916854858398438e-05 seconds
DEBUG 01-15 16:10:21.467414.467414 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.467524.467524 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.467164.467164 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.467998.467998 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.467262.467262 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.468935.468935 cuda_h.py:19] end allocate_cuda_memory cost 0.00039649009704589844 seconds
DEBUG 01-15 16:10:21.468730.468730 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.468648.468648 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.468500.468500 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.468655.468655 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b54ac2c8-eccc-435f-84bd-efacfed68ede
DEBUG 01-15 16:10:21.468384.468384 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.469724.469724 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.471449.471449 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b54ac2c8-eccc-435f-84bd-efacfed68ede
DEBUG 01-15 16:10:21.471322.471322 cuda_h.py:19] end load_into_gpu_async cost 0.0029163360595703125 seconds
DEBUG 01-15 16:10:21.471258.471258 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.471274.471274 cuda_h.py:19] end restore_tensors2 cost 0.00015020370483398438 seconds
DEBUG 01-15 16:10:21.471284.471284 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004273891448974609 seconds
INFO 01-15 16:10:21.472613.472613 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b54ac2c8-eccc-435f-84bd-efacfed68ede
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.473260.473260 cuda_h.py:19] end self_attn cost 0.004277229309082031 seconds
DEBUG 01-15 16:10:21.473804.473804 cuda_h.py:19] end iln_self_attn_paln cost 0.006843090057373047 seconds
DEBUG 01-15 16:10:21.474753.474753 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:10:21.474152.474152 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.474199.474199 cuda_h.py:19] end gate cost 0.0007615089416503906 seconds
DEBUG 01-15 16:10:21.474697.474697 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.475648.475648 lmp.py:1912] 
DEBUG 01-15 16:10:21.475648.475648 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.475881.475881 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.475345.475345 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.475088.475088 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.475400.475400 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.475281.475281 lmp.py:1917] 
DEBUG 01-15 16:10:21.475281.475281 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.475401.475401 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.475766.475766 lmp.py:1935]   Expert 15 |     63 | CPU
DEBUG 01-15 16:10:21.475171.475171 lmp.py:1935]   Expert 41 |     70 | CPU
DEBUG 01-15 16:10:21.475098.475098 lmp.py:1935]   Expert  0 |     74 | CPU
DEBUG 01-15 16:10:21.475788.475788 lmp.py:1935]   Expert 63 |     76 | CPU
DEBUG 01-15 16:10:21.475000.475000 lmp.py:1935]   Expert 20 |     83 | CPU
DEBUG 01-15 16:10:21.475451.475451 lmp.py:1935]   Expert 45 |     90 | CPU
DEBUG 01-15 16:10:21.475856.475856 lmp.py:1935]   Expert  7 |     93 | CPU
DEBUG 01-15 16:10:21.475306.475306 lmp.py:1935]   Expert 28 |     98 | CPU
DEBUG 01-15 16:10:21.475711.475711 lmp.py:1935]   Expert 54 |    108 | CPU
DEBUG 01-15 16:10:21.475308.475308 lmp.py:1935]   Expert 12 |    109 | CPU
DEBUG 01-15 16:10:21.475427.475427 lmp.py:1935]   Expert 40 |    120 | CPU
DEBUG 01-15 16:10:21.475309.475309 lmp.py:1935]   Expert 52 |    120 | CPU
DEBUG 01-15 16:10:21.475713.475713 lmp.py:1935]   Expert 59 |    122 | CPU
DEBUG 01-15 16:10:21.475403.475403 lmp.py:1935]   Expert  5 |    123 | CPU
DEBUG 01-15 16:10:21.475522.475522 lmp.py:1935]   Expert  4 |    131 | CPU
DEBUG 01-15 16:10:21.475165.475165 lmp.py:1935]   Expert 34 |    132 | CPU
DEBUG 01-15 16:10:21.475047.475047 lmp.py:1935]   Expert 62 |    136 | CPU
DEBUG 01-15 16:10:21.475167.475167 lmp.py:1935]   Expert 13 |    137 | CPU
DEBUG 01-15 16:10:21.475763.475763 lmp.py:1935]   Expert 61 |    137 | CPU
DEBUG 01-15 16:10:21.475837.475837 lmp.py:1935]   Expert 42 |    138 | CPU
DEBUG 01-15 16:10:21.475672.475672 lmp.py:1935]   Expert 55 |    139 | CPU
DEBUG 01-15 16:10:21.475792.475792 lmp.py:1935]   Expert 21 |    140 | CPU
DEBUG 01-15 16:10:21.475958.475958 lmp.py:1935]   Expert 14 |    145 | CPU
DEBUG 01-15 16:10:21.475362.475362 lmp.py:1935]   Expert 10 |    147 | CPU
DEBUG 01-15 16:10:21.475244.475244 lmp.py:1935]   Expert 22 |    152 | CPU
DEBUG 01-15 16:10:21.475125.475125 lmp.py:1935]   Expert 51 |    156 | CPU
DEBUG 01-15 16:10:21.475291.475291 lmp.py:1935]   Expert 32 |    158 | CPU
DEBUG 01-15 16:10:21.475696.475696 lmp.py:1935]   Expert 25 |    166 | CPU
DEBUG 01-15 16:10:21.475339.475339 lmp.py:1935]   Expert 50 |    174 | CPU
DEBUG 01-15 16:10:21.475982.475982 lmp.py:1935]   Expert 47 |    175 | CPU
DEBUG 01-15 16:10:21.475625.475625 lmp.py:1935]   Expert  1 |    176 | CPU
DEBUG 01-15 16:10:21.475029.475029 lmp.py:1935]   Expert 53 |    176 | CPU
DEBUG 01-15 16:10:21.475010.475010 lmp.py:1935]   Expert 26 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476514.476514 lmp.py:1935]   Expert 19 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476541.476541 lmp.py:1935]   Expert  6 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476376.476376 lmp.py:1935]   Expert 35 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476496.476496 lmp.py:1935]   Expert 11 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476331.476331 lmp.py:1935]   Expert  2 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476690.476690 lmp.py:1935]   Expert 30 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476525.476525 lmp.py:1935]   Expert 56 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476883.476883 lmp.py:1935]   Expert 57 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476718.476718 lmp.py:1935]   Expert 48 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476315.476315 lmp.py:1935]   Expert 44 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476793.476793 lmp.py:1935]   Expert 16 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476628.476628 lmp.py:1935]   Expert 24 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476847.476847 lmp.py:1935]   Expert 46 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476682.476682 lmp.py:1935]   Expert 18 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476848.476848 lmp.py:1935]   Expert 39 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476776.476776 lmp.py:1935]   Expert 29 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476704.476704 lmp.py:1935]   Expert 37 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476631.476631 lmp.py:1935]   Expert 31 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476082.476082 lmp.py:1935]   Expert 36 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476248.476248 lmp.py:1935]   Expert 60 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476190.476190 lmp.py:1935]   Expert  3 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476117.476117 lmp.py:1935]   Expert 38 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476045.476045 lmp.py:1935]   Expert  9 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476741.476741 lmp.py:1935]   Expert 17 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476576.476576 lmp.py:1935]   Expert 23 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476696.476696 lmp.py:1935]   Expert 27 |    349 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476101.476101 lmp.py:1935]   Expert 43 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476505.476505 lmp.py:1935]   Expert  8 |    399 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476433.476433 lmp.py:1935]   Expert 33 |    399 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476599.476599 lmp.py:1935]   Expert 58 |    444 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.476765.476765 lmp.py:1935]   Expert 49 |    537 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.476739.476739 lmp.py:1937] 
DEBUG 01-15 16:10:21.476739.476739 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.476144.476144 lmp.py:1938]   CPU:   4064 tokens
DEBUG 01-15 16:10:21.476548.476548 lmp.py:1942]   cuda:1:   4124 tokens (16 experts)
DEBUG 01-15 16:10:21.476238.476238 lmp.py:1942]   cuda:2:   4100 tokens (16 experts)
DEBUG 01-15 16:10:21.476212.476212 lmp.py:1943]   Total GPU:   8224 tokens
DEBUG 01-15 16:10:21.476947.476947 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.476947.476947 lmp.py:1944] 
DEBUG 01-15 16:10:21.476835.476835 cuda_h.py:19] end experts_map_get cost 0.001722097396850586 seconds
DEBUG 01-15 16:10:21.476539.476539 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.476580.476580 lmp.py:1953] 
DEBUG 01-15 16:10:21.476580.476580 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.476463.476463 cuda_h.py:19] end cpu_experts_submit cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:10:21.476728.476728 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.476366.476366 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.478233.478233 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.33.gate_proj.weight': 0, 'model.layers.15.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.44.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.44.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.44.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.49.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.49.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.49.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.57.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.57.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.57.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.26.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.26.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.26.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.27.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.27.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.27.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.60.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.60.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.60.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.15.mlp.experts.2.gate_proj.weight': 0, 'model.layers.15.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.39.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.39.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.39.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.9.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.9.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.9.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.43.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.43.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.43.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.46.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.46.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.46.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.48.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.48.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.48.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.19.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.19.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.19.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.58.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.58.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.58.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(18933612544, 5767168, 0, 0), (18939379712, 5767168, 5767168, 0), (18927845376, 5767168, 11534336, 0), (18968215552, 5767168, 17301504, 0), (18973982720, 5767168, 23068672, 0), (18962448384, 5767168, 28835840, 0), (19020120064, 5767168, 34603008, 0), (19025887232, 5767168, 40370176, 0), (19014352896, 5767168, 46137344, 0), (18552979456, 5767168, 51904512, 0), (18558746624, 5767168, 57671680, 0), (18547212288, 5767168, 63438848, 0), (19123929088, 5767168, 69206016, 0), (19129696256, 5767168, 74973184, 0), (19118161920, 5767168, 80740352, 0), (19210436608, 5767168, 86507520, 0), (19216203776, 5767168, 92274688, 0), (19204669440, 5767168, 98041856, 0), (18656788480, 5767168, 103809024, 0), (18662555648, 5767168, 109576192, 0), (18651021312, 5767168, 115343360, 0), (18674089984, 5767168, 121110528, 0), (18679857152, 5767168, 126877696, 0), (18668322816, 5767168, 132644864, 0), (18777899008, 5767168, 138412032, 0), (18783666176, 5767168, 144179200, 0), (18772131840, 5767168, 149946368, 0), (19348848640, 5767168, 155713536, 0), (19354615808, 5767168, 161480704, 0), (19343081472, 5767168, 167247872, 0), (18812502016, 5767168, 173015040, 0), (18818269184, 5767168, 178782208, 0), (18806734848, 5767168, 184549376, 0), (18829803520, 5767168, 190316544, 0), (18835570688, 5767168, 196083712, 0), (18824036352, 5767168, 201850880, 0), (19400753152, 5767168, 207618048, 0), (19406520320, 5767168, 213385216, 0), (19394985984, 5767168, 219152384, 0), (18864406528, 5767168, 224919552, 0), (18870173696, 5767168, 230686720, 0), (18858639360, 5767168, 236453888, 0), (18881708032, 5767168, 242221056, 0), (18887475200, 5767168, 247988224, 0), (18875940864, 5767168, 253755392, 0), (18899009536, 5767168, 259522560, 0), (18904776704, 5767168, 265289728, 0), (18893242368, 5767168, 271056896, 0)], 2: [(18397265920, 5767168, 0, 0), (18403033088, 5767168, 5767168, 0), (18391498752, 5767168, 11534336, 0), (18414567424, 5767168, 17301504, 0), (18420334592, 5767168, 23068672, 0), (18408800256, 5767168, 28835840, 0), (18985517056, 5767168, 34603008, 0), (18991284224, 5767168, 40370176, 0), (18979749888, 5767168, 46137344, 0), (19002818560, 5767168, 51904512, 0), (19008585728, 5767168, 57671680, 0), (18997051392, 5767168, 63438848, 0), (18466471936, 5767168, 69206016, 0), (18472239104, 5767168, 74973184, 0), (18460704768, 5767168, 80740352, 0), (19037421568, 5767168, 86507520, 0), (19043188736, 5767168, 92274688, 0), (19031654400, 5767168, 98041856, 0), (18501074944, 5767168, 103809024, 0), (18506842112, 5767168, 109576192, 0), (18495307776, 5767168, 115343360, 0), (18518376448, 5767168, 121110528, 0), (18524143616, 5767168, 126877696, 0), (18512609280, 5767168, 132644864, 0), (19106627584, 5767168, 138412032, 0), (19112394752, 5767168, 144179200, 0), (19100860416, 5767168, 149946368, 0), (19158532096, 5767168, 155713536, 0), (19164299264, 5767168, 161480704, 0), (19152764928, 5767168, 167247872, 0), (18639486976, 5767168, 173015040, 0), (18645254144, 5767168, 178782208, 0), (18633719808, 5767168, 184549376, 0), (19193135104, 5767168, 190316544, 0), (19198902272, 5767168, 196083712, 0), (19187367936, 5767168, 201850880, 0), (18691391488, 5767168, 207618048, 0), (18697158656, 5767168, 213385216, 0), (18685624320, 5767168, 219152384, 0), (18760597504, 5767168, 224919552, 0), (18766364672, 5767168, 230686720, 0), (18754830336, 5767168, 236453888, 0), (19331547136, 5767168, 242221056, 0), (19337314304, 5767168, 247988224, 0), (19325779968, 5767168, 253755392, 0), (19366150144, 5767168, 259522560, 0), (19371917312, 5767168, 265289728, 0), (19360382976, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c0a3fc0>, 2: <capsule object NULL at 0x7a4e34219a70>}
DEBUG 01-15 16:10:21.478555.478555 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.478829.478829 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.478723.478723 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3cfbff6a-9ef0-4b94-95d0-acb9e42c5c0e
DEBUG 01-15 16:10:21.478398.478398 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:21.479234.479234 client.py:127] Model loaded
DEBUG 01-15 16:10:21.479112.479112 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.479191.479191 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:21.479169.479169 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3cfbff6a-9ef0-4b94-95d0-acb9e42c5c0e
DEBUG 01-15 16:10:21.479793.479793 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.479568.479568 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030303001403808594 seconds
DEBUG 01-15 16:10:21.480100.480100 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.480176.480176 cuda_h.py:19] end move_flatidxs cost 0.0008840560913085938 seconds
DEBUG 01-15 16:10:21.480959.480959 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.481244.481244 cuda_h.py:19] end restore2model cost 0.0010685920715332031 seconds
DEBUG 01-15 16:10:21.481210.481210 cuda_h.py:19] end sllm_worker_task cost 0.013858795166015625 seconds
DEBUG 01-15 16:10:21.483555.483555 cuda_h.py:19] end restore2model cost 0.0038404464721679688 seconds
DEBUG 01-15 16:10:21.484941.484941 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007122516632080078 seconds
DEBUG 01-15 16:10:21.484498.484498 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.484151.484151 cuda_h.py:19] end gpu_sexperts cost 0.00027441978454589844 seconds
DEBUG 01-15 16:10:21.484517.484517 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.484631.484631 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8835067749023438e-05 seconds
DEBUG 01-15 16:10:21.484950.484950 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.484799.484799 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.485370.485370 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008101463317871094 seconds
DEBUG 01-15 16:10:21.485875.485875 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.485989.485989 cuda_h.py:19] end gpu_group_list cost 0.00019431114196777344 seconds
DEBUG 01-15 16:10:21.485443.485443 cuda_h.py:19] end group_tensors cost 0.0046999454498291016 seconds
DEBUG 01-15 16:10:21.486550.486550 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.486696.486696 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.487743.487743 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009083747863769531 seconds
DEBUG 01-15 16:10:21.487771.487771 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.487050.487050 cuda_h.py:19] end gpu_group_list cost 0.00020694732666015625 seconds
DEBUG 01-15 16:10:21.488004.488004 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.488072.488072 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3cfbff6a-9ef0-4b94-95d0-acb9e42c5c0e
DEBUG 01-15 16:10:21.489123.489123 cuda_h.py:19] end group pad cost 0.0036003589630126953 seconds
DEBUG 01-15 16:10:21.489112.489112 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.507654.507654 client.py:127] Model loaded
DEBUG 01-15 16:10:21.508203.508203 cuda_h.py:19] end wait_experts_multi_device cost 0.01971149444580078 seconds
DEBUG 01-15 16:10:21.508442.508442 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.512178.512178 cuda_h.py:19] end group_einsum cost 0.022080659866333008 seconds
DEBUG 01-15 16:10:21.512687.512687 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.516610.516610 cuda_h.py:19] end get_outputs_cpu1 cost 0.003938436508178711 seconds
DEBUG 01-15 16:10:21.517746.517746 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03765511512756348 seconds
DEBUG 01-15 16:10:21.517515.517515 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.009452104568481445 seconds
DEBUG 01-15 16:10:21.517631.517631 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.519887.519887 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.519701.519701 cuda_h.py:19] end index_scatter cost 9.059906005859375e-05 seconds
DEBUG 01-15 16:10:21.519070.519070 cuda_h.py:19] end cpuoutputsdeal cost 0.001985311508178711 seconds
DEBUG 01-15 16:10:21.519776.519776 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.519274.519274 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.520295.520295 cuda_h.py:19] end gpu_group_tensor cost 0.00017905235290527344 seconds
DEBUG 01-15 16:10:21.520224.520224 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.520078.520078 cuda_h.py:19] end gpu_group_tensor cost 0.000164031982421875 seconds
DEBUG 01-15 16:10:21.520548.520548 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.521407.521407 cuda_h.py:19] end gpu_group_einsum cost 0.0006699562072753906 seconds
DEBUG 01-15 16:10:21.521664.521664 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.522645.522645 cuda_h.py:19] end gpu_group_einsum cost 0.0005462169647216797 seconds
DEBUG 01-15 16:10:21.522034.522034 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.522793.522793 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.522014.522014 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002105236053466797 seconds
DEBUG 01-15 16:10:21.522572.522572 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.522874.522874 cuda_h.py:19] end concat_expert_out cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:10:21.522420.522420 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.522213.522213 cuda_h.py:19] end index_scatter cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:10:21.523349.523349 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001043081283569336 seconds
DEBUG 01-15 16:10:21.523996.523996 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.523389.523389 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.523681.523681 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001704692840576172 seconds
DEBUG 01-15 16:10:21.523596.523596 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.523991.523991 cuda_h.py:19] end concat_expert_out cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:10:21.523676.523676 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.524223.524223 cuda_h.py:19] end index_scatter cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:10:21.524629.524629 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006580352783203125 seconds
DEBUG 01-15 16:10:21.524228.524228 cuda_h.py:19] end gpu_experts_multi_device cost 0.039713144302368164 seconds
DEBUG 01-15 16:10:21.524331.524331 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.05023932456970215 seconds
DEBUG 01-15 16:10:21.524592.524592 cuda_h.py:19] end prefill_layer cost 0.057936668395996094 seconds
DEBUG 01-15 16:10:21.524594.524594 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:10:21.524172.524172 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.524133.524133 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:10:21.524949.524949 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:21.525818.525818 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:21.525609.525609 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:21.525624.525624 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 9.894371032714844e-05 seconds
DEBUG 01-15 16:10:21.525148.525148 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.525854.525854 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.525417.525417 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.525519.525519 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.525838.525838 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.526530.526530 cuda_h.py:19] end allocate_cuda_memory cost 0.0003933906555175781 seconds
DEBUG 01-15 16:10:21.526259.526259 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.526481.526481 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.526857.526857 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.526562.526562 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9c5b3673-fdc2-4ca8-b5e3-60af4d2bb148
DEBUG 01-15 16:10:21.527940.527940 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.527974.527974 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.529819.529819 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9c5b3673-fdc2-4ca8-b5e3-60af4d2bb148
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
DEBUG 01-15 16:10:21.529814.529814 cuda_h.py:19] end load_into_gpu_async cost 0.0028045177459716797 seconds
DEBUG 01-15 16:10:21.529665.529665 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.529554.529554 cuda_h.py:19] end restore_tensors2 cost 0.00015044212341308594 seconds
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
DEBUG 01-15 16:10:21.529723.529723 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004144430160522461 seconds
INFO 01-15 16:10:21.530139.530139 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9c5b3673-fdc2-4ca8-b5e3-60af4d2bb148
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.530278.530278 cuda_h.py:19] end self_attn cost 0.0034580230712890625 seconds
DEBUG 01-15 16:10:21.531857.531857 cuda_h.py:19] end iln_self_attn_paln cost 0.005885601043701172 seconds
DEBUG 01-15 16:10:21.531210.531210 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:10:21.531588.531588 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.531009.531009 cuda_h.py:19] end gate cost 0.0006608963012695312 seconds
DEBUG 01-15 16:10:21.531361.531361 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.532564.532564 lmp.py:1912] 
DEBUG 01-15 16:10:21.532564.532564 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.532751.532751 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.532070.532070 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.532097.532097 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.532184.532184 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.532112.532112 lmp.py:1917] 
DEBUG 01-15 16:10:21.532112.532112 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.532993.532993 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.532835.532835 lmp.py:1935]   Expert 58 |     35 | CPU
DEBUG 01-15 16:10:21.532716.532716 lmp.py:1935]   Expert 47 |     58 | CPU
DEBUG 01-15 16:10:21.532882.532882 lmp.py:1935]   Expert 31 |     59 | CPU
DEBUG 01-15 16:10:21.532386.532386 lmp.py:1935]   Expert 49 |     60 | CPU
DEBUG 01-15 16:10:21.532983.532983 lmp.py:1935]   Expert  4 |     64 | CPU
DEBUG 01-15 16:10:21.532056.532056 lmp.py:1935]   Expert 38 |     70 | CPU
DEBUG 01-15 16:10:21.532845.532845 lmp.py:1935]   Expert 45 |     73 | CPU
DEBUG 01-15 16:10:21.532727.532727 lmp.py:1935]   Expert 43 |     82 | CPU
DEBUG 01-15 16:10:21.532370.532370 lmp.py:1935]   Expert 41 |     84 | CPU
DEBUG 01-15 16:10:21.532251.532251 lmp.py:1935]   Expert 33 |     95 | CPU
DEBUG 01-15 16:10:21.532848.532848 lmp.py:1935]   Expert 57 |    102 | CPU
DEBUG 01-15 16:10:21.532491.532491 lmp.py:1935]   Expert 50 |    104 | CPU
DEBUG 01-15 16:10:21.532087.532087 lmp.py:1935]   Expert 11 |    109 | CPU
DEBUG 01-15 16:10:21.532207.532207 lmp.py:1935]   Expert  2 |    115 | CPU
DEBUG 01-15 16:10:21.532565.532565 lmp.py:1935]   Expert 51 |    116 | CPU
DEBUG 01-15 16:10:21.532162.532162 lmp.py:1935]   Expert  0 |    122 | CPU
DEBUG 01-15 16:10:21.532235.532235 lmp.py:1935]   Expert 14 |    125 | CPU
DEBUG 01-15 16:10:21.532547.532547 lmp.py:1935]   Expert 54 |    127 | CPU
DEBUG 01-15 16:10:21.532144.532144 lmp.py:1935]   Expert 56 |    139 | CPU
DEBUG 01-15 16:10:21.532025.532025 lmp.py:1935]   Expert 26 |    140 | CPU
DEBUG 01-15 16:10:21.532145.532145 lmp.py:1935]   Expert 34 |    142 | CPU
DEBUG 01-15 16:10:21.532503.532503 lmp.py:1935]   Expert 27 |    154 | CPU
DEBUG 01-15 16:10:21.532623.532623 lmp.py:1935]   Expert 28 |    155 | CPU
DEBUG 01-15 16:10:21.532505.532505 lmp.py:1935]   Expert 55 |    158 | CPU
DEBUG 01-15 16:10:21.532386.532386 lmp.py:1935]   Expert 25 |    162 | CPU
DEBUG 01-15 16:10:21.532029.532029 lmp.py:1935]   Expert 10 |    163 | CPU
DEBUG 01-15 16:10:21.532149.532149 lmp.py:1935]   Expert  9 |    177 | CPU
DEBUG 01-15 16:10:21.532030.532030 lmp.py:1935]   Expert 13 |    179 | CPU
DEBUG 01-15 16:10:21.532912.532912 lmp.py:1935]   Expert 61 |    185 | CPU
DEBUG 01-15 16:10:21.532555.532555 lmp.py:1935]   Expert 48 |    190 | CPU
DEBUG 01-15 16:10:21.533198.533198 lmp.py:1935]   Expert  6 |    191 | CPU
DEBUG 01-15 16:10:21.533602.533602 lmp.py:1935]   Expert  7 |    196 | CPU
DEBUG 01-15 16:10:21.533391.533391 lmp.py:1935]   Expert 24 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533464.533464 lmp.py:1935]   Expert 46 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533015.533015 lmp.py:1935]   Expert 42 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533803.533803 lmp.py:1935]   Expert 18 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533831.533831 lmp.py:1935]   Expert 40 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533096.533096 lmp.py:1935]   Expert 12 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533646.533646 lmp.py:1935]   Expert 63 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533482.533482 lmp.py:1935]   Expert 29 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533555.533555 lmp.py:1935]   Expert 21 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533152.533152 lmp.py:1935]   Expert 59 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533225.533225 lmp.py:1935]   Expert 22 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533822.533822 lmp.py:1935]   Expert 32 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533657.533657 lmp.py:1935]   Expert 19 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533777.533777 lmp.py:1935]   Expert 36 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533373.533373 lmp.py:1935]   Expert 37 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533447.533447 lmp.py:1935]   Expert  3 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533712.533712 lmp.py:1935]   Expert  1 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533978.533978 lmp.py:1935]   Expert 16 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533720.533720 lmp.py:1935]   Expert 20 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533271.533271 lmp.py:1935]   Expert  5 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533106.533106 lmp.py:1935]   Expert  8 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533418.533418 lmp.py:1935]   Expert 30 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533253.533253 lmp.py:1935]   Expert 62 |    272 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533565.533565 lmp.py:1935]   Expert 15 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533400.533400 lmp.py:1935]   Expert 39 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533996.533996 lmp.py:1935]   Expert 35 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533646.533646 lmp.py:1935]   Expert 17 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533196.533196 lmp.py:1935]   Expert 60 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533224.533224 lmp.py:1935]   Expert 52 |    352 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533728.533728 lmp.py:1935]   Expert 23 |    373 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533516.533516 lmp.py:1935]   Expert 44 |    377 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.533305.533305 lmp.py:1935]   Expert 53 |    436 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.533663.533663 lmp.py:1937] 
DEBUG 01-15 16:10:21.533663.533663 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.533260.533260 lmp.py:1938]   CPU:   3931 tokens
DEBUG 01-15 16:10:21.533764.533764 lmp.py:1942]   cuda:1:   4179 tokens (16 experts)
DEBUG 01-15 16:10:21.533897.533897 lmp.py:1942]   cuda:2:   4178 tokens (16 experts)
DEBUG 01-15 16:10:21.533971.533971 lmp.py:1943]   Total GPU:   8357 tokens
DEBUG 01-15 16:10:21.533852.533852 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.533852.533852 lmp.py:1944] 
DEBUG 01-15 16:10:21.533886.533886 cuda_h.py:19] end experts_map_get cost 0.001796722412109375 seconds
DEBUG 01-15 16:10:21.533306.533306 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.533538.533538 lmp.py:1953] 
DEBUG 01-15 16:10:21.533538.533538 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.533090.533090 cuda_h.py:19] end cpu_experts_submit cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:10:21.533309.533309 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.534000.534000 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.535458.535458 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.1.gate_proj.weight': 0, 'model.layers.16.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.52.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.52.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.52.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.20.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.20.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.20.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.21.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.21.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.21.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.16.mlp.experts.32.gate_proj.weight': 0, 'model.layers.16.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.36.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.36.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.36.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.40.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.40.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.40.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.59.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.59.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.59.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(19487260672, 5767168, 0, 0), (19493027840, 5767168, 5767168, 0), (19481493504, 5767168, 11534336, 0), (20110114816, 5767168, 17301504, 0), (20115881984, 5767168, 23068672, 0), (20104347648, 5767168, 28835840, 0), (20144717824, 5767168, 34603008, 0), (20150484992, 5767168, 40370176, 0), (20138950656, 5767168, 46137344, 0), (19608371200, 5767168, 51904512, 0), (19614138368, 5767168, 57671680, 0), (19602604032, 5767168, 63438848, 0), (20196622336, 5767168, 69206016, 0), (20202389504, 5767168, 74973184, 0), (20190855168, 5767168, 80740352, 0), (19677577216, 5767168, 86507520, 0), (19683344384, 5767168, 92274688, 0), (19671810048, 5767168, 98041856, 0), (20265828352, 5767168, 103809024, 0), (20271595520, 5767168, 109576192, 0), (20260061184, 5767168, 115343360, 0), (19764084736, 5767168, 121110528, 0), (19769851904, 5767168, 126877696, 0), (19758317568, 5767168, 132644864, 0), (19798687744, 5767168, 138412032, 0), (19804454912, 5767168, 144179200, 0), (19792920576, 5767168, 149946368, 0), (20369637376, 5767168, 155713536, 0), (20375404544, 5767168, 161480704, 0), (20363870208, 5767168, 167247872, 0), (20386938880, 5767168, 173015040, 0), (20392706048, 5767168, 178782208, 0), (20381171712, 5767168, 184549376, 0), (19815989248, 5767168, 190316544, 0), (19821756416, 5767168, 196083712, 0), (19810222080, 5767168, 201850880, 0), (19850592256, 5767168, 207618048, 0), (19856359424, 5767168, 213385216, 0), (19844825088, 5767168, 219152384, 0), (19833290752, 5767168, 224919552, 0), (19839057920, 5767168, 230686720, 0), (19827523584, 5767168, 236453888, 0), (19971702784, 5767168, 242221056, 0), (19977469952, 5767168, 247988224, 0), (19965935616, 5767168, 253755392, 0), (20542652416, 5767168, 259522560, 0), (20548419584, 5767168, 265289728, 0), (20536885248, 5767168, 271056896, 0)], 2: [(20023607296, 5767168, 0, 0), (20029374464, 5767168, 5767168, 0), (20017840128, 5767168, 11534336, 0), (20075511808, 5767168, 17301504, 0), (20081278976, 5767168, 23068672, 0), (20069744640, 5767168, 28835840, 0), (19521863680, 5767168, 34603008, 0), (19527630848, 5767168, 40370176, 0), (19516096512, 5767168, 46137344, 0), (19556466688, 5767168, 51904512, 0), (19562233856, 5767168, 57671680, 0), (19550699520, 5767168, 63438848, 0), (20092813312, 5767168, 69206016, 0), (20098580480, 5767168, 74973184, 0), (20087046144, 5767168, 80740352, 0), (20162019328, 5767168, 86507520, 0), (20167786496, 5767168, 92274688, 0), (20156252160, 5767168, 98041856, 0), (20231225344, 5767168, 103809024, 0), (20236992512, 5767168, 109576192, 0), (20225458176, 5767168, 115343360, 0), (19729481728, 5767168, 121110528, 0), (19735248896, 5767168, 126877696, 0), (19723714560, 5767168, 132644864, 0), (19746783232, 5767168, 138412032, 0), (19752550400, 5767168, 144179200, 0), (19741016064, 5767168, 149946368, 0), (19781386240, 5767168, 155713536, 0), (19787153408, 5767168, 161480704, 0), (19775619072, 5767168, 167247872, 0), (19867893760, 5767168, 173015040, 0), (19873660928, 5767168, 178782208, 0), (19862126592, 5767168, 184549376, 0), (19885195264, 5767168, 190316544, 0), (19890962432, 5767168, 196083712, 0), (19879428096, 5767168, 201850880, 0), (20490747904, 5767168, 207618048, 0), (20496515072, 5767168, 213385216, 0), (20484980736, 5767168, 219152384, 0), (20508049408, 5767168, 224919552, 0), (20513816576, 5767168, 230686720, 0), (20502282240, 5767168, 236453888, 0), (19989004288, 5767168, 242221056, 0), (19994771456, 5767168, 247988224, 0), (19983237120, 5767168, 253755392, 0), (20559953920, 5767168, 259522560, 0), (20565721088, 5767168, 265289728, 0), (20554186752, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e34219170>, 2: <capsule object NULL at 0x7a4e34219a40>}
DEBUG 01-15 16:10:21.535230.535230 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.536338.536338 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.536810.536810 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5cb839d7-2d08-49c8-a908-76ecdeabe764
DEBUG 01-15 16:10:21.536412.536412 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:21.536816.536816 client.py:127] Model loaded
DEBUG 01-15 16:10:21.536112.536112 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.537054.537054 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:21.537109.537109 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5cb839d7-2d08-49c8-a908-76ecdeabe764
DEBUG 01-15 16:10:21.537337.537337 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.537429.537429 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036880970001220703 seconds
DEBUG 01-15 16:10:21.537584.537584 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.538707.538707 cuda_h.py:19] end move_flatidxs cost 0.0009102821350097656 seconds
DEBUG 01-15 16:10:21.538822.538822 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.538338.538338 cuda_h.py:19] end restore2model cost 0.0011026859283447266 seconds
DEBUG 01-15 16:10:21.539506.539506 cuda_h.py:19] end sllm_worker_task cost 0.013663053512573242 seconds
DEBUG 01-15 16:10:21.541764.541764 cuda_h.py:19] end restore2model cost 0.0037353038787841797 seconds
DEBUG 01-15 16:10:21.541342.541342 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007691860198974609 seconds
DEBUG 01-15 16:10:21.541158.541158 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.542142.542142 cuda_h.py:19] end gpu_sexperts cost 0.0002732276916503906 seconds
DEBUG 01-15 16:10:21.542779.542779 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.542748.542748 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.811981201171875e-05 seconds
DEBUG 01-15 16:10:21.542967.542967 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.542908.542908 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.543142.543142 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008089542388916016 seconds
DEBUG 01-15 16:10:21.543362.543362 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.543224.543224 cuda_h.py:19] end gpu_group_list cost 0.00018477439880371094 seconds
DEBUG 01-15 16:10:21.544143.544143 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.544201.544201 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008985996246337891 seconds
DEBUG 01-15 16:10:21.545475.545475 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.545794.545794 cuda_h.py:19] end gpu_group_list cost 0.0002028942108154297 seconds
DEBUG 01-15 16:10:21.545444.545444 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.545704.545704 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5cb839d7-2d08-49c8-a908-76ecdeabe764
DEBUG 01-15 16:10:21.547958.547958 cuda_h.py:19] end group_tensors cost 0.008596181869506836 seconds
DEBUG 01-15 16:10:21.547751.547751 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.551218.551218 cuda_h.py:19] end group pad cost 0.0038046836853027344 seconds
DEBUG 01-15 16:10:21.551961.551961 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.564129.564129 client.py:127] Model loaded
DEBUG 01-15 16:10:21.564994.564994 cuda_h.py:19] end wait_experts_multi_device cost 0.01874256134033203 seconds
DEBUG 01-15 16:10:21.564187.564187 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.575361.575361 cuda_h.py:19] end group_einsum cost 0.02311229705810547 seconds
DEBUG 01-15 16:10:21.575474.575474 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.580721.580721 cuda_h.py:19] end get_outputs_cpu1 cost 0.00510406494140625 seconds
DEBUG 01-15 16:10:21.581440.581440 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04410076141357422 seconds
DEBUG 01-15 16:10:21.582078.582078 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.017237186431884766 seconds
DEBUG 01-15 16:10:21.582215.582215 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.584886.584886 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.584274.584274 cuda_h.py:19] end index_scatter cost 0.00020265579223632812 seconds
DEBUG 01-15 16:10:21.584288.584288 cuda_h.py:19] end cpuoutputsdeal cost 0.002711772918701172 seconds
DEBUG 01-15 16:10:21.585988.585988 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.585798.585798 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.585444.585444 cuda_h.py:19] end gpu_group_tensor cost 0.00023412704467773438 seconds
DEBUG 01-15 16:10:21.585877.585877 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.585781.585781 cuda_h.py:19] end gpu_group_tensor cost 0.0002186298370361328 seconds
DEBUG 01-15 16:10:21.586303.586303 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.586991.586991 cuda_h.py:19] end gpu_group_einsum cost 0.0008740425109863281 seconds
DEBUG 01-15 16:10:21.587302.587302 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.587970.587970 cuda_h.py:19] end gpu_group_einsum cost 0.0006814002990722656 seconds
DEBUG 01-15 16:10:21.588983.588983 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.588452.588452 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.588306.588306 cuda_h.py:19] end all_expert_outputs_slices cost 0.00030684471130371094 seconds
DEBUG 01-15 16:10:21.588633.588633 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.588413.588413 cuda_h.py:19] end concat_expert_out cost 8.869171142578125e-05 seconds
DEBUG 01-15 16:10:21.589139.589139 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.589979.589979 cuda_h.py:19] end index_scatter cost 9.465217590332031e-05 seconds
DEBUG 01-15 16:10:21.589251.589251 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001373291015625 seconds
DEBUG 01-15 16:10:21.589561.589561 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.589458.589458 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.590269.590269 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022220611572265625 seconds
DEBUG 01-15 16:10:21.590582.590582 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.590971.590971 cuda_h.py:19] end concat_expert_out cost 8.749961853027344e-05 seconds
DEBUG 01-15 16:10:21.590067.590067 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.590185.590185 cuda_h.py:19] end index_scatter cost 8.916854858398438e-05 seconds
DEBUG 01-15 16:10:21.590373.590373 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008454322814941406 seconds
DEBUG 01-15 16:10:21.590052.590052 cuda_h.py:19] end gpu_experts_multi_device cost 0.04859352111816406 seconds
DEBUG 01-15 16:10:21.590004.590004 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.05967569351196289 seconds
DEBUG 01-15 16:10:21.591426.591426 cuda_h.py:19] end prefill_layer cost 0.0664980411529541 seconds
DEBUG 01-15 16:10:21.591595.591595 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:10:21.591762.591762 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.591883.591883 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:10:21.591335.591335 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:21.591602.591602 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:21.591420.591420 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:10:21.591071.591071 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 0.0001201629638671875 seconds
DEBUG 01-15 16:10:21.591755.591755 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.592409.592409 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.592029.592029 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.592228.592228 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.592187.592187 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.593271.593271 cuda_h.py:19] end allocate_cuda_memory cost 0.00042128562927246094 seconds
DEBUG 01-15 16:10:21.593561.593561 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.593975.593975 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.593078.593078 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.593233.593233 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c929327c-b56d-4ea2-a166-3e5ae4046859
DEBUG 01-15 16:10:21.593843.593843 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.594832.594832 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.595402.595402 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c929327c-b56d-4ea2-a166-3e5ae4046859
DEBUG 01-15 16:10:21.596415.596415 cuda_h.py:19] end load_into_gpu_async cost 0.002635955810546875 seconds
DEBUG 01-15 16:10:21.596093.596093 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.596498.596498 cuda_h.py:19] end restore_tensors2 cost 0.00014972686767578125 seconds
DEBUG 01-15 16:10:21.596554.596554 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0038976669311523438 seconds
INFO 01-15 16:10:21.596819.596819 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c929327c-b56d-4ea2-a166-3e5ae4046859
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.600931.600931 cuda_h.py:19] end self_attn cost 0.00596308708190918 seconds
DEBUG 01-15 16:10:21.600810.600810 cuda_h.py:19] end iln_self_attn_paln cost 0.008896350860595703 seconds
DEBUG 01-15 16:10:21.601097.601097 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:10:21.601404.601404 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.601540.601540 cuda_h.py:19] end gate cost 0.0008602142333984375 seconds
DEBUG 01-15 16:10:21.602251.602251 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.602193.602193 lmp.py:1912] 
DEBUG 01-15 16:10:21.602193.602193 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.602109.602109 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.602388.602388 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.602806.602806 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.602078.602078 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.602589.602589 lmp.py:1917] 
DEBUG 01-15 16:10:21.602589.602589 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.602193.602193 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.602710.602710 lmp.py:1935]   Expert  4 |     10 | CPU
DEBUG 01-15 16:10:21.602267.602267 lmp.py:1935]   Expert 28 |     28 | CPU
DEBUG 01-15 16:10:21.602871.602871 lmp.py:1935]   Expert  7 |     46 | CPU
DEBUG 01-15 16:10:21.602759.602759 lmp.py:1935]   Expert 53 |     57 | CPU
DEBUG 01-15 16:10:21.603892.603892 lmp.py:1935]   Expert 52 |     68 | CPU
DEBUG 01-15 16:10:21.603026.603026 lmp.py:1935]   Expert 43 |     71 | CPU
DEBUG 01-15 16:10:21.603682.603682 lmp.py:1935]   Expert 49 |     83 | CPU
DEBUG 01-15 16:10:21.603100.603100 lmp.py:1935]   Expert 12 |     87 | CPU
DEBUG 01-15 16:10:21.603042.603042 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:10:21.603221.603221 lmp.py:1935]   Expert 33 |    106 | CPU
DEBUG 01-15 16:10:21.603640.603640 lmp.py:1935]   Expert 24 |    107 | CPU
DEBUG 01-15 16:10:21.603104.603104 lmp.py:1935]   Expert 50 |    110 | CPU
DEBUG 01-15 16:10:21.603092.603092 lmp.py:1935]   Expert  2 |    111 | CPU
DEBUG 01-15 16:10:21.603318.603318 lmp.py:1935]   Expert 60 |    111 | CPU
DEBUG 01-15 16:10:21.603305.603305 lmp.py:1935]   Expert 15 |    113 | CPU
DEBUG 01-15 16:10:21.603485.603485 lmp.py:1935]   Expert 39 |    116 | CPU
DEBUG 01-15 16:10:21.603287.603287 lmp.py:1935]   Expert 36 |    118 | CPU
DEBUG 01-15 16:10:21.603706.603706 lmp.py:1935]   Expert 25 |    122 | CPU
DEBUG 01-15 16:10:21.603693.603693 lmp.py:1935]   Expert  6 |    123 | CPU
DEBUG 01-15 16:10:21.603681.603681 lmp.py:1935]   Expert 61 |    132 | CPU
DEBUG 01-15 16:10:21.603668.603668 lmp.py:1935]   Expert 59 |    135 | CPU
DEBUG 01-15 16:10:21.603656.603656 lmp.py:1935]   Expert  3 |    142 | CPU
DEBUG 01-15 16:10:21.603405.603405 lmp.py:1935]   Expert 58 |    144 | CPU
DEBUG 01-15 16:10:21.603393.603393 lmp.py:1935]   Expert 27 |    145 | CPU
DEBUG 01-15 16:10:21.603857.603857 lmp.py:1935]   Expert  8 |    149 | CPU
DEBUG 01-15 16:10:21.603799.603799 lmp.py:1935]   Expert 30 |    151 | CPU
DEBUG 01-15 16:10:21.603548.603548 lmp.py:1935]   Expert 31 |    152 | CPU
DEBUG 01-15 16:10:21.603251.603251 lmp.py:1935]   Expert 10 |    155 | CPU
DEBUG 01-15 16:10:21.603907.603907 lmp.py:1935]   Expert 38 |    157 | CPU
DEBUG 01-15 16:10:21.603372.603372 lmp.py:1935]   Expert 40 |    160 | CPU
DEBUG 01-15 16:10:21.603075.603075 lmp.py:1935]   Expert 57 |    160 | CPU
DEBUG 01-15 16:10:21.603254.603254 lmp.py:1935]   Expert 14 |    162 | CPU
DEBUG 01-15 16:10:21.603341.603341 lmp.py:1935]   Expert 54 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.603667.603667 lmp.py:1935]   Expert 41 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.603992.603992 lmp.py:1935]   Expert 37 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.603126.603126 lmp.py:1935]   Expert 46 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.603259.603259 lmp.py:1935]   Expert 32 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.603677.603677 lmp.py:1935]   Expert 42 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.603288.603288 lmp.py:1935]   Expert 19 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.603421.603421 lmp.py:1935]   Expert 11 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.603316.603316 lmp.py:1935]   Expert 34 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604972.604972 lmp.py:1935]   Expert 22 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604152.604152 lmp.py:1935]   Expert 18 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604809.604809 lmp.py:1935]   Expert 26 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604181.604181 lmp.py:1935]   Expert  0 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604552.604552 lmp.py:1935]   Expert 56 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604686.604686 lmp.py:1935]   Expert  1 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604104.604104 lmp.py:1935]   Expert 44 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604284.604284 lmp.py:1935]   Expert 51 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604986.604986 lmp.py:1935]   Expert 20 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604928.604928 lmp.py:1935]   Expert 29 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604346.604346 lmp.py:1935]   Expert 48 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604241.604241 lmp.py:1935]   Expert 45 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604374.604374 lmp.py:1935]   Expert 21 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604031.604031 lmp.py:1935]   Expert 35 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604039.604039 lmp.py:1935]   Expert 55 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604219.604219 lmp.py:1935]   Expert 16 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604875.604875 lmp.py:1935]   Expert  5 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604532.604532 lmp.py:1935]   Expert 23 |    373 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604188.604188 lmp.py:1935]   Expert 13 |    380 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604798.604798 lmp.py:1935]   Expert 17 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604647.604647 lmp.py:1935]   Expert  9 |    456 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604542.604542 lmp.py:1935]   Expert 63 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.604437.604437 lmp.py:1935]   Expert 62 |   1181 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.604186.604186 lmp.py:1937] 
DEBUG 01-15 16:10:21.604186.604186 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.604843.604843 lmp.py:1938]   CPU:   3635 tokens
DEBUG 01-15 16:10:21.604261.604261 lmp.py:1942]   cuda:1:   4282 tokens (15 experts)
DEBUG 01-15 16:10:21.604679.604679 lmp.py:1942]   cuda:2:   4371 tokens (17 experts)
DEBUG 01-15 16:10:21.604859.604859 lmp.py:1943]   Total GPU:   8653 tokens
DEBUG 01-15 16:10:21.604277.604277 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.604277.604277 lmp.py:1944] 
DEBUG 01-15 16:10:21.604324.604324 cuda_h.py:19] end experts_map_get cost 0.002753019332885742 seconds
DEBUG 01-15 16:10:21.604831.604831 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.604746.604746 lmp.py:1953] 
DEBUG 01-15 16:10:21.604746.604746 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.605848.605848 cuda_h.py:19] end cpu_experts_submit cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:10:21.605319.605319 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.605991.605991 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.605720.605720 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.1.gate_proj.weight': 0, 'model.layers.17.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.41.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.41.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.41.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.42.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.42.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.42.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.56.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.56.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.56.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.62.up_proj.weight': 253755392}, 2: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.32.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.32.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.32.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.35.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.35.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.35.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.26.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.26.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.26.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.29.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.63.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.63.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.63.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(20594556928, 5767168, 0, 0), (20600324096, 5767168, 5767168, 0), (20588789760, 5767168, 11534336, 0), (20663762944, 5767168, 17301504, 0), (20669530112, 5767168, 23068672, 0), (20657995776, 5767168, 28835840, 0), (21286617088, 5767168, 34603008, 0), (21292384256, 5767168, 40370176, 0), (21280849920, 5767168, 46137344, 0), (21303918592, 5767168, 51904512, 0), (21309685760, 5767168, 57671680, 0), (21298151424, 5767168, 63438848, 0), (20767571968, 5767168, 69206016, 0), (20773339136, 5767168, 74973184, 0), (20761804800, 5767168, 80740352, 0), (20802174976, 5767168, 86507520, 0), (20807942144, 5767168, 92274688, 0), (20796407808, 5767168, 98041856, 0), (21373124608, 5767168, 103809024, 0), (21378891776, 5767168, 109576192, 0), (21367357440, 5767168, 115343360, 0), (21407727616, 5767168, 121110528, 0), (21413494784, 5767168, 126877696, 0), (21401960448, 5767168, 132644864, 0), (20888682496, 5767168, 138412032, 0), (20894449664, 5767168, 144179200, 0), (20882915328, 5767168, 149946368, 0), (20923285504, 5767168, 155713536, 0), (20929052672, 5767168, 161480704, 0), (20917518336, 5767168, 167247872, 0), (20940587008, 5767168, 173015040, 0), (20946354176, 5767168, 178782208, 0), (20934819840, 5767168, 184549376, 0), (20957888512, 5767168, 190316544, 0), (20963655680, 5767168, 196083712, 0), (20952121344, 5767168, 201850880, 0), (21528838144, 5767168, 207618048, 0), (21534605312, 5767168, 213385216, 0), (21523070976, 5767168, 219152384, 0), (21546139648, 5767168, 224919552, 0), (21551906816, 5767168, 230686720, 0), (21540372480, 5767168, 236453888, 0), (21649948672, 5767168, 242221056, 0), (21655715840, 5767168, 247988224, 0), (21644181504, 5767168, 253755392, 0)], 2: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (21130903552, 5767168, 17301504, 0), (21136670720, 5767168, 23068672, 0), (21125136384, 5767168, 28835840, 0), (21165506560, 5767168, 34603008, 0), (21171273728, 5767168, 40370176, 0), (21159739392, 5767168, 46137344, 0), (21182808064, 5767168, 51904512, 0), (21188575232, 5767168, 57671680, 0), (21177040896, 5767168, 63438848, 0), (21217411072, 5767168, 69206016, 0), (21223178240, 5767168, 74973184, 0), (21211643904, 5767168, 80740352, 0), (20732968960, 5767168, 86507520, 0), (20738736128, 5767168, 92274688, 0), (20727201792, 5767168, 98041856, 0), (21338521600, 5767168, 103809024, 0), (21344288768, 5767168, 109576192, 0), (21332754432, 5767168, 115343360, 0), (21355823104, 5767168, 121110528, 0), (21361590272, 5767168, 126877696, 0), (21350055936, 5767168, 132644864, 0), (20854079488, 5767168, 138412032, 0), (20859846656, 5767168, 144179200, 0), (20848312320, 5767168, 149946368, 0), (20871380992, 5767168, 155713536, 0), (20877148160, 5767168, 161480704, 0), (20865613824, 5767168, 167247872, 0), (21459632128, 5767168, 173015040, 0), (21465399296, 5767168, 178782208, 0), (21453864960, 5767168, 184549376, 0), (20905984000, 5767168, 190316544, 0), (20911751168, 5767168, 196083712, 0), (20900216832, 5767168, 201850880, 0), (21511536640, 5767168, 207618048, 0), (21517303808, 5767168, 213385216, 0), (21505769472, 5767168, 219152384, 0), (20975190016, 5767168, 224919552, 0), (20980957184, 5767168, 230686720, 0), (20969422848, 5767168, 236453888, 0), (21027094528, 5767168, 242221056, 0), (21032861696, 5767168, 247988224, 0), (21021327360, 5767168, 253755392, 0), (21078999040, 5767168, 259522560, 0), (21084766208, 5767168, 265289728, 0), (21073231872, 5767168, 271056896, 0), (21667250176, 5767168, 276824064, 0), (21673017344, 5767168, 282591232, 0), (21661483008, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e547306c0>, 2: <capsule object NULL at 0x7a4e34218cf0>}
DEBUG 01-15 16:10:21.606575.606575 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.606167.606167 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.606752.606752 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, db45a00c-8e10-42d6-8de1-aeb9a7ca7489
DEBUG 01-15 16:10:21.606688.606688 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.606588.606588 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.607275.607275 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:21.607049.607049 client.py:127] Model loaded
DEBUG 01-15 16:10:21.607723.607723 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.608936.608936 cuda_h.py:19] end move_flatidxs cost 0.0008614063262939453 seconds
DEBUG 01-15 16:10:21.608448.608448 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.608024.608024 cuda_h.py:19] end restore2model cost 0.0010018348693847656 seconds
DEBUG 01-15 16:10:21.608976.608976 cuda_h.py:19] end sllm_worker_task cost 0.016011953353881836 seconds
INFO 01-15 16:10:21.609994.609994 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, db45a00c-8e10-42d6-8de1-aeb9a7ca7489
DEBUG 01-15 16:10:21.609549.609549 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004366874694824219 seconds
DEBUG 01-15 16:10:21.609320.609320 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.612973.612973 cuda_h.py:19] end restore2model cost 0.003041982650756836 seconds
DEBUG 01-15 16:10:21.612100.612100 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007673978805541992 seconds
DEBUG 01-15 16:10:21.612088.612088 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.613246.613246 cuda_h.py:19] end gpu_sexperts cost 0.0003275871276855469 seconds
DEBUG 01-15 16:10:21.613413.613413 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.613488.613488 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.3603439331054688e-05 seconds
DEBUG 01-15 16:10:21.613853.613853 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.613132.613132 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.614465.614465 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010192394256591797 seconds
DEBUG 01-15 16:10:21.614408.614408 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.614295.614295 cuda_h.py:19] end gpu_group_list cost 0.000164031982421875 seconds
DEBUG 01-15 16:10:21.615042.615042 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.616886.616886 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012204647064208984 seconds
DEBUG 01-15 16:10:21.616975.616975 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.617697.617697 cuda_h.py:19] end gpu_group_list cost 0.0001838207244873047 seconds
DEBUG 01-15 16:10:21.617672.617672 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.618508.618508 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, db45a00c-8e10-42d6-8de1-aeb9a7ca7489
DEBUG 01-15 16:10:21.618984.618984 cuda_h.py:19] end group_tensors cost 0.010041475296020508 seconds
DEBUG 01-15 16:10:21.618725.618725 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.622737.622737 cuda_h.py:19] end group pad cost 0.0036787986755371094 seconds
DEBUG 01-15 16:10:21.622196.622196 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.640148.640148 client.py:127] Model loaded
DEBUG 01-15 16:10:21.640094.640094 cuda_h.py:19] end wait_experts_multi_device cost 0.02278757095336914 seconds
DEBUG 01-15 16:10:21.641145.641145 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.643486.643486 cuda_h.py:19] end group_einsum cost 0.020635604858398438 seconds
DEBUG 01-15 16:10:21.643219.643219 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.647158.647158 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036356449127197266 seconds
DEBUG 01-15 16:10:21.648115.648115 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.041359663009643555 seconds
DEBUG 01-15 16:10:21.648726.648726 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007761955261230469 seconds
DEBUG 01-15 16:10:21.649731.649731 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.650880.650880 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.651457.651457 cuda_h.py:19] end index_scatter cost 0.00010156631469726562 seconds
DEBUG 01-15 16:10:21.651099.651099 cuda_h.py:19] end cpuoutputsdeal cost 0.002404451370239258 seconds
DEBUG 01-15 16:10:21.651693.651693 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.651112.651112 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.651108.651108 cuda_h.py:19] end gpu_group_tensor cost 0.0002181529998779297 seconds
DEBUG 01-15 16:10:21.652673.652673 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.652477.652477 cuda_h.py:19] end gpu_group_tensor cost 0.00020623207092285156 seconds
DEBUG 01-15 16:10:21.652027.652027 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.653162.653162 cuda_h.py:19] end gpu_group_einsum cost 0.0013799667358398438 seconds
DEBUG 01-15 16:10:21.654380.654380 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.654611.654611 cuda_h.py:19] end gpu_group_einsum cost 0.000675201416015625 seconds
DEBUG 01-15 16:10:21.655763.655763 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.655279.655279 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.655006.655006 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002875328063964844 seconds
DEBUG 01-15 16:10:21.655280.655280 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.655099.655099 cuda_h.py:19] end concat_expert_out cost 8.440017700195312e-05 seconds
DEBUG 01-15 16:10:21.655772.655772 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.656982.656982 cuda_h.py:19] end index_scatter cost 9.036064147949219e-05 seconds
DEBUG 01-15 16:10:21.656889.656889 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0013070106506347656 seconds
DEBUG 01-15 16:10:21.656292.656292 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.656951.656951 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.657774.657774 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019812583923339844 seconds
DEBUG 01-15 16:10:21.657657.657657 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.657655.657655 cuda_h.py:19] end concat_expert_out cost 8.153915405273438e-05 seconds
DEBUG 01-15 16:10:21.657128.657128 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.657524.657524 cuda_h.py:19] end index_scatter cost 8.606910705566406e-05 seconds
DEBUG 01-15 16:10:21.657712.657712 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008099079132080078 seconds
DEBUG 01-15 16:10:21.657184.657184 cuda_h.py:19] end gpu_experts_multi_device cost 0.04428553581237793 seconds
DEBUG 01-15 16:10:21.657902.657902 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.056627750396728516 seconds
DEBUG 01-15 16:10:21.658491.658491 cuda_h.py:19] end prefill_layer cost 0.06642651557922363 seconds
DEBUG 01-15 16:10:21.658129.658129 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:10:21.658163.658163 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.658150.658150 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:10:21.658138.658138 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:21.658132.658132 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:21.658651.658651 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 3.9577484130859375e-05 seconds
DEBUG 01-15 16:10:21.658673.658673 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 9.298324584960938e-05 seconds
DEBUG 01-15 16:10:21.658753.658753 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.658815.658815 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.658159.658159 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.658016.658016 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.658368.658368 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.659205.659205 cuda_h.py:19] end allocate_cuda_memory cost 0.0003597736358642578 seconds
DEBUG 01-15 16:10:21.659516.659516 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.659003.659003 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.659545.659545 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.659051.659051 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 67594374-4bf1-403d-bfb7-39044ce27f39
DEBUG 01-15 16:10:21.660244.660244 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.660081.660081 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.662245.662245 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 67594374-4bf1-403d-bfb7-39044ce27f39
DEBUG 01-15 16:10:21.662893.662893 cuda_h.py:19] end load_into_gpu_async cost 0.0026175975799560547 seconds
DEBUG 01-15 16:10:21.662095.662095 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.662958.662958 cuda_h.py:19] end restore_tensors2 cost 0.00014901161193847656 seconds
DEBUG 01-15 16:10:21.662207.662207 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003930568695068359 seconds
INFO 01-15 16:10:21.663418.663418 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 67594374-4bf1-403d-bfb7-39044ce27f39
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.665623.665623 cuda_h.py:19] end self_attn cost 0.0048220157623291016 seconds
DEBUG 01-15 16:10:21.665428.665428 cuda_h.py:19] end iln_self_attn_paln cost 0.007448434829711914 seconds
DEBUG 01-15 16:10:21.665694.665694 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:10:21.665934.665934 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.666242.666242 cuda_h.py:19] end gate cost 0.0008563995361328125 seconds
DEBUG 01-15 16:10:21.666032.666032 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.667937.667937 lmp.py:1912] 
DEBUG 01-15 16:10:21.667937.667937 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.667316.667316 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.667826.667826 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.667284.667284 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.667941.667941 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.667537.667537 lmp.py:1917] 
DEBUG 01-15 16:10:21.667537.667537 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.667372.667372 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.667691.667691 lmp.py:1935]   Expert 32 |     33 | CPU
DEBUG 01-15 16:10:21.667049.667049 lmp.py:1935]   Expert  5 |     51 | CPU
DEBUG 01-15 16:10:21.667931.667931 lmp.py:1935]   Expert 30 |     51 | CPU
DEBUG 01-15 16:10:21.667574.667574 lmp.py:1935]   Expert 46 |     73 | CPU
DEBUG 01-15 16:10:21.667031.667031 lmp.py:1935]   Expert 40 |     89 | CPU
DEBUG 01-15 16:10:21.667012.667012 lmp.py:1935]   Expert  8 |     91 | CPU
DEBUG 01-15 16:10:21.667470.667470 lmp.py:1935]   Expert 12 |    100 | CPU
DEBUG 01-15 16:10:21.667166.667166 lmp.py:1935]   Expert 17 |    105 | CPU
DEBUG 01-15 16:10:21.667862.667862 lmp.py:1935]   Expert 27 |    110 | CPU
DEBUG 01-15 16:10:21.667558.667558 lmp.py:1935]   Expert 60 |    110 | CPU
DEBUG 01-15 16:10:21.667062.667062 lmp.py:1935]   Expert 58 |    115 | CPU
DEBUG 01-15 16:10:21.667805.667805 lmp.py:1935]   Expert  3 |    117 | CPU
DEBUG 01-15 16:10:21.667070.667070 lmp.py:1935]   Expert 21 |    119 | CPU
DEBUG 01-15 16:10:21.667574.667574 lmp.py:1935]   Expert 28 |    119 | CPU
DEBUG 01-15 16:10:21.667840.667840 lmp.py:1935]   Expert 29 |    123 | CPU
DEBUG 01-15 16:10:21.667344.667344 lmp.py:1935]   Expert 25 |    124 | CPU
DEBUG 01-15 16:10:21.667848.667848 lmp.py:1935]   Expert 41 |    129 | CPU
DEBUG 01-15 16:10:21.667875.667875 lmp.py:1935]   Expert 35 |    133 | CPU
DEBUG 01-15 16:10:21.667809.667809 lmp.py:1935]   Expert 19 |    135 | CPU
DEBUG 01-15 16:10:21.667505.667505 lmp.py:1935]   Expert  0 |    143 | CPU
DEBUG 01-15 16:10:21.668725.668725 lmp.py:1935]   Expert 52 |    145 | CPU
DEBUG 01-15 16:10:21.668706.668706 lmp.py:1935]   Expert  6 |    146 | CPU
DEBUG 01-15 16:10:21.668686.668686 lmp.py:1935]   Expert 56 |    149 | CPU
DEBUG 01-15 16:10:21.668714.668714 lmp.py:1935]   Expert 37 |    150 | CPU
DEBUG 01-15 16:10:21.668979.668979 lmp.py:1935]   Expert 54 |    152 | CPU
DEBUG 01-15 16:10:21.668529.668529 lmp.py:1935]   Expert 48 |    154 | CPU
DEBUG 01-15 16:10:21.668033.668033 lmp.py:1935]   Expert 53 |    155 | CPU
DEBUG 01-15 16:10:21.668299.668299 lmp.py:1935]   Expert 63 |    157 | CPU
DEBUG 01-15 16:10:21.668565.668565 lmp.py:1935]   Expert 36 |    161 | CPU
DEBUG 01-15 16:10:21.668261.668261 lmp.py:1935]   Expert 59 |    171 | CPU
DEBUG 01-15 16:10:21.668480.668480 lmp.py:1935]   Expert  9 |    180 | CPU
DEBUG 01-15 16:10:21.668699.668699 lmp.py:1935]   Expert  1 |    185 | CPU
DEBUG 01-15 16:10:21.668111.668111 lmp.py:1935]   Expert 39 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668807.668807 lmp.py:1935]   Expert 20 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668264.668264 lmp.py:1935]   Expert 61 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668245.668245 lmp.py:1935]   Expert 42 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668226.668226 lmp.py:1935]   Expert  7 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668684.668684 lmp.py:1935]   Expert 43 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668937.668937 lmp.py:1935]   Expert 11 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668302.668302 lmp.py:1935]   Expert 34 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668190.668190 lmp.py:1935]   Expert 47 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668555.668555 lmp.py:1935]   Expert 55 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668682.668682 lmp.py:1935]   Expert 57 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668378.668378 lmp.py:1935]   Expert 13 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668835.668835 lmp.py:1935]   Expert 16 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668532.668532 lmp.py:1935]   Expert 18 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668228.668228 lmp.py:1935]   Expert 15 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668924.668924 lmp.py:1935]   Expert  4 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668381.668381 lmp.py:1935]   Expert 45 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668078.668078 lmp.py:1935]   Expert 50 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668012.668012 lmp.py:1935]   Expert 22 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668470.668470 lmp.py:1935]   Expert 33 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668119.668119 lmp.py:1935]   Expert 31 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668008.668008 lmp.py:1935]   Expert 51 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668704.668704 lmp.py:1935]   Expert 49 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668923.668923 lmp.py:1935]   Expert 38 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668381.668381 lmp.py:1935]   Expert 26 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668077.668077 lmp.py:1935]   Expert 10 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668296.668296 lmp.py:1935]   Expert 44 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668992.668992 lmp.py:1935]   Expert  2 |    297 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668211.668211 lmp.py:1935]   Expert 24 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668146.668146 lmp.py:1935]   Expert 14 |    313 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668034.668034 lmp.py:1935]   Expert 23 |    407 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.668161.668161 lmp.py:1935]   Expert 62 |    673 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.668618.668618 lmp.py:1937] 
DEBUG 01-15 16:10:21.668618.668618 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.669791.669791 lmp.py:1938]   CPU:   3975 tokens
DEBUG 01-15 16:10:21.669726.669726 lmp.py:1942]   cuda:1:   4253 tokens (16 experts)
DEBUG 01-15 16:10:21.669945.669945 lmp.py:1942]   cuda:2:   4060 tokens (16 experts)
DEBUG 01-15 16:10:21.669687.669687 lmp.py:1943]   Total GPU:   8313 tokens
DEBUG 01-15 16:10:21.669430.669430 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.669430.669430 lmp.py:1944] 
DEBUG 01-15 16:10:21.669563.669563 cuda_h.py:19] end experts_map_get cost 0.0021851062774658203 seconds
DEBUG 01-15 16:10:21.669394.669394 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.669773.669773 lmp.py:1953] 
DEBUG 01-15 16:10:21.669773.669773 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.669477.669477 cuda_h.py:19] end cpu_experts_submit cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:10:21.669411.669411 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.669162.669162 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.670181.670181 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.33.gate_proj.weight': 0, 'model.layers.18.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.42.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.42.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.42.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.44.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.44.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.44.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.18.mlp.experts.2.gate_proj.weight': 0, 'model.layers.18.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.38.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.38.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.38.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.22.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.22.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.22.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.23.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.23.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.23.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.61.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.61.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.61.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(22255501312, 5767168, 0, 0), (22261268480, 5767168, 5767168, 0), (22249734144, 5767168, 11534336, 0), (21805662208, 5767168, 17301504, 0), (21811429376, 5767168, 23068672, 0), (21799895040, 5767168, 28835840, 0), (22359310336, 5767168, 34603008, 0), (22365077504, 5767168, 40370176, 0), (22353543168, 5767168, 46137344, 0), (22411214848, 5767168, 51904512, 0), (22416982016, 5767168, 57671680, 0), (22405447680, 5767168, 63438848, 0), (22445817856, 5767168, 69206016, 0), (22451585024, 5767168, 74973184, 0), (22440050688, 5767168, 80740352, 0), (22463119360, 5767168, 86507520, 0), (22468886528, 5767168, 92274688, 0), (22457352192, 5767168, 98041856, 0), (22497722368, 5767168, 103809024, 0), (22503489536, 5767168, 109576192, 0), (22491955200, 5767168, 115343360, 0), (21961375744, 5767168, 121110528, 0), (21967142912, 5767168, 126877696, 0), (21955608576, 5767168, 132644864, 0), (22532325376, 5767168, 138412032, 0), (22538092544, 5767168, 144179200, 0), (22526558208, 5767168, 149946368, 0), (21995978752, 5767168, 155713536, 0), (22001745920, 5767168, 161480704, 0), (21990211584, 5767168, 167247872, 0), (22030581760, 5767168, 173015040, 0), (22036348928, 5767168, 178782208, 0), (22024814592, 5767168, 184549376, 0), (22099787776, 5767168, 190316544, 0), (22105554944, 5767168, 196083712, 0), (22094020608, 5767168, 201850880, 0), (22670737408, 5767168, 207618048, 0), (22676504576, 5767168, 213385216, 0), (22664970240, 5767168, 219152384, 0), (22134390784, 5767168, 224919552, 0), (22140157952, 5767168, 230686720, 0), (22128623616, 5767168, 236453888, 0), (22757244928, 5767168, 242221056, 0), (22763012096, 5767168, 247988224, 0), (22751477760, 5767168, 253755392, 0), (22220898304, 5767168, 259522560, 0), (22226665472, 5767168, 265289728, 0), (22215131136, 5767168, 271056896, 0)], 2: [(21719154688, 5767168, 0, 0), (21724921856, 5767168, 5767168, 0), (21713387520, 5767168, 11534336, 0), (22272802816, 5767168, 17301504, 0), (22278569984, 5767168, 23068672, 0), (22267035648, 5767168, 28835840, 0), (21753757696, 5767168, 34603008, 0), (21759524864, 5767168, 40370176, 0), (21747990528, 5767168, 46137344, 0), (22342008832, 5767168, 51904512, 0), (22347776000, 5767168, 57671680, 0), (22336241664, 5767168, 63438848, 0), (21857566720, 5767168, 69206016, 0), (21863333888, 5767168, 74973184, 0), (21851799552, 5767168, 80740352, 0), (21874868224, 5767168, 86507520, 0), (21880635392, 5767168, 92274688, 0), (21869101056, 5767168, 98041856, 0), (22428516352, 5767168, 103809024, 0), (22434283520, 5767168, 109576192, 0), (22422749184, 5767168, 115343360, 0), (21909471232, 5767168, 121110528, 0), (21915238400, 5767168, 126877696, 0), (21903704064, 5767168, 132644864, 0), (21926772736, 5767168, 138412032, 0), (21932539904, 5767168, 144179200, 0), (21921005568, 5767168, 149946368, 0), (21944074240, 5767168, 155713536, 0), (21949841408, 5767168, 161480704, 0), (21938307072, 5767168, 167247872, 0), (22549626880, 5767168, 173015040, 0), (22555394048, 5767168, 178782208, 0), (22543859712, 5767168, 184549376, 0), (22566928384, 5767168, 190316544, 0), (22572695552, 5767168, 196083712, 0), (22561161216, 5767168, 201850880, 0), (22636134400, 5767168, 207618048, 0), (22641901568, 5767168, 213385216, 0), (22630367232, 5767168, 219152384, 0), (22065184768, 5767168, 224919552, 0), (22070951936, 5767168, 230686720, 0), (22059417600, 5767168, 236453888, 0), (22082486272, 5767168, 242221056, 0), (22088253440, 5767168, 247988224, 0), (22076719104, 5767168, 253755392, 0), (22739943424, 5767168, 259522560, 0), (22745710592, 5767168, 265289728, 0), (22734176256, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e6c3ac6c0>, 2: <capsule object NULL at 0x7a4e34219710>}
DEBUG 01-15 16:10:21.670784.670784 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.670601.670601 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.670080.670080 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 46bc57f1-d3a5-4376-ae02-8f72f8769e71
DEBUG 01-15 16:10:21.670524.670524 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.671912.671912 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:21.671925.671925 client.py:127] Model loaded
DEBUG 01-15 16:10:21.671168.671168 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.671916.671916 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.672881.672881 cuda_h.py:19] end move_flatidxs cost 0.0008397102355957031 seconds
DEBUG 01-15 16:10:21.672333.672333 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.672749.672749 cuda_h.py:19] end restore2model cost 0.0010423660278320312 seconds
DEBUG 01-15 16:10:21.672296.672296 cuda_h.py:19] end sllm_worker_task cost 0.013912439346313477 seconds
INFO 01-15 16:10:21.672464.672464 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 46bc57f1-d3a5-4376-ae02-8f72f8769e71
DEBUG 01-15 16:10:21.673588.673588 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003995656967163086 seconds
DEBUG 01-15 16:10:21.673313.673313 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.676999.676999 cuda_h.py:19] end restore2model cost 0.0030660629272460938 seconds
DEBUG 01-15 16:10:21.676842.676842 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00731658935546875 seconds
DEBUG 01-15 16:10:21.676784.676784 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.677842.677842 cuda_h.py:19] end gpu_sexperts cost 0.000324249267578125 seconds
DEBUG 01-15 16:10:21.677917.677917 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.677753.677753 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.3126602172851562e-05 seconds
DEBUG 01-15 16:10:21.677072.677072 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.677305.677305 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.678606.678606 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001066446304321289 seconds
DEBUG 01-15 16:10:21.677269.677269 cuda_h.py:19] end group_tensors cost 0.005388021469116211 seconds
DEBUG 01-15 16:10:21.678118.678118 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.678366.678366 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.678383.678383 cuda_h.py:19] end gpu_group_list cost 0.00016427040100097656 seconds
DEBUG 01-15 16:10:21.680850.680850 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.682275.682275 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0017218589782714844 seconds
DEBUG 01-15 16:10:21.682948.682948 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.682923.682923 cuda_h.py:19] end group pad cost 0.0039408206939697266 seconds
DEBUG 01-15 16:10:21.682110.682110 cuda_h.py:19] end gpu_group_list cost 0.0002532005310058594 seconds
DEBUG 01-15 16:10:21.682190.682190 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:21.684179.684179 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.685303.685303 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 46bc57f1-d3a5-4376-ae02-8f72f8769e71
INFO 01-15 16:10:21.699486.699486 client.py:127] Model loaded
DEBUG 01-15 16:10:21.700683.700683 cuda_h.py:19] end wait_experts_multi_device cost 0.014800548553466797 seconds
DEBUG 01-15 16:10:21.700022.700022 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.710356.710356 cuda_h.py:19] end group_einsum cost 0.027575016021728516 seconds
DEBUG 01-15 16:10:21.710494.710494 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.714173.714173 cuda_h.py:19] end get_outputs_cpu1 cost 0.004187107086181641 seconds
DEBUG 01-15 16:10:21.715773.715773 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04460620880126953 seconds
DEBUG 01-15 16:10:21.716647.716647 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01659870147705078 seconds
DEBUG 01-15 16:10:21.716674.716674 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.719602.719602 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.719639.719639 cuda_h.py:19] end index_scatter cost 0.00014519691467285156 seconds
DEBUG 01-15 16:10:21.720385.720385 cuda_h.py:19] end cpuoutputsdeal cost 0.0033347606658935547 seconds
DEBUG 01-15 16:10:21.720749.720749 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.720533.720533 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.721605.721605 cuda_h.py:19] end gpu_group_tensor cost 0.0002868175506591797 seconds
DEBUG 01-15 16:10:21.721489.721489 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.721965.721965 cuda_h.py:19] end gpu_group_tensor cost 0.00027060508728027344 seconds
DEBUG 01-15 16:10:21.721078.721078 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.722278.722278 cuda_h.py:19] end gpu_group_einsum cost 0.00112152099609375 seconds
DEBUG 01-15 16:10:21.723618.723618 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.724161.724161 cuda_h.py:19] end gpu_group_einsum cost 0.00087738037109375 seconds
DEBUG 01-15 16:10:21.724130.724130 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.724343.724343 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.725465.725465 cuda_h.py:19] end all_expert_outputs_slices cost 0.00037741661071777344 seconds
DEBUG 01-15 16:10:21.725719.725719 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.725593.725593 cuda_h.py:19] end concat_expert_out cost 0.00011181831359863281 seconds
DEBUG 01-15 16:10:21.725552.725552 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.725552.725552 cuda_h.py:19] end index_scatter cost 0.00012350082397460938 seconds
DEBUG 01-15 16:10:21.726350.726350 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0016968250274658203 seconds
DEBUG 01-15 16:10:21.726390.726390 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.726997.726997 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.726645.726645 cuda_h.py:19] end all_expert_outputs_slices cost 0.00029277801513671875 seconds
DEBUG 01-15 16:10:21.727985.727985 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.727766.727766 cuda_h.py:19] end concat_expert_out cost 0.00011420249938964844 seconds
DEBUG 01-15 16:10:21.727023.727023 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.727540.727540 cuda_h.py:19] end index_scatter cost 0.00011706352233886719 seconds
DEBUG 01-15 16:10:21.727185.727185 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011289119720458984 seconds
DEBUG 01-15 16:10:21.727535.727535 cuda_h.py:19] end gpu_experts_multi_device cost 0.05058574676513672 seconds
DEBUG 01-15 16:10:21.727395.727395 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.06202578544616699 seconds
DEBUG 01-15 16:10:21.728031.728031 cuda_h.py:19] end prefill_layer cost 0.0705723762512207 seconds
DEBUG 01-15 16:10:21.728333.728333 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:10:21.728129.728129 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.728124.728124 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:10:21.729118.729118 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:21.729927.729927 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:21.729374.729374 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 4.5299530029296875e-05 seconds
DEBUG 01-15 16:10:21.729329.729329 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 9.107589721679688e-05 seconds
DEBUG 01-15 16:10:21.729462.729462 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.729504.729504 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.729474.729474 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.729886.729886 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.729412.729412 cuda_h.py:19] end allocate_cuda_memory cost 0.0002467632293701172 seconds
DEBUG 01-15 16:10:21.729123.729123 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.729648.729648 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.729908.729908 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.729756.729756 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8f43055a-af77-4b5f-bc3b-17d17570b45a
DEBUG 01-15 16:10:21.729614.729614 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.730694.730694 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.730520.730520 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.731053.731053 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8f43055a-af77-4b5f-bc3b-17d17570b45a
DEBUG 01-15 16:10:21.731843.731843 cuda_h.py:19] end load_into_gpu_async cost 0.0014796257019042969 seconds
DEBUG 01-15 16:10:21.731546.731546 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.731642.731642 cuda_h.py:19] end restore_tensors2 cost 7.748603820800781e-05 seconds
DEBUG 01-15 16:10:21.731352.731352 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002058267593383789 seconds
INFO 01-15 16:10:21.731288.731288 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8f43055a-af77-4b5f-bc3b-17d17570b45a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.734105.734105 cuda_h.py:19] end self_attn cost 0.0035562515258789062 seconds
DEBUG 01-15 16:10:21.734826.734826 cuda_h.py:19] end iln_self_attn_paln cost 0.005286216735839844 seconds
DEBUG 01-15 16:10:21.734477.734477 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:10:21.734154.734154 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.735247.735247 cuda_h.py:19] end gate cost 0.0007624626159667969 seconds
DEBUG 01-15 16:10:21.735474.735474 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.735780.735780 lmp.py:1912] 
DEBUG 01-15 16:10:21.735780.735780 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.735265.735265 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.736736.736736 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.736724.736724 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.736327.736327 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.736500.736500 lmp.py:1917] 
DEBUG 01-15 16:10:21.736500.736500 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.736773.736773 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.736674.736674 lmp.py:1935]   Expert 44 |     39 | CPU
DEBUG 01-15 16:10:21.736185.736185 lmp.py:1935]   Expert  1 |     47 | CPU
DEBUG 01-15 16:10:21.736650.736650 lmp.py:1935]   Expert 60 |     61 | CPU
DEBUG 01-15 16:10:21.736730.736730 lmp.py:1935]   Expert 28 |     72 | CPU
DEBUG 01-15 16:10:21.736095.736095 lmp.py:1935]   Expert 48 |     78 | CPU
DEBUG 01-15 16:10:21.736414.736414 lmp.py:1935]   Expert 27 |     87 | CPU
DEBUG 01-15 16:10:21.736494.736494 lmp.py:1935]   Expert  0 |    100 | CPU
DEBUG 01-15 16:10:21.736859.736859 lmp.py:1935]   Expert 62 |    108 | CPU
DEBUG 01-15 16:10:21.736721.736721 lmp.py:1935]   Expert 30 |    111 | CPU
DEBUG 01-15 16:10:21.736801.736801 lmp.py:1935]   Expert 42 |    111 | CPU
DEBUG 01-15 16:10:21.736557.736557 lmp.py:1935]   Expert 22 |    114 | CPU
DEBUG 01-15 16:10:21.736207.736207 lmp.py:1935]   Expert 59 |    118 | CPU
DEBUG 01-15 16:10:21.736142.736142 lmp.py:1935]   Expert  8 |    121 | CPU
DEBUG 01-15 16:10:21.736553.736553 lmp.py:1935]   Expert 58 |    123 | CPU
DEBUG 01-15 16:10:21.736726.736726 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:10:21.736376.736376 lmp.py:1935]   Expert 12 |    127 | CPU
DEBUG 01-15 16:10:21.736310.736310 lmp.py:1935]   Expert 50 |    135 | CPU
DEBUG 01-15 16:10:21.736245.736245 lmp.py:1935]   Expert  5 |    144 | CPU
DEBUG 01-15 16:10:21.736133.736133 lmp.py:1935]   Expert 56 |    144 | CPU
DEBUG 01-15 16:10:21.736306.736306 lmp.py:1935]   Expert 55 |    151 | CPU
DEBUG 01-15 16:10:21.736479.736479 lmp.py:1935]   Expert 15 |    153 | CPU
DEBUG 01-15 16:10:21.736652.736652 lmp.py:1935]   Expert 57 |    153 | CPU
DEBUG 01-15 16:10:21.736586.736586 lmp.py:1935]   Expert 26 |    154 | CPU
DEBUG 01-15 16:10:21.736759.736759 lmp.py:1935]   Expert 32 |    158 | CPU
DEBUG 01-15 16:10:21.736455.736455 lmp.py:1935]   Expert 47 |    158 | CPU
DEBUG 01-15 16:10:21.736151.736151 lmp.py:1935]   Expert 34 |    161 | CPU
DEBUG 01-15 16:10:21.736324.736324 lmp.py:1935]   Expert 24 |    163 | CPU
DEBUG 01-15 16:10:21.736689.736689 lmp.py:1935]   Expert  2 |    165 | CPU
DEBUG 01-15 16:10:21.736101.736101 lmp.py:1935]   Expert 52 |    168 | CPU
DEBUG 01-15 16:10:21.736512.736512 lmp.py:1935]   Expert 13 |    169 | CPU
DEBUG 01-15 16:10:21.736923.736923 lmp.py:1935]   Expert 18 |    170 | CPU
DEBUG 01-15 16:10:21.736619.736619 lmp.py:1935]   Expert 40 |    170 | CPU
DEBUG 01-15 16:10:21.736607.736607 lmp.py:1935]   Expert  6 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.736879.736879 lmp.py:1935]   Expert 41 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.736721.736721 lmp.py:1935]   Expert 54 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.736086.736086 lmp.py:1935]   Expert  3 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.736690.736690 lmp.py:1935]   Expert 19 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737532.737532 lmp.py:1935]   Expert 20 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737897.737897 lmp.py:1935]   Expert 46 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737262.737262 lmp.py:1935]   Expert 37 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737534.737534 lmp.py:1935]   Expert 25 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737853.737853 lmp.py:1935]   Expert 51 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737218.737218 lmp.py:1935]   Expert 43 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737583.737583 lmp.py:1935]   Expert 11 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737948.737948 lmp.py:1935]   Expert 17 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737551.737551 lmp.py:1935]   Expert 31 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737439.737439 lmp.py:1935]   Expert 23 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737804.737804 lmp.py:1935]   Expert 35 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737408.737408 lmp.py:1935]   Expert 49 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737011.737011 lmp.py:1935]   Expert 39 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737615.737615 lmp.py:1935]   Expert 53 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737980.737980 lmp.py:1935]   Expert 10 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737106.737106 lmp.py:1935]   Expert 33 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737710.737710 lmp.py:1935]   Expert 36 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737790.737790 lmp.py:1935]   Expert 38 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737586.737586 lmp.py:1935]   Expert  4 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737620.737620 lmp.py:1935]   Expert 21 |    337 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737561.737561 lmp.py:1935]   Expert 14 |    346 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737595.737595 lmp.py:1935]   Expert 45 |    372 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737675.737675 lmp.py:1935]   Expert 63 |    372 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737232.737232 lmp.py:1935]   Expert  9 |    391 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737789.737789 lmp.py:1935]   Expert 61 |    391 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737108.737108 lmp.py:1935]   Expert 29 |    489 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.737427.737427 lmp.py:1935]   Expert  7 |    516 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.737553.737553 lmp.py:1937] 
DEBUG 01-15 16:10:21.737553.737553 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.737164.737164 lmp.py:1938]   CPU:   4058 tokens
DEBUG 01-15 16:10:21.737674.737674 lmp.py:1942]   cuda:1:   4115 tokens (16 experts)
DEBUG 01-15 16:10:21.737185.737185 lmp.py:1942]   cuda:2:   4115 tokens (16 experts)
DEBUG 01-15 16:10:21.737789.737789 lmp.py:1943]   Total GPU:   8230 tokens
DEBUG 01-15 16:10:21.737154.737154 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.737154.737154 lmp.py:1944] 
DEBUG 01-15 16:10:21.737148.737148 cuda_h.py:19] end experts_map_get cost 0.0022835731506347656 seconds
DEBUG 01-15 16:10:21.737872.737872 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.737496.737496 lmp.py:1953] 
DEBUG 01-15 16:10:21.737496.737496 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.738353.738353 cuda_h.py:19] end cpu_experts_submit cost 6.794929504394531e-05 seconds
DEBUG 01-15 16:10:21.738725.738725 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.738688.738688 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.738833.738833 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.35.gate_proj.weight': 0, 'model.layers.19.mlp.experts.35.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.35.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.38.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.38.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.38.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.39.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.39.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.39.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.3.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.3.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.3.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.10.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.10.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.10.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.43.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.43.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.43.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.41.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.41.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.41.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.31.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.31.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.31.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.19.mlp.experts.33.gate_proj.weight': 0, 'model.layers.19.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.20.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.20.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.20.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.54.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.54.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.54.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.23.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.23.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.23.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.29.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(23397400576, 5767168, 0, 0), (23403167744, 5767168, 5767168, 0), (23391633408, 5767168, 11534336, 0), (23414702080, 5767168, 17301504, 0), (23420469248, 5767168, 23068672, 0), (23408934912, 5767168, 28835840, 0), (23432003584, 5767168, 34603008, 0), (23437770752, 5767168, 40370176, 0), (23426236416, 5767168, 46137344, 0), (23449305088, 5767168, 51904512, 0), (23455072256, 5767168, 57671680, 0), (23443537920, 5767168, 63438848, 0), (22912958464, 5767168, 69206016, 0), (22918725632, 5767168, 74973184, 0), (22907191296, 5767168, 80740352, 0), (23466606592, 5767168, 86507520, 0), (23472373760, 5767168, 92274688, 0), (23460839424, 5767168, 98041856, 0), (22843752448, 5767168, 103809024, 0), (22849519616, 5767168, 109576192, 0), (22837985280, 5767168, 115343360, 0), (22964862976, 5767168, 121110528, 0), (22970630144, 5767168, 126877696, 0), (22959095808, 5767168, 132644864, 0), (23535812608, 5767168, 138412032, 0), (23541579776, 5767168, 144179200, 0), (23530045440, 5767168, 149946368, 0), (23501209600, 5767168, 155713536, 0), (23506976768, 5767168, 161480704, 0), (23495442432, 5767168, 167247872, 0), (23120576512, 5767168, 173015040, 0), (23126343680, 5767168, 178782208, 0), (23114809344, 5767168, 184549376, 0), (23155179520, 5767168, 190316544, 0), (23160946688, 5767168, 196083712, 0), (23149412352, 5767168, 201850880, 0), (23224385536, 5767168, 207618048, 0), (23230152704, 5767168, 213385216, 0), (23218618368, 5767168, 219152384, 0), (23328194560, 5767168, 224919552, 0), (23333961728, 5767168, 230686720, 0), (23322427392, 5767168, 236453888, 0), (23847239680, 5767168, 242221056, 0), (23853006848, 5767168, 247988224, 0), (23841472512, 5767168, 253755392, 0), (23881842688, 5767168, 259522560, 0), (23887609856, 5767168, 265289728, 0), (23876075520, 5767168, 271056896, 0)], 2: [(23362797568, 5767168, 0, 0), (23368564736, 5767168, 5767168, 0), (23357030400, 5767168, 11534336, 0), (22861053952, 5767168, 17301504, 0), (22866821120, 5767168, 23068672, 0), (22855286784, 5767168, 28835840, 0), (22895656960, 5767168, 34603008, 0), (22901424128, 5767168, 40370176, 0), (22889889792, 5767168, 46137344, 0), (22947561472, 5767168, 51904512, 0), (22953328640, 5767168, 57671680, 0), (22941794304, 5767168, 63438848, 0), (22982164480, 5767168, 69206016, 0), (22987931648, 5767168, 74973184, 0), (22976397312, 5767168, 80740352, 0), (23570415616, 5767168, 86507520, 0), (23576182784, 5767168, 92274688, 0), (23564648448, 5767168, 98041856, 0), (23034068992, 5767168, 103809024, 0), (23039836160, 5767168, 109576192, 0), (23028301824, 5767168, 115343360, 0), (23587717120, 5767168, 121110528, 0), (23593484288, 5767168, 126877696, 0), (23581949952, 5767168, 132644864, 0), (23639621632, 5767168, 138412032, 0), (23645388800, 5767168, 144179200, 0), (23633854464, 5767168, 149946368, 0), (23085973504, 5767168, 155713536, 0), (23091740672, 5767168, 161480704, 0), (23080206336, 5767168, 167247872, 0), (23674224640, 5767168, 173015040, 0), (23679991808, 5767168, 178782208, 0), (23668457472, 5767168, 184549376, 0), (23137878016, 5767168, 190316544, 0), (23143645184, 5767168, 196083712, 0), (23132110848, 5767168, 201850880, 0), (23708827648, 5767168, 207618048, 0), (23714594816, 5767168, 213385216, 0), (23703060480, 5767168, 219152384, 0), (23726129152, 5767168, 224919552, 0), (23731896320, 5767168, 230686720, 0), (23720361984, 5767168, 236453888, 0), (23189782528, 5767168, 242221056, 0), (23195549696, 5767168, 247988224, 0), (23184015360, 5767168, 253755392, 0), (23293591552, 5767168, 259522560, 0), (23299358720, 5767168, 265289728, 0), (23287824384, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e342198c0>, 2: <capsule object NULL at 0x7a4e34218600>}
DEBUG 01-15 16:10:21.739704.739704 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.739593.739593 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.739111.739111 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1f31dbe9-2827-4b16-bf9a-3d35607da434
DEBUG 01-15 16:10:21.739549.739549 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:21.739793.739793 client.py:127] Model loaded
DEBUG 01-15 16:10:21.739159.739159 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.739352.739352 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.740553.740553 cuda_h.py:19] end restore2model cost 0.0004611015319824219 seconds
DEBUG 01-15 16:10:21.740778.740778 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.740628.740628 cuda_h.py:19] end sllm_worker_task cost 0.011145353317260742 seconds
INFO 01-15 16:10:21.740304.740304 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1f31dbe9-2827-4b16-bf9a-3d35607da434
DEBUG 01-15 16:10:21.741293.741293 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030062198638916016 seconds
DEBUG 01-15 16:10:21.741010.741010 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.741282.741282 cuda_h.py:19] end move_flatidxs cost 0.0009288787841796875 seconds
DEBUG 01-15 16:10:21.741556.741556 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.743163.743163 cuda_h.py:19] end restore2model cost 0.002537250518798828 seconds
DEBUG 01-15 16:10:21.743714.743714 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0057849884033203125 seconds
DEBUG 01-15 16:10:21.743530.743530 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.744706.744706 cuda_h.py:19] end gpu_sexperts cost 0.00027441978454589844 seconds
DEBUG 01-15 16:10:21.744390.744390 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.744259.744259 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:10:21.744624.744624 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.744042.744042 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.745541.745541 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008280277252197266 seconds
DEBUG 01-15 16:10:21.745099.745099 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.745127.745127 cuda_h.py:19] end gpu_group_list cost 0.00019431114196777344 seconds
DEBUG 01-15 16:10:21.746159.746159 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.747508.747508 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008671283721923828 seconds
DEBUG 01-15 16:10:21.747298.747298 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.747159.747159 cuda_h.py:19] end gpu_group_list cost 0.0001850128173828125 seconds
DEBUG 01-15 16:10:21.748152.748152 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.748220.748220 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1f31dbe9-2827-4b16-bf9a-3d35607da434
DEBUG 01-15 16:10:21.748832.748832 cuda_h.py:19] end group_tensors cost 0.006615877151489258 seconds
DEBUG 01-15 16:10:21.749394.749394 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.755120.755120 cuda_h.py:19] end group pad cost 0.006371259689331055 seconds
DEBUG 01-15 16:10:21.755817.755817 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.768892.768892 client.py:127] Model loaded
DEBUG 01-15 16:10:21.768684.768684 cuda_h.py:19] end wait_experts_multi_device cost 0.020165443420410156 seconds
DEBUG 01-15 16:10:21.768546.768546 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.776356.776356 cuda_h.py:19] end group_einsum cost 0.020714521408081055 seconds
DEBUG 01-15 16:10:21.776334.776334 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.780422.780422 cuda_h.py:19] end get_outputs_cpu1 cost 0.003925323486328125 seconds
DEBUG 01-15 16:10:21.781579.781579 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04148292541503906 seconds
DEBUG 01-15 16:10:21.782274.782274 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013947010040283203 seconds
DEBUG 01-15 16:10:21.782107.782107 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.784054.784054 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.784824.784824 cuda_h.py:19] end index_scatter cost 0.00012350082397460938 seconds
DEBUG 01-15 16:10:21.785852.785852 cuda_h.py:19] end cpuoutputsdeal cost 0.0027840137481689453 seconds
DEBUG 01-15 16:10:21.785181.785181 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.785230.785230 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.785684.785684 cuda_h.py:19] end gpu_group_tensor cost 0.0002338886260986328 seconds
DEBUG 01-15 16:10:21.785163.785163 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.786550.786550 cuda_h.py:19] end gpu_group_tensor cost 0.00022172927856445312 seconds
DEBUG 01-15 16:10:21.786086.786086 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.787730.787730 cuda_h.py:19] end gpu_group_einsum cost 0.0009343624114990234 seconds
DEBUG 01-15 16:10:21.787724.787724 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.788667.788667 cuda_h.py:19] end gpu_group_einsum cost 0.0007700920104980469 seconds
DEBUG 01-15 16:10:21.788031.788031 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.788236.788236 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.789520.789520 cuda_h.py:19] end all_expert_outputs_slices cost 0.00030994415283203125 seconds
DEBUG 01-15 16:10:21.789197.789197 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.789155.789155 cuda_h.py:19] end concat_expert_out cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:10:21.789594.789594 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.789989.789989 cuda_h.py:19] end index_scatter cost 6.937980651855469e-05 seconds
DEBUG 01-15 16:10:21.789111.789111 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011334419250488281 seconds
DEBUG 01-15 16:10:21.790076.790076 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.790555.790555 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.790574.790574 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001499652862548828 seconds
DEBUG 01-15 16:10:21.790198.790198 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.790341.790341 cuda_h.py:19] end concat_expert_out cost 6.413459777832031e-05 seconds
DEBUG 01-15 16:10:21.790443.790443 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.790162.790162 cuda_h.py:19] end index_scatter cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:10:21.790892.790892 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005884170532226562 seconds
DEBUG 01-15 16:10:21.790484.790484 cuda_h.py:19] end gpu_experts_multi_device cost 0.04643058776855469 seconds
DEBUG 01-15 16:10:21.790097.790097 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.056244850158691406 seconds
DEBUG 01-15 16:10:21.791331.791331 cuda_h.py:19] end prefill_layer cost 0.06236529350280762 seconds
DEBUG 01-15 16:10:21.791036.791036 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:10:21.791514.791514 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.791468.791468 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:10:21.791139.791139 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:21.791670.791670 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:21.791123.791123 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:21.791608.791608 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 9.584426879882812e-05 seconds
DEBUG 01-15 16:10:21.791602.791602 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.791831.791831 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.792037.792037 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.792331.792331 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.792736.792736 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.792702.792702 cuda_h.py:19] end allocate_cuda_memory cost 0.00043487548828125 seconds
DEBUG 01-15 16:10:21.793092.793092 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.793792.793792 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.793545.793545 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.793322.793322 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f173b21f-393b-4986-be27-ee9e74608515
DEBUG 01-15 16:10:21.793012.793012 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.794817.794817 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.795201.795201 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f173b21f-393b-4986-be27-ee9e74608515
DEBUG 01-15 16:10:21.795790.795790 cuda_h.py:19] end load_into_gpu_async cost 0.0023131370544433594 seconds
DEBUG 01-15 16:10:21.795799.795799 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.795734.795734 cuda_h.py:19] end restore_tensors2 cost 0.00015544891357421875 seconds
DEBUG 01-15 16:10:21.796408.796408 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0037021636962890625 seconds
INFO 01-15 16:10:21.796982.796982 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f173b21f-393b-4986-be27-ee9e74608515
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.799119.799119 cuda_h.py:19] end self_attn cost 0.005545616149902344 seconds
DEBUG 01-15 16:10:21.800695.800695 cuda_h.py:19] end iln_self_attn_paln cost 0.008448362350463867 seconds
DEBUG 01-15 16:10:21.800975.800975 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:10:21.800553.800553 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.801840.801840 cuda_h.py:19] end gate cost 0.0008375644683837891 seconds
DEBUG 01-15 16:10:21.801498.801498 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.801233.801233 lmp.py:1912] 
DEBUG 01-15 16:10:21.801233.801233 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.801757.801757 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.801221.801221 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.801679.801679 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.801753.801753 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.801396.801396 lmp.py:1917] 
DEBUG 01-15 16:10:21.801396.801396 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.801330.801330 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.802132.802132 lmp.py:1935]   Expert 54 |     21 | CPU
DEBUG 01-15 16:10:21.802828.802828 lmp.py:1935]   Expert  3 |     33 | CPU
DEBUG 01-15 16:10:21.802286.802286 lmp.py:1935]   Expert  8 |     39 | CPU
DEBUG 01-15 16:10:21.802267.802267 lmp.py:1935]   Expert 28 |     45 | CPU
DEBUG 01-15 16:10:21.802678.802678 lmp.py:1935]   Expert 63 |     54 | CPU
DEBUG 01-15 16:10:21.802851.802851 lmp.py:1935]   Expert 43 |     55 | CPU
DEBUG 01-15 16:10:21.802786.802786 lmp.py:1935]   Expert 36 |     74 | CPU
DEBUG 01-15 16:10:21.802005.802005 lmp.py:1935]   Expert 38 |     77 | CPU
DEBUG 01-15 16:10:21.802655.802655 lmp.py:1935]   Expert  6 |     81 | CPU
DEBUG 01-15 16:10:21.802874.802874 lmp.py:1935]   Expert 39 |     94 | CPU
DEBUG 01-15 16:10:21.802378.802378 lmp.py:1935]   Expert 57 |     98 | CPU
DEBUG 01-15 16:10:21.802120.802120 lmp.py:1935]   Expert 41 |    104 | CPU
DEBUG 01-15 16:10:21.802863.802863 lmp.py:1935]   Expert 12 |    110 | CPU
DEBUG 01-15 16:10:21.802128.802128 lmp.py:1935]   Expert 52 |    112 | CPU
DEBUG 01-15 16:10:21.802109.802109 lmp.py:1935]   Expert 19 |    120 | CPU
DEBUG 01-15 16:10:21.802375.802375 lmp.py:1935]   Expert 47 |    127 | CPU
DEBUG 01-15 16:10:21.802594.802594 lmp.py:1935]   Expert 13 |    136 | CPU
DEBUG 01-15 16:10:21.802336.802336 lmp.py:1935]   Expert 22 |    140 | CPU
DEBUG 01-15 16:10:21.802556.802556 lmp.py:1935]   Expert 50 |    146 | CPU
DEBUG 01-15 16:10:21.802775.802775 lmp.py:1935]   Expert 46 |    148 | CPU
DEBUG 01-15 16:10:21.802710.802710 lmp.py:1935]   Expert 24 |    162 | CPU
DEBUG 01-15 16:10:21.802214.802214 lmp.py:1935]   Expert 40 |    163 | CPU
DEBUG 01-15 16:10:21.802956.802956 lmp.py:1935]   Expert 20 |    166 | CPU
DEBUG 01-15 16:10:21.802175.802175 lmp.py:1935]   Expert 55 |    166 | CPU
DEBUG 01-15 16:10:21.802918.802918 lmp.py:1935]   Expert  2 |    169 | CPU
DEBUG 01-15 16:10:21.802183.802183 lmp.py:1935]   Expert 23 |    172 | CPU
DEBUG 01-15 16:10:21.802164.802164 lmp.py:1935]   Expert 37 |    173 | CPU
DEBUG 01-15 16:10:21.802668.802668 lmp.py:1935]   Expert 53 |    173 | CPU
DEBUG 01-15 16:10:21.802887.802887 lmp.py:1935]   Expert 49 |    175 | CPU
DEBUG 01-15 16:10:21.802868.802868 lmp.py:1935]   Expert 61 |    175 | CPU
DEBUG 01-15 16:10:21.802326.802326 lmp.py:1935]   Expert 21 |    177 | CPU
DEBUG 01-15 16:10:21.802068.802068 lmp.py:1935]   Expert 42 |    178 | CPU
DEBUG 01-15 16:10:21.802910.802910 lmp.py:1935]   Expert 18 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.802798.802798 lmp.py:1935]   Expert 33 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.802971.802971 lmp.py:1935]   Expert 32 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.802621.802621 lmp.py:1935]   Expert 30 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.802555.802555 lmp.py:1935]   Expert  0 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.802159.802159 lmp.py:1935]   Expert  5 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.802047.802047 lmp.py:1935]   Expert 16 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.802220.802220 lmp.py:1935]   Expert 14 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.802916.802916 lmp.py:1935]   Expert  7 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.802612.802612 lmp.py:1935]   Expert 34 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.802547.802547 lmp.py:1935]   Expert 31 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.802527.802527 lmp.py:1935]   Expert 62 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.802224.802224 lmp.py:1935]   Expert 59 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.802158.802158 lmp.py:1935]   Expert 60 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803808.803808 lmp.py:1935]   Expert  9 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803504.803504 lmp.py:1935]   Expert 17 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.803392.803392 lmp.py:1935]   Expert 10 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803611.803611 lmp.py:1935]   Expert 29 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.803592.803592 lmp.py:1935]   Expert  4 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.803050.803050 lmp.py:1935]   Expert 15 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.803508.803508 lmp.py:1935]   Expert 58 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803727.803727 lmp.py:1935]   Expert 26 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803184.803184 lmp.py:1935]   Expert 51 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.803927.803927 lmp.py:1935]   Expert 11 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803385.803385 lmp.py:1935]   Expert 44 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803604.803604 lmp.py:1935]   Expert 56 |    289 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.803015.803015 lmp.py:1935]   Expert 27 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.803427.803427 lmp.py:1935]   Expert  1 |    331 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803123.803123 lmp.py:1935]   Expert 45 |    367 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.803296.803296 lmp.py:1935]   Expert 25 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803753.803753 lmp.py:1935]   Expert 35 |    519 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.803211.803211 lmp.py:1935]   Expert 48 |    643 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.803238.803238 lmp.py:1937] 
DEBUG 01-15 16:10:21.803238.803238 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.803173.803173 lmp.py:1938]   CPU:   3863 tokens
DEBUG 01-15 16:10:21.803153.803153 lmp.py:1942]   cuda:1:   4213 tokens (16 experts)
DEBUG 01-15 16:10:21.803611.803611 lmp.py:1942]   cuda:2:   4212 tokens (16 experts)
DEBUG 01-15 16:10:21.803592.803592 lmp.py:1943]   Total GPU:   8425 tokens
DEBUG 01-15 16:10:21.803334.803334 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.803334.803334 lmp.py:1944] 
DEBUG 01-15 16:10:21.803421.803421 cuda_h.py:19] end experts_map_get cost 0.0022280216217041016 seconds
DEBUG 01-15 16:10:21.803312.803312 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.803267.803267 lmp.py:1953] 
DEBUG 01-15 16:10:21.803267.803267 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.803223.803223 cuda_h.py:19] end cpu_experts_submit cost 7.2479248046875e-05 seconds
DEBUG 01-15 16:10:21.803687.803687 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.803909.803909 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.804242.804242 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.32.gate_proj.weight': 0, 'model.layers.20.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.59.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.59.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.59.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.35.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.35.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.35.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.58.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.58.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.58.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.5.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.5.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.5.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.10.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.10.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.10.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.11.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.11.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.11.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.44.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.44.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.44.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(24452792320, 5767168, 0, 0), (24458559488, 5767168, 5767168, 0), (24447025152, 5767168, 11534336, 0), (24470093824, 5767168, 17301504, 0), (24475860992, 5767168, 23068672, 0), (24464326656, 5767168, 28835840, 0), (23968350208, 5767168, 34603008, 0), (23974117376, 5767168, 40370176, 0), (23962583040, 5767168, 46137344, 0), (24020254720, 5767168, 51904512, 0), (24026021888, 5767168, 57671680, 0), (24014487552, 5767168, 63438848, 0), (24919932928, 5767168, 69206016, 0), (24925700096, 5767168, 74973184, 0), (24914165760, 5767168, 80740352, 0), (24677711872, 5767168, 86507520, 0), (24683479040, 5767168, 92274688, 0), (24671944704, 5767168, 98041856, 0), (24141365248, 5767168, 103809024, 0), (24147132416, 5767168, 109576192, 0), (24135598080, 5767168, 115343360, 0), (24158666752, 5767168, 121110528, 0), (24164433920, 5767168, 126877696, 0), (24152899584, 5767168, 132644864, 0), (24729616384, 5767168, 138412032, 0), (24735383552, 5767168, 144179200, 0), (24723849216, 5767168, 149946368, 0), (24193269760, 5767168, 155713536, 0), (24199036928, 5767168, 161480704, 0), (24187502592, 5767168, 167247872, 0), (24175968256, 5767168, 173015040, 0), (24181735424, 5767168, 178782208, 0), (24170201088, 5767168, 184549376, 0), (24781520896, 5767168, 190316544, 0), (24787288064, 5767168, 196083712, 0), (24775753728, 5767168, 201850880, 0), (24868028416, 5767168, 207618048, 0), (24873795584, 5767168, 213385216, 0), (24862261248, 5767168, 219152384, 0), (24366284800, 5767168, 224919552, 0), (24372051968, 5767168, 230686720, 0), (24360517632, 5767168, 236453888, 0), (24400887808, 5767168, 242221056, 0), (24406654976, 5767168, 247988224, 0), (24395120640, 5767168, 253755392, 0), (24971837440, 5767168, 259522560, 0), (24977604608, 5767168, 265289728, 0), (24966070272, 5767168, 271056896, 0)], 2: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (23916445696, 5767168, 17301504, 0), (23922212864, 5767168, 23068672, 0), (23910678528, 5767168, 28835840, 0), (24487395328, 5767168, 34603008, 0), (24493162496, 5767168, 40370176, 0), (24481628160, 5767168, 46137344, 0), (24504696832, 5767168, 51904512, 0), (24510464000, 5767168, 57671680, 0), (24498929664, 5767168, 63438848, 0), (24902631424, 5767168, 69206016, 0), (24908398592, 5767168, 74973184, 0), (24896864256, 5767168, 80740352, 0), (23985651712, 5767168, 86507520, 0), (23991418880, 5767168, 92274688, 0), (23979884544, 5767168, 98041856, 0), (24054857728, 5767168, 103809024, 0), (24060624896, 5767168, 109576192, 0), (24049090560, 5767168, 115343360, 0), (24072159232, 5767168, 121110528, 0), (24077926400, 5767168, 126877696, 0), (24066392064, 5767168, 132644864, 0), (24089460736, 5767168, 138412032, 0), (24095227904, 5767168, 144179200, 0), (24083693568, 5767168, 149946368, 0), (24660410368, 5767168, 155713536, 0), (24666177536, 5767168, 161480704, 0), (24654643200, 5767168, 167247872, 0), (24210571264, 5767168, 173015040, 0), (24216338432, 5767168, 178782208, 0), (24204804096, 5767168, 184549376, 0), (24331681792, 5767168, 190316544, 0), (24337448960, 5767168, 196083712, 0), (24325914624, 5767168, 201850880, 0), (24348983296, 5767168, 207618048, 0), (24354750464, 5767168, 213385216, 0), (24343216128, 5767168, 219152384, 0), (24937234432, 5767168, 224919552, 0), (24943001600, 5767168, 230686720, 0), (24931467264, 5767168, 236453888, 0), (24418189312, 5767168, 242221056, 0), (24423956480, 5767168, 247988224, 0), (24412422144, 5767168, 253755392, 0), (24435490816, 5767168, 259522560, 0), (24441257984, 5767168, 265289728, 0), (24429723648, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e6c4686c0>, 2: <capsule object NULL at 0x7a4e546786c0>}
DEBUG 01-15 16:10:21.805657.805657 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.805435.805435 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.805867.805867 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 805b8867-a74d-414c-9dcf-5cf16439a7a6
DEBUG 01-15 16:10:21.805763.805763 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:21.806610.806610 client.py:127] Model loaded
DEBUG 01-15 16:10:21.806106.806106 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.806815.806815 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.807726.807726 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.807606.807606 cuda_h.py:19] end restore2model cost 0.0011830329895019531 seconds
DEBUG 01-15 16:10:21.808791.808791 cuda_h.py:19] end move_flatidxs cost 0.0010693073272705078 seconds
INFO 01-15 16:10:21.808102.808102 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 805b8867-a74d-414c-9dcf-5cf16439a7a6
DEBUG 01-15 16:10:21.808918.808918 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.808544.808544 cuda_h.py:19] end sllm_worker_task cost 0.016069412231445312 seconds
DEBUG 01-15 16:10:21.808104.808104 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004970550537109375 seconds
DEBUG 01-15 16:10:21.809886.809886 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.812220.812220 cuda_h.py:19] end restore2model cost 0.0030143260955810547 seconds
DEBUG 01-15 16:10:21.812779.812779 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008430242538452148 seconds
DEBUG 01-15 16:10:21.812548.812548 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.812403.812403 cuda_h.py:19] end gpu_sexperts cost 0.00038361549377441406 seconds
DEBUG 01-15 16:10:21.812485.812485 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.812143.812143 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 3.0517578125e-05 seconds
DEBUG 01-15 16:10:21.812461.812461 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.812125.812125 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.814143.814143 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011019706726074219 seconds
DEBUG 01-15 16:10:21.814231.814231 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.814231.814231 cuda_h.py:19] end gpu_group_list cost 0.00018072128295898438 seconds
DEBUG 01-15 16:10:21.815768.815768 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.816241.816241 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011987686157226562 seconds
DEBUG 01-15 16:10:21.816382.816382 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.816575.816575 cuda_h.py:19] end gpu_group_list cost 0.00017952919006347656 seconds
DEBUG 01-15 16:10:21.817900.817900 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.817166.817166 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 805b8867-a74d-414c-9dcf-5cf16439a7a6
DEBUG 01-15 16:10:21.817879.817879 cuda_h.py:19] end group_tensors cost 0.008884906768798828 seconds
DEBUG 01-15 16:10:21.817335.817335 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.823733.823733 cuda_h.py:19] end group pad cost 0.005505800247192383 seconds
DEBUG 01-15 16:10:21.823292.823292 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.834536.834536 client.py:127] Model loaded
DEBUG 01-15 16:10:21.834040.834040 cuda_h.py:19] end wait_experts_multi_device cost 0.016976594924926758 seconds
DEBUG 01-15 16:10:21.834341.834341 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.844775.844775 cuda_h.py:19] end group_einsum cost 0.021259307861328125 seconds
DEBUG 01-15 16:10:21.845224.845224 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.848305.848305 cuda_h.py:19] end get_outputs_cpu1 cost 0.003742694854736328 seconds
DEBUG 01-15 16:10:21.849959.849959 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04300093650817871 seconds
DEBUG 01-15 16:10:21.850441.850441 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.015931129455566406 seconds
DEBUG 01-15 16:10:21.851111.851111 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.853970.853970 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.854808.854808 cuda_h.py:19] end index_scatter cost 0.00014781951904296875 seconds
DEBUG 01-15 16:10:21.854448.854448 cuda_h.py:19] end cpuoutputsdeal cost 0.003444194793701172 seconds
DEBUG 01-15 16:10:21.854249.854249 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.854801.854801 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.855781.855781 cuda_h.py:19] end gpu_group_tensor cost 0.0002899169921875 seconds
DEBUG 01-15 16:10:21.855426.855426 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.855948.855948 cuda_h.py:19] end gpu_group_tensor cost 0.0002720355987548828 seconds
DEBUG 01-15 16:10:21.855300.855300 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.857925.857925 cuda_h.py:19] end gpu_group_einsum cost 0.0011527538299560547 seconds
DEBUG 01-15 16:10:21.857655.857655 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.858001.858001 cuda_h.py:19] end gpu_group_einsum cost 0.0009052753448486328 seconds
DEBUG 01-15 16:10:21.858234.858234 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.858586.858586 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.859331.859331 cuda_h.py:19] end all_expert_outputs_slices cost 0.00037932395935058594 seconds
DEBUG 01-15 16:10:21.859823.859823 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.859366.859366 cuda_h.py:19] end concat_expert_out cost 0.00011205673217773438 seconds
DEBUG 01-15 16:10:21.859232.859232 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.860564.860564 cuda_h.py:19] end index_scatter cost 0.00012183189392089844 seconds
DEBUG 01-15 16:10:21.860016.860016 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0017042160034179688 seconds
DEBUG 01-15 16:10:21.860358.860358 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.860215.860215 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.860168.860168 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013899803161621094 seconds
DEBUG 01-15 16:10:21.860453.860453 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.860066.860066 cuda_h.py:19] end concat_expert_out cost 5.91278076171875e-05 seconds
DEBUG 01-15 16:10:21.861161.861161 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.861443.861443 cuda_h.py:19] end index_scatter cost 5.91278076171875e-05 seconds
DEBUG 01-15 16:10:21.861358.861358 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005512237548828125 seconds
DEBUG 01-15 16:10:21.861897.861897 cuda_h.py:19] end gpu_experts_multi_device cost 0.04839944839477539 seconds
DEBUG 01-15 16:10:21.861311.861311 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.06102871894836426 seconds
DEBUG 01-15 16:10:21.861407.861407 cuda_h.py:19] end prefill_layer cost 0.07035040855407715 seconds
DEBUG 01-15 16:10:21.861111.861111 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:10:21.861728.861728 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.861345.861345 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:10:21.862677.862677 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:21.862063.862063 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:21.862509.862509 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:10:21.862750.862750 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.862917.862917 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 0.00017380714416503906 seconds
DEBUG 01-15 16:10:21.862987.862987 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.862723.862723 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.862263.862263 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.862386.862386 cuda_h.py:19] end allocate_cuda_memory cost 0.0002353191375732422 seconds
DEBUG 01-15 16:10:21.862150.862150 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.862244.862244 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.862551.862551 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.863114.863114 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 50d7c087-683f-4373-a0ee-71f4215b6e7e
DEBUG 01-15 16:10:21.863833.863833 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.863847.863847 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.863463.863463 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.864043.864043 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 50d7c087-683f-4373-a0ee-71f4215b6e7e
DEBUG 01-15 16:10:21.864982.864982 cuda_h.py:19] end load_into_gpu_async cost 0.0020351409912109375 seconds
DEBUG 01-15 16:10:21.865281.865281 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.865432.865432 cuda_h.py:19] end restore_tensors2 cost 0.00011801719665527344 seconds
DEBUG 01-15 16:10:21.865486.865486 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002761363983154297 seconds
INFO 01-15 16:10:21.865938.865938 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 50d7c087-683f-4373-a0ee-71f4215b6e7e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
INFO 01-15 16:10:21.872936.872936 client.py:127] Model loaded
DEBUG 01-15 16:10:21.872827.872827 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.873997.873997 cuda_h.py:19] end self_attn cost 0.009557247161865234 seconds
DEBUG 01-15 16:10:21.873893.873893 cuda_h.py:19] end restore2model cost 0.0007305145263671875 seconds
DEBUG 01-15 16:10:21.873665.873665 cuda_h.py:19] end sllm_worker_task cost 0.011358261108398438 seconds
DEBUG 01-15 16:10:21.874213.874213 cuda_h.py:19] end iln_self_attn_paln cost 0.011514425277709961 seconds
DEBUG 01-15 16:10:21.874633.874633 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:10:21.874919.874919 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.875152.875152 cuda_h.py:19] end gate cost 0.0008020401000976562 seconds
DEBUG 01-15 16:10:21.875942.875942 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.875238.875238 lmp.py:1912] 
DEBUG 01-15 16:10:21.875238.875238 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.875855.875855 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.875889.875889 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.875108.875108 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.875420.875420 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.875494.875494 lmp.py:1917] 
DEBUG 01-15 16:10:21.875494.875494 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.875090.875090 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.875647.875647 lmp.py:1935]   Expert 44 |     30 | CPU
DEBUG 01-15 16:10:21.875482.875482 lmp.py:1935]   Expert  9 |     35 | CPU
DEBUG 01-15 16:10:21.875125.875125 lmp.py:1935]   Expert 11 |     36 | CPU
DEBUG 01-15 16:10:21.875530.875530 lmp.py:1935]   Expert 56 |     59 | CPU
DEBUG 01-15 16:10:21.875173.875173 lmp.py:1935]   Expert 54 |     79 | CPU
DEBUG 01-15 16:10:21.875008.875008 lmp.py:1935]   Expert 62 |     92 | CPU
DEBUG 01-15 16:10:21.875320.875320 lmp.py:1935]   Expert  7 |     94 | CPU
DEBUG 01-15 16:10:21.875393.875393 lmp.py:1935]   Expert 47 |     94 | CPU
DEBUG 01-15 16:10:21.875467.875467 lmp.py:1935]   Expert 51 |    100 | CPU
DEBUG 01-15 16:10:21.875871.875871 lmp.py:1935]   Expert 60 |    106 | CPU
DEBUG 01-15 16:10:21.875038.875038 lmp.py:1935]   Expert 22 |    108 | CPU
DEBUG 01-15 16:10:21.875442.875442 lmp.py:1935]   Expert 53 |    108 | CPU
DEBUG 01-15 16:10:21.875131.875131 lmp.py:1935]   Expert 41 |    111 | CPU
DEBUG 01-15 16:10:21.875059.875059 lmp.py:1935]   Expert 52 |    111 | CPU
DEBUG 01-15 16:10:21.876987.876987 lmp.py:1935]   Expert 48 |    125 | CPU
DEBUG 01-15 16:10:21.876153.876153 lmp.py:1935]   Expert  8 |    126 | CPU
DEBUG 01-15 16:10:21.876081.876081 lmp.py:1935]   Expert  1 |    127 | CPU
DEBUG 01-15 16:10:21.876247.876247 lmp.py:1935]   Expert  2 |    129 | CPU
DEBUG 01-15 16:10:21.876844.876844 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:10:21.876202.876202 lmp.py:1935]   Expert 32 |    131 | CPU
DEBUG 01-15 16:10:21.876037.876037 lmp.py:1935]   Expert 27 |    137 | CPU
DEBUG 01-15 16:10:21.876203.876203 lmp.py:1935]   Expert 35 |    138 | CPU
DEBUG 01-15 16:10:21.876131.876131 lmp.py:1935]   Expert 23 |    142 | CPU
DEBUG 01-15 16:10:21.876820.876820 lmp.py:1935]   Expert 59 |    143 | CPU
DEBUG 01-15 16:10:21.876509.876509 lmp.py:1935]   Expert 39 |    146 | CPU
DEBUG 01-15 16:10:21.876437.876437 lmp.py:1935]   Expert 26 |    148 | CPU
DEBUG 01-15 16:10:21.876126.876126 lmp.py:1935]   Expert 50 |    150 | CPU
DEBUG 01-15 16:10:21.876816.876816 lmp.py:1935]   Expert 14 |    159 | CPU
DEBUG 01-15 16:10:21.876267.876267 lmp.py:1935]   Expert 46 |    164 | CPU
DEBUG 01-15 16:10:21.876194.876194 lmp.py:1935]   Expert 24 |    168 | CPU
DEBUG 01-15 16:10:21.876076.876076 lmp.py:1935]   Expert 38 |    168 | CPU
DEBUG 01-15 16:10:21.876911.876911 lmp.py:1935]   Expert  0 |    172 | CPU
DEBUG 01-15 16:10:21.876176.876176 lmp.py:1935]   Expert  4 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876535.876535 lmp.py:1935]   Expert 34 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876654.876654 lmp.py:1935]   Expert 49 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876536.876536 lmp.py:1935]   Expert 40 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876643.876643 lmp.py:1935]   Expert  5 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876955.876955 lmp.py:1935]   Expert 63 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876313.876313 lmp.py:1935]   Expert 19 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876672.876672 lmp.py:1935]   Expert 13 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876791.876791 lmp.py:1935]   Expert 29 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876150.876150 lmp.py:1935]   Expert 43 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876508.876508 lmp.py:1935]   Expert 57 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876297.876297 lmp.py:1935]   Expert 61 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876085.876085 lmp.py:1935]   Expert 31 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876682.876682 lmp.py:1935]   Expert 33 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876802.876802 lmp.py:1935]   Expert 16 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876922.876922 lmp.py:1935]   Expert 20 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876041.876041 lmp.py:1935]   Expert  3 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876400.876400 lmp.py:1935]   Expert 37 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876281.876281 lmp.py:1935]   Expert 15 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876401.876401 lmp.py:1935]   Expert 36 |    273 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876282.876282 lmp.py:1935]   Expert 12 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876164.876164 lmp.py:1935]   Expert 18 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876807.876807 lmp.py:1935]   Expert 28 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876165.876165 lmp.py:1935]   Expert 17 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876046.876046 lmp.py:1935]   Expert 55 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876166.876166 lmp.py:1935]   Expert 30 |    320 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876955.876955 lmp.py:1935]   Expert 25 |    325 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876505.876505 lmp.py:1935]   Expert 58 |    337 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876532.876532 lmp.py:1935]   Expert 10 |    362 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876367.876367 lmp.py:1935]   Expert 45 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876249.876249 lmp.py:1935]   Expert 21 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.876369.876369 lmp.py:1935]   Expert 42 |    646 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.876058.876058 lmp.py:1937] 
DEBUG 01-15 16:10:21.876058.876058 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.876939.876939 lmp.py:1938]   CPU:   3765 tokens
DEBUG 01-15 16:10:21.876821.876821 lmp.py:1942]   cuda:1:   4346 tokens (16 experts)
DEBUG 01-15 16:10:21.876702.876702 lmp.py:1942]   cuda:2:   4177 tokens (16 experts)
DEBUG 01-15 16:10:21.877868.877868 lmp.py:1943]   Total GPU:   8523 tokens
DEBUG 01-15 16:10:21.877273.877273 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.877273.877273 lmp.py:1944] 
DEBUG 01-15 16:10:21.877783.877783 cuda_h.py:19] end experts_map_get cost 0.001959085464477539 seconds
DEBUG 01-15 16:10:21.877938.877938 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.877317.877317 lmp.py:1953] 
DEBUG 01-15 16:10:21.877317.877317 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.877783.877783 cuda_h.py:19] end cpu_experts_submit cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:10:21.877671.877671 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.877852.877852 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.878055.878055 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.33.gate_proj.weight': 0, 'model.layers.21.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.63.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.63.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.63.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.57.up_proj.weight': 271056896}, 2: {'model.layers.21.mlp.experts.36.gate_proj.weight': 0, 'model.layers.21.mlp.experts.36.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.36.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.43.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.43.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.43.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(25577390080, 5767168, 0, 0), (25583157248, 5767168, 5767168, 0), (25571622912, 5767168, 11534336, 0), (25594691584, 5767168, 17301504, 0), (25600458752, 5767168, 23068672, 0), (25588924416, 5767168, 28835840, 0), (25058344960, 5767168, 34603008, 0), (25064112128, 5767168, 40370176, 0), (25052577792, 5767168, 46137344, 0), (25075646464, 5767168, 51904512, 0), (25081413632, 5767168, 57671680, 0), (25069879296, 5767168, 63438848, 0), (25733103616, 5767168, 69206016, 0), (25738870784, 5767168, 74973184, 0), (25727336448, 5767168, 80740352, 0), (25179455488, 5767168, 86507520, 0), (25185222656, 5767168, 92274688, 0), (25173688320, 5767168, 98041856, 0), (25214058496, 5767168, 103809024, 0), (25219825664, 5767168, 109576192, 0), (25208291328, 5767168, 115343360, 0), (25265963008, 5767168, 121110528, 0), (25271730176, 5767168, 126877696, 0), (25260195840, 5767168, 132644864, 0), (25283264512, 5767168, 138412032, 0), (25289031680, 5767168, 144179200, 0), (25277497344, 5767168, 149946368, 0), (26096435200, 5767168, 155713536, 0), (26102202368, 5767168, 161480704, 0), (26090668032, 5767168, 167247872, 0), (25335169024, 5767168, 173015040, 0), (25340936192, 5767168, 178782208, 0), (25329401856, 5767168, 184549376, 0), (25958023168, 5767168, 190316544, 0), (25963790336, 5767168, 196083712, 0), (25952256000, 5767168, 201850880, 0), (25438978048, 5767168, 207618048, 0), (25444745216, 5767168, 213385216, 0), (25433210880, 5767168, 219152384, 0), (25490882560, 5767168, 224919552, 0), (25496649728, 5767168, 230686720, 0), (25485115392, 5767168, 236453888, 0), (25508184064, 5767168, 242221056, 0), (25513951232, 5767168, 247988224, 0), (25502416896, 5767168, 253755392, 0), (25992626176, 5767168, 259522560, 0), (25998393344, 5767168, 265289728, 0), (25986859008, 5767168, 271056896, 0)], 2: [(25629294592, 5767168, 0, 0), (25635061760, 5767168, 5767168, 0), (25623527424, 5767168, 11534336, 0), (25646596096, 5767168, 17301504, 0), (25652363264, 5767168, 23068672, 0), (25640828928, 5767168, 28835840, 0), (25092947968, 5767168, 34603008, 0), (25098715136, 5767168, 40370176, 0), (25087180800, 5767168, 46137344, 0), (25698500608, 5767168, 51904512, 0), (25704267776, 5767168, 57671680, 0), (25692733440, 5767168, 63438848, 0), (25750405120, 5767168, 69206016, 0), (25756172288, 5767168, 74973184, 0), (25744637952, 5767168, 80740352, 0), (25785008128, 5767168, 86507520, 0), (25790775296, 5767168, 92274688, 0), (25779240960, 5767168, 98041856, 0), (25231360000, 5767168, 103809024, 0), (25237127168, 5767168, 109576192, 0), (25225592832, 5767168, 115343360, 0), (25300566016, 5767168, 121110528, 0), (25306333184, 5767168, 126877696, 0), (25294798848, 5767168, 132644864, 0), (25317867520, 5767168, 138412032, 0), (25323634688, 5767168, 144179200, 0), (25312100352, 5767168, 149946368, 0), (25854214144, 5767168, 155713536, 0), (25859981312, 5767168, 161480704, 0), (25848446976, 5767168, 167247872, 0), (25352470528, 5767168, 173015040, 0), (25358237696, 5767168, 178782208, 0), (25346703360, 5767168, 184549376, 0), (25369772032, 5767168, 190316544, 0), (25375539200, 5767168, 196083712, 0), (25364004864, 5767168, 201850880, 0), (26009927680, 5767168, 207618048, 0), (26015694848, 5767168, 213385216, 0), (26004160512, 5767168, 219152384, 0), (26061832192, 5767168, 224919552, 0), (26067599360, 5767168, 230686720, 0), (26056065024, 5767168, 236453888, 0), (25525485568, 5767168, 242221056, 0), (25531252736, 5767168, 247988224, 0), (25519718400, 5767168, 253755392, 0), (25542787072, 5767168, 259522560, 0), (25548554240, 5767168, 265289728, 0), (25537019904, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e5424c6c0>, 2: <capsule object NULL at 0x7a4f2c0a4c60>}
DEBUG 01-15 16:10:21.878695.878695 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.878922.878922 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.878108.878108 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8315c198-2e4a-44e9-a0dd-b5895389a127
DEBUG 01-15 16:10:21.878109.878109 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:21.879821.879821 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8315c198-2e4a-44e9-a0dd-b5895389a127
DEBUG 01-15 16:10:21.880818.880818 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.880235.880235 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:21.880708.880708 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0031936168670654297 seconds
DEBUG 01-15 16:10:21.880579.880579 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.881064.881064 cuda_h.py:19] end move_flatidxs cost 0.0009579658508300781 seconds
DEBUG 01-15 16:10:21.881953.881953 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.883314.883314 cuda_h.py:19] end restore2model cost 0.0031402111053466797 seconds
DEBUG 01-15 16:10:21.883157.883157 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006582975387573242 seconds
DEBUG 01-15 16:10:21.883337.883337 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.884860.884860 cuda_h.py:19] end gpu_sexperts cost 0.0003504753112792969 seconds
DEBUG 01-15 16:10:21.884358.884358 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.884903.884903 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:10:21.884461.884461 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.884124.884124 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.885619.885619 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010979175567626953 seconds
DEBUG 01-15 16:10:21.885469.885469 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.885940.885940 cuda_h.py:19] end gpu_group_list cost 0.00017642974853515625 seconds
DEBUG 01-15 16:10:21.886476.886476 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.888842.888842 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001148223876953125 seconds
DEBUG 01-15 16:10:21.888685.888685 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.888917.888917 cuda_h.py:19] end gpu_group_list cost 0.00017452239990234375 seconds
DEBUG 01-15 16:10:21.889765.889765 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.889555.889555 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8315c198-2e4a-44e9-a0dd-b5895389a127
DEBUG 01-15 16:10:21.893012.893012 cuda_h.py:19] end group_tensors cost 0.012181520462036133 seconds
DEBUG 01-15 16:10:21.894309.894309 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.898503.898503 cuda_h.py:19] end group pad cost 0.0043370723724365234 seconds
DEBUG 01-15 16:10:21.898637.898637 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.906132.906132 client.py:127] Model loaded
DEBUG 01-15 16:10:21.906124.906124 cuda_h.py:19] end wait_experts_multi_device cost 0.017183542251586914 seconds
DEBUG 01-15 16:10:21.906894.906894 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.921163.921163 cuda_h.py:19] end group_einsum cost 0.02214527130126953 seconds
DEBUG 01-15 16:10:21.921473.921473 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.925714.925714 cuda_h.py:19] end get_outputs_cpu1 cost 0.0037550926208496094 seconds
DEBUG 01-15 16:10:21.926678.926678 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04597592353820801 seconds
DEBUG 01-15 16:10:21.926391.926391 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.020321130752563477 seconds
DEBUG 01-15 16:10:21.926612.926612 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.927486.927486 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.928868.928868 cuda_h.py:19] end index_scatter cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:10:21.928798.928798 cuda_h.py:19] end cpuoutputsdeal cost 0.0013623237609863281 seconds
DEBUG 01-15 16:10:21.928967.928967 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.928491.928491 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.928748.928748 cuda_h.py:19] end gpu_group_tensor cost 0.0001251697540283203 seconds
DEBUG 01-15 16:10:21.928080.928080 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.928502.928502 cuda_h.py:19] end gpu_group_tensor cost 0.00010704994201660156 seconds
DEBUG 01-15 16:10:21.928201.928201 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.929916.929916 cuda_h.py:19] end gpu_group_einsum cost 0.0005490779876708984 seconds
DEBUG 01-15 16:10:21.929007.929007 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.930447.930447 cuda_h.py:19] end gpu_group_einsum cost 0.000438690185546875 seconds
DEBUG 01-15 16:10:21.930749.930749 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.930038.930038 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.930846.930846 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016498565673828125 seconds
DEBUG 01-15 16:10:21.930423.930423 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.930161.930161 cuda_h.py:19] end concat_expert_out cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:10:21.930979.930979 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.930372.930372 cuda_h.py:19] end index_scatter cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:21.931797.931797 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000789642333984375 seconds
DEBUG 01-15 16:10:21.931588.931588 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.931564.931564 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.931641.931641 cuda_h.py:19] end all_expert_outputs_slices cost 0.00010037422180175781 seconds
DEBUG 01-15 16:10:21.931821.931821 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.931353.931353 cuda_h.py:19] end concat_expert_out cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:10:21.931428.931428 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.931298.931298 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:21.931438.931438 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00041675567626953125 seconds
DEBUG 01-15 16:10:21.931374.931374 cuda_h.py:19] end gpu_experts_multi_device cost 0.04720783233642578 seconds
DEBUG 01-15 16:10:21.931992.931992 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.05757308006286621 seconds
DEBUG 01-15 16:10:21.932161.932161 cuda_h.py:19] end prefill_layer cost 0.07005119323730469 seconds
DEBUG 01-15 16:10:21.932381.932381 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:10:21.932892.932892 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.932734.932734 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:10:21.932530.932530 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:21.932994.932994 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:21.932221.932221 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 3.7670135498046875e-05 seconds
DEBUG 01-15 16:10:21.932970.932970 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 6.604194641113281e-05 seconds
DEBUG 01-15 16:10:21.932521.932521 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.932954.932954 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.932566.932566 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.932378.932378 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.932598.932598 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.932211.932211 cuda_h.py:19] end allocate_cuda_memory cost 0.0002646446228027344 seconds
DEBUG 01-15 16:10:21.933864.933864 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.933918.933918 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.933251.933251 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.933915.933915 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8bfed4bd-eddf-4a5f-9e66-08086955a7fe
DEBUG 01-15 16:10:21.933495.933495 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.933521.933521 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.934227.934227 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8bfed4bd-eddf-4a5f-9e66-08086955a7fe
DEBUG 01-15 16:10:21.935289.935289 cuda_h.py:19] end load_into_gpu_async cost 0.001992940902709961 seconds
DEBUG 01-15 16:10:21.935237.935237 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.935162.935162 cuda_h.py:19] end restore_tensors2 cost 8.749961853027344e-05 seconds
DEBUG 01-15 16:10:21.935402.935402 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002651691436767578 seconds
INFO 01-15 16:10:21.935629.935629 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8bfed4bd-eddf-4a5f-9e66-08086955a7fe
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:21.936940.936940 cuda_h.py:19] end self_attn cost 0.003131866455078125 seconds
DEBUG 01-15 16:10:21.937820.937820 cuda_h.py:19] end iln_self_attn_paln cost 0.0047528743743896484 seconds
DEBUG 01-15 16:10:21.937709.937709 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:10:21.937671.937671 cuda_h.py:10] start gate
DEBUG 01-15 16:10:21.937267.937267 cuda_h.py:19] end gate cost 0.0007455348968505859 seconds
DEBUG 01-15 16:10:21.938786.938786 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:21.938437.938437 lmp.py:1912] 
DEBUG 01-15 16:10:21.938437.938437 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:21.938445.938445 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:21.938638.938638 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:21.938771.938771 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:21.938520.938520 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:21.938316.938316 lmp.py:1917] 
DEBUG 01-15 16:10:21.938316.938316 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:21.938449.938449 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:21.938451.938451 lmp.py:1935]   Expert 25 |     13 | CPU
DEBUG 01-15 16:10:21.938253.938253 lmp.py:1935]   Expert 48 |     33 | CPU
DEBUG 01-15 16:10:21.938102.938102 lmp.py:1935]   Expert 45 |     35 | CPU
DEBUG 01-15 16:10:21.938235.938235 lmp.py:1935]   Expert  9 |     63 | CPU
DEBUG 01-15 16:10:21.938892.938892 lmp.py:1935]   Expert 54 |     81 | CPU
DEBUG 01-15 16:10:21.938740.938740 lmp.py:1935]   Expert 43 |     83 | CPU
DEBUG 01-15 16:10:21.938920.938920 lmp.py:1935]   Expert  0 |     86 | CPU
DEBUG 01-15 16:10:21.938576.938576 lmp.py:1935]   Expert 20 |     88 | CPU
DEBUG 01-15 16:10:21.938803.938803 lmp.py:1935]   Expert 47 |     88 | CPU
DEBUG 01-15 16:10:21.938267.938267 lmp.py:1935]   Expert 57 |     93 | CPU
DEBUG 01-15 16:10:21.938493.938493 lmp.py:1935]   Expert 36 |     95 | CPU
DEBUG 01-15 16:10:21.939481.939481 lmp.py:1935]   Expert  6 |     96 | CPU
DEBUG 01-15 16:10:21.939230.939230 lmp.py:1935]   Expert 15 |    104 | CPU
DEBUG 01-15 16:10:21.939456.939456 lmp.py:1935]   Expert 62 |    104 | CPU
DEBUG 01-15 16:10:21.939636.939636 lmp.py:1935]   Expert  1 |    105 | CPU
DEBUG 01-15 16:10:21.939054.939054 lmp.py:1935]   Expert 13 |    107 | CPU
DEBUG 01-15 16:10:21.939949.939949 lmp.py:1935]   Expert 50 |    107 | CPU
DEBUG 01-15 16:10:21.939652.939652 lmp.py:1935]   Expert 61 |    108 | CPU
DEBUG 01-15 16:10:21.939401.939401 lmp.py:1935]   Expert 38 |    111 | CPU
DEBUG 01-15 16:10:21.939627.939627 lmp.py:1935]   Expert 37 |    115 | CPU
DEBUG 01-15 16:10:21.939138.939138 lmp.py:1935]   Expert 14 |    119 | CPU
DEBUG 01-15 16:10:21.939364.939364 lmp.py:1935]   Expert 46 |    124 | CPU
DEBUG 01-15 16:10:21.939351.939351 lmp.py:1935]   Expert  7 |    137 | CPU
DEBUG 01-15 16:10:21.939054.939054 lmp.py:1935]   Expert 28 |    137 | CPU
DEBUG 01-15 16:10:21.939042.939042 lmp.py:1935]   Expert 52 |    140 | CPU
DEBUG 01-15 16:10:21.939983.939983 lmp.py:1935]   Expert 21 |    141 | CPU
DEBUG 01-15 16:10:21.939640.939640 lmp.py:1935]   Expert 44 |    141 | CPU
DEBUG 01-15 16:10:21.939296.939296 lmp.py:1935]   Expert 24 |    151 | CPU
DEBUG 01-15 16:10:21.939761.939761 lmp.py:1935]   Expert 10 |    153 | CPU
DEBUG 01-15 16:10:21.939510.939510 lmp.py:1935]   Expert 42 |    154 | CPU
DEBUG 01-15 16:10:21.939021.939021 lmp.py:1935]   Expert 11 |    159 | CPU
DEBUG 01-15 16:10:21.939770.939770 lmp.py:1935]   Expert  2 |    165 | CPU
DEBUG 01-15 16:10:21.939481.939481 lmp.py:1935]   Expert 35 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.939568.939568 lmp.py:1935]   Expert 26 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.939940.939940 lmp.py:1935]   Expert 31 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.939358.939358 lmp.py:1935]   Expert 19 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.939551.939551 lmp.py:1935]   Expert  3 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.939300.939300 lmp.py:1935]   Expert 32 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.939242.939242 lmp.py:1935]   Expert 12 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.939468.939468 lmp.py:1935]   Expert 56 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.939601.939601 lmp.py:1935]   Expert 60 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.939019.939019 lmp.py:1935]   Expert 40 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.939437.939437 lmp.py:1935]   Expert 41 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.939140.939140 lmp.py:1935]   Expert 53 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.939605.939605 lmp.py:1935]   Expert 16 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.939592.939592 lmp.py:1935]   Expert 23 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.939818.939818 lmp.py:1935]   Expert  8 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.940283.940283 lmp.py:1935]   Expert 58 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.940701.940701 lmp.py:1935]   Expert 51 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.940357.940357 lmp.py:1935]   Expert 59 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.940776.940776 lmp.py:1935]   Expert  4 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.940240.940240 lmp.py:1935]   Expert 55 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.940228.940228 lmp.py:1935]   Expert 49 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.940215.940215 lmp.py:1935]   Expert 29 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.940965.940965 lmp.py:1935]   Expert 18 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.940952.940952 lmp.py:1935]   Expert 34 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.940701.940701 lmp.py:1935]   Expert 63 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.940596.940596 lmp.py:1935]   Expert 27 |    357 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.940014.940014 lmp.py:1935]   Expert 39 |    378 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.940671.940671 lmp.py:1935]   Expert 17 |    395 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.940897.940897 lmp.py:1935]   Expert 22 |    428 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.940634.940634 lmp.py:1935]   Expert 33 |    447 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.940337.940337 lmp.py:1935]   Expert 30 |    458 | GPU2(cuda:2)
DEBUG 01-15 16:10:21.940278.940278 lmp.py:1935]   Expert  5 |    710 | GPU1(cuda:1)
DEBUG 01-15 16:10:21.940266.940266 lmp.py:1937] 
DEBUG 01-15 16:10:21.940266.940266 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:21.940207.940207 lmp.py:1938]   CPU:   3436 tokens
DEBUG 01-15 16:10:21.940910.940910 lmp.py:1942]   cuda:1:   4466 tokens (16 experts)
DEBUG 01-15 16:10:21.940328.940328 lmp.py:1942]   cuda:2:   4386 tokens (16 experts)
DEBUG 01-15 16:10:21.940031.940031 lmp.py:1943]   Total GPU:   8852 tokens
DEBUG 01-15 16:10:21.940972.940972 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:21.940972.940972 lmp.py:1944] 
DEBUG 01-15 16:10:21.940828.940828 cuda_h.py:19] end experts_map_get cost 0.0025892257690429688 seconds
DEBUG 01-15 16:10:21.940751.940751 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:21.940805.940805 lmp.py:1953] 
DEBUG 01-15 16:10:21.940805.940805 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:21.940470.940470 cuda_h.py:19] end cpu_experts_submit cost 6.771087646484375e-05 seconds
DEBUG 01-15 16:10:21.940987.940987 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:21.940659.940659 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:21.941636.941636 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.34.gate_proj.weight': 0, 'model.layers.22.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.35.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.35.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.35.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.39.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.39.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.39.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.41.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.41.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.41.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.12.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.12.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.12.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.49.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.49.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.49.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.56.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.56.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.56.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.31.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.31.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.31.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.22.mlp.experts.32.gate_proj.weight': 0, 'model.layers.22.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.26.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.26.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.26.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.59.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.59.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.59.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(26701987840, 5767168, 0, 0), (26707755008, 5767168, 5767168, 0), (26696220672, 5767168, 11534336, 0), (26165641216, 5767168, 17301504, 0), (26171408384, 5767168, 23068672, 0), (26159874048, 5767168, 28835840, 0), (26182942720, 5767168, 34603008, 0), (26188709888, 5767168, 40370176, 0), (26177175552, 5767168, 46137344, 0), (26200244224, 5767168, 51904512, 0), (26206011392, 5767168, 57671680, 0), (26194477056, 5767168, 63438848, 0), (26719289344, 5767168, 69206016, 0), (26725056512, 5767168, 74973184, 0), (26713522176, 5767168, 80740352, 0), (26788495360, 5767168, 86507520, 0), (26794262528, 5767168, 92274688, 0), (26782728192, 5767168, 98041856, 0), (26252148736, 5767168, 103809024, 0), (26257915904, 5767168, 109576192, 0), (26246381568, 5767168, 115343360, 0), (26823098368, 5767168, 121110528, 0), (26828865536, 5767168, 126877696, 0), (26817331200, 5767168, 132644864, 0), (26321354752, 5767168, 138412032, 0), (26327121920, 5767168, 144179200, 0), (26315587584, 5767168, 149946368, 0), (26390560768, 5767168, 155713536, 0), (26396327936, 5767168, 161480704, 0), (26384793600, 5767168, 167247872, 0), (26961510400, 5767168, 173015040, 0), (26967277568, 5767168, 178782208, 0), (26955743232, 5767168, 184549376, 0), (26996113408, 5767168, 190316544, 0), (27001880576, 5767168, 196083712, 0), (26990346240, 5767168, 201850880, 0), (26494369792, 5767168, 207618048, 0), (26500136960, 5767168, 213385216, 0), (26488602624, 5767168, 219152384, 0), (27082620928, 5767168, 224919552, 0), (27088388096, 5767168, 230686720, 0), (27076853760, 5767168, 236453888, 0), (26650083328, 5767168, 242221056, 0), (26655850496, 5767168, 247988224, 0), (26644316160, 5767168, 253755392, 0), (27203731456, 5767168, 259522560, 0), (27209498624, 5767168, 265289728, 0), (27197964288, 5767168, 271056896, 0)], 2: [(26667384832, 5767168, 0, 0), (26673152000, 5767168, 5767168, 0), (26661617664, 5767168, 11534336, 0), (26684686336, 5767168, 17301504, 0), (26690453504, 5767168, 23068672, 0), (26678919168, 5767168, 28835840, 0), (26563575808, 5767168, 34603008, 0), (26569342976, 5767168, 40370176, 0), (26557808640, 5767168, 46137344, 0), (26805796864, 5767168, 51904512, 0), (26811564032, 5767168, 57671680, 0), (26800029696, 5767168, 63438848, 0), (27134525440, 5767168, 69206016, 0), (27140292608, 5767168, 74973184, 0), (27128758272, 5767168, 80740352, 0), (26407862272, 5767168, 86507520, 0), (26413629440, 5767168, 92274688, 0), (26402095104, 5767168, 98041856, 0), (26425163776, 5767168, 103809024, 0), (26430930944, 5767168, 109576192, 0), (26419396608, 5767168, 115343360, 0), (26442465280, 5767168, 121110528, 0), (26448232448, 5767168, 126877696, 0), (26436698112, 5767168, 132644864, 0), (26511671296, 5767168, 138412032, 0), (26517438464, 5767168, 144179200, 0), (26505904128, 5767168, 149946368, 0), (27030716416, 5767168, 155713536, 0), (27036483584, 5767168, 161480704, 0), (27024949248, 5767168, 167247872, 0), (27065319424, 5767168, 173015040, 0), (27071086592, 5767168, 178782208, 0), (27059552256, 5767168, 184549376, 0), (27117223936, 5767168, 190316544, 0), (27122991104, 5767168, 196083712, 0), (27111456768, 5767168, 201850880, 0), (26580877312, 5767168, 207618048, 0), (26586644480, 5767168, 213385216, 0), (26575110144, 5767168, 219152384, 0), (27151826944, 5767168, 224919552, 0), (27157594112, 5767168, 230686720, 0), (27146059776, 5767168, 236453888, 0), (26615480320, 5767168, 242221056, 0), (26621247488, 5767168, 247988224, 0), (26609713152, 5767168, 253755392, 0), (26632781824, 5767168, 259522560, 0), (26638548992, 5767168, 265289728, 0), (26627014656, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e5418c6c0>, 2: <capsule object NULL at 0x7a4e34219c80>}
DEBUG 01-15 16:10:21.942298.942298 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.942617.942617 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.942705.942705 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f1de957e-71d7-4d36-8a64-322738e37718
DEBUG 01-15 16:10:21.942518.942518 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:21.942405.942405 client.py:127] Model loaded
DEBUG 01-15 16:10:21.942473.942473 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.942561.942561 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:21.943565.943565 cuda_h.py:19] end restore2model cost 0.0003523826599121094 seconds
DEBUG 01-15 16:10:21.943381.943381 cuda_h.py:19] end sllm_worker_task cost 0.010583639144897461 seconds
DEBUG 01-15 16:10:21.943837.943837 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:21.943317.943317 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f1de957e-71d7-4d36-8a64-322738e37718
DEBUG 01-15 16:10:21.943708.943708 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002804279327392578 seconds
DEBUG 01-15 16:10:21.943849.943849 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:21.944769.944769 cuda_h.py:19] end move_flatidxs cost 0.0008513927459716797 seconds
DEBUG 01-15 16:10:21.944499.944499 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:21.946276.946276 cuda_h.py:19] end restore2model cost 0.002631664276123047 seconds
DEBUG 01-15 16:10:21.946642.946642 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005684375762939453 seconds
DEBUG 01-15 16:10:21.946392.946392 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:21.946713.946713 cuda_h.py:19] end gpu_sexperts cost 0.000274658203125 seconds
DEBUG 01-15 16:10:21.946496.946496 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:21.947988.947988 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:10:21.947399.947399 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:21.947817.947817 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.947912.947912 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008108615875244141 seconds
DEBUG 01-15 16:10:21.947324.947324 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.948748.948748 cuda_h.py:19] end gpu_group_list cost 0.00017213821411132812 seconds
DEBUG 01-15 16:10:21.948714.948714 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:21.949711.949711 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008494853973388672 seconds
DEBUG 01-15 16:10:21.949885.949885 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:21.950256.950256 cuda_h.py:19] end gpu_group_list cost 0.00017261505126953125 seconds
DEBUG 01-15 16:10:21.950859.950859 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:21.950596.950596 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f1de957e-71d7-4d36-8a64-322738e37718
DEBUG 01-15 16:10:21.953829.953829 cuda_h.py:19] end group_tensors cost 0.009229898452758789 seconds
DEBUG 01-15 16:10:21.954186.954186 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:21.958866.958866 cuda_h.py:19] end group pad cost 0.004031181335449219 seconds
DEBUG 01-15 16:10:21.958609.958609 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:21.970376.970376 client.py:127] Model loaded
DEBUG 01-15 16:10:21.970430.970430 cuda_h.py:19] end wait_experts_multi_device cost 0.020138263702392578 seconds
DEBUG 01-15 16:10:21.970611.970611 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:21.983771.983771 cuda_h.py:19] end group_einsum cost 0.024929046630859375 seconds
DEBUG 01-15 16:10:21.983512.983512 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:21.986007.986007 cuda_h.py:19] end get_outputs_cpu1 cost 0.003448009490966797 seconds
DEBUG 01-15 16:10:21.987362.987362 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.045006513595581055 seconds
DEBUG 01-15 16:10:21.988164.988164 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.017432689666748047 seconds
DEBUG 01-15 16:10:21.988452.988452 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:21.990708.990708 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.990740.990740 cuda_h.py:19] end index_scatter cost 8.678436279296875e-05 seconds
DEBUG 01-15 16:10:21.990373.990373 cuda_h.py:19] end cpuoutputsdeal cost 0.0019345283508300781 seconds
DEBUG 01-15 16:10:21.990005.990005 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:21.990960.990960 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.990907.990907 cuda_h.py:19] end gpu_group_tensor cost 0.00017118453979492188 seconds
DEBUG 01-15 16:10:21.990339.990339 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:21.991822.991822 cuda_h.py:19] end gpu_group_tensor cost 0.00014829635620117188 seconds
DEBUG 01-15 16:10:21.991786.991786 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.991604.991604 cuda_h.py:19] end gpu_group_einsum cost 0.0006561279296875 seconds
DEBUG 01-15 16:10:21.992808.992808 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:21.992353.992353 cuda_h.py:19] end gpu_group_einsum cost 0.0005831718444824219 seconds
DEBUG 01-15 16:10:21.992119.992119 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.993772.993772 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.993639.993639 cuda_h.py:19] end all_expert_outputs_slices cost 0.00030994415283203125 seconds
DEBUG 01-15 16:10:21.993064.993064 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.993802.993802 cuda_h.py:19] end concat_expert_out cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:21.993128.993128 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.993165.993165 cuda_h.py:19] end index_scatter cost 6.103515625e-05 seconds
DEBUG 01-15 16:10:21.993061.993061 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009872913360595703 seconds
DEBUG 01-15 16:10:21.994489.994489 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:21.994200.994200 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:21.994373.994373 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019931793212890625 seconds
DEBUG 01-15 16:10:21.994036.994036 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:21.994165.994165 cuda_h.py:19] end concat_expert_out cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:10:21.994254.994254 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:21.994144.994144 cuda_h.py:19] end index_scatter cost 5.936622619628906e-05 seconds
DEBUG 01-15 16:10:21.994622.994622 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005815029144287109 seconds
DEBUG 01-15 16:10:21.994254.994254 cuda_h.py:19] end gpu_experts_multi_device cost 0.047657012939453125 seconds
DEBUG 01-15 16:10:21.994939.994939 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.057645559310913086 seconds
DEBUG 01-15 16:10:21.995742.995742 cuda_h.py:19] end prefill_layer cost 0.06313300132751465 seconds
DEBUG 01-15 16:10:21.995380.995380 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:10:21.995228.995228 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:21.995984.995984 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:10:21.995979.995979 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:21.995880.995880 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:21.995313.995313 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 4.315376281738281e-05 seconds
DEBUG 01-15 16:10:21.995076.995076 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 8.058547973632812e-05 seconds
DEBUG 01-15 16:10:21.995209.995209 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:21.995305.995305 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:21.995434.995434 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:21.995608.995608 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:21.996386.996386 cuda_h.py:19] end allocate_cuda_memory cost 0.00025272369384765625 seconds
DEBUG 01-15 16:10:21.996019.996019 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:21.996431.996431 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:21.996131.996131 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:21.996650.996650 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:21.996697.996697 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 48a720d5-ef7b-47e3-bb33-f44127199ee4
DEBUG 01-15 16:10:21.996085.996085 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:21.996613.996613 cuda_h.py:10] start self_attn
INFO 01-15 16:10:21.998900.998900 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 48a720d5-ef7b-47e3-bb33-f44127199ee4
DEBUG 01-15 16:10:21.998425.998425 cuda_h.py:19] end load_into_gpu_async cost 0.0018379688262939453 seconds
DEBUG 01-15 16:10:21.998782.998782 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:21.998277.998277 cuda_h.py:19] end restore_tensors2 cost 0.00020933151245117188 seconds
DEBUG 01-15 16:10:21.998356.998356 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0031545162200927734 seconds
INFO 01-15 16:10:21.999098.999098 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 48a720d5-ef7b-47e3-bb33-f44127199ee4
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:22.002342.002342 cuda_h.py:19] end self_attn cost 0.005064487457275391 seconds
DEBUG 01-15 16:10:22.002380.002380 cuda_h.py:19] end iln_self_attn_paln cost 0.006829500198364258 seconds
DEBUG 01-15 16:10:22.002971.002971 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:10:22.002403.002403 cuda_h.py:10] start gate
DEBUG 01-15 16:10:22.003937.003937 cuda_h.py:19] end gate cost 0.0007071495056152344 seconds
DEBUG 01-15 16:10:22.003390.003390 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:22.003048.003048 lmp.py:1912] 
DEBUG 01-15 16:10:22.003048.003048 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:22.003301.003301 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:22.003766.003766 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:22.003939.003939 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:22.003204.003204 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:22.003278.003278 lmp.py:1917] 
DEBUG 01-15 16:10:22.003278.003278 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:22.003590.003590 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:22.004624.004624 lmp.py:1935]   Expert  5 |     13 | CPU
DEBUG 01-15 16:10:22.004174.004174 lmp.py:1935]   Expert 56 |     31 | CPU
DEBUG 01-15 16:10:22.004532.004532 lmp.py:1935]   Expert 27 |     83 | CPU
DEBUG 01-15 16:10:22.004891.004891 lmp.py:1935]   Expert 16 |     86 | CPU
DEBUG 01-15 16:10:22.004249.004249 lmp.py:1935]   Expert 17 |     90 | CPU
DEBUG 01-15 16:10:22.004369.004369 lmp.py:1935]   Expert 40 |     95 | CPU
DEBUG 01-15 16:10:22.004488.004488 lmp.py:1935]   Expert 51 |    105 | CPU
DEBUG 01-15 16:10:22.004370.004370 lmp.py:1935]   Expert 63 |    105 | CPU
DEBUG 01-15 16:10:22.004682.004682 lmp.py:1935]   Expert 53 |    106 | CPU
DEBUG 01-15 16:10:22.004517.004517 lmp.py:1935]   Expert 49 |    107 | CPU
DEBUG 01-15 16:10:22.004590.004590 lmp.py:1935]   Expert 28 |    109 | CPU
DEBUG 01-15 16:10:22.004664.004664 lmp.py:1935]   Expert  7 |    116 | CPU
DEBUG 01-15 16:10:22.004260.004260 lmp.py:1935]   Expert 38 |    118 | CPU
DEBUG 01-15 16:10:22.004903.004903 lmp.py:1935]   Expert 37 |    124 | CPU
DEBUG 01-15 16:10:22.004785.004785 lmp.py:1935]   Expert 47 |    124 | CPU
DEBUG 01-15 16:10:22.004905.004905 lmp.py:1935]   Expert 62 |    125 | CPU
DEBUG 01-15 16:10:22.004786.004786 lmp.py:1935]   Expert 11 |    130 | CPU
DEBUG 01-15 16:10:22.004906.004906 lmp.py:1935]   Expert 58 |    130 | CPU
DEBUG 01-15 16:10:22.004026.004026 lmp.py:1935]   Expert 57 |    136 | CPU
DEBUG 01-15 16:10:22.004907.004907 lmp.py:1935]   Expert  1 |    146 | CPU
DEBUG 01-15 16:10:22.004742.004742 lmp.py:1935]   Expert 39 |    146 | CPU
DEBUG 01-15 16:10:22.004875.004875 lmp.py:1935]   Expert 14 |    149 | CPU
DEBUG 01-15 16:10:22.004187.004187 lmp.py:1935]   Expert 52 |    155 | CPU
DEBUG 01-15 16:10:22.004261.004261 lmp.py:1935]   Expert 25 |    156 | CPU
DEBUG 01-15 16:10:22.004096.004096 lmp.py:1935]   Expert 23 |    160 | CPU
DEBUG 01-15 16:10:22.004216.004216 lmp.py:1935]   Expert 21 |    163 | CPU
DEBUG 01-15 16:10:22.004859.004859 lmp.py:1935]   Expert 33 |    163 | CPU
DEBUG 01-15 16:10:22.004502.004502 lmp.py:1935]   Expert  6 |    170 | CPU
DEBUG 01-15 16:10:22.004145.004145 lmp.py:1935]   Expert 60 |    171 | CPU
DEBUG 01-15 16:10:22.004026.004026 lmp.py:1935]   Expert 45 |    176 | CPU
DEBUG 01-15 16:10:22.004669.004669 lmp.py:1935]   Expert 19 |    178 | CPU
DEBUG 01-15 16:10:22.004550.004550 lmp.py:1935]   Expert 44 |    179 | CPU
DEBUG 01-15 16:10:22.004339.004339 lmp.py:1935]   Expert 12 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004320.004320 lmp.py:1935]   Expert  4 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004586.004586 lmp.py:1935]   Expert  3 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004090.004090 lmp.py:1935]   Expert 30 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004594.004594 lmp.py:1935]   Expert 31 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004905.004905 lmp.py:1935]   Expert 55 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004264.004264 lmp.py:1935]   Expert 36 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004099.004099 lmp.py:1935]   Expert  9 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004411.004411 lmp.py:1935]   Expert  0 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004246.004246 lmp.py:1935]   Expert 34 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004842.004842 lmp.py:1935]   Expert 22 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004439.004439 lmp.py:1935]   Expert 41 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004797.004797 lmp.py:1935]   Expert 26 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004394.004394 lmp.py:1935]   Expert 54 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004852.004852 lmp.py:1935]   Expert 43 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004117.004117 lmp.py:1935]   Expert 59 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004667.004667 lmp.py:1935]   Expert 13 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004695.004695 lmp.py:1935]   Expert 50 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004483.004483 lmp.py:1935]   Expert 18 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.004795.004795 lmp.py:1935]   Expert 20 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.004822.004822 lmp.py:1935]   Expert 15 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.005850.005850 lmp.py:1935]   Expert 24 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.005161.005161 lmp.py:1935]   Expert 42 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.005427.005427 lmp.py:1935]   Expert 29 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.005693.005693 lmp.py:1935]   Expert 61 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.005958.005958 lmp.py:1935]   Expert 35 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.005701.005701 lmp.py:1935]   Expert 32 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.005489.005489 lmp.py:1935]   Expert  2 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.005040.005040 lmp.py:1935]   Expert  8 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.005828.005828 lmp.py:1935]   Expert 10 |    342 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.005902.005902 lmp.py:1935]   Expert 46 |    424 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.005452.005452 lmp.py:1935]   Expert 48 |    450 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.005334.005334 lmp.py:1937] 
DEBUG 01-15 16:10:22.005334.005334 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:22.005937.005937 lmp.py:1938]   CPU:   4045 tokens
DEBUG 01-15 16:10:22.005964.005964 lmp.py:1942]   cuda:1:   4121 tokens (16 experts)
DEBUG 01-15 16:10:22.005799.005799 lmp.py:1942]   cuda:2:   4122 tokens (16 experts)
DEBUG 01-15 16:10:22.005681.005681 lmp.py:1943]   Total GPU:   8243 tokens
DEBUG 01-15 16:10:22.005085.005085 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:22.005085.005085 lmp.py:1944] 
DEBUG 01-15 16:10:22.005642.005642 cuda_h.py:19] end experts_map_get cost 0.001981496810913086 seconds
DEBUG 01-15 16:10:22.005168.005168 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:22.005162.005162 lmp.py:1953] 
DEBUG 01-15 16:10:22.005162.005162 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:22.005568.005568 cuda_h.py:19] end cpu_experts_submit cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:10:22.005456.005456 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:22.005670.005670 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:22.006641.006641 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.32.gate_proj.weight': 0, 'model.layers.23.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.0.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.0.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.0.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.3.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.3.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.3.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.43.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.43.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.43.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.9.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.9.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.9.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.12.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.12.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.12.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.48.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.48.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.48.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.50.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.50.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.50.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.26.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.26.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.26.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.23.mlp.experts.2.gate_proj.weight': 0, 'model.layers.23.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(27774681088, 5767168, 0, 0), (27780448256, 5767168, 5767168, 0), (27768913920, 5767168, 11534336, 0), (27221032960, 5767168, 17301504, 0), (27226800128, 5767168, 23068672, 0), (27215265792, 5767168, 28835840, 0), (27826585600, 5767168, 34603008, 0), (27832352768, 5767168, 40370176, 0), (27820818432, 5767168, 46137344, 0), (27272937472, 5767168, 51904512, 0), (27278704640, 5767168, 57671680, 0), (27267170304, 5767168, 63438848, 0), (27359444992, 5767168, 69206016, 0), (27365212160, 5767168, 74973184, 0), (27353677824, 5767168, 80740352, 0), (27930394624, 5767168, 86507520, 0), (27936161792, 5767168, 92274688, 0), (27924627456, 5767168, 98041856, 0), (27947696128, 5767168, 103809024, 0), (27953463296, 5767168, 109576192, 0), (27941928960, 5767168, 115343360, 0), (27964997632, 5767168, 121110528, 0), (27970764800, 5767168, 126877696, 0), (27959230464, 5767168, 132644864, 0), (27376746496, 5767168, 138412032, 0), (27382513664, 5767168, 144179200, 0), (27370979328, 5767168, 149946368, 0), (27428651008, 5767168, 155713536, 0), (27434418176, 5767168, 161480704, 0), (27422883840, 5767168, 167247872, 0), (28051505152, 5767168, 173015040, 0), (28057272320, 5767168, 178782208, 0), (28045737984, 5767168, 184549376, 0), (27532460032, 5767168, 190316544, 0), (27538227200, 5767168, 196083712, 0), (27526692864, 5767168, 201850880, 0), (28086108160, 5767168, 207618048, 0), (28091875328, 5767168, 213385216, 0), (28080340992, 5767168, 219152384, 0), (27636269056, 5767168, 224919552, 0), (27642036224, 5767168, 230686720, 0), (27630501888, 5767168, 236453888, 0), (27670872064, 5767168, 242221056, 0), (27676639232, 5767168, 247988224, 0), (27665104896, 5767168, 253755392, 0), (27757379584, 5767168, 259522560, 0), (27763146752, 5767168, 265289728, 0), (27751612416, 5767168, 271056896, 0)], 2: [(27255635968, 5767168, 0, 0), (27261403136, 5767168, 5767168, 0), (27249868800, 5767168, 11534336, 0), (27809284096, 5767168, 17301504, 0), (27815051264, 5767168, 23068672, 0), (27803516928, 5767168, 28835840, 0), (27843887104, 5767168, 34603008, 0), (27849654272, 5767168, 40370176, 0), (27838119936, 5767168, 46137344, 0), (27290238976, 5767168, 51904512, 0), (27296006144, 5767168, 57671680, 0), (27284471808, 5767168, 63438848, 0), (27394048000, 5767168, 69206016, 0), (27399815168, 5767168, 74973184, 0), (27388280832, 5767168, 80740352, 0), (27445952512, 5767168, 86507520, 0), (27451719680, 5767168, 92274688, 0), (27440185344, 5767168, 98041856, 0), (28016902144, 5767168, 103809024, 0), (28022669312, 5767168, 109576192, 0), (28011134976, 5767168, 115343360, 0), (27480555520, 5767168, 121110528, 0), (27486322688, 5767168, 126877696, 0), (27474788352, 5767168, 132644864, 0), (27722776576, 5767168, 138412032, 0), (27728543744, 5767168, 144179200, 0), (27717009408, 5767168, 149946368, 0), (27567063040, 5767168, 155713536, 0), (27572830208, 5767168, 161480704, 0), (27561295872, 5767168, 167247872, 0), (28172615680, 5767168, 173015040, 0), (28178382848, 5767168, 178782208, 0), (28166848512, 5767168, 184549376, 0), (28155314176, 5767168, 190316544, 0), (28161081344, 5767168, 196083712, 0), (28149547008, 5767168, 201850880, 0), (27601666048, 5767168, 207618048, 0), (27607433216, 5767168, 213385216, 0), (27595898880, 5767168, 219152384, 0), (28241821696, 5767168, 224919552, 0), (28247588864, 5767168, 230686720, 0), (28236054528, 5767168, 236453888, 0), (28276424704, 5767168, 242221056, 0), (28282191872, 5767168, 247988224, 0), (28270657536, 5767168, 253755392, 0), (27740078080, 5767168, 259522560, 0), (27745845248, 5767168, 265289728, 0), (27734310912, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e543046c0>, 2: <capsule object NULL at 0x7a4e34219ef0>}
DEBUG 01-15 16:10:22.006574.006574 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:22.006940.006940 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:22.006219.006219 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c3b8ecde-8ccd-4f13-bff3-cb444494dc7b
DEBUG 01-15 16:10:22.006033.006033 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:22.007685.007685 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:22.007722.007722 client.py:127] Model loaded
DEBUG 01-15 16:10:22.007911.007911 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:22.007781.007781 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:22.007134.007134 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c3b8ecde-8ccd-4f13-bff3-cb444494dc7b
DEBUG 01-15 16:10:22.008765.008765 cuda_h.py:19] end move_flatidxs cost 0.0008440017700195312 seconds
DEBUG 01-15 16:10:22.008733.008733 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:22.008699.008699 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0026209354400634766 seconds
DEBUG 01-15 16:10:22.008331.008331 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:22.009298.009298 cuda_h.py:19] end restore2model cost 0.0007419586181640625 seconds
DEBUG 01-15 16:10:22.009156.009156 cuda_h.py:19] end sllm_worker_task cost 0.013461112976074219 seconds
DEBUG 01-15 16:10:22.012297.012297 cuda_h.py:19] end restore2model cost 0.0036885738372802734 seconds
DEBUG 01-15 16:10:22.012114.012114 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0065686702728271484 seconds
DEBUG 01-15 16:10:22.012532.012532 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:22.012544.012544 cuda_h.py:19] end gpu_sexperts cost 0.0003249645233154297 seconds
DEBUG 01-15 16:10:22.012235.012235 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:22.012588.012588 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7404556274414062e-05 seconds
DEBUG 01-15 16:10:22.012429.012429 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:22.012424.012424 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.013533.013533 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010645389556884766 seconds
DEBUG 01-15 16:10:22.013999.013999 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.014178.014178 cuda_h.py:19] end gpu_group_list cost 0.0001723766326904297 seconds
DEBUG 01-15 16:10:22.014614.014614 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.016304.016304 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011475086212158203 seconds
DEBUG 01-15 16:10:22.016630.016630 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.016915.016915 cuda_h.py:19] end gpu_group_list cost 0.00017762184143066406 seconds
DEBUG 01-15 16:10:22.017306.017306 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:22.017427.017427 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c3b8ecde-8ccd-4f13-bff3-cb444494dc7b
DEBUG 01-15 16:10:22.016913.016913 cuda_h.py:19] end group_tensors cost 0.008484840393066406 seconds
DEBUG 01-15 16:10:22.017182.017182 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:22.022434.022434 cuda_h.py:19] end group pad cost 0.005293607711791992 seconds
DEBUG 01-15 16:10:22.022277.022277 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:22.034848.034848 client.py:127] Model loaded
DEBUG 01-15 16:10:22.034511.034511 cuda_h.py:19] end wait_experts_multi_device cost 0.016920804977416992 seconds
DEBUG 01-15 16:10:22.034062.034062 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:22.044027.044027 cuda_h.py:19] end group_einsum cost 0.02170848846435547 seconds
DEBUG 01-15 16:10:22.044621.044621 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:22.048918.048918 cuda_h.py:19] end get_outputs_cpu1 cost 0.003829479217529297 seconds
DEBUG 01-15 16:10:22.049403.049403 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04231834411621094 seconds
DEBUG 01-15 16:10:22.049432.049432 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.015507698059082031 seconds
DEBUG 01-15 16:10:22.050058.050058 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:22.051348.051348 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.051566.051566 cuda_h.py:19] end index_scatter cost 8.654594421386719e-05 seconds
DEBUG 01-15 16:10:22.052106.052106 cuda_h.py:19] end cpuoutputsdeal cost 0.0019919872283935547 seconds
DEBUG 01-15 16:10:22.052977.052977 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:22.052839.052839 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.052779.052779 cuda_h.py:19] end gpu_group_tensor cost 0.00016617774963378906 seconds
DEBUG 01-15 16:10:22.052782.052782 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.052616.052616 cuda_h.py:19] end gpu_group_tensor cost 0.00015783309936523438 seconds
DEBUG 01-15 16:10:22.052580.052580 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.053220.053220 cuda_h.py:19] end gpu_group_einsum cost 0.0006632804870605469 seconds
DEBUG 01-15 16:10:22.053000.053000 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.054412.054412 cuda_h.py:19] end gpu_group_einsum cost 0.0005545616149902344 seconds
DEBUG 01-15 16:10:22.054794.054794 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.054639.054639 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.054698.054698 cuda_h.py:19] end all_expert_outputs_slices cost 0.00030994415283203125 seconds
DEBUG 01-15 16:10:22.054553.054553 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.055788.055788 cuda_h.py:19] end concat_expert_out cost 6.747245788574219e-05 seconds
DEBUG 01-15 16:10:22.055830.055830 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.055774.055774 cuda_h.py:19] end index_scatter cost 6.270408630371094e-05 seconds
DEBUG 01-15 16:10:22.055518.055518 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010027885437011719 seconds
DEBUG 01-15 16:10:22.055561.055561 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.055027.055027 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.055399.055399 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020360946655273438 seconds
DEBUG 01-15 16:10:22.055586.055586 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.055205.055205 cuda_h.py:19] end concat_expert_out cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:22.056101.056101 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.056197.056197 cuda_h.py:19] end index_scatter cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:10:22.056391.056391 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006082057952880859 seconds
DEBUG 01-15 16:10:22.056307.056307 cuda_h.py:19] end gpu_experts_multi_device cost 0.04359841346740723 seconds
DEBUG 01-15 16:10:22.056754.056754 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.05381059646606445 seconds
DEBUG 01-15 16:10:22.056366.056366 cuda_h.py:19] end prefill_layer cost 0.06144261360168457 seconds
DEBUG 01-15 16:10:22.056295.056295 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:10:22.056097.056097 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:22.056330.056330 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:10:22.056086.056086 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:22.057942.057942 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:22.057613.057613 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 4.2438507080078125e-05 seconds
DEBUG 01-15 16:10:22.057376.057376 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 7.987022399902344e-05 seconds
DEBUG 01-15 16:10:22.057556.057556 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:22.057386.057386 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:22.057502.057502 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:22.057717.057717 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:22.057791.057791 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:22.057708.057708 cuda_h.py:19] end allocate_cuda_memory cost 0.00025200843811035156 seconds
DEBUG 01-15 16:10:22.057439.057439 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:22.057779.057779 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:22.057582.057582 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:22.057053.057053 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, aa5d769c-8814-429e-a447-fc9a0abc9011
DEBUG 01-15 16:10:22.058534.058534 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:22.058531.058531 cuda_h.py:10] start self_attn
INFO 01-15 16:10:22.059814.059814 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, aa5d769c-8814-429e-a447-fc9a0abc9011
DEBUG 01-15 16:10:22.059611.059611 cuda_h.py:19] end load_into_gpu_async cost 0.0015261173248291016 seconds
DEBUG 01-15 16:10:22.059175.059175 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:22.059384.059384 cuda_h.py:19] end restore_tensors2 cost 8.7738037109375e-05 seconds
DEBUG 01-15 16:10:22.059889.059889 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002167224884033203 seconds
INFO 01-15 16:10:22.059196.059196 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, aa5d769c-8814-429e-a447-fc9a0abc9011
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:22.062468.062468 cuda_h.py:19] end self_attn cost 0.004366397857666016 seconds
DEBUG 01-15 16:10:22.063897.063897 cuda_h.py:19] end iln_self_attn_paln cost 0.006008625030517578 seconds
DEBUG 01-15 16:10:22.063680.063680 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:10:22.063728.063728 cuda_h.py:10] start gate
DEBUG 01-15 16:10:22.064562.064562 cuda_h.py:19] end gate cost 0.0007529258728027344 seconds
DEBUG 01-15 16:10:22.064729.064729 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:22.064355.064355 lmp.py:1912] 
DEBUG 01-15 16:10:22.064355.064355 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:22.064688.064688 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:22.064960.064960 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:22.064179.064179 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:22.064730.064730 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:22.064088.064088 lmp.py:1917] 
DEBUG 01-15 16:10:22.064088.064088 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:22.064638.064638 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:22.064911.064911 lmp.py:1935]   Expert 36 |     20 | CPU
DEBUG 01-15 16:10:22.064746.064746 lmp.py:1935]   Expert 35 |     30 | CPU
DEBUG 01-15 16:10:22.064865.064865 lmp.py:1935]   Expert 25 |     47 | CPU
DEBUG 01-15 16:10:22.064508.064508 lmp.py:1935]   Expert 46 |     47 | CPU
DEBUG 01-15 16:10:22.064390.064390 lmp.py:1935]   Expert 51 |     53 | CPU
DEBUG 01-15 16:10:22.064794.064794 lmp.py:1935]   Expert 30 |     61 | CPU
DEBUG 01-15 16:10:22.064437.064437 lmp.py:1935]   Expert 16 |     62 | CPU
DEBUG 01-15 16:10:22.064319.064319 lmp.py:1935]   Expert  0 |     64 | CPU
DEBUG 01-15 16:10:22.064915.064915 lmp.py:1935]   Expert 43 |     70 | CPU
DEBUG 01-15 16:10:22.064035.064035 lmp.py:1935]   Expert 47 |     71 | CPU
DEBUG 01-15 16:10:22.064393.064393 lmp.py:1935]   Expert 55 |     73 | CPU
DEBUG 01-15 16:10:22.064513.064513 lmp.py:1935]   Expert 39 |     74 | CPU
DEBUG 01-15 16:10:22.065633.065633 lmp.py:1935]   Expert 42 |     74 | CPU
DEBUG 01-15 16:10:22.065276.065276 lmp.py:1935]   Expert 44 |     75 | CPU
DEBUG 01-15 16:10:22.065919.065919 lmp.py:1935]   Expert  2 |     82 | CPU
DEBUG 01-15 16:10:22.065562.065562 lmp.py:1935]   Expert  4 |    106 | CPU
DEBUG 01-15 16:10:22.065205.065205 lmp.py:1935]   Expert 33 |    119 | CPU
DEBUG 01-15 16:10:22.065609.065609 lmp.py:1935]   Expert  6 |    120 | CPU
DEBUG 01-15 16:10:22.065590.065590 lmp.py:1935]   Expert 48 |    120 | CPU
DEBUG 01-15 16:10:22.065141.065141 lmp.py:1935]   Expert 13 |    123 | CPU
DEBUG 01-15 16:10:22.065453.065453 lmp.py:1935]   Expert 24 |    123 | CPU
DEBUG 01-15 16:10:22.065003.065003 lmp.py:1935]   Expert 61 |    125 | CPU
DEBUG 01-15 16:10:22.065315.065315 lmp.py:1935]   Expert 56 |    131 | CPU
DEBUG 01-15 16:10:22.065150.065150 lmp.py:1935]   Expert 29 |    132 | CPU
DEBUG 01-15 16:10:22.065508.065508 lmp.py:1935]   Expert 15 |    134 | CPU
DEBUG 01-15 16:10:22.065105.065105 lmp.py:1935]   Expert 54 |    140 | CPU
DEBUG 01-15 16:10:22.065178.065178 lmp.py:1935]   Expert  9 |    142 | CPU
DEBUG 01-15 16:10:22.065536.065536 lmp.py:1935]   Expert 38 |    144 | CPU
DEBUG 01-15 16:10:22.065371.065371 lmp.py:1935]   Expert 20 |    146 | CPU
DEBUG 01-15 16:10:22.065399.065399 lmp.py:1935]   Expert  7 |    147 | CPU
DEBUG 01-15 16:10:22.065711.065711 lmp.py:1935]   Expert 59 |    147 | CPU
DEBUG 01-15 16:10:22.065499.065499 lmp.py:1935]   Expert 62 |    157 | CPU
DEBUG 01-15 16:10:22.065242.065242 lmp.py:1935]   Expert 45 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065269.065269 lmp.py:1935]   Expert 19 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065296.065296 lmp.py:1935]   Expert 34 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065846.065846 lmp.py:1935]   Expert 50 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065874.065874 lmp.py:1935]   Expert 57 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065185.065185 lmp.py:1935]   Expert 31 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065974.065974 lmp.py:1935]   Expert 10 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065478.065478 lmp.py:1935]   Expert 23 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065744.065744 lmp.py:1935]   Expert 60 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065771.065771 lmp.py:1935]   Expert  8 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065036.065036 lmp.py:1935]   Expert 18 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065587.065587 lmp.py:1935]   Expert 22 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065899.065899 lmp.py:1935]   Expert 53 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065211.065211 lmp.py:1935]   Expert 52 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065284.065284 lmp.py:1935]   Expert 37 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065073.065073 lmp.py:1935]   Expert  5 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065623.065623 lmp.py:1935]   Expert 17 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065889.065889 lmp.py:1935]   Expert 11 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065154.065154 lmp.py:1935]   Expert  1 |    272 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065897.065897 lmp.py:1935]   Expert 49 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065685.065685 lmp.py:1935]   Expert 41 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065997.065997 lmp.py:1935]   Expert 28 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065071.065071 lmp.py:1935]   Expert 26 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065621.065621 lmp.py:1935]   Expert 32 |    292 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065695.065695 lmp.py:1935]   Expert 58 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065245.065245 lmp.py:1935]   Expert 40 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065318.065318 lmp.py:1935]   Expert 14 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065869.065869 lmp.py:1935]   Expert 12 |    329 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065134.065134 lmp.py:1935]   Expert 63 |    334 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.065162.065162 lmp.py:1935]   Expert 21 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065950.065950 lmp.py:1935]   Expert 27 |    668 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.065739.065739 lmp.py:1935]   Expert  3 |   1019 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.066859.066859 lmp.py:1937] 
DEBUG 01-15 16:10:22.066859.066859 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:22.066171.066171 lmp.py:1938]   CPU:   3159 tokens
DEBUG 01-15 16:10:22.066721.066721 lmp.py:1942]   cuda:1:   4632 tokens (16 experts)
DEBUG 01-15 16:10:22.066033.066033 lmp.py:1942]   cuda:2:   4497 tokens (16 experts)
DEBUG 01-15 16:10:22.066630.066630 lmp.py:1943]   Total GPU:   9129 tokens
DEBUG 01-15 16:10:22.066909.066909 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:22.066909.066909 lmp.py:1944] 
DEBUG 01-15 16:10:22.066373.066373 cuda_h.py:19] end experts_map_get cost 0.0019876956939697266 seconds
DEBUG 01-15 16:10:22.066660.066660 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:22.066085.066085 lmp.py:1953] 
DEBUG 01-15 16:10:22.066085.066085 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:22.066061.066061 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:22.066472.066472 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:22.066970.066970 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:22.066004.066004 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.1.gate_proj.weight': 0, 'model.layers.24.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.58.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.58.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.58.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.10.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.10.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.10.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.26.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.26.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.26.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.24.mlp.experts.32.gate_proj.weight': 0, 'model.layers.24.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.40.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.40.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.40.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.60.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.60.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.60.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.52.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.52.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.52.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(28345630720, 5767168, 0, 0), (28351397888, 5767168, 5767168, 0), (28339863552, 5767168, 11534336, 0), (28380233728, 5767168, 17301504, 0), (28386000896, 5767168, 23068672, 0), (28374466560, 5767168, 28835840, 0), (29331816448, 5767168, 34603008, 0), (29337583616, 5767168, 40370176, 0), (29326049280, 5767168, 46137344, 0), (28414836736, 5767168, 51904512, 0), (28420603904, 5767168, 57671680, 0), (28409069568, 5767168, 63438848, 0), (28968484864, 5767168, 69206016, 0), (28974252032, 5767168, 74973184, 0), (28962717696, 5767168, 80740352, 0), (29037690880, 5767168, 86507520, 0), (29043458048, 5767168, 92274688, 0), (29031923712, 5767168, 98041856, 0), (28501344256, 5767168, 103809024, 0), (28507111424, 5767168, 109576192, 0), (28495577088, 5767168, 115343360, 0), (29106896896, 5767168, 121110528, 0), (29112664064, 5767168, 126877696, 0), (29101129728, 5767168, 132644864, 0), (28570550272, 5767168, 138412032, 0), (28576317440, 5767168, 144179200, 0), (28564783104, 5767168, 149946368, 0), (28639756288, 5767168, 155713536, 0), (28645523456, 5767168, 161480704, 0), (28633989120, 5767168, 167247872, 0), (29193404416, 5767168, 173015040, 0), (29199171584, 5767168, 178782208, 0), (29187637248, 5767168, 184549376, 0), (28657057792, 5767168, 190316544, 0), (28662824960, 5767168, 196083712, 0), (28651290624, 5767168, 201850880, 0), (28708962304, 5767168, 207618048, 0), (28714729472, 5767168, 213385216, 0), (28703195136, 5767168, 219152384, 0), (28726263808, 5767168, 224919552, 0), (28732030976, 5767168, 230686720, 0), (28720496640, 5767168, 236453888, 0), (28778168320, 5767168, 242221056, 0), (28783935488, 5767168, 247988224, 0), (28772401152, 5767168, 253755392, 0), (29418323968, 5767168, 259522560, 0), (29424091136, 5767168, 265289728, 0), (29412556800, 5767168, 271056896, 0)], 2: [(28881977344, 5767168, 0, 0), (28887744512, 5767168, 5767168, 0), (28876210176, 5767168, 11534336, 0), (28916580352, 5767168, 17301504, 0), (28922347520, 5767168, 23068672, 0), (28910813184, 5767168, 28835840, 0), (29020389376, 5767168, 34603008, 0), (29026156544, 5767168, 40370176, 0), (29014622208, 5767168, 46137344, 0), (28466741248, 5767168, 51904512, 0), (28472508416, 5767168, 57671680, 0), (28460974080, 5767168, 63438848, 0), (28518645760, 5767168, 69206016, 0), (28524412928, 5767168, 74973184, 0), (28512878592, 5767168, 80740352, 0), (28535947264, 5767168, 86507520, 0), (28541714432, 5767168, 92274688, 0), (28530180096, 5767168, 98041856, 0), (29366419456, 5767168, 103809024, 0), (29372186624, 5767168, 109576192, 0), (29360652288, 5767168, 115343360, 0), (29176102912, 5767168, 121110528, 0), (29181870080, 5767168, 126877696, 0), (29170335744, 5767168, 132644864, 0), (28622454784, 5767168, 138412032, 0), (28628221952, 5767168, 144179200, 0), (28616687616, 5767168, 149946368, 0), (29228007424, 5767168, 155713536, 0), (29233774592, 5767168, 161480704, 0), (29222240256, 5767168, 167247872, 0), (28691660800, 5767168, 173015040, 0), (28697427968, 5767168, 178782208, 0), (28685893632, 5767168, 184549376, 0), (29245308928, 5767168, 190316544, 0), (29251076096, 5767168, 196083712, 0), (29239541760, 5767168, 201850880, 0), (29314514944, 5767168, 207618048, 0), (29320282112, 5767168, 213385216, 0), (29308747776, 5767168, 219152384, 0), (28795469824, 5767168, 224919552, 0), (28801236992, 5767168, 230686720, 0), (28789702656, 5767168, 236453888, 0), (28812771328, 5767168, 242221056, 0), (28818538496, 5767168, 247988224, 0), (28807004160, 5767168, 253755392, 0), (28864675840, 5767168, 259522560, 0), (28870443008, 5767168, 265289728, 0), (28858908672, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c28ca80>, 2: <capsule object NULL at 0x7a4e34219980>}
DEBUG 01-15 16:10:22.067053.067053 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:10:22.067975.067975 client.py:127] Model loaded
DEBUG 01-15 16:10:22.067942.067942 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:22.067184.067184 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5a24926d-48ed-412e-b51f-0021b52121b5
DEBUG 01-15 16:10:22.067549.067549 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:22.067953.067953 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:22.067653.067653 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:22.068938.068938 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:22.068221.068221 cuda_h.py:19] end restore2model cost 0.00036525726318359375 seconds
DEBUG 01-15 16:10:22.069201.069201 cuda_h.py:19] end move_flatidxs cost 0.0008561611175537109 seconds
DEBUG 01-15 16:10:22.069375.069375 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:22.068666.068666 cuda_h.py:19] end sllm_worker_task cost 0.010961532592773438 seconds
INFO 01-15 16:10:22.070591.070591 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5a24926d-48ed-412e-b51f-0021b52121b5
DEBUG 01-15 16:10:22.071831.071831 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004709720611572266 seconds
DEBUG 01-15 16:10:22.071211.071211 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:22.074402.074402 cuda_h.py:19] end restore2model cost 0.0030889511108398438 seconds
DEBUG 01-15 16:10:22.074053.074053 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008022069931030273 seconds
DEBUG 01-15 16:10:22.074279.074279 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:22.074900.074900 cuda_h.py:19] end gpu_sexperts cost 0.0003192424774169922 seconds
DEBUG 01-15 16:10:22.074398.074398 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:22.074082.074082 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:10:22.074732.074732 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:22.074627.074627 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.075014.075014 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010590553283691406 seconds
DEBUG 01-15 16:10:22.076241.076241 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.076811.076811 cuda_h.py:19] end gpu_group_list cost 0.00017881393432617188 seconds
DEBUG 01-15 16:10:22.077638.077638 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.078004.078004 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011439323425292969 seconds
DEBUG 01-15 16:10:22.078184.078184 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.078629.078629 cuda_h.py:19] end gpu_group_list cost 0.00017523765563964844 seconds
DEBUG 01-15 16:10:22.078788.078788 cuda_h.py:19] end group_tensors cost 0.009528636932373047 seconds
DEBUG 01-15 16:10:22.079866.079866 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:22.079880.079880 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:22.079094.079094 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5a24926d-48ed-412e-b51f-0021b52121b5
DEBUG 01-15 16:10:22.084800.084800 cuda_h.py:19] end group pad cost 0.005096435546875 seconds
DEBUG 01-15 16:10:22.084735.084735 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:22.095581.095581 client.py:127] Model loaded
DEBUG 01-15 16:10:22.095422.095422 cuda_h.py:19] end wait_experts_multi_device cost 0.01591014862060547 seconds
DEBUG 01-15 16:10:22.095212.095212 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:22.102547.102547 cuda_h.py:19] end group_einsum cost 0.01819324493408203 seconds
DEBUG 01-15 16:10:22.103803.103803 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:22.106272.106272 cuda_h.py:19] end get_outputs_cpu1 cost 0.003258943557739258 seconds
DEBUG 01-15 16:10:22.107312.107312 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03912496566772461 seconds
DEBUG 01-15 16:10:22.107433.107433 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0118560791015625 seconds
DEBUG 01-15 16:10:22.107006.107006 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:22.108662.108662 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.108435.108435 cuda_h.py:19] end index_scatter cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:10:22.109240.109240 cuda_h.py:19] end cpuoutputsdeal cost 0.0016052722930908203 seconds
DEBUG 01-15 16:10:22.109772.109772 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:22.109628.109628 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.109998.109998 cuda_h.py:19] end gpu_group_tensor cost 0.00013589859008789062 seconds
DEBUG 01-15 16:10:22.109568.109568 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.109110.109110 cuda_h.py:19] end gpu_group_tensor cost 0.00012350082397460938 seconds
DEBUG 01-15 16:10:22.109776.109776 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.111534.111534 cuda_h.py:19] end gpu_group_einsum cost 0.0012445449829101562 seconds
DEBUG 01-15 16:10:22.111294.111294 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.111511.111511 cuda_h.py:19] end gpu_group_einsum cost 0.0004856586456298828 seconds
DEBUG 01-15 16:10:22.111363.111363 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.112473.112473 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.112090.112090 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019693374633789062 seconds
DEBUG 01-15 16:10:22.112383.112383 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.112447.112447 cuda_h.py:19] end concat_expert_out cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:10:22.112435.112435 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.112935.112935 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:22.112016.112016 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008492469787597656 seconds
DEBUG 01-15 16:10:22.112754.112754 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.112829.112829 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.113046.113046 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012731552124023438 seconds
DEBUG 01-15 16:10:22.113279.113279 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.113579.113579 cuda_h.py:19] end concat_expert_out cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:22.113469.113469 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.113438.113438 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:22.113678.113678 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046896934509277344 seconds
DEBUG 01-15 16:10:22.113581.113581 cuda_h.py:19] end gpu_experts_multi_device cost 0.03859424591064453 seconds
DEBUG 01-15 16:10:22.113683.113683 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.05028796195983887 seconds
DEBUG 01-15 16:10:22.113298.113298 cuda_h.py:19] end prefill_layer cost 0.056995391845703125 seconds
DEBUG 01-15 16:10:22.114280.114280 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:10:22.114937.114937 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:22.114832.114832 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:10:22.114442.114442 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:22.114390.114390 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:22.114353.114353 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:22.114586.114586 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 8.368492126464844e-05 seconds
DEBUG 01-15 16:10:22.114674.114674 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:22.114788.114788 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:22.114427.114427 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:22.114106.114106 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:22.114446.114446 cuda_h.py:19] end allocate_cuda_memory cost 0.0002079010009765625 seconds
DEBUG 01-15 16:10:22.114873.114873 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:22.114855.114855 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:22.115574.115574 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:22.115755.115755 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:22.115465.115465 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b790b718-ccf7-442f-88c0-d40efdad67ec
DEBUG 01-15 16:10:22.115945.115945 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:22.115665.115665 cuda_h.py:10] start self_attn
INFO 01-15 16:10:22.116974.116974 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b790b718-ccf7-442f-88c0-d40efdad67ec
DEBUG 01-15 16:10:22.116724.116724 cuda_h.py:19] end load_into_gpu_async cost 0.001514434814453125 seconds
DEBUG 01-15 16:10:22.116811.116811 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:22.116967.116967 cuda_h.py:19] end restore_tensors2 cost 8.392333984375e-05 seconds
DEBUG 01-15 16:10:22.116015.116015 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002240419387817383 seconds
INFO 01-15 16:10:22.116203.116203 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b790b718-ccf7-442f-88c0-d40efdad67ec
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:22.119934.119934 cuda_h.py:19] end self_attn cost 0.0038099288940429688 seconds
DEBUG 01-15 16:10:22.119229.119229 cuda_h.py:19] end iln_self_attn_paln cost 0.005307674407958984 seconds
DEBUG 01-15 16:10:22.119112.119112 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:10:22.119828.119828 cuda_h.py:10] start gate
DEBUG 01-15 16:10:22.120654.120654 cuda_h.py:19] end gate cost 0.0006742477416992188 seconds
DEBUG 01-15 16:10:22.120013.120013 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:22.120149.120149 lmp.py:1912] 
DEBUG 01-15 16:10:22.120149.120149 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:22.121674.121674 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:22.121992.121992 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:22.121258.121258 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:22.121616.121616 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:22.121750.121750 lmp.py:1917] 
DEBUG 01-15 16:10:22.121750.121750 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:22.121439.121439 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:22.121089.121089 lmp.py:1935]   Expert 13 |     27 | CPU
DEBUG 01-15 16:10:22.121732.121732 lmp.py:1935]   Expert 44 |     39 | CPU
DEBUG 01-15 16:10:22.121659.121659 lmp.py:1935]   Expert 25 |     41 | CPU
DEBUG 01-15 16:10:22.121349.121349 lmp.py:1935]   Expert  9 |     42 | CPU
DEBUG 01-15 16:10:22.121561.121561 lmp.py:1935]   Expert 16 |     45 | CPU
DEBUG 01-15 16:10:22.121442.121442 lmp.py:1935]   Expert 38 |     49 | CPU
DEBUG 01-15 16:10:22.121085.121085 lmp.py:1935]   Expert  2 |     52 | CPU
DEBUG 01-15 16:10:22.121682.121682 lmp.py:1935]   Expert 22 |     55 | CPU
DEBUG 01-15 16:10:22.121563.121563 lmp.py:1935]   Expert 33 |     57 | CPU
DEBUG 01-15 16:10:22.121445.121445 lmp.py:1935]   Expert 42 |     60 | CPU
DEBUG 01-15 16:10:22.121041.121041 lmp.py:1935]   Expert  5 |     66 | CPU
DEBUG 01-15 16:10:22.121684.121684 lmp.py:1935]   Expert 23 |     77 | CPU
DEBUG 01-15 16:10:22.121851.121851 lmp.py:1935]   Expert 24 |     79 | CPU
DEBUG 01-15 16:10:22.121778.121778 lmp.py:1935]   Expert 10 |     83 | CPU
DEBUG 01-15 16:10:22.121944.121944 lmp.py:1935]   Expert 59 |     99 | CPU
DEBUG 01-15 16:10:22.121111.121111 lmp.py:1935]   Expert 21 |    106 | CPU
DEBUG 01-15 16:10:22.121277.121277 lmp.py:1935]   Expert 46 |    115 | CPU
DEBUG 01-15 16:10:22.121204.121204 lmp.py:1935]   Expert 55 |    116 | CPU
DEBUG 01-15 16:10:22.121132.121132 lmp.py:1935]   Expert 45 |    118 | CPU
DEBUG 01-15 16:10:22.121298.121298 lmp.py:1935]   Expert 61 |    125 | CPU
DEBUG 01-15 16:10:22.121941.121941 lmp.py:1935]   Expert 31 |    129 | CPU
DEBUG 01-15 16:10:22.121584.121584 lmp.py:1935]   Expert 51 |    139 | CPU
DEBUG 01-15 16:10:22.121704.121704 lmp.py:1935]   Expert 36 |    142 | CPU
DEBUG 01-15 16:10:22.121347.121347 lmp.py:1935]   Expert  6 |    143 | CPU
DEBUG 01-15 16:10:22.121513.121513 lmp.py:1935]   Expert  8 |    146 | CPU
DEBUG 01-15 16:10:22.121170.121170 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:10:22.121859.121859 lmp.py:1935]   Expert  3 |    151 | CPU
DEBUG 01-15 16:10:22.121548.121548 lmp.py:1935]   Expert  0 |    152 | CPU
DEBUG 01-15 16:10:22.121999.121999 lmp.py:1935]   Expert 26 |    156 | CPU
DEBUG 01-15 16:10:22.121735.121735 lmp.py:1935]   Expert 48 |    157 | CPU
DEBUG 01-15 16:10:22.121947.121947 lmp.py:1935]   Expert 18 |    158 | CPU
DEBUG 01-15 16:10:22.121398.121398 lmp.py:1935]   Expert 41 |    167 | CPU
DEBUG 01-15 16:10:22.121280.121280 lmp.py:1935]   Expert 12 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.121353.121353 lmp.py:1935]   Expert  7 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.121188.121188 lmp.py:1935]   Expert 20 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.121070.121070 lmp.py:1935]   Expert 56 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.121951.121951 lmp.py:1935]   Expert 28 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.121832.121832 lmp.py:1935]   Expert 27 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.121998.121998 lmp.py:1935]   Expert 34 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.121926.121926 lmp.py:1935]   Expert  1 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.121092.121092 lmp.py:1935]   Expert 47 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.121258.121258 lmp.py:1935]   Expert 32 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.121186.121186 lmp.py:1935]   Expert 11 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.121352.121352 lmp.py:1935]   Expert 40 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.121280.121280 lmp.py:1935]   Expert 49 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.121923.121923 lmp.py:1935]   Expert 53 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.121804.121804 lmp.py:1935]   Expert 63 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.121209.121209 lmp.py:1935]   Expert 15 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.121329.121329 lmp.py:1935]   Expert 50 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.121495.121495 lmp.py:1935]   Expert 30 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.122423.122423 lmp.py:1935]   Expert 29 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.122350.122350 lmp.py:1935]   Expert  4 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.122278.122278 lmp.py:1935]   Expert 35 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.122967.122967 lmp.py:1935]   Expert 14 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.122895.122895 lmp.py:1935]   Expert 37 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.122061.122061 lmp.py:1935]   Expert 52 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.122227.122227 lmp.py:1935]   Expert 17 |    362 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.122824.122824 lmp.py:1935]   Expert 54 |    377 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.122705.122705 lmp.py:1935]   Expert 39 |    387 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.122110.122110 lmp.py:1935]   Expert 57 |    412 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.122753.122753 lmp.py:1935]   Expert 60 |    457 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.122396.122396 lmp.py:1935]   Expert 62 |    465 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.122562.122562 lmp.py:1935]   Expert 19 |    542 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.122490.122490 lmp.py:1935]   Expert 58 |    570 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.122464.122464 lmp.py:1937] 
DEBUG 01-15 16:10:22.122464.122464 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:22.122153.122153 lmp.py:1938]   CPU:   3238 tokens
DEBUG 01-15 16:10:22.122319.122319 lmp.py:1942]   cuda:1:   4526 tokens (16 experts)
DEBUG 01-15 16:10:22.122485.122485 lmp.py:1942]   cuda:2:   4524 tokens (16 experts)
DEBUG 01-15 16:10:22.122936.122936 lmp.py:1943]   Total GPU:   9050 tokens
DEBUG 01-15 16:10:22.122149.122149 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:22.122149.122149 lmp.py:1944] 
DEBUG 01-15 16:10:22.122229.122229 cuda_h.py:19] end experts_map_get cost 0.0016741752624511719 seconds
DEBUG 01-15 16:10:22.122794.122794 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:22.122120.122120 lmp.py:1953] 
DEBUG 01-15 16:10:22.122120.122120 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:22.122234.122234 cuda_h.py:19] end cpu_experts_submit cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:10:22.122758.122758 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:22.122396.122396 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:22.123014.123014 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.1.gate_proj.weight': 0, 'model.layers.25.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.25.mlp.experts.32.gate_proj.weight': 0, 'model.layers.25.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.50.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.50.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.50.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.52.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.52.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.52.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.57.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.57.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.57.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.27.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.27.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.27.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(29452926976, 5767168, 0, 0), (29458694144, 5767168, 5767168, 0), (29447159808, 5767168, 11534336, 0), (30023876608, 5767168, 17301504, 0), (30029643776, 5767168, 23068672, 0), (30018109440, 5767168, 28835840, 0), (29504831488, 5767168, 34603008, 0), (29510598656, 5767168, 40370176, 0), (29499064320, 5767168, 46137344, 0), (30075781120, 5767168, 51904512, 0), (30081548288, 5767168, 57671680, 0), (30070013952, 5767168, 63438848, 0), (30110384128, 5767168, 69206016, 0), (30116151296, 5767168, 74973184, 0), (30104616960, 5767168, 80740352, 0), (29643243520, 5767168, 86507520, 0), (29649010688, 5767168, 92274688, 0), (29637476352, 5767168, 98041856, 0), (29677846528, 5767168, 103809024, 0), (29683613696, 5767168, 109576192, 0), (29672079360, 5767168, 115343360, 0), (29695148032, 5767168, 121110528, 0), (29700915200, 5767168, 126877696, 0), (29689380864, 5767168, 132644864, 0), (30248796160, 5767168, 138412032, 0), (30254563328, 5767168, 144179200, 0), (30243028992, 5767168, 149946368, 0), (30283399168, 5767168, 155713536, 0), (30289166336, 5767168, 161480704, 0), (30277632000, 5767168, 167247872, 0), (30352605184, 5767168, 173015040, 0), (30358372352, 5767168, 178782208, 0), (30346838016, 5767168, 184549376, 0), (30369906688, 5767168, 190316544, 0), (30375673856, 5767168, 196083712, 0), (30364139520, 5767168, 201850880, 0), (30404509696, 5767168, 207618048, 0), (30410276864, 5767168, 213385216, 0), (30398742528, 5767168, 219152384, 0), (30439112704, 5767168, 224919552, 0), (30444879872, 5767168, 230686720, 0), (30433345536, 5767168, 236453888, 0), (30473715712, 5767168, 242221056, 0), (30479482880, 5767168, 247988224, 0), (30467948544, 5767168, 253755392, 0), (29954670592, 5767168, 259522560, 0), (29960437760, 5767168, 265289728, 0), (29948903424, 5767168, 271056896, 0)], 2: [(29989273600, 5767168, 0, 0), (29995040768, 5767168, 5767168, 0), (29983506432, 5767168, 11534336, 0), (30041178112, 5767168, 17301504, 0), (30046945280, 5767168, 23068672, 0), (30035410944, 5767168, 28835840, 0), (29556736000, 5767168, 34603008, 0), (29562503168, 5767168, 40370176, 0), (29550968832, 5767168, 46137344, 0), (30127685632, 5767168, 51904512, 0), (30133452800, 5767168, 57671680, 0), (30121918464, 5767168, 63438848, 0), (29625942016, 5767168, 69206016, 0), (29631709184, 5767168, 74973184, 0), (29620174848, 5767168, 80740352, 0), (29729751040, 5767168, 86507520, 0), (29735518208, 5767168, 92274688, 0), (29723983872, 5767168, 98041856, 0), (30300700672, 5767168, 103809024, 0), (30306467840, 5767168, 109576192, 0), (30294933504, 5767168, 115343360, 0), (29764354048, 5767168, 121110528, 0), (29770121216, 5767168, 126877696, 0), (29758586880, 5767168, 132644864, 0), (30335303680, 5767168, 138412032, 0), (30341070848, 5767168, 144179200, 0), (30329536512, 5767168, 149946368, 0), (29781655552, 5767168, 155713536, 0), (29787422720, 5767168, 161480704, 0), (29775888384, 5767168, 167247872, 0), (30421811200, 5767168, 173015040, 0), (30427578368, 5767168, 178782208, 0), (30416044032, 5767168, 184549376, 0), (29902766080, 5767168, 190316544, 0), (29908533248, 5767168, 196083712, 0), (29896998912, 5767168, 201850880, 0), (29920067584, 5767168, 207618048, 0), (29925834752, 5767168, 213385216, 0), (29914300416, 5767168, 219152384, 0), (29937369088, 5767168, 224919552, 0), (29943136256, 5767168, 230686720, 0), (29931601920, 5767168, 236453888, 0), (30508318720, 5767168, 242221056, 0), (30514085888, 5767168, 247988224, 0), (30502551552, 5767168, 253755392, 0), (30525620224, 5767168, 259522560, 0), (30531387392, 5767168, 265289728, 0), (30519853056, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e34219bf0>, 2: <capsule object NULL at 0x7a4e34219dd0>}
INFO 01-15 16:10:22.123318.123318 client.py:127] Model loaded
DEBUG 01-15 16:10:22.123414.123414 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:22.123661.123661 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:22.123822.123822 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:22.124622.124622 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 13c5d1e0-c731-4424-a6ae-d0a49f7e712c
DEBUG 01-15 16:10:22.124132.124132 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:22.124258.124258 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:22.124949.124949 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:22.124894.124894 cuda_h.py:19] end restore2model cost 0.0007011890411376953 seconds
DEBUG 01-15 16:10:22.124154.124154 cuda_h.py:19] end sllm_worker_task cost 0.010238409042358398 seconds
INFO 01-15 16:10:22.125261.125261 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 13c5d1e0-c731-4424-a6ae-d0a49f7e712c
DEBUG 01-15 16:10:22.125546.125546 cuda_h.py:19] end move_flatidxs cost 0.0009419918060302734 seconds
DEBUG 01-15 16:10:22.125468.125468 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:22.125758.125758 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002866029739379883 seconds
DEBUG 01-15 16:10:22.125283.125283 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:22.128615.128615 cuda_h.py:19] end restore2model cost 0.0025641918182373047 seconds
DEBUG 01-15 16:10:22.128451.128451 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005650758743286133 seconds
DEBUG 01-15 16:10:22.128293.128293 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:22.128767.128767 cuda_h.py:19] end gpu_sexperts cost 0.0002837181091308594 seconds
DEBUG 01-15 16:10:22.128312.128312 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:22.128658.128658 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:22.128831.128831 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:22.128057.128057 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.129397.129397 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008165836334228516 seconds
DEBUG 01-15 16:10:22.129524.129524 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.129227.129227 cuda_h.py:19] end gpu_group_list cost 0.00017309188842773438 seconds
DEBUG 01-15 16:10:22.130907.130907 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.131805.131805 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008480548858642578 seconds
DEBUG 01-15 16:10:22.131741.131741 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.131019.131019 cuda_h.py:19] end gpu_group_list cost 0.0001761913299560547 seconds
DEBUG 01-15 16:10:22.132430.132430 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:22.132736.132736 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 13c5d1e0-c731-4424-a6ae-d0a49f7e712c
DEBUG 01-15 16:10:22.135374.135374 cuda_h.py:19] end group_tensors cost 0.010000228881835938 seconds
DEBUG 01-15 16:10:22.136606.136606 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:22.141216.141216 cuda_h.py:19] end group pad cost 0.0050966739654541016 seconds
DEBUG 01-15 16:10:22.141105.141105 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:22.151929.151929 client.py:127] Model loaded
DEBUG 01-15 16:10:22.152574.152574 cuda_h.py:19] end wait_experts_multi_device cost 0.019647836685180664 seconds
DEBUG 01-15 16:10:22.152331.152331 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:22.160913.160913 cuda_h.py:19] end group_einsum cost 0.019294261932373047 seconds
DEBUG 01-15 16:10:22.160455.160455 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:22.164743.164743 cuda_h.py:19] end get_outputs_cpu1 cost 0.003376483917236328 seconds
DEBUG 01-15 16:10:22.165836.165836 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04094195365905762 seconds
DEBUG 01-15 16:10:22.165917.165917 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013288259506225586 seconds
DEBUG 01-15 16:10:22.165768.165768 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:22.166402.166402 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.166645.166645 cuda_h.py:19] end index_scatter cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:10:22.167782.167782 cuda_h.py:19] end cpuoutputsdeal cost 0.0015516281127929688 seconds
DEBUG 01-15 16:10:22.167453.167453 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:22.167116.167116 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.167347.167347 cuda_h.py:19] end gpu_group_tensor cost 0.00013875961303710938 seconds
DEBUG 01-15 16:10:22.167726.167726 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.167784.167784 cuda_h.py:19] end gpu_group_tensor cost 0.000118255615234375 seconds
DEBUG 01-15 16:10:22.167535.167535 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.168623.168623 cuda_h.py:19] end gpu_group_einsum cost 0.0005762577056884766 seconds
DEBUG 01-15 16:10:22.168495.168495 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.169222.169222 cuda_h.py:19] end gpu_group_einsum cost 0.00047278404235839844 seconds
DEBUG 01-15 16:10:22.169729.169729 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.169554.169554 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.169026.169026 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001964569091796875 seconds
DEBUG 01-15 16:10:22.169842.169842 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.169124.169124 cuda_h.py:19] end concat_expert_out cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:10:22.169583.169583 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.169837.169837 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:22.170487.170487 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008213520050048828 seconds
DEBUG 01-15 16:10:22.170371.170371 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.170015.170015 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.170749.170749 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001246929168701172 seconds
DEBUG 01-15 16:10:22.170836.170836 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.170468.170468 cuda_h.py:19] end concat_expert_out cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:22.170450.170450 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.170466.170466 cuda_h.py:19] end index_scatter cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:10:22.170798.170798 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00045490264892578125 seconds
DEBUG 01-15 16:10:22.170794.170794 cuda_h.py:19] end gpu_experts_multi_device cost 0.04197883605957031 seconds
DEBUG 01-15 16:10:22.170227.170227 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.05085897445678711 seconds
DEBUG 01-15 16:10:22.171657.171657 cuda_h.py:19] end prefill_layer cost 0.05706596374511719 seconds
DEBUG 01-15 16:10:22.171294.171294 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:10:22.171997.171997 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:22.171177.171177 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:10:22.171072.171072 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:22.171781.171781 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:22.171300.171300 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:10:22.171203.171203 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:22.171847.171847 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 0.00015974044799804688 seconds
DEBUG 01-15 16:10:22.171254.171254 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:22.171753.171753 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:22.171338.171338 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:22.171819.171819 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:22.172876.172876 cuda_h.py:19] end allocate_cuda_memory cost 0.0002608299255371094 seconds
DEBUG 01-15 16:10:22.172351.172351 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:22.172928.172928 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:22.172565.172565 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:22.172680.172680 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b721eef7-fad6-4db7-a0f4-0858c299fb43
DEBUG 01-15 16:10:22.172160.172160 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:22.172993.172993 cuda_h.py:10] start self_attn
INFO 01-15 16:10:22.173313.173313 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b721eef7-fad6-4db7-a0f4-0858c299fb43
DEBUG 01-15 16:10:22.173063.173063 cuda_h.py:19] end load_into_gpu_async cost 0.0014688968658447266 seconds
DEBUG 01-15 16:10:22.173104.173104 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:22.173022.173022 cuda_h.py:19] end restore_tensors2 cost 8.296966552734375e-05 seconds
DEBUG 01-15 16:10:22.173069.173069 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022873878479003906 seconds
INFO 01-15 16:10:22.174926.174926 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b721eef7-fad6-4db7-a0f4-0858c299fb43
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:22.176411.176411 cuda_h.py:19] end self_attn cost 0.0037069320678710938 seconds
DEBUG 01-15 16:10:22.176945.176945 cuda_h.py:19] end iln_self_attn_paln cost 0.005198240280151367 seconds
DEBUG 01-15 16:10:22.177205.177205 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:10:22.177206.177206 cuda_h.py:10] start gate
DEBUG 01-15 16:10:22.177907.177907 cuda_h.py:19] end gate cost 0.0007238388061523438 seconds
DEBUG 01-15 16:10:22.177412.177412 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:22.178551.178551 lmp.py:1912] 
DEBUG 01-15 16:10:22.178551.178551 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:22.178452.178452 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:22.178818.178818 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:22.178083.178083 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:22.178680.178680 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:22.178846.178846 lmp.py:1917] 
DEBUG 01-15 16:10:22.178846.178846 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:22.178204.178204 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:22.178715.178715 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:10:22.178788.178788 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:10:22.178147.178147 lmp.py:1935]   Expert 11 |     29 | CPU
DEBUG 01-15 16:10:22.178028.178028 lmp.py:1935]   Expert  7 |     40 | CPU
DEBUG 01-15 16:10:22.178671.178671 lmp.py:1935]   Expert  3 |     43 | CPU
DEBUG 01-15 16:10:22.178314.178314 lmp.py:1935]   Expert 51 |     44 | CPU
DEBUG 01-15 16:10:22.178719.178719 lmp.py:1935]   Expert 62 |     44 | CPU
DEBUG 01-15 16:10:22.178123.178123 lmp.py:1935]   Expert 29 |     52 | CPU
DEBUG 01-15 16:10:22.178528.178528 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:10:22.178124.178124 lmp.py:1935]   Expert 17 |     54 | CPU
DEBUG 01-15 16:10:22.178721.178721 lmp.py:1935]   Expert  6 |     59 | CPU
DEBUG 01-15 16:10:22.178841.178841 lmp.py:1935]   Expert  9 |     66 | CPU
DEBUG 01-15 16:10:22.178484.178484 lmp.py:1935]   Expert 38 |     77 | CPU
DEBUG 01-15 16:10:22.178127.178127 lmp.py:1935]   Expert 63 |     79 | CPU
DEBUG 01-15 16:10:22.178770.178770 lmp.py:1935]   Expert 55 |     82 | CPU
DEBUG 01-15 16:10:22.178174.178174 lmp.py:1935]   Expert 59 |     89 | CPU
DEBUG 01-15 16:10:22.178817.178817 lmp.py:1935]   Expert 48 |     92 | CPU
DEBUG 01-15 16:10:22.178460.178460 lmp.py:1935]   Expert  8 |     96 | CPU
DEBUG 01-15 16:10:22.178626.178626 lmp.py:1935]   Expert 19 |     97 | CPU
DEBUG 01-15 16:10:22.178031.178031 lmp.py:1935]   Expert 49 |    103 | CPU
DEBUG 01-15 16:10:22.178674.178674 lmp.py:1935]   Expert 22 |    106 | CPU
DEBUG 01-15 16:10:22.178555.178555 lmp.py:1935]   Expert 24 |    111 | CPU
DEBUG 01-15 16:10:22.178437.178437 lmp.py:1935]   Expert 36 |    115 | CPU
DEBUG 01-15 16:10:22.178318.178318 lmp.py:1935]   Expert 34 |    117 | CPU
DEBUG 01-15 16:10:22.178676.178676 lmp.py:1935]   Expert 42 |    118 | CPU
DEBUG 01-15 16:10:22.178558.178558 lmp.py:1935]   Expert 50 |    120 | CPU
DEBUG 01-15 16:10:22.178962.178962 lmp.py:1935]   Expert 39 |    123 | CPU
DEBUG 01-15 16:10:22.178797.178797 lmp.py:1935]   Expert  4 |    132 | CPU
DEBUG 01-15 16:10:22.178917.178917 lmp.py:1935]   Expert 37 |    142 | CPU
DEBUG 01-15 16:10:22.178037.178037 lmp.py:1935]   Expert 41 |    143 | CPU
DEBUG 01-15 16:10:22.178760.178760 lmp.py:1935]   Expert 15 |    148 | CPU
DEBUG 01-15 16:10:22.178926.178926 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:10:22.179761.179761 lmp.py:1935]   Expert 56 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179550.179550 lmp.py:1935]   Expert 16 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179385.179385 lmp.py:1935]   Expert 60 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179459.179459 lmp.py:1935]   Expert 44 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179817.179817 lmp.py:1935]   Expert  1 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179937.179937 lmp.py:1935]   Expert 21 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179818.179818 lmp.py:1935]   Expert 43 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179461.179461 lmp.py:1935]   Expert 47 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179104.179104 lmp.py:1935]   Expert 53 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179224.179224 lmp.py:1935]   Expert 12 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179867.179867 lmp.py:1935]   Expert 33 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179702.179702 lmp.py:1935]   Expert 13 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179299.179299 lmp.py:1935]   Expert 32 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179134.179134 lmp.py:1935]   Expert 28 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179730.179730 lmp.py:1935]   Expert  0 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179373.179373 lmp.py:1935]   Expert 54 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179255.179255 lmp.py:1935]   Expert 31 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179136.179136 lmp.py:1935]   Expert 26 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179779.179779 lmp.py:1935]   Expert 10 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179422.179422 lmp.py:1935]   Expert 18 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179065.179065 lmp.py:1935]   Expert 57 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179900.179900 lmp.py:1935]   Expert  2 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179020.179020 lmp.py:1935]   Expert 58 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179616.179616 lmp.py:1935]   Expert 40 |    342 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179213.179213 lmp.py:1935]   Expert 25 |    357 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179810.179810 lmp.py:1935]   Expert 45 |    364 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179691.179691 lmp.py:1935]   Expert  5 |    438 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179049.179049 lmp.py:1935]   Expert 35 |    461 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179931.179931 lmp.py:1935]   Expert 27 |    486 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179051.179051 lmp.py:1935]   Expert 46 |    552 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179932.179932 lmp.py:1935]   Expert 52 |    595 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.179813.179813 lmp.py:1935]   Expert 14 |    887 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.179503.179503 lmp.py:1937] 
DEBUG 01-15 16:10:22.179503.179503 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:22.179736.179736 lmp.py:1938]   CPU:   2750 tokens
DEBUG 01-15 16:10:22.179524.179524 lmp.py:1942]   cuda:1:   4822 tokens (16 experts)
DEBUG 01-15 16:10:22.179406.179406 lmp.py:1942]   cuda:2:   4716 tokens (16 experts)
DEBUG 01-15 16:10:22.179095.179095 lmp.py:1943]   Total GPU:   9538 tokens
DEBUG 01-15 16:10:22.179307.179307 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:22.179307.179307 lmp.py:1944] 
DEBUG 01-15 16:10:22.179719.179719 cuda_h.py:19] end experts_map_get cost 0.001809835433959961 seconds
DEBUG 01-15 16:10:22.179522.179522 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:22.179325.179325 lmp.py:1953] 
DEBUG 01-15 16:10:22.179325.179325 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:22.179254.179254 cuda_h.py:19] end cpu_experts_submit cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:22.179234.179234 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:22.179872.179872 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:22.180192.180192 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.32.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.32.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.32.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.33.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.33.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.33.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.1.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.1.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.1.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.5.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.5.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.5.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.60.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.60.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.60.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.56.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.56.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.56.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.31.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.31.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.31.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.57.up_proj.weight': 271056896}, 2: {'model.layers.26.mlp.experts.2.gate_proj.weight': 0, 'model.layers.26.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.40.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.40.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.40.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.21.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.21.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.21.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.26.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.26.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.26.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.28.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.28.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.28.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (31096569856, 5767168, 17301504, 0), (31102337024, 5767168, 23068672, 0), (31090802688, 5767168, 28835840, 0), (31113871360, 5767168, 34603008, 0), (31119638528, 5767168, 40370176, 0), (31108104192, 5767168, 46137344, 0), (30560223232, 5767168, 51904512, 0), (30565990400, 5767168, 57671680, 0), (30554456064, 5767168, 63438848, 0), (30629429248, 5767168, 69206016, 0), (30635196416, 5767168, 74973184, 0), (30623662080, 5767168, 80740352, 0), (30715936768, 5767168, 86507520, 0), (30721703936, 5767168, 92274688, 0), (30710169600, 5767168, 98041856, 0), (31286886400, 5767168, 103809024, 0), (31292653568, 5767168, 109576192, 0), (31281119232, 5767168, 115343360, 0), (30785142784, 5767168, 121110528, 0), (30790909952, 5767168, 126877696, 0), (30779375616, 5767168, 132644864, 0), (31356092416, 5767168, 138412032, 0), (31361859584, 5767168, 144179200, 0), (31350325248, 5767168, 149946368, 0), (31581011968, 5767168, 155713536, 0), (31586779136, 5767168, 161480704, 0), (31575244800, 5767168, 167247872, 0), (31511805952, 5767168, 173015040, 0), (31517573120, 5767168, 178782208, 0), (31506038784, 5767168, 184549376, 0), (30975459328, 5767168, 190316544, 0), (30981226496, 5767168, 196083712, 0), (30969692160, 5767168, 201850880, 0), (31546408960, 5767168, 207618048, 0), (31552176128, 5767168, 213385216, 0), (31540641792, 5767168, 219152384, 0), (31010062336, 5767168, 224919552, 0), (31015829504, 5767168, 230686720, 0), (31004295168, 5767168, 236453888, 0), (31079268352, 5767168, 242221056, 0), (31085035520, 5767168, 247988224, 0), (31073501184, 5767168, 253755392, 0), (31529107456, 5767168, 259522560, 0), (31534874624, 5767168, 265289728, 0), (31523340288, 5767168, 271056896, 0)], 2: [(30577524736, 5767168, 0, 0), (30583291904, 5767168, 5767168, 0), (30571757568, 5767168, 11534336, 0), (31148474368, 5767168, 17301504, 0), (31154241536, 5767168, 23068672, 0), (31142707200, 5767168, 28835840, 0), (31234981888, 5767168, 34603008, 0), (31240749056, 5767168, 40370176, 0), (31229214720, 5767168, 46137344, 0), (30750539776, 5767168, 51904512, 0), (30756306944, 5767168, 57671680, 0), (30744772608, 5767168, 63438848, 0), (31321489408, 5767168, 69206016, 0), (31327256576, 5767168, 74973184, 0), (31315722240, 5767168, 80740352, 0), (31338790912, 5767168, 86507520, 0), (31344558080, 5767168, 92274688, 0), (31333023744, 5767168, 98041856, 0), (30767841280, 5767168, 103809024, 0), (30773608448, 5767168, 109576192, 0), (30762074112, 5767168, 115343360, 0), (31304187904, 5767168, 121110528, 0), (31309955072, 5767168, 126877696, 0), (31298420736, 5767168, 132644864, 0), (30819745792, 5767168, 138412032, 0), (30825512960, 5767168, 144179200, 0), (30813978624, 5767168, 149946368, 0), (30854348800, 5767168, 155713536, 0), (30860115968, 5767168, 161480704, 0), (30848581632, 5767168, 167247872, 0), (31442599936, 5767168, 173015040, 0), (31448367104, 5767168, 178782208, 0), (31436832768, 5767168, 184549376, 0), (31459901440, 5767168, 190316544, 0), (31465668608, 5767168, 196083712, 0), (31454134272, 5767168, 201850880, 0), (31477202944, 5767168, 207618048, 0), (31482970112, 5767168, 213385216, 0), (31471435776, 5767168, 219152384, 0), (30906253312, 5767168, 224919552, 0), (30912020480, 5767168, 230686720, 0), (30900486144, 5767168, 236453888, 0), (30992760832, 5767168, 242221056, 0), (30998528000, 5767168, 247988224, 0), (30986993664, 5767168, 253755392, 0), (31027363840, 5767168, 259522560, 0), (31033131008, 5767168, 265289728, 0), (31021596672, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e54304960>, 2: <capsule object NULL at 0x7a4e3421a040>}
DEBUG 01-15 16:10:22.180999.180999 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:22.181644.181644 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:22.181492.181492 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 299abbc2-abea-4a5d-9a1f-27e087893372
DEBUG 01-15 16:10:22.181889.181889 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:22.181487.181487 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:22.181226.181226 client.py:127] Model loaded
DEBUG 01-15 16:10:22.181817.181817 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:22.181033.181033 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:22.181333.181333 cuda_h.py:19] end restore2model cost 0.0003495216369628906 seconds
DEBUG 01-15 16:10:22.181911.181911 cuda_h.py:19] end sllm_worker_task cost 0.010329008102416992 seconds
INFO 01-15 16:10:22.182473.182473 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 299abbc2-abea-4a5d-9a1f-27e087893372
DEBUG 01-15 16:10:22.182553.182553 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0025091171264648438 seconds
DEBUG 01-15 16:10:22.182600.182600 cuda_h.py:19] end move_flatidxs cost 0.0008478164672851562 seconds
DEBUG 01-15 16:10:22.182409.182409 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:22.182900.182900 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:22.185040.185040 cuda_h.py:19] end restore2model cost 0.0025718212127685547 seconds
DEBUG 01-15 16:10:22.185499.185499 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005299568176269531 seconds
DEBUG 01-15 16:10:22.185009.185009 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:22.185623.185623 cuda_h.py:19] end gpu_sexperts cost 0.00027942657470703125 seconds
DEBUG 01-15 16:10:22.185929.185929 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:22.185136.185136 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7642974853515625e-05 seconds
DEBUG 01-15 16:10:22.185785.185785 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:22.185727.185727 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.186536.186536 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008115768432617188 seconds
DEBUG 01-15 16:10:22.186141.186141 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.186181.186181 cuda_h.py:19] end gpu_group_list cost 0.00017499923706054688 seconds
DEBUG 01-15 16:10:22.187179.187179 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.188091.188091 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008578300476074219 seconds
DEBUG 01-15 16:10:22.188742.188742 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.188351.188351 cuda_h.py:19] end gpu_group_list cost 0.00017452239990234375 seconds
DEBUG 01-15 16:10:22.189424.189424 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:22.189968.189968 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 299abbc2-abea-4a5d-9a1f-27e087893372
DEBUG 01-15 16:10:22.191271.191271 cuda_h.py:19] end group_tensors cost 0.008486270904541016 seconds
DEBUG 01-15 16:10:22.191380.191380 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:22.195027.195027 cuda_h.py:19] end group pad cost 0.003445148468017578 seconds
DEBUG 01-15 16:10:22.195294.195294 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:22.207894.207894 client.py:127] Model loaded
DEBUG 01-15 16:10:22.208703.208703 cuda_h.py:19] end wait_experts_multi_device cost 0.01895737648010254 seconds
DEBUG 01-15 16:10:22.208473.208473 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:22.214122.214122 cuda_h.py:19] end group_einsum cost 0.01892566680908203 seconds
DEBUG 01-15 16:10:22.214777.214777 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:22.217112.217112 cuda_h.py:19] end get_outputs_cpu1 cost 0.0030210018157958984 seconds
DEBUG 01-15 16:10:22.218472.218472 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03685426712036133 seconds
DEBUG 01-15 16:10:22.218253.218253 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01020956039428711 seconds
DEBUG 01-15 16:10:22.218482.218482 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:22.219931.219931 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.220380.220380 cuda_h.py:19] end index_scatter cost 8.416175842285156e-05 seconds
DEBUG 01-15 16:10:22.220615.220615 cuda_h.py:19] end cpuoutputsdeal cost 0.0015687942504882812 seconds
DEBUG 01-15 16:10:22.220128.220128 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:22.220368.220368 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.220553.220553 cuda_h.py:19] end gpu_group_tensor cost 0.00013971328735351562 seconds
DEBUG 01-15 16:10:22.220123.220123 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.220850.220850 cuda_h.py:19] end gpu_group_tensor cost 0.00012087821960449219 seconds
DEBUG 01-15 16:10:22.220178.220178 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.221729.221729 cuda_h.py:19] end gpu_group_einsum cost 0.0005662441253662109 seconds
DEBUG 01-15 16:10:22.221581.221581 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.222018.222018 cuda_h.py:19] end gpu_group_einsum cost 0.0005409717559814453 seconds
DEBUG 01-15 16:10:22.222771.222771 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.222457.222457 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.222690.222690 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001952648162841797 seconds
DEBUG 01-15 16:10:22.222268.222268 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.222152.222152 cuda_h.py:19] end concat_expert_out cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:10:22.222611.222611 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.223819.223819 cuda_h.py:19] end index_scatter cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:22.223568.223568 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008118152618408203 seconds
DEBUG 01-15 16:10:22.223459.223459 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.223295.223295 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.223552.223552 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001232624053955078 seconds
DEBUG 01-15 16:10:22.223162.223162 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.223893.223893 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:22.223829.223829 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.223892.223892 cuda_h.py:19] end index_scatter cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:10:22.223747.223747 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00045800209045410156 seconds
DEBUG 01-15 16:10:22.223604.223604 cuda_h.py:19] end gpu_experts_multi_device cost 0.03825521469116211 seconds
DEBUG 01-15 16:10:22.224083.224083 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.04696345329284668 seconds
DEBUG 01-15 16:10:22.224368.224368 cuda_h.py:19] end prefill_layer cost 0.0531916618347168 seconds
DEBUG 01-15 16:10:22.224774.224774 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:10:22.224954.224954 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:22.224372.224372 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:10:22.224744.224744 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:22.224422.224422 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:22.224730.224730 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:22.227060.227060 cuda_h.py:19] end self_attn cost 0.0026793479919433594 seconds
DEBUG 01-15 16:10:22.227196.227196 cuda_h.py:19] end iln_self_attn_paln cost 0.003390073776245117 seconds
DEBUG 01-15 16:10:22.228695.228695 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:10:22.228888.228888 cuda_h.py:10] start gate
DEBUG 01-15 16:10:22.228931.228931 cuda_h.py:19] end gate cost 0.0006594657897949219 seconds
DEBUG 01-15 16:10:22.228390.228390 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:22.229218.229218 lmp.py:1912] 
DEBUG 01-15 16:10:22.229218.229218 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:22.229027.229027 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:22.229445.229445 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:22.229287.229287 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:22.229937.229937 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:22.229156.229156 lmp.py:1917] 
DEBUG 01-15 16:10:22.229156.229156 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:22.229614.229614 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:22.229986.229986 lmp.py:1935]   Expert 18 |     66 | CPU
DEBUG 01-15 16:10:22.229682.229682 lmp.py:1935]   Expert 54 |     71 | CPU
DEBUG 01-15 16:10:22.229424.229424 lmp.py:1935]   Expert 47 |     72 | CPU
DEBUG 01-15 16:10:22.229167.229167 lmp.py:1935]   Expert 23 |     77 | CPU
DEBUG 01-15 16:10:22.229671.229671 lmp.py:1935]   Expert 48 |     79 | CPU
DEBUG 01-15 16:10:22.229367.229367 lmp.py:1935]   Expert 44 |     83 | CPU
DEBUG 01-15 16:10:22.229063.229063 lmp.py:1935]   Expert 45 |     83 | CPU
DEBUG 01-15 16:10:22.229474.229474 lmp.py:1935]   Expert 20 |     92 | CPU
DEBUG 01-15 16:10:22.229170.229170 lmp.py:1935]   Expert 31 |     95 | CPU
DEBUG 01-15 16:10:22.229151.229151 lmp.py:1935]   Expert 36 |    104 | CPU
DEBUG 01-15 16:10:22.229655.229655 lmp.py:1935]   Expert 61 |    114 | CPU
DEBUG 01-15 16:10:22.229921.229921 lmp.py:1935]   Expert 42 |    120 | CPU
DEBUG 01-15 16:10:22.229186.229186 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:10:22.229213.229213 lmp.py:1935]   Expert 43 |    121 | CPU
DEBUG 01-15 16:10:22.229148.229148 lmp.py:1935]   Expert 24 |    123 | CPU
DEBUG 01-15 16:10:22.229606.229606 lmp.py:1935]   Expert 33 |    123 | CPU
DEBUG 01-15 16:10:22.229063.229063 lmp.py:1935]   Expert 11 |    125 | CPU
DEBUG 01-15 16:10:22.229521.229521 lmp.py:1935]   Expert 49 |    128 | CPU
DEBUG 01-15 16:10:22.229787.229787 lmp.py:1935]   Expert 56 |    130 | CPU
DEBUG 01-15 16:10:22.229052.229052 lmp.py:1935]   Expert  6 |    135 | CPU
DEBUG 01-15 16:10:22.229318.229318 lmp.py:1935]   Expert 51 |    143 | CPU
DEBUG 01-15 16:10:22.229345.229345 lmp.py:1935]   Expert 17 |    148 | CPU
DEBUG 01-15 16:10:22.229610.229610 lmp.py:1935]   Expert  0 |    149 | CPU
DEBUG 01-15 16:10:22.229114.229114 lmp.py:1935]   Expert  5 |    153 | CPU
DEBUG 01-15 16:10:22.229572.229572 lmp.py:1935]   Expert 12 |    159 | CPU
DEBUG 01-15 16:10:22.229030.229030 lmp.py:1935]   Expert 57 |    159 | CPU
DEBUG 01-15 16:10:22.229487.229487 lmp.py:1935]   Expert 40 |    160 | CPU
DEBUG 01-15 16:10:22.229991.229991 lmp.py:1935]   Expert 55 |    160 | CPU
DEBUG 01-15 16:10:22.230019.230019 lmp.py:1935]   Expert 26 |    164 | CPU
DEBUG 01-15 16:10:22.230284.230284 lmp.py:1935]   Expert 59 |    164 | CPU
DEBUG 01-15 16:10:22.230311.230311 lmp.py:1935]   Expert 38 |    167 | CPU
DEBUG 01-15 16:10:22.230338.230338 lmp.py:1935]   Expert 46 |    167 | CPU
DEBUG 01-15 16:10:22.230273.230273 lmp.py:1935]   Expert 13 |    169 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230207.230207 lmp.py:1935]   Expert 58 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230142.230142 lmp.py:1935]   Expert 35 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230030.230030 lmp.py:1935]   Expert 30 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230680.230680 lmp.py:1935]   Expert 50 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230091.230091 lmp.py:1935]   Expert  7 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230503.230503 lmp.py:1935]   Expert 16 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230960.230960 lmp.py:1935]   Expert 15 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230180.230180 lmp.py:1935]   Expert 14 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230399.230399 lmp.py:1935]   Expert 32 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230618.230618 lmp.py:1935]   Expert  1 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230837.230837 lmp.py:1935]   Expert  3 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230249.230249 lmp.py:1935]   Expert  4 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230422.230422 lmp.py:1935]   Expert 34 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230833.230833 lmp.py:1935]   Expert 39 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230006.230006 lmp.py:1935]   Expert 28 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230940.230940 lmp.py:1935]   Expert 52 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230921.230921 lmp.py:1935]   Expert 25 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230140.230140 lmp.py:1935]   Expert 22 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230360.230360 lmp.py:1935]   Expert  2 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230579.230579 lmp.py:1935]   Expert 21 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230752.230752 lmp.py:1935]   Expert 41 |    281 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230163.230163 lmp.py:1935]   Expert 60 |    284 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230575.230575 lmp.py:1935]   Expert 63 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230509.230509 lmp.py:1935]   Expert 62 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230205.230205 lmp.py:1935]   Expert 29 |    297 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230948.230948 lmp.py:1935]   Expert 27 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230690.230690 lmp.py:1935]   Expert 37 |    323 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230671.230671 lmp.py:1935]   Expert  8 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230129.230129 lmp.py:1935]   Expert 53 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230540.230540 lmp.py:1935]   Expert 19 |    443 | GPU2(cuda:2)
DEBUG 01-15 16:10:22.230190.230190 lmp.py:1935]   Expert  9 |    613 | GPU1(cuda:1)
DEBUG 01-15 16:10:22.230932.230932 lmp.py:1937] 
DEBUG 01-15 16:10:22.230932.230932 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:22.230151.230151 lmp.py:1938]   CPU:   3955 tokens
DEBUG 01-15 16:10:22.230371.230371 lmp.py:1942]   cuda:1:   4201 tokens (16 experts)
DEBUG 01-15 16:10:22.230351.230351 lmp.py:1942]   cuda:2:   4132 tokens (16 experts)
DEBUG 01-15 16:10:22.230379.230379 lmp.py:1943]   Total GPU:   8333 tokens
DEBUG 01-15 16:10:22.230929.230929 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:22.230929.230929 lmp.py:1944] 
DEBUG 01-15 16:10:22.230393.230393 cuda_h.py:19] end experts_map_get cost 0.0020415782928466797 seconds
DEBUG 01-15 16:10:22.230389.230389 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:22.231529.231529 lmp.py:1953] 
DEBUG 01-15 16:10:22.231529.231529 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:22.231472.231472 cuda_h.py:19] end cpu_experts_submit cost 6.222724914550781e-05 seconds
DEBUG 01-15 16:10:22.231506.231506 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:22.231256.231256 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:22.231359.231359 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.1.gate_proj.weight': 0, 'model.layers.27.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.27.mlp.experts.32.gate_proj.weight': 0, 'model.layers.27.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.7.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.7.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.7.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(31667519488, 5767168, 0, 0), (31673286656, 5767168, 5767168, 0), (31661752320, 5767168, 11534336, 0), (31702122496, 5767168, 17301504, 0), (31707889664, 5767168, 23068672, 0), (31696355328, 5767168, 28835840, 0), (32255770624, 5767168, 34603008, 0), (32261537792, 5767168, 40370176, 0), (32250003456, 5767168, 46137344, 0), (32324976640, 5767168, 51904512, 0), (32330743808, 5767168, 57671680, 0), (32319209472, 5767168, 63438848, 0), (31788630016, 5767168, 69206016, 0), (31794397184, 5767168, 74973184, 0), (31782862848, 5767168, 80740352, 0), (31805931520, 5767168, 86507520, 0), (31811698688, 5767168, 92274688, 0), (31800164352, 5767168, 98041856, 0), (31875137536, 5767168, 103809024, 0), (31880904704, 5767168, 109576192, 0), (31869370368, 5767168, 115343360, 0), (31892439040, 5767168, 121110528, 0), (31898206208, 5767168, 126877696, 0), (31886671872, 5767168, 132644864, 0), (31927042048, 5767168, 138412032, 0), (31932809216, 5767168, 144179200, 0), (31921274880, 5767168, 149946368, 0), (32515293184, 5767168, 155713536, 0), (32521060352, 5767168, 161480704, 0), (32509526016, 5767168, 167247872, 0), (32549896192, 5767168, 173015040, 0), (32555663360, 5767168, 178782208, 0), (32544129024, 5767168, 184549376, 0), (32013549568, 5767168, 190316544, 0), (32019316736, 5767168, 196083712, 0), (32007782400, 5767168, 201850880, 0), (32030851072, 5767168, 207618048, 0), (32036618240, 5767168, 213385216, 0), (32025083904, 5767168, 219152384, 0), (32117358592, 5767168, 224919552, 0), (32123125760, 5767168, 230686720, 0), (32111591424, 5767168, 236453888, 0), (32688308224, 5767168, 242221056, 0), (32694075392, 5767168, 247988224, 0), (32682541056, 5767168, 253755392, 0), (32722911232, 5767168, 259522560, 0), (32728678400, 5767168, 265289728, 0), (32717144064, 5767168, 271056896, 0)], 2: [(32203866112, 5767168, 0, 0), (32209633280, 5767168, 5767168, 0), (32198098944, 5767168, 11534336, 0), (31684820992, 5767168, 17301504, 0), (31690588160, 5767168, 23068672, 0), (31679053824, 5767168, 28835840, 0), (32238469120, 5767168, 34603008, 0), (32244236288, 5767168, 40370176, 0), (32232701952, 5767168, 46137344, 0), (31719424000, 5767168, 51904512, 0), (31725191168, 5767168, 57671680, 0), (31713656832, 5767168, 63438848, 0), (32290373632, 5767168, 69206016, 0), (32296140800, 5767168, 74973184, 0), (32284606464, 5767168, 80740352, 0), (31771328512, 5767168, 86507520, 0), (31777095680, 5767168, 92274688, 0), (31765561344, 5767168, 98041856, 0), (32359579648, 5767168, 103809024, 0), (32365346816, 5767168, 109576192, 0), (32353812480, 5767168, 115343360, 0), (31909740544, 5767168, 121110528, 0), (31915507712, 5767168, 126877696, 0), (31903973376, 5767168, 132644864, 0), (31978946560, 5767168, 138412032, 0), (31984713728, 5767168, 144179200, 0), (31973179392, 5767168, 149946368, 0), (32567197696, 5767168, 155713536, 0), (32572964864, 5767168, 161480704, 0), (32561430528, 5767168, 167247872, 0), (32082755584, 5767168, 173015040, 0), (32088522752, 5767168, 178782208, 0), (32076988416, 5767168, 184549376, 0), (32653705216, 5767168, 190316544, 0), (32659472384, 5767168, 196083712, 0), (32647938048, 5767168, 201850880, 0), (32134660096, 5767168, 207618048, 0), (32140427264, 5767168, 213385216, 0), (32128892928, 5767168, 219152384, 0), (32151961600, 5767168, 224919552, 0), (32157728768, 5767168, 230686720, 0), (32146194432, 5767168, 236453888, 0), (32169263104, 5767168, 242221056, 0), (32175030272, 5767168, 247988224, 0), (32163495936, 5767168, 253755392, 0), (32740212736, 5767168, 259522560, 0), (32745979904, 5767168, 265289728, 0), (32734445568, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e344d46c0>, 2: <capsule object NULL at 0x7a4e34594960>}
DEBUG 01-15 16:10:22.232015.232015 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:22.232203.232203 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:22.232767.232767 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 46f9afdf-64be-4229-81c3-0d17509138ea
DEBUG 01-15 16:10:22.232073.232073 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:22.232027.232027 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:22.233971.233971 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:22.233605.233605 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 46f9afdf-64be-4229-81c3-0d17509138ea
DEBUG 01-15 16:10:22.234795.234795 cuda_h.py:19] end move_flatidxs cost 0.0008270740509033203 seconds
DEBUG 01-15 16:10:22.234923.234923 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:22.234994.234994 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003208160400390625 seconds
DEBUG 01-15 16:10:22.234778.234778 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:22.237780.237780 cuda_h.py:19] end restore2model cost 0.002599000930786133 seconds
DEBUG 01-15 16:10:22.237809.237809 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006059408187866211 seconds
DEBUG 01-15 16:10:22.237081.237081 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:22.237264.237264 cuda_h.py:19] end gpu_sexperts cost 0.0002772808074951172 seconds
DEBUG 01-15 16:10:22.237332.237332 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:22.237902.237902 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.238083.238083 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008027553558349609 seconds
DEBUG 01-15 16:10:22.238402.238402 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.238098.238098 cuda_h.py:19] end gpu_group_list cost 0.0001678466796875 seconds
DEBUG 01-15 16:10:22.238017.238017 cuda_h.py:19] end group_tensors cost 0.00473475456237793 seconds
DEBUG 01-15 16:10:22.239971.239971 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:22.239541.239541 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:22.240688.240688 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013628005981445312 seconds
DEBUG 01-15 16:10:22.240525.240525 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:22.241145.241145 cuda_h.py:19] end gpu_group_list cost 0.0002727508544921875 seconds
DEBUG 01-15 16:10:22.242103.242103 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:22.242165.242165 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 46f9afdf-64be-4229-81c3-0d17509138ea
DEBUG 01-15 16:10:22.242070.242070 cuda_h.py:19] end group pad cost 0.0034627914428710938 seconds
DEBUG 01-15 16:10:22.243767.243767 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:22.260024.260024 client.py:127] Model loaded
DEBUG 01-15 16:10:22.261560.261560 cuda_h.py:19] end wait_experts_multi_device cost 0.018718481063842773 seconds
DEBUG 01-15 16:10:22.261589.261589 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:22.262604.262604 cuda_h.py:19] end group_einsum cost 0.019768953323364258 seconds
DEBUG 01-15 16:10:22.263470.263470 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:22.266005.266005 cuda_h.py:19] end get_outputs_cpu1 cost 0.003657102584838867 seconds
DEBUG 01-15 16:10:22.267353.267353 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03457975387573242 seconds
DEBUG 01-15 16:10:22.267337.267337 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00667572021484375 seconds
DEBUG 01-15 16:10:22.267777.267777 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:22.269659.269659 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.269657.269657 cuda_h.py:19] end index_scatter cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:10:22.269627.269627 cuda_h.py:19] end cpuoutputsdeal cost 0.0016057491302490234 seconds
DEBUG 01-15 16:10:22.269034.269034 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:22.269843.269843 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.269451.269451 cuda_h.py:19] end gpu_group_tensor cost 0.0001361370086669922 seconds
DEBUG 01-15 16:10:22.269545.269545 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:22.270941.270941 cuda_h.py:19] end gpu_group_tensor cost 0.00012159347534179688 seconds
DEBUG 01-15 16:10:22.270315.270315 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.270574.270574 cuda_h.py:19] end gpu_group_einsum cost 0.0005626678466796875 seconds
DEBUG 01-15 16:10:22.270380.270380 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:22.271702.271702 cuda_h.py:19] end gpu_group_einsum cost 0.000457763671875 seconds
DEBUG 01-15 16:10:22.271110.271110 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.271505.271505 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.271738.271738 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019550323486328125 seconds
DEBUG 01-15 16:10:22.271554.271554 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.272696.272696 cuda_h.py:19] end concat_expert_out cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:10:22.272347.272347 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.272046.272046 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:22.272902.272902 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008349418640136719 seconds
DEBUG 01-15 16:10:22.272137.272137 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:22.272927.272927 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:22.272283.272283 cuda_h.py:19] end all_expert_outputs_slices cost 0.000125885009765625 seconds
DEBUG 01-15 16:10:22.272417.272417 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:22.272148.272148 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:22.272845.272845 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:22.272053.272053 cuda_h.py:19] end index_scatter cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:22.273909.273909 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004627704620361328 seconds
DEBUG 01-15 16:10:22.273520.273520 cuda_h.py:19] end gpu_experts_multi_device cost 0.035469770431518555 seconds
DEBUG 01-15 16:10:22.273192.273192 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.045066118240356445 seconds
DEBUG 01-15 16:10:22.273402.273402 cuda_h.py:19] end prefill_layer cost 0.0489656925201416 seconds
DEBUG 01-15 16:10:22.273133.273133 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:10:22.273412.273412 cuda_h.py:19] end prefill cost 2.5601401329040527 seconds
DEBUG 01-15 16:10:24.489707.489707 cuda_h.py:10] start generate_input_ids
generate input ids cost 0.09403204917907715 s
DEBUG 01-15 16:10:24.845951.845951 cuda_h.py:19] end generate_input_ids cost 0.3552567958831787 seconds
DEBUG 01-15 16:10:24.845037.845037 cuda_h.py:10] start init_cache
DEBUG 01-15 16:10:24.845896.845896 cuda_h.py:19] end init_cache cost 9.72747802734375e-05 seconds
DEBUG 01-15 16:10:27.213586.213586 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:10:27.215542.215542 cuda_h.py:19] end init_meta_layer cost 1.1920928955078125e-05 seconds
DEBUG 01-15 16:10:27.215888.215888 cuda_h.py:10] start init_weights
DEBUG 01-15 16:10:27.215737.215737 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:27.215400.215400 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:27.215550.215550 cuda_h.py:19] end allocate_cuda_memory cost 0.000675201416015625 seconds
DEBUG 01-15 16:10:27.216115.216115 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:27.216586.216586 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:27.216025.216025 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:27.216582.216582 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2c0c7b2c-2494-4321-9295-fe5291e55e78
DEBUG 01-15 16:10:27.216982.216982 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:27.217034.217034 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2c0c7b2c-2494-4321-9295-fe5291e55e78
DEBUG 01-15 16:10:27.217731.217731 cuda_h.py:19] end load_into_gpu_async cost 0.0013704299926757812 seconds
DEBUG 01-15 16:10:27.217812.217812 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:27.217934.217934 cuda_h.py:19] end restore_tensors2 cost 5.841255187988281e-05 seconds
DEBUG 01-15 16:10:27.217544.217544 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002328634262084961 seconds
DEBUG 01-15 16:10:27.217909.217909 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:27.217816.217816 cuda_h.py:19] end restore2model cost 0.0001537799835205078 seconds
INFO 01-15 16:10:27.217525.217525 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2c0c7b2c-2494-4321-9295-fe5291e55e78
INFO 01-15 16:10:27.294533.294533 client.py:127] Model loaded
DEBUG 01-15 16:10:27.294538.294538 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:10:27.294269.294269 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:27.295091.295091 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:27.295510.295510 cuda_h.py:19] end allocate_cuda_memory cost 0.0007221698760986328 seconds
DEBUG 01-15 16:10:27.296901.296901 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:27.296660.296660 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:27.296122.296122 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:27.296055.296055 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 988bfa72-596b-48b6-aaa7-340face89f3d
DEBUG 01-15 16:10:27.296538.296538 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:27.297807.297807 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 988bfa72-596b-48b6-aaa7-340face89f3d
DEBUG 01-15 16:10:27.297354.297354 cuda_h.py:19] end load_into_gpu_async cost 0.0015666484832763672 seconds
DEBUG 01-15 16:10:27.297780.297780 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:27.298039.298039 cuda_h.py:19] end restore_tensors2 cost 0.00015306472778320312 seconds
DEBUG 01-15 16:10:27.298532.298532 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0031557083129882812 seconds
INFO 01-15 16:10:27.298911.298911 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 988bfa72-596b-48b6-aaa7-340face89f3d
INFO 01-15 16:10:27.313979.313979 client.py:127] Model loaded
DEBUG 01-15 16:10:27.313167.313167 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:27.314961.314961 cuda_h.py:19] end restore2model cost 0.0005340576171875 seconds
DEBUG 01-15 16:10:27.314970.314970 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.019477367401123047 seconds
DEBUG 01-15 16:10:27.314846.314846 cuda_h.py:19] end init_weights cost 0.09899449348449707 seconds
DEBUG 01-15 16:10:27.314039.314039 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:10:28.050425.050425 cuda_h.py:19] end copy_emodel cost 0.7365245819091797 seconds
DEBUG 01-15 16:10:28.051816.051816 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:10:28.052689.052689 cuda_h.py:19] end init_inputs_tokens cost 0.0004916191101074219 seconds
DEBUG 01-15 16:10:28.052757.052757 cuda_h.py:10] start prefill
DEBUG 01-15 16:10:28.052665.052665 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.052507.052507 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:10:28.052249.052249 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:28.052913.052913 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:28.052948.052948 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 3.266334533691406e-05 seconds
DEBUG 01-15 16:10:28.052340.052340 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 8.034706115722656e-05 seconds
DEBUG 01-15 16:10:28.052890.052890 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.052157.052157 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.052458.052458 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.052371.052371 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.052698.052698 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.053700.053700 cuda_h.py:19] end allocate_cuda_memory cost 0.00020313262939453125 seconds
DEBUG 01-15 16:10:28.053517.053517 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.053810.053810 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.053546.053546 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.053402.053402 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7385151f-ba29-415c-b6bf-af5e5bf1f9e0
DEBUG 01-15 16:10:28.053074.053074 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.053152.053152 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.054594.054594 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7385151f-ba29-415c-b6bf-af5e5bf1f9e0
DEBUG 01-15 16:10:28.054967.054967 cuda_h.py:19] end load_into_gpu_async cost 0.0013432502746582031 seconds
DEBUG 01-15 16:10:28.054630.054630 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.054648.054648 cuda_h.py:19] end restore_tensors2 cost 8.249282836914062e-05 seconds
DEBUG 01-15 16:10:28.054133.054133 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0019214153289794922 seconds
INFO 01-15 16:10:28.054843.054843 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7385151f-ba29-415c-b6bf-af5e5bf1f9e0
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.058850.058850 cuda_h.py:19] end self_attn cost 0.004365682601928711 seconds
DEBUG 01-15 16:10:28.058573.058573 cuda_h.py:19] end iln_self_attn_paln cost 0.006126880645751953 seconds
DEBUG 01-15 16:10:28.058403.058403 cuda_h.py:10] start dense_mlp
INFO 01-15 16:10:28.062109.062109 client.py:127] Model loaded
DEBUG 01-15 16:10:28.062105.062105 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.062026.062026 cuda_h.py:19] end restore2model cost 0.0005578994750976562 seconds
DEBUG 01-15 16:10:28.062068.062068 cuda_h.py:19] end sllm_worker_task cost 0.009929418563842773 seconds
DEBUG 01-15 16:10:28.062502.062502 cuda_h.py:19] end dense_mlp cost 0.0040895938873291016 seconds
DEBUG 01-15 16:10:28.062022.062022 cuda_h.py:19] end prefill_layer cost 0.010628938674926758 seconds
DEBUG 01-15 16:10:28.063401.063401 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:10:28.063573.063573 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.063654.063654 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:10:28.063780.063780 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:28.063576.063576 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:28.063875.063875 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 2.0503997802734375e-05 seconds
DEBUG 01-15 16:10:28.063386.063386 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:10:28.063221.063221 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.063323.063323 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.063305.063305 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.063009.063009 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.063667.063667 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.063894.063894 cuda_h.py:19] end allocate_cuda_memory cost 0.00017452239990234375 seconds
DEBUG 01-15 16:10:28.063943.063943 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.063946.063946 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.063888.063888 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.063579.063579 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bd1171d3-fb06-4e9a-8ce9-76f673d0a665
DEBUG 01-15 16:10:28.064775.064775 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.064105.064105 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.065847.065847 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bd1171d3-fb06-4e9a-8ce9-76f673d0a665
DEBUG 01-15 16:10:28.065982.065982 cuda_h.py:19] end load_into_gpu_async cost 0.0012879371643066406 seconds
DEBUG 01-15 16:10:28.065520.065520 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.065306.065306 cuda_h.py:19] end restore_tensors2 cost 7.796287536621094e-05 seconds
DEBUG 01-15 16:10:28.065248.065248 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.001958608627319336 seconds
INFO 01-15 16:10:28.065198.065198 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bd1171d3-fb06-4e9a-8ce9-76f673d0a665
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.067959.067959 cuda_h.py:19] end self_attn cost 0.003306865692138672 seconds
DEBUG 01-15 16:10:28.067717.067717 cuda_h.py:19] end iln_self_attn_paln cost 0.004690647125244141 seconds
DEBUG 01-15 16:10:28.067401.067401 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:10:28.067733.067733 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.068456.068456 cuda_h.py:19] end gate cost 0.0007812976837158203 seconds
DEBUG 01-15 16:10:28.068338.068338 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.069819.069819 lmp.py:1912] 
DEBUG 01-15 16:10:28.069819.069819 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.069675.069675 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.069993.069993 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.069974.069974 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.069147.069147 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.069459.069459 lmp.py:1917] 
DEBUG 01-15 16:10:28.069459.069459 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.069294.069294 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.069374.069374 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:10:28.069494.069494 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:10:28.069137.069137 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:28.069257.069257 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:10:28.069900.069900 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:10:28.069735.069735 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:10:28.069570.069570 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:10:28.069405.069405 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:10:28.069002.069002 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:10:28.069122.069122 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:10:28.069765.069765 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:10:28.069646.069646 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:10:28.069289.069289 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:10:28.069932.069932 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:10:28.069098.069098 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:10:28.069887.069887 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:10:28.069768.069768 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:10:28.069173.069173 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:10:28.069339.069339 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:10:28.069697.069697 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:10:28.069532.069532 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:10:28.069367.069367 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:10:28.069249.069249 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:10:28.069892.069892 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:10:28.069296.069296 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:10:28.069939.069939 lmp.py:1935]   Expert 34 |    161 | CPU
DEBUG 01-15 16:10:28.069105.069105 lmp.py:1935]   Expert 35 |    164 | CPU
DEBUG 01-15 16:10:28.069271.069271 lmp.py:1935]   Expert 36 |    168 | CPU
DEBUG 01-15 16:10:28.069676.069676 lmp.py:1935]   Expert 11 |    170 | CPU
DEBUG 01-15 16:10:28.069080.069080 lmp.py:1935]   Expert 17 |    170 | CPU
DEBUG 01-15 16:10:28.069485.069485 lmp.py:1935]   Expert 59 |    174 | CPU
DEBUG 01-15 16:10:28.069559.069559 lmp.py:1935]   Expert 10 |    180 | CPU
DEBUG 01-15 16:10:28.069586.069586 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.069136.069136 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.069878.069878 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.069475.069475 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.069356.069356 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070953.070953 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070073.070073 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070670.070670 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070551.070551 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070671.070671 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070029.070029 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070626.070626 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070745.070745 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070534.070534 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070608.070608 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070158.070158 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070516.070516 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070636.070636 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070279.070279 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070160.070160 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070042.070042 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070685.070685 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070566.070566 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070209.070209 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070329.070329 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070641.070641 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070714.070714 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070503.070503 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070861.070861 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070981.070981 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070578.070578 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.070651.070651 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.070056.070056 lmp.py:1937] 
DEBUG 01-15 16:10:28.070056.070056 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.070699.070699 lmp.py:1938]   CPU:   4059 tokens
DEBUG 01-15 16:10:28.070819.070819 lmp.py:1942]   cuda:1:   4114 tokens (16 experts)
DEBUG 01-15 16:10:28.070462.070462 lmp.py:1942]   cuda:2:   4115 tokens (16 experts)
DEBUG 01-15 16:10:28.070204.070204 lmp.py:1943]   Total GPU:   8229 tokens
DEBUG 01-15 16:10:28.070324.070324 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.070324.070324 lmp.py:1944] 
DEBUG 01-15 16:10:28.070643.070643 cuda_h.py:19] end experts_map_get cost 0.0017392635345458984 seconds
DEBUG 01-15 16:10:28.070360.070360 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.070593.070593 lmp.py:1953] 
DEBUG 01-15 16:10:28.070593.070593 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.070760.070760 cuda_h.py:19] end cpu_experts_submit cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:10:28.070980.070980 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.070200.070200 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.071197.071197 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.1.mlp.experts.33.gate_proj.weight': 0, 'model.layers.1.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.14.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.14.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.14.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.15.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.15.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.15.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.21.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.21.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.21.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.23.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.23.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.23.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.56.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3535273984, 5767168, 34603008, 0), (3541041152, 5767168, 40370176, 0), (3529506816, 5767168, 46137344, 0), (3016228864, 5767168, 51904512, 0), (3021996032, 5767168, 57671680, 0), (3010461696, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3690987520, 5767168, 86507520, 0), (3696754688, 5767168, 92274688, 0), (3685220352, 5767168, 98041856, 0), (3362258944, 5767168, 103809024, 0), (3368026112, 5767168, 109576192, 0), (3356491776, 5767168, 115343360, 0), (3725590528, 5767168, 121110528, 0), (3731357696, 5767168, 126877696, 0), (3719823360, 5767168, 132644864, 0), (3189243904, 5767168, 138412032, 0), (3195011072, 5767168, 144179200, 0), (3183476736, 5767168, 149946368, 0), (3206545408, 5767168, 155713536, 0), (3212312576, 5767168, 161480704, 0), (3200778240, 5767168, 167247872, 0), (3275751424, 5767168, 173015040, 0), (3281518592, 5767168, 178782208, 0), (3269984256, 5767168, 184549376, 0), (3846701056, 5767168, 190316544, 0), (3852468224, 5767168, 196083712, 0), (3840933888, 5767168, 201850880, 0), (3310354432, 5767168, 207618048, 0), (3316121600, 5767168, 213385216, 0), (3304587264, 5767168, 219152384, 0), (3915907072, 5767168, 224919552, 0), (3921674240, 5767168, 230686720, 0), (3910139904, 5767168, 236453888, 0), (3379560448, 5767168, 242221056, 0), (3385327616, 5767168, 247988224, 0), (3373793280, 5767168, 253755392, 0), (3950510080, 5767168, 259522560, 0), (3956277248, 5767168, 265289728, 0), (3944742912, 5767168, 271056896, 0)], 2: [(3431464960, 5767168, 0, 0), (3437232128, 5767168, 5767168, 0), (3425697792, 5767168, 11534336, 0), (2895118336, 5767168, 17301504, 0), (2900885504, 5767168, 23068672, 0), (2889351168, 5767168, 28835840, 0), (2929721344, 5767168, 34603008, 0), (2935488512, 5767168, 40370176, 0), (2923954176, 5767168, 46137344, 0), (2947022848, 5767168, 51904512, 0), (2952790016, 5767168, 57671680, 0), (2941255680, 5767168, 63438848, 0), (3552575488, 5767168, 69206016, 0), (3558342656, 5767168, 74973184, 0), (3546808320, 5767168, 80740352, 0), (2998927360, 5767168, 86507520, 0), (3004694528, 5767168, 92274688, 0), (2993160192, 5767168, 98041856, 0), (3587178496, 5767168, 103809024, 0), (3592945664, 5767168, 109576192, 0), (3581411328, 5767168, 115343360, 0), (3639083008, 5767168, 121110528, 0), (3644850176, 5767168, 126877696, 0), (3633315840, 5767168, 132644864, 0), (3656384512, 5767168, 138412032, 0), (3662151680, 5767168, 144179200, 0), (3650617344, 5767168, 149946368, 0), (3102736384, 5767168, 155713536, 0), (3108503552, 5767168, 161480704, 0), (3096969216, 5767168, 167247872, 0), (3137339392, 5767168, 173015040, 0), (3143106560, 5767168, 178782208, 0), (3131572224, 5767168, 184549376, 0), (3120037888, 5767168, 190316544, 0), (3125805056, 5767168, 196083712, 0), (3114270720, 5767168, 201850880, 0), (3777495040, 5767168, 207618048, 0), (3783262208, 5767168, 213385216, 0), (3771727872, 5767168, 219152384, 0), (3223846912, 5767168, 224919552, 0), (3229614080, 5767168, 230686720, 0), (3218079744, 5767168, 236453888, 0), (3258449920, 5767168, 242221056, 0), (3264217088, 5767168, 247988224, 0), (3252682752, 5767168, 253755392, 0), (3829399552, 5767168, 259522560, 0), (3835166720, 5767168, 265289728, 0), (3823632384, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c248570>, 2: <capsule object NULL at 0x7a4f2c2483c0>}
DEBUG 01-15 16:10:28.071189.071189 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.071310.071310 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.071350.071350 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9d529ff0-2c1c-4c07-997f-ad14d70de845
DEBUG 01-15 16:10:28.072581.072581 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:28.072061.072061 client.py:127] Model loaded
DEBUG 01-15 16:10:28.072859.072859 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.073194.073194 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:28.073372.073372 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.073874.073874 cuda_h.py:19] end restore2model cost 0.0007791519165039062 seconds
INFO 01-15 16:10:28.073414.073414 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9d529ff0-2c1c-4c07-997f-ad14d70de845
DEBUG 01-15 16:10:28.073768.073768 cuda_h.py:19] end sllm_worker_task cost 0.010595560073852539 seconds
DEBUG 01-15 16:10:28.074789.074789 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0035076141357421875 seconds
DEBUG 01-15 16:10:28.074119.074119 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.074583.074583 cuda_h.py:19] end move_flatidxs cost 0.001093149185180664 seconds
DEBUG 01-15 16:10:28.074572.074572 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.077419.077419 cuda_h.py:19] end restore2model cost 0.0025942325592041016 seconds
DEBUG 01-15 16:10:28.077415.077415 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006440639495849609 seconds
DEBUG 01-15 16:10:28.077687.077687 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.077525.077525 cuda_h.py:19] end gpu_sexperts cost 0.00026917457580566406 seconds
DEBUG 01-15 16:10:28.077593.077593 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.077223.077223 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4781951904296875e-05 seconds
DEBUG 01-15 16:10:28.077443.077443 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.077782.077782 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.078168.078168 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008130073547363281 seconds
DEBUG 01-15 16:10:28.078680.078680 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.078342.078342 cuda_h.py:19] end gpu_group_list cost 0.00017833709716796875 seconds
DEBUG 01-15 16:10:28.080760.080760 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.080653.080653 cuda_h.py:19] end group_tensors cost 0.005526542663574219 seconds
DEBUG 01-15 16:10:28.081662.081662 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.082609.082609 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0021181106567382812 seconds
DEBUG 01-15 16:10:28.082308.082308 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.082220.082220 cuda_h.py:19] end gpu_group_list cost 0.0002741813659667969 seconds
DEBUG 01-15 16:10:28.083120.083120 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.083289.083289 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9d529ff0-2c1c-4c07-997f-ad14d70de845
DEBUG 01-15 16:10:28.085170.085170 cuda_h.py:19] end group pad cost 0.004426240921020508 seconds
DEBUG 01-15 16:10:28.085920.085920 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.100932.100932 client.py:127] Model loaded
DEBUG 01-15 16:10:28.100142.100142 cuda_h.py:19] end wait_experts_multi_device cost 0.01698613166809082 seconds
DEBUG 01-15 16:10:28.101958.101958 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.109587.109587 cuda_h.py:19] end group_einsum cost 0.02413797378540039 seconds
DEBUG 01-15 16:10:28.110977.110977 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.116396.116396 cuda_h.py:19] end get_outputs_cpu1 cost 0.0060808658599853516 seconds
DEBUG 01-15 16:10:28.117488.117488 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.043686628341674805 seconds
DEBUG 01-15 16:10:28.117447.117447 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01631450653076172 seconds
DEBUG 01-15 16:10:28.117013.117013 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.118283.118283 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.118945.118945 cuda_h.py:19] end index_scatter cost 0.00010061264038085938 seconds
DEBUG 01-15 16:10:28.119783.119783 cuda_h.py:19] end cpuoutputsdeal cost 0.0015816688537597656 seconds
DEBUG 01-15 16:10:28.119553.119553 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.119409.119409 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.119034.119034 cuda_h.py:19] end gpu_group_tensor cost 0.0006310939788818359 seconds
DEBUG 01-15 16:10:28.119162.119162 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.120370.120370 cuda_h.py:19] end gpu_group_tensor cost 0.0006387233734130859 seconds
DEBUG 01-15 16:10:28.120408.120408 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.121227.121227 cuda_h.py:19] end gpu_group_einsum cost 0.0006911754608154297 seconds
DEBUG 01-15 16:10:28.121251.121251 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.122179.122179 cuda_h.py:19] end gpu_group_einsum cost 0.0009469985961914062 seconds
DEBUG 01-15 16:10:28.122527.122527 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.122925.122925 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.123264.123264 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016355514526367188 seconds
DEBUG 01-15 16:10:28.123881.123881 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.123465.123465 cuda_h.py:19] end concat_expert_out cost 0.0002186298370361328 seconds
DEBUG 01-15 16:10:28.123488.123488 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.123445.123445 cuda_h.py:19] end index_scatter cost 6.604194641113281e-05 seconds
DEBUG 01-15 16:10:28.123010.123010 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011150836944580078 seconds
DEBUG 01-15 16:10:28.124423.124423 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.124889.124889 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.124630.124630 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001277923583984375 seconds
DEBUG 01-15 16:10:28.124909.124909 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.124878.124878 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:28.124172.124172 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.124732.124732 cuda_h.py:19] end index_scatter cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:10:28.124541.124541 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005042552947998047 seconds
DEBUG 01-15 16:10:28.124067.124067 cuda_h.py:19] end gpu_experts_multi_device cost 0.046887874603271484 seconds
DEBUG 01-15 16:10:28.124076.124076 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.056719064712524414 seconds
DEBUG 01-15 16:10:28.125074.125074 cuda_h.py:19] end prefill_layer cost 0.06199836730957031 seconds
DEBUG 01-15 16:10:28.125679.125679 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:10:28.125859.125859 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.125515.125515 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:10:28.125125.125125 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:28.125881.125881 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:28.125115.125115 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 3.62396240234375e-05 seconds
DEBUG 01-15 16:10:28.125110.125110 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:10:28.125859.125859 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.125444.125444 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.125321.125321 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.125636.125636 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.125379.125379 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.125897.125897 cuda_h.py:19] end allocate_cuda_memory cost 0.00020241737365722656 seconds
DEBUG 01-15 16:10:28.125920.125920 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.126782.126782 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.126679.126679 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.126249.126249 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6cda42dc-d4fb-4338-a848-a634c16c8531
DEBUG 01-15 16:10:28.126399.126399 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.126090.126090 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.127518.127518 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6cda42dc-d4fb-4338-a848-a634c16c8531
DEBUG 01-15 16:10:28.127407.127407 cuda_h.py:19] end load_into_gpu_async cost 0.0016548633575439453 seconds
DEBUG 01-15 16:10:28.127210.127210 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.127174.127174 cuda_h.py:19] end restore_tensors2 cost 8.20159912109375e-05 seconds
DEBUG 01-15 16:10:28.127506.127506 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022230148315429688 seconds
INFO 01-15 16:10:28.127363.127363 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6cda42dc-d4fb-4338-a848-a634c16c8531
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.130687.130687 cuda_h.py:19] end self_attn cost 0.0035262107849121094 seconds
DEBUG 01-15 16:10:28.130717.130717 cuda_h.py:19] end iln_self_attn_paln cost 0.005019664764404297 seconds
DEBUG 01-15 16:10:28.130110.130110 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:10:28.130774.130774 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.131982.131982 cuda_h.py:19] end gate cost 0.0006399154663085938 seconds
DEBUG 01-15 16:10:28.131480.131480 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.131100.131100 lmp.py:1912] 
DEBUG 01-15 16:10:28.131100.131100 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.131764.131764 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.131890.131890 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.131156.131156 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.131037.131037 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.131442.131442 lmp.py:1917] 
DEBUG 01-15 16:10:28.131442.131442 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.131562.131562 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.131403.131403 lmp.py:1935]   Expert 58 |     51 | CPU
DEBUG 01-15 16:10:28.131762.131762 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:10:28.131643.131643 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:28.131524.131524 lmp.py:1935]   Expert 17 |     84 | CPU
DEBUG 01-15 16:10:28.131406.131406 lmp.py:1935]   Expert 24 |     87 | CPU
DEBUG 01-15 16:10:28.131049.131049 lmp.py:1935]   Expert  0 |     88 | CPU
DEBUG 01-15 16:10:28.131407.131407 lmp.py:1935]   Expert 28 |    105 | CPU
DEBUG 01-15 16:10:28.131527.131527 lmp.py:1935]   Expert 34 |    115 | CPU
DEBUG 01-15 16:10:28.131931.131931 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:10:28.131859.131859 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:10:28.131025.131025 lmp.py:1935]   Expert  9 |    130 | CPU
DEBUG 01-15 16:10:28.131191.131191 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:10:28.131881.131881 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:10:28.131808.131808 lmp.py:1935]   Expert 23 |    135 | CPU
DEBUG 01-15 16:10:28.131736.131736 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:28.131141.131141 lmp.py:1935]   Expert 30 |    144 | CPU
DEBUG 01-15 16:10:28.132022.132022 lmp.py:1935]   Expert 45 |    146 | CPU
DEBUG 01-15 16:10:28.132665.132665 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:10:28.132070.132070 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:10:28.132527.132527 lmp.py:1935]   Expert  1 |    153 | CPU
DEBUG 01-15 16:10:28.132978.132978 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:10:28.132952.132952 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:10:28.132688.132688 lmp.py:1935]   Expert 29 |    161 | CPU
DEBUG 01-15 16:10:28.132662.132662 lmp.py:1935]   Expert 25 |    164 | CPU
DEBUG 01-15 16:10:28.132874.132874 lmp.py:1935]   Expert 54 |    167 | CPU
DEBUG 01-15 16:10:28.132610.132610 lmp.py:1935]   Expert  6 |    171 | CPU
DEBUG 01-15 16:10:28.132822.132822 lmp.py:1935]   Expert 49 |    171 | CPU
DEBUG 01-15 16:10:28.132558.132558 lmp.py:1935]   Expert 48 |    172 | CPU
DEBUG 01-15 16:10:28.132294.132294 lmp.py:1935]   Expert 12 |    176 | CPU
DEBUG 01-15 16:10:28.132268.132268 lmp.py:1935]   Expert 35 |    176 | CPU
DEBUG 01-15 16:10:28.132765.132765 lmp.py:1935]   Expert 37 |    176 | CPU
DEBUG 01-15 16:10:28.132739.132739 lmp.py:1935]   Expert 60 |    186 | CPU
DEBUG 01-15 16:10:28.132382.132382 lmp.py:1935]   Expert 13 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132263.132263 lmp.py:1935]   Expert 33 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132906.132906 lmp.py:1935]   Expert 53 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132549.132549 lmp.py:1935]   Expert 10 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132431.132431 lmp.py:1935]   Expert 16 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132074.132074 lmp.py:1935]   Expert 21 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132478.132478 lmp.py:1935]   Expert 40 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132711.132711 lmp.py:1935]   Expert 43 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132400.132400 lmp.py:1935]   Expert 38 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132567.132567 lmp.py:1935]   Expert  5 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132494.132494 lmp.py:1935]   Expert 44 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132422.132422 lmp.py:1935]   Expert 50 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132111.132111 lmp.py:1935]   Expert 52 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132469.132469 lmp.py:1935]   Expert 19 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132874.132874 lmp.py:1935]   Expert 41 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132755.132755 lmp.py:1935]   Expert  4 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132398.132398 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132280.132280 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132446.132446 lmp.py:1935]   Expert 56 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132374.132374 lmp.py:1935]   Expert 31 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132063.132063 lmp.py:1935]   Expert 20 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132991.132991 lmp.py:1935]   Expert 39 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132157.132157 lmp.py:1935]   Expert 22 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132085.132085 lmp.py:1935]   Expert  2 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132774.132774 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132702.132702 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132868.132868 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132272.132272 lmp.py:1935]   Expert 18 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132677.132677 lmp.py:1935]   Expert 14 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132081.132081 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132248.132248 lmp.py:1935]   Expert 11 |    389 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.132414.132414 lmp.py:1935]   Expert 61 |    461 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.132149.132149 lmp.py:1937] 
DEBUG 01-15 16:10:28.132149.132149 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.132362.132362 lmp.py:1938]   CPU:   4339 tokens
DEBUG 01-15 16:10:28.132289.132289 lmp.py:1942]   cuda:1:   3974 tokens (16 experts)
DEBUG 01-15 16:10:28.132979.132979 lmp.py:1942]   cuda:2:   3975 tokens (16 experts)
DEBUG 01-15 16:10:28.132191.132191 lmp.py:1943]   Total GPU:   7949 tokens
DEBUG 01-15 16:10:28.132450.132450 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.132450.132450 lmp.py:1944] 
DEBUG 01-15 16:10:28.132861.132861 cuda_h.py:19] end experts_map_get cost 0.0016548633575439453 seconds
DEBUG 01-15 16:10:28.133565.133565 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.133891.133891 lmp.py:1953] 
DEBUG 01-15 16:10:28.133891.133891 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.133343.133343 cuda_h.py:19] end cpu_experts_submit cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:28.133562.133562 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.133676.133676 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.134479.134479 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.33.gate_proj.weight': 0, 'model.layers.2.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.2.mlp.experts.2.gate_proj.weight': 0, 'model.layers.2.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.38.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.38.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.38.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.41.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.41.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.41.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.50.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.50.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.50.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.20.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.20.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.20.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(4538761216, 5767168, 0, 0), (4544528384, 5767168, 5767168, 0), (4532994048, 5767168, 11534336, 0), (4037017600, 5767168, 17301504, 0), (4042784768, 5767168, 23068672, 0), (4031250432, 5767168, 28835840, 0), (4054319104, 5767168, 34603008, 0), (4060086272, 5767168, 40370176, 0), (4048551936, 5767168, 46137344, 0), (4642570240, 5767168, 51904512, 0), (4648337408, 5767168, 57671680, 0), (4636803072, 5767168, 63438848, 0), (4659871744, 5767168, 69206016, 0), (4665638912, 5767168, 74973184, 0), (4654104576, 5767168, 80740352, 0), (4694474752, 5767168, 86507520, 0), (4700241920, 5767168, 92274688, 0), (4688707584, 5767168, 98041856, 0), (4729077760, 5767168, 103809024, 0), (4734844928, 5767168, 109576192, 0), (4723310592, 5767168, 115343360, 0), (4210032640, 5767168, 121110528, 0), (4215799808, 5767168, 126877696, 0), (4204265472, 5767168, 132644864, 0), (4296540160, 5767168, 138412032, 0), (4302307328, 5767168, 144179200, 0), (4290772992, 5767168, 149946368, 0), (4331143168, 5767168, 155713536, 0), (4336910336, 5767168, 161480704, 0), (4325376000, 5767168, 167247872, 0), (4348444672, 5767168, 173015040, 0), (4354211840, 5767168, 178782208, 0), (4342677504, 5767168, 184549376, 0), (4884791296, 5767168, 190316544, 0), (4890558464, 5767168, 196083712, 0), (4879024128, 5767168, 201850880, 0), (4936695808, 5767168, 207618048, 0), (4942462976, 5767168, 213385216, 0), (4930928640, 5767168, 219152384, 0), (4988600320, 5767168, 224919552, 0), (4994367488, 5767168, 230686720, 0), (4982833152, 5767168, 236453888, 0), (5023203328, 5767168, 242221056, 0), (5028970496, 5767168, 247988224, 0), (5017436160, 5767168, 253755392, 0), (5057806336, 5767168, 259522560, 0), (5063573504, 5767168, 265289728, 0), (5052039168, 5767168, 271056896, 0)], 2: [(4002414592, 5767168, 0, 0), (4008181760, 5767168, 5767168, 0), (3996647424, 5767168, 11534336, 0), (4625268736, 5767168, 17301504, 0), (4631035904, 5767168, 23068672, 0), (4619501568, 5767168, 28835840, 0), (4677173248, 5767168, 34603008, 0), (4682940416, 5767168, 40370176, 0), (4671406080, 5767168, 46137344, 0), (4140826624, 5767168, 51904512, 0), (4146593792, 5767168, 57671680, 0), (4135059456, 5767168, 63438848, 0), (4158128128, 5767168, 69206016, 0), (4163895296, 5767168, 74973184, 0), (4152360960, 5767168, 80740352, 0), (4711776256, 5767168, 86507520, 0), (4717543424, 5767168, 92274688, 0), (4706009088, 5767168, 98041856, 0), (4192731136, 5767168, 103809024, 0), (4198498304, 5767168, 109576192, 0), (4186963968, 5767168, 115343360, 0), (4763680768, 5767168, 121110528, 0), (4769447936, 5767168, 126877696, 0), (4757913600, 5767168, 132644864, 0), (4780982272, 5767168, 138412032, 0), (4786749440, 5767168, 144179200, 0), (4775215104, 5767168, 149946368, 0), (4244635648, 5767168, 155713536, 0), (4250402816, 5767168, 161480704, 0), (4238868480, 5767168, 167247872, 0), (4279238656, 5767168, 173015040, 0), (4285005824, 5767168, 178782208, 0), (4273471488, 5767168, 184549376, 0), (4832886784, 5767168, 190316544, 0), (4838653952, 5767168, 196083712, 0), (4827119616, 5767168, 201850880, 0), (4313841664, 5767168, 207618048, 0), (4319608832, 5767168, 213385216, 0), (4308074496, 5767168, 219152384, 0), (4867489792, 5767168, 224919552, 0), (4873256960, 5767168, 230686720, 0), (4861722624, 5767168, 236453888, 0), (4919394304, 5767168, 242221056, 0), (4925161472, 5767168, 247988224, 0), (4913627136, 5767168, 253755392, 0), (4504158208, 5767168, 259522560, 0), (4509925376, 5767168, 265289728, 0), (4498391040, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a5b06d23450>, 2: <capsule object NULL at 0x7a4e346486c0>}
DEBUG 01-15 16:10:28.134714.134714 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.134458.134458 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.134353.134353 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3051aeb9-7136-4c3c-bc34-8e63052a7d9d
DEBUG 01-15 16:10:28.134796.134796 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:28.135425.135425 client.py:127] Model loaded
DEBUG 01-15 16:10:28.135930.135930 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.135910.135910 cuda_h.py:19] end restore2model cost 0.0003361701965332031 seconds
DEBUG 01-15 16:10:28.135679.135679 cuda_h.py:19] end sllm_worker_task cost 0.010013341903686523 seconds
DEBUG 01-15 16:10:28.135652.135652 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:28.135533.135533 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3051aeb9-7136-4c3c-bc34-8e63052a7d9d
DEBUG 01-15 16:10:28.135512.135512 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.136787.136787 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003066539764404297 seconds
DEBUG 01-15 16:10:28.136902.136902 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.137628.137628 cuda_h.py:19] end move_flatidxs cost 0.0010402202606201172 seconds
DEBUG 01-15 16:10:28.137696.137696 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.138593.138593 cuda_h.py:19] end restore2model cost 0.0025811195373535156 seconds
DEBUG 01-15 16:10:28.139635.139635 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00589299201965332 seconds
DEBUG 01-15 16:10:28.139166.139166 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.139772.139772 cuda_h.py:19] end gpu_sexperts cost 0.00027060508728027344 seconds
DEBUG 01-15 16:10:28.139648.139648 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.139563.139563 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5020370483398438e-05 seconds
DEBUG 01-15 16:10:28.139504.139504 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.139161.139161 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.140057.140057 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008032321929931641 seconds
DEBUG 01-15 16:10:28.140529.140529 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.140059.140059 cuda_h.py:19] end gpu_group_list cost 0.00018548965454101562 seconds
DEBUG 01-15 16:10:28.141680.141680 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.142507.142507 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008985996246337891 seconds
DEBUG 01-15 16:10:28.142132.142132 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.142424.142424 cuda_h.py:19] end gpu_group_list cost 0.00018525123596191406 seconds
DEBUG 01-15 16:10:28.142821.142821 cuda_h.py:19] end group_tensors cost 0.0052852630615234375 seconds
DEBUG 01-15 16:10:28.143375.143375 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.143325.143325 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.143393.143393 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3051aeb9-7136-4c3c-bc34-8e63052a7d9d
DEBUG 01-15 16:10:28.149534.149534 cuda_h.py:19] end group pad cost 0.006067991256713867 seconds
DEBUG 01-15 16:10:28.149377.149377 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.162352.162352 client.py:127] Model loaded
DEBUG 01-15 16:10:28.162763.162763 cuda_h.py:19] end wait_experts_multi_device cost 0.01913619041442871 seconds
DEBUG 01-15 16:10:28.162983.162983 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.169423.169423 cuda_h.py:19] end group_einsum cost 0.02028036117553711 seconds
DEBUG 01-15 16:10:28.169932.169932 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.174923.174923 cuda_h.py:19] end get_outputs_cpu1 cost 0.0041980743408203125 seconds
DEBUG 01-15 16:10:28.174262.174262 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039105892181396484 seconds
DEBUG 01-15 16:10:28.175827.175827 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012670755386352539 seconds
DEBUG 01-15 16:10:28.175731.175731 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.176753.176753 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.176162.176162 cuda_h.py:19] end index_scatter cost 7.605552673339844e-05 seconds
DEBUG 01-15 16:10:28.177703.177703 cuda_h.py:19] end cpuoutputsdeal cost 0.0016934871673583984 seconds
DEBUG 01-15 16:10:28.177004.177004 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.177720.177720 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.177580.177580 cuda_h.py:19] end gpu_group_tensor cost 0.0001442432403564453 seconds
DEBUG 01-15 16:10:28.177774.177774 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.177263.177263 cuda_h.py:19] end gpu_group_tensor cost 0.00015282630920410156 seconds
DEBUG 01-15 16:10:28.177982.177982 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.178901.178901 cuda_h.py:19] end gpu_group_einsum cost 0.0006954669952392578 seconds
DEBUG 01-15 16:10:28.178886.178886 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.178317.178317 cuda_h.py:19] end gpu_group_einsum cost 0.00037217140197753906 seconds
DEBUG 01-15 16:10:28.179420.179420 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.179391.179391 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.179385.179385 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016570091247558594 seconds
DEBUG 01-15 16:10:28.179810.179810 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.179157.179157 cuda_h.py:19] end concat_expert_out cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:10:28.179622.179622 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.179712.179712 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:28.179045.179045 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007836818695068359 seconds
DEBUG 01-15 16:10:28.180319.180319 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.180447.180447 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.180056.180056 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013446807861328125 seconds
DEBUG 01-15 16:10:28.180812.180812 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.180755.180755 cuda_h.py:19] end concat_expert_out cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:10:28.180579.180579 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.180655.180655 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:28.180656.180656 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00052642822265625 seconds
DEBUG 01-15 16:10:28.180612.180612 cuda_h.py:19] end gpu_experts_multi_device cost 0.04117274284362793 seconds
DEBUG 01-15 16:10:28.180191.180191 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.05024099349975586 seconds
DEBUG 01-15 16:10:28.181131.181131 cuda_h.py:19] end prefill_layer cost 0.05596303939819336 seconds
DEBUG 01-15 16:10:28.181259.181259 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:10:28.181869.181869 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.181830.181830 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:10:28.181586.181586 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:28.181011.181011 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:28.181775.181775 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 4.2438507080078125e-05 seconds
DEBUG 01-15 16:10:28.181723.181723 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 7.677078247070312e-05 seconds
DEBUG 01-15 16:10:28.181950.181950 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.181813.181813 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.181857.181857 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.181541.181541 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.181168.181168 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.182866.182866 cuda_h.py:19] end allocate_cuda_memory cost 0.00022530555725097656 seconds
DEBUG 01-15 16:10:28.182915.182915 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.182777.182777 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.182799.182799 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.182224.182224 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 89204600-f1c7-4ef4-97ab-81d2eca21abe
DEBUG 01-15 16:10:28.182280.182280 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.182643.182643 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.183418.183418 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 89204600-f1c7-4ef4-97ab-81d2eca21abe
DEBUG 01-15 16:10:28.183506.183506 cuda_h.py:19] end load_into_gpu_async cost 0.0015900135040283203 seconds
DEBUG 01-15 16:10:28.183262.183262 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.183657.183657 cuda_h.py:19] end restore_tensors2 cost 8.320808410644531e-05 seconds
DEBUG 01-15 16:10:28.184910.184910 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021877288818359375 seconds
INFO 01-15 16:10:28.184303.184303 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 89204600-f1c7-4ef4-97ab-81d2eca21abe
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.186236.186236 cuda_h.py:19] end self_attn cost 0.003801107406616211 seconds
DEBUG 01-15 16:10:28.186870.186870 cuda_h.py:19] end iln_self_attn_paln cost 0.0054891109466552734 seconds
DEBUG 01-15 16:10:28.187414.187414 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:10:28.187416.187416 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.187939.187939 cuda_h.py:19] end gate cost 0.0007703304290771484 seconds
DEBUG 01-15 16:10:28.187107.187107 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.188647.188647 lmp.py:1912] 
DEBUG 01-15 16:10:28.188647.188647 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.188503.188503 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.188583.188583 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.188849.188849 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.188969.188969 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.188850.188850 lmp.py:1917] 
DEBUG 01-15 16:10:28.188850.188850 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.188030.188030 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.188587.188587 lmp.py:1935]   Expert  1 |     49 | CPU
DEBUG 01-15 16:10:28.188707.188707 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:10:28.188634.188634 lmp.py:1935]   Expert  7 |     74 | CPU
DEBUG 01-15 16:10:28.188085.188085 lmp.py:1935]   Expert 48 |     81 | CPU
DEBUG 01-15 16:10:28.188920.188920 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:10:28.188040.188040 lmp.py:1935]   Expert 30 |    109 | CPU
DEBUG 01-15 16:10:28.188114.188114 lmp.py:1935]   Expert 61 |    115 | CPU
DEBUG 01-15 16:10:28.188710.188710 lmp.py:1935]   Expert 18 |    118 | CPU
DEBUG 01-15 16:10:28.188830.188830 lmp.py:1935]   Expert 32 |    119 | CPU
DEBUG 01-15 16:10:28.188950.188950 lmp.py:1935]   Expert 45 |    119 | CPU
DEBUG 01-15 16:10:28.188785.188785 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:10:28.188428.188428 lmp.py:1935]   Expert 39 |    134 | CPU
DEBUG 01-15 16:10:28.188832.188832 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:28.188475.188475 lmp.py:1935]   Expert 36 |    139 | CPU
DEBUG 01-15 16:10:28.188880.188880 lmp.py:1935]   Expert  5 |    141 | CPU
DEBUG 01-15 16:10:28.188046.188046 lmp.py:1935]   Expert 11 |    142 | CPU
DEBUG 01-15 16:10:28.188451.188451 lmp.py:1935]   Expert  6 |    143 | CPU
DEBUG 01-15 16:10:28.188809.188809 lmp.py:1935]   Expert 59 |    143 | CPU
DEBUG 01-15 16:10:28.188690.188690 lmp.py:1935]   Expert 51 |    145 | CPU
DEBUG 01-15 16:10:28.188810.188810 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:10:28.188168.188168 lmp.py:1935]   Expert 49 |    155 | CPU
DEBUG 01-15 16:10:28.188288.188288 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:10:28.188693.188693 lmp.py:1935]   Expert  9 |    159 | CPU
DEBUG 01-15 16:10:28.188859.188859 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:10:28.188787.188787 lmp.py:1935]   Expert 52 |    168 | CPU
DEBUG 01-15 16:10:28.188953.188953 lmp.py:1935]   Expert 56 |    168 | CPU
DEBUG 01-15 16:10:28.188880.188880 lmp.py:1935]   Expert 40 |    169 | CPU
DEBUG 01-15 16:10:28.188047.188047 lmp.py:1935]   Expert 16 |    172 | CPU
DEBUG 01-15 16:10:28.188928.188928 lmp.py:1935]   Expert 35 |    172 | CPU
DEBUG 01-15 16:10:28.188809.188809 lmp.py:1935]   Expert  4 |    186 | CPU
DEBUG 01-15 16:10:28.188181.188181 lmp.py:1935]   Expert 37 |    190 | CPU
DEBUG 01-15 16:10:28.188586.188586 lmp.py:1935]   Expert 42 |    190 | CPU
DEBUG 01-15 16:10:28.189182.189182 lmp.py:1935]   Expert 13 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189494.189494 lmp.py:1935]   Expert 17 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189614.189614 lmp.py:1935]   Expert 38 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189734.189734 lmp.py:1935]   Expert 62 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189854.189854 lmp.py:1935]   Expert 21 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189735.189735 lmp.py:1935]   Expert  3 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189378.189378 lmp.py:1935]   Expert 44 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189259.189259 lmp.py:1935]   Expert 28 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189571.189571 lmp.py:1935]   Expert 58 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189930.189930 lmp.py:1935]   Expert 60 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189288.189288 lmp.py:1935]   Expert 47 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189646.189646 lmp.py:1935]   Expert 10 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189004.189004 lmp.py:1935]   Expert 53 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189601.189601 lmp.py:1935]   Expert 55 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189005.189005 lmp.py:1935]   Expert 20 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189125.189125 lmp.py:1935]   Expert 57 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189245.189245 lmp.py:1935]   Expert 33 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189888.189888 lmp.py:1935]   Expert 31 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189531.189531 lmp.py:1935]   Expert 46 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189936.189936 lmp.py:1935]   Expert  8 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189817.189817 lmp.py:1935]   Expert 19 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189937.189937 lmp.py:1935]   Expert 24 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189057.189057 lmp.py:1935]   Expert 14 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189653.189653 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189250.189250 lmp.py:1935]   Expert 29 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189370.189370 lmp.py:1935]   Expert 12 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189013.189013 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189656.189656 lmp.py:1935]   Expert  0 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189060.189060 lmp.py:1935]   Expert 43 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189180.189180 lmp.py:1935]   Expert 54 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189585.189585 lmp.py:1935]   Expert 41 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.189704.189704 lmp.py:1935]   Expert 25 |    410 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.189109.189109 lmp.py:1937] 
DEBUG 01-15 16:10:28.189109.189109 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.189958.189958 lmp.py:1938]   CPU:   4408 tokens
DEBUG 01-15 16:10:28.189031.189031 lmp.py:1942]   cuda:1:   3938 tokens (16 experts)
DEBUG 01-15 16:10:28.189959.189959 lmp.py:1942]   cuda:2:   3942 tokens (16 experts)
DEBUG 01-15 16:10:28.189694.189694 lmp.py:1943]   Total GPU:   7880 tokens
DEBUG 01-15 16:10:28.189715.189715 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.189715.189715 lmp.py:1944] 
DEBUG 01-15 16:10:28.189649.189649 cuda_h.py:19] end experts_map_get cost 0.0017268657684326172 seconds
DEBUG 01-15 16:10:28.189592.189592 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.189156.189156 lmp.py:1953] 
DEBUG 01-15 16:10:28.189156.189156 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.189038.189038 cuda_h.py:19] end cpu_experts_submit cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:28.189781.189781 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.189319.189319 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.190303.190303 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.43.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.43.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.43.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.3.mlp.experts.33.gate_proj.weight': 0, 'model.layers.3.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.38.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.38.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.38.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.55.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.55.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.55.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.54.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.54.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.54.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.57.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.57.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.57.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5127012352, 5767168, 17301504, 0), (5132779520, 5767168, 23068672, 0), (5121245184, 5767168, 28835840, 0), (5819072512, 5767168, 34603008, 0), (5824839680, 5767168, 40370176, 0), (5813305344, 5767168, 46137344, 0), (5836374016, 5767168, 51904512, 0), (5842141184, 5767168, 57671680, 0), (5830606848, 5767168, 63438848, 0), (5300027392, 5767168, 69206016, 0), (5305794560, 5767168, 74973184, 0), (5294260224, 5767168, 80740352, 0), (5317328896, 5767168, 86507520, 0), (5323096064, 5767168, 92274688, 0), (5311561728, 5767168, 98041856, 0), (5870977024, 5767168, 103809024, 0), (5876744192, 5767168, 109576192, 0), (5865209856, 5767168, 115343360, 0), (5888278528, 5767168, 121110528, 0), (5894045696, 5767168, 126877696, 0), (5882511360, 5767168, 132644864, 0), (5369233408, 5767168, 138412032, 0), (5375000576, 5767168, 144179200, 0), (5363466240, 5767168, 149946368, 0), (5403836416, 5767168, 155713536, 0), (5409603584, 5767168, 161480704, 0), (5398069248, 5767168, 167247872, 0), (5421137920, 5767168, 173015040, 0), (5426905088, 5767168, 178782208, 0), (5415370752, 5767168, 184549376, 0), (5992087552, 5767168, 190316544, 0), (5997854720, 5767168, 196083712, 0), (5986320384, 5767168, 201850880, 0), (5507645440, 5767168, 207618048, 0), (5513412608, 5767168, 213385216, 0), (5501878272, 5767168, 219152384, 0), (5559549952, 5767168, 224919552, 0), (5565317120, 5767168, 230686720, 0), (5553782784, 5767168, 236453888, 0), (5576851456, 5767168, 242221056, 0), (5582618624, 5767168, 247988224, 0), (5571084288, 5767168, 253755392, 0), (5611454464, 5767168, 259522560, 0), (5617221632, 5767168, 265289728, 0), (5605687296, 5767168, 271056896, 0)], 2: [(5646057472, 5767168, 0, 0), (5651824640, 5767168, 5767168, 0), (5640290304, 5767168, 11534336, 0), (5732564992, 5767168, 17301504, 0), (5738332160, 5767168, 23068672, 0), (5726797824, 5767168, 28835840, 0), (5213519872, 5767168, 34603008, 0), (5219287040, 5767168, 40370176, 0), (5207752704, 5767168, 46137344, 0), (5784469504, 5767168, 51904512, 0), (5790236672, 5767168, 57671680, 0), (5778702336, 5767168, 63438848, 0), (5248122880, 5767168, 69206016, 0), (5253890048, 5767168, 74973184, 0), (5242355712, 5767168, 80740352, 0), (5282725888, 5767168, 86507520, 0), (5288493056, 5767168, 92274688, 0), (5276958720, 5767168, 98041856, 0), (6026690560, 5767168, 103809024, 0), (6032457728, 5767168, 109576192, 0), (6020923392, 5767168, 115343360, 0), (6009389056, 5767168, 121110528, 0), (6015156224, 5767168, 126877696, 0), (6003621888, 5767168, 132644864, 0), (5455740928, 5767168, 138412032, 0), (5461508096, 5767168, 144179200, 0), (5449973760, 5767168, 149946368, 0), (5490343936, 5767168, 155713536, 0), (5496111104, 5767168, 161480704, 0), (5484576768, 5767168, 167247872, 0), (6061293568, 5767168, 173015040, 0), (6067060736, 5767168, 178782208, 0), (6055526400, 5767168, 184549376, 0), (6078595072, 5767168, 190316544, 0), (6084362240, 5767168, 196083712, 0), (6072827904, 5767168, 201850880, 0), (5438439424, 5767168, 207618048, 0), (5444206592, 5767168, 213385216, 0), (5432672256, 5767168, 219152384, 0), (6113198080, 5767168, 224919552, 0), (6118965248, 5767168, 230686720, 0), (6107430912, 5767168, 236453888, 0), (6147801088, 5767168, 242221056, 0), (6153568256, 5767168, 247988224, 0), (6142033920, 5767168, 253755392, 0), (6165102592, 5767168, 259522560, 0), (6170869760, 5767168, 265289728, 0), (6159335424, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a5afdc8c4b0>, 2: <capsule object NULL at 0x7a4e344d4960>}
DEBUG 01-15 16:10:28.190003.190003 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.191840.191840 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.191973.191973 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d40123a3-1d46-4d32-b701-740e1fd0e852
DEBUG 01-15 16:10:28.191079.191079 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.191784.191784 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:28.191527.191527 client.py:127] Model loaded
DEBUG 01-15 16:10:28.191642.191642 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.191270.191270 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.191475.191475 cuda_h.py:19] end restore2model cost 0.00033545494079589844 seconds
DEBUG 01-15 16:10:28.191483.191483 cuda_h.py:19] end sllm_worker_task cost 0.01015329360961914 seconds
INFO 01-15 16:10:28.192196.192196 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d40123a3-1d46-4d32-b701-740e1fd0e852
DEBUG 01-15 16:10:28.192407.192407 cuda_h.py:19] end move_flatidxs cost 0.0008485317230224609 seconds
DEBUG 01-15 16:10:28.192282.192282 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.192113.192113 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0027306079864501953 seconds
DEBUG 01-15 16:10:28.192182.192182 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.195978.195978 cuda_h.py:19] end restore2model cost 0.002554655075073242 seconds
DEBUG 01-15 16:10:28.195212.195212 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005527973175048828 seconds
DEBUG 01-15 16:10:28.195246.195246 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.195706.195706 cuda_h.py:19] end gpu_sexperts cost 0.0002732276916503906 seconds
DEBUG 01-15 16:10:28.195012.195012 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.195550.195550 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:10:28.195915.195915 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.195195.195195 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.196793.196793 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008287429809570312 seconds
DEBUG 01-15 16:10:28.196689.196689 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.197213.197213 cuda_h.py:19] end gpu_group_list cost 0.00018143653869628906 seconds
DEBUG 01-15 16:10:28.197264.197264 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.197047.197047 cuda_h.py:19] end group_tensors cost 0.004598379135131836 seconds
DEBUG 01-15 16:10:28.197066.197066 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.199148.199148 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012164115905761719 seconds
DEBUG 01-15 16:10:28.199430.199430 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.199658.199658 cuda_h.py:19] end gpu_group_list cost 0.00027108192443847656 seconds
DEBUG 01-15 16:10:28.200987.200987 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.200692.200692 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d40123a3-1d46-4d32-b701-740e1fd0e852
DEBUG 01-15 16:10:28.202163.202163 cuda_h.py:19] end group pad cost 0.0042340755462646484 seconds
DEBUG 01-15 16:10:28.202860.202860 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.218720.218720 client.py:127] Model loaded
DEBUG 01-15 16:10:28.218957.218957 cuda_h.py:19] end wait_experts_multi_device cost 0.0177762508392334 seconds
DEBUG 01-15 16:10:28.218018.218018 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.233272.233272 cuda_h.py:19] end group_einsum cost 0.030628204345703125 seconds
DEBUG 01-15 16:10:28.233642.233642 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.239162.239162 cuda_h.py:19] end get_outputs_cpu1 cost 0.006560087203979492 seconds
DEBUG 01-15 16:10:28.240971.240971 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04923057556152344 seconds
DEBUG 01-15 16:10:28.241328.241328 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.02264094352722168 seconds
DEBUG 01-15 16:10:28.241324.241324 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.242025.242025 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.242043.242043 cuda_h.py:19] end index_scatter cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:10:28.242319.242319 cuda_h.py:19] end cpuoutputsdeal cost 0.0015704631805419922 seconds
DEBUG 01-15 16:10:28.242328.242328 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.242899.242899 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.243322.243322 cuda_h.py:19] end gpu_group_tensor cost 0.00013947486877441406 seconds
DEBUG 01-15 16:10:28.243230.243230 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.243540.243540 cuda_h.py:19] end gpu_group_tensor cost 0.00012683868408203125 seconds
DEBUG 01-15 16:10:28.243014.243014 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.243102.243102 cuda_h.py:19] end gpu_group_einsum cost 0.0006110668182373047 seconds
DEBUG 01-15 16:10:28.244007.244007 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.244190.244190 cuda_h.py:19] end gpu_group_einsum cost 0.0004630088806152344 seconds
DEBUG 01-15 16:10:28.244101.244101 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.244191.244191 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.245456.245456 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015854835510253906 seconds
DEBUG 01-15 16:10:28.245689.245689 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.245936.245936 cuda_h.py:19] end concat_expert_out cost 4.696846008300781e-05 seconds
DEBUG 01-15 16:10:28.245779.245779 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.245749.245749 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:28.245227.245227 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007445812225341797 seconds
DEBUG 01-15 16:10:28.245025.245025 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.245491.245491 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.245808.245808 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013065338134765625 seconds
DEBUG 01-15 16:10:28.245610.245610 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.245149.245149 cuda_h.py:19] end concat_expert_out cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:28.246654.246654 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.246922.246922 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:28.246831.246831 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004825592041015625 seconds
DEBUG 01-15 16:10:28.246880.246880 cuda_h.py:19] end gpu_experts_multi_device cost 0.05033707618713379 seconds
DEBUG 01-15 16:10:28.246789.246789 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.05923318862915039 seconds
DEBUG 01-15 16:10:28.246583.246583 cuda_h.py:19] end prefill_layer cost 0.06541752815246582 seconds
DEBUG 01-15 16:10:28.246857.246857 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:10:28.246990.246990 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.246885.246885 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:10:28.246496.246496 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:28.246490.246490 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:28.246393.246393 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:10:28.246103.246103 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 7.510185241699219e-05 seconds
DEBUG 01-15 16:10:28.246660.246660 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.247623.247623 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.247592.247592 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.247191.247191 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.247458.247458 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.247024.247024 cuda_h.py:19] end allocate_cuda_memory cost 0.00023794174194335938 seconds
DEBUG 01-15 16:10:28.247186.247186 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.247379.247379 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.247493.247493 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.247487.247487 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c7d153a9-0205-4f04-8489-b3f3aebe803b
DEBUG 01-15 16:10:28.247630.247630 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.248898.248898 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.249480.249480 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c7d153a9-0205-4f04-8489-b3f3aebe803b
DEBUG 01-15 16:10:28.249138.249138 cuda_h.py:19] end load_into_gpu_async cost 0.0015063285827636719 seconds
DEBUG 01-15 16:10:28.249179.249179 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.249328.249328 cuda_h.py:19] end restore_tensors2 cost 7.82012939453125e-05 seconds
DEBUG 01-15 16:10:28.249422.249422 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021042823791503906 seconds
INFO 01-15 16:10:28.249133.249133 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c7d153a9-0205-4f04-8489-b3f3aebe803b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.251415.251415 cuda_h.py:19] end self_attn cost 0.003665447235107422 seconds
DEBUG 01-15 16:10:28.252048.252048 cuda_h.py:19] end iln_self_attn_paln cost 0.005238771438598633 seconds
DEBUG 01-15 16:10:28.252070.252070 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:10:28.252833.252833 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.253787.253787 cuda_h.py:19] end gate cost 0.0007700920104980469 seconds
DEBUG 01-15 16:10:28.253385.253385 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.253090.253090 lmp.py:1912] 
DEBUG 01-15 16:10:28.253090.253090 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.253800.253800 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.253927.253927 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.253954.253954 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.253074.253074 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.253478.253478 lmp.py:1917] 
DEBUG 01-15 16:10:28.253478.253478 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.253883.253883 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.253248.253248 lmp.py:1935]   Expert 14 |     62 | CPU
DEBUG 01-15 16:10:28.253129.253129 lmp.py:1935]   Expert 57 |     74 | CPU
DEBUG 01-15 16:10:28.253295.253295 lmp.py:1935]   Expert 13 |     76 | CPU
DEBUG 01-15 16:10:28.253985.253985 lmp.py:1935]   Expert 26 |     82 | CPU
DEBUG 01-15 16:10:28.253674.253674 lmp.py:1935]   Expert 31 |     90 | CPU
DEBUG 01-15 16:10:28.253363.253363 lmp.py:1935]   Expert 54 |     92 | CPU
DEBUG 01-15 16:10:28.253768.253768 lmp.py:1935]   Expert 11 |     93 | CPU
DEBUG 01-15 16:10:28.253172.253172 lmp.py:1935]   Expert 45 |     96 | CPU
DEBUG 01-15 16:10:28.253577.253577 lmp.py:1935]   Expert 58 |    102 | CPU
DEBUG 01-15 16:10:28.253366.253366 lmp.py:1935]   Expert 30 |    107 | CPU
DEBUG 01-15 16:10:28.253486.253486 lmp.py:1935]   Expert 51 |    108 | CPU
DEBUG 01-15 16:10:28.253367.253367 lmp.py:1935]   Expert 36 |    112 | CPU
DEBUG 01-15 16:10:28.253771.253771 lmp.py:1935]   Expert 10 |    114 | CPU
DEBUG 01-15 16:10:28.253176.253176 lmp.py:1935]   Expert 32 |    115 | CPU
DEBUG 01-15 16:10:28.253581.253581 lmp.py:1935]   Expert 20 |    128 | CPU
DEBUG 01-15 16:10:28.253985.253985 lmp.py:1935]   Expert  8 |    133 | CPU
DEBUG 01-15 16:10:28.253390.253390 lmp.py:1935]   Expert 63 |    137 | CPU
DEBUG 01-15 16:10:28.253794.253794 lmp.py:1935]   Expert  4 |    138 | CPU
DEBUG 01-15 16:10:28.253960.253960 lmp.py:1935]   Expert 53 |    140 | CPU
DEBUG 01-15 16:10:28.253603.253603 lmp.py:1935]   Expert 34 |    143 | CPU
DEBUG 01-15 16:10:28.254962.254962 lmp.py:1935]   Expert 61 |    144 | CPU
DEBUG 01-15 16:10:28.254081.254081 lmp.py:1935]   Expert 16 |    147 | CPU
DEBUG 01-15 16:10:28.254724.254724 lmp.py:1935]   Expert 47 |    148 | CPU
DEBUG 01-15 16:10:28.254129.254129 lmp.py:1935]   Expert 60 |    158 | CPU
DEBUG 01-15 16:10:28.254295.254295 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:10:28.254461.254461 lmp.py:1935]   Expert 17 |    161 | CPU
DEBUG 01-15 16:10:28.254389.254389 lmp.py:1935]   Expert 42 |    162 | CPU
DEBUG 01-15 16:10:28.254793.254793 lmp.py:1935]   Expert 29 |    170 | CPU
DEBUG 01-15 16:10:28.254960.254960 lmp.py:1935]   Expert 44 |    170 | CPU
DEBUG 01-15 16:10:28.254649.254649 lmp.py:1935]   Expert 27 |    175 | CPU
DEBUG 01-15 16:10:28.254815.254815 lmp.py:1935]   Expert  7 |    177 | CPU
DEBUG 01-15 16:10:28.254981.254981 lmp.py:1935]   Expert 41 |    179 | CPU
DEBUG 01-15 16:10:28.254055.254055 lmp.py:1935]   Expert 48 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254082.254082 lmp.py:1935]   Expert 56 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254917.254917 lmp.py:1935]   Expert  9 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254752.254752 lmp.py:1935]   Expert  2 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254110.254110 lmp.py:1935]   Expert  3 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254707.254707 lmp.py:1935]   Expert 15 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254019.254019 lmp.py:1935]   Expert 24 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254092.254092 lmp.py:1935]   Expert  0 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254927.254927 lmp.py:1935]   Expert 18 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254001.254001 lmp.py:1935]   Expert 55 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254597.254597 lmp.py:1935]   Expert 40 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254194.254194 lmp.py:1935]   Expert 38 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254075.254075 lmp.py:1935]   Expert 22 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254434.254434 lmp.py:1935]   Expert 23 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254315.254315 lmp.py:1935]   Expert  6 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254435.254435 lmp.py:1935]   Expert 37 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254316.254316 lmp.py:1935]   Expert 46 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254959.254959 lmp.py:1935]   Expert 19 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254602.254602 lmp.py:1935]   Expert 39 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254960.254960 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254080.254080 lmp.py:1935]   Expert 12 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254677.254677 lmp.py:1935]   Expert 50 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254797.254797 lmp.py:1935]   Expert 62 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254917.254917 lmp.py:1935]   Expert 21 |    281 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254560.254560 lmp.py:1935]   Expert 35 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254203.254203 lmp.py:1935]   Expert 49 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254322.254322 lmp.py:1935]   Expert 33 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254681.254681 lmp.py:1935]   Expert 52 |    299 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254324.254324 lmp.py:1935]   Expert  1 |    347 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254205.254205 lmp.py:1935]   Expert  5 |    382 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254848.254848 lmp.py:1935]   Expert 43 |    438 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.254729.254729 lmp.py:1935]   Expert 59 |    588 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.254849.254849 lmp.py:1937] 
DEBUG 01-15 16:10:28.254849.254849 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.254446.254446 lmp.py:1938]   CPU:   4092 tokens
DEBUG 01-15 16:10:28.254042.254042 lmp.py:1942]   cuda:1:   4129 tokens (16 experts)
DEBUG 01-15 16:10:28.254116.254116 lmp.py:1942]   cuda:2:   4067 tokens (16 experts)
DEBUG 01-15 16:10:28.254282.254282 lmp.py:1943]   Total GPU:   8196 tokens
DEBUG 01-15 16:10:28.254210.254210 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.254210.254210 lmp.py:1944] 
DEBUG 01-15 16:10:28.254336.254336 cuda_h.py:19] end experts_map_get cost 0.0016908645629882812 seconds
DEBUG 01-15 16:10:28.254802.254802 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.255843.255843 lmp.py:1953] 
DEBUG 01-15 16:10:28.255843.255843 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.255864.255864 cuda_h.py:19] end cpu_experts_submit cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:28.255037.255037 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.255959.255959 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.255866.255866 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.40.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.40.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.40.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.48.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.48.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.48.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.25.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.25.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.25.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6753353728, 5767168, 17301504, 0), (6759120896, 5767168, 23068672, 0), (6747586560, 5767168, 28835840, 0), (6787956736, 5767168, 34603008, 0), (6793723904, 5767168, 40370176, 0), (6782189568, 5767168, 46137344, 0), (6286213120, 5767168, 51904512, 0), (6291980288, 5767168, 57671680, 0), (6280445952, 5767168, 63438848, 0), (6857162752, 5767168, 69206016, 0), (6862929920, 5767168, 74973184, 0), (6851395584, 5767168, 80740352, 0), (6874464256, 5767168, 86507520, 0), (6880231424, 5767168, 92274688, 0), (6868697088, 5767168, 98041856, 0), (6338117632, 5767168, 103809024, 0), (6343884800, 5767168, 109576192, 0), (6332350464, 5767168, 115343360, 0), (6390022144, 5767168, 121110528, 0), (6395789312, 5767168, 126877696, 0), (6384254976, 5767168, 132644864, 0), (6978273280, 5767168, 138412032, 0), (6984040448, 5767168, 144179200, 0), (6972506112, 5767168, 149946368, 0), (6441926656, 5767168, 155713536, 0), (6447693824, 5767168, 161480704, 0), (6436159488, 5767168, 167247872, 0), (7012876288, 5767168, 173015040, 0), (7018643456, 5767168, 178782208, 0), (7007109120, 5767168, 184549376, 0), (6493831168, 5767168, 190316544, 0), (6499598336, 5767168, 196083712, 0), (6488064000, 5767168, 201850880, 0), (6563037184, 5767168, 207618048, 0), (6568804352, 5767168, 213385216, 0), (6557270016, 5767168, 219152384, 0), (6597640192, 5767168, 224919552, 0), (6603407360, 5767168, 230686720, 0), (6591873024, 5767168, 236453888, 0), (7203192832, 5767168, 242221056, 0), (7208960000, 5767168, 247988224, 0), (7197425664, 5767168, 253755392, 0), (7255097344, 5767168, 259522560, 0), (7260864512, 5767168, 265289728, 0), (7249330176, 5767168, 271056896, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6217007104, 5767168, 17301504, 0), (6222774272, 5767168, 23068672, 0), (6211239936, 5767168, 28835840, 0), (6234308608, 5767168, 34603008, 0), (6240075776, 5767168, 40370176, 0), (6228541440, 5767168, 46137344, 0), (6268911616, 5767168, 51904512, 0), (6274678784, 5767168, 57671680, 0), (6263144448, 5767168, 63438848, 0), (6822559744, 5767168, 69206016, 0), (6828326912, 5767168, 74973184, 0), (6816792576, 5767168, 80740352, 0), (6839861248, 5767168, 86507520, 0), (6845628416, 5767168, 92274688, 0), (6834094080, 5767168, 98041856, 0), (6926368768, 5767168, 103809024, 0), (6932135936, 5767168, 109576192, 0), (6920601600, 5767168, 115343360, 0), (7030177792, 5767168, 121110528, 0), (7035944960, 5767168, 126877696, 0), (7024410624, 5767168, 132644864, 0), (7047479296, 5767168, 138412032, 0), (7053246464, 5767168, 144179200, 0), (7041712128, 5767168, 149946368, 0), (6511132672, 5767168, 155713536, 0), (6516899840, 5767168, 161480704, 0), (6505365504, 5767168, 167247872, 0), (7082082304, 5767168, 173015040, 0), (7087849472, 5767168, 178782208, 0), (7076315136, 5767168, 184549376, 0), (6545735680, 5767168, 190316544, 0), (6551502848, 5767168, 196083712, 0), (6539968512, 5767168, 201850880, 0), (7133986816, 5767168, 207618048, 0), (7139753984, 5767168, 213385216, 0), (7128219648, 5767168, 219152384, 0), (6580338688, 5767168, 224919552, 0), (6586105856, 5767168, 230686720, 0), (6574571520, 5767168, 236453888, 0), (7151288320, 5767168, 242221056, 0), (7157055488, 5767168, 247988224, 0), (7145521152, 5767168, 253755392, 0), (6614941696, 5767168, 259522560, 0), (6620708864, 5767168, 265289728, 0), (6609174528, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e34218b10>, 2: <capsule object NULL at 0x7a4e34219fb0>}
DEBUG 01-15 16:10:28.256817.256817 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.256991.256991 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.256363.256363 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a04aafe6-18a5-42b8-b8f3-24289a657bbd
DEBUG 01-15 16:10:28.256701.256701 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.256645.256645 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:28.256277.256277 client.py:127] Model loaded
DEBUG 01-15 16:10:28.256391.256391 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.256025.256025 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.257278.257278 cuda_h.py:19] end restore2model cost 0.00034117698669433594 seconds
DEBUG 01-15 16:10:28.257809.257809 cuda_h.py:19] end sllm_worker_task cost 0.009944915771484375 seconds
INFO 01-15 16:10:28.257864.257864 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a04aafe6-18a5-42b8-b8f3-24289a657bbd
DEBUG 01-15 16:10:28.257658.257658 cuda_h.py:19] end move_flatidxs cost 0.0008254051208496094 seconds
DEBUG 01-15 16:10:28.257865.257865 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.258926.258926 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002928018569946289 seconds
DEBUG 01-15 16:10:28.258988.258988 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.260811.260811 cuda_h.py:19] end restore2model cost 0.0025739669799804688 seconds
DEBUG 01-15 16:10:28.260992.260992 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005736589431762695 seconds
DEBUG 01-15 16:10:28.260072.260072 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.261526.261526 cuda_h.py:19] end gpu_sexperts cost 0.0002684593200683594 seconds
DEBUG 01-15 16:10:28.261163.261163 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.261224.261224 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:10:28.261397.261397 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.261577.261577 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.262539.262539 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008180141448974609 seconds
DEBUG 01-15 16:10:28.262097.262097 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.262151.262151 cuda_h.py:19] end gpu_group_list cost 0.00018644332885742188 seconds
DEBUG 01-15 16:10:28.263309.263309 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.262580.262580 cuda_h.py:19] end group_tensors cost 0.004914045333862305 seconds
DEBUG 01-15 16:10:28.263021.263021 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.264173.264173 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014164447784423828 seconds
DEBUG 01-15 16:10:28.264434.264434 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.265146.265146 cuda_h.py:19] end gpu_group_list cost 0.0002791881561279297 seconds
DEBUG 01-15 16:10:28.265196.265196 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.266291.266291 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a04aafe6-18a5-42b8-b8f3-24289a657bbd
DEBUG 01-15 16:10:28.267128.267128 cuda_h.py:19] end group pad cost 0.004134178161621094 seconds
DEBUG 01-15 16:10:28.267872.267872 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.284807.284807 client.py:127] Model loaded
DEBUG 01-15 16:10:28.284045.284045 cuda_h.py:19] end wait_experts_multi_device cost 0.01851820945739746 seconds
DEBUG 01-15 16:10:28.284612.284612 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.288954.288954 cuda_h.py:19] end group_einsum cost 0.02097463607788086 seconds
DEBUG 01-15 16:10:28.288787.288787 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.293858.293858 cuda_h.py:19] end get_outputs_cpu1 cost 0.0040056705474853516 seconds
DEBUG 01-15 16:10:28.293695.293695 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03702044486999512 seconds
DEBUG 01-15 16:10:28.294318.294318 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00931692123413086 seconds
DEBUG 01-15 16:10:28.294937.294937 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.295732.295732 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.295697.295697 cuda_h.py:19] end index_scatter cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:10:28.295926.295926 cuda_h.py:19] end cpuoutputsdeal cost 0.0015990734100341797 seconds
DEBUG 01-15 16:10:28.295081.295081 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.295890.295890 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.296221.296221 cuda_h.py:19] end gpu_group_tensor cost 0.00014209747314453125 seconds
DEBUG 01-15 16:10:28.296507.296507 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.296660.296660 cuda_h.py:19] end gpu_group_tensor cost 0.0003910064697265625 seconds
DEBUG 01-15 16:10:28.296783.296783 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.297275.297275 cuda_h.py:19] end gpu_group_einsum cost 0.0009768009185791016 seconds
DEBUG 01-15 16:10:28.297266.297266 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.298655.298655 cuda_h.py:19] end gpu_group_einsum cost 0.0004749298095703125 seconds
DEBUG 01-15 16:10:28.298216.298216 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.298094.298094 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.298049.298049 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019979476928710938 seconds
DEBUG 01-15 16:10:28.298103.298103 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.299478.299478 cuda_h.py:19] end concat_expert_out cost 6.270408630371094e-05 seconds
DEBUG 01-15 16:10:28.299413.299413 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.299668.299668 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:28.299404.299404 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008215904235839844 seconds
DEBUG 01-15 16:10:28.299480.299480 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.299177.299177 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.299911.299911 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001246929168701172 seconds
DEBUG 01-15 16:10:28.299998.299998 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.299491.299491 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:28.299426.299426 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.300873.300873 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:28.300775.300775 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046253204345703125 seconds
DEBUG 01-15 16:10:28.300154.300154 cuda_h.py:19] end gpu_experts_multi_device cost 0.03880715370178223 seconds
DEBUG 01-15 16:10:28.300872.300872 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.04785609245300293 seconds
DEBUG 01-15 16:10:28.300573.300573 cuda_h.py:19] end prefill_layer cost 0.05376791954040527 seconds
DEBUG 01-15 16:10:28.300185.300185 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:10:28.300841.300841 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.300259.300259 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:10:28.300393.300393 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:28.300387.300387 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:28.300290.300290 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 4.029273986816406e-05 seconds
DEBUG 01-15 16:10:28.300285.300285 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:10:28.300173.300173 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.300374.300374 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.301536.301536 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.301420.301420 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.301355.301355 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.301767.301767 cuda_h.py:19] end allocate_cuda_memory cost 0.0001952648162841797 seconds
DEBUG 01-15 16:10:28.301783.301783 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.301884.301884 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.301952.301952 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.301661.301661 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 49debed4-9589-49fb-bd51-044d9357db82
DEBUG 01-15 16:10:28.301519.301519 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.302880.302880 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.303846.303846 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 49debed4-9589-49fb-bd51-044d9357db82
DEBUG 01-15 16:10:28.303165.303165 cuda_h.py:19] end load_into_gpu_async cost 0.0022792816162109375 seconds
DEBUG 01-15 16:10:28.303491.303491 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.303309.303309 cuda_h.py:19] end restore_tensors2 cost 8.0108642578125e-05 seconds
DEBUG 01-15 16:10:28.303403.303403 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002833843231201172 seconds
INFO 01-15 16:10:28.304485.304485 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 49debed4-9589-49fb-bd51-044d9357db82
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.305676.305676 cuda_h.py:19] end self_attn cost 0.0031130313873291016 seconds
DEBUG 01-15 16:10:28.305117.305117 cuda_h.py:19] end iln_self_attn_paln cost 0.0046312808990478516 seconds
DEBUG 01-15 16:10:28.305185.305185 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:10:28.305948.305948 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.306111.306111 cuda_h.py:19] end gate cost 0.0006783008575439453 seconds
DEBUG 01-15 16:10:28.306854.306854 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.306537.306537 lmp.py:1912] 
DEBUG 01-15 16:10:28.306537.306537 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.306174.306174 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.306208.306208 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.306712.306712 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.306309.306309 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.306713.306713 lmp.py:1917] 
DEBUG 01-15 16:10:28.306713.306713 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.306356.306356 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.306483.306483 lmp.py:1935]   Expert 34 |     25 | CPU
DEBUG 01-15 16:10:28.306126.306126 lmp.py:1935]   Expert 45 |     65 | CPU
DEBUG 01-15 16:10:28.306292.306292 lmp.py:1935]   Expert 22 |     73 | CPU
DEBUG 01-15 16:10:28.307981.307981 lmp.py:1935]   Expert 57 |     75 | CPU
DEBUG 01-15 16:10:28.307909.307909 lmp.py:1935]   Expert 17 |     95 | CPU
DEBUG 01-15 16:10:28.307360.307360 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:10:28.307241.307241 lmp.py:1935]   Expert  4 |    101 | CPU
DEBUG 01-15 16:10:28.307646.307646 lmp.py:1935]   Expert 28 |    106 | CPU
DEBUG 01-15 16:10:28.307050.307050 lmp.py:1935]   Expert 32 |    113 | CPU
DEBUG 01-15 16:10:28.307693.307693 lmp.py:1935]   Expert 60 |    113 | CPU
DEBUG 01-15 16:10:28.307621.307621 lmp.py:1935]   Expert 36 |    124 | CPU
DEBUG 01-15 16:10:28.307310.307310 lmp.py:1935]   Expert 14 |    125 | CPU
DEBUG 01-15 16:10:28.307761.307761 lmp.py:1935]   Expert 12 |    127 | CPU
DEBUG 01-15 16:10:28.307212.307212 lmp.py:1935]   Expert 16 |    128 | CPU
DEBUG 01-15 16:10:28.307425.307425 lmp.py:1935]   Expert 52 |    130 | CPU
DEBUG 01-15 16:10:28.307875.307875 lmp.py:1935]   Expert 25 |    131 | CPU
DEBUG 01-15 16:10:28.307088.307088 lmp.py:1935]   Expert  8 |    135 | CPU
DEBUG 01-15 16:10:28.307539.307539 lmp.py:1935]   Expert  2 |    139 | CPU
DEBUG 01-15 16:10:28.307466.307466 lmp.py:1935]   Expert 35 |    144 | CPU
DEBUG 01-15 16:10:28.307394.307394 lmp.py:1935]   Expert  5 |    146 | CPU
DEBUG 01-15 16:10:28.307560.307560 lmp.py:1935]   Expert 30 |    153 | CPU
DEBUG 01-15 16:10:28.307726.307726 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:10:28.307893.307893 lmp.py:1935]   Expert 61 |    157 | CPU
DEBUG 01-15 16:10:28.307582.307582 lmp.py:1935]   Expert  0 |    158 | CPU
DEBUG 01-15 16:10:28.307033.307033 lmp.py:1935]   Expert 39 |    158 | CPU
DEBUG 01-15 16:10:28.307722.307722 lmp.py:1935]   Expert  3 |    168 | CPU
DEBUG 01-15 16:10:28.307173.307173 lmp.py:1935]   Expert 13 |    169 | CPU
DEBUG 01-15 16:10:28.307624.307624 lmp.py:1935]   Expert 42 |    171 | CPU
DEBUG 01-15 16:10:28.307836.307836 lmp.py:1935]   Expert 31 |    172 | CPU
DEBUG 01-15 16:10:28.307287.307287 lmp.py:1935]   Expert 44 |    174 | CPU
DEBUG 01-15 16:10:28.307738.307738 lmp.py:1935]   Expert 41 |    175 | CPU
DEBUG 01-15 16:10:28.307951.307951 lmp.py:1935]   Expert 46 |    178 | CPU
DEBUG 01-15 16:10:28.307024.307024 lmp.py:1935]   Expert  9 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307382.307382 lmp.py:1935]   Expert 43 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307555.307555 lmp.py:1935]   Expert 62 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307152.307152 lmp.py:1935]   Expert 26 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307510.307510 lmp.py:1935]   Expert 50 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307107.307107 lmp.py:1935]   Expert 27 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307227.307227 lmp.py:1935]   Expert 51 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307631.307631 lmp.py:1935]   Expert 18 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307797.307797 lmp.py:1935]   Expert 49 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307123.307123 lmp.py:1935]   Expert 11 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307289.307289 lmp.py:1935]   Expert 47 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307455.307455 lmp.py:1935]   Expert 19 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307383.307383 lmp.py:1935]   Expert 20 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307549.307549 lmp.py:1935]   Expert 63 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307715.307715 lmp.py:1935]   Expert 55 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307596.307596 lmp.py:1935]   Expert 56 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307239.307239 lmp.py:1935]   Expert 38 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307882.307882 lmp.py:1935]   Expert 48 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307048.307048 lmp.py:1935]   Expert  1 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307215.307215 lmp.py:1935]   Expert 10 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307381.307381 lmp.py:1935]   Expert 54 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307547.307547 lmp.py:1935]   Expert 21 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307905.307905 lmp.py:1935]   Expert  7 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307548.307548 lmp.py:1935]   Expert 33 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307429.307429 lmp.py:1935]   Expert 29 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307072.307072 lmp.py:1935]   Expert 40 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.307192.307192 lmp.py:1935]   Expert 24 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.307312.307312 lmp.py:1935]   Expert 59 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.308193.308193 lmp.py:1935]   Expert 37 |    333 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.308836.308836 lmp.py:1935]   Expert 58 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.308956.308956 lmp.py:1935]   Expert  6 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.308652.308652 lmp.py:1935]   Expert 53 |    854 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.308057.308057 lmp.py:1937] 
DEBUG 01-15 16:10:28.308057.308057 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.308461.308461 lmp.py:1938]   CPU:   4181 tokens
DEBUG 01-15 16:10:28.308151.308151 lmp.py:1942]   cuda:1:   3974 tokens (15 experts)
DEBUG 01-15 16:10:28.308317.308317 lmp.py:1942]   cuda:2:   4133 tokens (17 experts)
DEBUG 01-15 16:10:28.308291.308291 lmp.py:1943]   Total GPU:   8107 tokens
DEBUG 01-15 16:10:28.308503.308503 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.308503.308503 lmp.py:1944] 
DEBUG 01-15 16:10:28.308438.308438 cuda_h.py:19] end experts_map_get cost 0.0017669200897216797 seconds
DEBUG 01-15 16:10:28.308235.308235 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.308037.308037 lmp.py:1953] 
DEBUG 01-15 16:10:28.308037.308037 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.308674.308674 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:28.308894.308894 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.308352.308352 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.310597.310597 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.33.gate_proj.weight': 0, 'model.layers.5.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.40.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.40.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.40.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.43.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.43.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.43.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.51.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.51.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.51.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.50.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.50.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.50.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.26.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.26.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.26.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.54.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.54.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.54.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.27.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.27.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.27.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.29.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.62.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.62.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.62.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(7860649984, 5767168, 0, 0), (7866417152, 5767168, 5767168, 0), (7854882816, 5767168, 11534336, 0), (7981760512, 5767168, 17301504, 0), (7987527680, 5767168, 23068672, 0), (7975993344, 5767168, 28835840, 0), (7462715392, 5767168, 34603008, 0), (7468482560, 5767168, 40370176, 0), (7456948224, 5767168, 46137344, 0), (7480016896, 5767168, 51904512, 0), (7485784064, 5767168, 57671680, 0), (7474249728, 5767168, 63438848, 0), (8033665024, 5767168, 69206016, 0), (8039432192, 5767168, 74973184, 0), (8027897856, 5767168, 80740352, 0), (8120172544, 5767168, 86507520, 0), (8125939712, 5767168, 92274688, 0), (8114405376, 5767168, 98041856, 0), (7601127424, 5767168, 103809024, 0), (7606894592, 5767168, 109576192, 0), (7595360256, 5767168, 115343360, 0), (8172077056, 5767168, 121110528, 0), (8177844224, 5767168, 126877696, 0), (8166309888, 5767168, 132644864, 0), (7635730432, 5767168, 138412032, 0), (7641497600, 5767168, 144179200, 0), (7629963264, 5767168, 149946368, 0), (7653031936, 5767168, 155713536, 0), (7658799104, 5767168, 161480704, 0), (7647264768, 5767168, 167247872, 0), (8206680064, 5767168, 173015040, 0), (8212447232, 5767168, 178782208, 0), (8200912896, 5767168, 184549376, 0), (8154775552, 5767168, 190316544, 0), (8160542720, 5767168, 196083712, 0), (8149008384, 5767168, 201850880, 0), (8258584576, 5767168, 207618048, 0), (8264351744, 5767168, 213385216, 0), (8252817408, 5767168, 219152384, 0), (8310489088, 5767168, 224919552, 0), (8316256256, 5767168, 230686720, 0), (8304721920, 5767168, 236453888, 0), (8379695104, 5767168, 242221056, 0), (8385462272, 5767168, 247988224, 0), (8373927936, 5767168, 253755392, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (7739539456, 5767168, 17301504, 0), (7745306624, 5767168, 23068672, 0), (7733772288, 5767168, 28835840, 0), (7929856000, 5767168, 34603008, 0), (7935623168, 5767168, 40370176, 0), (7924088832, 5767168, 46137344, 0), (7393509376, 5767168, 51904512, 0), (7399276544, 5767168, 57671680, 0), (7387742208, 5767168, 63438848, 0), (7410810880, 5767168, 69206016, 0), (7416578048, 5767168, 74973184, 0), (7405043712, 5767168, 80740352, 0), (7947157504, 5767168, 86507520, 0), (7952924672, 5767168, 92274688, 0), (7941390336, 5767168, 98041856, 0), (7445413888, 5767168, 103809024, 0), (7451181056, 5767168, 109576192, 0), (7439646720, 5767168, 115343360, 0), (8102871040, 5767168, 121110528, 0), (8108638208, 5767168, 126877696, 0), (8097103872, 5767168, 132644864, 0), (8137474048, 5767168, 138412032, 0), (8143241216, 5767168, 144179200, 0), (8131706880, 5767168, 149946368, 0), (7618428928, 5767168, 155713536, 0), (7624196096, 5767168, 161480704, 0), (7612661760, 5767168, 167247872, 0), (8223981568, 5767168, 173015040, 0), (8229748736, 5767168, 178782208, 0), (8218214400, 5767168, 184549376, 0), (8241283072, 5767168, 190316544, 0), (8247050240, 5767168, 196083712, 0), (8235515904, 5767168, 201850880, 0), (7704936448, 5767168, 207618048, 0), (7710703616, 5767168, 213385216, 0), (7699169280, 5767168, 219152384, 0), (8293187584, 5767168, 224919552, 0), (8298954752, 5767168, 230686720, 0), (8287420416, 5767168, 236453888, 0), (7756840960, 5767168, 242221056, 0), (7762608128, 5767168, 247988224, 0), (7751073792, 5767168, 253755392, 0), (7791443968, 5767168, 259522560, 0), (7797211136, 5767168, 265289728, 0), (7785676800, 5767168, 271056896, 0), (8362393600, 5767168, 276824064, 0), (8368160768, 5767168, 282591232, 0), (8356626432, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e3421a0d0>, 2: <capsule object NULL at 0x7a4f2c248600>}
DEBUG 01-15 16:10:28.310350.310350 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.310756.310756 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.310128.310128 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 29cf942c-6530-4616-88b4-0dd9355deac8
DEBUG 01-15 16:10:28.310134.310134 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.310091.310091 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:28.310252.310252 client.py:127] Model loaded
DEBUG 01-15 16:10:28.310982.310982 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.311770.311770 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.311047.311047 cuda_h.py:19] end restore2model cost 0.0003342628479003906 seconds
DEBUG 01-15 16:10:28.311194.311194 cuda_h.py:19] end sllm_worker_task cost 0.010271310806274414 seconds
INFO 01-15 16:10:28.311019.311019 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 29cf942c-6530-4616-88b4-0dd9355deac8
DEBUG 01-15 16:10:28.312064.312064 cuda_h.py:19] end move_flatidxs cost 0.0008254051208496094 seconds
DEBUG 01-15 16:10:28.312794.312794 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.312889.312889 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0037336349487304688 seconds
DEBUG 01-15 16:10:28.312945.312945 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.314552.314552 cuda_h.py:19] end restore2model cost 0.0024874210357666016 seconds
DEBUG 01-15 16:10:28.314965.314965 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0064547061920166016 seconds
DEBUG 01-15 16:10:28.314045.314045 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.315068.315068 cuda_h.py:19] end gpu_sexperts cost 0.00026726722717285156 seconds
DEBUG 01-15 16:10:28.315706.315706 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.315005.315005 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.71661376953125e-05 seconds
DEBUG 01-15 16:10:28.315370.315370 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.315835.315835 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.316053.316053 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007634162902832031 seconds
DEBUG 01-15 16:10:28.316035.316035 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.316599.316599 cuda_h.py:19] end gpu_group_list cost 0.0001766681671142578 seconds
DEBUG 01-15 16:10:28.317431.317431 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.318854.318854 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009217262268066406 seconds
DEBUG 01-15 16:10:28.318472.318472 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.318063.318063 cuda_h.py:19] end gpu_group_list cost 0.00019502639770507812 seconds
DEBUG 01-15 16:10:28.318745.318745 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.318290.318290 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 29cf942c-6530-4616-88b4-0dd9355deac8
DEBUG 01-15 16:10:28.322414.322414 cuda_h.py:19] end group_tensors cost 0.010178089141845703 seconds
DEBUG 01-15 16:10:28.323970.323970 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.326849.326849 cuda_h.py:19] end group pad cost 0.0038628578186035156 seconds
DEBUG 01-15 16:10:28.327315.327315 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.341306.341306 client.py:127] Model loaded
DEBUG 01-15 16:10:28.341317.341317 cuda_h.py:19] end wait_experts_multi_device cost 0.02216053009033203 seconds
DEBUG 01-15 16:10:28.341478.341478 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.346767.346767 cuda_h.py:19] end group_einsum cost 0.019560575485229492 seconds
DEBUG 01-15 16:10:28.346303.346303 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.350511.350511 cuda_h.py:19] end get_outputs_cpu1 cost 0.0039403438568115234 seconds
DEBUG 01-15 16:10:28.351698.351698 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04062318801879883 seconds
DEBUG 01-15 16:10:28.351156.351156 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01064610481262207 seconds
DEBUG 01-15 16:10:28.352603.352603 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.353944.353944 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.353625.353625 cuda_h.py:19] end index_scatter cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:10:28.353178.353178 cuda_h.py:19] end cpuoutputsdeal cost 0.0016829967498779297 seconds
DEBUG 01-15 16:10:28.353942.353942 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.353320.353320 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.354863.354863 cuda_h.py:19] end gpu_group_tensor cost 0.00036144256591796875 seconds
DEBUG 01-15 16:10:28.354554.354554 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.355337.355337 cuda_h.py:19] end gpu_group_tensor cost 0.0006070137023925781 seconds
DEBUG 01-15 16:10:28.355991.355991 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.355041.355041 cuda_h.py:19] end gpu_group_einsum cost 0.0006520748138427734 seconds
DEBUG 01-15 16:10:28.355257.355257 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.356448.356448 cuda_h.py:19] end gpu_group_einsum cost 0.0005080699920654297 seconds
DEBUG 01-15 16:10:28.356194.356194 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.356973.356973 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.357279.357279 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021386146545410156 seconds
DEBUG 01-15 16:10:28.357334.357334 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.357423.357423 cuda_h.py:19] end concat_expert_out cost 6.365776062011719e-05 seconds
DEBUG 01-15 16:10:28.357551.357551 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.357574.357574 cuda_h.py:19] end index_scatter cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:28.357555.357555 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008440017700195312 seconds
DEBUG 01-15 16:10:28.357346.357346 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.357613.357613 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.357877.357877 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012612342834472656 seconds
DEBUG 01-15 16:10:28.357679.357679 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.357980.357980 cuda_h.py:19] end concat_expert_out cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:28.358107.358107 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.358130.358130 cuda_h.py:19] end index_scatter cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:28.358131.358131 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004730224609375 seconds
DEBUG 01-15 16:10:28.358611.358611 cuda_h.py:19] end gpu_experts_multi_device cost 0.04295635223388672 seconds
DEBUG 01-15 16:10:28.358189.358189 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.05270957946777344 seconds
DEBUG 01-15 16:10:28.358507.358507 cuda_h.py:19] end prefill_layer cost 0.058008670806884766 seconds
DEBUG 01-15 16:10:28.358019.358019 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:10:28.358914.358914 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.358808.358808 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:10:28.358148.358148 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:28.358235.358235 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:28.358561.358561 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:10:28.358079.358079 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:10:28.359398.359398 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.359930.359930 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.359615.359615 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.359545.359545 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.359242.359242 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.359827.359827 cuda_h.py:19] end allocate_cuda_memory cost 0.0002086162567138672 seconds
DEBUG 01-15 16:10:28.359697.359697 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.359606.359606 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.359243.359243 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.359907.359907 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d5faeaa7-ec74-481d-800e-08b3fd0a6878
DEBUG 01-15 16:10:28.359480.359480 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.360826.360826 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.361989.361989 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d5faeaa7-ec74-481d-800e-08b3fd0a6878
DEBUG 01-15 16:10:28.361262.361262 cuda_h.py:19] end load_into_gpu_async cost 0.0016067028045654297 seconds
DEBUG 01-15 16:10:28.361349.361349 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.361214.361214 cuda_h.py:19] end restore_tensors2 cost 7.963180541992188e-05 seconds
DEBUG 01-15 16:10:28.361308.361308 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021796226501464844 seconds
INFO 01-15 16:10:28.361065.361065 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d5faeaa7-ec74-481d-800e-08b3fd0a6878
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.363474.363474 cuda_h.py:19] end self_attn cost 0.0034623146057128906 seconds
DEBUG 01-15 16:10:28.363399.363399 cuda_h.py:19] end iln_self_attn_paln cost 0.004952669143676758 seconds
DEBUG 01-15 16:10:28.364513.364513 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:10:28.364799.364799 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.364638.364638 cuda_h.py:19] end gate cost 0.0006866455078125 seconds
DEBUG 01-15 16:10:28.364421.364421 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.365557.365557 lmp.py:1912] 
DEBUG 01-15 16:10:28.365557.365557 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.365936.365936 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.365016.365016 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.365236.365236 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.365309.365309 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.365429.365429 lmp.py:1917] 
DEBUG 01-15 16:10:28.365429.365429 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.365787.365787 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.365867.365867 lmp.py:1935]   Expert  1 |     45 | CPU
DEBUG 01-15 16:10:28.365464.365464 lmp.py:1935]   Expert  7 |     59 | CPU
DEBUG 01-15 16:10:28.365107.365107 lmp.py:1935]   Expert 37 |     71 | CPU
DEBUG 01-15 16:10:28.365988.365988 lmp.py:1935]   Expert 17 |     76 | CPU
DEBUG 01-15 16:10:28.365631.365631 lmp.py:1935]   Expert 54 |     78 | CPU
DEBUG 01-15 16:10:28.365036.365036 lmp.py:1935]   Expert 18 |     84 | CPU
DEBUG 01-15 16:10:28.365633.365633 lmp.py:1935]   Expert  9 |     90 | CPU
DEBUG 01-15 16:10:28.365991.365991 lmp.py:1935]   Expert 13 |     91 | CPU
DEBUG 01-15 16:10:28.365111.365111 lmp.py:1935]   Expert 22 |    101 | CPU
DEBUG 01-15 16:10:28.365469.365469 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:10:28.365827.365827 lmp.py:1935]   Expert  0 |    107 | CPU
DEBUG 01-15 16:10:28.365470.365470 lmp.py:1935]   Expert 26 |    118 | CPU
DEBUG 01-15 16:10:28.365875.365875 lmp.py:1935]   Expert 16 |    120 | CPU
DEBUG 01-15 16:10:28.365518.365518 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:10:28.365161.365161 lmp.py:1935]   Expert 63 |    129 | CPU
DEBUG 01-15 16:10:28.365280.365280 lmp.py:1935]   Expert 59 |    133 | CPU
DEBUG 01-15 16:10:28.365923.365923 lmp.py:1935]   Expert 62 |    140 | CPU
DEBUG 01-15 16:10:28.365566.365566 lmp.py:1935]   Expert 43 |    142 | CPU
DEBUG 01-15 16:10:28.365732.365732 lmp.py:1935]   Expert 28 |    145 | CPU
DEBUG 01-15 16:10:28.365389.365389 lmp.py:1935]   Expert 33 |    147 | CPU
DEBUG 01-15 16:10:28.365794.365794 lmp.py:1935]   Expert 29 |    149 | CPU
DEBUG 01-15 16:10:28.365436.365436 lmp.py:1935]   Expert  2 |    155 | CPU
DEBUG 01-15 16:10:28.365841.365841 lmp.py:1935]   Expert 51 |    164 | CPU
DEBUG 01-15 16:10:28.365484.365484 lmp.py:1935]   Expert 45 |    165 | CPU
DEBUG 01-15 16:10:28.365173.365173 lmp.py:1935]   Expert 55 |    165 | CPU
DEBUG 01-15 16:10:28.365101.365101 lmp.py:1935]   Expert 11 |    166 | CPU
DEBUG 01-15 16:10:28.365552.365552 lmp.py:1935]   Expert 53 |    167 | CPU
DEBUG 01-15 16:10:28.365480.365480 lmp.py:1935]   Expert 23 |    168 | CPU
DEBUG 01-15 16:10:28.365169.365169 lmp.py:1935]   Expert 32 |    168 | CPU
DEBUG 01-15 16:10:28.365335.365335 lmp.py:1935]   Expert  3 |    169 | CPU
DEBUG 01-15 16:10:28.365024.365024 lmp.py:1935]   Expert 40 |    169 | CPU
DEBUG 01-15 16:10:28.365952.365952 lmp.py:1935]   Expert 14 |    173 | CPU
DEBUG 01-15 16:10:28.365549.365549 lmp.py:1935]   Expert 34 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.365622.365622 lmp.py:1935]   Expert 41 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.365219.365219 lmp.py:1935]   Expert 52 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.365816.365816 lmp.py:1935]   Expert 42 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.365459.365459 lmp.py:1935]   Expert 21 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.365294.365294 lmp.py:1935]   Expert 57 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366890.366890 lmp.py:1935]   Expert 15 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366248.366248 lmp.py:1935]   Expert 30 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366130.366130 lmp.py:1935]   Expert 35 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366965.366965 lmp.py:1935]   Expert  4 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366085.366085 lmp.py:1935]   Expert 12 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366966.366966 lmp.py:1935]   Expert 46 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366132.366132 lmp.py:1935]   Expert 19 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366775.366775 lmp.py:1935]   Expert 24 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366180.366180 lmp.py:1935]   Expert 50 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366823.366823 lmp.py:1935]   Expert  8 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366466.366466 lmp.py:1935]   Expert 44 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366347.366347 lmp.py:1935]   Expert 49 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366229.366229 lmp.py:1935]   Expert 38 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366825.366825 lmp.py:1935]   Expert 47 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366945.366945 lmp.py:1935]   Expert  6 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366018.366018 lmp.py:1935]   Expert 31 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366900.366900 lmp.py:1935]   Expert 61 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366543.366543 lmp.py:1935]   Expert 39 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366186.366186 lmp.py:1935]   Expert  5 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366067.366067 lmp.py:1935]   Expert 36 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366710.366710 lmp.py:1935]   Expert 27 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366830.366830 lmp.py:1935]   Expert 60 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366188.366188 lmp.py:1935]   Expert 20 |    338 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366023.366023 lmp.py:1935]   Expert 48 |    369 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366858.366858 lmp.py:1935]   Expert 25 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.366455.366455 lmp.py:1935]   Expert 56 |    556 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.366383.366383 lmp.py:1937] 
DEBUG 01-15 16:10:28.366383.366383 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.366026.366026 lmp.py:1938]   CPU:   4076 tokens
DEBUG 01-15 16:10:28.366907.366907 lmp.py:1942]   cuda:1:   4115 tokens (16 experts)
DEBUG 01-15 16:10:28.366550.366550 lmp.py:1942]   cuda:2:   4097 tokens (16 experts)
DEBUG 01-15 16:10:28.366239.366239 lmp.py:1943]   Total GPU:   8212 tokens
DEBUG 01-15 16:10:28.366929.366929 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.366929.366929 lmp.py:1944] 
DEBUG 01-15 16:10:28.366340.366340 cuda_h.py:19] end experts_map_get cost 0.0017006397247314453 seconds
DEBUG 01-15 16:10:28.366236.366236 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.366992.366992 lmp.py:1953] 
DEBUG 01-15 16:10:28.366992.366992 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.366060.366060 cuda_h.py:19] end cpu_experts_submit cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:28.366710.366710 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.366917.366917 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.367345.367345 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.34.gate_proj.weight': 0, 'model.layers.6.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.41.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.41.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.41.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.6.mlp.experts.36.gate_proj.weight': 0, 'model.layers.6.mlp.experts.36.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.36.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.39.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.39.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.39.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.42.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.42.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.42.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.15.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.15.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.15.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.52.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.52.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.52.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(8985247744, 5767168, 0, 0), (8991014912, 5767168, 5767168, 0), (8979480576, 5767168, 11534336, 0), (9002549248, 5767168, 17301504, 0), (9008316416, 5767168, 23068672, 0), (8996782080, 5767168, 28835840, 0), (8466202624, 5767168, 34603008, 0), (8471969792, 5767168, 40370176, 0), (8460435456, 5767168, 46137344, 0), (8483504128, 5767168, 51904512, 0), (8489271296, 5767168, 57671680, 0), (8477736960, 5767168, 63438848, 0), (8500805632, 5767168, 69206016, 0), (8506572800, 5767168, 74973184, 0), (8495038464, 5767168, 80740352, 0), (9054453760, 5767168, 86507520, 0), (9060220928, 5767168, 92274688, 0), (9048686592, 5767168, 98041856, 0), (8535408640, 5767168, 103809024, 0), (8541175808, 5767168, 109576192, 0), (8529641472, 5767168, 115343360, 0), (9106358272, 5767168, 121110528, 0), (9112125440, 5767168, 126877696, 0), (9100591104, 5767168, 132644864, 0), (8725725184, 5767168, 138412032, 0), (8731492352, 5767168, 144179200, 0), (8719958016, 5767168, 149946368, 0), (8743026688, 5767168, 155713536, 0), (8748793856, 5767168, 161480704, 0), (8737259520, 5767168, 167247872, 0), (8760328192, 5767168, 173015040, 0), (8766095360, 5767168, 178782208, 0), (8754561024, 5767168, 184549376, 0), (9365880832, 5767168, 190316544, 0), (9371648000, 5767168, 196083712, 0), (9360113664, 5767168, 201850880, 0), (8812232704, 5767168, 207618048, 0), (8817999872, 5767168, 213385216, 0), (8806465536, 5767168, 219152384, 0), (8864137216, 5767168, 224919552, 0), (8869904384, 5767168, 230686720, 0), (8858370048, 5767168, 236453888, 0), (9452388352, 5767168, 242221056, 0), (9458155520, 5767168, 247988224, 0), (9446621184, 5767168, 253755392, 0), (8916041728, 5767168, 259522560, 0), (8921808896, 5767168, 265289728, 0), (8910274560, 5767168, 271056896, 0)], 2: [(9019850752, 5767168, 0, 0), (9025617920, 5767168, 5767168, 0), (9014083584, 5767168, 11534336, 0), (9071755264, 5767168, 17301504, 0), (9077522432, 5767168, 23068672, 0), (9065988096, 5767168, 28835840, 0), (9123659776, 5767168, 34603008, 0), (9129426944, 5767168, 40370176, 0), (9117892608, 5767168, 46137344, 0), (9158262784, 5767168, 51904512, 0), (9164029952, 5767168, 57671680, 0), (9152495616, 5767168, 63438848, 0), (8604614656, 5767168, 69206016, 0), (8610381824, 5767168, 74973184, 0), (8598847488, 5767168, 80740352, 0), (9192865792, 5767168, 86507520, 0), (9198632960, 5767168, 92274688, 0), (9187098624, 5767168, 98041856, 0), (9210167296, 5767168, 103809024, 0), (9215934464, 5767168, 109576192, 0), (9204400128, 5767168, 115343360, 0), (9227468800, 5767168, 121110528, 0), (9233235968, 5767168, 126877696, 0), (9221701632, 5767168, 132644864, 0), (9244770304, 5767168, 138412032, 0), (9250537472, 5767168, 144179200, 0), (9239003136, 5767168, 149946368, 0), (9262071808, 5767168, 155713536, 0), (9267838976, 5767168, 161480704, 0), (9256304640, 5767168, 167247872, 0), (8656519168, 5767168, 173015040, 0), (8662286336, 5767168, 178782208, 0), (8650752000, 5767168, 184549376, 0), (9383182336, 5767168, 190316544, 0), (9388949504, 5767168, 196083712, 0), (9377415168, 5767168, 201850880, 0), (9296674816, 5767168, 207618048, 0), (9302441984, 5767168, 213385216, 0), (9290907648, 5767168, 219152384, 0), (8829534208, 5767168, 224919552, 0), (8835301376, 5767168, 230686720, 0), (8823767040, 5767168, 236453888, 0), (9435086848, 5767168, 242221056, 0), (9440854016, 5767168, 247988224, 0), (9429319680, 5767168, 253755392, 0), (8933343232, 5767168, 259522560, 0), (8939110400, 5767168, 265289728, 0), (8927576064, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c248150>, 2: <capsule object NULL at 0x7a5b06d22f40>}
DEBUG 01-15 16:10:28.368939.368939 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.368113.368113 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.368438.368438 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b26b460e-82d2-4d09-b7b1-8e12a33b2318
DEBUG 01-15 16:10:28.368444.368444 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:28.368643.368643 client.py:127] Model loaded
DEBUG 01-15 16:10:28.368903.368903 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.369865.369865 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:28.369167.369167 cuda_h.py:19] end restore2model cost 0.0003383159637451172 seconds
DEBUG 01-15 16:10:28.369029.369029 cuda_h.py:19] end sllm_worker_task cost 0.009917259216308594 seconds
INFO 01-15 16:10:28.369398.369398 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b26b460e-82d2-4d09-b7b1-8e12a33b2318
DEBUG 01-15 16:10:28.369842.369842 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.369156.369156 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002953052520751953 seconds
DEBUG 01-15 16:10:28.369702.369702 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.370921.370921 cuda_h.py:19] end move_flatidxs cost 0.0009105205535888672 seconds
DEBUG 01-15 16:10:28.370174.370174 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.372362.372362 cuda_h.py:19] end restore2model cost 0.0024826526641845703 seconds
DEBUG 01-15 16:10:28.372437.372437 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005674600601196289 seconds
DEBUG 01-15 16:10:28.372041.372041 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.372203.372203 cuda_h.py:19] end gpu_sexperts cost 0.0002644062042236328 seconds
DEBUG 01-15 16:10:28.372648.372648 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.372279.372279 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.3828277587890625e-05 seconds
DEBUG 01-15 16:10:28.372213.372213 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.372631.372631 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.373070.373070 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008192062377929688 seconds
DEBUG 01-15 16:10:28.373589.373589 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.374490.374490 cuda_h.py:19] end gpu_group_list cost 0.0001785755157470703 seconds
DEBUG 01-15 16:10:28.374934.374934 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.375534.375534 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008747577667236328 seconds
DEBUG 01-15 16:10:28.375623.375623 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.376616.376616 cuda_h.py:19] end gpu_group_list cost 0.0001761913299560547 seconds
DEBUG 01-15 16:10:28.376285.376285 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.376930.376930 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b26b460e-82d2-4d09-b7b1-8e12a33b2318
DEBUG 01-15 16:10:28.378093.378093 cuda_h.py:19] end group_tensors cost 0.008609533309936523 seconds
DEBUG 01-15 16:10:28.379387.379387 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.385104.385104 cuda_h.py:19] end group pad cost 0.0057430267333984375 seconds
DEBUG 01-15 16:10:28.385377.385377 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.396605.396605 client.py:127] Model loaded
DEBUG 01-15 16:10:28.396065.396065 cuda_h.py:19] end wait_experts_multi_device cost 0.01961493492126465 seconds
DEBUG 01-15 16:10:28.396672.396672 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.404308.404308 cuda_h.py:19] end group_einsum cost 0.01937723159790039 seconds
DEBUG 01-15 16:10:28.405585.405585 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.409830.409830 cuda_h.py:19] end get_outputs_cpu1 cost 0.0038836002349853516 seconds
DEBUG 01-15 16:10:28.409409.409409 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04070639610290527 seconds
DEBUG 01-15 16:10:28.410471.410471 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013712882995605469 seconds
DEBUG 01-15 16:10:28.410091.410091 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.411329.411329 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.411017.411017 cuda_h.py:19] end index_scatter cost 7.772445678710938e-05 seconds
DEBUG 01-15 16:10:28.412855.412855 cuda_h.py:19] end cpuoutputsdeal cost 0.0016102790832519531 seconds
DEBUG 01-15 16:10:28.412148.412148 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.412719.412719 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.412116.412116 cuda_h.py:19] end gpu_group_tensor cost 0.00014543533325195312 seconds
DEBUG 01-15 16:10:28.412025.412025 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.412097.412097 cuda_h.py:19] end gpu_group_tensor cost 0.00016236305236816406 seconds
DEBUG 01-15 16:10:28.412816.412816 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.413315.413315 cuda_h.py:19] end gpu_group_einsum cost 0.0005972385406494141 seconds
DEBUG 01-15 16:10:28.413757.413757 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.414656.414656 cuda_h.py:19] end gpu_group_einsum cost 0.00049591064453125 seconds
DEBUG 01-15 16:10:28.414456.414456 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.414758.414758 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.414004.414004 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020360946655273438 seconds
DEBUG 01-15 16:10:28.414820.414820 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.414625.414625 cuda_h.py:19] end concat_expert_out cost 6.413459777832031e-05 seconds
DEBUG 01-15 16:10:28.414945.414945 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.414968.414968 cuda_h.py:19] end index_scatter cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:28.415055.415055 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008444786071777344 seconds
DEBUG 01-15 16:10:28.415237.415237 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.415988.415988 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.415166.415166 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001342296600341797 seconds
DEBUG 01-15 16:10:28.415921.415921 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.415467.415467 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:28.415503.415503 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.415856.415856 cuda_h.py:19] end index_scatter cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:28.415858.415858 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004899501800537109 seconds
DEBUG 01-15 16:10:28.415237.415237 cuda_h.py:19] end gpu_experts_multi_device cost 0.0428309440612793 seconds
DEBUG 01-15 16:10:28.415008.415008 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.051737308502197266 seconds
DEBUG 01-15 16:10:28.416697.416697 cuda_h.py:19] end prefill_layer cost 0.05739188194274902 seconds
DEBUG 01-15 16:10:28.416911.416911 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:10:28.416567.416567 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.416939.416939 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:10:28.416549.416549 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:28.416305.416305 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:28.418778.418778 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 8.96453857421875e-05 seconds
DEBUG 01-15 16:10:28.418985.418985 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 0.0001513957977294922 seconds
DEBUG 01-15 16:10:28.418496.418496 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.418525.418525 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.418471.418471 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.419553.419553 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.419122.419122 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.419337.419337 cuda_h.py:19] end allocate_cuda_memory cost 0.00046753883361816406 seconds
DEBUG 01-15 16:10:28.419823.419823 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.419593.419593 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.419383.419383 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.419477.419477 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9a714581-c550-4287-be6d-12a7ab1305cd
DEBUG 01-15 16:10:28.420984.420984 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.420505.420505 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.422682.422682 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9a714581-c550-4287-be6d-12a7ab1305cd
DEBUG 01-15 16:10:28.422242.422242 cuda_h.py:19] end load_into_gpu_async cost 0.002375364303588867 seconds
DEBUG 01-15 16:10:28.422059.422059 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.422037.422037 cuda_h.py:19] end restore_tensors2 cost 8.845329284667969e-05 seconds
DEBUG 01-15 16:10:28.422846.422846 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003275632858276367 seconds
INFO 01-15 16:10:28.422630.422630 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9a714581-c550-4287-be6d-12a7ab1305cd
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.425315.425315 cuda_h.py:19] end self_attn cost 0.005120038986206055 seconds
DEBUG 01-15 16:10:28.426302.426302 cuda_h.py:19] end iln_self_attn_paln cost 0.0074422359466552734 seconds
DEBUG 01-15 16:10:28.426946.426946 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:10:28.426279.426279 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.427178.427178 cuda_h.py:19] end gate cost 0.00090789794921875 seconds
DEBUG 01-15 16:10:28.427299.427299 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.427416.427416 lmp.py:1912] 
DEBUG 01-15 16:10:28.427416.427416 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.427510.427510 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.427783.427783 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.427002.427002 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.427791.427791 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.427434.427434 lmp.py:1917] 
DEBUG 01-15 16:10:28.427434.427434 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.427553.427553 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.427395.427395 lmp.py:1935]   Expert 50 |     43 | CPU
DEBUG 01-15 16:10:28.427138.427138 lmp.py:1935]   Expert 46 |     53 | CPU
DEBUG 01-15 16:10:28.428211.428211 lmp.py:1935]   Expert  3 |     54 | CPU
DEBUG 01-15 16:10:28.428046.428046 lmp.py:1935]   Expert  1 |     78 | CPU
DEBUG 01-15 16:10:28.428643.428643 lmp.py:1935]   Expert  4 |     87 | CPU
DEBUG 01-15 16:10:28.428432.428432 lmp.py:1935]   Expert 29 |     89 | CPU
DEBUG 01-15 16:10:28.428028.428028 lmp.py:1935]   Expert 15 |     95 | CPU
DEBUG 01-15 16:10:28.428148.428148 lmp.py:1935]   Expert 40 |     97 | CPU
DEBUG 01-15 16:10:28.428745.428745 lmp.py:1935]   Expert  8 |    110 | CPU
DEBUG 01-15 16:10:28.428865.428865 lmp.py:1935]   Expert 28 |    113 | CPU
DEBUG 01-15 16:10:28.428223.428223 lmp.py:1935]   Expert 41 |    113 | CPU
DEBUG 01-15 16:10:28.428581.428581 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:10:28.428178.428178 lmp.py:1935]   Expert 27 |    127 | CPU
DEBUG 01-15 16:10:28.428298.428298 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:10:28.428848.428848 lmp.py:1935]   Expert 13 |    129 | CPU
DEBUG 01-15 16:10:28.428444.428444 lmp.py:1935]   Expert 48 |    131 | CPU
DEBUG 01-15 16:10:28.428564.428564 lmp.py:1935]   Expert 54 |    135 | CPU
DEBUG 01-15 16:10:28.428684.428684 lmp.py:1935]   Expert  7 |    136 | CPU
DEBUG 01-15 16:10:28.428281.428281 lmp.py:1935]   Expert 51 |    138 | CPU
DEBUG 01-15 16:10:28.428401.428401 lmp.py:1935]   Expert 60 |    138 | CPU
DEBUG 01-15 16:10:28.428282.428282 lmp.py:1935]   Expert 18 |    141 | CPU
DEBUG 01-15 16:10:28.428163.428163 lmp.py:1935]   Expert 39 |    141 | CPU
DEBUG 01-15 16:10:28.428045.428045 lmp.py:1935]   Expert 14 |    146 | CPU
DEBUG 01-15 16:10:28.428940.428940 lmp.py:1935]   Expert 43 |    146 | CPU
DEBUG 01-15 16:10:28.428418.428418 lmp.py:1935]   Expert 52 |    148 | CPU
DEBUG 01-15 16:10:28.428491.428491 lmp.py:1935]   Expert 20 |    149 | CPU
DEBUG 01-15 16:10:28.428849.428849 lmp.py:1935]   Expert 56 |    149 | CPU
DEBUG 01-15 16:10:28.428208.428208 lmp.py:1935]   Expert 55 |    152 | CPU
DEBUG 01-15 16:10:28.428804.428804 lmp.py:1935]   Expert 36 |    154 | CPU
DEBUG 01-15 16:10:28.428593.428593 lmp.py:1935]   Expert 10 |    156 | CPU
DEBUG 01-15 16:10:28.428667.428667 lmp.py:1935]   Expert 11 |    158 | CPU
DEBUG 01-15 16:10:28.428224.428224 lmp.py:1935]   Expert 45 |    160 | CPU
DEBUG 01-15 16:10:28.428457.428457 lmp.py:1935]   Expert  5 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.428497.428497 lmp.py:1935]   Expert 62 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.428776.428776 lmp.py:1935]   Expert 57 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.428625.428625 lmp.py:1935]   Expert 33 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.428686.428686 lmp.py:1935]   Expert 44 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.428681.428681 lmp.py:1935]   Expert 53 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.428483.428483 lmp.py:1935]   Expert 58 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.428093.428093 lmp.py:1935]   Expert 25 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.428041.428041 lmp.py:1935]   Expert 32 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.428559.428559 lmp.py:1935]   Expert  2 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.428977.428977 lmp.py:1935]   Expert 35 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.428766.428766 lmp.py:1935]   Expert 31 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.428601.428601 lmp.py:1935]   Expert 21 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.428197.428197 lmp.py:1935]   Expert 63 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.428033.428033 lmp.py:1935]   Expert 49 |    205 | GPU2(cuda:2)
INFO 01-15 16:10:28.429399.429399 client.py:127] Model loaded
DEBUG 01-15 16:10:28.429573.429573 lmp.py:1935]   Expert 17 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.429826.429826 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.429390.429390 lmp.py:1935]   Expert 42 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.429363.429363 lmp.py:1935]   Expert 34 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.429059.429059 lmp.py:1935]   Expert 37 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.429563.429563 lmp.py:1935]   Expert 59 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.429544.429544 lmp.py:1935]   Expert 22 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.429571.429571 lmp.py:1935]   Expert  0 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.429883.429883 lmp.py:1935]   Expert 19 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.429434.429434 lmp.py:1935]   Expert 24 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.429269.429269 lmp.py:1935]   Expert 61 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.429296.429296 lmp.py:1935]   Expert 30 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.429846.429846 lmp.py:1935]   Expert 47 |    321 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.429396.429396 lmp.py:1935]   Expert 38 |    365 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.429424.429424 lmp.py:1935]   Expert 26 |    374 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.429451.429451 lmp.py:1935]   Expert 12 |    425 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.429524.429524 lmp.py:1935]   Expert  9 |    679 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.429836.429836 lmp.py:1935]   Expert 23 |    701 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.429718.429718 lmp.py:1937] 
DEBUG 01-15 16:10:28.429718.429718 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.429745.429745 lmp.py:1938]   CPU:   3920 tokens
DEBUG 01-15 16:10:28.429249.429249 lmp.py:1942]   cuda:1:   4186 tokens (16 experts)
DEBUG 01-15 16:10:28.429561.429561 lmp.py:1942]   cuda:2:   4182 tokens (16 experts)
DEBUG 01-15 16:10:28.429396.429396 lmp.py:1943]   Total GPU:   8368 tokens
DEBUG 01-15 16:10:28.429515.429515 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.429515.429515 lmp.py:1944] 
DEBUG 01-15 16:10:28.429887.429887 cuda_h.py:19] end experts_map_get cost 0.002507925033569336 seconds
DEBUG 01-15 16:10:28.429514.429514 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.429429.429429 lmp.py:1953] 
DEBUG 01-15 16:10:28.429429.429429 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.430610.430610 cuda_h.py:19] end cpu_experts_submit cost 6.389617919921875e-05 seconds
DEBUG 01-15 16:10:28.430975.430975 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.430235.430235 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.430206.430206 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.2.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.2.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.2.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.58.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.58.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.58.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.37.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.37.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.37.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.38.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.38.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.38.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.7.mlp.experts.32.gate_proj.weight': 0, 'model.layers.7.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (10075242496, 5767168, 17301504, 0), (10081009664, 5767168, 23068672, 0), (10069475328, 5767168, 28835840, 0), (10092544000, 5767168, 34603008, 0), (10098311168, 5767168, 40370176, 0), (10086776832, 5767168, 46137344, 0), (9538895872, 5767168, 51904512, 0), (9544663040, 5767168, 57671680, 0), (9533128704, 5767168, 63438848, 0), (10507780096, 5767168, 69206016, 0), (10513547264, 5767168, 74973184, 0), (10502012928, 5767168, 80740352, 0), (10144448512, 5767168, 86507520, 0), (10150215680, 5767168, 92274688, 0), (10138681344, 5767168, 98041856, 0), (10161750016, 5767168, 103809024, 0), (10167517184, 5767168, 109576192, 0), (10155982848, 5767168, 115343360, 0), (9798418432, 5767168, 121110528, 0), (9804185600, 5767168, 126877696, 0), (9792651264, 5767168, 132644864, 0), (9833021440, 5767168, 138412032, 0), (9838788608, 5767168, 144179200, 0), (9827254272, 5767168, 149946368, 0), (9867624448, 5767168, 155713536, 0), (9873391616, 5767168, 161480704, 0), (9861857280, 5767168, 167247872, 0), (9902227456, 5767168, 173015040, 0), (9907994624, 5767168, 178782208, 0), (9896460288, 5767168, 184549376, 0), (9936830464, 5767168, 190316544, 0), (9942597632, 5767168, 196083712, 0), (9931063296, 5767168, 201850880, 0), (9954131968, 5767168, 207618048, 0), (9959899136, 5767168, 213385216, 0), (9948364800, 5767168, 219152384, 0), (10559684608, 5767168, 224919552, 0), (10565451776, 5767168, 230686720, 0), (10553917440, 5767168, 236453888, 0), (10576986112, 5767168, 242221056, 0), (10582753280, 5767168, 247988224, 0), (10571218944, 5767168, 253755392, 0), (10040639488, 5767168, 259522560, 0), (10046406656, 5767168, 265289728, 0), (10034872320, 5767168, 271056896, 0)], 2: [(10057940992, 5767168, 0, 0), (10063708160, 5767168, 5767168, 0), (10052173824, 5767168, 11534336, 0), (10109845504, 5767168, 17301504, 0), (10115612672, 5767168, 23068672, 0), (10104078336, 5767168, 28835840, 0), (9590800384, 5767168, 34603008, 0), (9596567552, 5767168, 40370176, 0), (9585033216, 5767168, 46137344, 0), (9660006400, 5767168, 51904512, 0), (9665773568, 5767168, 57671680, 0), (9654239232, 5767168, 63438848, 0), (10230956032, 5767168, 69206016, 0), (10236723200, 5767168, 74973184, 0), (10225188864, 5767168, 80740352, 0), (9711910912, 5767168, 86507520, 0), (9717678080, 5767168, 92274688, 0), (9706143744, 5767168, 98041856, 0), (10265559040, 5767168, 103809024, 0), (10271326208, 5767168, 109576192, 0), (10259791872, 5767168, 115343360, 0), (10317463552, 5767168, 121110528, 0), (10323230720, 5767168, 126877696, 0), (10311696384, 5767168, 132644864, 0), (10352066560, 5767168, 138412032, 0), (10357833728, 5767168, 144179200, 0), (10346299392, 5767168, 149946368, 0), (10421272576, 5767168, 155713536, 0), (10427039744, 5767168, 161480704, 0), (10415505408, 5767168, 167247872, 0), (9884925952, 5767168, 173015040, 0), (9890693120, 5767168, 178782208, 0), (9879158784, 5767168, 184549376, 0), (9919528960, 5767168, 190316544, 0), (9925296128, 5767168, 196083712, 0), (9913761792, 5767168, 201850880, 0), (10490478592, 5767168, 207618048, 0), (10496245760, 5767168, 213385216, 0), (10484711424, 5767168, 219152384, 0), (10525081600, 5767168, 224919552, 0), (10530848768, 5767168, 230686720, 0), (10519314432, 5767168, 236453888, 0), (10023337984, 5767168, 242221056, 0), (10029105152, 5767168, 247988224, 0), (10017570816, 5767168, 253755392, 0), (10594287616, 5767168, 259522560, 0), (10600054784, 5767168, 265289728, 0), (10588520448, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e3421a190>, 2: <capsule object NULL at 0x7a51b06462e0>}
DEBUG 01-15 16:10:28.431626.431626 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.431800.431800 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.431364.431364 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3ce623d7-9154-4030-81c1-3d2de46a5822
DEBUG 01-15 16:10:28.431768.431768 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:28.431121.431121 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.431102.431102 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.431468.431468 cuda_h.py:19] end restore2model cost 0.002676248550415039 seconds
DEBUG 01-15 16:10:28.431536.431536 cuda_h.py:19] end sllm_worker_task cost 0.01295924186706543 seconds
INFO 01-15 16:10:28.432165.432165 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3ce623d7-9154-4030-81c1-3d2de46a5822
DEBUG 01-15 16:10:28.432079.432079 cuda_h.py:19] end move_flatidxs cost 0.0008318424224853516 seconds
DEBUG 01-15 16:10:28.432955.432955 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.433277.433277 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0037882328033447266 seconds
DEBUG 01-15 16:10:28.434697.434697 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.437175.437175 cuda_h.py:19] end restore2model cost 0.003543853759765625 seconds
DEBUG 01-15 16:10:28.437954.437954 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007645606994628906 seconds
DEBUG 01-15 16:10:28.437571.437571 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.437864.437864 cuda_h.py:19] end group_tensors cost 0.004565715789794922 seconds
DEBUG 01-15 16:10:28.438158.438158 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.438518.438518 cuda_h.py:19] end gpu_sexperts cost 0.00037789344787597656 seconds
DEBUG 01-15 16:10:28.438507.438507 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.438119.438119 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.574920654296875e-05 seconds
DEBUG 01-15 16:10:28.438290.438290 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.438981.438981 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.440867.440867 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0018620491027832031 seconds
DEBUG 01-15 16:10:28.440348.440348 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.441144.441144 cuda_h.py:19] end gpu_group_list cost 0.0003581047058105469 seconds
DEBUG 01-15 16:10:28.442973.442973 cuda_h.py:19] end group pad cost 0.00451207160949707 seconds
DEBUG 01-15 16:10:28.442955.442955 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:28.442226.442226 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.447945.447945 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.004866361618041992 seconds
DEBUG 01-15 16:10:28.448004.448004 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.448007.448007 cuda_h.py:19] end gpu_group_list cost 0.00040841102600097656 seconds
DEBUG 01-15 16:10:28.451548.451548 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.451450.451450 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3ce623d7-9154-4030-81c1-3d2de46a5822
INFO 01-15 16:10:28.459687.459687 client.py:127] Model loaded
DEBUG 01-15 16:10:28.460788.460788 cuda_h.py:19] end wait_experts_multi_device cost 0.00817561149597168 seconds
DEBUG 01-15 16:10:28.460513.460513 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.462855.462855 cuda_h.py:19] end group_einsum cost 0.019473791122436523 seconds
DEBUG 01-15 16:10:28.462283.462283 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.466939.466939 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036766529083251953 seconds
DEBUG 01-15 16:10:28.466193.466193 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03518271446228027 seconds
DEBUG 01-15 16:10:28.467882.467882 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007025718688964844 seconds
DEBUG 01-15 16:10:28.467799.467799 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.468540.468540 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.469666.469666 cuda_h.py:19] end index_scatter cost 8.320808410644531e-05 seconds
DEBUG 01-15 16:10:28.469762.469762 cuda_h.py:19] end cpuoutputsdeal cost 0.002039670944213867 seconds
DEBUG 01-15 16:10:28.469487.469487 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.469250.469250 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.469653.469653 cuda_h.py:19] end gpu_group_tensor cost 0.00015926361083984375 seconds
DEBUG 01-15 16:10:28.469562.469562 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.469640.469640 cuda_h.py:19] end gpu_group_tensor cost 0.00013256072998046875 seconds
DEBUG 01-15 16:10:28.470320.470320 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.470819.470819 cuda_h.py:19] end gpu_group_einsum cost 0.00063323974609375 seconds
DEBUG 01-15 16:10:28.470162.470162 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.471446.471446 cuda_h.py:19] end gpu_group_einsum cost 0.0004971027374267578 seconds
DEBUG 01-15 16:10:28.471344.471344 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.471938.471938 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.471344.471344 cuda_h.py:19] end all_expert_outputs_slices cost 0.000217437744140625 seconds
DEBUG 01-15 16:10:28.471921.471921 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.472196.472196 cuda_h.py:19] end concat_expert_out cost 5.9604644775390625e-05 seconds
DEBUG 01-15 16:10:28.472086.472086 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.472486.472486 cuda_h.py:19] end index_scatter cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:28.472541.472541 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008618831634521484 seconds
DEBUG 01-15 16:10:28.472206.472206 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.472811.472811 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.472790.472790 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012755393981933594 seconds
DEBUG 01-15 16:10:28.472639.472639 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.472893.472893 cuda_h.py:19] end concat_expert_out cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:28.472597.472597 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.473759.473759 cuda_h.py:19] end index_scatter cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:28.473568.473568 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00047278404235839844 seconds
DEBUG 01-15 16:10:28.473570.473570 cuda_h.py:19] end gpu_experts_multi_device cost 0.03463482856750488 seconds
DEBUG 01-15 16:10:28.473334.473334 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.04695558547973633 seconds
DEBUG 01-15 16:10:28.473381.473381 cuda_h.py:19] end prefill_layer cost 0.057308197021484375 seconds
DEBUG 01-15 16:10:28.473986.473986 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:10:28.473689.473689 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.473584.473584 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:10:28.473717.473717 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:28.473950.473950 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:28.473992.473992 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:10:28.473510.473510 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 7.104873657226562e-05 seconds
DEBUG 01-15 16:10:28.473636.473636 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.474268.474268 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.474787.474787 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.474056.474056 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.474277.474277 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.474981.474981 cuda_h.py:19] end allocate_cuda_memory cost 0.00023698806762695312 seconds
DEBUG 01-15 16:10:28.474375.474375 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.474568.474568 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.474967.474967 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.474200.474200 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9f17101b-35f2-4f1e-b95a-33564f974aca
DEBUG 01-15 16:10:28.474011.474011 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.475631.475631 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.476322.476322 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9f17101b-35f2-4f1e-b95a-33564f974aca
DEBUG 01-15 16:10:28.476881.476881 cuda_h.py:19] end load_into_gpu_async cost 0.002020597457885742 seconds
DEBUG 01-15 16:10:28.476637.476637 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.476316.476316 cuda_h.py:19] end restore_tensors2 cost 8.273124694824219e-05 seconds
DEBUG 01-15 16:10:28.476887.476887 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0026144981384277344 seconds
INFO 01-15 16:10:28.476399.476399 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9f17101b-35f2-4f1e-b95a-33564f974aca
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.478381.478381 cuda_h.py:19] end self_attn cost 0.0035343170166015625 seconds
DEBUG 01-15 16:10:28.479604.479604 cuda_h.py:19] end iln_self_attn_paln cost 0.00513911247253418 seconds
DEBUG 01-15 16:10:28.479580.479580 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:10:28.479442.479442 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.479176.479176 cuda_h.py:19] end gate cost 0.0007119178771972656 seconds
DEBUG 01-15 16:10:28.479959.479959 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.480247.480247 lmp.py:1912] 
DEBUG 01-15 16:10:28.480247.480247 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.480295.480295 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.480852.480852 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.480992.480992 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.480827.480827 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.480470.480470 lmp.py:1917] 
DEBUG 01-15 16:10:28.480470.480470 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.480782.480782 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.480386.480386 lmp.py:1935]   Expert 38 |     12 | CPU
DEBUG 01-15 16:10:28.480267.480267 lmp.py:1935]   Expert 39 |     58 | CPU
DEBUG 01-15 16:10:28.480672.480672 lmp.py:1935]   Expert  7 |     73 | CPU
DEBUG 01-15 16:10:28.480599.480599 lmp.py:1935]   Expert 30 |     74 | CPU
DEBUG 01-15 16:10:28.480050.480050 lmp.py:1935]   Expert 14 |     93 | CPU
DEBUG 01-15 16:10:28.480216.480216 lmp.py:1935]   Expert 24 |     93 | CPU
DEBUG 01-15 16:10:28.480144.480144 lmp.py:1935]   Expert 27 |     94 | CPU
DEBUG 01-15 16:10:28.480833.480833 lmp.py:1935]   Expert 36 |     97 | CPU
DEBUG 01-15 16:10:28.480761.480761 lmp.py:1935]   Expert 40 |     97 | CPU
DEBUG 01-15 16:10:28.480643.480643 lmp.py:1935]   Expert 17 |     99 | CPU
DEBUG 01-15 16:10:28.480239.480239 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:10:28.480882.480882 lmp.py:1935]   Expert 32 |    106 | CPU
DEBUG 01-15 16:10:28.480525.480525 lmp.py:1935]   Expert 18 |    110 | CPU
DEBUG 01-15 16:10:28.480705.480705 lmp.py:1935]   Expert 48 |    111 | CPU
DEBUG 01-15 16:10:28.480156.480156 lmp.py:1935]   Expert 12 |    112 | CPU
DEBUG 01-15 16:10:28.480130.480130 lmp.py:1935]   Expert  1 |    114 | CPU
DEBUG 01-15 16:10:28.480104.480104 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:10:28.480078.480078 lmp.py:1935]   Expert 59 |    131 | CPU
DEBUG 01-15 16:10:28.480529.480529 lmp.py:1935]   Expert 42 |    135 | CPU
DEBUG 01-15 16:10:28.480503.480503 lmp.py:1935]   Expert  0 |    141 | CPU
DEBUG 01-15 16:10:28.480954.480954 lmp.py:1935]   Expert 22 |    146 | CPU
DEBUG 01-15 16:10:28.480120.480120 lmp.py:1935]   Expert 51 |    148 | CPU
DEBUG 01-15 16:10:28.480809.480809 lmp.py:1935]   Expert 53 |    148 | CPU
DEBUG 01-15 16:10:28.480498.480498 lmp.py:1935]   Expert  8 |    161 | CPU
DEBUG 01-15 16:10:28.480949.480949 lmp.py:1935]   Expert 44 |    166 | CPU
DEBUG 01-15 16:10:28.480400.480400 lmp.py:1935]   Expert 60 |    169 | CPU
DEBUG 01-15 16:10:28.480613.480613 lmp.py:1935]   Expert 15 |    170 | CPU
DEBUG 01-15 16:10:28.480348.480348 lmp.py:1935]   Expert 29 |    171 | CPU
DEBUG 01-15 16:10:28.480845.480845 lmp.py:1935]   Expert 54 |    173 | CPU
DEBUG 01-15 16:10:28.480581.480581 lmp.py:1935]   Expert 35 |    178 | CPU
DEBUG 01-15 16:10:28.480317.480317 lmp.py:1935]   Expert 34 |    180 | CPU
DEBUG 01-15 16:10:28.481814.481814 lmp.py:1935]   Expert 33 |    181 | CPU
DEBUG 01-15 16:10:28.481457.481457 lmp.py:1935]   Expert 47 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481623.481623 lmp.py:1935]   Expert 19 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481551.481551 lmp.py:1935]   Expert  9 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481717.481717 lmp.py:1935]   Expert  3 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481645.481645 lmp.py:1935]   Expert 21 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481811.481811 lmp.py:1935]   Expert 46 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481454.481454 lmp.py:1935]   Expert 45 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481858.481858 lmp.py:1935]   Expert 56 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481501.481501 lmp.py:1935]   Expert 20 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481144.481144 lmp.py:1935]   Expert 49 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481310.481310 lmp.py:1935]   Expert 28 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481476.481476 lmp.py:1935]   Expert 57 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481643.481643 lmp.py:1935]   Expert  2 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481570.481570 lmp.py:1935]   Expert 13 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481260.481260 lmp.py:1935]   Expert  4 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481711.481711 lmp.py:1935]   Expert 43 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481161.481161 lmp.py:1935]   Expert 10 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481851.481851 lmp.py:1935]   Expert 50 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481778.481778 lmp.py:1935]   Expert 41 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481468.481468 lmp.py:1935]   Expert 26 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481872.481872 lmp.py:1935]   Expert 63 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481277.481277 lmp.py:1935]   Expert 37 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481735.481735 lmp.py:1935]   Expert 61 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481093.481093 lmp.py:1935]   Expert 31 |    273 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481497.481497 lmp.py:1935]   Expert 52 |    305 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481663.481663 lmp.py:1935]   Expert 58 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481591.481591 lmp.py:1935]   Expert 62 |    326 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481280.481280 lmp.py:1935]   Expert 55 |    341 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481208.481208 lmp.py:1935]   Expert 11 |    381 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481374.481374 lmp.py:1935]   Expert 23 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481064.481064 lmp.py:1935]   Expert 25 |    408 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.481991.481991 lmp.py:1935]   Expert  5 |    513 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.481648.481648 lmp.py:1937] 
DEBUG 01-15 16:10:28.481648.481648 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.481814.481814 lmp.py:1938]   CPU:   3974 tokens
DEBUG 01-15 16:10:28.481219.481219 lmp.py:1942]   cuda:1:   4177 tokens (16 experts)
DEBUG 01-15 16:10:28.481385.481385 lmp.py:1942]   cuda:2:   4137 tokens (16 experts)
DEBUG 01-15 16:10:28.481359.481359 lmp.py:1943]   Total GPU:   8314 tokens
DEBUG 01-15 16:10:28.481141.481141 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.481141.481141 lmp.py:1944] 
DEBUG 01-15 16:10:28.481790.481790 cuda_h.py:19] end experts_map_get cost 0.001672506332397461 seconds
DEBUG 01-15 16:10:28.481495.481495 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.481396.481396 lmp.py:1953] 
DEBUG 01-15 16:10:28.481396.481396 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.481941.481941 cuda_h.py:19] end cpu_experts_submit cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:28.481896.481896 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.481149.481149 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.483599.483599 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.52.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.52.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.52.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.8.mlp.experts.26.gate_proj.weight': 0, 'model.layers.8.mlp.experts.26.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.26.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.43.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.43.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.43.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.49.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.49.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.49.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.57.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.57.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.57.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (10680795136, 5767168, 17301504, 0), (10686562304, 5767168, 23068672, 0), (10675027968, 5767168, 28835840, 0), (10698096640, 5767168, 34603008, 0), (10703863808, 5767168, 40370176, 0), (10692329472, 5767168, 46137344, 0), (11320950784, 5767168, 51904512, 0), (11326717952, 5767168, 57671680, 0), (11315183616, 5767168, 63438848, 0), (10784604160, 5767168, 69206016, 0), (10790371328, 5767168, 74973184, 0), (10778836992, 5767168, 80740352, 0), (10801905664, 5767168, 86507520, 0), (10807672832, 5767168, 92274688, 0), (10796138496, 5767168, 98041856, 0), (10767302656, 5767168, 103809024, 0), (10773069824, 5767168, 109576192, 0), (10761535488, 5767168, 115343360, 0), (11407458304, 5767168, 121110528, 0), (11413225472, 5767168, 126877696, 0), (11401691136, 5767168, 132644864, 0), (11424759808, 5767168, 138412032, 0), (11430526976, 5767168, 144179200, 0), (11418992640, 5767168, 149946368, 0), (11511267328, 5767168, 155713536, 0), (11517034496, 5767168, 161480704, 0), (11505500160, 5767168, 167247872, 0), (10957619200, 5767168, 173015040, 0), (10963386368, 5767168, 178782208, 0), (10951852032, 5767168, 184549376, 0), (11580473344, 5767168, 190316544, 0), (11586240512, 5767168, 196083712, 0), (11574706176, 5767168, 201850880, 0), (11096031232, 5767168, 207618048, 0), (11101798400, 5767168, 213385216, 0), (11090264064, 5767168, 219152384, 0), (11666980864, 5767168, 224919552, 0), (11672748032, 5767168, 230686720, 0), (11661213696, 5767168, 236453888, 0), (11684282368, 5767168, 242221056, 0), (11690049536, 5767168, 247988224, 0), (11678515200, 5767168, 253755392, 0), (11701583872, 5767168, 259522560, 0), (11707351040, 5767168, 265289728, 0), (11695816704, 5767168, 271056896, 0)], 2: [(11061428224, 5767168, 0, 0), (11067195392, 5767168, 5767168, 0), (11055661056, 5767168, 11534336, 0), (10663493632, 5767168, 17301504, 0), (10669260800, 5767168, 23068672, 0), (10657726464, 5767168, 28835840, 0), (11251744768, 5767168, 34603008, 0), (11257511936, 5767168, 40370176, 0), (11245977600, 5767168, 46137344, 0), (11355553792, 5767168, 51904512, 0), (11361320960, 5767168, 57671680, 0), (11349786624, 5767168, 63438848, 0), (10836508672, 5767168, 69206016, 0), (10842275840, 5767168, 74973184, 0), (10830741504, 5767168, 80740352, 0), (11390156800, 5767168, 86507520, 0), (11395923968, 5767168, 92274688, 0), (11384389632, 5767168, 98041856, 0), (11459362816, 5767168, 103809024, 0), (11465129984, 5767168, 109576192, 0), (11453595648, 5767168, 115343360, 0), (11476664320, 5767168, 121110528, 0), (11482431488, 5767168, 126877696, 0), (11470897152, 5767168, 132644864, 0), (10940317696, 5767168, 138412032, 0), (10946084864, 5767168, 144179200, 0), (10934550528, 5767168, 149946368, 0), (11009523712, 5767168, 155713536, 0), (11015290880, 5767168, 161480704, 0), (11003756544, 5767168, 167247872, 0), (10974920704, 5767168, 173015040, 0), (10980687872, 5767168, 178782208, 0), (10969153536, 5767168, 184549376, 0), (11563171840, 5767168, 190316544, 0), (11568939008, 5767168, 196083712, 0), (11557404672, 5767168, 201850880, 0), (11044126720, 5767168, 207618048, 0), (11049893888, 5767168, 213385216, 0), (11038359552, 5767168, 219152384, 0), (11615076352, 5767168, 224919552, 0), (11620843520, 5767168, 230686720, 0), (11609309184, 5767168, 236453888, 0), (11597774848, 5767168, 242221056, 0), (11603542016, 5767168, 247988224, 0), (11592007680, 5767168, 253755392, 0), (11147935744, 5767168, 259522560, 0), (11153702912, 5767168, 265289728, 0), (11142168576, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c2c09f0>, 2: <capsule object NULL at 0x7a51b0646070>}
DEBUG 01-15 16:10:28.483366.483366 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.483487.483487 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.483336.483336 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2738fa25-e43b-4c85-9d31-bdb38581a9b8
INFO 01-15 16:10:28.483239.483239 client.py:127] Model loaded
DEBUG 01-15 16:10:28.483127.483127 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:28.483724.483724 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.483073.483073 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.484713.484713 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.484610.484610 cuda_h.py:19] end restore2model cost 0.00048279762268066406 seconds
DEBUG 01-15 16:10:28.484532.484532 cuda_h.py:19] end sllm_worker_task cost 0.010262250900268555 seconds
DEBUG 01-15 16:10:28.485843.485843 cuda_h.py:19] end move_flatidxs cost 0.0008313655853271484 seconds
DEBUG 01-15 16:10:28.485017.485017 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:28.485497.485497 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2738fa25-e43b-4c85-9d31-bdb38581a9b8
DEBUG 01-15 16:10:28.485231.485231 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0037841796875 seconds
DEBUG 01-15 16:10:28.485492.485492 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.488849.488849 cuda_h.py:19] end restore2model cost 0.0027196407318115234 seconds
DEBUG 01-15 16:10:28.488905.488905 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006760597229003906 seconds
DEBUG 01-15 16:10:28.488700.488700 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.488267.488267 cuda_h.py:19] end gpu_sexperts cost 0.0002815723419189453 seconds
DEBUG 01-15 16:10:28.489619.489619 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.489634.489634 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:10:28.489807.489807 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.489040.489040 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.490329.490329 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014789104461669922 seconds
DEBUG 01-15 16:10:28.490153.490153 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.490783.490783 cuda_h.py:19] end gpu_group_list cost 0.0001728534698486328 seconds
DEBUG 01-15 16:10:28.491352.491352 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.492187.492187 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009300708770751953 seconds
DEBUG 01-15 16:10:28.492812.492812 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.493997.493997 cuda_h.py:19] end gpu_group_list cost 0.0001761913299560547 seconds
DEBUG 01-15 16:10:28.493660.493660 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.493019.493019 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2738fa25-e43b-4c85-9d31-bdb38581a9b8
DEBUG 01-15 16:10:28.495771.495771 cuda_h.py:19] end group_tensors cost 0.010201692581176758 seconds
DEBUG 01-15 16:10:28.496079.496079 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.499939.499939 cuda_h.py:19] end group pad cost 0.003461122512817383 seconds
DEBUG 01-15 16:10:28.499305.499305 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.512956.512956 client.py:127] Model loaded
DEBUG 01-15 16:10:28.512776.512776 cuda_h.py:19] end wait_experts_multi_device cost 0.018625736236572266 seconds
DEBUG 01-15 16:10:28.512538.512538 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.519410.519410 cuda_h.py:19] end group_einsum cost 0.02023148536682129 seconds
DEBUG 01-15 16:10:28.520296.520296 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.523383.523383 cuda_h.py:19] end get_outputs_cpu1 cost 0.0037169456481933594 seconds
DEBUG 01-15 16:10:28.524552.524552 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04091072082519531 seconds
DEBUG 01-15 16:10:28.525722.525722 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012928247451782227 seconds
DEBUG 01-15 16:10:28.525480.525480 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.526302.526302 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.526016.526016 cuda_h.py:19] end index_scatter cost 7.700920104980469e-05 seconds
DEBUG 01-15 16:10:28.527563.527563 cuda_h.py:19] end cpuoutputsdeal cost 0.0016369819641113281 seconds
DEBUG 01-15 16:10:28.527426.527426 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.527043.527043 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.527181.527181 cuda_h.py:19] end gpu_group_tensor cost 0.00013828277587890625 seconds
DEBUG 01-15 16:10:28.527560.527560 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.527194.527194 cuda_h.py:19] end gpu_group_tensor cost 0.00012302398681640625 seconds
DEBUG 01-15 16:10:28.527807.527807 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.528974.528974 cuda_h.py:19] end gpu_group_einsum cost 0.0005993843078613281 seconds
DEBUG 01-15 16:10:28.528629.528629 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.529077.529077 cuda_h.py:19] end gpu_group_einsum cost 0.0004782676696777344 seconds
DEBUG 01-15 16:10:28.529161.529161 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.529516.529516 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.529346.529346 cuda_h.py:19] end all_expert_outputs_slices cost 0.000213623046875 seconds
DEBUG 01-15 16:10:28.529401.529401 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.529921.529921 cuda_h.py:19] end concat_expert_out cost 6.4849853515625e-05 seconds
DEBUG 01-15 16:10:28.529095.529095 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.529548.529548 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:28.530205.530205 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008516311645507812 seconds
DEBUG 01-15 16:10:28.530672.530672 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.530091.530091 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.530123.530123 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013065338134765625 seconds
DEBUG 01-15 16:10:28.530879.530879 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.530995.530995 cuda_h.py:19] end concat_expert_out cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:10:28.530930.530930 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.530238.530238 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:28.530808.530808 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004851818084716797 seconds
DEBUG 01-15 16:10:28.530235.530235 cuda_h.py:19] end gpu_experts_multi_device cost 0.04161953926086426 seconds
DEBUG 01-15 16:10:28.530807.530807 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.051642656326293945 seconds
DEBUG 01-15 16:10:28.531064.531064 cuda_h.py:19] end prefill_layer cost 0.05744314193725586 seconds
DEBUG 01-15 16:10:28.531754.531754 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:10:28.531934.531934 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.531114.531114 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:10:28.531347.531347 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:28.531910.531910 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:28.531383.531383 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 3.910064697265625e-05 seconds
DEBUG 01-15 16:10:28.531477.531477 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 7.605552673339844e-05 seconds
DEBUG 01-15 16:10:28.531795.531795 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.531758.531758 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.531728.531728 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.531016.531016 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.531906.531906 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.532260.532260 cuda_h.py:19] end allocate_cuda_memory cost 0.00025582313537597656 seconds
DEBUG 01-15 16:10:28.532568.532568 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.532106.532106 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.532366.532366 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.532122.532122 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 087ec0fb-d20e-452d-ba06-817ead1ae0cd
DEBUG 01-15 16:10:28.532411.532411 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.532354.532354 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.533127.533127 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 087ec0fb-d20e-452d-ba06-817ead1ae0cd
DEBUG 01-15 16:10:28.533877.533877 cuda_h.py:19] end load_into_gpu_async cost 0.0014808177947998047 seconds
DEBUG 01-15 16:10:28.533487.533487 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.533498.533498 cuda_h.py:19] end restore_tensors2 cost 8.20159912109375e-05 seconds
DEBUG 01-15 16:10:28.533307.533307 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021109580993652344 seconds
INFO 01-15 16:10:28.533554.533554 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 087ec0fb-d20e-452d-ba06-817ead1ae0cd
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.535952.535952 cuda_h.py:19] end self_attn cost 0.0031404495239257812 seconds
DEBUG 01-15 16:10:28.536969.536969 cuda_h.py:19] end iln_self_attn_paln cost 0.0047647953033447266 seconds
DEBUG 01-15 16:10:28.536183.536183 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:10:28.536707.536707 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.537256.537256 cuda_h.py:19] end gate cost 0.0007011890411376953 seconds
DEBUG 01-15 16:10:28.537953.537953 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.537509.537509 lmp.py:1912] 
DEBUG 01-15 16:10:28.537509.537509 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.537716.537716 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.537803.537803 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.537545.537545 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.537665.537665 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.537354.537354 lmp.py:1917] 
DEBUG 01-15 16:10:28.537354.537354 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.537759.537759 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.537362.537362 lmp.py:1935]   Expert 24 |     39 | CPU
DEBUG 01-15 16:10:28.537005.537005 lmp.py:1935]   Expert  2 |     46 | CPU
DEBUG 01-15 16:10:28.537410.537410 lmp.py:1935]   Expert 32 |     64 | CPU
DEBUG 01-15 16:10:28.537337.537337 lmp.py:1935]   Expert 26 |     65 | CPU
DEBUG 01-15 16:10:28.537504.537504 lmp.py:1935]   Expert 19 |     69 | CPU
DEBUG 01-15 16:10:28.537955.537955 lmp.py:1935]   Expert 50 |     70 | CPU
DEBUG 01-15 16:10:28.537644.537644 lmp.py:1935]   Expert 15 |     78 | CPU
DEBUG 01-15 16:10:28.537101.537101 lmp.py:1935]   Expert  4 |     81 | CPU
DEBUG 01-15 16:10:28.537791.537791 lmp.py:1935]   Expert 28 |     83 | CPU
DEBUG 01-15 16:10:28.537480.537480 lmp.py:1935]   Expert 60 |     83 | CPU
DEBUG 01-15 16:10:28.537931.537931 lmp.py:1935]   Expert  7 |     84 | CPU
DEBUG 01-15 16:10:28.537826.537826 lmp.py:1935]   Expert 59 |     90 | CPU
DEBUG 01-15 16:10:28.537277.537277 lmp.py:1935]   Expert 49 |     98 | CPU
DEBUG 01-15 16:10:28.537012.537012 lmp.py:1935]   Expert 23 |     99 | CPU
DEBUG 01-15 16:10:28.537225.537225 lmp.py:1935]   Expert  5 |    102 | CPU
DEBUG 01-15 16:10:28.537437.537437 lmp.py:1935]   Expert 12 |    105 | CPU
DEBUG 01-15 16:10:28.538411.538411 lmp.py:1935]   Expert 10 |    111 | CPU
DEBUG 01-15 16:10:28.538624.538624 lmp.py:1935]   Expert 27 |    112 | CPU
DEBUG 01-15 16:10:28.538836.538836 lmp.py:1935]   Expert  3 |    123 | CPU
DEBUG 01-15 16:10:28.538764.538764 lmp.py:1935]   Expert 41 |    124 | CPU
DEBUG 01-15 16:10:28.538215.538215 lmp.py:1935]   Expert 25 |    129 | CPU
DEBUG 01-15 16:10:28.538189.538189 lmp.py:1935]   Expert 13 |    130 | CPU
DEBUG 01-15 16:10:28.538163.538163 lmp.py:1935]   Expert 20 |    130 | CPU
DEBUG 01-15 16:10:28.538376.538376 lmp.py:1935]   Expert 40 |    130 | CPU
DEBUG 01-15 16:10:28.538350.538350 lmp.py:1935]   Expert 16 |    132 | CPU
DEBUG 01-15 16:10:28.538324.538324 lmp.py:1935]   Expert 37 |    145 | CPU
DEBUG 01-15 16:10:28.538536.538536 lmp.py:1935]   Expert 35 |    146 | CPU
DEBUG 01-15 16:10:28.538987.538987 lmp.py:1935]   Expert 17 |    149 | CPU
DEBUG 01-15 16:10:28.538723.538723 lmp.py:1935]   Expert 47 |    150 | CPU
DEBUG 01-15 16:10:28.538697.538697 lmp.py:1935]   Expert 22 |    157 | CPU
DEBUG 01-15 16:10:28.538432.538432 lmp.py:1935]   Expert 53 |    168 | CPU
DEBUG 01-15 16:10:28.538406.538406 lmp.py:1935]   Expert 39 |    171 | CPU
DEBUG 01-15 16:10:28.538017.538017 lmp.py:1935]   Expert 38 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538898.538898 lmp.py:1935]   Expert 44 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538064.538064 lmp.py:1935]   Expert 36 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538992.538992 lmp.py:1935]   Expert 58 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538919.538919 lmp.py:1935]   Expert 52 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538039.538039 lmp.py:1935]   Expert 18 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538205.538205 lmp.py:1935]   Expert 62 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538186.538186 lmp.py:1935]   Expert 11 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538114.538114 lmp.py:1935]   Expert 48 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538803.538803 lmp.py:1935]   Expert 30 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538016.538016 lmp.py:1935]   Expert 14 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538228.538228 lmp.py:1935]   Expert  1 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538441.538441 lmp.py:1935]   Expert 42 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538892.538892 lmp.py:1935]   Expert 45 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538104.538104 lmp.py:1935]   Expert 31 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538316.538316 lmp.py:1935]   Expert  6 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538006.538006 lmp.py:1935]   Expert 51 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538218.538218 lmp.py:1935]   Expert 29 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538431.538431 lmp.py:1935]   Expert 34 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538312.538312 lmp.py:1935]   Expert 33 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538001.538001 lmp.py:1935]   Expert 57 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538691.538691 lmp.py:1935]   Expert 61 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538903.538903 lmp.py:1935]   Expert 43 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538546.538546 lmp.py:1935]   Expert  0 |    320 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538712.538712 lmp.py:1935]   Expert 46 |    348 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538402.538402 lmp.py:1935]   Expert  8 |    382 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538806.538806 lmp.py:1935]   Expert  9 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538972.538972 lmp.py:1935]   Expert 56 |    393 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538900.538900 lmp.py:1935]   Expert 54 |    396 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538305.538305 lmp.py:1935]   Expert 63 |    405 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538232.538232 lmp.py:1935]   Expert 55 |    424 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.538398.538398 lmp.py:1935]   Expert 21 |    490 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.538611.538611 lmp.py:1937] 
DEBUG 01-15 16:10:28.538611.538611 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.538015.538015 lmp.py:1938]   CPU:   3463 tokens
DEBUG 01-15 16:10:28.538420.538420 lmp.py:1942]   cuda:1:   4411 tokens (16 experts)
DEBUG 01-15 16:10:28.538778.538778 lmp.py:1942]   cuda:2:   4414 tokens (16 experts)
DEBUG 01-15 16:10:28.538706.538706 lmp.py:1943]   Total GPU:   8825 tokens
DEBUG 01-15 16:10:28.538918.538918 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.538918.538918 lmp.py:1944] 
DEBUG 01-15 16:10:28.538330.538330 cuda_h.py:19] end experts_map_get cost 0.0017213821411132812 seconds
DEBUG 01-15 16:10:28.539895.539895 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.539220.539220 lmp.py:1953] 
DEBUG 01-15 16:10:28.539220.539220 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.539103.539103 cuda_h.py:19] end cpu_experts_submit cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:28.539037.539037 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.539483.539483 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.541636.541636 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.34.gate_proj.weight': 0, 'model.layers.9.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.43.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.43.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.43.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.44.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.44.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.44.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.45.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.45.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.45.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.46.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.46.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.46.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.54.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.54.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.54.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.62.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.62.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.62.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.56.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.56.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.56.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.1.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.1.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.1.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.9.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.9.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.9.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.42.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.42.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.42.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.11.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.11.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.11.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.48.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.48.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.48.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.52.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.52.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.52.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.55.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.55.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.55.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(12307136512, 5767168, 0, 0), (12312903680, 5767168, 5767168, 0), (12301369344, 5767168, 11534336, 0), (12462850048, 5767168, 17301504, 0), (12468617216, 5767168, 23068672, 0), (12457082880, 5767168, 28835840, 0), (12480151552, 5767168, 34603008, 0), (12485918720, 5767168, 40370176, 0), (12474384384, 5767168, 46137344, 0), (12497453056, 5767168, 51904512, 0), (12503220224, 5767168, 57671680, 0), (12491685888, 5767168, 63438848, 0), (12514754560, 5767168, 69206016, 0), (12520521728, 5767168, 74973184, 0), (12508987392, 5767168, 80740352, 0), (11961106432, 5767168, 86507520, 0), (11966873600, 5767168, 92274688, 0), (11955339264, 5767168, 98041856, 0), (12030312448, 5767168, 103809024, 0), (12036079616, 5767168, 109576192, 0), (12024545280, 5767168, 115343360, 0), (12082216960, 5767168, 121110528, 0), (12087984128, 5767168, 126877696, 0), (12076449792, 5767168, 132644864, 0), (12653166592, 5767168, 138412032, 0), (12658933760, 5767168, 144179200, 0), (12647399424, 5767168, 149946368, 0), (12791578624, 5767168, 155713536, 0), (12797345792, 5767168, 161480704, 0), (12785811456, 5767168, 167247872, 0), (12687769600, 5767168, 173015040, 0), (12693536768, 5767168, 178782208, 0), (12682002432, 5767168, 184549376, 0), (12705071104, 5767168, 190316544, 0), (12710838272, 5767168, 196083712, 0), (12699303936, 5767168, 201850880, 0), (12722372608, 5767168, 207618048, 0), (12728139776, 5767168, 213385216, 0), (12716605440, 5767168, 219152384, 0), (12220628992, 5767168, 224919552, 0), (12226396160, 5767168, 230686720, 0), (12214861824, 5767168, 236453888, 0), (12237930496, 5767168, 242221056, 0), (12243697664, 5767168, 247988224, 0), (12232163328, 5767168, 253755392, 0), (12255232000, 5767168, 259522560, 0), (12260999168, 5767168, 265289728, 0), (12249464832, 5767168, 271056896, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (12289835008, 5767168, 17301504, 0), (12295602176, 5767168, 23068672, 0), (12284067840, 5767168, 28835840, 0), (11736186880, 5767168, 34603008, 0), (11741954048, 5767168, 40370176, 0), (11730419712, 5767168, 46137344, 0), (12341739520, 5767168, 51904512, 0), (12347506688, 5767168, 57671680, 0), (12335972352, 5767168, 63438848, 0), (11822694400, 5767168, 69206016, 0), (11828461568, 5767168, 74973184, 0), (11816927232, 5767168, 80740352, 0), (12376342528, 5767168, 86507520, 0), (12382109696, 5767168, 92274688, 0), (12370575360, 5767168, 98041856, 0), (11857297408, 5767168, 103809024, 0), (11863064576, 5767168, 109576192, 0), (11851530240, 5767168, 115343360, 0), (11874598912, 5767168, 121110528, 0), (11880366080, 5767168, 126877696, 0), (11868831744, 5767168, 132644864, 0), (12445548544, 5767168, 138412032, 0), (12451315712, 5767168, 144179200, 0), (12439781376, 5767168, 149946368, 0), (11909201920, 5767168, 155713536, 0), (11914969088, 5767168, 161480704, 0), (11903434752, 5767168, 167247872, 0), (12549357568, 5767168, 173015040, 0), (12555124736, 5767168, 178782208, 0), (12543590400, 5767168, 184549376, 0), (12601262080, 5767168, 190316544, 0), (12607029248, 5767168, 196083712, 0), (12595494912, 5767168, 201850880, 0), (12618563584, 5767168, 207618048, 0), (12624330752, 5767168, 213385216, 0), (12612796416, 5767168, 219152384, 0), (12670468096, 5767168, 224919552, 0), (12676235264, 5767168, 230686720, 0), (12664700928, 5767168, 236453888, 0), (12774277120, 5767168, 242221056, 0), (12780044288, 5767168, 247988224, 0), (12768509952, 5767168, 253755392, 0), (12808880128, 5767168, 259522560, 0), (12814647296, 5767168, 265289728, 0), (12803112960, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c2487b0>, 2: <capsule object NULL at 0x7a5afab81f80>}
DEBUG 01-15 16:10:28.541952.541952 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.541788.541788 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.541445.541445 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4de3fdb3-2a85-454e-99e2-eb66e6166457
DEBUG 01-15 16:10:28.541531.541531 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.541083.541083 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:28.541508.541508 client.py:127] Model loaded
DEBUG 01-15 16:10:28.541518.541518 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.542715.542715 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.542711.542711 cuda_h.py:19] end restore2model cost 0.0003886222839355469 seconds
DEBUG 01-15 16:10:28.542295.542295 cuda_h.py:19] end sllm_worker_task cost 0.010714530944824219 seconds
INFO 01-15 16:10:28.542267.542267 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4de3fdb3-2a85-454e-99e2-eb66e6166457
DEBUG 01-15 16:10:28.543076.543076 cuda_h.py:19] end move_flatidxs cost 0.0008296966552734375 seconds
DEBUG 01-15 16:10:28.543190.543190 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.543289.543289 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0040740966796875 seconds
DEBUG 01-15 16:10:28.543544.543544 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.545076.545076 cuda_h.py:19] end restore2model cost 0.002604961395263672 seconds
DEBUG 01-15 16:10:28.546502.546502 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006921052932739258 seconds
DEBUG 01-15 16:10:28.546059.546059 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.546420.546420 cuda_h.py:19] end gpu_sexperts cost 0.00027060508728027344 seconds
DEBUG 01-15 16:10:28.546534.546534 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.546933.546933 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:10:28.546013.546013 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.546385.546385 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.547208.547208 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008213520050048828 seconds
DEBUG 01-15 16:10:28.547005.547005 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.547337.547337 cuda_h.py:19] end gpu_group_list cost 0.0001804828643798828 seconds
DEBUG 01-15 16:10:28.548899.548899 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.547411.547411 cuda_h.py:19] end group_tensors cost 0.004760026931762695 seconds
DEBUG 01-15 16:10:28.548446.548446 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.549036.549036 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012476444244384766 seconds
DEBUG 01-15 16:10:28.549444.549444 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.550011.550011 cuda_h.py:19] end gpu_group_list cost 0.0002741813659667969 seconds
DEBUG 01-15 16:10:28.551188.551188 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.551019.551019 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4de3fdb3-2a85-454e-99e2-eb66e6166457
DEBUG 01-15 16:10:28.552565.552565 cuda_h.py:19] end group pad cost 0.0038957595825195312 seconds
DEBUG 01-15 16:10:28.552024.552024 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.569439.569439 client.py:127] Model loaded
DEBUG 01-15 16:10:28.569095.569095 cuda_h.py:19] end wait_experts_multi_device cost 0.018459558486938477 seconds
DEBUG 01-15 16:10:28.569885.569885 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.570577.570577 cuda_h.py:19] end group_einsum cost 0.017421960830688477 seconds
DEBUG 01-15 16:10:28.570370.570370 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.573684.573684 cuda_h.py:19] end get_outputs_cpu1 cost 0.0033423900604248047 seconds
DEBUG 01-15 16:10:28.574064.574064 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03239727020263672 seconds
DEBUG 01-15 16:10:28.574645.574645 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.004858970642089844 seconds
DEBUG 01-15 16:10:28.574178.574178 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.576095.576095 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.576306.576306 cuda_h.py:19] end index_scatter cost 0.00010251998901367188 seconds
DEBUG 01-15 16:10:28.576678.576678 cuda_h.py:19] end cpuoutputsdeal cost 0.0018351078033447266 seconds
DEBUG 01-15 16:10:28.576032.576032 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.576794.576794 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.576430.576430 cuda_h.py:19] end gpu_group_tensor cost 0.0001556873321533203 seconds
DEBUG 01-15 16:10:28.576000.576000 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.577357.577357 cuda_h.py:19] end gpu_group_tensor cost 0.00012683868408203125 seconds
DEBUG 01-15 16:10:28.577122.577122 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.577517.577517 cuda_h.py:19] end gpu_group_einsum cost 0.0004904270172119141 seconds
DEBUG 01-15 16:10:28.577389.577389 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.578721.578721 cuda_h.py:19] end gpu_group_einsum cost 0.0003719329833984375 seconds
DEBUG 01-15 16:10:28.578778.578778 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.578675.578675 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.578331.578331 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016617774963378906 seconds
DEBUG 01-15 16:10:28.578564.578564 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.578103.578103 cuda_h.py:19] end concat_expert_out cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:10:28.578562.578562 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.578200.578200 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:28.579142.579142 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007483959197998047 seconds
DEBUG 01-15 16:10:28.579556.579556 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.579968.579968 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.579139.579139 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013065338134765625 seconds
DEBUG 01-15 16:10:28.579988.579988 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.579957.579957 cuda_h.py:19] end concat_expert_out cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:28.579509.579509 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.579386.579386 cuda_h.py:19] end index_scatter cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:28.579195.579195 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00047016143798828125 seconds
DEBUG 01-15 16:10:28.579913.579913 cuda_h.py:19] end gpu_experts_multi_device cost 0.03336358070373535 seconds
DEBUG 01-15 16:10:28.579107.579107 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.043592214584350586 seconds
DEBUG 01-15 16:10:28.580709.580709 cuda_h.py:19] end prefill_layer cost 0.049030303955078125 seconds
DEBUG 01-15 16:10:28.580811.580811 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:10:28.580898.580898 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.580270.580270 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:10:28.580642.580642 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:28.580444.580444 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:28.580917.580917 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 3.62396240234375e-05 seconds
DEBUG 01-15 16:10:28.580719.580719 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:28.580799.580799 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.580332.580332 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.580897.580897 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.580159.580159 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.580717.580717 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.581376.581376 cuda_h.py:19] end allocate_cuda_memory cost 0.00023698806762695312 seconds
DEBUG 01-15 16:10:28.581736.581736 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.581406.581406 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.581328.581328 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.581608.581608 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b0923d5a-4b33-48b1-8590-cd33d7f39cc7
DEBUG 01-15 16:10:28.581658.581658 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.581806.581806 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.583549.583549 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b0923d5a-4b33-48b1-8590-cd33d7f39cc7
DEBUG 01-15 16:10:28.583512.583512 cuda_h.py:19] end load_into_gpu_async cost 0.001973390579223633 seconds
DEBUG 01-15 16:10:28.583268.583268 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.583809.583809 cuda_h.py:19] end restore_tensors2 cost 8.487701416015625e-05 seconds
DEBUG 01-15 16:10:28.583571.583571 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002588987350463867 seconds
INFO 01-15 16:10:28.583408.583408 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b0923d5a-4b33-48b1-8590-cd33d7f39cc7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.585881.585881 cuda_h.py:19] end self_attn cost 0.0033528804779052734 seconds
DEBUG 01-15 16:10:28.585568.585568 cuda_h.py:19] end iln_self_attn_paln cost 0.004923105239868164 seconds
DEBUG 01-15 16:10:28.585589.585589 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:10:28.585829.585829 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.586390.586390 cuda_h.py:19] end gate cost 0.0006911754608154297 seconds
DEBUG 01-15 16:10:28.586372.586372 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.586200.586200 lmp.py:1912] 
DEBUG 01-15 16:10:28.586200.586200 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.586731.586731 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.586341.586341 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.586898.586898 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.586025.586025 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.587675.587675 lmp.py:1917] 
DEBUG 01-15 16:10:28.587675.587675 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.587848.587848 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.587173.587173 lmp.py:1935]   Expert 43 |     17 | CPU
DEBUG 01-15 16:10:28.587061.587061 lmp.py:1935]   Expert 27 |     31 | CPU
DEBUG 01-15 16:10:28.587234.587234 lmp.py:1935]   Expert 26 |     52 | CPU
DEBUG 01-15 16:10:28.587930.587930 lmp.py:1935]   Expert 34 |     53 | CPU
DEBUG 01-15 16:10:28.587103.587103 lmp.py:1935]   Expert 56 |     54 | CPU
DEBUG 01-15 16:10:28.587468.587468 lmp.py:1935]   Expert  3 |     59 | CPU
DEBUG 01-15 16:10:28.587356.587356 lmp.py:1935]   Expert  4 |     68 | CPU
DEBUG 01-15 16:10:28.587768.587768 lmp.py:1935]   Expert 61 |     82 | CPU
DEBUG 01-15 16:10:28.587464.587464 lmp.py:1935]   Expert 14 |     95 | CPU
DEBUG 01-15 16:10:28.587398.587398 lmp.py:1935]   Expert 38 |    102 | CPU
DEBUG 01-15 16:10:28.587618.587618 lmp.py:1935]   Expert  2 |    112 | CPU
DEBUG 01-15 16:10:28.587837.587837 lmp.py:1935]   Expert 17 |    119 | CPU
DEBUG 01-15 16:10:28.587295.587295 lmp.py:1935]   Expert 22 |    123 | CPU
DEBUG 01-15 16:10:28.587991.587991 lmp.py:1935]   Expert 47 |    128 | CPU
DEBUG 01-15 16:10:28.587879.587879 lmp.py:1935]   Expert 37 |    132 | CPU
DEBUG 01-15 16:10:28.587244.587244 lmp.py:1935]   Expert 55 |    132 | CPU
DEBUG 01-15 16:10:28.587463.587463 lmp.py:1935]   Expert 28 |    135 | CPU
DEBUG 01-15 16:10:28.587106.587106 lmp.py:1935]   Expert 54 |    135 | CPU
DEBUG 01-15 16:10:28.587511.587511 lmp.py:1935]   Expert  7 |    142 | CPU
DEBUG 01-15 16:10:28.587915.587915 lmp.py:1935]   Expert 15 |    145 | CPU
DEBUG 01-15 16:10:28.587320.587320 lmp.py:1935]   Expert  5 |    146 | CPU
DEBUG 01-15 16:10:28.587486.587486 lmp.py:1935]   Expert 48 |    146 | CPU
DEBUG 01-15 16:10:28.587652.587652 lmp.py:1935]   Expert 51 |    149 | CPU
DEBUG 01-15 16:10:28.587057.587057 lmp.py:1935]   Expert 60 |    149 | CPU
DEBUG 01-15 16:10:28.587700.587700 lmp.py:1935]   Expert 45 |    150 | CPU
DEBUG 01-15 16:10:28.587104.587104 lmp.py:1935]   Expert 12 |    153 | CPU
DEBUG 01-15 16:10:28.587270.587270 lmp.py:1935]   Expert 63 |    155 | CPU
DEBUG 01-15 16:10:28.587152.587152 lmp.py:1935]   Expert 19 |    158 | CPU
DEBUG 01-15 16:10:28.587271.587271 lmp.py:1935]   Expert  6 |    167 | CPU
DEBUG 01-15 16:10:28.587868.587868 lmp.py:1935]   Expert 57 |    169 | CPU
DEBUG 01-15 16:10:28.587749.587749 lmp.py:1935]   Expert 52 |    172 | CPU
DEBUG 01-15 16:10:28.587631.587631 lmp.py:1935]   Expert 50 |    179 | CPU
DEBUG 01-15 16:10:28.587420.587420 lmp.py:1935]   Expert 18 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.587255.587255 lmp.py:1935]   Expert 44 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.587851.587851 lmp.py:1935]   Expert 13 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.587686.587686 lmp.py:1935]   Expert 31 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.587045.587045 lmp.py:1935]   Expert 30 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.587641.587641 lmp.py:1935]   Expert 23 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.587238.587238 lmp.py:1935]   Expert 53 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.587311.587311 lmp.py:1935]   Expert 39 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.587385.587385 lmp.py:1935]   Expert 20 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.587220.587220 lmp.py:1935]   Expert 59 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.587293.587293 lmp.py:1935]   Expert 21 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.587652.587652 lmp.py:1935]   Expert 29 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.587771.587771 lmp.py:1935]   Expert 16 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.587891.587891 lmp.py:1935]   Expert 36 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.587250.587250 lmp.py:1935]   Expert 41 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.587608.587608 lmp.py:1935]   Expert 25 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.588728.588728 lmp.py:1935]   Expert 32 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.588086.588086 lmp.py:1935]   Expert 49 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.588159.588159 lmp.py:1935]   Expert 46 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.588471.588471 lmp.py:1935]   Expert  8 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.588545.588545 lmp.py:1935]   Expert 10 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.588241.588241 lmp.py:1935]   Expert 42 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.588599.588599 lmp.py:1935]   Expert 62 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.588242.588242 lmp.py:1935]   Expert 35 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.588885.588885 lmp.py:1935]   Expert 33 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.588289.588289 lmp.py:1935]   Expert  9 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.588694.588694 lmp.py:1935]   Expert 58 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.588099.588099 lmp.py:1935]   Expert 40 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.588265.588265 lmp.py:1935]   Expert 11 |    422 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.588385.588385 lmp.py:1935]   Expert  0 |    425 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.588504.588504 lmp.py:1935]   Expert 24 |    566 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.588624.588624 lmp.py:1935]   Expert  1 |    652 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.588790.588790 lmp.py:1937] 
DEBUG 01-15 16:10:28.588790.588790 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.588387.588387 lmp.py:1938]   CPU:   3809 tokens
DEBUG 01-15 16:10:28.588507.588507 lmp.py:1942]   cuda:1:   4241 tokens (16 experts)
DEBUG 01-15 16:10:28.588673.588673 lmp.py:1942]   cuda:2:   4238 tokens (16 experts)
DEBUG 01-15 16:10:28.588885.588885 lmp.py:1943]   Total GPU:   8479 tokens
DEBUG 01-15 16:10:28.588621.588621 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.588621.588621 lmp.py:1944] 
DEBUG 01-15 16:10:28.588423.588423 cuda_h.py:19] end experts_map_get cost 0.0019028186798095703 seconds
DEBUG 01-15 16:10:28.588320.588320 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.588890.588890 lmp.py:1953] 
DEBUG 01-15 16:10:28.588890.588890 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.588833.588833 cuda_h.py:19] end cpu_experts_submit cost 6.413459777832031e-05 seconds
DEBUG 01-15 16:10:28.588098.588098 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.588020.588020 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.589927.589927 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.1.gate_proj.weight': 0, 'model.layers.10.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.32.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.32.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.32.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.10.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.10.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.10.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.41.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.41.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.41.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.13.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.13.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.13.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(12843483136, 5767168, 0, 0), (12849250304, 5767168, 5767168, 0), (12837715968, 5767168, 11534336, 0), (13431734272, 5767168, 17301504, 0), (13437501440, 5767168, 23068672, 0), (13425967104, 5767168, 28835840, 0), (13500940288, 5767168, 34603008, 0), (13506707456, 5767168, 40370176, 0), (13495173120, 5767168, 46137344, 0), (12981895168, 5767168, 51904512, 0), (12987662336, 5767168, 57671680, 0), (12976128000, 5767168, 63438848, 0), (13552844800, 5767168, 69206016, 0), (13558611968, 5767168, 74973184, 0), (13547077632, 5767168, 80740352, 0), (13016498176, 5767168, 86507520, 0), (13022265344, 5767168, 92274688, 0), (13010731008, 5767168, 98041856, 0), (13587447808, 5767168, 103809024, 0), (13593214976, 5767168, 109576192, 0), (13581680640, 5767168, 115343360, 0), (13622050816, 5767168, 121110528, 0), (13627817984, 5767168, 126877696, 0), (13616283648, 5767168, 132644864, 0), (13103005696, 5767168, 138412032, 0), (13108772864, 5767168, 144179200, 0), (13097238528, 5767168, 149946368, 0), (13673955328, 5767168, 155713536, 0), (13679722496, 5767168, 161480704, 0), (13668188160, 5767168, 167247872, 0), (13172211712, 5767168, 173015040, 0), (13177978880, 5767168, 178782208, 0), (13166444544, 5767168, 184549376, 0), (13258719232, 5767168, 190316544, 0), (13264486400, 5767168, 196083712, 0), (13252952064, 5767168, 201850880, 0), (13829668864, 5767168, 207618048, 0), (13835436032, 5767168, 213385216, 0), (13823901696, 5767168, 219152384, 0), (13327925248, 5767168, 224919552, 0), (13333692416, 5767168, 230686720, 0), (13322158080, 5767168, 236453888, 0), (13345226752, 5767168, 242221056, 0), (13350993920, 5767168, 247988224, 0), (13339459584, 5767168, 253755392, 0), (13362528256, 5767168, 259522560, 0), (13368295424, 5767168, 265289728, 0), (13356761088, 5767168, 271056896, 0)], 2: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (13397131264, 5767168, 17301504, 0), (13402898432, 5767168, 23068672, 0), (13391364096, 5767168, 28835840, 0), (13379829760, 5767168, 34603008, 0), (13385596928, 5767168, 40370176, 0), (13374062592, 5767168, 46137344, 0), (13449035776, 5767168, 51904512, 0), (13454802944, 5767168, 57671680, 0), (13443268608, 5767168, 63438848, 0), (13518241792, 5767168, 69206016, 0), (13524008960, 5767168, 74973184, 0), (13512474624, 5767168, 80740352, 0), (12964593664, 5767168, 86507520, 0), (12970360832, 5767168, 92274688, 0), (12958826496, 5767168, 98041856, 0), (12999196672, 5767168, 103809024, 0), (13004963840, 5767168, 109576192, 0), (12993429504, 5767168, 115343360, 0), (13535543296, 5767168, 121110528, 0), (13541310464, 5767168, 126877696, 0), (13529776128, 5767168, 132644864, 0), (13051101184, 5767168, 138412032, 0), (13056868352, 5767168, 144179200, 0), (13045334016, 5767168, 149946368, 0), (13137608704, 5767168, 155713536, 0), (13143375872, 5767168, 161480704, 0), (13131841536, 5767168, 167247872, 0), (13189513216, 5767168, 173015040, 0), (13195280384, 5767168, 178782208, 0), (13183746048, 5767168, 184549376, 0), (13743161344, 5767168, 190316544, 0), (13748928512, 5767168, 196083712, 0), (13737394176, 5767168, 201850880, 0), (13224116224, 5767168, 207618048, 0), (13229883392, 5767168, 213385216, 0), (13218349056, 5767168, 219152384, 0), (13241417728, 5767168, 224919552, 0), (13247184896, 5767168, 230686720, 0), (13235650560, 5767168, 236453888, 0), (13846970368, 5767168, 242221056, 0), (13852737536, 5767168, 247988224, 0), (13841203200, 5767168, 253755392, 0), (13898874880, 5767168, 259522560, 0), (13904642048, 5767168, 265289728, 0), (13893107712, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c2c3e10>, 2: <capsule object NULL at 0x7a51b06462b0>}
DEBUG 01-15 16:10:28.590620.590620 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.590907.590907 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.590048.590048 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2bd91eef-f229-4071-bd18-1b642636e478
DEBUG 01-15 16:10:28.590916.590916 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:28.590242.590242 client.py:127] Model loaded
DEBUG 01-15 16:10:28.590914.590914 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.590786.590786 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.590833.590833 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.591745.591745 cuda_h.py:19] end restore2model cost 0.00047016143798828125 seconds
DEBUG 01-15 16:10:28.591952.591952 cuda_h.py:19] end sllm_worker_task cost 0.010249137878417969 seconds
DEBUG 01-15 16:10:28.591597.591597 cuda_h.py:19] end move_flatidxs cost 0.0008206367492675781 seconds
DEBUG 01-15 16:10:28.591897.591897 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:28.591942.591942 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2bd91eef-f229-4071-bd18-1b642636e478
DEBUG 01-15 16:10:28.592216.592216 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036225318908691406 seconds
DEBUG 01-15 16:10:28.592278.592278 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.595142.595142 cuda_h.py:19] end restore2model cost 0.0026242733001708984 seconds
DEBUG 01-15 16:10:28.595846.595846 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006498098373413086 seconds
DEBUG 01-15 16:10:28.595688.595688 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.595149.595149 cuda_h.py:19] end gpu_sexperts cost 0.00027370452880859375 seconds
DEBUG 01-15 16:10:28.595786.595786 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.595947.595947 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.9788742065429688e-05 seconds
DEBUG 01-15 16:10:28.595073.595073 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.595637.595637 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.596891.596891 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008223056793212891 seconds
DEBUG 01-15 16:10:28.596264.596264 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.596119.596119 cuda_h.py:19] end gpu_group_list cost 0.00017881393432617188 seconds
DEBUG 01-15 16:10:28.597389.597389 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.598758.598758 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008716583251953125 seconds
DEBUG 01-15 16:10:28.598231.598231 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.598331.598331 cuda_h.py:19] end gpu_group_list cost 0.0001838207244873047 seconds
DEBUG 01-15 16:10:28.599285.599285 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.599783.599783 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2bd91eef-f229-4071-bd18-1b642636e478
DEBUG 01-15 16:10:28.600749.600749 cuda_h.py:19] end group_tensors cost 0.008806228637695312 seconds
DEBUG 01-15 16:10:28.601242.601242 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.605724.605724 cuda_h.py:19] end group pad cost 0.003839731216430664 seconds
DEBUG 01-15 16:10:28.605243.605243 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.617113.617113 client.py:127] Model loaded
DEBUG 01-15 16:10:28.618662.618662 cuda_h.py:19] end wait_experts_multi_device cost 0.018713712692260742 seconds
DEBUG 01-15 16:10:28.618531.618531 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.624822.624822 cuda_h.py:19] end group_einsum cost 0.018986225128173828 seconds
DEBUG 01-15 16:10:28.624298.624298 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.628172.628172 cuda_h.py:19] end get_outputs_cpu1 cost 0.003870725631713867 seconds
DEBUG 01-15 16:10:28.629688.629688 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03863883018493652 seconds
DEBUG 01-15 16:10:28.629075.629075 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.011433839797973633 seconds
DEBUG 01-15 16:10:28.629879.629879 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.630562.630562 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.631608.631608 cuda_h.py:19] end index_scatter cost 7.462501525878906e-05 seconds
DEBUG 01-15 16:10:28.631890.631890 cuda_h.py:19] end cpuoutputsdeal cost 0.0016524791717529297 seconds
DEBUG 01-15 16:10:28.631853.631853 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.631516.631516 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.631886.631886 cuda_h.py:19] end gpu_group_tensor cost 0.00013756752014160156 seconds
DEBUG 01-15 16:10:28.631172.631172 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.631382.631382 cuda_h.py:19] end gpu_group_tensor cost 0.000125885009765625 seconds
DEBUG 01-15 16:10:28.631187.631187 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.632759.632759 cuda_h.py:19] end gpu_group_einsum cost 0.0006167888641357422 seconds
DEBUG 01-15 16:10:28.632155.632155 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.633390.633390 cuda_h.py:19] end gpu_group_einsum cost 0.00043082237243652344 seconds
DEBUG 01-15 16:10:28.633678.633678 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.633913.633913 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.633099.633099 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016880035400390625 seconds
DEBUG 01-15 16:10:28.633524.633524 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.633348.633348 cuda_h.py:19] end concat_expert_out cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:28.633953.633953 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.633306.633306 cuda_h.py:19] end index_scatter cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:10:28.634811.634811 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007529258728027344 seconds
DEBUG 01-15 16:10:28.634033.634033 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.634975.634975 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.634239.634239 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012636184692382812 seconds
DEBUG 01-15 16:10:28.634803.634803 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.634733.634733 cuda_h.py:19] end concat_expert_out cost 5.602836608886719e-05 seconds
DEBUG 01-15 16:10:28.634053.634053 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.634460.634460 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:28.634553.634553 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00048613548278808594 seconds
DEBUG 01-15 16:10:28.634510.634510 cuda_h.py:19] end gpu_experts_multi_device cost 0.03930783271789551 seconds
DEBUG 01-15 16:10:28.634234.634234 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.04928445816040039 seconds
DEBUG 01-15 16:10:28.635201.635201 cuda_h.py:19] end prefill_layer cost 0.05489635467529297 seconds
DEBUG 01-15 16:10:28.635984.635984 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:10:28.635687.635687 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.635867.635867 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:10:28.635000.635000 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:28.635332.635332 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:28.635566.635566 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 3.933906555175781e-05 seconds
DEBUG 01-15 16:10:28.635799.635799 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 7.319450378417969e-05 seconds
DEBUG 01-15 16:10:28.635403.635403 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.635643.635643 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.635165.635165 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.635895.635895 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.636402.636402 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.636743.636743 cuda_h.py:19] end allocate_cuda_memory cost 0.00024628639221191406 seconds
DEBUG 01-15 16:10:28.636395.636395 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.636450.636450 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.636902.636902 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.636134.636134 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3c51de34-07cc-4b23-9f72-f9c90e15ef3b
DEBUG 01-15 16:10:28.636145.636145 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.636096.636096 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.637413.637413 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3c51de34-07cc-4b23-9f72-f9c90e15ef3b
DEBUG 01-15 16:10:28.638256.638256 cuda_h.py:19] end load_into_gpu_async cost 0.0015423297882080078 seconds
DEBUG 01-15 16:10:28.638535.638535 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.638076.638076 cuda_h.py:19] end restore_tensors2 cost 8.630752563476562e-05 seconds
DEBUG 01-15 16:10:28.638600.638600 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021712779998779297 seconds
INFO 01-15 16:10:28.638503.638503 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3c51de34-07cc-4b23-9f72-f9c90e15ef3b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.640433.640433 cuda_h.py:19] end self_attn cost 0.003475666046142578 seconds
DEBUG 01-15 16:10:28.640809.640809 cuda_h.py:19] end iln_self_attn_paln cost 0.005181312561035156 seconds
DEBUG 01-15 16:10:28.640023.640023 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:10:28.640739.640739 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.641473.641473 cuda_h.py:19] end gate cost 0.0007121562957763672 seconds
DEBUG 01-15 16:10:28.641740.641740 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.642466.642466 lmp.py:1912] 
DEBUG 01-15 16:10:28.642466.642466 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.642891.642891 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.642256.642256 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.642998.642998 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.642072.642072 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.642192.642192 lmp.py:1917] 
DEBUG 01-15 16:10:28.642192.642192 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.642073.642073 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.642915.642915 lmp.py:1935]   Expert 39 |     15 | CPU
DEBUG 01-15 16:10:28.642035.642035 lmp.py:1935]   Expert 13 |     16 | CPU
DEBUG 01-15 16:10:28.642201.642201 lmp.py:1935]   Expert 49 |     38 | CPU
DEBUG 01-15 16:10:28.642128.642128 lmp.py:1935]   Expert 35 |     54 | CPU
DEBUG 01-15 16:10:28.642056.642056 lmp.py:1935]   Expert 19 |     64 | CPU
DEBUG 01-15 16:10:28.642984.642984 lmp.py:1935]   Expert  9 |     72 | CPU
DEBUG 01-15 16:10:28.642435.642435 lmp.py:1935]   Expert 32 |     74 | CPU
DEBUG 01-15 16:10:28.642793.642793 lmp.py:1935]   Expert 41 |     76 | CPU
DEBUG 01-15 16:10:28.642674.642674 lmp.py:1935]   Expert 26 |     77 | CPU
DEBUG 01-15 16:10:28.642794.642794 lmp.py:1935]   Expert 33 |     82 | CPU
DEBUG 01-15 16:10:28.642060.642060 lmp.py:1935]   Expert 46 |     85 | CPU
DEBUG 01-15 16:10:28.642464.642464 lmp.py:1935]   Expert 23 |     86 | CPU
DEBUG 01-15 16:10:28.642915.642915 lmp.py:1935]   Expert 31 |     92 | CPU
DEBUG 01-15 16:10:28.642889.642889 lmp.py:1935]   Expert 18 |     93 | CPU
DEBUG 01-15 16:10:28.642579.642579 lmp.py:1935]   Expert 38 |     99 | CPU
DEBUG 01-15 16:10:28.642029.642029 lmp.py:1935]   Expert  3 |    102 | CPU
DEBUG 01-15 16:10:28.642480.642480 lmp.py:1935]   Expert 17 |    105 | CPU
DEBUG 01-15 16:10:28.642693.642693 lmp.py:1935]   Expert  6 |    107 | CPU
DEBUG 01-15 16:10:28.642382.642382 lmp.py:1935]   Expert 20 |    117 | CPU
DEBUG 01-15 16:10:28.642833.642833 lmp.py:1935]   Expert 40 |    129 | CPU
DEBUG 01-15 16:10:28.642522.642522 lmp.py:1935]   Expert 61 |    132 | CPU
DEBUG 01-15 16:10:28.642165.642165 lmp.py:1935]   Expert 15 |    133 | CPU
DEBUG 01-15 16:10:28.642047.642047 lmp.py:1935]   Expert 62 |    133 | CPU
DEBUG 01-15 16:10:28.642690.642690 lmp.py:1935]   Expert 43 |    134 | CPU
DEBUG 01-15 16:10:28.642856.642856 lmp.py:1935]   Expert 44 |    135 | CPU
DEBUG 01-15 16:10:28.642545.642545 lmp.py:1935]   Expert 50 |    136 | CPU
DEBUG 01-15 16:10:28.642473.642473 lmp.py:1935]   Expert 16 |    139 | CPU
DEBUG 01-15 16:10:28.642639.642639 lmp.py:1935]   Expert 63 |    139 | CPU
DEBUG 01-15 16:10:28.642462.642462 lmp.py:1935]   Expert 42 |    140 | CPU
DEBUG 01-15 16:10:28.642595.642595 lmp.py:1935]   Expert 59 |    141 | CPU
DEBUG 01-15 16:10:28.642523.642523 lmp.py:1935]   Expert  2 |    147 | CPU
DEBUG 01-15 16:10:28.642974.642974 lmp.py:1935]   Expert 36 |    152 | CPU
DEBUG 01-15 16:10:28.642285.642285 lmp.py:1935]   Expert 10 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.642074.642074 lmp.py:1935]   Expert  5 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.642671.642671 lmp.py:1935]   Expert 34 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.642029.642029 lmp.py:1935]   Expert 27 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.642626.642626 lmp.py:1935]   Expert 52 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.642269.642269 lmp.py:1935]   Expert 45 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.642912.642912 lmp.py:1935]   Expert 60 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.642316.642316 lmp.py:1935]   Expert 48 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.642959.642959 lmp.py:1935]   Expert 56 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643364.643364 lmp.py:1935]   Expert 51 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643768.643768 lmp.py:1935]   Expert 24 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643411.643411 lmp.py:1935]   Expert  7 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643577.643577 lmp.py:1935]   Expert 53 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643220.643220 lmp.py:1935]   Expert  8 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643817.643817 lmp.py:1935]   Expert 57 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643414.643414 lmp.py:1935]   Expert 47 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643249.643249 lmp.py:1935]   Expert 29 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643653.643653 lmp.py:1935]   Expert 21 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643296.643296 lmp.py:1935]   Expert 14 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643939.643939 lmp.py:1935]   Expert  0 |    287 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643582.643582 lmp.py:1935]   Expert  4 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643987.643987 lmp.py:1935]   Expert 22 |    315 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643153.643153 lmp.py:1935]   Expert 55 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643558.643558 lmp.py:1935]   Expert 37 |    318 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643201.643201 lmp.py:1935]   Expert  1 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643559.643559 lmp.py:1935]   Expert 58 |    320 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643155.643155 lmp.py:1935]   Expert 54 |    329 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643514.643514 lmp.py:1935]   Expert 28 |    361 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643587.643587 lmp.py:1935]   Expert 12 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643992.643992 lmp.py:1935]   Expert 11 |    399 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643396.643396 lmp.py:1935]   Expert 25 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.643801.643801 lmp.py:1935]   Expert 30 |    831 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.643490.643490 lmp.py:1937] 
DEBUG 01-15 16:10:28.643490.643490 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.643895.643895 lmp.py:1938]   CPU:   3244 tokens
DEBUG 01-15 16:10:28.643776.643776 lmp.py:1942]   cuda:1:   4444 tokens (15 experts)
DEBUG 01-15 16:10:28.643942.643942 lmp.py:1942]   cuda:2:   4600 tokens (17 experts)
DEBUG 01-15 16:10:28.643631.643631 lmp.py:1943]   Total GPU:   9044 tokens
DEBUG 01-15 16:10:28.643036.643036 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.643036.643036 lmp.py:1944] 
DEBUG 01-15 16:10:28.643070.643070 cuda_h.py:19] end experts_map_get cost 0.001718759536743164 seconds
DEBUG 01-15 16:10:28.643827.643827 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.643676.643676 lmp.py:1953] 
DEBUG 01-15 16:10:28.643676.643676 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.643505.643505 cuda_h.py:19] end cpu_experts_submit cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:28.643553.643553 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.643621.643621 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.644420.644420 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.34.gate_proj.weight': 0, 'model.layers.11.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.45.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.45.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.45.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.48.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.48.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.48.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.21.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.21.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.21.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.22.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.22.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.22.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.53.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.53.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.53.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.24.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.24.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.24.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.57.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.57.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.57.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.58.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.58.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.58.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.27.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.27.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.27.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.30.up_proj.weight': 253755392}, 2: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.47.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.47.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.47.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.55.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.55.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.55.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.25.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.25.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.25.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.60.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.60.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.60.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(14521729024, 5767168, 0, 0), (14527496192, 5767168, 5767168, 0), (14515961856, 5767168, 11534336, 0), (14002683904, 5767168, 17301504, 0), (14008451072, 5767168, 23068672, 0), (13996916736, 5767168, 28835840, 0), (14573633536, 5767168, 34603008, 0), (14579400704, 5767168, 40370176, 0), (14567866368, 5767168, 46137344, 0), (14712045568, 5767168, 51904512, 0), (14717812736, 5767168, 57671680, 0), (14706278400, 5767168, 63438848, 0), (14763950080, 5767168, 69206016, 0), (14769717248, 5767168, 74973184, 0), (14758182912, 5767168, 80740352, 0), (14296809472, 5767168, 86507520, 0), (14302576640, 5767168, 92274688, 0), (14291042304, 5767168, 98041856, 0), (14314110976, 5767168, 103809024, 0), (14319878144, 5767168, 109576192, 0), (14308343808, 5767168, 115343360, 0), (14850457600, 5767168, 121110528, 0), (14856224768, 5767168, 126877696, 0), (14844690432, 5767168, 132644864, 0), (14348713984, 5767168, 138412032, 0), (14354481152, 5767168, 144179200, 0), (14342946816, 5767168, 149946368, 0), (14919663616, 5767168, 155713536, 0), (14925430784, 5767168, 161480704, 0), (14913896448, 5767168, 167247872, 0), (14936965120, 5767168, 173015040, 0), (14942732288, 5767168, 178782208, 0), (14931197952, 5767168, 184549376, 0), (14400618496, 5767168, 190316544, 0), (14406385664, 5767168, 196083712, 0), (14394851328, 5767168, 201850880, 0), (14417920000, 5767168, 207618048, 0), (14423687168, 5767168, 213385216, 0), (14412152832, 5767168, 219152384, 0), (14435221504, 5767168, 224919552, 0), (14440988672, 5767168, 230686720, 0), (14429454336, 5767168, 236453888, 0), (14452523008, 5767168, 242221056, 0), (14458290176, 5767168, 247988224, 0), (14446755840, 5767168, 253755392, 0)], 2: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (13950779392, 5767168, 17301504, 0), (13956546560, 5767168, 23068672, 0), (13945012224, 5767168, 28835840, 0), (14019985408, 5767168, 34603008, 0), (14025752576, 5767168, 40370176, 0), (14014218240, 5767168, 46137344, 0), (14054588416, 5767168, 51904512, 0), (14060355584, 5767168, 57671680, 0), (14048821248, 5767168, 63438848, 0), (14071889920, 5767168, 69206016, 0), (14077657088, 5767168, 74973184, 0), (14066122752, 5767168, 80740352, 0), (14106492928, 5767168, 86507520, 0), (14112260096, 5767168, 92274688, 0), (14100725760, 5767168, 98041856, 0), (14123794432, 5767168, 103809024, 0), (14129561600, 5767168, 109576192, 0), (14118027264, 5767168, 115343360, 0), (14141095936, 5767168, 121110528, 0), (14146863104, 5767168, 126877696, 0), (14135328768, 5767168, 132644864, 0), (14175698944, 5767168, 138412032, 0), (14181466112, 5767168, 144179200, 0), (14169931776, 5767168, 149946368, 0), (14746648576, 5767168, 155713536, 0), (14752415744, 5767168, 161480704, 0), (14740881408, 5767168, 167247872, 0), (14815854592, 5767168, 173015040, 0), (14821621760, 5767168, 178782208, 0), (14810087424, 5767168, 184549376, 0), (14833156096, 5767168, 190316544, 0), (14838923264, 5767168, 196083712, 0), (14827388928, 5767168, 201850880, 0), (14867759104, 5767168, 207618048, 0), (14873526272, 5767168, 213385216, 0), (14861991936, 5767168, 219152384, 0), (14885060608, 5767168, 224919552, 0), (14890827776, 5767168, 230686720, 0), (14879293440, 5767168, 236453888, 0), (14902362112, 5767168, 242221056, 0), (14908129280, 5767168, 247988224, 0), (14896594944, 5767168, 253755392, 0), (14366015488, 5767168, 259522560, 0), (14371782656, 5767168, 265289728, 0), (14360248320, 5767168, 271056896, 0), (14971568128, 5767168, 276824064, 0), (14977335296, 5767168, 282591232, 0), (14965800960, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4372280>, 2: <capsule object NULL at 0x7a51b0646250>}
DEBUG 01-15 16:10:28.645797.645797 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.645679.645679 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.645528.645528 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9e466fdf-1231-43c5-83a8-4b0051a65333
DEBUG 01-15 16:10:28.645866.645866 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.645734.645734 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:28.645392.645392 client.py:127] Model loaded
DEBUG 01-15 16:10:28.645310.645310 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.645757.645757 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.646529.646529 cuda_h.py:19] end restore2model cost 0.00037360191345214844 seconds
DEBUG 01-15 16:10:28.646305.646305 cuda_h.py:19] end sllm_worker_task cost 0.010286808013916016 seconds
INFO 01-15 16:10:28.646859.646859 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9e466fdf-1231-43c5-83a8-4b0051a65333
DEBUG 01-15 16:10:28.646781.646781 cuda_h.py:19] end move_flatidxs cost 0.0008330345153808594 seconds
DEBUG 01-15 16:10:28.646557.646557 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.647855.647855 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034055709838867188 seconds
DEBUG 01-15 16:10:28.647056.647056 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.650609.650609 cuda_h.py:19] end restore2model cost 0.0029952526092529297 seconds
DEBUG 01-15 16:10:28.650367.650367 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006685495376586914 seconds
DEBUG 01-15 16:10:28.650593.650593 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.650710.650710 cuda_h.py:19] end gpu_sexperts cost 0.00029587745666503906 seconds
DEBUG 01-15 16:10:28.650301.650301 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.650554.650554 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:10:28.650204.650204 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.650861.650861 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.651312.651312 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007932186126708984 seconds
DEBUG 01-15 16:10:28.651824.651824 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.651850.651850 cuda_h.py:19] end gpu_group_list cost 0.00016570091247558594 seconds
DEBUG 01-15 16:10:28.652631.652631 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.653756.653756 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009038448333740234 seconds
DEBUG 01-15 16:10:28.653791.653791 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.653414.653414 cuda_h.py:19] end gpu_group_list cost 0.00018286705017089844 seconds
DEBUG 01-15 16:10:28.654302.654302 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.654231.654231 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9e466fdf-1231-43c5-83a8-4b0051a65333
DEBUG 01-15 16:10:28.657201.657201 cuda_h.py:19] end group_tensors cost 0.010301351547241211 seconds
DEBUG 01-15 16:10:28.657017.657017 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.661507.661507 cuda_h.py:19] end group pad cost 0.003926753997802734 seconds
DEBUG 01-15 16:10:28.661165.661165 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.678125.678125 client.py:127] Model loaded
DEBUG 01-15 16:10:28.678680.678680 cuda_h.py:19] end wait_experts_multi_device cost 0.023638248443603516 seconds
DEBUG 01-15 16:10:28.678292.678292 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.681268.681268 cuda_h.py:19] end group_einsum cost 0.01919078826904297 seconds
DEBUG 01-15 16:10:28.681333.681333 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.684230.684230 cuda_h.py:19] end get_outputs_cpu1 cost 0.003396272659301758 seconds
DEBUG 01-15 16:10:28.685410.685410 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03989052772521973 seconds
DEBUG 01-15 16:10:28.686784.686784 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007476329803466797 seconds
DEBUG 01-15 16:10:28.686019.686019 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.687383.687383 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.687667.687667 cuda_h.py:19] end index_scatter cost 0.00010061264038085938 seconds
DEBUG 01-15 16:10:28.687996.687996 cuda_h.py:19] end cpuoutputsdeal cost 0.0016286373138427734 seconds
DEBUG 01-15 16:10:28.687290.687290 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.687145.687145 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.688522.688522 cuda_h.py:19] end gpu_group_tensor cost 0.0001380443572998047 seconds
DEBUG 01-15 16:10:28.688569.688569 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.688919.688919 cuda_h.py:19] end gpu_group_tensor cost 0.0001227855682373047 seconds
DEBUG 01-15 16:10:28.688339.688339 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.689962.689962 cuda_h.py:19] end gpu_group_einsum cost 0.0009319782257080078 seconds
DEBUG 01-15 16:10:28.689549.689549 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.690509.690509 cuda_h.py:19] end gpu_group_einsum cost 0.0005059242248535156 seconds
DEBUG 01-15 16:10:28.690308.690308 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.690610.690610 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.690222.690222 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022530555725097656 seconds
DEBUG 01-15 16:10:28.690422.690422 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.690372.690372 cuda_h.py:19] end concat_expert_out cost 6.0558319091796875e-05 seconds
DEBUG 01-15 16:10:28.690739.690739 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.690238.690238 cuda_h.py:19] end index_scatter cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:10:28.691564.691564 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008699893951416016 seconds
DEBUG 01-15 16:10:28.691640.691640 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.691576.691576 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.691601.691601 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012755393981933594 seconds
DEBUG 01-15 16:10:28.691926.691926 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.691704.691704 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:28.691017.691017 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.691894.691894 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:28.691511.691511 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046324729919433594 seconds
DEBUG 01-15 16:10:28.691236.691236 cuda_h.py:19] end gpu_experts_multi_device cost 0.04086732864379883 seconds
DEBUG 01-15 16:10:28.691861.691861 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.05090045928955078 seconds
DEBUG 01-15 16:10:28.692668.692668 cuda_h.py:19] end prefill_layer cost 0.05676627159118652 seconds
DEBUG 01-15 16:10:28.692863.692863 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:10:28.692089.692089 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.692030.692030 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:10:28.692038.692038 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:28.692655.692655 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:28.692558.692558 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 3.9577484130859375e-05 seconds
DEBUG 01-15 16:10:28.692029.692029 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 7.414817810058594e-05 seconds
DEBUG 01-15 16:10:28.692586.692586 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.692635.692635 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.692415.692415 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.692337.692337 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.692567.692567 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.693212.693212 cuda_h.py:19] end allocate_cuda_memory cost 0.00022411346435546875 seconds
DEBUG 01-15 16:10:28.693241.693241 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.693103.693103 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.693171.693171 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.693358.693358 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c0ab1d9a-adf7-49c0-809f-ae540eac8d3d
DEBUG 01-15 16:10:28.693553.693553 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.693365.693365 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.695746.695746 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c0ab1d9a-adf7-49c0-809f-ae540eac8d3d
DEBUG 01-15 16:10:28.695994.695994 cuda_h.py:19] end load_into_gpu_async cost 0.0020596981048583984 seconds
DEBUG 01-15 16:10:28.695465.695465 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.695866.695866 cuda_h.py:19] end restore_tensors2 cost 8.797645568847656e-05 seconds
DEBUG 01-15 16:10:28.695629.695629 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0026700496673583984 seconds
INFO 01-15 16:10:28.695849.695849 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c0ab1d9a-adf7-49c0-809f-ae540eac8d3d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.697893.697893 cuda_h.py:19] end self_attn cost 0.0032296180725097656 seconds
DEBUG 01-15 16:10:28.697235.697235 cuda_h.py:19] end iln_self_attn_paln cost 0.004903554916381836 seconds
DEBUG 01-15 16:10:28.697972.697972 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:10:28.697218.697218 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.698601.698601 cuda_h.py:19] end gate cost 0.0006983280181884766 seconds
DEBUG 01-15 16:10:28.698583.698583 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.698179.698179 lmp.py:1912] 
DEBUG 01-15 16:10:28.698179.698179 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.698035.698035 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.698592.698592 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.698619.698619 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.698739.698739 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.698143.698143 lmp.py:1917] 
DEBUG 01-15 16:10:28.698143.698143 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.698501.698501 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.698820.698820 lmp.py:1935]   Expert 12 |     20 | CPU
DEBUG 01-15 16:10:28.698225.698225 lmp.py:1935]   Expert 47 |     25 | CPU
DEBUG 01-15 16:10:28.698152.698152 lmp.py:1935]   Expert 27 |     32 | CPU
DEBUG 01-15 16:10:28.699603.699603 lmp.py:1935]   Expert 38 |     32 | CPU
DEBUG 01-15 16:10:28.699200.699200 lmp.py:1935]   Expert 16 |     37 | CPU
DEBUG 01-15 16:10:28.699081.699081 lmp.py:1935]   Expert 52 |     41 | CPU
DEBUG 01-15 16:10:28.699724.699724 lmp.py:1935]   Expert 63 |     45 | CPU
DEBUG 01-15 16:10:28.699083.699083 lmp.py:1935]   Expert  4 |     59 | CPU
DEBUG 01-15 16:10:28.699964.699964 lmp.py:1935]   Expert 44 |     60 | CPU
DEBUG 01-15 16:10:28.699561.699561 lmp.py:1935]   Expert 61 |     64 | CPU
DEBUG 01-15 16:10:28.699680.699680 lmp.py:1935]   Expert 43 |     65 | CPU
DEBUG 01-15 16:10:28.699515.699515 lmp.py:1935]   Expert 34 |     77 | CPU
DEBUG 01-15 16:10:28.699112.699112 lmp.py:1935]   Expert 53 |     83 | CPU
DEBUG 01-15 16:10:28.699517.699517 lmp.py:1935]   Expert  0 |     88 | CPU
DEBUG 01-15 16:10:28.699921.699921 lmp.py:1935]   Expert 37 |     89 | CPU
DEBUG 01-15 16:10:28.699326.699326 lmp.py:1935]   Expert 32 |     90 | CPU
DEBUG 01-15 16:10:28.699207.699207 lmp.py:1935]   Expert 13 |    105 | CPU
DEBUG 01-15 16:10:28.699850.699850 lmp.py:1935]   Expert 39 |    114 | CPU
DEBUG 01-15 16:10:28.699255.699255 lmp.py:1935]   Expert 21 |    118 | CPU
DEBUG 01-15 16:10:28.699375.699375 lmp.py:1935]   Expert 11 |    121 | CPU
DEBUG 01-15 16:10:28.699017.699017 lmp.py:1935]   Expert 20 |    127 | CPU
DEBUG 01-15 16:10:28.699660.699660 lmp.py:1935]   Expert  8 |    129 | CPU
DEBUG 01-15 16:10:28.699065.699065 lmp.py:1935]   Expert 60 |    131 | CPU
DEBUG 01-15 16:10:28.699476.699476 lmp.py:1935]   Expert 22 |    138 | CPU
DEBUG 01-15 16:10:28.699358.699358 lmp.py:1935]   Expert 14 |    139 | CPU
DEBUG 01-15 16:10:28.699762.699762 lmp.py:1935]   Expert 57 |    139 | CPU
DEBUG 01-15 16:10:28.699405.699405 lmp.py:1935]   Expert 45 |    150 | CPU
DEBUG 01-15 16:10:28.699479.699479 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:10:28.699122.699122 lmp.py:1935]   Expert 23 |    158 | CPU
DEBUG 01-15 16:10:28.699334.699334 lmp.py:1935]   Expert 17 |    159 | CPU
DEBUG 01-15 16:10:28.699785.699785 lmp.py:1935]   Expert 18 |    159 | CPU
DEBUG 01-15 16:10:28.699793.699793 lmp.py:1935]   Expert  7 |    162 | CPU
DEBUG 01-15 16:10:28.699913.699913 lmp.py:1935]   Expert 58 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699702.699702 lmp.py:1935]   Expert 30 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699060.699060 lmp.py:1935]   Expert 42 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699180.699180 lmp.py:1935]   Expert 48 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699776.699776 lmp.py:1935]   Expert 49 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699419.699419 lmp.py:1935]   Expert 62 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699585.699585 lmp.py:1935]   Expert 55 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699752.699752 lmp.py:1935]   Expert 35 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699679.699679 lmp.py:1935]   Expert 51 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699369.699369 lmp.py:1935]   Expert 29 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699535.699535 lmp.py:1935]   Expert  6 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699893.699893 lmp.py:1935]   Expert 25 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699013.699013 lmp.py:1935]   Expert 36 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699133.699133 lmp.py:1935]   Expert  1 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699776.699776 lmp.py:1935]   Expert 31 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699419.699419 lmp.py:1935]   Expert 28 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699346.699346 lmp.py:1935]   Expert 54 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699036.699036 lmp.py:1935]   Expert  5 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699202.699202 lmp.py:1935]   Expert 41 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699129.699129 lmp.py:1935]   Expert  9 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699580.699580 lmp.py:1935]   Expert 19 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699508.699508 lmp.py:1935]   Expert 24 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699197.699197 lmp.py:1935]   Expert 50 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699125.699125 lmp.py:1935]   Expert 46 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.699530.699530 lmp.py:1935]   Expert 59 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.699411.699411 lmp.py:1935]   Expert 56 |    373 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.700054.700054 lmp.py:1935]   Expert 26 |    404 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.700697.700697 lmp.py:1935]   Expert 33 |    421 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.700863.700863 lmp.py:1935]   Expert  3 |    587 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.700791.700791 lmp.py:1935]   Expert 10 |    641 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.700719.700719 lmp.py:1935]   Expert 15 |    644 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.700408.700408 lmp.py:1935]   Expert 40 |    792 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.700905.700905 lmp.py:1937] 
DEBUG 01-15 16:10:28.700905.700905 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.700833.700833 lmp.py:1938]   CPU:   3113 tokens
DEBUG 01-15 16:10:28.700237.700237 lmp.py:1942]   cuda:1:   4588 tokens (16 experts)
DEBUG 01-15 16:10:28.700403.700403 lmp.py:1942]   cuda:2:   4587 tokens (16 experts)
DEBUG 01-15 16:10:28.700616.700616 lmp.py:1943]   Total GPU:   9175 tokens
DEBUG 01-15 16:10:28.700590.700590 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.700590.700590 lmp.py:1944] 
DEBUG 01-15 16:10:28.700240.700240 cuda_h.py:19] end experts_map_get cost 0.0017867088317871094 seconds
DEBUG 01-15 16:10:28.700043.700043 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.700561.700561 lmp.py:1953] 
DEBUG 01-15 16:10:28.700561.700561 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.700298.700298 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:28.700563.700563 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.700453.700453 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.702570.702570 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.1.gate_proj.weight': 0, 'model.layers.12.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.58.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.58.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.58.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.50.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.50.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.50.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.51.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.51.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.51.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.54.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.54.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.54.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.59.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.59.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.59.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.12.mlp.experts.33.gate_proj.weight': 0, 'model.layers.12.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.42.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.42.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.42.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.49.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.49.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.49.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.56.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.56.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.56.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(15058075648, 5767168, 0, 0), (15063842816, 5767168, 5767168, 0), (15052308480, 5767168, 11534336, 0), (15092678656, 5767168, 17301504, 0), (15098445824, 5767168, 23068672, 0), (15086911488, 5767168, 28835840, 0), (16044261376, 5767168, 34603008, 0), (16050028544, 5767168, 40370176, 0), (16038494208, 5767168, 46137344, 0), (15127281664, 5767168, 51904512, 0), (15133048832, 5767168, 57671680, 0), (15121514496, 5767168, 63438848, 0), (15732834304, 5767168, 69206016, 0), (15738601472, 5767168, 74973184, 0), (15727067136, 5767168, 80740352, 0), (15871246336, 5767168, 86507520, 0), (15877013504, 5767168, 92274688, 0), (15865479168, 5767168, 98041856, 0), (15905849344, 5767168, 103809024, 0), (15911616512, 5767168, 109576192, 0), (15900082176, 5767168, 115343360, 0), (15923150848, 5767168, 121110528, 0), (15928918016, 5767168, 126877696, 0), (15917383680, 5767168, 132644864, 0), (15975055360, 5767168, 138412032, 0), (15980822528, 5767168, 144179200, 0), (15969288192, 5767168, 149946368, 0), (15456010240, 5767168, 155713536, 0), (15461777408, 5767168, 161480704, 0), (15450243072, 5767168, 167247872, 0), (15473311744, 5767168, 173015040, 0), (15479078912, 5767168, 178782208, 0), (15467544576, 5767168, 184549376, 0), (15490613248, 5767168, 190316544, 0), (15496380416, 5767168, 196083712, 0), (15484846080, 5767168, 201850880, 0), (16061562880, 5767168, 207618048, 0), (16067330048, 5767168, 213385216, 0), (16055795712, 5767168, 219152384, 0), (15542517760, 5767168, 224919552, 0), (15548284928, 5767168, 230686720, 0), (15536750592, 5767168, 236453888, 0), (16113467392, 5767168, 242221056, 0), (16119234560, 5767168, 247988224, 0), (16107700224, 5767168, 253755392, 0), (15577120768, 5767168, 259522560, 0), (15582887936, 5767168, 265289728, 0), (15571353600, 5767168, 271056896, 0)], 2: [(15611723776, 5767168, 0, 0), (15617490944, 5767168, 5767168, 0), (15605956608, 5767168, 11534336, 0), (15646326784, 5767168, 17301504, 0), (15652093952, 5767168, 23068672, 0), (15640559616, 5767168, 28835840, 0), (15663628288, 5767168, 34603008, 0), (15669395456, 5767168, 40370176, 0), (15657861120, 5767168, 46137344, 0), (15144583168, 5767168, 51904512, 0), (15150350336, 5767168, 57671680, 0), (15138816000, 5767168, 63438848, 0), (15196487680, 5767168, 69206016, 0), (15202254848, 5767168, 74973184, 0), (15190720512, 5767168, 80740352, 0), (15213789184, 5767168, 86507520, 0), (15219556352, 5767168, 92274688, 0), (15208022016, 5767168, 98041856, 0), (15750135808, 5767168, 103809024, 0), (15755902976, 5767168, 109576192, 0), (15744368640, 5767168, 115343360, 0), (15767437312, 5767168, 121110528, 0), (15773204480, 5767168, 126877696, 0), (15761670144, 5767168, 132644864, 0), (15836643328, 5767168, 138412032, 0), (15842410496, 5767168, 144179200, 0), (15830876160, 5767168, 149946368, 0), (15300296704, 5767168, 155713536, 0), (15306063872, 5767168, 161480704, 0), (15294529536, 5767168, 167247872, 0), (15888547840, 5767168, 173015040, 0), (15894315008, 5767168, 178782208, 0), (15882780672, 5767168, 184549376, 0), (15369502720, 5767168, 190316544, 0), (15375269888, 5767168, 196083712, 0), (15363735552, 5767168, 201850880, 0), (15992356864, 5767168, 207618048, 0), (15998124032, 5767168, 213385216, 0), (15986589696, 5767168, 219152384, 0), (16009658368, 5767168, 224919552, 0), (16015425536, 5767168, 230686720, 0), (16003891200, 5767168, 236453888, 0), (15525216256, 5767168, 242221056, 0), (15530983424, 5767168, 247988224, 0), (15519449088, 5767168, 253755392, 0), (15559819264, 5767168, 259522560, 0), (15565586432, 5767168, 265289728, 0), (15554052096, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c2486f0>, 2: <capsule object NULL at 0x7a51b0646160>}
DEBUG 01-15 16:10:28.702713.702713 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.702019.702019 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.702914.702914 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6f079206-2a9f-4d41-92a6-c31fb3076508
DEBUG 01-15 16:10:28.702630.702630 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:28.703907.703907 client.py:127] Model loaded
DEBUG 01-15 16:10:28.703996.703996 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.703857.703857 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:28.703750.703750 cuda_h.py:19] end restore2model cost 0.00034689903259277344 seconds
DEBUG 01-15 16:10:28.703758.703758 cuda_h.py:19] end sllm_worker_task cost 0.01099705696105957 seconds
DEBUG 01-15 16:10:28.703675.703675 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:28.703397.703397 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6f079206-2a9f-4d41-92a6-c31fb3076508
DEBUG 01-15 16:10:28.704208.704208 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003917217254638672 seconds
DEBUG 01-15 16:10:28.704323.704323 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.704592.704592 cuda_h.py:19] end move_flatidxs cost 0.0008254051208496094 seconds
DEBUG 01-15 16:10:28.704322.704322 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.707922.707922 cuda_h.py:19] end restore2model cost 0.00261688232421875 seconds
DEBUG 01-15 16:10:28.707163.707163 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006799936294555664 seconds
DEBUG 01-15 16:10:28.707197.707197 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.707088.707088 cuda_h.py:19] end gpu_sexperts cost 0.00027370452880859375 seconds
DEBUG 01-15 16:10:28.707394.707394 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.707502.707502 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:28.707151.707151 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.707854.707854 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.708300.708300 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008227825164794922 seconds
DEBUG 01-15 16:10:28.708143.708143 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.708945.708945 cuda_h.py:19] end gpu_group_list cost 0.00017595291137695312 seconds
DEBUG 01-15 16:10:28.709665.709665 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.710134.710134 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008821487426757812 seconds
DEBUG 01-15 16:10:28.710513.710513 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.710414.710414 cuda_h.py:19] end gpu_group_list cost 0.00017762184143066406 seconds
DEBUG 01-15 16:10:28.711255.711255 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.711707.711707 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6f079206-2a9f-4d41-92a6-c31fb3076508
DEBUG 01-15 16:10:28.713465.713465 cuda_h.py:19] end group_tensors cost 0.008775711059570312 seconds
DEBUG 01-15 16:10:28.714410.714410 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.717148.717148 cuda_h.py:19] end group pad cost 0.0034074783325195312 seconds
DEBUG 01-15 16:10:28.717176.717176 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.730474.730474 client.py:127] Model loaded
DEBUG 01-15 16:10:28.730069.730069 cuda_h.py:19] end wait_experts_multi_device cost 0.01949334144592285 seconds
DEBUG 01-15 16:10:28.730832.730832 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.737896.737896 cuda_h.py:19] end group_einsum cost 0.019229650497436523 seconds
DEBUG 01-15 16:10:28.737206.737206 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.740055.740055 cuda_h.py:19] end get_outputs_cpu1 cost 0.0031614303588867188 seconds
DEBUG 01-15 16:10:28.741866.741866 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.037578582763671875 seconds
DEBUG 01-15 16:10:28.741872.741872 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010645389556884766 seconds
DEBUG 01-15 16:10:28.741200.741200 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.742590.742590 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.743602.743602 cuda_h.py:19] end index_scatter cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:10:28.743500.743500 cuda_h.py:19] end cpuoutputsdeal cost 0.0015871524810791016 seconds
DEBUG 01-15 16:10:28.743840.743840 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.743457.743457 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.743635.743635 cuda_h.py:19] end gpu_group_tensor cost 0.00013518333435058594 seconds
DEBUG 01-15 16:10:28.743205.743205 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.743793.743793 cuda_h.py:19] end gpu_group_tensor cost 0.00012183189392089844 seconds
DEBUG 01-15 16:10:28.743174.743174 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.744847.744847 cuda_h.py:19] end gpu_group_einsum cost 0.0006539821624755859 seconds
DEBUG 01-15 16:10:28.744905.744905 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.745261.745261 cuda_h.py:19] end gpu_group_einsum cost 0.00048065185546875 seconds
DEBUG 01-15 16:10:28.745292.745292 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.745594.745594 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.745609.745609 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002079010009765625 seconds
DEBUG 01-15 16:10:28.745617.745617 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.745898.745898 cuda_h.py:19] end concat_expert_out cost 6.413459777832031e-05 seconds
DEBUG 01-15 16:10:28.746788.746788 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.746480.746480 cuda_h.py:19] end index_scatter cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:10:28.746183.746183 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008497238159179688 seconds
DEBUG 01-15 16:10:28.746875.746875 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.746479.746479 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.746504.746504 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012803077697753906 seconds
DEBUG 01-15 16:10:28.746830.746830 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.746415.746415 cuda_h.py:19] end concat_expert_out cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:28.746397.746397 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.746897.746897 cuda_h.py:19] end index_scatter cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:10:28.746660.746660 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004715919494628906 seconds
DEBUG 01-15 16:10:28.747516.747516 cuda_h.py:19] end gpu_experts_multi_device cost 0.039362192153930664 seconds
DEBUG 01-15 16:10:28.747426.747426 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.04952859878540039 seconds
DEBUG 01-15 16:10:28.747002.747002 cuda_h.py:19] end prefill_layer cost 0.055136680603027344 seconds
DEBUG 01-15 16:10:28.747977.747977 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:10:28.747396.747396 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.747575.747575 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:10:28.747947.747947 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:28.747511.747511 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:28.747507.747507 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:10:28.747786.747786 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 7.176399230957031e-05 seconds
DEBUG 01-15 16:10:28.747151.747151 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.747915.747915 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.748767.748767 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.748212.748212 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.748065.748065 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.748129.748129 cuda_h.py:19] end allocate_cuda_memory cost 0.00028634071350097656 seconds
DEBUG 01-15 16:10:28.748323.748323 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.748324.748324 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.748485.748485 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.748241.748241 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5e7d4df0-568e-4519-8df8-96db751b2ee6
DEBUG 01-15 16:10:28.748344.748344 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.749660.749660 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.750653.750653 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5e7d4df0-568e-4519-8df8-96db751b2ee6
DEBUG 01-15 16:10:28.750019.750019 cuda_h.py:19] end load_into_gpu_async cost 0.0021619796752929688 seconds
DEBUG 01-15 16:10:28.750060.750060 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.750369.750369 cuda_h.py:19] end restore_tensors2 cost 8.96453857421875e-05 seconds
DEBUG 01-15 16:10:28.750701.750701 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0028133392333984375 seconds
INFO 01-15 16:10:28.751597.751597 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5e7d4df0-568e-4519-8df8-96db751b2ee6
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.752445.752445 cuda_h.py:19] end self_attn cost 0.0032091140747070312 seconds
DEBUG 01-15 16:10:28.752549.752549 cuda_h.py:19] end iln_self_attn_paln cost 0.004930734634399414 seconds
DEBUG 01-15 16:10:28.752094.752094 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:10:28.752287.752287 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.753438.753438 cuda_h.py:19] end gate cost 0.0006976127624511719 seconds
DEBUG 01-15 16:10:28.753658.753658 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.754777.754777 lmp.py:1912] 
DEBUG 01-15 16:10:28.754777.754777 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.754587.754587 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.754528.754528 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.754370.754370 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.754543.754543 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.754000.754000 lmp.py:1917] 
DEBUG 01-15 16:10:28.754000.754000 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.754173.754173 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.754784.754784 lmp.py:1935]   Expert 42 |     22 | CPU
DEBUG 01-15 16:10:28.754764.754764 lmp.py:1935]   Expert 19 |     23 | CPU
DEBUG 01-15 16:10:28.754507.754507 lmp.py:1935]   Expert 30 |     27 | CPU
DEBUG 01-15 16:10:28.754534.754534 lmp.py:1935]   Expert 32 |     42 | CPU
DEBUG 01-15 16:10:28.754276.754276 lmp.py:1935]   Expert  6 |     58 | CPU
DEBUG 01-15 16:10:28.754496.754496 lmp.py:1935]   Expert 53 |     73 | CPU
DEBUG 01-15 16:10:28.754099.754099 lmp.py:1935]   Expert  5 |     76 | CPU
DEBUG 01-15 16:10:28.754226.754226 lmp.py:1935]   Expert  1 |     80 | CPU
DEBUG 01-15 16:10:28.754160.754160 lmp.py:1935]   Expert 13 |    121 | CPU
DEBUG 01-15 16:10:28.754379.754379 lmp.py:1935]   Expert  9 |    122 | CPU
DEBUG 01-15 16:10:28.754075.754075 lmp.py:1935]   Expert 63 |    125 | CPU
DEBUG 01-15 16:10:28.754725.754725 lmp.py:1935]   Expert 34 |    127 | CPU
DEBUG 01-15 16:10:28.754130.754130 lmp.py:1935]   Expert 58 |    129 | CPU
DEBUG 01-15 16:10:28.754773.754773 lmp.py:1935]   Expert 50 |    134 | CPU
DEBUG 01-15 16:10:28.754416.754416 lmp.py:1935]   Expert 26 |    135 | CPU
DEBUG 01-15 16:10:28.754012.754012 lmp.py:1935]   Expert 11 |    136 | CPU
DEBUG 01-15 16:10:28.754132.754132 lmp.py:1935]   Expert 31 |    137 | CPU
DEBUG 01-15 16:10:28.754014.754014 lmp.py:1935]   Expert 18 |    141 | CPU
DEBUG 01-15 16:10:28.754372.754372 lmp.py:1935]   Expert 59 |    141 | CPU
DEBUG 01-15 16:10:28.754015.754015 lmp.py:1935]   Expert 40 |    144 | CPU
DEBUG 01-15 16:10:28.754419.754419 lmp.py:1935]   Expert 12 |    148 | CPU
DEBUG 01-15 16:10:28.754824.754824 lmp.py:1935]   Expert 46 |    150 | CPU
DEBUG 01-15 16:10:28.754467.754467 lmp.py:1935]   Expert  2 |    151 | CPU
DEBUG 01-15 16:10:28.754633.754633 lmp.py:1935]   Expert  4 |    151 | CPU
DEBUG 01-15 16:10:28.754038.754038 lmp.py:1935]   Expert 48 |    152 | CPU
DEBUG 01-15 16:10:28.754442.754442 lmp.py:1935]   Expert 56 |    152 | CPU
DEBUG 01-15 16:10:28.754847.754847 lmp.py:1935]   Expert 20 |    154 | CPU
DEBUG 01-15 16:10:28.754490.754490 lmp.py:1935]   Expert 33 |    156 | CPU
DEBUG 01-15 16:10:28.754133.754133 lmp.py:1935]   Expert 61 |    157 | CPU
DEBUG 01-15 16:10:28.754014.754014 lmp.py:1935]   Expert 35 |    162 | CPU
DEBUG 01-15 16:10:28.754849.754849 lmp.py:1935]   Expert 10 |    166 | CPU
DEBUG 01-15 16:10:28.754684.754684 lmp.py:1935]   Expert 55 |    171 | CPU
DEBUG 01-15 16:10:28.754473.754473 lmp.py:1935]   Expert 51 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.754023.754023 lmp.py:1935]   Expert 36 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.754097.754097 lmp.py:1935]   Expert  8 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.754170.754170 lmp.py:1935]   Expert 37 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.754767.754767 lmp.py:1935]   Expert 52 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.754602.754602 lmp.py:1935]   Expert  0 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.754199.754199 lmp.py:1935]   Expert 57 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.754511.754511 lmp.py:1935]   Expert 39 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.754822.754822 lmp.py:1935]   Expert 25 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755896.755896 lmp.py:1935]   Expert 62 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755969.755969 lmp.py:1935]   Expert 38 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755566.755566 lmp.py:1935]   Expert  7 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755924.755924 lmp.py:1935]   Expert 27 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755044.755044 lmp.py:1935]   Expert  3 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755641.755641 lmp.py:1935]   Expert 24 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755999.755999 lmp.py:1935]   Expert 28 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755921.755921 lmp.py:1935]   Expert 60 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755756.755756 lmp.py:1935]   Expert 21 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755353.755353 lmp.py:1935]   Expert 49 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755188.755188 lmp.py:1935]   Expert 16 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755798.755798 lmp.py:1935]   Expert 43 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755633.755633 lmp.py:1935]   Expert 23 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755799.755799 lmp.py:1935]   Expert 29 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755204.755204 lmp.py:1935]   Expert 15 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755608.755608 lmp.py:1935]   Expert 47 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755013.755013 lmp.py:1935]   Expert 22 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755179.755179 lmp.py:1935]   Expert 41 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755822.755822 lmp.py:1935]   Expert 44 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755227.755227 lmp.py:1935]   Expert 54 |    358 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755631.755631 lmp.py:1935]   Expert 14 |    374 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755036.755036 lmp.py:1935]   Expert 17 |    405 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.755440.755440 lmp.py:1935]   Expert 45 |    450 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.755845.755845 lmp.py:1937] 
DEBUG 01-15 16:10:28.755845.755845 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.755726.755726 lmp.py:1938]   CPU:   3863 tokens
DEBUG 01-15 16:10:28.755608.755608 lmp.py:1942]   cuda:1:   4212 tokens (16 experts)
DEBUG 01-15 16:10:28.755489.755489 lmp.py:1942]   cuda:2:   4213 tokens (16 experts)
DEBUG 01-15 16:10:28.755178.755178 lmp.py:1943]   Total GPU:   8425 tokens
DEBUG 01-15 16:10:28.755629.755629 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.755629.755629 lmp.py:1944] 
DEBUG 01-15 16:10:28.755802.755802 cuda_h.py:19] end experts_map_get cost 0.0019004344940185547 seconds
DEBUG 01-15 16:10:28.755937.755937 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.755593.755593 lmp.py:1953] 
DEBUG 01-15 16:10:28.755593.755593 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.755423.755423 cuda_h.py:19] end cpu_experts_submit cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:28.755424.755424 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.755108.755108 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.757731.757731 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.3.gate_proj.weight': 0, 'model.layers.13.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.29.up_proj.weight': 271056896}, 2: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.38.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.38.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.38.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.14.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.14.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.14.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.51.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.51.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.51.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(16199974912, 5767168, 0, 0), (16205742080, 5767168, 5767168, 0), (16194207744, 5767168, 11534336, 0), (16770924544, 5767168, 17301504, 0), (16776691712, 5767168, 23068672, 0), (16765157376, 5767168, 28835840, 0), (16788226048, 5767168, 34603008, 0), (16793993216, 5767168, 40370176, 0), (16782458880, 5767168, 46137344, 0), (16269180928, 5767168, 51904512, 0), (16274948096, 5767168, 57671680, 0), (16263413760, 5767168, 63438848, 0), (16822829056, 5767168, 69206016, 0), (16828596224, 5767168, 74973184, 0), (16817061888, 5767168, 80740352, 0), (16857432064, 5767168, 86507520, 0), (16863199232, 5767168, 92274688, 0), (16851664896, 5767168, 98041856, 0), (16892035072, 5767168, 103809024, 0), (16897802240, 5767168, 109576192, 0), (16886267904, 5767168, 115343360, 0), (16926638080, 5767168, 121110528, 0), (16932405248, 5767168, 126877696, 0), (16920870912, 5767168, 132644864, 0), (16961241088, 5767168, 138412032, 0), (16967008256, 5767168, 144179200, 0), (16955473920, 5767168, 149946368, 0), (16995844096, 5767168, 155713536, 0), (17001611264, 5767168, 161480704, 0), (16990076928, 5767168, 167247872, 0), (17047748608, 5767168, 173015040, 0), (17053515776, 5767168, 178782208, 0), (17041981440, 5767168, 184549376, 0), (16511401984, 5767168, 190316544, 0), (16517169152, 5767168, 196083712, 0), (16505634816, 5767168, 201850880, 0), (17082351616, 5767168, 207618048, 0), (17088118784, 5767168, 213385216, 0), (17076584448, 5767168, 219152384, 0), (16580608000, 5767168, 224919552, 0), (16586375168, 5767168, 230686720, 0), (16574840832, 5767168, 236453888, 0), (16632512512, 5767168, 242221056, 0), (16638279680, 5767168, 247988224, 0), (16626745344, 5767168, 253755392, 0), (16649814016, 5767168, 259522560, 0), (16655581184, 5767168, 265289728, 0), (16644046848, 5767168, 271056896, 0)], 2: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16805527552, 5767168, 17301504, 0), (16811294720, 5767168, 23068672, 0), (16799760384, 5767168, 28835840, 0), (16286482432, 5767168, 34603008, 0), (16292249600, 5767168, 40370176, 0), (16280715264, 5767168, 46137344, 0), (16909336576, 5767168, 51904512, 0), (16915103744, 5767168, 57671680, 0), (16903569408, 5767168, 63438848, 0), (16390291456, 5767168, 69206016, 0), (16396058624, 5767168, 74973184, 0), (16384524288, 5767168, 80740352, 0), (16407592960, 5767168, 86507520, 0), (16413360128, 5767168, 92274688, 0), (16401825792, 5767168, 98041856, 0), (16424894464, 5767168, 103809024, 0), (16430661632, 5767168, 109576192, 0), (16419127296, 5767168, 115343360, 0), (16442195968, 5767168, 121110528, 0), (16447963136, 5767168, 126877696, 0), (16436428800, 5767168, 132644864, 0), (17030447104, 5767168, 138412032, 0), (17036214272, 5767168, 144179200, 0), (17024679936, 5767168, 149946368, 0), (16528703488, 5767168, 155713536, 0), (16534470656, 5767168, 161480704, 0), (16522936320, 5767168, 167247872, 0), (16546004992, 5767168, 173015040, 0), (16551772160, 5767168, 178782208, 0), (16540237824, 5767168, 184549376, 0), (16563306496, 5767168, 190316544, 0), (16569073664, 5767168, 196083712, 0), (16557539328, 5767168, 201850880, 0), (17134256128, 5767168, 207618048, 0), (17140023296, 5767168, 213385216, 0), (17128488960, 5767168, 219152384, 0), (16615211008, 5767168, 224919552, 0), (16620978176, 5767168, 230686720, 0), (16609443840, 5767168, 236453888, 0), (17186160640, 5767168, 242221056, 0), (17191927808, 5767168, 247988224, 0), (17180393472, 5767168, 253755392, 0), (17220763648, 5767168, 259522560, 0), (17226530816, 5767168, 265289728, 0), (17214996480, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b06460a0>, 2: <capsule object NULL at 0x7a51b0645f80>}
DEBUG 01-15 16:10:28.757670.757670 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.757129.757129 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.757547.757547 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bb30afc8-f0cf-4b0b-8df6-0ef8654853da
DEBUG 01-15 16:10:28.758825.758825 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.758299.758299 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:28.758140.758140 client.py:127] Model loaded
DEBUG 01-15 16:10:28.758104.758104 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.758501.758501 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.758554.758554 cuda_h.py:19] end restore2model cost 0.00036787986755371094 seconds
DEBUG 01-15 16:10:28.758993.758993 cuda_h.py:19] end sllm_worker_task cost 0.010665655136108398 seconds
INFO 01-15 16:10:28.759667.759667 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bb30afc8-f0cf-4b0b-8df6-0ef8654853da
DEBUG 01-15 16:10:28.759961.759961 cuda_h.py:19] end move_flatidxs cost 0.0008378028869628906 seconds
DEBUG 01-15 16:10:28.759851.759851 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.759714.759714 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036470890045166016 seconds
DEBUG 01-15 16:10:28.759293.759293 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.762660.762660 cuda_h.py:19] end restore2model cost 0.0026242733001708984 seconds
DEBUG 01-15 16:10:28.762656.762656 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006503582000732422 seconds
DEBUG 01-15 16:10:28.762643.762643 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.762958.762958 cuda_h.py:19] end gpu_sexperts cost 0.00027179718017578125 seconds
DEBUG 01-15 16:10:28.762072.762072 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.762756.762756 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7642974853515625e-05 seconds
DEBUG 01-15 16:10:28.762929.762929 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.762586.762586 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.763640.763640 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008175373077392578 seconds
DEBUG 01-15 16:10:28.763245.763245 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.763947.763947 cuda_h.py:19] end gpu_group_list cost 0.00017404556274414062 seconds
DEBUG 01-15 16:10:28.764350.764350 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.765064.765064 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008826255798339844 seconds
DEBUG 01-15 16:10:28.765675.765675 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.765384.765384 cuda_h.py:19] end gpu_group_list cost 0.0001766681671142578 seconds
DEBUG 01-15 16:10:28.766040.766040 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.766207.766207 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bb30afc8-f0cf-4b0b-8df6-0ef8654853da
DEBUG 01-15 16:10:28.769299.769299 cuda_h.py:19] end group_tensors cost 0.010362625122070312 seconds
DEBUG 01-15 16:10:28.770350.770350 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.774979.774979 cuda_h.py:19] end group pad cost 0.003713369369506836 seconds
DEBUG 01-15 16:10:28.774246.774246 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.786021.786021 client.py:127] Model loaded
DEBUG 01-15 16:10:28.786537.786537 cuda_h.py:19] end wait_experts_multi_device cost 0.020309925079345703 seconds
DEBUG 01-15 16:10:28.786783.786783 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.793403.793403 cuda_h.py:19] end group_einsum cost 0.018840551376342773 seconds
DEBUG 01-15 16:10:28.793852.793852 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.797251.797251 cuda_h.py:19] end get_outputs_cpu1 cost 0.003731250762939453 seconds
DEBUG 01-15 16:10:28.798146.798146 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03984880447387695 seconds
DEBUG 01-15 16:10:28.798962.798962 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01162576675415039 seconds
DEBUG 01-15 16:10:28.798496.798496 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.799151.799151 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.799686.799686 cuda_h.py:19] end index_scatter cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:10:28.800008.800008 cuda_h.py:19] end cpuoutputsdeal cost 0.0016021728515625 seconds
DEBUG 01-15 16:10:28.800878.800878 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.800888.800888 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.800411.800411 cuda_h.py:19] end gpu_group_tensor cost 0.00014090538024902344 seconds
DEBUG 01-15 16:10:28.800743.800743 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.800139.800139 cuda_h.py:19] end gpu_group_tensor cost 0.00012183189392089844 seconds
DEBUG 01-15 16:10:28.800559.800559 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.801127.801127 cuda_h.py:19] end gpu_group_einsum cost 0.0004761219024658203 seconds
DEBUG 01-15 16:10:28.801766.801766 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.801250.801250 cuda_h.py:19] end gpu_group_einsum cost 0.00034332275390625 seconds
DEBUG 01-15 16:10:28.802869.802869 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.802144.802144 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.802946.802946 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016832351684570312 seconds
DEBUG 01-15 16:10:28.802417.802417 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.802380.802380 cuda_h.py:19] end concat_expert_out cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:10:28.802554.802554 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.802239.802239 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:28.802081.802081 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007431507110595703 seconds
DEBUG 01-15 16:10:28.802203.802203 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.803470.803470 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.803555.803555 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013637542724609375 seconds
DEBUG 01-15 16:10:28.803788.803788 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.803996.803996 cuda_h.py:19] end concat_expert_out cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:28.803170.803170 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.803809.803809 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:28.803903.803903 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004801750183105469 seconds
DEBUG 01-15 16:10:28.803044.803044 cuda_h.py:19] end gpu_experts_multi_device cost 0.04079174995422363 seconds
DEBUG 01-15 16:10:28.803391.803391 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.05078291893005371 seconds
DEBUG 01-15 16:10:28.804199.804199 cuda_h.py:19] end prefill_layer cost 0.056389808654785156 seconds
DEBUG 01-15 16:10:28.804234.804234 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:10:28.804175.804175 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.804355.804355 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:10:28.804727.804727 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:28.804006.804006 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:28.804048.804048 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 3.7670135498046875e-05 seconds
DEBUG 01-15 16:10:28.804565.804565 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:10:28.804361.804361 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.804317.804317 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.804567.804567 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.804012.804012 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.804765.804765 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.805073.805073 cuda_h.py:19] end allocate_cuda_memory cost 0.00025725364685058594 seconds
DEBUG 01-15 16:10:28.805976.805976 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.805692.805692 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.805522.805522 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.805755.805755 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0433e28b-964d-498b-ba23-200d21824ad0
DEBUG 01-15 16:10:28.805096.805096 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.805234.805234 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.806192.806192 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0433e28b-964d-498b-ba23-200d21824ad0
DEBUG 01-15 16:10:28.806035.806035 cuda_h.py:19] end load_into_gpu_async cost 0.001516103744506836 seconds
DEBUG 01-15 16:10:28.806360.806360 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.806285.806285 cuda_h.py:19] end restore_tensors2 cost 8.869171142578125e-05 seconds
DEBUG 01-15 16:10:28.806094.806094 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021293163299560547 seconds
INFO 01-15 16:10:28.806103.806103 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0433e28b-964d-498b-ba23-200d21824ad0
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.808071.808071 cuda_h.py:19] end self_attn cost 0.003179311752319336 seconds
DEBUG 01-15 16:10:28.809982.809982 cuda_h.py:19] end iln_self_attn_paln cost 0.004883289337158203 seconds
DEBUG 01-15 16:10:28.809911.809911 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:10:28.809151.809151 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.810142.810142 cuda_h.py:19] end gate cost 0.0006918907165527344 seconds
DEBUG 01-15 16:10:28.810455.810455 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.810853.810853 lmp.py:1912] 
DEBUG 01-15 16:10:28.810853.810853 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.810662.810662 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.810981.810981 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.810723.810723 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.810558.810558 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.810440.810440 lmp.py:1917] 
DEBUG 01-15 16:10:28.810440.810440 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.810275.810275 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.810832.810832 lmp.py:1935]   Expert 34 |     27 | CPU
DEBUG 01-15 16:10:28.810190.810190 lmp.py:1935]   Expert  7 |     30 | CPU
DEBUG 01-15 16:10:28.810548.810548 lmp.py:1935]   Expert 13 |     42 | CPU
DEBUG 01-15 16:10:28.810191.810191 lmp.py:1935]   Expert 54 |     76 | CPU
DEBUG 01-15 16:10:28.810596.810596 lmp.py:1935]   Expert 18 |     82 | CPU
DEBUG 01-15 16:10:28.810192.810192 lmp.py:1935]   Expert 39 |     87 | CPU
DEBUG 01-15 16:10:28.810312.810312 lmp.py:1935]   Expert 49 |     89 | CPU
DEBUG 01-15 16:10:28.810670.810670 lmp.py:1935]   Expert 59 |    103 | CPU
DEBUG 01-15 16:10:28.810790.810790 lmp.py:1935]   Expert 16 |    106 | CPU
DEBUG 01-15 16:10:28.810910.810910 lmp.py:1935]   Expert 21 |    106 | CPU
DEBUG 01-15 16:10:28.810553.810553 lmp.py:1935]   Expert  0 |    109 | CPU
DEBUG 01-15 16:10:28.810719.810719 lmp.py:1935]   Expert 41 |    117 | CPU
DEBUG 01-15 16:10:28.810124.810124 lmp.py:1935]   Expert 22 |    122 | CPU
DEBUG 01-15 16:10:28.810528.810528 lmp.py:1935]   Expert 45 |    122 | CPU
DEBUG 01-15 16:10:28.810171.810171 lmp.py:1935]   Expert 15 |    123 | CPU
DEBUG 01-15 16:10:28.810814.810814 lmp.py:1935]   Expert 17 |    123 | CPU
DEBUG 01-15 16:10:28.810696.810696 lmp.py:1935]   Expert 61 |    135 | CPU
DEBUG 01-15 16:10:28.810815.810815 lmp.py:1935]   Expert  8 |    136 | CPU
DEBUG 01-15 16:10:28.810935.810935 lmp.py:1935]   Expert 35 |    136 | CPU
DEBUG 01-15 16:10:28.810770.810770 lmp.py:1935]   Expert 38 |    137 | CPU
DEBUG 01-15 16:10:28.811605.811605 lmp.py:1935]   Expert 52 |    137 | CPU
DEBUG 01-15 16:10:28.811487.811487 lmp.py:1935]   Expert 12 |    141 | CPU
DEBUG 01-15 16:10:28.811368.811368 lmp.py:1935]   Expert 48 |    149 | CPU
DEBUG 01-15 16:10:28.811011.811011 lmp.py:1935]   Expert 31 |    150 | CPU
DEBUG 01-15 16:10:28.811131.811131 lmp.py:1935]   Expert 53 |    153 | CPU
DEBUG 01-15 16:10:28.811536.811536 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:10:28.811940.811940 lmp.py:1935]   Expert 50 |    159 | CPU
DEBUG 01-15 16:10:28.811060.811060 lmp.py:1935]   Expert 40 |    160 | CPU
DEBUG 01-15 16:10:28.811418.811418 lmp.py:1935]   Expert 60 |    160 | CPU
DEBUG 01-15 16:10:28.811492.811492 lmp.py:1935]   Expert 27 |    175 | CPU
DEBUG 01-15 16:10:28.811327.811327 lmp.py:1935]   Expert 19 |    194 | CPU
DEBUG 01-15 16:10:28.811685.811685 lmp.py:1935]   Expert  4 |    198 | CPU
DEBUG 01-15 16:10:28.811235.811235 lmp.py:1935]   Expert 29 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811309.811309 lmp.py:1935]   Expert 30 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811667.811667 lmp.py:1935]   Expert 11 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811264.811264 lmp.py:1935]   Expert 26 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811603.811603 lmp.py:1935]   Expert 20 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811438.811438 lmp.py:1935]   Expert 57 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811796.811796 lmp.py:1935]   Expert  6 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811869.811869 lmp.py:1935]   Expert 46 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811751.811751 lmp.py:1935]   Expert 43 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811632.811632 lmp.py:1935]   Expert  2 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811990.811990 lmp.py:1935]   Expert 23 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811872.811872 lmp.py:1935]   Expert 33 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811992.811992 lmp.py:1935]   Expert 42 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811635.811635 lmp.py:1935]   Expert 55 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811754.811754 lmp.py:1935]   Expert 32 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811636.811636 lmp.py:1935]   Expert 56 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811471.811471 lmp.py:1935]   Expert  9 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811544.811544 lmp.py:1935]   Expert  3 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811618.811618 lmp.py:1935]   Expert 14 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811215.811215 lmp.py:1935]   Expert 28 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811003.811003 lmp.py:1935]   Expert 44 |    275 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811123.811123 lmp.py:1935]   Expert  1 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811005.811005 lmp.py:1935]   Expert 51 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811647.811647 lmp.py:1935]   Expert 58 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811529.811529 lmp.py:1935]   Expert 63 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811649.811649 lmp.py:1935]   Expert 37 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811292.811292 lmp.py:1935]   Expert 47 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811650.811650 lmp.py:1935]   Expert 24 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811770.811770 lmp.py:1935]   Expert 10 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811605.811605 lmp.py:1935]   Expert 62 |    313 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811678.811678 lmp.py:1935]   Expert 25 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.811798.811798 lmp.py:1935]   Expert  5 |    365 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.811249.811249 lmp.py:1937] 
DEBUG 01-15 16:10:28.811249.811249 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.811892.811892 lmp.py:1938]   CPU:   3939 tokens
DEBUG 01-15 16:10:28.811773.811773 lmp.py:1942]   cuda:1:   4176 tokens (16 experts)
DEBUG 01-15 16:10:28.811178.811178 lmp.py:1942]   cuda:2:   4173 tokens (16 experts)
DEBUG 01-15 16:10:28.811106.811106 lmp.py:1943]   Total GPU:   8349 tokens
DEBUG 01-15 16:10:28.811795.811795 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.811795.811795 lmp.py:1944] 
DEBUG 01-15 16:10:28.811922.811922 cuda_h.py:19] end experts_map_get cost 0.0018215179443359375 seconds
DEBUG 01-15 16:10:28.812394.812394 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.812673.812673 lmp.py:1953] 
DEBUG 01-15 16:10:28.812673.812673 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.812933.812933 cuda_h.py:19] end cpu_experts_submit cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:28.812152.812152 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.812075.812075 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.814415.814415 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.32.gate_proj.weight': 0, 'model.layers.14.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.33.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.33.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.33.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.2.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.2.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.2.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.5.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.5.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.5.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.6.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.6.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.6.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.10.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.10.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.10.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.43.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.43.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.43.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.44.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.44.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.44.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.11.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.11.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.11.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.14.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.14.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.14.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.47.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.47.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.47.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.20.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.20.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.20.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.14.mlp.experts.3.gate_proj.weight': 0, 'model.layers.14.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.56.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.56.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.56.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.26.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.26.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.26.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.51.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.51.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.51.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.55.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.55.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.55.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.57.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(17809014784, 5767168, 0, 0), (17814781952, 5767168, 5767168, 0), (17803247616, 5767168, 11534336, 0), (17272668160, 5767168, 17301504, 0), (17278435328, 5767168, 23068672, 0), (17266900992, 5767168, 28835840, 0), (17826316288, 5767168, 34603008, 0), (17832083456, 5767168, 40370176, 0), (17820549120, 5767168, 46137344, 0), (17289969664, 5767168, 51904512, 0), (17295736832, 5767168, 57671680, 0), (17284202496, 5767168, 63438848, 0), (17341874176, 5767168, 69206016, 0), (17347641344, 5767168, 74973184, 0), (17336107008, 5767168, 80740352, 0), (17359175680, 5767168, 86507520, 0), (17364942848, 5767168, 92274688, 0), (17353408512, 5767168, 98041856, 0), (17411080192, 5767168, 103809024, 0), (17416847360, 5767168, 109576192, 0), (17405313024, 5767168, 115343360, 0), (17428381696, 5767168, 121110528, 0), (17434148864, 5767168, 126877696, 0), (17422614528, 5767168, 132644864, 0), (17999331328, 5767168, 138412032, 0), (18005098496, 5767168, 144179200, 0), (17993564160, 5767168, 149946368, 0), (18016632832, 5767168, 155713536, 0), (18022400000, 5767168, 161480704, 0), (18010865664, 5767168, 167247872, 0), (17445683200, 5767168, 173015040, 0), (17451450368, 5767168, 178782208, 0), (17439916032, 5767168, 184549376, 0), (17497587712, 5767168, 190316544, 0), (17503354880, 5767168, 196083712, 0), (17491820544, 5767168, 201850880, 0), (18068537344, 5767168, 207618048, 0), (18074304512, 5767168, 213385216, 0), (18062770176, 5767168, 219152384, 0), (17601396736, 5767168, 224919552, 0), (17607163904, 5767168, 230686720, 0), (17595629568, 5767168, 236453888, 0), (17774411776, 5767168, 242221056, 0), (17780178944, 5767168, 247988224, 0), (17768644608, 5767168, 253755392, 0), (18345361408, 5767168, 259522560, 0), (18351128576, 5767168, 265289728, 0), (18339594240, 5767168, 271056896, 0)], 2: [(17307271168, 5767168, 0, 0), (17313038336, 5767168, 5767168, 0), (17301504000, 5767168, 11534336, 0), (18224250880, 5767168, 17301504, 0), (18230018048, 5767168, 23068672, 0), (18218483712, 5767168, 28835840, 0), (17895522304, 5767168, 34603008, 0), (17901289472, 5767168, 40370176, 0), (17889755136, 5767168, 46137344, 0), (17705205760, 5767168, 51904512, 0), (17710972928, 5767168, 57671680, 0), (17699438592, 5767168, 63438848, 0), (17982029824, 5767168, 69206016, 0), (17987796992, 5767168, 74973184, 0), (17976262656, 5767168, 80740352, 0), (18051235840, 5767168, 86507520, 0), (18057003008, 5767168, 92274688, 0), (18045468672, 5767168, 98041856, 0), (18137743360, 5767168, 103809024, 0), (18143510528, 5767168, 109576192, 0), (18131976192, 5767168, 115343360, 0), (17653301248, 5767168, 121110528, 0), (17659068416, 5767168, 126877696, 0), (17647534080, 5767168, 132644864, 0), (18206949376, 5767168, 138412032, 0), (18212716544, 5767168, 144179200, 0), (18201182208, 5767168, 149946368, 0), (17670602752, 5767168, 155713536, 0), (17676369920, 5767168, 161480704, 0), (17664835584, 5767168, 167247872, 0), (17687904256, 5767168, 173015040, 0), (17693671424, 5767168, 178782208, 0), (17682137088, 5767168, 184549376, 0), (18258853888, 5767168, 190316544, 0), (18264621056, 5767168, 196083712, 0), (18253086720, 5767168, 201850880, 0), (17739808768, 5767168, 207618048, 0), (17745575936, 5767168, 213385216, 0), (17734041600, 5767168, 219152384, 0), (17757110272, 5767168, 224919552, 0), (17762877440, 5767168, 230686720, 0), (17751343104, 5767168, 236453888, 0), (18328059904, 5767168, 242221056, 0), (18333827072, 5767168, 247988224, 0), (18322292736, 5767168, 253755392, 0), (18241552384, 5767168, 259522560, 0), (18247319552, 5767168, 265289728, 0), (18235785216, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c239f80>, 2: <capsule object NULL at 0x7a4f2c361f80>}
DEBUG 01-15 16:10:28.814837.814837 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.814435.814435 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.814376.814376 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0aceafb8-9a72-47c2-911e-4e1b1d821ee1
DEBUG 01-15 16:10:28.814867.814867 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.814916.814916 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:28.814751.814751 client.py:127] Model loaded
DEBUG 01-15 16:10:28.814569.814569 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.815164.815164 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.815298.815298 cuda_h.py:19] end restore2model cost 0.0003638267517089844 seconds
DEBUG 01-15 16:10:28.815736.815736 cuda_h.py:19] end sllm_worker_task cost 0.010764598846435547 seconds
INFO 01-15 16:10:28.815133.815133 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0aceafb8-9a72-47c2-911e-4e1b1d821ee1
DEBUG 01-15 16:10:28.815797.815797 cuda_h.py:19] end move_flatidxs cost 0.0008275508880615234 seconds
DEBUG 01-15 16:10:28.815527.815527 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.816418.816418 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003907680511474609 seconds
DEBUG 01-15 16:10:28.816043.816043 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.818548.818548 cuda_h.py:19] end restore2model cost 0.0025832653045654297 seconds
DEBUG 01-15 16:10:28.818868.818868 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006719827651977539 seconds
DEBUG 01-15 16:10:28.818425.818425 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.819786.819786 cuda_h.py:19] end gpu_sexperts cost 0.00026988983154296875 seconds
DEBUG 01-15 16:10:28.819662.819662 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.819008.819008 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:28.819135.819135 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.819314.819314 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.820839.820839 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008127689361572266 seconds
DEBUG 01-15 16:10:28.820113.820113 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.820021.820021 cuda_h.py:19] end gpu_group_list cost 0.0001838207244873047 seconds
DEBUG 01-15 16:10:28.821192.821192 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.820920.820920 cuda_h.py:19] end group_tensors cost 0.004782199859619141 seconds
DEBUG 01-15 16:10:28.821910.821910 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.822109.822109 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012083053588867188 seconds
DEBUG 01-15 16:10:28.822476.822476 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.822123.822123 cuda_h.py:19] end gpu_group_list cost 0.00029206275939941406 seconds
DEBUG 01-15 16:10:28.823114.823114 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.823646.823646 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0aceafb8-9a72-47c2-911e-4e1b1d821ee1
DEBUG 01-15 16:10:28.825579.825579 cuda_h.py:19] end group pad cost 0.0039179325103759766 seconds
DEBUG 01-15 16:10:28.825561.825561 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.843387.843387 client.py:127] Model loaded
DEBUG 01-15 16:10:28.843204.843204 cuda_h.py:19] end wait_experts_multi_device cost 0.01960015296936035 seconds
DEBUG 01-15 16:10:28.843338.843338 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.849865.849865 cuda_h.py:19] end group_einsum cost 0.023418903350830078 seconds
DEBUG 01-15 16:10:28.849552.849552 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.853647.853647 cuda_h.py:19] end get_outputs_cpu1 cost 0.003751993179321289 seconds
DEBUG 01-15 16:10:28.853768.853768 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03883481025695801 seconds
DEBUG 01-15 16:10:28.854505.854505 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010442972183227539 seconds
DEBUG 01-15 16:10:28.854978.854978 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.855898.855898 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.855102.855102 cuda_h.py:19] end index_scatter cost 7.557868957519531e-05 seconds
DEBUG 01-15 16:10:28.855986.855986 cuda_h.py:19] end cpuoutputsdeal cost 0.0015823841094970703 seconds
DEBUG 01-15 16:10:28.855327.855327 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.855036.855036 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.856406.856406 cuda_h.py:19] end gpu_group_tensor cost 0.00013709068298339844 seconds
DEBUG 01-15 16:10:28.856653.856653 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.856048.856048 cuda_h.py:19] end gpu_group_tensor cost 0.00012159347534179688 seconds
DEBUG 01-15 16:10:28.856901.856901 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.857962.857962 cuda_h.py:19] end gpu_group_einsum cost 0.0005888938903808594 seconds
DEBUG 01-15 16:10:28.857867.857867 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.857554.857554 cuda_h.py:19] end gpu_group_einsum cost 0.0004811286926269531 seconds
DEBUG 01-15 16:10:28.857254.857254 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.857794.857794 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.858141.858141 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002090930938720703 seconds
DEBUG 01-15 16:10:28.858910.858910 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.858430.858430 cuda_h.py:19] end concat_expert_out cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:10:28.858889.858889 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.858912.858912 cuda_h.py:19] end index_scatter cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:10:28.858655.858655 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008387565612792969 seconds
DEBUG 01-15 16:10:28.858353.858353 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.858474.858474 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.859837.859837 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001308917999267578 seconds
DEBUG 01-15 16:10:28.859116.859116 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.859947.859947 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:28.859929.859929 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.859184.859184 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:28.859754.859754 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004715919494628906 seconds
DEBUG 01-15 16:10:28.859075.859075 cuda_h.py:19] end gpu_experts_multi_device cost 0.040135860443115234 seconds
DEBUG 01-15 16:10:28.859946.859946 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.05023789405822754 seconds
DEBUG 01-15 16:10:28.859832.859832 cuda_h.py:19] end prefill_layer cost 0.05579018592834473 seconds
DEBUG 01-15 16:10:28.859106.859106 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:10:28.860809.860809 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.860750.860750 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:10:28.860883.860883 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:28.860401.860401 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:28.860635.860635 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 3.910064697265625e-05 seconds
DEBUG 01-15 16:10:28.860914.860914 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:10:28.860994.860994 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.860388.860388 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.860550.860550 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.860215.860215 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.860105.860105 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.861114.861114 cuda_h.py:19] end allocate_cuda_memory cost 0.0004222393035888672 seconds
DEBUG 01-15 16:10:28.861500.861500 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.861170.861170 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.861238.861238 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.861471.861471 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 06644477-b04d-40db-b760-3c1d749663b2
DEBUG 01-15 16:10:28.861289.861289 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.861348.861348 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.862080.862080 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 06644477-b04d-40db-b760-3c1d749663b2
DEBUG 01-15 16:10:28.862161.862161 cuda_h.py:19] end load_into_gpu_async cost 0.00150299072265625 seconds
DEBUG 01-15 16:10:28.862487.862487 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.862842.862842 cuda_h.py:19] end restore_tensors2 cost 8.916854858398438e-05 seconds
DEBUG 01-15 16:10:28.862604.862604 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002290487289428711 seconds
INFO 01-15 16:10:28.862838.862838 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 06644477-b04d-40db-b760-3c1d749663b2
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.865608.865608 cuda_h.py:19] end self_attn cost 0.0037271976470947266 seconds
DEBUG 01-15 16:10:28.865003.865003 cuda_h.py:19] end iln_self_attn_paln cost 0.005554676055908203 seconds
DEBUG 01-15 16:10:28.865455.865455 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:10:28.865695.865695 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.866610.866610 cuda_h.py:19] end gate cost 0.0008141994476318359 seconds
DEBUG 01-15 16:10:28.866824.866824 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.867965.867965 lmp.py:1912] 
DEBUG 01-15 16:10:28.867965.867965 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.867664.867664 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.867287.867287 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.867580.867580 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.867436.867436 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.867013.867013 lmp.py:1917] 
DEBUG 01-15 16:10:28.867013.867013 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.867584.867584 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.867904.867904 lmp.py:1935]   Expert 15 |     61 | CPU
DEBUG 01-15 16:10:28.867991.867991 lmp.py:1935]   Expert 41 |     73 | CPU
DEBUG 01-15 16:10:28.867224.867224 lmp.py:1935]   Expert  0 |     74 | CPU
DEBUG 01-15 16:10:28.867794.867794 lmp.py:1935]   Expert 63 |     76 | CPU
DEBUG 01-15 16:10:28.867206.867206 lmp.py:1935]   Expert 20 |     83 | CPU
DEBUG 01-15 16:10:28.867193.867193 lmp.py:1935]   Expert 45 |     87 | CPU
DEBUG 01-15 16:10:28.867466.867466 lmp.py:1935]   Expert  7 |     91 | CPU
DEBUG 01-15 16:10:28.867592.867592 lmp.py:1935]   Expert 28 |     98 | CPU
DEBUG 01-15 16:10:28.867997.867997 lmp.py:1935]   Expert 54 |    110 | CPU
DEBUG 01-15 16:10:28.867640.867640 lmp.py:1935]   Expert 12 |    111 | CPU
DEBUG 01-15 16:10:28.867568.867568 lmp.py:1935]   Expert 52 |    117 | CPU
DEBUG 01-15 16:10:28.867780.867780 lmp.py:1935]   Expert 40 |    120 | CPU
DEBUG 01-15 16:10:28.867516.867516 lmp.py:1935]   Expert 59 |    123 | CPU
DEBUG 01-15 16:10:28.867728.867728 lmp.py:1935]   Expert  5 |    125 | CPU
DEBUG 01-15 16:10:28.867941.867941 lmp.py:1935]   Expert  4 |    131 | CPU
DEBUG 01-15 16:10:28.867915.867915 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:10:28.867889.867889 lmp.py:1935]   Expert 13 |    135 | CPU
DEBUG 01-15 16:10:28.867863.867863 lmp.py:1935]   Expert 62 |    135 | CPU
DEBUG 01-15 16:10:28.867790.867790 lmp.py:1935]   Expert 21 |    137 | CPU
DEBUG 01-15 16:10:28.867480.867480 lmp.py:1935]   Expert 55 |    137 | CPU
DEBUG 01-15 16:10:28.867169.867169 lmp.py:1935]   Expert 61 |    137 | CPU
DEBUG 01-15 16:10:28.868335.868335 lmp.py:1935]   Expert 42 |    141 | CPU
DEBUG 01-15 16:10:28.868263.868263 lmp.py:1935]   Expert 14 |    146 | CPU
DEBUG 01-15 16:10:28.868714.868714 lmp.py:1935]   Expert 22 |    146 | CPU
DEBUG 01-15 16:10:28.868688.868688 lmp.py:1935]   Expert 10 |    150 | CPU
DEBUG 01-15 16:10:28.868662.868662 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:10:28.868636.868636 lmp.py:1935]   Expert 32 |    159 | CPU
DEBUG 01-15 16:10:28.868610.868610 lmp.py:1935]   Expert 25 |    168 | CPU
DEBUG 01-15 16:10:28.868822.868822 lmp.py:1935]   Expert  1 |    173 | CPU
DEBUG 01-15 16:10:28.868558.868558 lmp.py:1935]   Expert 47 |    174 | CPU
DEBUG 01-15 16:10:28.868770.868770 lmp.py:1935]   Expert 19 |    177 | CPU
DEBUG 01-15 16:10:28.868506.868506 lmp.py:1935]   Expert 50 |    177 | CPU
DEBUG 01-15 16:10:28.868387.868387 lmp.py:1935]   Expert 53 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868746.868746 lmp.py:1935]   Expert 26 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868866.868866 lmp.py:1935]   Expert  2 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868224.868224 lmp.py:1935]   Expert  6 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868582.868582 lmp.py:1935]   Expert 30 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868940.868940 lmp.py:1935]   Expert 11 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868106.868106 lmp.py:1935]   Expert 35 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868272.868272 lmp.py:1935]   Expert 57 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868439.868439 lmp.py:1935]   Expert 56 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868366.868366 lmp.py:1935]   Expert 48 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868486.868486 lmp.py:1935]   Expert 24 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868381.868381 lmp.py:1935]   Expert 44 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868263.868263 lmp.py:1935]   Expert 16 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868667.868667 lmp.py:1935]   Expert 46 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868548.868548 lmp.py:1935]   Expert 39 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868860.868860 lmp.py:1935]   Expert 18 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868219.868219 lmp.py:1935]   Expert 29 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868577.868577 lmp.py:1935]   Expert 37 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868412.868412 lmp.py:1935]   Expert 31 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868532.868532 lmp.py:1935]   Expert  3 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868413.868413 lmp.py:1935]   Expert 36 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868056.868056 lmp.py:1935]   Expert 60 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868414.868414 lmp.py:1935]   Expert 38 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868057.868057 lmp.py:1935]   Expert 17 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868939.868939 lmp.py:1935]   Expert  9 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868820.868820 lmp.py:1935]   Expert 23 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868701.868701 lmp.py:1935]   Expert 27 |    347 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868775.868775 lmp.py:1935]   Expert 43 |    361 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868610.868610 lmp.py:1935]   Expert 33 |    400 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868968.868968 lmp.py:1935]   Expert  8 |    402 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868326.868326 lmp.py:1935]   Expert 58 |    444 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.868208.868208 lmp.py:1935]   Expert 49 |    545 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.868612.868612 lmp.py:1937] 
DEBUG 01-15 16:10:28.868612.868612 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.868017.868017 lmp.py:1938]   CPU:   4060 tokens
DEBUG 01-15 16:10:28.868137.868137 lmp.py:1942]   cuda:1:   4124 tokens (16 experts)
DEBUG 01-15 16:10:28.868780.868780 lmp.py:1942]   cuda:2:   4104 tokens (16 experts)
DEBUG 01-15 16:10:28.868708.868708 lmp.py:1943]   Total GPU:   8228 tokens
DEBUG 01-15 16:10:28.868112.868112 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.868112.868112 lmp.py:1944] 
DEBUG 01-15 16:10:28.868908.868908 cuda_h.py:19] end experts_map_get cost 0.0020678043365478516 seconds
DEBUG 01-15 16:10:28.868526.868526 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.868567.868567 lmp.py:1953] 
DEBUG 01-15 16:10:28.868567.868567 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.869542.869542 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:28.869046.869046 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.869683.869683 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.869641.869641 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.33.gate_proj.weight': 0, 'model.layers.15.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.38.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.38.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.38.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.15.mlp.experts.3.gate_proj.weight': 0, 'model.layers.15.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.26.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.26.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.26.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.17.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.17.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.17.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.58.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.58.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.58.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.60.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.60.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.60.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(18933612544, 5767168, 0, 0), (18939379712, 5767168, 5767168, 0), (18927845376, 5767168, 11534336, 0), (18397265920, 5767168, 17301504, 0), (18403033088, 5767168, 23068672, 0), (18391498752, 5767168, 28835840, 0), (18985517056, 5767168, 34603008, 0), (18991284224, 5767168, 40370176, 0), (18979749888, 5767168, 46137344, 0), (19020120064, 5767168, 51904512, 0), (19025887232, 5767168, 57671680, 0), (19014352896, 5767168, 63438848, 0), (19037421568, 5767168, 69206016, 0), (19043188736, 5767168, 74973184, 0), (19031654400, 5767168, 80740352, 0), (18518376448, 5767168, 86507520, 0), (18524143616, 5767168, 92274688, 0), (18512609280, 5767168, 98041856, 0), (18552979456, 5767168, 103809024, 0), (18558746624, 5767168, 109576192, 0), (18547212288, 5767168, 115343360, 0), (18639486976, 5767168, 121110528, 0), (18645254144, 5767168, 126877696, 0), (18633719808, 5767168, 132644864, 0), (19210436608, 5767168, 138412032, 0), (19216203776, 5767168, 144179200, 0), (19204669440, 5767168, 149946368, 0), (19279642624, 5767168, 155713536, 0), (19285409792, 5767168, 161480704, 0), (19273875456, 5767168, 167247872, 0), (18777899008, 5767168, 173015040, 0), (18783666176, 5767168, 178782208, 0), (18772131840, 5767168, 184549376, 0), (19331547136, 5767168, 190316544, 0), (19337314304, 5767168, 196083712, 0), (19325779968, 5767168, 201850880, 0), (18829803520, 5767168, 207618048, 0), (18835570688, 5767168, 213385216, 0), (18824036352, 5767168, 219152384, 0), (18864406528, 5767168, 224919552, 0), (18870173696, 5767168, 230686720, 0), (18858639360, 5767168, 236453888, 0), (18881708032, 5767168, 242221056, 0), (18887475200, 5767168, 247988224, 0), (18875940864, 5767168, 253755392, 0), (18899009536, 5767168, 259522560, 0), (18904776704, 5767168, 265289728, 0), (18893242368, 5767168, 271056896, 0)], 2: [(18414567424, 5767168, 0, 0), (18420334592, 5767168, 5767168, 0), (18408800256, 5767168, 11534336, 0), (18968215552, 5767168, 17301504, 0), (18973982720, 5767168, 23068672, 0), (18962448384, 5767168, 28835840, 0), (19002818560, 5767168, 34603008, 0), (19008585728, 5767168, 40370176, 0), (18997051392, 5767168, 46137344, 0), (18466471936, 5767168, 51904512, 0), (18472239104, 5767168, 57671680, 0), (18460704768, 5767168, 63438848, 0), (18812502016, 5767168, 69206016, 0), (18818269184, 5767168, 74973184, 0), (18806734848, 5767168, 80740352, 0), (18501074944, 5767168, 86507520, 0), (18506842112, 5767168, 92274688, 0), (18495307776, 5767168, 98041856, 0), (19106627584, 5767168, 103809024, 0), (19112394752, 5767168, 109576192, 0), (19100860416, 5767168, 115343360, 0), (19123929088, 5767168, 121110528, 0), (19129696256, 5767168, 126877696, 0), (19118161920, 5767168, 132644864, 0), (19158532096, 5767168, 138412032, 0), (19164299264, 5767168, 144179200, 0), (19152764928, 5767168, 149946368, 0), (19193135104, 5767168, 155713536, 0), (19198902272, 5767168, 161480704, 0), (19187367936, 5767168, 167247872, 0), (18656788480, 5767168, 173015040, 0), (18662555648, 5767168, 178782208, 0), (18651021312, 5767168, 184549376, 0), (18674089984, 5767168, 190316544, 0), (18679857152, 5767168, 196083712, 0), (18668322816, 5767168, 201850880, 0), (18760597504, 5767168, 207618048, 0), (18766364672, 5767168, 213385216, 0), (18754830336, 5767168, 219152384, 0), (19348848640, 5767168, 224919552, 0), (19354615808, 5767168, 230686720, 0), (19343081472, 5767168, 236453888, 0), (19366150144, 5767168, 242221056, 0), (19371917312, 5767168, 247988224, 0), (19360382976, 5767168, 253755392, 0), (19400753152, 5767168, 259522560, 0), (19406520320, 5767168, 265289728, 0), (19394985984, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e44505f80>, 2: <capsule object NULL at 0x7a51b0645fe0>}
INFO 01-15 16:10:28.870665.870665 client.py:127] Model loaded
DEBUG 01-15 16:10:28.870370.870370 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.870326.870326 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.870103.870103 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.870943.870943 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2ff7b105-3618-49ea-a868-bc9c39ae1b25
DEBUG 01-15 16:10:28.870169.870169 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:28.870665.870665 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.870940.870940 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.871116.871116 cuda_h.py:19] end restore2model cost 0.000732421875 seconds
DEBUG 01-15 16:10:28.871515.871515 cuda_h.py:19] end sllm_worker_task cost 0.010728597640991211 seconds
INFO 01-15 16:10:28.871169.871169 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2ff7b105-3618-49ea-a868-bc9c39ae1b25
DEBUG 01-15 16:10:28.871229.871229 cuda_h.py:19] end move_flatidxs cost 0.000850677490234375 seconds
DEBUG 01-15 16:10:28.871595.871595 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.872825.872825 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030214786529541016 seconds
DEBUG 01-15 16:10:28.872828.872828 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.874916.874916 cuda_h.py:19] end restore2model cost 0.0025916099548339844 seconds
DEBUG 01-15 16:10:28.874904.874904 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005841732025146484 seconds
DEBUG 01-15 16:10:28.874700.874700 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.875704.875704 cuda_h.py:19] end gpu_sexperts cost 0.00028777122497558594 seconds
DEBUG 01-15 16:10:28.875057.875057 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.875595.875595 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7404556274414062e-05 seconds
DEBUG 01-15 16:10:28.875483.875483 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.875663.875663 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.876201.876201 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008220672607421875 seconds
DEBUG 01-15 16:10:28.876812.876812 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.876482.876482 cuda_h.py:19] end gpu_group_list cost 0.00018334388732910156 seconds
DEBUG 01-15 16:10:28.877639.877639 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.878386.878386 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008726119995117188 seconds
DEBUG 01-15 16:10:28.878481.878481 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.877971.877971 cuda_h.py:19] end group_tensors cost 0.0058481693267822266 seconds
DEBUG 01-15 16:10:28.878528.878528 cuda_h.py:19] end gpu_group_list cost 0.00017976760864257812 seconds
DEBUG 01-15 16:10:28.878829.878829 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.879115.879115 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.879026.879026 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2ff7b105-3618-49ea-a868-bc9c39ae1b25
DEBUG 01-15 16:10:28.882518.882518 cuda_h.py:19] end group pad cost 0.003924846649169922 seconds
DEBUG 01-15 16:10:28.882262.882262 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.899020.899020 client.py:127] Model loaded
DEBUG 01-15 16:10:28.899768.899768 cuda_h.py:19] end wait_experts_multi_device cost 0.020365476608276367 seconds
DEBUG 01-15 16:10:28.899982.899982 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.902375.902375 cuda_h.py:19] end group_einsum cost 0.019312381744384766 seconds
DEBUG 01-15 16:10:28.902188.902188 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.906176.906176 cuda_h.py:19] end get_outputs_cpu1 cost 0.003919124603271484 seconds
DEBUG 01-15 16:10:28.906358.906358 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03623795509338379 seconds
DEBUG 01-15 16:10:28.907701.907701 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0071828365325927734 seconds
DEBUG 01-15 16:10:28.907902.907902 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.908064.908064 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.908712.908712 cuda_h.py:19] end index_scatter cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:10:28.909166.909166 cuda_h.py:19] end cpuoutputsdeal cost 0.0017020702362060547 seconds
DEBUG 01-15 16:10:28.909314.909314 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.909646.909646 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.909539.909539 cuda_h.py:19] end gpu_group_tensor cost 0.0001366138458251953 seconds
DEBUG 01-15 16:10:28.909918.909918 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.909413.909413 cuda_h.py:19] end gpu_group_tensor cost 0.00012612342834472656 seconds
DEBUG 01-15 16:10:28.909933.909933 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.910684.910684 cuda_h.py:19] end gpu_group_einsum cost 0.0006089210510253906 seconds
DEBUG 01-15 16:10:28.910357.910357 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.910891.910891 cuda_h.py:19] end gpu_group_einsum cost 0.0004420280456542969 seconds
DEBUG 01-15 16:10:28.911186.911186 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.911368.911368 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.911785.911785 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001666545867919922 seconds
DEBUG 01-15 16:10:28.911495.911495 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.911219.911219 cuda_h.py:19] end concat_expert_out cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:10:28.911394.911394 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.911370.911370 cuda_h.py:19] end index_scatter cost 5.340576171875e-05 seconds
DEBUG 01-15 16:10:28.911451.911451 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007457733154296875 seconds
DEBUG 01-15 16:10:28.911765.911765 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.911509.911509 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.912395.912395 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013017654418945312 seconds
DEBUG 01-15 16:10:28.912005.912005 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.912544.912544 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:28.912096.912096 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.912781.912781 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:28.912657.912657 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004811286926269531 seconds
DEBUG 01-15 16:10:28.912228.912228 cuda_h.py:19] end gpu_experts_multi_device cost 0.0370783805847168 seconds
DEBUG 01-15 16:10:28.912330.912330 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.04668116569519043 seconds
DEBUG 01-15 16:10:28.912337.912337 cuda_h.py:19] end prefill_layer cost 0.05292034149169922 seconds
DEBUG 01-15 16:10:28.913935.913935 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:10:28.913399.913399 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.913817.913817 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:10:28.913712.913712 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:28.913514.913514 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:28.913126.913126 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 3.695487976074219e-05 seconds
DEBUG 01-15 16:10:28.913690.913690 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 6.866455078125e-05 seconds
DEBUG 01-15 16:10:28.913340.913340 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.913302.913302 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.913941.913941 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.913401.913401 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.913575.913575 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.913818.913818 cuda_h.py:19] end allocate_cuda_memory cost 0.00027942657470703125 seconds
DEBUG 01-15 16:10:28.913834.913834 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.913550.913550 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.914764.914764 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.914189.914189 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a459f7ea-d89f-4b9a-8a2a-f9e793fadc8e
DEBUG 01-15 16:10:28.914477.914477 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.914421.914421 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.915074.915074 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a459f7ea-d89f-4b9a-8a2a-f9e793fadc8e
DEBUG 01-15 16:10:28.915156.915156 cuda_h.py:19] end load_into_gpu_async cost 0.0014643669128417969 seconds
DEBUG 01-15 16:10:28.915243.915243 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.915836.915836 cuda_h.py:19] end restore_tensors2 cost 9.036064147949219e-05 seconds
DEBUG 01-15 16:10:28.915599.915599 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021126270294189453 seconds
INFO 01-15 16:10:28.915601.915601 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a459f7ea-d89f-4b9a-8a2a-f9e793fadc8e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.918113.918113 cuda_h.py:19] end self_attn cost 0.0035626888275146484 seconds
DEBUG 01-15 16:10:28.918746.918746 cuda_h.py:19] end iln_self_attn_paln cost 0.005181312561035156 seconds
DEBUG 01-15 16:10:28.918768.918768 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:10:28.918292.918292 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.919399.919399 cuda_h.py:19] end gate cost 0.0007770061492919922 seconds
DEBUG 01-15 16:10:28.919427.919427 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.919868.919868 lmp.py:1912] 
DEBUG 01-15 16:10:28.919868.919868 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.919009.919009 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.919566.919566 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.919593.919593 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.919713.919713 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.919879.919879 lmp.py:1917] 
DEBUG 01-15 16:10:28.919879.919879 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.919237.919237 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.919079.919079 lmp.py:1935]   Expert 58 |     38 | CPU
DEBUG 01-15 16:10:28.919960.919960 lmp.py:1935]   Expert 31 |     59 | CPU
DEBUG 01-15 16:10:28.919888.919888 lmp.py:1935]   Expert 47 |     59 | CPU
DEBUG 01-15 16:10:28.919816.919816 lmp.py:1935]   Expert 49 |     62 | CPU
DEBUG 01-15 16:10:28.919505.919505 lmp.py:1935]   Expert  4 |     65 | CPU
DEBUG 01-15 16:10:28.919194.919194 lmp.py:1935]   Expert 38 |     70 | CPU
DEBUG 01-15 16:10:28.919645.919645 lmp.py:1935]   Expert 45 |     73 | CPU
DEBUG 01-15 16:10:28.920527.920527 lmp.py:1935]   Expert 43 |     82 | CPU
DEBUG 01-15 16:10:28.920454.920454 lmp.py:1935]   Expert 41 |     83 | CPU
DEBUG 01-15 16:10:28.920859.920859 lmp.py:1935]   Expert 33 |     98 | CPU
DEBUG 01-15 16:10:28.920025.920025 lmp.py:1935]   Expert 57 |    102 | CPU
DEBUG 01-15 16:10:28.920589.920589 lmp.py:1935]   Expert 50 |    103 | CPU
DEBUG 01-15 16:10:28.920947.920947 lmp.py:1935]   Expert 11 |    104 | CPU
DEBUG 01-15 16:10:28.920113.920113 lmp.py:1935]   Expert  2 |    115 | CPU
DEBUG 01-15 16:10:28.920279.920279 lmp.py:1935]   Expert 51 |    115 | CPU
DEBUG 01-15 16:10:28.920207.920207 lmp.py:1935]   Expert 14 |    120 | CPU
DEBUG 01-15 16:10:28.920373.920373 lmp.py:1935]   Expert  0 |    123 | CPU
DEBUG 01-15 16:10:28.920301.920301 lmp.py:1935]   Expert 54 |    127 | CPU
DEBUG 01-15 16:10:28.920752.920752 lmp.py:1935]   Expert 56 |    140 | CPU
DEBUG 01-15 16:10:28.920918.920918 lmp.py:1935]   Expert 26 |    142 | CPU
DEBUG 01-15 16:10:28.920607.920607 lmp.py:1935]   Expert 34 |    142 | CPU
DEBUG 01-15 16:10:28.920774.920774 lmp.py:1935]   Expert 27 |    153 | CPU
DEBUG 01-15 16:10:28.920893.920893 lmp.py:1935]   Expert 28 |    158 | CPU
DEBUG 01-15 16:10:28.920298.920298 lmp.py:1935]   Expert 55 |    158 | CPU
DEBUG 01-15 16:10:28.920702.920702 lmp.py:1935]   Expert 10 |    164 | CPU
DEBUG 01-15 16:10:28.920869.920869 lmp.py:1935]   Expert 25 |    165 | CPU
DEBUG 01-15 16:10:28.920512.920512 lmp.py:1935]   Expert  9 |    178 | CPU
DEBUG 01-15 16:10:28.920678.920678 lmp.py:1935]   Expert 13 |    179 | CPU
DEBUG 01-15 16:10:28.920665.920665 lmp.py:1935]   Expert 61 |    187 | CPU
DEBUG 01-15 16:10:28.920070.920070 lmp.py:1935]   Expert  7 |    191 | CPU
DEBUG 01-15 16:10:28.920474.920474 lmp.py:1935]   Expert 48 |    192 | CPU
DEBUG 01-15 16:10:28.920164.920164 lmp.py:1935]   Expert  6 |    194 | CPU
DEBUG 01-15 16:10:28.920237.920237 lmp.py:1935]   Expert 42 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920311.920311 lmp.py:1935]   Expert 46 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920431.920431 lmp.py:1935]   Expert 24 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920742.920742 lmp.py:1935]   Expert 18 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920578.920578 lmp.py:1935]   Expert 40 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920174.920174 lmp.py:1935]   Expert 12 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920009.920009 lmp.py:1935]   Expert 63 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920367.920367 lmp.py:1935]   Expert 29 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920726.920726 lmp.py:1935]   Expert 21 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920845.920845 lmp.py:1935]   Expert 59 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920965.920965 lmp.py:1935]   Expert 22 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920608.920608 lmp.py:1935]   Expert 32 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920251.920251 lmp.py:1935]   Expert 19 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920133.920133 lmp.py:1935]   Expert 36 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920968.920968 lmp.py:1935]   Expert  3 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920803.920803 lmp.py:1935]   Expert 37 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920161.920161 lmp.py:1935]   Expert  1 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920519.920519 lmp.py:1935]   Expert 16 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920116.920116 lmp.py:1935]   Expert 20 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920997.920997 lmp.py:1935]   Expert  5 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920640.920640 lmp.py:1935]   Expert 30 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920522.920522 lmp.py:1935]   Expert  8 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920403.920403 lmp.py:1935]   Expert 15 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920808.920808 lmp.py:1935]   Expert 62 |    272 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920212.920212 lmp.py:1935]   Expert 39 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920855.920855 lmp.py:1935]   Expert 35 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920213.920213 lmp.py:1935]   Expert 17 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920333.920333 lmp.py:1935]   Expert 60 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.920930.920930 lmp.py:1935]   Expert 52 |    355 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.920765.920765 lmp.py:1935]   Expert 23 |    365 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.921646.921646 lmp.py:1935]   Expert 44 |    380 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.921051.921051 lmp.py:1935]   Expert 53 |    437 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.921979.921979 lmp.py:1937] 
DEBUG 01-15 16:10:28.921979.921979 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.921383.921383 lmp.py:1938]   CPU:   3941 tokens
DEBUG 01-15 16:10:28.921265.921265 lmp.py:1942]   cuda:1:   4176 tokens (16 experts)
DEBUG 01-15 16:10:28.921669.921669 lmp.py:1942]   cuda:2:   4171 tokens (16 experts)
DEBUG 01-15 16:10:28.921597.921597 lmp.py:1943]   Total GPU:   8347 tokens
DEBUG 01-15 16:10:28.921001.921001 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.921001.921001 lmp.py:1944] 
DEBUG 01-15 16:10:28.921320.921320 cuda_h.py:19] end experts_map_get cost 0.0017147064208984375 seconds
DEBUG 01-15 16:10:28.921269.921269 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.921787.921787 lmp.py:1953] 
DEBUG 01-15 16:10:28.921787.921787 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.921001.921001 cuda_h.py:19] end cpu_experts_submit cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:10:28.921743.921743 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.921665.921665 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.922514.922514 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.1.gate_proj.weight': 0, 'model.layers.16.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.52.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.52.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.52.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.53.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.53.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.53.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.16.mlp.experts.32.gate_proj.weight': 0, 'model.layers.16.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.29.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(19487260672, 5767168, 0, 0), (19493027840, 5767168, 5767168, 0), (19481493504, 5767168, 11534336, 0), (19521863680, 5767168, 17301504, 0), (19527630848, 5767168, 23068672, 0), (19516096512, 5767168, 28835840, 0), (20144717824, 5767168, 34603008, 0), (20150484992, 5767168, 40370176, 0), (20138950656, 5767168, 46137344, 0), (20162019328, 5767168, 51904512, 0), (20167786496, 5767168, 57671680, 0), (20156252160, 5767168, 63438848, 0), (20196622336, 5767168, 69206016, 0), (20202389504, 5767168, 74973184, 0), (20190855168, 5767168, 80740352, 0), (19764084736, 5767168, 86507520, 0), (19769851904, 5767168, 92274688, 0), (19758317568, 5767168, 98041856, 0), (19798687744, 5767168, 103809024, 0), (19804454912, 5767168, 109576192, 0), (19792920576, 5767168, 115343360, 0), (20369637376, 5767168, 121110528, 0), (20375404544, 5767168, 126877696, 0), (20363870208, 5767168, 132644864, 0), (20386938880, 5767168, 138412032, 0), (20392706048, 5767168, 144179200, 0), (20381171712, 5767168, 149946368, 0), (19815989248, 5767168, 155713536, 0), (19821756416, 5767168, 161480704, 0), (19810222080, 5767168, 167247872, 0), (19989004288, 5767168, 173015040, 0), (19994771456, 5767168, 178782208, 0), (19983237120, 5767168, 184549376, 0), (19850592256, 5767168, 190316544, 0), (19856359424, 5767168, 196083712, 0), (19844825088, 5767168, 201850880, 0), (19885195264, 5767168, 207618048, 0), (19890962432, 5767168, 213385216, 0), (19879428096, 5767168, 219152384, 0), (20490747904, 5767168, 224919552, 0), (20496515072, 5767168, 230686720, 0), (20484980736, 5767168, 236453888, 0), (20542652416, 5767168, 242221056, 0), (20548419584, 5767168, 247988224, 0), (20536885248, 5767168, 253755392, 0), (20559953920, 5767168, 259522560, 0), (20565721088, 5767168, 265289728, 0), (20554186752, 5767168, 271056896, 0)], 2: [(20023607296, 5767168, 0, 0), (20029374464, 5767168, 5767168, 0), (20017840128, 5767168, 11534336, 0), (20075511808, 5767168, 17301504, 0), (20081278976, 5767168, 23068672, 0), (20069744640, 5767168, 28835840, 0), (20092813312, 5767168, 34603008, 0), (20098580480, 5767168, 40370176, 0), (20087046144, 5767168, 46137344, 0), (19556466688, 5767168, 51904512, 0), (19562233856, 5767168, 57671680, 0), (19550699520, 5767168, 63438848, 0), (20110114816, 5767168, 69206016, 0), (20115881984, 5767168, 74973184, 0), (20104347648, 5767168, 80740352, 0), (19608371200, 5767168, 86507520, 0), (19614138368, 5767168, 92274688, 0), (19602604032, 5767168, 98041856, 0), (20231225344, 5767168, 103809024, 0), (20236992512, 5767168, 109576192, 0), (20225458176, 5767168, 115343360, 0), (19677577216, 5767168, 121110528, 0), (19683344384, 5767168, 126877696, 0), (19671810048, 5767168, 132644864, 0), (20265828352, 5767168, 138412032, 0), (20271595520, 5767168, 144179200, 0), (20260061184, 5767168, 149946368, 0), (19729481728, 5767168, 155713536, 0), (19735248896, 5767168, 161480704, 0), (19723714560, 5767168, 167247872, 0), (19746783232, 5767168, 173015040, 0), (19752550400, 5767168, 178782208, 0), (19741016064, 5767168, 184549376, 0), (19781386240, 5767168, 190316544, 0), (19787153408, 5767168, 196083712, 0), (19775619072, 5767168, 201850880, 0), (19833290752, 5767168, 207618048, 0), (19839057920, 5767168, 213385216, 0), (19827523584, 5767168, 219152384, 0), (19867893760, 5767168, 224919552, 0), (19873660928, 5767168, 230686720, 0), (19862126592, 5767168, 236453888, 0), (20508049408, 5767168, 242221056, 0), (20513816576, 5767168, 247988224, 0), (20502282240, 5767168, 253755392, 0), (19971702784, 5767168, 259522560, 0), (19977469952, 5767168, 265289728, 0), (19965935616, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e446e1f80>, 2: <capsule object NULL at 0x7a5b00c99f80>}
DEBUG 01-15 16:10:28.922812.922812 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.922410.922410 sllm_store_c.py:29] call client load into gpu
INFO 01-15 16:10:28.922896.922896 client.py:127] Model loaded
DEBUG 01-15 16:10:28.922217.922217 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.922647.922647 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, dadccc4c-26a2-4fc3-a43f-5c5c42333f51
DEBUG 01-15 16:10:28.922613.922613 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:28.923150.923150 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.923651.923651 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.923699.923699 cuda_h.py:19] end restore2model cost 0.0006988048553466797 seconds
DEBUG 01-15 16:10:28.923621.923621 cuda_h.py:19] end sllm_worker_task cost 0.010144233703613281 seconds
DEBUG 01-15 16:10:28.924153.924153 cuda_h.py:19] end move_flatidxs cost 0.0008554458618164062 seconds
DEBUG 01-15 16:10:28.924611.924611 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:28.924929.924929 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, dadccc4c-26a2-4fc3-a43f-5c5c42333f51
DEBUG 01-15 16:10:28.924479.924479 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0033075809478759766 seconds
DEBUG 01-15 16:10:28.924004.924004 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.927556.927556 cuda_h.py:19] end restore2model cost 0.0025823116302490234 seconds
DEBUG 01-15 16:10:28.927406.927406 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006119489669799805 seconds
DEBUG 01-15 16:10:28.927870.927870 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.927470.927470 cuda_h.py:19] end gpu_sexperts cost 0.0002701282501220703 seconds
DEBUG 01-15 16:10:28.927345.927345 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.927360.927360 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:10:28.927295.927295 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.927236.927236 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.928311.928311 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008320808410644531 seconds
DEBUG 01-15 16:10:28.928830.928830 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.929824.929824 cuda_h.py:19] end gpu_group_list cost 0.0001773834228515625 seconds
DEBUG 01-15 16:10:28.929710.929710 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.930086.930086 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008814334869384766 seconds
DEBUG 01-15 16:10:28.930850.930850 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.931181.931181 cuda_h.py:19] end gpu_group_list cost 0.00017952919006347656 seconds
DEBUG 01-15 16:10:28.931307.931307 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.931282.931282 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, dadccc4c-26a2-4fc3-a43f-5c5c42333f51
DEBUG 01-15 16:10:28.934501.934501 cuda_h.py:19] end group_tensors cost 0.010302066802978516 seconds
DEBUG 01-15 16:10:28.935765.935765 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:28.939287.939287 cuda_h.py:19] end group pad cost 0.003668069839477539 seconds
DEBUG 01-15 16:10:28.939222.939222 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:28.951771.951771 client.py:127] Model loaded
DEBUG 01-15 16:10:28.951307.951307 cuda_h.py:19] end wait_experts_multi_device cost 0.019905567169189453 seconds
DEBUG 01-15 16:10:28.951799.951799 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:28.963610.963610 cuda_h.py:19] end group_einsum cost 0.024304866790771484 seconds
DEBUG 01-15 16:10:28.963114.963114 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:28.967429.967429 cuda_h.py:19] end get_outputs_cpu1 cost 0.004023075103759766 seconds
DEBUG 01-15 16:10:28.968861.968861 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04567313194274902 seconds
DEBUG 01-15 16:10:28.969609.969609 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.017414093017578125 seconds
DEBUG 01-15 16:10:28.969699.969699 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:28.970871.970871 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.970903.970903 cuda_h.py:19] end index_scatter cost 7.987022399902344e-05 seconds
DEBUG 01-15 16:10:28.970847.970847 cuda_h.py:19] end cpuoutputsdeal cost 0.001615285873413086 seconds
DEBUG 01-15 16:10:28.970095.970095 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:28.971235.971235 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.971558.971558 cuda_h.py:19] end gpu_group_tensor cost 0.0001373291015625 seconds
DEBUG 01-15 16:10:28.971175.971175 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:28.971094.971094 cuda_h.py:19] end gpu_group_tensor cost 0.00012159347534179688 seconds
DEBUG 01-15 16:10:28.971137.971137 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.971929.971929 cuda_h.py:19] end gpu_group_einsum cost 0.0004680156707763672 seconds
DEBUG 01-15 16:10:28.972039.972039 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:28.972556.972556 cuda_h.py:19] end gpu_group_einsum cost 0.0005402565002441406 seconds
DEBUG 01-15 16:10:28.972494.972494 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.972703.972703 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.973646.973646 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022649765014648438 seconds
DEBUG 01-15 16:10:28.973177.973177 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.973075.973075 cuda_h.py:19] end concat_expert_out cost 6.175041198730469e-05 seconds
DEBUG 01-15 16:10:28.973441.973441 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.973411.973411 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:28.973922.973922 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008580684661865234 seconds
DEBUG 01-15 16:10:28.973766.973766 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:28.973179.973179 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:28.974303.974303 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001304149627685547 seconds
DEBUG 01-15 16:10:28.974298.974298 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:28.974029.974029 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:28.974680.974680 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:28.974504.974504 cuda_h.py:19] end index_scatter cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:28.974075.974075 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00047135353088378906 seconds
DEBUG 01-15 16:10:28.974931.974931 cuda_h.py:19] end gpu_experts_multi_device cost 0.046478986740112305 seconds
DEBUG 01-15 16:10:28.974079.974079 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.05595684051513672 seconds
DEBUG 01-15 16:10:28.974343.974343 cuda_h.py:19] end prefill_layer cost 0.06179618835449219 seconds
DEBUG 01-15 16:10:28.974571.974571 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:10:28.974989.974989 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:28.974407.974407 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:10:28.975302.975302 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:28.975104.975104 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:28.975000.975000 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 3.62396240234375e-05 seconds
DEBUG 01-15 16:10:28.975518.975518 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 6.866455078125e-05 seconds
DEBUG 01-15 16:10:28.975883.975883 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:28.975800.975800 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:28.975742.975742 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:28.975872.975872 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:28.975523.975523 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:28.975773.975773 cuda_h.py:19] end allocate_cuda_memory cost 0.00028443336486816406 seconds
DEBUG 01-15 16:10:28.975027.975027 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:28.975366.975366 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.975003.975003 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.976475.976475 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6be36a2a-4d8f-4cab-bb42-4747a41e4ee4
DEBUG 01-15 16:10:28.976293.976293 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:28.976826.976826 cuda_h.py:10] start self_attn
INFO 01-15 16:10:28.977048.977048 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6be36a2a-4d8f-4cab-bb42-4747a41e4ee4
DEBUG 01-15 16:10:28.977467.977467 cuda_h.py:19] end load_into_gpu_async cost 0.0018296241760253906 seconds
DEBUG 01-15 16:10:28.977554.977554 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:28.977194.977194 cuda_h.py:19] end restore_tensors2 cost 8.893013000488281e-05 seconds
DEBUG 01-15 16:10:28.977387.977387 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002486705780029297 seconds
INFO 01-15 16:10:28.978760.978760 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6be36a2a-4d8f-4cab-bb42-4747a41e4ee4
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:28.979712.979712 cuda_h.py:19] end self_attn cost 0.0032491683959960938 seconds
DEBUG 01-15 16:10:28.980385.980385 cuda_h.py:19] end iln_self_attn_paln cost 0.004866361618041992 seconds
DEBUG 01-15 16:10:28.980883.980883 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:10:28.980931.980931 cuda_h.py:10] start gate
DEBUG 01-15 16:10:28.980452.980452 cuda_h.py:19] end gate cost 0.0006961822509765625 seconds
DEBUG 01-15 16:10:28.980911.980911 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:28.981314.981314 lmp.py:1912] 
DEBUG 01-15 16:10:28.981314.981314 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:28.981600.981600 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:28.981872.981872 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:28.981092.981092 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:28.981119.981119 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:28.981477.981477 lmp.py:1917] 
DEBUG 01-15 16:10:28.981477.981477 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:28.981835.981835 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:28.981154.981154 lmp.py:1935]   Expert  4 |     10 | CPU
DEBUG 01-15 16:10:28.981466.981466 lmp.py:1935]   Expert 28 |     27 | CPU
DEBUG 01-15 16:10:28.981586.981586 lmp.py:1935]   Expert  7 |     48 | CPU
DEBUG 01-15 16:10:28.981229.981229 lmp.py:1935]   Expert 53 |     58 | CPU
DEBUG 01-15 16:10:28.981110.981110 lmp.py:1935]   Expert 52 |     66 | CPU
DEBUG 01-15 16:10:28.981753.981753 lmp.py:1935]   Expert 43 |     72 | CPU
DEBUG 01-15 16:10:28.981588.981588 lmp.py:1935]   Expert 49 |     84 | CPU
DEBUG 01-15 16:10:28.981469.981469 lmp.py:1935]   Expert 12 |     90 | CPU
DEBUG 01-15 16:10:28.981589.981589 lmp.py:1935]   Expert 33 |    105 | CPU
DEBUG 01-15 16:10:28.981947.981947 lmp.py:1935]   Expert 47 |    105 | CPU
DEBUG 01-15 16:10:28.981590.981590 lmp.py:1935]   Expert 24 |    107 | CPU
DEBUG 01-15 16:10:28.981995.981995 lmp.py:1935]   Expert 50 |    108 | CPU
DEBUG 01-15 16:10:28.981161.981161 lmp.py:1935]   Expert  2 |    110 | CPU
DEBUG 01-15 16:10:28.981566.981566 lmp.py:1935]   Expert 15 |    112 | CPU
DEBUG 01-15 16:10:28.981493.981493 lmp.py:1935]   Expert 60 |    112 | CPU
DEBUG 01-15 16:10:28.981660.981660 lmp.py:1935]   Expert 39 |    115 | CPU
DEBUG 01-15 16:10:28.981826.981826 lmp.py:1935]   Expert 36 |    117 | CPU
DEBUG 01-15 16:10:28.981945.981945 lmp.py:1935]   Expert 25 |    125 | CPU
DEBUG 01-15 16:10:28.981588.981588 lmp.py:1935]   Expert  6 |    126 | CPU
DEBUG 01-15 16:10:28.981391.981391 lmp.py:1935]   Expert 61 |    132 | CPU
DEBUG 01-15 16:10:28.981272.981272 lmp.py:1935]   Expert 59 |    136 | CPU
DEBUG 01-15 16:10:28.981200.981200 lmp.py:1935]   Expert  3 |    141 | CPU
DEBUG 01-15 16:10:28.981889.981889 lmp.py:1935]   Expert 27 |    143 | CPU
DEBUG 01-15 16:10:28.981340.981340 lmp.py:1935]   Expert 58 |    144 | CPU
DEBUG 01-15 16:10:28.981791.981791 lmp.py:1935]   Expert  8 |    149 | CPU
DEBUG 01-15 16:10:28.981480.981480 lmp.py:1935]   Expert 31 |    150 | CPU
DEBUG 01-15 16:10:28.981931.981931 lmp.py:1935]   Expert 30 |    153 | CPU
DEBUG 01-15 16:10:28.981859.981859 lmp.py:1935]   Expert 10 |    157 | CPU
DEBUG 01-15 16:10:28.981025.981025 lmp.py:1935]   Expert 38 |    159 | CPU
DEBUG 01-15 16:10:28.981430.981430 lmp.py:1935]   Expert 40 |    159 | CPU
DEBUG 01-15 16:10:28.982834.982834 lmp.py:1935]   Expert 57 |    159 | CPU
DEBUG 01-15 16:10:28.982762.982762 lmp.py:1935]   Expert 14 |    161 | CPU
DEBUG 01-15 16:10:28.982120.982120 lmp.py:1935]   Expert 37 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982478.982478 lmp.py:1935]   Expert 41 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982598.982598 lmp.py:1935]   Expert 32 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982956.982956 lmp.py:1935]   Expert 46 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982838.982838 lmp.py:1935]   Expert 54 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982719.982719 lmp.py:1935]   Expert 42 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982362.982362 lmp.py:1935]   Expert 19 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982449.982449 lmp.py:1935]   Expert 11 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982569.982569 lmp.py:1935]   Expert 34 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982450.982450 lmp.py:1935]   Expert 18 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982093.982093 lmp.py:1935]   Expert 22 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982259.982259 lmp.py:1935]   Expert 26 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982664.982664 lmp.py:1935]   Expert  0 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982830.982830 lmp.py:1935]   Expert 56 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982235.982235 lmp.py:1935]   Expert  1 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982162.982162 lmp.py:1935]   Expert 44 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982567.982567 lmp.py:1935]   Expert 51 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982972.982972 lmp.py:1935]   Expert 20 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982330.982330 lmp.py:1935]   Expert 29 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982973.982973 lmp.py:1935]   Expert 48 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982616.982616 lmp.py:1935]   Expert 45 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982020.982020 lmp.py:1935]   Expert 21 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982140.982140 lmp.py:1935]   Expert 35 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982545.982545 lmp.py:1935]   Expert 16 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982711.982711 lmp.py:1935]   Expert 55 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982638.982638 lmp.py:1935]   Expert  5 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982566.982566 lmp.py:1935]   Expert 23 |    374 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982256.982256 lmp.py:1935]   Expert 13 |    384 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982422.982422 lmp.py:1935]   Expert 17 |    435 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982541.982541 lmp.py:1935]   Expert 63 |    455 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982423.982423 lmp.py:1935]   Expert  9 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:10:28.982066.982066 lmp.py:1935]   Expert 62 |   1184 | GPU1(cuda:1)
DEBUG 01-15 16:10:28.982994.982994 lmp.py:1937] 
DEBUG 01-15 16:10:28.982994.982994 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:28.982113.982113 lmp.py:1938]   CPU:   3640 tokens
DEBUG 01-15 16:10:28.982518.982518 lmp.py:1942]   cuda:1:   4290 tokens (15 experts)
DEBUG 01-15 16:10:28.982684.982684 lmp.py:1942]   cuda:2:   4358 tokens (17 experts)
DEBUG 01-15 16:10:28.982135.982135 lmp.py:1943]   Total GPU:   8648 tokens
DEBUG 01-15 16:10:28.982347.982347 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:28.982347.982347 lmp.py:1944] 
DEBUG 01-15 16:10:28.982759.982759 cuda_h.py:19] end experts_map_get cost 0.0017523765563964844 seconds
DEBUG 01-15 16:10:28.982609.982609 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:28.982457.982457 lmp.py:1953] 
DEBUG 01-15 16:10:28.982457.982457 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:28.982717.982717 cuda_h.py:19] end cpu_experts_submit cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:28.982003.982003 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:28.982879.982879 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:28.984202.984202 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.5.gate_proj.weight': 0, 'model.layers.17.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.42.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.42.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.42.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.44.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.44.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.44.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.56.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.56.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.56.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.62.up_proj.weight': 253755392}, 2: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.35.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.35.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.35.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.32.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.32.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.32.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.26.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.26.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.26.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.29.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.63.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.63.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.63.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(20663762944, 5767168, 0, 0), (20669530112, 5767168, 5767168, 0), (20657995776, 5767168, 11534336, 0), (21217411072, 5767168, 17301504, 0), (21223178240, 5767168, 23068672, 0), (21211643904, 5767168, 28835840, 0), (21303918592, 5767168, 34603008, 0), (21309685760, 5767168, 40370176, 0), (21298151424, 5767168, 46137344, 0), (20767571968, 5767168, 51904512, 0), (20773339136, 5767168, 57671680, 0), (20761804800, 5767168, 63438848, 0), (21338521600, 5767168, 69206016, 0), (21344288768, 5767168, 74973184, 0), (21332754432, 5767168, 80740352, 0), (20802174976, 5767168, 86507520, 0), (20807942144, 5767168, 92274688, 0), (20796407808, 5767168, 98041856, 0), (21373124608, 5767168, 103809024, 0), (21378891776, 5767168, 109576192, 0), (21367357440, 5767168, 115343360, 0), (21407727616, 5767168, 121110528, 0), (21413494784, 5767168, 126877696, 0), (21401960448, 5767168, 132644864, 0), (20888682496, 5767168, 138412032, 0), (20894449664, 5767168, 144179200, 0), (20882915328, 5767168, 149946368, 0), (20923285504, 5767168, 155713536, 0), (20929052672, 5767168, 161480704, 0), (20917518336, 5767168, 167247872, 0), (20940587008, 5767168, 173015040, 0), (20946354176, 5767168, 178782208, 0), (20934819840, 5767168, 184549376, 0), (20957888512, 5767168, 190316544, 0), (20963655680, 5767168, 196083712, 0), (20952121344, 5767168, 201850880, 0), (21528838144, 5767168, 207618048, 0), (21534605312, 5767168, 213385216, 0), (21523070976, 5767168, 219152384, 0), (21546139648, 5767168, 224919552, 0), (21551906816, 5767168, 230686720, 0), (21540372480, 5767168, 236453888, 0), (21649948672, 5767168, 242221056, 0), (21655715840, 5767168, 247988224, 0), (21644181504, 5767168, 253755392, 0)], 2: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (20594556928, 5767168, 17301504, 0), (20600324096, 5767168, 23068672, 0), (20588789760, 5767168, 28835840, 0), (21165506560, 5767168, 34603008, 0), (21171273728, 5767168, 40370176, 0), (21159739392, 5767168, 46137344, 0), (21182808064, 5767168, 51904512, 0), (21188575232, 5767168, 57671680, 0), (21177040896, 5767168, 63438848, 0), (21130903552, 5767168, 69206016, 0), (21136670720, 5767168, 74973184, 0), (21125136384, 5767168, 80740352, 0), (20732968960, 5767168, 86507520, 0), (20738736128, 5767168, 92274688, 0), (20727201792, 5767168, 98041856, 0), (21286617088, 5767168, 103809024, 0), (21292384256, 5767168, 109576192, 0), (21280849920, 5767168, 115343360, 0), (21355823104, 5767168, 121110528, 0), (21361590272, 5767168, 126877696, 0), (21350055936, 5767168, 132644864, 0), (20854079488, 5767168, 138412032, 0), (20859846656, 5767168, 144179200, 0), (20848312320, 5767168, 149946368, 0), (20871380992, 5767168, 155713536, 0), (20877148160, 5767168, 161480704, 0), (20865613824, 5767168, 167247872, 0), (21459632128, 5767168, 173015040, 0), (21465399296, 5767168, 178782208, 0), (21453864960, 5767168, 184549376, 0), (20905984000, 5767168, 190316544, 0), (20911751168, 5767168, 196083712, 0), (20900216832, 5767168, 201850880, 0), (21511536640, 5767168, 207618048, 0), (21517303808, 5767168, 213385216, 0), (21505769472, 5767168, 219152384, 0), (20975190016, 5767168, 224919552, 0), (20980957184, 5767168, 230686720, 0), (20969422848, 5767168, 236453888, 0), (21027094528, 5767168, 242221056, 0), (21032861696, 5767168, 247988224, 0), (21021327360, 5767168, 253755392, 0), (21078999040, 5767168, 259522560, 0), (21084766208, 5767168, 265289728, 0), (21073231872, 5767168, 271056896, 0), (21667250176, 5767168, 276824064, 0), (21673017344, 5767168, 282591232, 0), (21661483008, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4691f80>, 2: <capsule object NULL at 0x7a51b0646400>}
DEBUG 01-15 16:10:28.984843.984843 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:28.984249.984249 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:28.984429.984429 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b29d3d8f-99fb-4519-b72b-3e4f6c0d4a11
DEBUG 01-15 16:10:28.985615.985615 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:28.985809.985809 client.py:127] Model loaded
DEBUG 01-15 16:10:28.985282.985282 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:28.985197.985197 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.985354.985354 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:28.985747.985747 cuda_h.py:19] end restore2model cost 0.0003719329833984375 seconds
DEBUG 01-15 16:10:28.985185.985185 cuda_h.py:19] end sllm_worker_task cost 0.010352134704589844 seconds
INFO 01-15 16:10:28.986395.986395 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b29d3d8f-99fb-4519-b72b-3e4f6c0d4a11
DEBUG 01-15 16:10:28.986992.986992 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0035271644592285156 seconds
DEBUG 01-15 16:10:28.986848.986848 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:28.986961.986961 cuda_h.py:19] end move_flatidxs cost 0.0008740425109863281 seconds
DEBUG 01-15 16:10:28.986168.986168 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:28.989265.989265 cuda_h.py:19] end restore2model cost 0.0025200843811035156 seconds
DEBUG 01-15 16:10:28.989201.989201 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006268024444580078 seconds
DEBUG 01-15 16:10:28.989188.989188 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:28.989311.989311 cuda_h.py:19] end gpu_sexperts cost 0.00026917457580566406 seconds
DEBUG 01-15 16:10:28.989948.989948 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:28.989963.989963 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:10:28.989374.989374 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:28.989508.989508 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.990555.990555 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007760524749755859 seconds
DEBUG 01-15 16:10:28.990881.990881 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.990517.990517 cuda_h.py:19] end gpu_group_list cost 0.00015854835510253906 seconds
DEBUG 01-15 16:10:28.991879.991879 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:28.992131.992131 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009288787841796875 seconds
DEBUG 01-15 16:10:28.992365.992365 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:28.992319.992319 cuda_h.py:19] end gpu_group_list cost 0.00018143653869628906 seconds
DEBUG 01-15 16:10:28.993730.993730 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:28.993805.993805 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b29d3d8f-99fb-4519-b72b-3e4f6c0d4a11
DEBUG 01-15 16:10:28.995382.995382 cuda_h.py:19] end group_tensors cost 0.009318113327026367 seconds
DEBUG 01-15 16:10:28.996613.996613 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.000008.000008 cuda_h.py:19] end group pad cost 0.004030942916870117 seconds
DEBUG 01-15 16:10:29.000229.000229 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.016308.016308 client.py:127] Model loaded
DEBUG 01-15 16:10:29.016774.016774 cuda_h.py:19] end wait_experts_multi_device cost 0.023336172103881836 seconds
DEBUG 01-15 16:10:29.016156.016156 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.021392.021392 cuda_h.py:19] end group_einsum cost 0.020824193954467773 seconds
DEBUG 01-15 16:10:29.021165.021165 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.025121.025121 cuda_h.py:19] end get_outputs_cpu1 cost 0.0035533905029296875 seconds
DEBUG 01-15 16:10:29.026678.026678 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04085183143615723 seconds
DEBUG 01-15 16:10:29.026183.026183 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00961923599243164 seconds
DEBUG 01-15 16:10:29.026941.026941 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.027943.027943 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.028021.028021 cuda_h.py:19] end index_scatter cost 7.62939453125e-05 seconds
DEBUG 01-15 16:10:29.028430.028430 cuda_h.py:19] end cpuoutputsdeal cost 0.001688241958618164 seconds
DEBUG 01-15 16:10:29.028353.028353 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.028685.028685 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.028539.028539 cuda_h.py:19] end gpu_group_tensor cost 0.0001418590545654297 seconds
DEBUG 01-15 16:10:29.028110.028110 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.028255.028255 cuda_h.py:19] end gpu_group_tensor cost 0.00014519691467285156 seconds
DEBUG 01-15 16:10:29.029119.029119 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.030608.030608 cuda_h.py:19] end gpu_group_einsum cost 0.0012919902801513672 seconds
DEBUG 01-15 16:10:29.030937.030937 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.031701.031701 cuda_h.py:19] end gpu_group_einsum cost 0.0003943443298339844 seconds
DEBUG 01-15 16:10:29.031089.031089 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.031986.031986 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.031609.031609 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017690658569335938 seconds
DEBUG 01-15 16:10:29.031034.031034 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.031626.031626 cuda_h.py:19] end concat_expert_out cost 4.6253204345703125e-05 seconds
DEBUG 01-15 16:10:29.031469.031469 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.031698.031698 cuda_h.py:19] end index_scatter cost 6.437301635742188e-05 seconds
DEBUG 01-15 16:10:29.031017.031017 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007786750793457031 seconds
DEBUG 01-15 16:10:29.032331.032331 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.032598.032598 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.032755.032755 cuda_h.py:19] end all_expert_outputs_slices cost 0.00011968612670898438 seconds
DEBUG 01-15 16:10:29.032558.032558 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.032766.032766 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:29.032530.032530 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.032480.032480 cuda_h.py:19] end index_scatter cost 6.771087646484375e-05 seconds
DEBUG 01-15 16:10:29.032673.032673 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004971027374267578 seconds
DEBUG 01-15 16:10:29.032914.032914 cuda_h.py:19] end gpu_experts_multi_device cost 0.04300069808959961 seconds
DEBUG 01-15 16:10:29.032208.032208 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.052596330642700195 seconds
DEBUG 01-15 16:10:29.033188.033188 cuda_h.py:19] end prefill_layer cost 0.0581212043762207 seconds
DEBUG 01-15 16:10:29.033938.033938 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:10:29.033833.033833 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.033774.033774 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:10:29.033146.033146 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:29.033902.033902 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:29.033275.033275 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 3.62396240234375e-05 seconds
DEBUG 01-15 16:10:29.033462.033462 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:10:29.033019.033019 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.033604.033604 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.033984.033984 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:29.033637.033637 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:29.033050.033050 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:29.033012.033012 cuda_h.py:19] end allocate_cuda_memory cost 0.000213623046875 seconds
DEBUG 01-15 16:10:29.034558.034558 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:29.034943.034943 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.034058.034058 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.034529.034529 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d02b38ae-247b-4bfd-b36e-4d76da95a726
DEBUG 01-15 16:10:29.034725.034725 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.034237.034237 cuda_h.py:10] start self_attn
INFO 01-15 16:10:29.035012.035012 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d02b38ae-247b-4bfd-b36e-4d76da95a726
DEBUG 01-15 16:10:29.035047.035047 cuda_h.py:19] end load_into_gpu_async cost 0.0015149116516113281 seconds
DEBUG 01-15 16:10:29.035849.035849 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:29.035582.035582 cuda_h.py:19] end restore_tensors2 cost 8.654594421386719e-05 seconds
DEBUG 01-15 16:10:29.035345.035345 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002101421356201172 seconds
INFO 01-15 16:10:29.035724.035724 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d02b38ae-247b-4bfd-b36e-4d76da95a726
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.038253.038253 cuda_h.py:19] end self_attn cost 0.003381490707397461 seconds
DEBUG 01-15 16:10:29.038734.038734 cuda_h.py:19] end iln_self_attn_paln cost 0.00491642951965332 seconds
DEBUG 01-15 16:10:29.038994.038994 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:10:29.038472.038472 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.039814.039814 cuda_h.py:19] end gate cost 0.0006701946258544922 seconds
DEBUG 01-15 16:10:29.039743.039743 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.039117.039117 lmp.py:1912] 
DEBUG 01-15 16:10:29.039117.039117 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.039258.039258 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.039292.039292 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.039796.039796 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.039631.039631 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.039750.039750 lmp.py:1917] 
DEBUG 01-15 16:10:29.039750.039750 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.039030.039030 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.039395.039395 lmp.py:1935]   Expert 32 |     32 | CPU
DEBUG 01-15 16:10:29.039514.039514 lmp.py:1935]   Expert  5 |     51 | CPU
DEBUG 01-15 16:10:29.039442.039442 lmp.py:1935]   Expert 30 |     54 | CPU
DEBUG 01-15 16:10:29.039608.039608 lmp.py:1935]   Expert 46 |     73 | CPU
DEBUG 01-15 16:10:29.039013.039013 lmp.py:1935]   Expert  8 |     89 | CPU
DEBUG 01-15 16:10:29.039848.039848 lmp.py:1935]   Expert 40 |     91 | CPU
DEBUG 01-15 16:10:29.039968.039968 lmp.py:1935]   Expert 12 |    100 | CPU
DEBUG 01-15 16:10:29.039849.039849 lmp.py:1935]   Expert 17 |    108 | CPU
DEBUG 01-15 16:10:29.039492.039492 lmp.py:1935]   Expert 27 |    111 | CPU
DEBUG 01-15 16:10:29.039135.039135 lmp.py:1935]   Expert  3 |    113 | CPU
DEBUG 01-15 16:10:29.039540.039540 lmp.py:1935]   Expert 60 |    114 | CPU
DEBUG 01-15 16:10:29.039706.039706 lmp.py:1935]   Expert 58 |    115 | CPU
DEBUG 01-15 16:10:29.039633.039633 lmp.py:1935]   Expert 28 |    120 | CPU
DEBUG 01-15 16:10:29.039561.039561 lmp.py:1935]   Expert 21 |    121 | CPU
DEBUG 01-15 16:10:29.039727.039727 lmp.py:1935]   Expert 29 |    121 | CPU
DEBUG 01-15 16:10:29.039370.039370 lmp.py:1935]   Expert 25 |    123 | CPU
DEBUG 01-15 16:10:29.039490.039490 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:10:29.039895.039895 lmp.py:1935]   Expert 35 |    133 | CPU
DEBUG 01-15 16:10:29.040776.040776 lmp.py:1935]   Expert 19 |    136 | CPU
DEBUG 01-15 16:10:29.040134.040134 lmp.py:1935]   Expert  0 |    144 | CPU
DEBUG 01-15 16:10:29.040539.040539 lmp.py:1935]   Expert 52 |    144 | CPU
DEBUG 01-15 16:10:29.040420.040420 lmp.py:1935]   Expert  6 |    145 | CPU
DEBUG 01-15 16:10:29.040871.040871 lmp.py:1935]   Expert 56 |    148 | CPU
DEBUG 01-15 16:10:29.040560.040560 lmp.py:1935]   Expert 54 |    150 | CPU
DEBUG 01-15 16:10:29.040250.040250 lmp.py:1935]   Expert 37 |    156 | CPU
DEBUG 01-15 16:10:29.040939.040939 lmp.py:1935]   Expert 63 |    157 | CPU
DEBUG 01-15 16:10:29.040628.040628 lmp.py:1935]   Expert 53 |    158 | CPU
DEBUG 01-15 16:10:29.040079.040079 lmp.py:1935]   Expert 48 |    161 | CPU
DEBUG 01-15 16:10:29.040768.040768 lmp.py:1935]   Expert 36 |    163 | CPU
DEBUG 01-15 16:10:29.040458.040458 lmp.py:1935]   Expert 59 |    168 | CPU
DEBUG 01-15 16:10:29.040101.040101 lmp.py:1935]   Expert  9 |    182 | CPU
DEBUG 01-15 16:10:29.040982.040982 lmp.py:1935]   Expert  1 |    186 | CPU
DEBUG 01-15 16:10:29.040294.040294 lmp.py:1935]   Expert 39 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040891.040891 lmp.py:1935]   Expert 20 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040534.040534 lmp.py:1935]   Expert 61 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040177.040177 lmp.py:1935]   Expert  7 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040820.040820 lmp.py:1935]   Expert 11 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040701.040701 lmp.py:1935]   Expert 42 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040629.040629 lmp.py:1935]   Expert 34 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040510.040510 lmp.py:1935]   Expert 43 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040021.040021 lmp.py:1935]   Expert 47 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040379.040379 lmp.py:1935]   Expert 55 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040976.040976 lmp.py:1935]   Expert 16 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040096.040096 lmp.py:1935]   Expert 13 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040931.040931 lmp.py:1935]   Expert 57 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040527.040527 lmp.py:1935]   Expert 18 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040409.040409 lmp.py:1935]   Expert 15 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040529.040529 lmp.py:1935]   Expert  4 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040410.040410 lmp.py:1935]   Expert 22 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040053.040053 lmp.py:1935]   Expert 33 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040696.040696 lmp.py:1935]   Expert 50 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040577.040577 lmp.py:1935]   Expert 45 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040459.040459 lmp.py:1935]   Expert 31 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040863.040863 lmp.py:1935]   Expert 51 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040029.040029 lmp.py:1935]   Expert 49 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040672.040672 lmp.py:1935]   Expert 38 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040554.040554 lmp.py:1935]   Expert 26 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040435.040435 lmp.py:1935]   Expert 10 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040032.040032 lmp.py:1935]   Expert 44 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040867.040867 lmp.py:1935]   Expert  2 |    298 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040225.040225 lmp.py:1935]   Expert 24 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040583.040583 lmp.py:1935]   Expert 14 |    312 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040226.040226 lmp.py:1935]   Expert 23 |    403 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.040631.040631 lmp.py:1935]   Expert 62 |    672 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.040320.040320 lmp.py:1937] 
DEBUG 01-15 16:10:29.040320.040320 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.040725.040725 lmp.py:1938]   CPU:   3997 tokens
DEBUG 01-15 16:10:29.040129.040129 lmp.py:1942]   cuda:1:   4050 tokens (15 experts)
DEBUG 01-15 16:10:29.040534.040534 lmp.py:1942]   cuda:2:   4241 tokens (17 experts)
DEBUG 01-15 16:10:29.040223.040223 lmp.py:1943]   Total GPU:   8291 tokens
DEBUG 01-15 16:10:29.040674.040674 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.040674.040674 lmp.py:1944] 
DEBUG 01-15 16:10:29.040277.040277 cuda_h.py:19] end experts_map_get cost 0.001699209213256836 seconds
DEBUG 01-15 16:10:29.041035.041035 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.041075.041075 lmp.py:1953] 
DEBUG 01-15 16:10:29.041075.041075 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.041382.041382 cuda_h.py:19] end cpu_experts_submit cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:29.041170.041170 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.041616.041616 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.042342.042342 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.33.gate_proj.weight': 0, 'model.layers.18.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.42.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.42.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.42.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.43.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.43.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.43.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.62.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.62.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.62.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.31.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.31.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.31.up_proj.weight': 253755392}, 2: {'model.layers.18.mlp.experts.2.gate_proj.weight': 0, 'model.layers.18.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.38.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.38.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.38.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.39.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.39.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.39.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.10.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.10.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.10.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.47.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.47.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.47.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.22.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.22.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.22.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.23.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.23.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.23.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.57.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.61.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.61.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.61.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(22255501312, 5767168, 0, 0), (22261268480, 5767168, 5767168, 0), (22249734144, 5767168, 11534336, 0), (22411214848, 5767168, 17301504, 0), (22416982016, 5767168, 23068672, 0), (22405447680, 5767168, 28835840, 0), (22428516352, 5767168, 34603008, 0), (22434283520, 5767168, 40370176, 0), (22422749184, 5767168, 46137344, 0), (22445817856, 5767168, 51904512, 0), (22451585024, 5767168, 57671680, 0), (22440050688, 5767168, 63438848, 0), (21909471232, 5767168, 69206016, 0), (21915238400, 5767168, 74973184, 0), (21903704064, 5767168, 80740352, 0), (21874868224, 5767168, 86507520, 0), (21880635392, 5767168, 92274688, 0), (21869101056, 5767168, 98041856, 0), (21944074240, 5767168, 103809024, 0), (21949841408, 5767168, 109576192, 0), (21938307072, 5767168, 115343360, 0), (21961375744, 5767168, 121110528, 0), (21967142912, 5767168, 126877696, 0), (21955608576, 5767168, 132644864, 0), (22532325376, 5767168, 138412032, 0), (22538092544, 5767168, 144179200, 0), (22526558208, 5767168, 149946368, 0), (22549626880, 5767168, 155713536, 0), (22555394048, 5767168, 161480704, 0), (22543859712, 5767168, 167247872, 0), (22030581760, 5767168, 173015040, 0), (22036348928, 5767168, 178782208, 0), (22024814592, 5767168, 184549376, 0), (22099787776, 5767168, 190316544, 0), (22105554944, 5767168, 196083712, 0), (22094020608, 5767168, 201850880, 0), (22134390784, 5767168, 207618048, 0), (22140157952, 5767168, 213385216, 0), (22128623616, 5767168, 219152384, 0), (22757244928, 5767168, 224919552, 0), (22763012096, 5767168, 230686720, 0), (22751477760, 5767168, 236453888, 0), (22220898304, 5767168, 242221056, 0), (22226665472, 5767168, 247988224, 0), (22215131136, 5767168, 253755392, 0)], 2: [(21719154688, 5767168, 0, 0), (21724921856, 5767168, 5767168, 0), (21713387520, 5767168, 11534336, 0), (22272802816, 5767168, 17301504, 0), (22278569984, 5767168, 23068672, 0), (22267035648, 5767168, 28835840, 0), (21753757696, 5767168, 34603008, 0), (21759524864, 5767168, 40370176, 0), (21747990528, 5767168, 46137344, 0), (22342008832, 5767168, 51904512, 0), (22347776000, 5767168, 57671680, 0), (22336241664, 5767168, 63438848, 0), (21805662208, 5767168, 69206016, 0), (21811429376, 5767168, 74973184, 0), (21799895040, 5767168, 80740352, 0), (22359310336, 5767168, 86507520, 0), (22365077504, 5767168, 92274688, 0), (22353543168, 5767168, 98041856, 0), (21857566720, 5767168, 103809024, 0), (21863333888, 5767168, 109576192, 0), (21851799552, 5767168, 115343360, 0), (22463119360, 5767168, 121110528, 0), (22468886528, 5767168, 126877696, 0), (22457352192, 5767168, 132644864, 0), (21926772736, 5767168, 138412032, 0), (21932539904, 5767168, 144179200, 0), (21921005568, 5767168, 149946368, 0), (22497722368, 5767168, 155713536, 0), (22503489536, 5767168, 161480704, 0), (22491955200, 5767168, 167247872, 0), (21995978752, 5767168, 173015040, 0), (22001745920, 5767168, 178782208, 0), (21990211584, 5767168, 184549376, 0), (22566928384, 5767168, 190316544, 0), (22572695552, 5767168, 196083712, 0), (22561161216, 5767168, 201850880, 0), (22636134400, 5767168, 207618048, 0), (22641901568, 5767168, 213385216, 0), (22630367232, 5767168, 219152384, 0), (22065184768, 5767168, 224919552, 0), (22070951936, 5767168, 230686720, 0), (22059417600, 5767168, 236453888, 0), (22082486272, 5767168, 242221056, 0), (22088253440, 5767168, 247988224, 0), (22076719104, 5767168, 253755392, 0), (22670737408, 5767168, 259522560, 0), (22676504576, 5767168, 265289728, 0), (22664970240, 5767168, 271056896, 0), (22739943424, 5767168, 276824064, 0), (22745710592, 5767168, 282591232, 0), (22734176256, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b0646640>, 2: <capsule object NULL at 0x7a51b0646130>}
DEBUG 01-15 16:10:29.042101.042101 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.042805.042805 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.042892.042892 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bb5ffb7d-f614-46ed-bb27-0713b7bcee42
DEBUG 01-15 16:10:29.042495.042495 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:29.043967.043967 client.py:127] Model loaded
DEBUG 01-15 16:10:29.043758.043758 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.043717.043717 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:29.043316.043316 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:29.044104.044104 cuda_h.py:19] end restore2model cost 0.0005807876586914062 seconds
INFO 01-15 16:10:29.044883.044883 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bb5ffb7d-f614-46ed-bb27-0713b7bcee42
DEBUG 01-15 16:10:29.044925.044925 cuda_h.py:19] end sllm_worker_task cost 0.010575532913208008 seconds
DEBUG 01-15 16:10:29.044376.044376 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034761428833007812 seconds
DEBUG 01-15 16:10:29.044300.044300 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.044281.044281 cuda_h.py:19] end move_flatidxs cost 0.0008563995361328125 seconds
DEBUG 01-15 16:10:29.044025.044025 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:29.047485.047485 cuda_h.py:19] end restore2model cost 0.0025243759155273438 seconds
DEBUG 01-15 16:10:29.047613.047613 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006273031234741211 seconds
DEBUG 01-15 16:10:29.047170.047170 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.047485.047485 cuda_h.py:19] end gpu_sexperts cost 0.00027060508728027344 seconds
DEBUG 01-15 16:10:29.047745.047745 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:29.047568.047568 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.430511474609375e-05 seconds
DEBUG 01-15 16:10:29.047264.047264 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.047682.047682 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.048789.048789 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007846355438232422 seconds
DEBUG 01-15 16:10:29.048731.048731 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.048619.048619 cuda_h.py:19] end gpu_group_list cost 0.00016927719116210938 seconds
DEBUG 01-15 16:10:29.049577.049577 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.050948.050948 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009140968322753906 seconds
DEBUG 01-15 16:10:29.050413.050413 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.050672.050672 cuda_h.py:19] end gpu_group_list cost 0.00019669532775878906 seconds
DEBUG 01-15 16:10:29.050993.050993 cuda_h.py:19] end group_tensors cost 0.005932331085205078 seconds
DEBUG 01-15 16:10:29.051984.051984 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.051198.051198 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bb5ffb7d-f614-46ed-bb27-0713b7bcee42
DEBUG 01-15 16:10:29.051479.051479 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.057179.057179 cuda_h.py:19] end group pad cost 0.005442142486572266 seconds
DEBUG 01-15 16:10:29.057592.057592 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.074149.074149 client.py:127] Model loaded
DEBUG 01-15 16:10:29.075462.075462 cuda_h.py:19] end wait_experts_multi_device cost 0.02347588539123535 seconds
DEBUG 01-15 16:10:29.075233.075233 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.077453.077453 cuda_h.py:19] end group_einsum cost 0.02010178565979004 seconds
DEBUG 01-15 16:10:29.077366.077366 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.082247.082247 cuda_h.py:19] end get_outputs_cpu1 cost 0.004682302474975586 seconds
DEBUG 01-15 16:10:29.082976.082976 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03930807113647461 seconds
DEBUG 01-15 16:10:29.083832.083832 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008147954940795898 seconds
DEBUG 01-15 16:10:29.083695.083695 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.084046.084046 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.084720.084720 cuda_h.py:19] end index_scatter cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:10:29.085088.085088 cuda_h.py:19] end cpuoutputsdeal cost 0.0015854835510253906 seconds
DEBUG 01-15 16:10:29.085097.085097 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.085667.085667 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.085229.085229 cuda_h.py:19] end gpu_group_tensor cost 0.00013709068298339844 seconds
DEBUG 01-15 16:10:29.085085.085085 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.085434.085434 cuda_h.py:19] end gpu_group_tensor cost 0.0001232624053955078 seconds
DEBUG 01-15 16:10:29.085716.085716 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.086817.086817 cuda_h.py:19] end gpu_group_einsum cost 0.0005881786346435547 seconds
DEBUG 01-15 16:10:29.086431.086431 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.086594.086594 cuda_h.py:19] end gpu_group_einsum cost 0.0004837512969970703 seconds
DEBUG 01-15 16:10:29.087195.087195 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.087642.087642 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.087088.087088 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021004676818847656 seconds
DEBUG 01-15 16:10:29.087427.087427 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.087709.087709 cuda_h.py:19] end concat_expert_out cost 6.437301635742188e-05 seconds
DEBUG 01-15 16:10:29.087644.087644 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.087521.087521 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:29.088026.088026 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008397102355957031 seconds
DEBUG 01-15 16:10:29.088102.088102 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.088276.088276 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.088149.088149 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012087821960449219 seconds
DEBUG 01-15 16:10:29.088282.088282 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.088133.088133 cuda_h.py:19] end concat_expert_out cost 6.794929504394531e-05 seconds
DEBUG 01-15 16:10:29.088420.088420 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.088251.088251 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:29.088822.088822 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004954338073730469 seconds
DEBUG 01-15 16:10:29.088109.088109 cuda_h.py:19] end gpu_experts_multi_device cost 0.040838003158569336 seconds
DEBUG 01-15 16:10:29.088018.088018 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.05033540725708008 seconds
DEBUG 01-15 16:10:29.089488.089488 cuda_h.py:19] end prefill_layer cost 0.05592942237854004 seconds
DEBUG 01-15 16:10:29.089378.089378 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:10:29.089557.089557 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.089737.089737 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:10:29.089109.089109 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:29.089911.089911 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:29.089192.089192 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:10:29.089325.089325 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:10:29.089259.089259 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.089792.089792 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.089703.089703 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:29.089146.089146 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:29.089303.089303 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:29.090937.090937 cuda_h.py:19] end allocate_cuda_memory cost 0.00040984153747558594 seconds
DEBUG 01-15 16:10:29.090228.090228 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:29.090457.090457 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.090786.090786 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.091723.091723 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8f4ccb7d-e101-439a-86fa-c21fe87cc154
DEBUG 01-15 16:10:29.091908.091908 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.091938.091938 cuda_h.py:10] start self_attn
INFO 01-15 16:10:29.092774.092774 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8f4ccb7d-e101-439a-86fa-c21fe87cc154
DEBUG 01-15 16:10:29.092746.092746 cuda_h.py:19] end load_into_gpu_async cost 0.0021047592163085938 seconds
DEBUG 01-15 16:10:29.092822.092822 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:29.093062.093062 cuda_h.py:19] end restore_tensors2 cost 0.000148773193359375 seconds
DEBUG 01-15 16:10:29.093350.093350 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0034096240997314453 seconds
INFO 01-15 16:10:29.093979.093979 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8f4ccb7d-e101-439a-86fa-c21fe87cc154
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.095266.095266 cuda_h.py:19] end self_attn cost 0.004235267639160156 seconds
DEBUG 01-15 16:10:29.096859.096859 cuda_h.py:19] end iln_self_attn_paln cost 0.006588935852050781 seconds
DEBUG 01-15 16:10:29.096595.096595 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:10:29.096451.096451 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.096802.096802 cuda_h.py:19] end gate cost 0.0007503032684326172 seconds
DEBUG 01-15 16:10:29.096777.096777 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.097105.097105 lmp.py:1912] 
DEBUG 01-15 16:10:29.097105.097105 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.097007.097007 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.097610.097610 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.097876.097876 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.097950.097950 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.097831.097831 lmp.py:1917] 
DEBUG 01-15 16:10:29.097831.097831 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.097189.097189 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.097508.097508 lmp.py:1935]   Expert 44 |     37 | CPU
DEBUG 01-15 16:10:29.097343.097343 lmp.py:1935]   Expert  1 |     46 | CPU
DEBUG 01-15 16:10:29.097986.097986 lmp.py:1935]   Expert 60 |     62 | CPU
DEBUG 01-15 16:10:29.097867.097867 lmp.py:1935]   Expert 28 |     70 | CPU
DEBUG 01-15 16:10:29.097749.097749 lmp.py:1935]   Expert 48 |     80 | CPU
DEBUG 01-15 16:10:29.097868.097868 lmp.py:1935]   Expert 27 |     84 | CPU
DEBUG 01-15 16:10:29.097227.097227 lmp.py:1935]   Expert  0 |     98 | CPU
DEBUG 01-15 16:10:29.097062.097062 lmp.py:1935]   Expert 62 |    106 | CPU
DEBUG 01-15 16:10:29.097897.097897 lmp.py:1935]   Expert 42 |    112 | CPU
DEBUG 01-15 16:10:29.097805.097805 lmp.py:1935]   Expert 22 |    113 | CPU
DEBUG 01-15 16:10:29.097972.097972 lmp.py:1935]   Expert 30 |    114 | CPU
DEBUG 01-15 16:10:29.097899.097899 lmp.py:1935]   Expert 59 |    116 | CPU
DEBUG 01-15 16:10:29.097827.097827 lmp.py:1935]   Expert 58 |    119 | CPU
DEBUG 01-15 16:10:29.097232.097232 lmp.py:1935]   Expert 16 |    124 | CPU
DEBUG 01-15 16:10:29.097682.097682 lmp.py:1935]   Expert  8 |    130 | CPU
DEBUG 01-15 16:10:29.097610.097610 lmp.py:1935]   Expert 12 |    130 | CPU
DEBUG 01-15 16:10:29.097015.097015 lmp.py:1935]   Expert 50 |    134 | CPU
DEBUG 01-15 16:10:29.097135.097135 lmp.py:1935]   Expert  5 |    143 | CPU
DEBUG 01-15 16:10:29.097777.097777 lmp.py:1935]   Expert 56 |    145 | CPU
DEBUG 01-15 16:10:29.097897.097897 lmp.py:1935]   Expert 15 |    151 | CPU
DEBUG 01-15 16:10:29.097825.097825 lmp.py:1935]   Expert 55 |    152 | CPU
DEBUG 01-15 16:10:29.097276.097276 lmp.py:1935]   Expert 57 |    152 | CPU
DEBUG 01-15 16:10:29.097204.097204 lmp.py:1935]   Expert 26 |    154 | CPU
DEBUG 01-15 16:10:29.097370.097370 lmp.py:1935]   Expert 32 |    157 | CPU
DEBUG 01-15 16:10:29.097297.097297 lmp.py:1935]   Expert 47 |    157 | CPU
DEBUG 01-15 16:10:29.097464.097464 lmp.py:1935]   Expert 34 |    161 | CPU
DEBUG 01-15 16:10:29.097391.097391 lmp.py:1935]   Expert 24 |    162 | CPU
DEBUG 01-15 16:10:29.097557.097557 lmp.py:1935]   Expert 52 |    168 | CPU
DEBUG 01-15 16:10:29.097247.097247 lmp.py:1935]   Expert  2 |    169 | CPU
DEBUG 01-15 16:10:29.097890.097890 lmp.py:1935]   Expert 13 |    169 | CPU
DEBUG 01-15 16:10:29.098771.098771 lmp.py:1935]   Expert 40 |    170 | CPU
DEBUG 01-15 16:10:29.098414.098414 lmp.py:1935]   Expert  6 |    172 | CPU
DEBUG 01-15 16:10:29.098441.098441 lmp.py:1935]   Expert 41 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098276.098276 lmp.py:1935]   Expert 18 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098396.098396 lmp.py:1935]   Expert 54 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098231.098231 lmp.py:1935]   Expert  3 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098113.098113 lmp.py:1935]   Expert 37 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098471.098471 lmp.py:1935]   Expert 20 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098829.098829 lmp.py:1935]   Expert 46 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098664.098664 lmp.py:1935]   Expert 19 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098261.098261 lmp.py:1935]   Expert 25 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098334.098334 lmp.py:1935]   Expert 51 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098931.098931 lmp.py:1935]   Expert 17 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098528.098528 lmp.py:1935]   Expert 43 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098409.098409 lmp.py:1935]   Expert 11 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098529.098529 lmp.py:1935]   Expert 31 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098172.098172 lmp.py:1935]   Expert 35 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098292.098292 lmp.py:1935]   Expert 23 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098173.098173 lmp.py:1935]   Expert 49 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098293.098293 lmp.py:1935]   Expert 39 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098889.098889 lmp.py:1935]   Expert 53 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098486.098486 lmp.py:1935]   Expert 10 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098321.098321 lmp.py:1935]   Expert 33 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098156.098156 lmp.py:1935]   Expert 36 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098276.098276 lmp.py:1935]   Expert 38 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098634.098634 lmp.py:1935]   Expert  4 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098516.098516 lmp.py:1935]   Expert 21 |    337 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098635.098635 lmp.py:1935]   Expert 14 |    348 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098517.098517 lmp.py:1935]   Expert 63 |    367 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098398.098398 lmp.py:1935]   Expert 45 |    375 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098041.098041 lmp.py:1935]   Expert 61 |    389 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098115.098115 lmp.py:1935]   Expert  9 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098950.098950 lmp.py:1935]   Expert 29 |    485 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.098023.098023 lmp.py:1935]   Expert  7 |    515 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.098189.098189 lmp.py:1937] 
DEBUG 01-15 16:10:29.098189.098189 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.098548.098548 lmp.py:1938]   CPU:   4057 tokens
DEBUG 01-15 16:10:29.098667.098667 lmp.py:1942]   cuda:1:   4116 tokens (16 experts)
DEBUG 01-15 16:10:29.098787.098787 lmp.py:1942]   cuda:2:   4115 tokens (16 experts)
DEBUG 01-15 16:10:29.098715.098715 lmp.py:1943]   Total GPU:   8231 tokens
DEBUG 01-15 16:10:29.098709.098709 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.098709.098709 lmp.py:1944] 
DEBUG 01-15 16:10:29.098028.098028 cuda_h.py:19] end experts_map_get cost 0.0017364025115966797 seconds
DEBUG 01-15 16:10:29.098116.098116 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.098396.098396 lmp.py:1953] 
DEBUG 01-15 16:10:29.098396.098396 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.098324.098324 cuda_h.py:19] end cpu_experts_submit cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:29.098113.098113 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.098512.098512 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.099037.099037 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.36.gate_proj.weight': 0, 'model.layers.19.mlp.experts.36.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.36.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.10.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.10.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.10.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.51.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.51.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.51.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.31.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.31.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.31.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.19.mlp.experts.33.gate_proj.weight': 0, 'model.layers.19.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.3.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.3.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.3.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.46.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.46.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.46.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.49.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.49.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.49.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.19.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.19.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.19.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.53.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.53.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.53.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.25.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.25.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.25.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.29.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(23414702080, 5767168, 0, 0), (23420469248, 5767168, 5767168, 0), (23408934912, 5767168, 11534336, 0), (23432003584, 5767168, 17301504, 0), (23437770752, 5767168, 23068672, 0), (23426236416, 5767168, 28835840, 0), (23449305088, 5767168, 34603008, 0), (23455072256, 5767168, 40370176, 0), (23443537920, 5767168, 46137344, 0), (22912958464, 5767168, 51904512, 0), (22918725632, 5767168, 57671680, 0), (22907191296, 5767168, 63438848, 0), (23466606592, 5767168, 69206016, 0), (23472373760, 5767168, 74973184, 0), (23460839424, 5767168, 80740352, 0), (23501209600, 5767168, 86507520, 0), (23506976768, 5767168, 92274688, 0), (23495442432, 5767168, 98041856, 0), (22964862976, 5767168, 103809024, 0), (22970630144, 5767168, 109576192, 0), (22959095808, 5767168, 115343360, 0), (23085973504, 5767168, 121110528, 0), (23091740672, 5767168, 126877696, 0), (23080206336, 5767168, 132644864, 0), (23674224640, 5767168, 138412032, 0), (23679991808, 5767168, 144179200, 0), (23668457472, 5767168, 149946368, 0), (23137878016, 5767168, 155713536, 0), (23143645184, 5767168, 161480704, 0), (23132110848, 5767168, 167247872, 0), (23155179520, 5767168, 173015040, 0), (23160946688, 5767168, 178782208, 0), (23149412352, 5767168, 184549376, 0), (23726129152, 5767168, 190316544, 0), (23731896320, 5767168, 196083712, 0), (23720361984, 5767168, 201850880, 0), (23189782528, 5767168, 207618048, 0), (23195549696, 5767168, 213385216, 0), (23184015360, 5767168, 219152384, 0), (23328194560, 5767168, 224919552, 0), (23333961728, 5767168, 230686720, 0), (23322427392, 5767168, 236453888, 0), (23847239680, 5767168, 242221056, 0), (23853006848, 5767168, 247988224, 0), (23841472512, 5767168, 253755392, 0), (23881842688, 5767168, 259522560, 0), (23887609856, 5767168, 265289728, 0), (23876075520, 5767168, 271056896, 0)], 2: [(23362797568, 5767168, 0, 0), (23368564736, 5767168, 5767168, 0), (23357030400, 5767168, 11534336, 0), (23397400576, 5767168, 17301504, 0), (23403167744, 5767168, 23068672, 0), (23391633408, 5767168, 28835840, 0), (22861053952, 5767168, 34603008, 0), (22866821120, 5767168, 40370176, 0), (22855286784, 5767168, 46137344, 0), (22843752448, 5767168, 51904512, 0), (22849519616, 5767168, 57671680, 0), (22837985280, 5767168, 63438848, 0), (22947561472, 5767168, 69206016, 0), (22953328640, 5767168, 74973184, 0), (22941794304, 5767168, 80740352, 0), (22982164480, 5767168, 86507520, 0), (22987931648, 5767168, 92274688, 0), (22976397312, 5767168, 98041856, 0), (23535812608, 5767168, 103809024, 0), (23541579776, 5767168, 109576192, 0), (23530045440, 5767168, 115343360, 0), (23570415616, 5767168, 121110528, 0), (23576182784, 5767168, 126877696, 0), (23564648448, 5767168, 132644864, 0), (23034068992, 5767168, 138412032, 0), (23039836160, 5767168, 144179200, 0), (23028301824, 5767168, 149946368, 0), (23587717120, 5767168, 155713536, 0), (23593484288, 5767168, 161480704, 0), (23581949952, 5767168, 167247872, 0), (23639621632, 5767168, 173015040, 0), (23645388800, 5767168, 178782208, 0), (23633854464, 5767168, 184549376, 0), (23103275008, 5767168, 190316544, 0), (23109042176, 5767168, 196083712, 0), (23097507840, 5767168, 201850880, 0), (23120576512, 5767168, 207618048, 0), (23126343680, 5767168, 213385216, 0), (23114809344, 5767168, 219152384, 0), (23708827648, 5767168, 224919552, 0), (23714594816, 5767168, 230686720, 0), (23703060480, 5767168, 236453888, 0), (23224385536, 5767168, 242221056, 0), (23230152704, 5767168, 247988224, 0), (23218618368, 5767168, 253755392, 0), (23293591552, 5767168, 259522560, 0), (23299358720, 5767168, 265289728, 0), (23287824384, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e34255f80>, 2: <capsule object NULL at 0x7a51b0646310>}
DEBUG 01-15 16:10:29.099174.099174 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.100262.100262 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.100542.100542 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f5a84a5d-2e2a-49cc-8023-5604152e62ea
DEBUG 01-15 16:10:29.100846.100846 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.100088.100088 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:29.100538.100538 client.py:127] Model loaded
DEBUG 01-15 16:10:29.100429.100429 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:29.100807.100807 cuda_h.py:10] start restore2model
INFO 01-15 16:10:29.101397.101397 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f5a84a5d-2e2a-49cc-8023-5604152e62ea
DEBUG 01-15 16:10:29.101492.101492 cuda_h.py:19] end move_flatidxs cost 0.0008313655853271484 seconds
DEBUG 01-15 16:10:29.101029.101029 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:29.101278.101278 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0026772022247314453 seconds
DEBUG 01-15 16:10:29.101042.101042 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.102497.102497 cuda_h.py:19] end restore2model cost 0.0006847381591796875 seconds
DEBUG 01-15 16:10:29.102288.102288 cuda_h.py:19] end sllm_worker_task cost 0.012770414352416992 seconds
DEBUG 01-15 16:10:29.104274.104274 cuda_h.py:19] end restore2model cost 0.003116607666015625 seconds
DEBUG 01-15 16:10:29.104190.104190 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006038188934326172 seconds
DEBUG 01-15 16:10:29.105132.105132 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.105400.105400 cuda_h.py:19] end gpu_sexperts cost 0.00027108192443847656 seconds
DEBUG 01-15 16:10:29.105514.105514 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:29.105529.105529 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:10:29.105940.105940 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.105597.105597 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.106407.106407 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008118152618408203 seconds
DEBUG 01-15 16:10:29.106727.106727 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.106422.106422 cuda_h.py:19] end gpu_group_list cost 0.0001685619354248047 seconds
DEBUG 01-15 16:10:29.106754.106754 cuda_h.py:19] end group_tensors cost 0.004779815673828125 seconds
DEBUG 01-15 16:10:29.107063.107063 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.107792.107792 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.108613.108613 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014150142669677734 seconds
DEBUG 01-15 16:10:29.109233.109233 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.109084.109084 cuda_h.py:19] end gpu_group_list cost 0.00025844573974609375 seconds
DEBUG 01-15 16:10:29.110226.110226 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.110189.110189 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f5a84a5d-2e2a-49cc-8023-5604152e62ea
DEBUG 01-15 16:10:29.110971.110971 cuda_h.py:19] end group pad cost 0.0037393569946289062 seconds
DEBUG 01-15 16:10:29.110523.110523 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.128812.128812 client.py:127] Model loaded
DEBUG 01-15 16:10:29.129837.129837 cuda_h.py:19] end wait_experts_multi_device cost 0.0187680721282959 seconds
DEBUG 01-15 16:10:29.129037.129037 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.130585.130585 cuda_h.py:19] end group_einsum cost 0.019384145736694336 seconds
DEBUG 01-15 16:10:29.130590.130590 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.134891.134891 cuda_h.py:19] end get_outputs_cpu1 cost 0.003974437713623047 seconds
DEBUG 01-15 16:10:29.135846.135846 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03482770919799805 seconds
DEBUG 01-15 16:10:29.135535.135535 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0064961910247802734 seconds
DEBUG 01-15 16:10:29.135968.135968 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.136101.136101 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.137073.137073 cuda_h.py:19] end index_scatter cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:10:29.137216.137216 cuda_h.py:19] end cpuoutputsdeal cost 0.0016565322875976562 seconds
DEBUG 01-15 16:10:29.137365.137365 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.137459.137459 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.137775.137775 cuda_h.py:19] end gpu_group_tensor cost 0.00013256072998046875 seconds
DEBUG 01-15 16:10:29.137392.137392 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.137642.137642 cuda_h.py:19] end gpu_group_tensor cost 0.00011992454528808594 seconds
DEBUG 01-15 16:10:29.137209.137209 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.138471.138471 cuda_h.py:19] end gpu_group_einsum cost 0.0004646778106689453 seconds
DEBUG 01-15 16:10:29.138527.138527 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.139074.139074 cuda_h.py:19] end gpu_group_einsum cost 0.00045752525329589844 seconds
DEBUG 01-15 16:10:29.139343.139343 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.139553.139553 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.139885.139885 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019931793212890625 seconds
DEBUG 01-15 16:10:29.139986.139986 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.139983.139983 cuda_h.py:19] end concat_expert_out cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:10:29.139773.139773 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.139458.139458 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:29.140154.140154 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008254051208496094 seconds
DEBUG 01-15 16:10:29.140561.140561 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.140351.140351 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.140543.140543 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001437664031982422 seconds
DEBUG 01-15 16:10:29.140299.140299 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.140414.140414 cuda_h.py:19] end concat_expert_out cost 5.340576171875e-05 seconds
DEBUG 01-15 16:10:29.140350.140350 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.140319.140319 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:29.140175.140175 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004878044128417969 seconds
DEBUG 01-15 16:10:29.140555.140555 cuda_h.py:19] end gpu_experts_multi_device cost 0.03533124923706055 seconds
DEBUG 01-15 16:10:29.140319.140319 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.0447087287902832 seconds
DEBUG 01-15 16:10:29.141543.141543 cuda_h.py:19] end prefill_layer cost 0.051959991455078125 seconds
DEBUG 01-15 16:10:29.141486.141486 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:10:29.141427.141427 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.141560.141560 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:10:29.141694.141694 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:29.141450.141450 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:29.141969.141969 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 3.886222839355469e-05 seconds
DEBUG 01-15 16:10:29.141532.141532 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 7.128715515136719e-05 seconds
DEBUG 01-15 16:10:29.141374.141374 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.141291.141291 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.141929.141929 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:29.141860.141860 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:29.141934.141934 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:29.142110.142110 cuda_h.py:19] end allocate_cuda_memory cost 0.0002675056457519531 seconds
DEBUG 01-15 16:10:29.142332.142332 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:29.142717.142717 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.142739.142739 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.142018.142018 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7fee4ed8-4fc1-49d3-a7e3-5955a9f6a304
DEBUG 01-15 16:10:29.142399.142399 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.142786.142786 cuda_h.py:10] start self_attn
INFO 01-15 16:10:29.143457.143457 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7fee4ed8-4fc1-49d3-a7e3-5955a9f6a304
DEBUG 01-15 16:10:29.143730.143730 cuda_h.py:19] end load_into_gpu_async cost 0.0013782978057861328 seconds
DEBUG 01-15 16:10:29.143056.143056 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:29.143980.143980 cuda_h.py:19] end restore_tensors2 cost 8.845329284667969e-05 seconds
DEBUG 01-15 16:10:29.143743.143743 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002024412155151367 seconds
INFO 01-15 16:10:29.143500.143500 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7fee4ed8-4fc1-49d3-a7e3-5955a9f6a304
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.146762.146762 cuda_h.py:19] end self_attn cost 0.00356292724609375 seconds
DEBUG 01-15 16:10:29.146613.146613 cuda_h.py:19] end iln_self_attn_paln cost 0.005128383636474609 seconds
DEBUG 01-15 16:10:29.146350.146350 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:10:29.146590.146590 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.147115.147115 cuda_h.py:19] end gate cost 0.0008051395416259766 seconds
DEBUG 01-15 16:10:29.147666.147666 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.148261.148261 lmp.py:1912] 
DEBUG 01-15 16:10:29.148261.148261 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.148640.148640 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.148005.148005 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.148271.148271 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.148390.148390 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.148557.148557 lmp.py:1917] 
DEBUG 01-15 16:10:29.148557.148557 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.148438.148438 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.148041.148041 lmp.py:1935]   Expert 54 |     22 | CPU
DEBUG 01-15 16:10:29.148923.148923 lmp.py:1935]   Expert  3 |     33 | CPU
DEBUG 01-15 16:10:29.148850.148850 lmp.py:1935]   Expert  8 |     40 | CPU
DEBUG 01-15 16:10:29.148778.148778 lmp.py:1935]   Expert 28 |     43 | CPU
DEBUG 01-15 16:10:29.148229.148229 lmp.py:1935]   Expert 43 |     54 | CPU
DEBUG 01-15 16:10:29.148349.148349 lmp.py:1935]   Expert 63 |     55 | CPU
DEBUG 01-15 16:10:29.148230.148230 lmp.py:1935]   Expert 38 |     75 | CPU
DEBUG 01-15 16:10:29.148589.148589 lmp.py:1935]   Expert 36 |     76 | CPU
DEBUG 01-15 16:10:29.148947.148947 lmp.py:1935]   Expert  6 |     82 | CPU
DEBUG 01-15 16:10:29.148828.148828 lmp.py:1935]   Expert 57 |     97 | CPU
DEBUG 01-15 16:10:29.148101.148101 lmp.py:1935]   Expert 39 |     98 | CPU
DEBUG 01-15 16:10:29.148651.148651 lmp.py:1935]   Expert 41 |    102 | CPU
DEBUG 01-15 16:10:29.148294.148294 lmp.py:1935]   Expert 12 |    109 | CPU
DEBUG 01-15 16:10:29.148698.148698 lmp.py:1935]   Expert 52 |    112 | CPU
DEBUG 01-15 16:10:29.148341.148341 lmp.py:1935]   Expert 19 |    120 | CPU
DEBUG 01-15 16:10:29.148746.148746 lmp.py:1935]   Expert 47 |    125 | CPU
DEBUG 01-15 16:10:29.148150.148150 lmp.py:1935]   Expert 13 |    134 | CPU
DEBUG 01-15 16:10:29.148793.148793 lmp.py:1935]   Expert 22 |    144 | CPU
DEBUG 01-15 16:10:29.148960.148960 lmp.py:1935]   Expert 46 |    144 | CPU
DEBUG 01-15 16:10:29.148126.148126 lmp.py:1935]   Expert 50 |    151 | CPU
DEBUG 01-15 16:10:29.148292.148292 lmp.py:1935]   Expert 24 |    161 | CPU
DEBUG 01-15 16:10:29.148412.148412 lmp.py:1935]   Expert 40 |    162 | CPU
DEBUG 01-15 16:10:29.148531.148531 lmp.py:1935]   Expert 20 |    165 | CPU
DEBUG 01-15 16:10:29.148890.148890 lmp.py:1935]   Expert 55 |    167 | CPU
DEBUG 01-15 16:10:29.148771.148771 lmp.py:1935]   Expert 37 |    169 | CPU
DEBUG 01-15 16:10:29.148176.148176 lmp.py:1935]   Expert  2 |    170 | CPU
DEBUG 01-15 16:10:29.148580.148580 lmp.py:1935]   Expert 23 |    170 | CPU
DEBUG 01-15 16:10:29.148746.148746 lmp.py:1935]   Expert 53 |    175 | CPU
DEBUG 01-15 16:10:29.148674.148674 lmp.py:1935]   Expert 21 |    176 | CPU
DEBUG 01-15 16:10:29.148363.148363 lmp.py:1935]   Expert 42 |    177 | CPU
DEBUG 01-15 16:10:29.148768.148768 lmp.py:1935]   Expert 61 |    177 | CPU
DEBUG 01-15 16:10:29.148934.148934 lmp.py:1935]   Expert 49 |    178 | CPU
DEBUG 01-15 16:10:29.148200.148200 lmp.py:1935]   Expert 18 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.148988.148988 lmp.py:1935]   Expert 33 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.148062.148062 lmp.py:1935]   Expert 32 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.148566.148566 lmp.py:1935]   Expert  0 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.148878.148878 lmp.py:1935]   Expert  5 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.148998.148998 lmp.py:1935]   Expert 30 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.148767.148767 lmp.py:1935]   Expert 16 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.148649.148649 lmp.py:1935]   Expert 14 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.148530.148530 lmp.py:1935]   Expert 34 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.148650.148650 lmp.py:1935]   Expert  7 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.148962.148962 lmp.py:1935]   Expert 31 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149081.149081 lmp.py:1935]   Expert 62 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149678.149678 lmp.py:1935]   Expert 59 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149275.149275 lmp.py:1935]   Expert 60 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149156.149156 lmp.py:1935]   Expert  9 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149799.149799 lmp.py:1935]   Expert 10 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149204.149204 lmp.py:1935]   Expert 17 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149847.149847 lmp.py:1935]   Expert 29 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149490.149490 lmp.py:1935]   Expert 15 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149133.149133 lmp.py:1935]   Expert 58 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149206.149206 lmp.py:1935]   Expert  4 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149564.149564 lmp.py:1935]   Expert 26 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149923.149923 lmp.py:1935]   Expert 51 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149042.149042 lmp.py:1935]   Expert 11 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149924.149924 lmp.py:1935]   Expert 44 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149805.149805 lmp.py:1935]   Expert 56 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149448.149448 lmp.py:1935]   Expert 27 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149091.149091 lmp.py:1935]   Expert  1 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149496.149496 lmp.py:1935]   Expert 45 |    367 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149900.149900 lmp.py:1935]   Expert 25 |    465 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149305.149305 lmp.py:1935]   Expert 35 |    517 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.149901.149901 lmp.py:1935]   Expert 48 |    637 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.149068.149068 lmp.py:1937] 
DEBUG 01-15 16:10:29.149068.149068 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.149187.149187 lmp.py:1938]   CPU:   3863 tokens
DEBUG 01-15 16:10:29.149546.149546 lmp.py:1942]   cuda:1:   4213 tokens (16 experts)
DEBUG 01-15 16:10:29.149950.149950 lmp.py:1942]   cuda:2:   4212 tokens (16 experts)
DEBUG 01-15 16:10:29.149639.149639 lmp.py:1943]   Total GPU:   8425 tokens
DEBUG 01-15 16:10:29.149852.149852 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.149852.149852 lmp.py:1944] 
DEBUG 01-15 16:10:29.149740.149740 cuda_h.py:19] end experts_map_get cost 0.0017786026000976562 seconds
DEBUG 01-15 16:10:29.149928.149928 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.149929.149929 lmp.py:1953] 
DEBUG 01-15 16:10:29.149929.149929 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.149395.149395 cuda_h.py:19] end cpu_experts_submit cost 6.175041198730469e-05 seconds
DEBUG 01-15 16:10:29.149919.149919 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.149888.149888 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.150353.150353 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.32.gate_proj.weight': 0, 'model.layers.20.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.58.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.58.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.58.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.56.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.56.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.56.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.35.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.35.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.35.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.4.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.4.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.4.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.5.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.5.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.5.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.33.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.33.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.33.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.9.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.9.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.9.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.44.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.44.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.44.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(24452792320, 5767168, 0, 0), (24458559488, 5767168, 5767168, 0), (24447025152, 5767168, 11534336, 0), (24902631424, 5767168, 17301504, 0), (24908398592, 5767168, 23068672, 0), (24896864256, 5767168, 28835840, 0), (24020254720, 5767168, 34603008, 0), (24026021888, 5767168, 40370176, 0), (24014487552, 5767168, 46137344, 0), (24072159232, 5767168, 51904512, 0), (24077926400, 5767168, 57671680, 0), (24066392064, 5767168, 63438848, 0), (24089460736, 5767168, 69206016, 0), (24095227904, 5767168, 74973184, 0), (24083693568, 5767168, 80740352, 0), (24677711872, 5767168, 86507520, 0), (24683479040, 5767168, 92274688, 0), (24671944704, 5767168, 98041856, 0), (24141365248, 5767168, 103809024, 0), (24147132416, 5767168, 109576192, 0), (24135598080, 5767168, 115343360, 0), (24729616384, 5767168, 121110528, 0), (24735383552, 5767168, 126877696, 0), (24723849216, 5767168, 132644864, 0), (24193269760, 5767168, 138412032, 0), (24199036928, 5767168, 144179200, 0), (24187502592, 5767168, 149946368, 0), (24210571264, 5767168, 155713536, 0), (24216338432, 5767168, 161480704, 0), (24204804096, 5767168, 167247872, 0), (24868028416, 5767168, 173015040, 0), (24873795584, 5767168, 178782208, 0), (24862261248, 5767168, 184549376, 0), (24348983296, 5767168, 190316544, 0), (24354750464, 5767168, 196083712, 0), (24343216128, 5767168, 201850880, 0), (24366284800, 5767168, 207618048, 0), (24372051968, 5767168, 213385216, 0), (24360517632, 5767168, 219152384, 0), (24937234432, 5767168, 224919552, 0), (24943001600, 5767168, 230686720, 0), (24931467264, 5767168, 236453888, 0), (24418189312, 5767168, 242221056, 0), (24423956480, 5767168, 247988224, 0), (24412422144, 5767168, 253755392, 0), (24435490816, 5767168, 259522560, 0), (24441257984, 5767168, 265289728, 0), (24429723648, 5767168, 271056896, 0)], 2: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (23916445696, 5767168, 17301504, 0), (23922212864, 5767168, 23068672, 0), (23910678528, 5767168, 28835840, 0), (24487395328, 5767168, 34603008, 0), (24493162496, 5767168, 40370176, 0), (24481628160, 5767168, 46137344, 0), (24504696832, 5767168, 51904512, 0), (24510464000, 5767168, 57671680, 0), (24498929664, 5767168, 63438848, 0), (23968350208, 5767168, 69206016, 0), (23974117376, 5767168, 74973184, 0), (23962583040, 5767168, 80740352, 0), (23985651712, 5767168, 86507520, 0), (23991418880, 5767168, 92274688, 0), (23979884544, 5767168, 98041856, 0), (24470093824, 5767168, 103809024, 0), (24475860992, 5767168, 109576192, 0), (24464326656, 5767168, 115343360, 0), (24054857728, 5767168, 121110528, 0), (24060624896, 5767168, 126877696, 0), (24049090560, 5767168, 132644864, 0), (24660410368, 5767168, 138412032, 0), (24666177536, 5767168, 144179200, 0), (24654643200, 5767168, 149946368, 0), (24158666752, 5767168, 155713536, 0), (24164433920, 5767168, 161480704, 0), (24152899584, 5767168, 167247872, 0), (24175968256, 5767168, 173015040, 0), (24181735424, 5767168, 178782208, 0), (24170201088, 5767168, 184549376, 0), (24781520896, 5767168, 190316544, 0), (24787288064, 5767168, 196083712, 0), (24775753728, 5767168, 201850880, 0), (24331681792, 5767168, 207618048, 0), (24337448960, 5767168, 213385216, 0), (24325914624, 5767168, 219152384, 0), (24919932928, 5767168, 224919552, 0), (24925700096, 5767168, 230686720, 0), (24914165760, 5767168, 236453888, 0), (24400887808, 5767168, 242221056, 0), (24406654976, 5767168, 247988224, 0), (24395120640, 5767168, 253755392, 0), (24971837440, 5767168, 259522560, 0), (24977604608, 5767168, 265289728, 0), (24966070272, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b06461c0>, 2: <capsule object NULL at 0x7a51b0646010>}
DEBUG 01-15 16:10:29.150856.150856 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.150831.150831 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.150680.150680 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6d08b1a3-2e1b-4611-ba88-94fac8a59f3c
DEBUG 01-15 16:10:29.150935.150935 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:29.151699.151699 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.151391.151391 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:29.151652.151652 client.py:127] Model loaded
DEBUG 01-15 16:10:29.151005.151005 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.151746.151746 cuda_h.py:19] end restore2model cost 0.0003387928009033203 seconds
DEBUG 01-15 16:10:29.151323.151323 cuda_h.py:19] end sllm_worker_task cost 0.009900808334350586 seconds
INFO 01-15 16:10:29.151604.151604 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6d08b1a3-2e1b-4611-ba88-94fac8a59f3c
DEBUG 01-15 16:10:29.152917.152917 cuda_h.py:19] end move_flatidxs cost 0.0008301734924316406 seconds
DEBUG 01-15 16:10:29.152978.152978 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:29.152081.152081 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002568960189819336 seconds
DEBUG 01-15 16:10:29.152805.152805 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.154341.154341 cuda_h.py:19] end restore2model cost 0.002536296844482422 seconds
DEBUG 01-15 16:10:29.155231.155231 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005334615707397461 seconds
DEBUG 01-15 16:10:29.155503.155503 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.155964.155964 cuda_h.py:19] end gpu_sexperts cost 0.0002734661102294922 seconds
DEBUG 01-15 16:10:29.155555.155555 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:29.155901.155901 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:10:29.155789.155789 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.155445.155445 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.156778.156778 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008103847503662109 seconds
DEBUG 01-15 16:10:29.156873.156873 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.156768.156768 cuda_h.py:19] end gpu_group_list cost 0.00017404556274414062 seconds
DEBUG 01-15 16:10:29.157448.157448 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.158228.158228 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008652210235595703 seconds
DEBUG 01-15 16:10:29.158985.158985 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.158409.158409 cuda_h.py:19] end gpu_group_list cost 0.00017762184143066406 seconds
DEBUG 01-15 16:10:29.159965.159965 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.159225.159225 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6d08b1a3-2e1b-4611-ba88-94fac8a59f3c
DEBUG 01-15 16:10:29.160708.160708 cuda_h.py:19] end group_tensors cost 0.008892297744750977 seconds
DEBUG 01-15 16:10:29.161097.161097 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.165982.165982 cuda_h.py:19] end group pad cost 0.003830432891845703 seconds
DEBUG 01-15 16:10:29.165031.165031 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.178938.178938 client.py:127] Model loaded
DEBUG 01-15 16:10:29.178016.178016 cuda_h.py:19] end wait_experts_multi_device cost 0.019068002700805664 seconds
DEBUG 01-15 16:10:29.178693.178693 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.184396.184396 cuda_h.py:19] end group_einsum cost 0.018923044204711914 seconds
DEBUG 01-15 16:10:29.184414.184414 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.188016.188016 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036721229553222656 seconds
DEBUG 01-15 16:10:29.189390.189390 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03822946548461914 seconds
DEBUG 01-15 16:10:29.189373.189373 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.011325597763061523 seconds
DEBUG 01-15 16:10:29.189277.189277 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.190269.190269 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.191003.191003 cuda_h.py:19] end index_scatter cost 7.462501525878906e-05 seconds
DEBUG 01-15 16:10:29.191523.191523 cuda_h.py:19] end cpuoutputsdeal cost 0.0015821456909179688 seconds
DEBUG 01-15 16:10:29.191777.191777 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.191633.191633 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.191480.191480 cuda_h.py:19] end gpu_group_tensor cost 0.00013637542724609375 seconds
DEBUG 01-15 16:10:29.191050.191050 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.191731.191731 cuda_h.py:19] end gpu_group_tensor cost 0.00012040138244628906 seconds
DEBUG 01-15 16:10:29.192297.192297 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.192406.192406 cuda_h.py:19] end gpu_group_einsum cost 0.0006251335144042969 seconds
DEBUG 01-15 16:10:29.192126.192126 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.193012.193012 cuda_h.py:19] end gpu_group_einsum cost 0.0004870891571044922 seconds
DEBUG 01-15 16:10:29.193996.193996 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.193967.193967 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.193300.193300 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019812583923339844 seconds
DEBUG 01-15 16:10:29.193877.193877 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.194675.194675 cuda_h.py:19] end concat_expert_out cost 5.841255187988281e-05 seconds
DEBUG 01-15 16:10:29.194817.194817 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.194025.194025 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:29.194145.194145 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008285045623779297 seconds
DEBUG 01-15 16:10:29.194029.194029 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.194773.194773 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.194082.194082 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012755393981933594 seconds
DEBUG 01-15 16:10:29.194454.194454 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.194947.194947 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:29.194598.194598 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.194137.194137 cuda_h.py:19] end index_scatter cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:10:29.195277.195277 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004639625549316406 seconds
DEBUG 01-15 16:10:29.195796.195796 cuda_h.py:19] end gpu_experts_multi_device cost 0.03955364227294922 seconds
DEBUG 01-15 16:10:29.195898.195898 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.0483698844909668 seconds
DEBUG 01-15 16:10:29.195546.195546 cuda_h.py:19] end prefill_layer cost 0.05416440963745117 seconds
DEBUG 01-15 16:10:29.195005.195005 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:10:29.195708.195708 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.195126.195126 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:10:29.195260.195260 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:29.195777.195777 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:29.195389.195389 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 3.62396240234375e-05 seconds
DEBUG 01-15 16:10:29.195860.195860 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 7.009506225585938e-05 seconds
DEBUG 01-15 16:10:29.195463.195463 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.195665.195665 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.195111.195111 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:29.196571.196571 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:29.196884.196884 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:29.196684.196684 cuda_h.py:19] end allocate_cuda_memory cost 0.00030303001403808594 seconds
DEBUG 01-15 16:10:29.196117.196117 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:29.196025.196025 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.196762.196762 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.196949.196949 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ad6cb883-df72-4fab-bbca-1977369a415e
DEBUG 01-15 16:10:29.196668.196668 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.197354.197354 cuda_h.py:10] start self_attn
INFO 01-15 16:10:29.198833.198833 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ad6cb883-df72-4fab-bbca-1977369a415e
DEBUG 01-15 16:10:29.198199.198199 cuda_h.py:19] end load_into_gpu_async cost 0.001638650894165039 seconds
DEBUG 01-15 16:10:29.198286.198286 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:29.198403.198403 cuda_h.py:19] end restore_tensors2 cost 8.869171142578125e-05 seconds
DEBUG 01-15 16:10:29.198643.198643 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0023021697998046875 seconds
INFO 01-15 16:10:29.198499.198499 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ad6cb883-df72-4fab-bbca-1977369a415e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.200840.200840 cuda_h.py:19] end self_attn cost 0.0033757686614990234 seconds
DEBUG 01-15 16:10:29.200110.200110 cuda_h.py:19] end iln_self_attn_paln cost 0.0050432682037353516 seconds
DEBUG 01-15 16:10:29.200324.200324 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:10:29.200563.200563 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.201363.201363 cuda_h.py:19] end gate cost 0.0006902217864990234 seconds
DEBUG 01-15 16:10:29.201676.201676 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.202265.202265 lmp.py:1912] 
DEBUG 01-15 16:10:29.202265.202265 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.202366.202366 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.202638.202638 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.202619.202619 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.202693.202693 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.202528.202528 lmp.py:1917] 
DEBUG 01-15 16:10:29.202528.202528 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.202886.202886 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.202966.202966 lmp.py:1935]   Expert 44 |     30 | CPU
DEBUG 01-15 16:10:29.202040.202040 lmp.py:1935]   Expert  9 |     35 | CPU
DEBUG 01-15 16:10:29.202636.202636 lmp.py:1935]   Expert 11 |     36 | CPU
DEBUG 01-15 16:10:29.202279.202279 lmp.py:1935]   Expert 56 |     59 | CPU
DEBUG 01-15 16:10:29.202922.202922 lmp.py:1935]   Expert 54 |     80 | CPU
DEBUG 01-15 16:10:29.202519.202519 lmp.py:1935]   Expert 62 |     91 | CPU
DEBUG 01-15 16:10:29.202877.202877 lmp.py:1935]   Expert 47 |     94 | CPU
DEBUG 01-15 16:10:29.202235.202235 lmp.py:1935]   Expert  7 |     95 | CPU
DEBUG 01-15 16:10:29.202070.202070 lmp.py:1935]   Expert 51 |    101 | CPU
DEBUG 01-15 16:10:29.202952.202952 lmp.py:1935]   Expert 60 |    104 | CPU
DEBUG 01-15 16:10:29.202356.202356 lmp.py:1935]   Expert 22 |    108 | CPU
DEBUG 01-15 16:10:29.202761.202761 lmp.py:1935]   Expert 52 |    108 | CPU
DEBUG 01-15 16:10:29.202404.202404 lmp.py:1935]   Expert 41 |    110 | CPU
DEBUG 01-15 16:10:29.202808.202808 lmp.py:1935]   Expert 53 |    111 | CPU
DEBUG 01-15 16:10:29.202451.202451 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:10:29.202823.202823 lmp.py:1935]   Expert  1 |    128 | CPU
DEBUG 01-15 16:10:29.202989.202989 lmp.py:1935]   Expert  2 |    128 | CPU
DEBUG 01-15 16:10:29.202917.202917 lmp.py:1935]   Expert 48 |    128 | CPU
DEBUG 01-15 16:10:29.202799.202799 lmp.py:1935]   Expert  8 |    129 | CPU
DEBUG 01-15 16:10:29.202680.202680 lmp.py:1935]   Expert 32 |    130 | CPU
DEBUG 01-15 16:10:29.202561.202561 lmp.py:1935]   Expert 23 |    140 | CPU
DEBUG 01-15 16:10:29.202204.202204 lmp.py:1935]   Expert 27 |    140 | CPU
DEBUG 01-15 16:10:29.202894.202894 lmp.py:1935]   Expert 35 |    143 | CPU
DEBUG 01-15 16:10:29.202583.202583 lmp.py:1935]   Expert 59 |    144 | CPU
DEBUG 01-15 16:10:29.202511.202511 lmp.py:1935]   Expert 39 |    148 | CPU
DEBUG 01-15 16:10:29.202438.202438 lmp.py:1935]   Expert 26 |    149 | CPU
DEBUG 01-15 16:10:29.202128.202128 lmp.py:1935]   Expert 50 |    150 | CPU
DEBUG 01-15 16:10:29.202055.202055 lmp.py:1935]   Expert 14 |    155 | CPU
DEBUG 01-15 16:10:29.202506.202506 lmp.py:1935]   Expert 46 |    165 | CPU
DEBUG 01-15 16:10:29.202388.202388 lmp.py:1935]   Expert 24 |    168 | CPU
DEBUG 01-15 16:10:29.202269.202269 lmp.py:1935]   Expert  0 |    172 | CPU
DEBUG 01-15 16:10:29.202674.202674 lmp.py:1935]   Expert  4 |    173 | CPU
DEBUG 01-15 16:10:29.202939.202939 lmp.py:1935]   Expert 34 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.202251.202251 lmp.py:1935]   Expert 38 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.202325.202325 lmp.py:1935]   Expert 49 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.202444.202444 lmp.py:1935]   Expert 40 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.202326.202326 lmp.py:1935]   Expert 63 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.202207.202207 lmp.py:1935]   Expert  5 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203089.203089 lmp.py:1935]   Expert 19 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203970.203970 lmp.py:1935]   Expert 13 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203043.203043 lmp.py:1935]   Expert 29 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203402.203402 lmp.py:1935]   Expert 43 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203237.203237 lmp.py:1935]   Expert 57 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203310.203310 lmp.py:1935]   Expert 61 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203430.203430 lmp.py:1935]   Expert 33 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203073.203073 lmp.py:1935]   Expert 31 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203193.203193 lmp.py:1935]   Expert 16 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203074.203074 lmp.py:1935]   Expert 20 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203717.203717 lmp.py:1935]   Expert 37 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203599.203599 lmp.py:1935]   Expert  3 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203248.203248 lmp.py:1935]   Expert 15 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203130.203130 lmp.py:1935]   Expert 36 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203773.203773 lmp.py:1935]   Expert 18 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203369.203369 lmp.py:1935]   Expert 12 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203535.203535 lmp.py:1935]   Expert 17 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203225.203225 lmp.py:1935]   Expert 28 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203914.203914 lmp.py:1935]   Expert 55 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203842.203842 lmp.py:1935]   Expert 30 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203531.203531 lmp.py:1935]   Expert 25 |    325 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203220.203220 lmp.py:1935]   Expert 58 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203148.203148 lmp.py:1935]   Expert 10 |    363 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203076.203076 lmp.py:1935]   Expert 45 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203242.203242 lmp.py:1935]   Expert 21 |    395 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.203362.203362 lmp.py:1935]   Expert 42 |    647 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.203051.203051 lmp.py:1937] 
DEBUG 01-15 16:10:29.203051.203051 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.203694.203694 lmp.py:1938]   CPU:   3779 tokens
DEBUG 01-15 16:10:29.203575.203575 lmp.py:1942]   cuda:1:   4169 tokens (15 experts)
DEBUG 01-15 16:10:29.203457.203457 lmp.py:1942]   cuda:2:   4340 tokens (17 experts)
DEBUG 01-15 16:10:29.203908.203908 lmp.py:1943]   Total GPU:   8509 tokens
DEBUG 01-15 16:10:29.203882.203882 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.203882.203882 lmp.py:1944] 
DEBUG 01-15 16:10:29.203055.203055 cuda_h.py:19] end experts_map_get cost 0.0017921924591064453 seconds
DEBUG 01-15 16:10:29.203905.203905 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.203137.203137 lmp.py:1953] 
DEBUG 01-15 16:10:29.203137.203137 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.203921.203921 cuda_h.py:19] end cpu_experts_submit cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:29.203901.203901 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.203300.203300 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.204711.204711 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.33.gate_proj.weight': 0, 'model.layers.21.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.42.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.42.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.42.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.49.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.49.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.49.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.63.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.63.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.63.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.57.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.57.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.57.up_proj.weight': 253755392}, 2: {'model.layers.21.mlp.experts.34.gate_proj.weight': 0, 'model.layers.21.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.5.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.5.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.5.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.40.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.40.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.40.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.43.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.43.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.43.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.12.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.12.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.12.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.45.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.45.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.45.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.13.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.13.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.13.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.17.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.17.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.17.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.30.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.31.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.31.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.31.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(25577390080, 5767168, 0, 0), (25583157248, 5767168, 5767168, 0), (25571622912, 5767168, 11534336, 0), (25733103616, 5767168, 17301504, 0), (25738870784, 5767168, 23068672, 0), (25727336448, 5767168, 28835840, 0), (25179455488, 5767168, 34603008, 0), (25185222656, 5767168, 40370176, 0), (25173688320, 5767168, 46137344, 0), (25265963008, 5767168, 51904512, 0), (25271730176, 5767168, 57671680, 0), (25260195840, 5767168, 63438848, 0), (25283264512, 5767168, 69206016, 0), (25289031680, 5767168, 74973184, 0), (25277497344, 5767168, 80740352, 0), (25854214144, 5767168, 86507520, 0), (25859981312, 5767168, 92274688, 0), (25848446976, 5767168, 98041856, 0), (25317867520, 5767168, 103809024, 0), (25323634688, 5767168, 109576192, 0), (25312100352, 5767168, 115343360, 0), (25335169024, 5767168, 121110528, 0), (25340936192, 5767168, 126877696, 0), (25329401856, 5767168, 132644864, 0), (25352470528, 5767168, 138412032, 0), (25358237696, 5767168, 144179200, 0), (25346703360, 5767168, 149946368, 0), (26096435200, 5767168, 155713536, 0), (26102202368, 5767168, 161480704, 0), (26090668032, 5767168, 167247872, 0), (25958023168, 5767168, 173015040, 0), (25963790336, 5767168, 178782208, 0), (25952256000, 5767168, 184549376, 0), (25438978048, 5767168, 190316544, 0), (25444745216, 5767168, 196083712, 0), (25433210880, 5767168, 201850880, 0), (25490882560, 5767168, 207618048, 0), (25496649728, 5767168, 213385216, 0), (25485115392, 5767168, 219152384, 0), (25508184064, 5767168, 224919552, 0), (25513951232, 5767168, 230686720, 0), (25502416896, 5767168, 236453888, 0), (25992626176, 5767168, 242221056, 0), (25998393344, 5767168, 247988224, 0), (25986859008, 5767168, 253755392, 0)], 2: [(25594691584, 5767168, 0, 0), (25600458752, 5767168, 5767168, 0), (25588924416, 5767168, 11534336, 0), (25058344960, 5767168, 17301504, 0), (25064112128, 5767168, 23068672, 0), (25052577792, 5767168, 28835840, 0), (25629294592, 5767168, 34603008, 0), (25635061760, 5767168, 40370176, 0), (25623527424, 5767168, 46137344, 0), (25646596096, 5767168, 51904512, 0), (25652363264, 5767168, 57671680, 0), (25640828928, 5767168, 63438848, 0), (25092947968, 5767168, 69206016, 0), (25098715136, 5767168, 74973184, 0), (25087180800, 5767168, 80740352, 0), (25663897600, 5767168, 86507520, 0), (25669664768, 5767168, 92274688, 0), (25658130432, 5767168, 98041856, 0), (25698500608, 5767168, 103809024, 0), (25704267776, 5767168, 109576192, 0), (25692733440, 5767168, 115343360, 0), (25750405120, 5767168, 121110528, 0), (25756172288, 5767168, 126877696, 0), (25744637952, 5767168, 132644864, 0), (25214058496, 5767168, 138412032, 0), (25219825664, 5767168, 144179200, 0), (25208291328, 5767168, 149946368, 0), (25785008128, 5767168, 155713536, 0), (25790775296, 5767168, 161480704, 0), (25779240960, 5767168, 167247872, 0), (25231360000, 5767168, 173015040, 0), (25237127168, 5767168, 178782208, 0), (25225592832, 5767168, 184549376, 0), (25300566016, 5767168, 190316544, 0), (25306333184, 5767168, 196083712, 0), (25294798848, 5767168, 201850880, 0), (25369772032, 5767168, 207618048, 0), (25375539200, 5767168, 213385216, 0), (25364004864, 5767168, 219152384, 0), (26009927680, 5767168, 224919552, 0), (26015694848, 5767168, 230686720, 0), (26004160512, 5767168, 236453888, 0), (26061832192, 5767168, 242221056, 0), (26067599360, 5767168, 247988224, 0), (26056065024, 5767168, 253755392, 0), (25525485568, 5767168, 259522560, 0), (25531252736, 5767168, 265289728, 0), (25519718400, 5767168, 271056896, 0), (25542787072, 5767168, 276824064, 0), (25548554240, 5767168, 282591232, 0), (25537019904, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b87d1f80>, 2: <capsule object NULL at 0x7a51b0645ec0>}
DEBUG 01-15 16:10:29.205980.205980 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.205194.205194 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.205374.205374 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 557420f7-3c1b-4038-bbdf-398b3616af49
DEBUG 01-15 16:10:29.205516.205516 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:29.205270.205270 client.py:127] Model loaded
DEBUG 01-15 16:10:29.205668.205668 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:29.205776.205776 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.205125.205125 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.206369.206369 cuda_h.py:19] end restore2model cost 0.00046825408935546875 seconds
DEBUG 01-15 16:10:29.206337.206337 cuda_h.py:19] end sllm_worker_task cost 0.010229825973510742 seconds
DEBUG 01-15 16:10:29.206969.206969 cuda_h.py:19] end move_flatidxs cost 0.0008378028869628906 seconds
DEBUG 01-15 16:10:29.206461.206461 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:29.207927.207927 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 557420f7-3c1b-4038-bbdf-398b3616af49
DEBUG 01-15 16:10:29.207403.207403 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00383758544921875 seconds
DEBUG 01-15 16:10:29.207591.207591 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.210664.210664 cuda_h.py:19] end restore2model cost 0.0027208328247070312 seconds
DEBUG 01-15 16:10:29.210076.210076 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0067768096923828125 seconds
DEBUG 01-15 16:10:29.210846.210846 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.210671.210671 cuda_h.py:19] end gpu_sexperts cost 0.0002930164337158203 seconds
DEBUG 01-15 16:10:29.210262.210262 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:29.211370.211370 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4781951904296875e-05 seconds
DEBUG 01-15 16:10:29.211543.211543 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.211868.211868 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.211452.211452 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007851123809814453 seconds
DEBUG 01-15 16:10:29.211010.211010 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.212646.212646 cuda_h.py:19] end gpu_group_list cost 0.0001583099365234375 seconds
DEBUG 01-15 16:10:29.212849.212849 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.213120.213120 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009090900421142578 seconds
DEBUG 01-15 16:10:29.213447.213447 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.214348.214348 cuda_h.py:19] end gpu_group_list cost 0.0001785755157470703 seconds
DEBUG 01-15 16:10:29.214521.214521 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.214688.214688 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 557420f7-3c1b-4038-bbdf-398b3616af49
DEBUG 01-15 16:10:29.217056.217056 cuda_h.py:19] end group_tensors cost 0.010404825210571289 seconds
DEBUG 01-15 16:10:29.217181.217181 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.221559.221559 cuda_h.py:19] end group pad cost 0.0037381649017333984 seconds
DEBUG 01-15 16:10:29.221541.221541 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.237601.237601 client.py:127] Model loaded
DEBUG 01-15 16:10:29.237108.237108 cuda_h.py:19] end wait_experts_multi_device cost 0.022569656372070312 seconds
DEBUG 01-15 16:10:29.237296.237296 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.240908.240908 cuda_h.py:19] end group_einsum cost 0.0185854434967041 seconds
DEBUG 01-15 16:10:29.240947.240947 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.244510.244510 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036787986755371094 seconds
DEBUG 01-15 16:10:29.244670.244670 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03933143615722656 seconds
DEBUG 01-15 16:10:29.245780.245780 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007813215255737305 seconds
DEBUG 01-15 16:10:29.245823.245823 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.246669.246669 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.246926.246926 cuda_h.py:19] end index_scatter cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:10:29.247725.247725 cuda_h.py:19] end cpuoutputsdeal cost 0.0015745162963867188 seconds
DEBUG 01-15 16:10:29.247396.247396 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.247106.247106 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.247330.247330 cuda_h.py:19] end gpu_group_tensor cost 0.0001354217529296875 seconds
DEBUG 01-15 16:10:29.247231.247231 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.247389.247389 cuda_h.py:19] end gpu_group_tensor cost 0.0001227855682373047 seconds
DEBUG 01-15 16:10:29.247909.247909 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.248361.248361 cuda_h.py:19] end gpu_group_einsum cost 0.000598907470703125 seconds
DEBUG 01-15 16:10:29.248421.248421 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.249036.249036 cuda_h.py:19] end gpu_group_einsum cost 0.0004956722259521484 seconds
DEBUG 01-15 16:10:29.249789.249789 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.249044.249044 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.249398.249398 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002129077911376953 seconds
DEBUG 01-15 16:10:29.249737.249737 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.249979.249979 cuda_h.py:19] end concat_expert_out cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:10:29.249153.249153 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.249076.249076 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:29.250548.250548 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008563995361328125 seconds
DEBUG 01-15 16:10:29.250054.250054 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.250705.250705 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.250909.250909 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012040138244628906 seconds
DEBUG 01-15 16:10:29.250519.250519 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.250250.250250 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:29.250994.250994 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.250487.250487 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:29.250819.250819 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004584789276123047 seconds
DEBUG 01-15 16:10:29.250345.250345 cuda_h.py:19] end gpu_experts_multi_device cost 0.03975343704223633 seconds
DEBUG 01-15 16:10:29.250301.250301 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.049915313720703125 seconds
DEBUG 01-15 16:10:29.251042.251042 cuda_h.py:19] end prefill_layer cost 0.055622100830078125 seconds
DEBUG 01-15 16:10:29.251885.251885 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:10:29.251588.251588 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.251244.251244 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:10:29.251093.251093 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:29.251372.251372 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:29.251891.251891 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:10:29.251693.251693 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:10:29.251058.251058 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.251259.251259 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.251209.251209 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:29.251769.251769 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:29.251136.251136 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:29.252795.252795 cuda_h.py:19] end allocate_cuda_memory cost 0.0002727508544921875 seconds
DEBUG 01-15 16:10:29.252811.252811 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:29.252912.252912 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.252590.252590 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.252637.252637 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6c592031-a015-4089-9a52-81d69b7cc3b6
DEBUG 01-15 16:10:29.252787.252787 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.252955.252955 cuda_h.py:10] start self_attn
INFO 01-15 16:10:29.254992.254992 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6c592031-a015-4089-9a52-81d69b7cc3b6
DEBUG 01-15 16:10:29.254888.254888 cuda_h.py:19] end load_into_gpu_async cost 0.0020532608032226562 seconds
DEBUG 01-15 16:10:29.254929.254929 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:29.254621.254621 cuda_h.py:19] end restore_tensors2 cost 9.250640869140625e-05 seconds
DEBUG 01-15 16:10:29.254192.254192 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0026988983154296875 seconds
INFO 01-15 16:10:29.254287.254287 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6c592031-a015-4089-9a52-81d69b7cc3b6
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.256289.256289 cuda_h.py:19] end self_attn cost 0.0031921863555908203 seconds
DEBUG 01-15 16:10:29.256790.256790 cuda_h.py:19] end iln_self_attn_paln cost 0.004825115203857422 seconds
DEBUG 01-15 16:10:29.256481.256481 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:10:29.256720.256720 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.257487.257487 cuda_h.py:19] end gate cost 0.0007023811340332031 seconds
DEBUG 01-15 16:10:29.257800.257800 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.257635.257635 lmp.py:1912] 
DEBUG 01-15 16:10:29.257635.257635 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.257682.257682 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.257101.257101 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.257942.257942 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.257354.257354 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.257050.257050 lmp.py:1917] 
DEBUG 01-15 16:10:29.257050.257050 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.257461.257461 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.257833.257833 lmp.py:1935]   Expert 25 |     12 | CPU
DEBUG 01-15 16:10:29.257483.257483 lmp.py:1935]   Expert 48 |     32 | CPU
DEBUG 01-15 16:10:29.257179.257179 lmp.py:1935]   Expert 45 |     36 | CPU
DEBUG 01-15 16:10:29.257345.257345 lmp.py:1935]   Expert  9 |     64 | CPU
DEBUG 01-15 16:10:29.257180.257180 lmp.py:1935]   Expert  0 |     81 | CPU
DEBUG 01-15 16:10:29.257538.257538 lmp.py:1935]   Expert 43 |     82 | CPU
DEBUG 01-15 16:10:29.257181.257181 lmp.py:1935]   Expert 54 |     86 | CPU
DEBUG 01-15 16:10:29.258586.258586 lmp.py:1935]   Expert 57 |     87 | CPU
DEBUG 01-15 16:10:29.258229.258229 lmp.py:1935]   Expert 20 |     88 | CPU
DEBUG 01-15 16:10:29.258633.258633 lmp.py:1935]   Expert 36 |     93 | CPU
DEBUG 01-15 16:10:29.258515.258515 lmp.py:1935]   Expert 47 |     93 | CPU
DEBUG 01-15 16:10:29.258873.258873 lmp.py:1935]   Expert  6 |     95 | CPU
DEBUG 01-15 16:10:29.258470.258470 lmp.py:1935]   Expert 62 |    101 | CPU
DEBUG 01-15 16:10:29.258351.258351 lmp.py:1935]   Expert 61 |    105 | CPU
DEBUG 01-15 16:10:29.258232.258232 lmp.py:1935]   Expert 13 |    106 | CPU
DEBUG 01-15 16:10:29.258875.258875 lmp.py:1935]   Expert 15 |    106 | CPU
DEBUG 01-15 16:10:29.258280.258280 lmp.py:1935]   Expert  1 |    108 | CPU
DEBUG 01-15 16:10:29.258446.258446 lmp.py:1935]   Expert 38 |    109 | CPU
DEBUG 01-15 16:10:29.258851.258851 lmp.py:1935]   Expert 50 |    109 | CPU
DEBUG 01-15 16:10:29.258778.258778 lmp.py:1935]   Expert 37 |    115 | CPU
DEBUG 01-15 16:10:29.258183.258183 lmp.py:1935]   Expert 14 |    119 | CPU
DEBUG 01-15 16:10:29.258270.258270 lmp.py:1935]   Expert 46 |    121 | CPU
DEBUG 01-15 16:10:29.258913.258913 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:10:29.258556.258556 lmp.py:1935]   Expert 21 |    137 | CPU
DEBUG 01-15 16:10:29.258676.258676 lmp.py:1935]   Expert  7 |    138 | CPU
DEBUG 01-15 16:10:29.258080.258080 lmp.py:1935]   Expert 44 |    143 | CPU
DEBUG 01-15 16:10:29.258962.258962 lmp.py:1935]   Expert 52 |    144 | CPU
DEBUG 01-15 16:10:29.258889.258889 lmp.py:1935]   Expert 10 |    153 | CPU
DEBUG 01-15 16:10:29.258340.258340 lmp.py:1935]   Expert 24 |    153 | CPU
DEBUG 01-15 16:10:29.258268.258268 lmp.py:1935]   Expert 11 |    155 | CPU
DEBUG 01-15 16:10:29.258196.258196 lmp.py:1935]   Expert 42 |    155 | CPU
DEBUG 01-15 16:10:29.258123.258123 lmp.py:1935]   Expert  2 |    167 | CPU
DEBUG 01-15 16:10:29.258720.258720 lmp.py:1935]   Expert 26 |    167 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258078.258078 lmp.py:1935]   Expert 35 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258198.258198 lmp.py:1935]   Expert 31 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258318.258318 lmp.py:1935]   Expert  3 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258438.258438 lmp.py:1935]   Expert 19 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258034.258034 lmp.py:1935]   Expert 32 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258631.258631 lmp.py:1935]   Expert 12 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258228.258228 lmp.py:1935]   Expert 56 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258493.258493 lmp.py:1935]   Expert 60 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258282.258282 lmp.py:1935]   Expert 40 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258879.258879 lmp.py:1935]   Expert 41 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258522.258522 lmp.py:1935]   Expert 53 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258417.258417 lmp.py:1935]   Expert 23 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258344.258344 lmp.py:1935]   Expert 16 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258749.258749 lmp.py:1935]   Expert 51 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258915.258915 lmp.py:1935]   Expert  8 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258558.258558 lmp.py:1935]   Expert 58 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258155.258155 lmp.py:1935]   Expert 59 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258274.258274 lmp.py:1935]   Expert  4 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258156.258156 lmp.py:1935]   Expert 55 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258799.258799 lmp.py:1935]   Expert 49 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258442.258442 lmp.py:1935]   Expert 29 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258608.258608 lmp.py:1935]   Expert 34 |    280 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258774.258774 lmp.py:1935]   Expert 18 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258940.258940 lmp.py:1935]   Expert 63 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258106.258106 lmp.py:1935]   Expert 27 |    355 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258511.258511 lmp.py:1935]   Expert 39 |    376 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258677.258677 lmp.py:1935]   Expert 17 |    395 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.258558.258558 lmp.py:1935]   Expert 22 |    429 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.258201.258201 lmp.py:1935]   Expert 33 |    451 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.259321.259321 lmp.py:1935]   Expert 30 |    457 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.259441.259441 lmp.py:1935]   Expert  5 |    713 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.259130.259130 lmp.py:1937] 
DEBUG 01-15 16:10:29.259130.259130 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.259296.259296 lmp.py:1938]   CPU:   3429 tokens
DEBUG 01-15 16:10:29.259701.259701 lmp.py:1942]   cuda:1:   4468 tokens (16 experts)
DEBUG 01-15 16:10:29.259629.259629 lmp.py:1942]   cuda:2:   4391 tokens (16 experts)
DEBUG 01-15 16:10:29.259841.259841 lmp.py:1943]   Total GPU:   8859 tokens
DEBUG 01-15 16:10:29.259054.259054 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.259054.259054 lmp.py:1944] 
DEBUG 01-15 16:10:29.259227.259227 cuda_h.py:19] end experts_map_get cost 0.0018270015716552734 seconds
DEBUG 01-15 16:10:29.259553.259553 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.259879.259879 lmp.py:1953] 
DEBUG 01-15 16:10:29.259879.259879 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.259900.259900 cuda_h.py:19] end cpu_experts_submit cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:29.259689.259689 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.259618.259618 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.260467.260467 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.34.gate_proj.weight': 0, 'model.layers.22.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.49.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.49.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.49.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.22.mlp.experts.32.gate_proj.weight': 0, 'model.layers.22.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.59.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.59.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.59.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.51.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.51.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.51.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(26701987840, 5767168, 0, 0), (26707755008, 5767168, 5767168, 0), (26696220672, 5767168, 11534336, 0), (26165641216, 5767168, 17301504, 0), (26171408384, 5767168, 23068672, 0), (26159874048, 5767168, 28835840, 0), (26182942720, 5767168, 34603008, 0), (26188709888, 5767168, 40370176, 0), (26177175552, 5767168, 46137344, 0), (26200244224, 5767168, 51904512, 0), (26206011392, 5767168, 57671680, 0), (26194477056, 5767168, 63438848, 0), (26788495360, 5767168, 69206016, 0), (26794262528, 5767168, 74973184, 0), (26782728192, 5767168, 80740352, 0), (26252148736, 5767168, 86507520, 0), (26257915904, 5767168, 92274688, 0), (26246381568, 5767168, 98041856, 0), (26823098368, 5767168, 103809024, 0), (26828865536, 5767168, 109576192, 0), (26817331200, 5767168, 115343360, 0), (26321354752, 5767168, 121110528, 0), (26327121920, 5767168, 126877696, 0), (26315587584, 5767168, 132644864, 0), (26650083328, 5767168, 138412032, 0), (26655850496, 5767168, 144179200, 0), (26644316160, 5767168, 149946368, 0), (26390560768, 5767168, 155713536, 0), (26396327936, 5767168, 161480704, 0), (26384793600, 5767168, 167247872, 0), (26961510400, 5767168, 173015040, 0), (26967277568, 5767168, 178782208, 0), (26955743232, 5767168, 184549376, 0), (26494369792, 5767168, 190316544, 0), (26500136960, 5767168, 196083712, 0), (26488602624, 5767168, 201850880, 0), (26511671296, 5767168, 207618048, 0), (26517438464, 5767168, 213385216, 0), (26505904128, 5767168, 219152384, 0), (26563575808, 5767168, 224919552, 0), (26569342976, 5767168, 230686720, 0), (26557808640, 5767168, 236453888, 0), (27151826944, 5767168, 242221056, 0), (27157594112, 5767168, 247988224, 0), (27146059776, 5767168, 253755392, 0), (27203731456, 5767168, 259522560, 0), (27209498624, 5767168, 265289728, 0), (27197964288, 5767168, 271056896, 0)], 2: [(26667384832, 5767168, 0, 0), (26673152000, 5767168, 5767168, 0), (26661617664, 5767168, 11534336, 0), (26684686336, 5767168, 17301504, 0), (26690453504, 5767168, 23068672, 0), (26678919168, 5767168, 28835840, 0), (26719289344, 5767168, 34603008, 0), (26725056512, 5767168, 40370176, 0), (26713522176, 5767168, 46137344, 0), (26805796864, 5767168, 51904512, 0), (26811564032, 5767168, 57671680, 0), (26800029696, 5767168, 63438848, 0), (27134525440, 5767168, 69206016, 0), (27140292608, 5767168, 74973184, 0), (27128758272, 5767168, 80740352, 0), (26407862272, 5767168, 86507520, 0), (26413629440, 5767168, 92274688, 0), (26402095104, 5767168, 98041856, 0), (26425163776, 5767168, 103809024, 0), (26430930944, 5767168, 109576192, 0), (26419396608, 5767168, 115343360, 0), (26996113408, 5767168, 121110528, 0), (27001880576, 5767168, 126877696, 0), (26990346240, 5767168, 132644864, 0), (26442465280, 5767168, 138412032, 0), (26448232448, 5767168, 144179200, 0), (26436698112, 5767168, 149946368, 0), (27030716416, 5767168, 155713536, 0), (27036483584, 5767168, 161480704, 0), (27024949248, 5767168, 167247872, 0), (27065319424, 5767168, 173015040, 0), (27071086592, 5767168, 178782208, 0), (27059552256, 5767168, 184549376, 0), (27082620928, 5767168, 190316544, 0), (27088388096, 5767168, 196083712, 0), (27076853760, 5767168, 201850880, 0), (27117223936, 5767168, 207618048, 0), (27122991104, 5767168, 213385216, 0), (27111456768, 5767168, 219152384, 0), (26580877312, 5767168, 224919552, 0), (26586644480, 5767168, 230686720, 0), (26575110144, 5767168, 236453888, 0), (26615480320, 5767168, 242221056, 0), (26621247488, 5767168, 247988224, 0), (26609713152, 5767168, 253755392, 0), (26632781824, 5767168, 259522560, 0), (26638548992, 5767168, 265289728, 0), (26627014656, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b054df80>, 2: <capsule object NULL at 0x7a51b0646040>}
DEBUG 01-15 16:10:29.261638.261638 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.261044.261044 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.261177.261177 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6fdec3e2-59a9-440f-a226-ff0be68f695a
DEBUG 01-15 16:10:29.261919.261919 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.261565.261565 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:29.262492.262492 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:29.262565.262565 client.py:127] Model loaded
DEBUG 01-15 16:10:29.262342.262342 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.263255.263255 cuda_h.py:19] end restore2model cost 0.0003612041473388672 seconds
DEBUG 01-15 16:10:29.263979.263979 cuda_h.py:19] end sllm_worker_task cost 0.011400461196899414 seconds
DEBUG 01-15 16:10:29.263363.263363 cuda_h.py:19] end move_flatidxs cost 0.0008280277252197266 seconds
DEBUG 01-15 16:10:29.263378.263378 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:29.264960.264960 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6fdec3e2-59a9-440f-a226-ff0be68f695a
DEBUG 01-15 16:10:29.265112.265112 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005635738372802734 seconds
DEBUG 01-15 16:10:29.265446.265446 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.267799.267799 cuda_h.py:19] end restore2model cost 0.002614259719848633 seconds
DEBUG 01-15 16:10:29.267941.267941 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008481025695800781 seconds
DEBUG 01-15 16:10:29.267167.267167 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.268720.268720 cuda_h.py:19] end gpu_sexperts cost 0.00027060508728027344 seconds
DEBUG 01-15 16:10:29.268026.268026 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:29.268611.268611 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:10:29.268737.268737 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.268109.268109 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.269747.269747 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008251667022705078 seconds
DEBUG 01-15 16:10:29.269213.269213 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.269007.269007 cuda_h.py:19] end gpu_group_list cost 0.0001709461212158203 seconds
DEBUG 01-15 16:10:29.270297.270297 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.271030.271030 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008609294891357422 seconds
DEBUG 01-15 16:10:29.271065.271065 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.271628.271628 cuda_h.py:19] end gpu_group_list cost 0.00017333030700683594 seconds
DEBUG 01-15 16:10:29.271761.271761 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.271167.271167 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6fdec3e2-59a9-440f-a226-ff0be68f695a
DEBUG 01-15 16:10:29.272056.272056 cuda_h.py:19] end group_tensors cost 0.00874948501586914 seconds
DEBUG 01-15 16:10:29.272824.272824 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.276861.276861 cuda_h.py:19] end group pad cost 0.003626108169555664 seconds
DEBUG 01-15 16:10:29.276412.276412 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.292009.292009 client.py:127] Model loaded
DEBUG 01-15 16:10:29.292972.292972 cuda_h.py:19] end wait_experts_multi_device cost 0.02085137367248535 seconds
DEBUG 01-15 16:10:29.292439.292439 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.295560.295560 cuda_h.py:19] end group_einsum cost 0.018552064895629883 seconds
DEBUG 01-15 16:10:29.295134.295134 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.298567.298567 cuda_h.py:19] end get_outputs_cpu1 cost 0.0033783912658691406 seconds
DEBUG 01-15 16:10:29.299329.299329 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0373988151550293 seconds
DEBUG 01-15 16:10:29.299072.299072 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0067844390869140625 seconds
DEBUG 01-15 16:10:29.299764.299764 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.301015.301015 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.301119.301119 cuda_h.py:19] end index_scatter cost 7.176399230957031e-05 seconds
DEBUG 01-15 16:10:29.301971.301971 cuda_h.py:19] end cpuoutputsdeal cost 0.0015902519226074219 seconds
DEBUG 01-15 16:10:29.301550.301550 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.301021.301021 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.301391.301391 cuda_h.py:19] end gpu_group_tensor cost 0.00013637542724609375 seconds
DEBUG 01-15 16:10:29.301769.301769 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.302496.302496 cuda_h.py:19] end gpu_group_tensor cost 0.00012159347534179688 seconds
DEBUG 01-15 16:10:29.302963.302963 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.302541.302541 cuda_h.py:19] end gpu_group_einsum cost 0.0005869865417480469 seconds
DEBUG 01-15 16:10:29.302678.302678 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.303915.303915 cuda_h.py:19] end gpu_group_einsum cost 0.0005011558532714844 seconds
DEBUG 01-15 16:10:29.303423.303423 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.303579.303579 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.303435.303435 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019860267639160156 seconds
DEBUG 01-15 16:10:29.303105.303105 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.304479.304479 cuda_h.py:19] end concat_expert_out cost 6.461143493652344e-05 seconds
DEBUG 01-15 16:10:29.304984.304984 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.304477.304477 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:29.304518.304518 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008258819580078125 seconds
DEBUG 01-15 16:10:29.304085.304085 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.304259.304259 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.304092.304092 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001277923583984375 seconds
DEBUG 01-15 16:10:29.304748.304748 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.304950.304950 cuda_h.py:19] end concat_expert_out cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:29.304501.304501 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.305610.305610 cuda_h.py:19] end index_scatter cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:10:29.305273.305273 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00045561790466308594 seconds
DEBUG 01-15 16:10:29.305269.305269 cuda_h.py:19] end gpu_experts_multi_device cost 0.03681540489196777 seconds
DEBUG 01-15 16:10:29.305317.305317 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.048688411712646484 seconds
DEBUG 01-15 16:10:29.305635.305635 cuda_h.py:19] end prefill_layer cost 0.054181575775146484 seconds
DEBUG 01-15 16:10:29.305670.305670 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:10:29.305896.305896 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.305122.305122 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:10:29.305540.305540 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:29.305866.305866 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:29.305100.305100 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 3.981590270996094e-05 seconds
DEBUG 01-15 16:10:29.305764.305764 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:29.305600.305600 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 0.00016188621520996094 seconds
DEBUG 01-15 16:10:29.306431.306431 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:29.306453.306453 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.306608.306608 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:29.306944.306944 cuda_h.py:19] end allocate_cuda_memory cost 0.0002586841583251953 seconds
DEBUG 01-15 16:10:29.306325.306325 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.306738.306738 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:29.306503.306503 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.306047.306047 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.306757.306757 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f6978757-3b66-4629-abd2-aa58ecf55f9e
DEBUG 01-15 16:10:29.306907.306907 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.307044.307044 cuda_h.py:10] start self_attn
INFO 01-15 16:10:29.308506.308506 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f6978757-3b66-4629-abd2-aa58ecf55f9e
DEBUG 01-15 16:10:29.308634.308634 cuda_h.py:19] end load_into_gpu_async cost 0.0015358924865722656 seconds
DEBUG 01-15 16:10:29.308529.308529 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:29.308149.308149 cuda_h.py:19] end restore_tensors2 cost 0.00011324882507324219 seconds
DEBUG 01-15 16:10:29.308627.308627 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0023949146270751953 seconds
INFO 01-15 16:10:29.308537.308537 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f6978757-3b66-4629-abd2-aa58ecf55f9e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.310861.310861 cuda_h.py:19] end self_attn cost 0.0035507678985595703 seconds
DEBUG 01-15 16:10:29.311096.311096 cuda_h.py:19] end iln_self_attn_paln cost 0.005044460296630859 seconds
DEBUG 01-15 16:10:29.311356.311356 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:10:29.311642.311642 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.312519.312519 cuda_h.py:19] end gate cost 0.0008184909820556641 seconds
DEBUG 01-15 16:10:29.312593.312593 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.312023.312023 lmp.py:1912] 
DEBUG 01-15 16:10:29.312023.312023 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.312117.312117 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.312767.312767 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.312079.312079 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.312722.312722 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.312888.312888 lmp.py:1917] 
DEBUG 01-15 16:10:29.312888.312888 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.312054.312054 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.312658.312658 lmp.py:1935]   Expert  5 |     13 | CPU
DEBUG 01-15 16:10:29.312301.312301 lmp.py:1935]   Expert 56 |     35 | CPU
DEBUG 01-15 16:10:29.312467.312467 lmp.py:1935]   Expert 27 |     84 | CPU
DEBUG 01-15 16:10:29.312156.312156 lmp.py:1935]   Expert 16 |     87 | CPU
DEBUG 01-15 16:10:29.312607.312607 lmp.py:1935]   Expert 17 |     88 | CPU
DEBUG 01-15 16:10:29.312058.312058 lmp.py:1935]   Expert 40 |     96 | CPU
DEBUG 01-15 16:10:29.312608.312608 lmp.py:1935]   Expert 51 |    102 | CPU
DEBUG 01-15 16:10:29.312298.312298 lmp.py:1935]   Expert 63 |    104 | CPU
DEBUG 01-15 16:10:29.312987.312987 lmp.py:1935]   Expert 28 |    107 | CPU
DEBUG 01-15 16:10:29.312438.312438 lmp.py:1935]   Expert 49 |    108 | CPU
DEBUG 01-15 16:10:29.312889.312889 lmp.py:1935]   Expert 53 |    108 | CPU
DEBUG 01-15 16:10:29.312008.312008 lmp.py:1935]   Expert  7 |    113 | CPU
DEBUG 01-15 16:10:29.312890.312890 lmp.py:1935]   Expert 37 |    123 | CPU
DEBUG 01-15 16:10:29.312294.312294 lmp.py:1935]   Expert 38 |    123 | CPU
DEBUG 01-15 16:10:29.313460.313460 lmp.py:1935]   Expert 47 |    123 | CPU
DEBUG 01-15 16:10:29.313627.313627 lmp.py:1935]   Expert 62 |    123 | CPU
DEBUG 01-15 16:10:29.313031.313031 lmp.py:1935]   Expert 58 |    128 | CPU
DEBUG 01-15 16:10:29.313197.313197 lmp.py:1935]   Expert 11 |    129 | CPU
DEBUG 01-15 16:10:29.313840.313840 lmp.py:1935]   Expert 57 |    137 | CPU
DEBUG 01-15 16:10:29.313245.313245 lmp.py:1935]   Expert  1 |    146 | CPU
DEBUG 01-15 16:10:29.313365.313365 lmp.py:1935]   Expert 39 |    146 | CPU
DEBUG 01-15 16:10:29.313292.313292 lmp.py:1935]   Expert 14 |    148 | CPU
DEBUG 01-15 16:10:29.313174.313174 lmp.py:1935]   Expert 52 |    153 | CPU
DEBUG 01-15 16:10:29.313578.313578 lmp.py:1935]   Expert 23 |    156 | CPU
DEBUG 01-15 16:10:29.313221.313221 lmp.py:1935]   Expert 25 |    156 | CPU
DEBUG 01-15 16:10:29.313626.313626 lmp.py:1935]   Expert 33 |    162 | CPU
DEBUG 01-15 16:10:29.313746.313746 lmp.py:1935]   Expert 21 |    165 | CPU
DEBUG 01-15 16:10:29.313389.313389 lmp.py:1935]   Expert 60 |    170 | CPU
DEBUG 01-15 16:10:29.313793.313793 lmp.py:1935]   Expert  6 |    172 | CPU
DEBUG 01-15 16:10:29.313046.313046 lmp.py:1935]   Expert 45 |    176 | CPU
DEBUG 01-15 16:10:29.313166.313166 lmp.py:1935]   Expert 19 |    180 | CPU
DEBUG 01-15 16:10:29.313856.313856 lmp.py:1935]   Expert 44 |    181 | CPU
DEBUG 01-15 16:10:29.313167.313167 lmp.py:1935]   Expert  4 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313002.313002 lmp.py:1935]   Expert 12 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313838.313838 lmp.py:1935]   Expert 30 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313719.313719 lmp.py:1935]   Expert 31 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313839.313839 lmp.py:1935]   Expert 55 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313720.313720 lmp.py:1935]   Expert  3 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313078.313078 lmp.py:1935]   Expert 36 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313721.313721 lmp.py:1935]   Expert  9 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313841.313841 lmp.py:1935]   Expert  0 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313723.313723 lmp.py:1935]   Expert 22 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313842.313842 lmp.py:1935]   Expert 34 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313485.313485 lmp.py:1935]   Expert 41 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313844.313844 lmp.py:1935]   Expert 26 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313963.313963 lmp.py:1935]   Expert 54 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313606.313606 lmp.py:1935]   Expert 43 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313249.313249 lmp.py:1935]   Expert 59 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313369.313369 lmp.py:1935]   Expert 18 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313012.313012 lmp.py:1935]   Expert 13 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313622.313622 lmp.py:1935]   Expert 20 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313550.313550 lmp.py:1935]   Expert 15 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313431.313431 lmp.py:1935]   Expert 50 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313598.313598 lmp.py:1935]   Expert 24 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313525.313525 lmp.py:1935]   Expert 42 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313691.313691 lmp.py:1935]   Expert 29 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313858.313858 lmp.py:1935]   Expert 61 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313454.313454 lmp.py:1935]   Expert 35 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313859.313859 lmp.py:1935]   Expert 32 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313787.313787 lmp.py:1935]   Expert  2 |    337 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313714.313714 lmp.py:1935]   Expert  8 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313642.313642 lmp.py:1935]   Expert 10 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313047.313047 lmp.py:1935]   Expert 46 |    423 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.313213.313213 lmp.py:1935]   Expert 48 |    447 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.313902.313902 lmp.py:1937] 
DEBUG 01-15 16:10:29.313902.313902 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.313068.313068 lmp.py:1938]   CPU:   4042 tokens
DEBUG 01-15 16:10:29.313949.313949 lmp.py:1942]   cuda:1:   4125 tokens (16 experts)
DEBUG 01-15 16:10:29.313354.313354 lmp.py:1942]   cuda:2:   4121 tokens (16 experts)
DEBUG 01-15 16:10:29.314043.314043 lmp.py:1943]   Total GPU:   8246 tokens
DEBUG 01-15 16:10:29.314256.314256 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.314256.314256 lmp.py:1944] 
DEBUG 01-15 16:10:29.314382.314382 cuda_h.py:19] end experts_map_get cost 0.0017795562744140625 seconds
DEBUG 01-15 16:10:29.314948.314948 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.314532.314532 lmp.py:1953] 
DEBUG 01-15 16:10:29.314532.314532 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.314507.314507 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:29.314773.314773 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.314841.314841 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.315601.315601 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.32.gate_proj.weight': 0, 'model.layers.23.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.3.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.3.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.3.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.43.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.43.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.43.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.12.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.12.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.12.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.13.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.13.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.13.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.48.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.48.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.48.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.20.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.20.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.20.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.22.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.22.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.22.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.24.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.24.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.24.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.23.mlp.experts.0.gate_proj.weight': 0, 'model.layers.23.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.61.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.61.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.61.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(27774681088, 5767168, 0, 0), (27780448256, 5767168, 5767168, 0), (27768913920, 5767168, 11534336, 0), (27826585600, 5767168, 17301504, 0), (27832352768, 5767168, 23068672, 0), (27820818432, 5767168, 28835840, 0), (27843887104, 5767168, 34603008, 0), (27849654272, 5767168, 40370176, 0), (27838119936, 5767168, 46137344, 0), (27272937472, 5767168, 51904512, 0), (27278704640, 5767168, 57671680, 0), (27267170304, 5767168, 63438848, 0), (27359444992, 5767168, 69206016, 0), (27365212160, 5767168, 74973184, 0), (27353677824, 5767168, 80740352, 0), (27930394624, 5767168, 86507520, 0), (27936161792, 5767168, 92274688, 0), (27924627456, 5767168, 98041856, 0), (27947696128, 5767168, 103809024, 0), (27953463296, 5767168, 109576192, 0), (27941928960, 5767168, 115343360, 0), (27964997632, 5767168, 121110528, 0), (27970764800, 5767168, 126877696, 0), (27959230464, 5767168, 132644864, 0), (27428651008, 5767168, 138412032, 0), (27434418176, 5767168, 144179200, 0), (27422883840, 5767168, 149946368, 0), (27445952512, 5767168, 155713536, 0), (27451719680, 5767168, 161480704, 0), (27440185344, 5767168, 167247872, 0), (28051505152, 5767168, 173015040, 0), (28057272320, 5767168, 178782208, 0), (28045737984, 5767168, 184549376, 0), (27567063040, 5767168, 190316544, 0), (27572830208, 5767168, 196083712, 0), (27561295872, 5767168, 201850880, 0), (28155314176, 5767168, 207618048, 0), (28161081344, 5767168, 213385216, 0), (28149547008, 5767168, 219152384, 0), (27601666048, 5767168, 224919552, 0), (27607433216, 5767168, 230686720, 0), (27595898880, 5767168, 236453888, 0), (27636269056, 5767168, 242221056, 0), (27642036224, 5767168, 247988224, 0), (27630501888, 5767168, 253755392, 0), (27757379584, 5767168, 259522560, 0), (27763146752, 5767168, 265289728, 0), (27751612416, 5767168, 271056896, 0)], 2: [(27221032960, 5767168, 0, 0), (27226800128, 5767168, 5767168, 0), (27215265792, 5767168, 11534336, 0), (27255635968, 5767168, 17301504, 0), (27261403136, 5767168, 23068672, 0), (27249868800, 5767168, 28835840, 0), (27809284096, 5767168, 34603008, 0), (27815051264, 5767168, 40370176, 0), (27803516928, 5767168, 46137344, 0), (27290238976, 5767168, 51904512, 0), (27296006144, 5767168, 57671680, 0), (27284471808, 5767168, 63438848, 0), (27376746496, 5767168, 69206016, 0), (27382513664, 5767168, 74973184, 0), (27370979328, 5767168, 80740352, 0), (27394048000, 5767168, 86507520, 0), (27399815168, 5767168, 92274688, 0), (27388280832, 5767168, 98041856, 0), (28016902144, 5767168, 103809024, 0), (28022669312, 5767168, 109576192, 0), (28011134976, 5767168, 115343360, 0), (27480555520, 5767168, 121110528, 0), (27486322688, 5767168, 126877696, 0), (27474788352, 5767168, 132644864, 0), (28086108160, 5767168, 138412032, 0), (28091875328, 5767168, 144179200, 0), (28080340992, 5767168, 149946368, 0), (28276424704, 5767168, 155713536, 0), (28282191872, 5767168, 161480704, 0), (28270657536, 5767168, 167247872, 0), (27532460032, 5767168, 173015040, 0), (27538227200, 5767168, 178782208, 0), (27526692864, 5767168, 184549376, 0), (28172615680, 5767168, 190316544, 0), (28178382848, 5767168, 196083712, 0), (28166848512, 5767168, 201850880, 0), (27670872064, 5767168, 207618048, 0), (27676639232, 5767168, 213385216, 0), (27665104896, 5767168, 219152384, 0), (28241821696, 5767168, 224919552, 0), (28247588864, 5767168, 230686720, 0), (28236054528, 5767168, 236453888, 0), (27722776576, 5767168, 242221056, 0), (27728543744, 5767168, 247988224, 0), (27717009408, 5767168, 253755392, 0), (27740078080, 5767168, 259522560, 0), (27745845248, 5767168, 265289728, 0), (27734310912, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b06461f0>, 2: <capsule object NULL at 0x7a51b0646610>}
DEBUG 01-15 16:10:29.315255.315255 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.315661.315661 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.315318.315318 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ebe03639-3430-47ad-9984-abb546996d0f
DEBUG 01-15 16:10:29.315291.315291 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.315394.315394 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:29.315005.315005 client.py:127] Model loaded
DEBUG 01-15 16:10:29.315404.315404 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.316397.316397 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:29.316867.316867 cuda_h.py:19] end restore2model cost 0.0003464221954345703 seconds
DEBUG 01-15 16:10:29.316014.316014 cuda_h.py:19] end sllm_worker_task cost 0.010309934616088867 seconds
INFO 01-15 16:10:29.316529.316529 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ebe03639-3430-47ad-9984-abb546996d0f
DEBUG 01-15 16:10:29.316937.316937 cuda_h.py:19] end move_flatidxs cost 0.0008318424224853516 seconds
DEBUG 01-15 16:10:29.316236.316236 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:29.317589.317589 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0027341842651367188 seconds
DEBUG 01-15 16:10:29.317207.317207 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.319809.319809 cuda_h.py:19] end restore2model cost 0.002518892288208008 seconds
DEBUG 01-15 16:10:29.319453.319453 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0054662227630615234 seconds
DEBUG 01-15 16:10:29.319580.319580 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.320994.320994 cuda_h.py:19] end gpu_sexperts cost 0.0002753734588623047 seconds
DEBUG 01-15 16:10:29.320678.320678 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:29.320593.320593 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:10:29.320335.320335 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.320323.320323 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.321431.321431 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008220672607421875 seconds
DEBUG 01-15 16:10:29.321373.321373 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.321235.321235 cuda_h.py:19] end gpu_group_list cost 0.00018644332885742188 seconds
DEBUG 01-15 16:10:29.322147.322147 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.322893.322893 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008423328399658203 seconds
DEBUG 01-15 16:10:29.323689.323689 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.323975.323975 cuda_h.py:19] end gpu_group_list cost 0.00018286705017089844 seconds
DEBUG 01-15 16:10:29.323649.323649 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.323240.323240 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ebe03639-3430-47ad-9984-abb546996d0f
DEBUG 01-15 16:10:29.326905.326905 cuda_h.py:19] end group_tensors cost 0.00905752182006836 seconds
DEBUG 01-15 16:10:29.326406.326406 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.330945.330945 cuda_h.py:19] end group pad cost 0.0037860870361328125 seconds
DEBUG 01-15 16:10:29.330781.330781 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.342007.342007 client.py:127] Model loaded
DEBUG 01-15 16:10:29.343775.343775 cuda_h.py:19] end wait_experts_multi_device cost 0.019300222396850586 seconds
DEBUG 01-15 16:10:29.343590.343590 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.350330.350330 cuda_h.py:19] end group_einsum cost 0.020055294036865234 seconds
DEBUG 01-15 16:10:29.350394.350394 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.354031.354031 cuda_h.py:19] end get_outputs_cpu1 cost 0.00389862060546875 seconds
DEBUG 01-15 16:10:29.355105.355105 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03975510597229004 seconds
DEBUG 01-15 16:10:29.356532.356532 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012820005416870117 seconds
DEBUG 01-15 16:10:29.356389.356389 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.357490.357490 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.357800.357800 cuda_h.py:19] end index_scatter cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:10:29.357135.357135 cuda_h.py:19] end cpuoutputsdeal cost 0.0016713142395019531 seconds
DEBUG 01-15 16:10:29.357807.357807 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.358146.358146 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.358469.358469 cuda_h.py:19] end gpu_group_tensor cost 0.0001380443572998047 seconds
DEBUG 01-15 16:10:29.358325.358325 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.358913.358913 cuda_h.py:19] end gpu_group_tensor cost 0.00012350082397460938 seconds
DEBUG 01-15 16:10:29.358048.358048 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.359089.359089 cuda_h.py:19] end gpu_group_einsum cost 0.0005774497985839844 seconds
DEBUG 01-15 16:10:29.359995.359995 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.359066.359066 cuda_h.py:19] end gpu_group_einsum cost 0.00048422813415527344 seconds
DEBUG 01-15 16:10:29.359189.359189 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.360869.360869 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.360678.360678 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020051002502441406 seconds
DEBUG 01-15 16:10:29.360110.360110 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.360750.360750 cuda_h.py:19] end concat_expert_out cost 8.344650268554688e-05 seconds
DEBUG 01-15 16:10:29.360831.360831 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.360708.360708 cuda_h.py:19] end index_scatter cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:29.360067.360067 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008411407470703125 seconds
DEBUG 01-15 16:10:29.360666.360666 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.360217.360217 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.361574.361574 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001266002655029297 seconds
DEBUG 01-15 16:10:29.361469.361469 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.361816.361816 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:29.361228.361228 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.361575.361575 cuda_h.py:19] end index_scatter cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:10:29.361477.361477 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00045800209045410156 seconds
DEBUG 01-15 16:10:29.361526.361526 cuda_h.py:19] end gpu_experts_multi_device cost 0.04127001762390137 seconds
DEBUG 01-15 16:10:29.361528.361528 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.050200700759887695 seconds
DEBUG 01-15 16:10:29.361461.361461 cuda_h.py:19] end prefill_layer cost 0.056215763092041016 seconds
DEBUG 01-15 16:10:29.361920.361920 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:10:29.361146.361146 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.362611.362611 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:10:29.362314.362314 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:29.362924.362924 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:29.362111.362111 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 4.029273986816406e-05 seconds
DEBUG 01-15 16:10:29.362914.362914 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:10:29.362994.362994 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.362626.362626 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.362880.362880 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:29.362433.362433 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:29.362959.362959 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:29.362844.362844 cuda_h.py:19] end allocate_cuda_memory cost 0.0002961158752441406 seconds
DEBUG 01-15 16:10:29.362608.362608 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:29.362609.362609 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.362723.362723 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.363956.363956 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 70e445c5-4d34-49e1-8311-c92c0c8a13cf
DEBUG 01-15 16:10:29.363106.363106 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.363235.363235 cuda_h.py:10] start self_attn
INFO 01-15 16:10:29.364129.364129 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 70e445c5-4d34-49e1-8311-c92c0c8a13cf
DEBUG 01-15 16:10:29.364594.364594 cuda_h.py:19] end load_into_gpu_async cost 0.00153350830078125 seconds
DEBUG 01-15 16:10:29.364489.364489 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:29.364745.364745 cuda_h.py:19] end restore_tensors2 cost 8.821487426757812e-05 seconds
DEBUG 01-15 16:10:29.364316.364316 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021848678588867188 seconds
INFO 01-15 16:10:29.364994.364994 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 70e445c5-4d34-49e1-8311-c92c0c8a13cf
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.367251.367251 cuda_h.py:19] end self_attn cost 0.0035583972930908203 seconds
DEBUG 01-15 16:10:29.367539.367539 cuda_h.py:19] end iln_self_attn_paln cost 0.005179882049560547 seconds
DEBUG 01-15 16:10:29.367415.367415 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:10:29.367224.367224 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.368747.368747 cuda_h.py:19] end gate cost 0.0007336139678955078 seconds
DEBUG 01-15 16:10:29.368868.368868 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.368926.368926 lmp.py:1912] 
DEBUG 01-15 16:10:29.368926.368926 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.368305.368305 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.368478.368478 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.368028.368028 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.368148.368148 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.368837.368837 lmp.py:1917] 
DEBUG 01-15 16:10:29.368837.368837 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.368003.368003 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.368607.368607 lmp.py:1935]   Expert 36 |     21 | CPU
DEBUG 01-15 16:10:29.368488.368488 lmp.py:1935]   Expert 35 |     28 | CPU
DEBUG 01-15 16:10:29.368178.368178 lmp.py:1935]   Expert 25 |     46 | CPU
DEBUG 01-15 16:10:29.368867.368867 lmp.py:1935]   Expert 46 |     49 | CPU
DEBUG 01-15 16:10:29.368318.368318 lmp.py:1935]   Expert 51 |     51 | CPU
DEBUG 01-15 16:10:29.368007.368007 lmp.py:1935]   Expert 16 |     58 | CPU
DEBUG 01-15 16:10:29.368696.368696 lmp.py:1935]   Expert 30 |     62 | CPU
DEBUG 01-15 16:10:29.368909.368909 lmp.py:1935]   Expert  0 |     65 | CPU
DEBUG 01-15 16:10:29.368598.368598 lmp.py:1935]   Expert 43 |     69 | CPU
DEBUG 01-15 16:10:29.368811.368811 lmp.py:1935]   Expert 47 |     70 | CPU
DEBUG 01-15 16:10:29.368261.368261 lmp.py:1935]   Expert 42 |     75 | CPU
DEBUG 01-15 16:10:29.368474.368474 lmp.py:1935]   Expert 44 |     75 | CPU
DEBUG 01-15 16:10:29.369547.369547 lmp.py:1935]   Expert 39 |     76 | CPU
DEBUG 01-15 16:10:29.369237.369237 lmp.py:1935]   Expert 55 |     77 | CPU
DEBUG 01-15 16:10:29.369449.369449 lmp.py:1935]   Expert  2 |     81 | CPU
DEBUG 01-15 16:10:29.369662.369662 lmp.py:1935]   Expert  4 |    107 | CPU
DEBUG 01-15 16:10:29.369874.369874 lmp.py:1935]   Expert 33 |    119 | CPU
DEBUG 01-15 16:10:29.369087.369087 lmp.py:1935]   Expert  6 |    122 | CPU
DEBUG 01-15 16:10:29.369537.369537 lmp.py:1935]   Expert 13 |    122 | CPU
DEBUG 01-15 16:10:29.369896.369896 lmp.py:1935]   Expert 48 |    123 | CPU
DEBUG 01-15 16:10:29.369015.369015 lmp.py:1935]   Expert 24 |    126 | CPU
DEBUG 01-15 16:10:29.369420.369420 lmp.py:1935]   Expert 61 |    126 | CPU
DEBUG 01-15 16:10:29.369825.369825 lmp.py:1935]   Expert 56 |    131 | CPU
DEBUG 01-15 16:10:29.369706.369706 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:10:29.369826.369826 lmp.py:1935]   Expert 29 |    136 | CPU
DEBUG 01-15 16:10:29.369992.369992 lmp.py:1935]   Expert 54 |    139 | CPU
DEBUG 01-15 16:10:29.369920.369920 lmp.py:1935]   Expert  9 |    140 | CPU
DEBUG 01-15 16:10:29.369338.369338 lmp.py:1935]   Expert 38 |    140 | CPU
DEBUG 01-15 16:10:29.369789.369789 lmp.py:1935]   Expert  7 |    143 | CPU
DEBUG 01-15 16:10:29.369478.369478 lmp.py:1935]   Expert 20 |    144 | CPU
DEBUG 01-15 16:10:29.369167.369167 lmp.py:1935]   Expert 59 |    152 | CPU
DEBUG 01-15 16:10:29.369095.369095 lmp.py:1935]   Expert 62 |    158 | CPU
DEBUG 01-15 16:10:29.369692.369692 lmp.py:1935]   Expert 19 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369050.369050 lmp.py:1935]   Expert 45 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369931.369931 lmp.py:1935]   Expert 34 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369574.369574 lmp.py:1935]   Expert 50 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369217.369217 lmp.py:1935]   Expert 57 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369860.369860 lmp.py:1935]   Expert 10 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369503.369503 lmp.py:1935]   Expert 31 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369146.369146 lmp.py:1935]   Expert 23 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369028.369028 lmp.py:1935]   Expert  8 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369909.369909 lmp.py:1935]   Expert 60 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369029.369029 lmp.py:1935]   Expert 18 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369433.369433 lmp.py:1935]   Expert 22 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369315.369315 lmp.py:1935]   Expert 53 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369958.369958 lmp.py:1935]   Expert 52 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369839.369839 lmp.py:1935]   Expert 37 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369244.369244 lmp.py:1935]   Expert 17 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369125.369125 lmp.py:1935]   Expert  5 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369020.369020 lmp.py:1935]   Expert 11 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369186.369186 lmp.py:1935]   Expert  1 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369352.369352 lmp.py:1935]   Expert 49 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369757.369757 lmp.py:1935]   Expert 41 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369685.369685 lmp.py:1935]   Expert 26 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369089.369089 lmp.py:1935]   Expert 28 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369017.369017 lmp.py:1935]   Expert 32 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369183.369183 lmp.py:1935]   Expert 58 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369349.369349 lmp.py:1935]   Expert 40 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369515.369515 lmp.py:1935]   Expert 14 |    312 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369920.369920 lmp.py:1935]   Expert 12 |    329 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369086.369086 lmp.py:1935]   Expert 63 |    336 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369252.369252 lmp.py:1935]   Expert 21 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369418.369418 lmp.py:1935]   Expert 27 |    666 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.369015.369015 lmp.py:1935]   Expert  3 |   1013 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.369466.369466 lmp.py:1937] 
DEBUG 01-15 16:10:29.369466.369466 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.369393.369393 lmp.py:1938]   CPU:   3166 tokens
DEBUG 01-15 16:10:29.369321.369321 lmp.py:1942]   cuda:1:   4492 tokens (15 experts)
DEBUG 01-15 16:10:29.369249.369249 lmp.py:1942]   cuda:2:   4630 tokens (17 experts)
DEBUG 01-15 16:10:29.370938.370938 lmp.py:1943]   Total GPU:   9122 tokens
DEBUG 01-15 16:10:29.370912.370912 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.370912.370912 lmp.py:1944] 
DEBUG 01-15 16:10:29.370085.370085 cuda_h.py:19] end experts_map_get cost 0.0017096996307373047 seconds
DEBUG 01-15 16:10:29.370697.370697 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.370068.370068 lmp.py:1953] 
DEBUG 01-15 16:10:29.370068.370068 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.370944.370944 cuda_h.py:19] end cpu_experts_submit cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:10:29.370230.370230 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.370199.370199 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.371827.371827 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.1.gate_proj.weight': 0, 'model.layers.24.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.58.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.58.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.58.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.5.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.5.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.5.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.37.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.37.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.37.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.31.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.31.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.31.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.24.mlp.experts.32.gate_proj.weight': 0, 'model.layers.24.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.40.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.40.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.40.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.60.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.60.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.60.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.53.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.53.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.53.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.19.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.19.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.19.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.27.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.27.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.27.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.28.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.28.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.28.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(28345630720, 5767168, 0, 0), (28351397888, 5767168, 5767168, 0), (28339863552, 5767168, 11534336, 0), (28916580352, 5767168, 17301504, 0), (28922347520, 5767168, 23068672, 0), (28910813184, 5767168, 28835840, 0), (28380233728, 5767168, 34603008, 0), (28386000896, 5767168, 40370176, 0), (28374466560, 5767168, 46137344, 0), (29331816448, 5767168, 51904512, 0), (29337583616, 5767168, 57671680, 0), (29326049280, 5767168, 63438848, 0), (28414836736, 5767168, 69206016, 0), (28420603904, 5767168, 74973184, 0), (28409069568, 5767168, 80740352, 0), (28968484864, 5767168, 86507520, 0), (28974252032, 5767168, 92274688, 0), (28962717696, 5767168, 98041856, 0), (29037690880, 5767168, 103809024, 0), (29043458048, 5767168, 109576192, 0), (29031923712, 5767168, 115343360, 0), (28570550272, 5767168, 121110528, 0), (28576317440, 5767168, 126877696, 0), (28564783104, 5767168, 132644864, 0), (28639756288, 5767168, 138412032, 0), (28645523456, 5767168, 144179200, 0), (28633989120, 5767168, 149946368, 0), (28708962304, 5767168, 155713536, 0), (28714729472, 5767168, 161480704, 0), (28703195136, 5767168, 167247872, 0), (28726263808, 5767168, 173015040, 0), (28732030976, 5767168, 178782208, 0), (28720496640, 5767168, 184549376, 0), (29314514944, 5767168, 190316544, 0), (29320282112, 5767168, 196083712, 0), (29308747776, 5767168, 201850880, 0), (28778168320, 5767168, 207618048, 0), (28783935488, 5767168, 213385216, 0), (28772401152, 5767168, 219152384, 0), (28864675840, 5767168, 224919552, 0), (28870443008, 5767168, 230686720, 0), (28858908672, 5767168, 236453888, 0), (29418323968, 5767168, 242221056, 0), (29424091136, 5767168, 247988224, 0), (29412556800, 5767168, 253755392, 0)], 2: [(28881977344, 5767168, 0, 0), (28887744512, 5767168, 5767168, 0), (28876210176, 5767168, 11534336, 0), (29020389376, 5767168, 17301504, 0), (29026156544, 5767168, 23068672, 0), (29014622208, 5767168, 28835840, 0), (28466741248, 5767168, 34603008, 0), (28472508416, 5767168, 40370176, 0), (28460974080, 5767168, 46137344, 0), (28501344256, 5767168, 51904512, 0), (28507111424, 5767168, 57671680, 0), (28495577088, 5767168, 63438848, 0), (28518645760, 5767168, 69206016, 0), (28524412928, 5767168, 74973184, 0), (28512878592, 5767168, 80740352, 0), (28535947264, 5767168, 86507520, 0), (28541714432, 5767168, 92274688, 0), (28530180096, 5767168, 98041856, 0), (29106896896, 5767168, 103809024, 0), (29112664064, 5767168, 109576192, 0), (29101129728, 5767168, 115343360, 0), (29366419456, 5767168, 121110528, 0), (29372186624, 5767168, 126877696, 0), (29360652288, 5767168, 132644864, 0), (29176102912, 5767168, 138412032, 0), (29181870080, 5767168, 144179200, 0), (29170335744, 5767168, 149946368, 0), (28622454784, 5767168, 155713536, 0), (28628221952, 5767168, 161480704, 0), (28616687616, 5767168, 167247872, 0), (29193404416, 5767168, 173015040, 0), (29199171584, 5767168, 178782208, 0), (29187637248, 5767168, 184549376, 0), (29228007424, 5767168, 190316544, 0), (29233774592, 5767168, 196083712, 0), (29222240256, 5767168, 201850880, 0), (28691660800, 5767168, 207618048, 0), (28697427968, 5767168, 213385216, 0), (28685893632, 5767168, 219152384, 0), (29245308928, 5767168, 224919552, 0), (29251076096, 5767168, 230686720, 0), (29239541760, 5767168, 236453888, 0), (28657057792, 5767168, 242221056, 0), (28662824960, 5767168, 247988224, 0), (28651290624, 5767168, 253755392, 0), (28795469824, 5767168, 259522560, 0), (28801236992, 5767168, 265289728, 0), (28789702656, 5767168, 271056896, 0), (28812771328, 5767168, 276824064, 0), (28818538496, 5767168, 282591232, 0), (28807004160, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e345945d0>, 2: <capsule object NULL at 0x7a51b812df80>}
DEBUG 01-15 16:10:29.371554.371554 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:10:29.371325.371325 client.py:127] Model loaded
DEBUG 01-15 16:10:29.371711.371711 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.371343.371343 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 72ef7075-5e88-4056-aa3a-28dfd971cc79
DEBUG 01-15 16:10:29.371573.371573 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:29.372992.372992 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.371395.371395 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.372807.372807 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:29.372160.372160 cuda_h.py:19] end restore2model cost 0.0003674030303955078 seconds
DEBUG 01-15 16:10:29.372506.372506 cuda_h.py:19] end sllm_worker_task cost 0.010105371475219727 seconds
DEBUG 01-15 16:10:29.373109.373109 cuda_h.py:19] end move_flatidxs cost 0.00083160400390625 seconds
DEBUG 01-15 16:10:29.373885.373885 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:29.373359.373359 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 72ef7075-5e88-4056-aa3a-28dfd971cc79
DEBUG 01-15 16:10:29.373550.373550 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0033545494079589844 seconds
DEBUG 01-15 16:10:29.373500.373500 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.376455.376455 cuda_h.py:19] end restore2model cost 0.0026025772094726562 seconds
DEBUG 01-15 16:10:29.376352.376352 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006176948547363281 seconds
DEBUG 01-15 16:10:29.376624.376624 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.376853.376853 cuda_h.py:19] end gpu_sexperts cost 0.0002727508544921875 seconds
DEBUG 01-15 16:10:29.376775.376775 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:29.376644.376644 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:29.376625.376625 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.376136.376136 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.377116.377116 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007624626159667969 seconds
DEBUG 01-15 16:10:29.377813.377813 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.377502.377502 cuda_h.py:19] end gpu_group_list cost 0.00016379356384277344 seconds
DEBUG 01-15 16:10:29.378606.378606 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.379963.379963 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009059906005859375 seconds
DEBUG 01-15 16:10:29.379766.379766 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.379098.379098 cuda_h.py:19] end gpu_group_list cost 0.00018024444580078125 seconds
DEBUG 01-15 16:10:29.380278.380278 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.380253.380253 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 72ef7075-5e88-4056-aa3a-28dfd971cc79
DEBUG 01-15 16:10:29.381939.381939 cuda_h.py:19] end group_tensors cost 0.00871133804321289 seconds
DEBUG 01-15 16:10:29.382388.382388 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.386893.386893 cuda_h.py:19] end group pad cost 0.003551483154296875 seconds
DEBUG 01-15 16:10:29.386160.386160 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.403572.403572 client.py:127] Model loaded
DEBUG 01-15 16:10:29.403061.403061 cuda_h.py:19] end wait_experts_multi_device cost 0.023325443267822266 seconds
DEBUG 01-15 16:10:29.403129.403129 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.405808.405808 cuda_h.py:19] end group_einsum cost 0.01927924156188965 seconds
DEBUG 01-15 16:10:29.405429.405429 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.409314.409314 cuda_h.py:19] end get_outputs_cpu1 cost 0.003216981887817383 seconds
DEBUG 01-15 16:10:29.409055.409055 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03769207000732422 seconds
DEBUG 01-15 16:10:29.410957.410957 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006069183349609375 seconds
DEBUG 01-15 16:10:29.410582.410582 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.411732.411732 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.411644.411644 cuda_h.py:19] end index_scatter cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:10:29.411794.411794 cuda_h.py:19] end cpuoutputsdeal cost 0.0015573501586914062 seconds
DEBUG 01-15 16:10:29.411565.411565 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.411944.411944 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.412651.412651 cuda_h.py:19] end gpu_group_tensor cost 0.00014019012451171875 seconds
DEBUG 01-15 16:10:29.412268.412268 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.412956.412956 cuda_h.py:19] end gpu_group_tensor cost 0.000125885009765625 seconds
DEBUG 01-15 16:10:29.412807.412807 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.413320.413320 cuda_h.py:19] end gpu_group_einsum cost 0.0008382797241210938 seconds
DEBUG 01-15 16:10:29.413908.413908 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.413113.413113 cuda_h.py:19] end gpu_group_einsum cost 0.0005106925964355469 seconds
DEBUG 01-15 16:10:29.414621.414621 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.414591.414591 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.414514.414514 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021076202392578125 seconds
DEBUG 01-15 16:10:29.414091.414091 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.414996.414996 cuda_h.py:19] end concat_expert_out cost 6.580352783203125e-05 seconds
DEBUG 01-15 16:10:29.414455.414455 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.414901.414901 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:29.415498.415498 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008394718170166016 seconds
DEBUG 01-15 16:10:29.415614.415614 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.415973.415973 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.415130.415130 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012159347534179688 seconds
DEBUG 01-15 16:10:29.415410.415410 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.415472.415472 cuda_h.py:19] end concat_expert_out cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:29.415884.415884 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.415854.415854 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:29.415140.415140 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00045990943908691406 seconds
DEBUG 01-15 16:10:29.415473.415473 cuda_h.py:19] end gpu_experts_multi_device cost 0.03877067565917969 seconds
DEBUG 01-15 16:10:29.415814.415814 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.04826092720031738 seconds
DEBUG 01-15 16:10:29.416747.416747 cuda_h.py:19] end prefill_layer cost 0.05409979820251465 seconds
DEBUG 01-15 16:10:29.416967.416967 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:10:29.416385.416385 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.416579.416579 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:10:29.416427.416427 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:29.416706.416706 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:29.416894.416894 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 4.00543212890625e-05 seconds
DEBUG 01-15 16:10:29.416194.416194 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:10:29.416658.416658 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.416482.416482 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.416266.416266 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:29.416534.416534 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:29.416132.416132 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:29.417784.417784 cuda_h.py:19] end allocate_cuda_memory cost 0.0002315044403076172 seconds
DEBUG 01-15 16:10:29.417899.417899 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:29.417523.417523 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.417922.417922 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.417439.417439 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 33e94b35-aef2-4cf1-bf99-c41ed35043b3
DEBUG 01-15 16:10:29.417728.417728 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.417175.417175 cuda_h.py:10] start self_attn
INFO 01-15 16:10:29.418360.418360 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 33e94b35-aef2-4cf1-bf99-c41ed35043b3
DEBUG 01-15 16:10:29.419395.419395 cuda_h.py:19] end load_into_gpu_async cost 0.001905679702758789 seconds
DEBUG 01-15 16:10:29.419959.419959 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:29.419552.419552 cuda_h.py:19] end restore_tensors2 cost 9.059906005859375e-05 seconds
DEBUG 01-15 16:10:29.419838.419838 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002508878707885742 seconds
INFO 01-15 16:10:29.419338.419338 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 33e94b35-aef2-4cf1-bf99-c41ed35043b3
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.420098.420098 cuda_h.py:19] end self_attn cost 0.0031707286834716797 seconds
DEBUG 01-15 16:10:29.421864.421864 cuda_h.py:19] end iln_self_attn_paln cost 0.004717350006103516 seconds
DEBUG 01-15 16:10:29.421647.421647 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:10:29.421410.421410 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.421341.421341 cuda_h.py:19] end gate cost 0.0006830692291259766 seconds
DEBUG 01-15 16:10:29.422846.422846 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.422436.422436 lmp.py:1912] 
DEBUG 01-15 16:10:29.422436.422436 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.422245.422245 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.422709.422709 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.422313.422313 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.422009.422009 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.422466.422466 lmp.py:1917] 
DEBUG 01-15 16:10:29.422466.422466 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.422686.422686 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.422581.422581 lmp.py:1935]   Expert 13 |     26 | CPU
DEBUG 01-15 16:10:29.422184.422184 lmp.py:1935]   Expert 44 |     37 | CPU
DEBUG 01-15 16:10:29.422072.422072 lmp.py:1935]   Expert  9 |     40 | CPU
DEBUG 01-15 16:10:29.422245.422245 lmp.py:1935]   Expert 25 |     43 | CPU
DEBUG 01-15 16:10:29.422418.422418 lmp.py:1935]   Expert 16 |     46 | CPU
DEBUG 01-15 16:10:29.422306.422306 lmp.py:1935]   Expert 38 |     49 | CPU
DEBUG 01-15 16:10:29.422195.422195 lmp.py:1935]   Expert  2 |     52 | CPU
DEBUG 01-15 16:10:29.422844.422844 lmp.py:1935]   Expert 22 |     57 | CPU
DEBUG 01-15 16:10:29.422302.422302 lmp.py:1935]   Expert 33 |     58 | CPU
DEBUG 01-15 16:10:29.422998.422998 lmp.py:1935]   Expert 42 |     59 | CPU
DEBUG 01-15 16:10:29.422217.422217 lmp.py:1935]   Expert  5 |     65 | CPU
DEBUG 01-15 16:10:29.422913.422913 lmp.py:1935]   Expert 23 |     74 | CPU
DEBUG 01-15 16:10:29.422894.422894 lmp.py:1935]   Expert 24 |     80 | CPU
DEBUG 01-15 16:10:29.422259.422259 lmp.py:1935]   Expert 10 |     87 | CPU
DEBUG 01-15 16:10:29.422671.422671 lmp.py:1935]   Expert 59 |    101 | CPU
DEBUG 01-15 16:10:29.422082.422082 lmp.py:1935]   Expert 21 |    108 | CPU
DEBUG 01-15 16:10:29.422016.422016 lmp.py:1935]   Expert 55 |    114 | CPU
DEBUG 01-15 16:10:29.422382.422382 lmp.py:1935]   Expert 45 |    116 | CPU
DEBUG 01-15 16:10:29.423263.423263 lmp.py:1935]   Expert 46 |    117 | CPU
DEBUG 01-15 16:10:29.423429.423429 lmp.py:1935]   Expert 61 |    122 | CPU
DEBUG 01-15 16:10:29.423595.423595 lmp.py:1935]   Expert 31 |    128 | CPU
DEBUG 01-15 16:10:29.423523.423523 lmp.py:1935]   Expert 51 |    141 | CPU
DEBUG 01-15 16:10:29.423451.423451 lmp.py:1935]   Expert  6 |    143 | CPU
DEBUG 01-15 16:10:29.423094.423094 lmp.py:1935]   Expert 36 |    143 | CPU
DEBUG 01-15 16:10:29.423737.423737 lmp.py:1935]   Expert 43 |    143 | CPU
DEBUG 01-15 16:10:29.423618.423618 lmp.py:1935]   Expert  8 |    145 | CPU
DEBUG 01-15 16:10:29.423738.423738 lmp.py:1935]   Expert  0 |    150 | CPU
DEBUG 01-15 16:10:29.423904.423904 lmp.py:1935]   Expert  3 |    151 | CPU
DEBUG 01-15 16:10:29.423832.423832 lmp.py:1935]   Expert 18 |    156 | CPU
DEBUG 01-15 16:10:29.423998.423998 lmp.py:1935]   Expert 26 |    157 | CPU
DEBUG 01-15 16:10:29.423925.423925 lmp.py:1935]   Expert 48 |    160 | CPU
DEBUG 01-15 16:10:29.423092.423092 lmp.py:1935]   Expert 41 |    167 | CPU
DEBUG 01-15 16:10:29.423927.423927 lmp.py:1935]   Expert 12 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423762.423762 lmp.py:1935]   Expert  7 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423120.423120 lmp.py:1935]   Expert 20 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423478.423478 lmp.py:1935]   Expert 28 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423029.423029 lmp.py:1935]   Expert 56 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423546.423546 lmp.py:1935]   Expert 27 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423143.423143 lmp.py:1935]   Expert  1 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423739.423739 lmp.py:1935]   Expert 34 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423382.423382 lmp.py:1935]   Expert 47 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423025.423025 lmp.py:1935]   Expert 32 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423430.423430 lmp.py:1935]   Expert 11 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423073.423073 lmp.py:1935]   Expert 40 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423954.423954 lmp.py:1935]   Expert 49 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423597.423597 lmp.py:1935]   Expert 53 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423432.423432 lmp.py:1935]   Expert 63 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423029.423029 lmp.py:1935]   Expert 15 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423387.423387 lmp.py:1935]   Expert 29 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423461.423461 lmp.py:1935]   Expert 50 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423342.423342 lmp.py:1935]   Expert  4 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423985.423985 lmp.py:1935]   Expert 30 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423866.423866 lmp.py:1935]   Expert 35 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423748.423748 lmp.py:1935]   Expert 14 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423868.423868 lmp.py:1935]   Expert 37 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423987.423987 lmp.py:1935]   Expert 52 |    340 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423637.423637 lmp.py:1935]   Expert 17 |    356 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423757.423757 lmp.py:1935]   Expert 54 |    381 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423652.423652 lmp.py:1935]   Expert 39 |    389 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423818.423818 lmp.py:1935]   Expert 57 |    412 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423269.423269 lmp.py:1935]   Expert 60 |    457 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423243.423243 lmp.py:1935]   Expert 62 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423694.423694 lmp.py:1935]   Expert 19 |    544 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.423906.423906 lmp.py:1935]   Expert 58 |    571 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.423404.423404 lmp.py:1937] 
DEBUG 01-15 16:10:29.423404.423404 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.423378.423378 lmp.py:1938]   CPU:   3235 tokens
DEBUG 01-15 16:10:29.423590.423590 lmp.py:1942]   cuda:1:   4526 tokens (16 experts)
DEBUG 01-15 16:10:29.423756.423756 lmp.py:1942]   cuda:2:   4527 tokens (16 experts)
DEBUG 01-15 16:10:29.423969.423969 lmp.py:1943]   Total GPU:   9053 tokens
DEBUG 01-15 16:10:29.423466.423466 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.423466.423466 lmp.py:1944] 
DEBUG 01-15 16:10:29.423877.423877 cuda_h.py:19] end experts_map_get cost 0.0018870830535888672 seconds
DEBUG 01-15 16:10:29.424681.424681 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.424053.424053 lmp.py:1953] 
DEBUG 01-15 16:10:29.424053.424053 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.424597.424597 cuda_h.py:19] end cpu_experts_submit cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:29.424314.424314 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.424951.424951 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.426600.426600 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.1.gate_proj.weight': 0, 'model.layers.25.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.28.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.28.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.28.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.25.mlp.experts.32.gate_proj.weight': 0, 'model.layers.25.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(29452926976, 5767168, 0, 0), (29458694144, 5767168, 5767168, 0), (29447159808, 5767168, 11534336, 0), (30041178112, 5767168, 17301504, 0), (30046945280, 5767168, 23068672, 0), (30035410944, 5767168, 28835840, 0), (30110384128, 5767168, 34603008, 0), (30116151296, 5767168, 40370176, 0), (30104616960, 5767168, 46137344, 0), (30127685632, 5767168, 51904512, 0), (30133452800, 5767168, 57671680, 0), (30121918464, 5767168, 63438848, 0), (29556736000, 5767168, 69206016, 0), (29562503168, 5767168, 74973184, 0), (29550968832, 5767168, 80740352, 0), (29920067584, 5767168, 86507520, 0), (29925834752, 5767168, 92274688, 0), (29914300416, 5767168, 98041856, 0), (29695148032, 5767168, 103809024, 0), (29700915200, 5767168, 109576192, 0), (29689380864, 5767168, 115343360, 0), (30248796160, 5767168, 121110528, 0), (30254563328, 5767168, 126877696, 0), (30243028992, 5767168, 132644864, 0), (29729751040, 5767168, 138412032, 0), (29735518208, 5767168, 144179200, 0), (29723983872, 5767168, 149946368, 0), (30283399168, 5767168, 155713536, 0), (30289166336, 5767168, 161480704, 0), (30277632000, 5767168, 167247872, 0), (30335303680, 5767168, 173015040, 0), (30341070848, 5767168, 178782208, 0), (30329536512, 5767168, 184549376, 0), (30404509696, 5767168, 190316544, 0), (30410276864, 5767168, 196083712, 0), (30398742528, 5767168, 201850880, 0), (30439112704, 5767168, 207618048, 0), (30444879872, 5767168, 213385216, 0), (30433345536, 5767168, 219152384, 0), (30473715712, 5767168, 224919552, 0), (30479482880, 5767168, 230686720, 0), (30467948544, 5767168, 236453888, 0), (29954670592, 5767168, 242221056, 0), (29960437760, 5767168, 247988224, 0), (29948903424, 5767168, 253755392, 0), (30525620224, 5767168, 259522560, 0), (30531387392, 5767168, 265289728, 0), (30519853056, 5767168, 271056896, 0)], 2: [(29989273600, 5767168, 0, 0), (29995040768, 5767168, 5767168, 0), (29983506432, 5767168, 11534336, 0), (30023876608, 5767168, 17301504, 0), (30029643776, 5767168, 23068672, 0), (30018109440, 5767168, 28835840, 0), (29504831488, 5767168, 34603008, 0), (29510598656, 5767168, 40370176, 0), (29499064320, 5767168, 46137344, 0), (30075781120, 5767168, 51904512, 0), (30081548288, 5767168, 57671680, 0), (30070013952, 5767168, 63438848, 0), (29625942016, 5767168, 69206016, 0), (29631709184, 5767168, 74973184, 0), (29620174848, 5767168, 80740352, 0), (29643243520, 5767168, 86507520, 0), (29649010688, 5767168, 92274688, 0), (29637476352, 5767168, 98041856, 0), (29677846528, 5767168, 103809024, 0), (29683613696, 5767168, 109576192, 0), (29672079360, 5767168, 115343360, 0), (30300700672, 5767168, 121110528, 0), (30306467840, 5767168, 126877696, 0), (30294933504, 5767168, 132644864, 0), (29764354048, 5767168, 138412032, 0), (29770121216, 5767168, 144179200, 0), (29758586880, 5767168, 149946368, 0), (29781655552, 5767168, 155713536, 0), (29787422720, 5767168, 161480704, 0), (29775888384, 5767168, 167247872, 0), (30352605184, 5767168, 173015040, 0), (30358372352, 5767168, 178782208, 0), (30346838016, 5767168, 184549376, 0), (30369906688, 5767168, 190316544, 0), (30375673856, 5767168, 196083712, 0), (30364139520, 5767168, 201850880, 0), (30421811200, 5767168, 207618048, 0), (30427578368, 5767168, 213385216, 0), (30416044032, 5767168, 219152384, 0), (29902766080, 5767168, 224919552, 0), (29908533248, 5767168, 230686720, 0), (29896998912, 5767168, 236453888, 0), (29937369088, 5767168, 242221056, 0), (29943136256, 5767168, 247988224, 0), (29931601920, 5767168, 253755392, 0), (30508318720, 5767168, 259522560, 0), (30514085888, 5767168, 265289728, 0), (30502551552, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a519f6f9f80>, 2: <capsule object NULL at 0x7a51b06464f0>}
DEBUG 01-15 16:10:29.426332.426332 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.426500.426500 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.426679.426679 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0e4b4008-6f70-4aca-b0c3-eefa45417c76
DEBUG 01-15 16:10:29.426129.426129 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:29.427546.427546 client.py:127] Model loaded
DEBUG 01-15 16:10:29.427422.427422 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.427757.427757 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:29.427693.427693 cuda_h.py:19] end restore2model cost 0.0003428459167480469 seconds
DEBUG 01-15 16:10:29.427509.427509 cuda_h.py:19] end sllm_worker_task cost 0.010910511016845703 seconds
DEBUG 01-15 16:10:29.427449.427449 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:29.428181.428181 cuda_h.py:19] end move_flatidxs cost 0.0008294582366943359 seconds
DEBUG 01-15 16:10:29.428057.428057 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:29.428604.428604 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0e4b4008-6f70-4aca-b0c3-eefa45417c76
DEBUG 01-15 16:10:29.429471.429471 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005023479461669922 seconds
DEBUG 01-15 16:10:29.429090.429090 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.431207.431207 cuda_h.py:19] end restore2model cost 0.002475261688232422 seconds
DEBUG 01-15 16:10:29.431944.431944 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0077097415924072266 seconds
DEBUG 01-15 16:10:29.431309.431309 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.432332.432332 cuda_h.py:19] end gpu_sexperts cost 0.0002675056457519531 seconds
DEBUG 01-15 16:10:29.432638.432638 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:29.432699.432699 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:10:29.432111.432111 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.432244.432244 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.433537.433537 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008170604705810547 seconds
DEBUG 01-15 16:10:29.433288.433288 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.433712.433712 cuda_h.py:19] end gpu_group_list cost 0.00017905235290527344 seconds
DEBUG 01-15 16:10:29.434094.434094 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.435390.435390 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008578300476074219 seconds
DEBUG 01-15 16:10:29.435717.435717 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.435770.435770 cuda_h.py:19] end gpu_group_list cost 0.00018525123596191406 seconds
DEBUG 01-15 16:10:29.435664.435664 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.436116.436116 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0e4b4008-6f70-4aca-b0c3-eefa45417c76
DEBUG 01-15 16:10:29.438354.438354 cuda_h.py:19] end group_tensors cost 0.009238719940185547 seconds
DEBUG 01-15 16:10:29.438604.438604 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.442603.442603 cuda_h.py:19] end group pad cost 0.003667116165161133 seconds
DEBUG 01-15 16:10:29.442870.442870 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.456280.456280 client.py:127] Model loaded
DEBUG 01-15 16:10:29.456359.456359 cuda_h.py:19] end wait_experts_multi_device cost 0.020846843719482422 seconds
DEBUG 01-15 16:10:29.457628.457628 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.460852.460852 cuda_h.py:19] end group_einsum cost 0.018160343170166016 seconds
DEBUG 01-15 16:10:29.460248.460248 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.464491.464491 cuda_h.py:19] end get_outputs_cpu1 cost 0.0032346248626708984 seconds
DEBUG 01-15 16:10:29.464777.464777 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03730297088623047 seconds
DEBUG 01-15 16:10:29.465362.465362 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008170127868652344 seconds
DEBUG 01-15 16:10:29.465359.465359 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.466536.466536 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.466640.466640 cuda_h.py:19] end index_scatter cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:29.467226.467226 cuda_h.py:19] end cpuoutputsdeal cost 0.0015490055084228516 seconds
DEBUG 01-15 16:10:29.467567.467567 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.467422.467422 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.467030.467030 cuda_h.py:19] end gpu_group_tensor cost 0.0001366138458251953 seconds
DEBUG 01-15 16:10:29.467886.467886 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.467328.467328 cuda_h.py:19] end gpu_group_tensor cost 0.00012087821960449219 seconds
DEBUG 01-15 16:10:29.467894.467894 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.468451.468451 cuda_h.py:19] end gpu_group_einsum cost 0.0005736351013183594 seconds
DEBUG 01-15 16:10:29.468588.468588 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.468567.468567 cuda_h.py:19] end gpu_group_einsum cost 0.0004715919494628906 seconds
DEBUG 01-15 16:10:29.469790.469790 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.469853.469853 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.469663.469663 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019860267639160156 seconds
DEBUG 01-15 16:10:29.469764.469764 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.469476.469476 cuda_h.py:19] end concat_expert_out cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:10:29.469173.469173 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.469858.469858 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:29.469522.469522 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008358955383300781 seconds
DEBUG 01-15 16:10:29.470267.470267 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.470917.470917 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.470088.470088 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012826919555664062 seconds
DEBUG 01-15 16:10:29.470176.470176 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.470999.470999 cuda_h.py:19] end concat_expert_out cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:29.470266.470266 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.470329.470329 cuda_h.py:19] end index_scatter cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:29.470422.470422 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004661083221435547 seconds
DEBUG 01-15 16:10:29.470279.470279 cuda_h.py:19] end gpu_experts_multi_device cost 0.03823256492614746 seconds
DEBUG 01-15 16:10:29.470235.470235 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.04938340187072754 seconds
DEBUG 01-15 16:10:29.471731.471731 cuda_h.py:19] end prefill_layer cost 0.054789066314697266 seconds
DEBUG 01-15 16:10:29.471196.471196 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:10:29.471615.471615 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.471033.471033 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:10:29.471166.471166 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:29.471684.471684 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:29.471156.471156 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 3.910064697265625e-05 seconds
DEBUG 01-15 16:10:29.471290.471290 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 6.985664367675781e-05 seconds
DEBUG 01-15 16:10:29.471800.471800 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.471432.471432 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.471024.471024 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:29.471074.471074 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:29.471056.471056 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:29.471001.471001 cuda_h.py:19] end allocate_cuda_memory cost 0.0002720355987548828 seconds
DEBUG 01-15 16:10:29.472368.472368 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:29.472469.472469 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.472537.472537 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.472293.472293 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 176962d5-c59c-4d4c-a817-a0274ddbc6fb
DEBUG 01-15 16:10:29.472773.472773 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.472452.472452 cuda_h.py:10] start self_attn
INFO 01-15 16:10:29.474952.474952 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 176962d5-c59c-4d4c-a817-a0274ddbc6fb
DEBUG 01-15 16:10:29.474610.474610 cuda_h.py:19] end load_into_gpu_async cost 0.002034902572631836 seconds
DEBUG 01-15 16:10:29.474697.474697 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:29.474144.474144 cuda_h.py:19] end restore_tensors2 cost 8.797645568847656e-05 seconds
DEBUG 01-15 16:10:29.474550.474550 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0027043819427490234 seconds
INFO 01-15 16:10:29.474632.474632 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 176962d5-c59c-4d4c-a817-a0274ddbc6fb
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.475429.475429 cuda_h.py:19] end self_attn cost 0.0032138824462890625 seconds
DEBUG 01-15 16:10:29.476730.476730 cuda_h.py:19] end iln_self_attn_paln cost 0.004794597625732422 seconds
DEBUG 01-15 16:10:29.476844.476844 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:10:29.476508.476508 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.476140.476140 cuda_h.py:19] end gate cost 0.0006420612335205078 seconds
DEBUG 01-15 16:10:29.476115.476115 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.477066.477066 lmp.py:1912] 
DEBUG 01-15 16:10:29.477066.477066 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.477829.477829 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.477386.477386 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.477651.477651 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.477771.477771 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.477937.477937 lmp.py:1917] 
DEBUG 01-15 16:10:29.477937.477937 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.477057.477057 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.477661.477661 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:10:29.477780.477780 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:10:29.477708.477708 lmp.py:1935]   Expert 11 |     28 | CPU
DEBUG 01-15 16:10:29.477397.477397 lmp.py:1935]   Expert  7 |     42 | CPU
DEBUG 01-15 16:10:29.477471.477471 lmp.py:1935]   Expert 62 |     43 | CPU
DEBUG 01-15 16:10:29.477352.477352 lmp.py:1935]   Expert 51 |     45 | CPU
DEBUG 01-15 16:10:29.477757.477757 lmp.py:1935]   Expert  3 |     46 | CPU
DEBUG 01-15 16:10:29.477923.477923 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:10:29.477612.477612 lmp.py:1935]   Expert 17 |     53 | CPU
DEBUG 01-15 16:10:29.477825.477825 lmp.py:1935]   Expert 29 |     55 | CPU
DEBUG 01-15 16:10:29.477276.477276 lmp.py:1935]   Expert  6 |     60 | CPU
DEBUG 01-15 16:10:29.477488.477488 lmp.py:1935]   Expert  9 |     65 | CPU
DEBUG 01-15 16:10:29.477701.477701 lmp.py:1935]   Expert 38 |     76 | CPU
DEBUG 01-15 16:10:29.477913.477913 lmp.py:1935]   Expert 63 |     77 | CPU
DEBUG 01-15 16:10:29.477364.477364 lmp.py:1935]   Expert 55 |     81 | CPU
DEBUG 01-15 16:10:29.477053.477053 lmp.py:1935]   Expert 59 |     87 | CPU
DEBUG 01-15 16:10:29.477604.477604 lmp.py:1935]   Expert 48 |     89 | CPU
DEBUG 01-15 16:10:29.477485.477485 lmp.py:1935]   Expert  8 |     94 | CPU
DEBUG 01-15 16:10:29.477366.477366 lmp.py:1935]   Expert 19 |     95 | CPU
DEBUG 01-15 16:10:29.477248.477248 lmp.py:1935]   Expert 49 |    100 | CPU
DEBUG 01-15 16:10:29.477891.477891 lmp.py:1935]   Expert 22 |    104 | CPU
DEBUG 01-15 16:10:29.477818.477818 lmp.py:1935]   Expert 24 |    110 | CPU
DEBUG 01-15 16:10:29.477985.477985 lmp.py:1935]   Expert 34 |    114 | CPU
DEBUG 01-15 16:10:29.477674.477674 lmp.py:1935]   Expert 36 |    114 | CPU
DEBUG 01-15 16:10:29.477602.477602 lmp.py:1935]   Expert 42 |    117 | CPU
DEBUG 01-15 16:10:29.477768.477768 lmp.py:1935]   Expert 50 |    119 | CPU
DEBUG 01-15 16:10:29.477225.477225 lmp.py:1935]   Expert 39 |    125 | CPU
DEBUG 01-15 16:10:29.477345.477345 lmp.py:1935]   Expert  4 |    133 | CPU
DEBUG 01-15 16:10:29.477035.477035 lmp.py:1935]   Expert 37 |    142 | CPU
DEBUG 01-15 16:10:29.477962.477962 lmp.py:1935]   Expert 15 |    148 | CPU
DEBUG 01-15 16:10:29.477890.477890 lmp.py:1935]   Expert 41 |    149 | CPU
DEBUG 01-15 16:10:29.477102.477102 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:10:29.477222.477222 lmp.py:1935]   Expert 56 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.477342.477342 lmp.py:1935]   Expert 16 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478747.478747 lmp.py:1935]   Expert 44 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478390.478390 lmp.py:1935]   Expert 60 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478556.478556 lmp.py:1935]   Expert  1 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478391.478391 lmp.py:1935]   Expert 21 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478511.478511 lmp.py:1935]   Expert 43 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478392.478392 lmp.py:1935]   Expert 47 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478273.478273 lmp.py:1935]   Expert 53 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478155.478155 lmp.py:1935]   Expert 33 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478798.478798 lmp.py:1935]   Expert 12 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478725.478725 lmp.py:1935]   Expert 13 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478892.478892 lmp.py:1935]   Expert 32 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478058.478058 lmp.py:1935]   Expert 28 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478462.478462 lmp.py:1935]   Expert  0 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478867.478867 lmp.py:1935]   Expert 31 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478464.478464 lmp.py:1935]   Expert 54 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478583.478583 lmp.py:1935]   Expert 26 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478942.478942 lmp.py:1935]   Expert 10 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478823.478823 lmp.py:1935]   Expert 18 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478227.478227 lmp.py:1935]   Expert 57 |    273 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478632.478632 lmp.py:1935]   Expert  2 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478798.478798 lmp.py:1935]   Expert 58 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478203.478203 lmp.py:1935]   Expert 40 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478607.478607 lmp.py:1935]   Expert 25 |    361 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478204.478204 lmp.py:1935]   Expert 45 |    362 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478562.478562 lmp.py:1935]   Expert  5 |    444 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478444.478444 lmp.py:1935]   Expert 35 |    454 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478484.478484 lmp.py:1935]   Expert 27 |    486 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478650.478650 lmp.py:1935]   Expert 46 |    553 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478340.478340 lmp.py:1935]   Expert 52 |    595 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.478029.478029 lmp.py:1935]   Expert 14 |    884 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.478003.478003 lmp.py:1937] 
DEBUG 01-15 16:10:29.478003.478003 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.478931.478931 lmp.py:1938]   CPU:   2740 tokens
DEBUG 01-15 16:10:29.478097.478097 lmp.py:1942]   cuda:1:   4835 tokens (16 experts)
DEBUG 01-15 16:10:29.478978.478978 lmp.py:1942]   cuda:2:   4713 tokens (16 experts)
DEBUG 01-15 16:10:29.478668.478668 lmp.py:1943]   Total GPU:   9548 tokens
DEBUG 01-15 16:10:29.478595.478595 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.478595.478595 lmp.py:1944] 
DEBUG 01-15 16:10:29.478768.478768 cuda_h.py:19] end experts_map_get cost 0.001682281494140625 seconds
DEBUG 01-15 16:10:29.478757.478757 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.478321.478321 lmp.py:1953] 
DEBUG 01-15 16:10:29.478321.478321 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.478826.478826 cuda_h.py:19] end cpu_experts_submit cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:10:29.478045.478045 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.478491.478491 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.480757.480757 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.32.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.32.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.32.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.1.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.1.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.1.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.54.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.54.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.54.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.27.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.27.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.27.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.57.up_proj.weight': 271056896}, 2: {'model.layers.26.mlp.experts.33.gate_proj.weight': 0, 'model.layers.26.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.60.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.60.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.60.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (31096569856, 5767168, 17301504, 0), (31102337024, 5767168, 23068672, 0), (31090802688, 5767168, 28835840, 0), (30560223232, 5767168, 34603008, 0), (30565990400, 5767168, 40370176, 0), (30554456064, 5767168, 46137344, 0), (30629429248, 5767168, 51904512, 0), (30635196416, 5767168, 57671680, 0), (30623662080, 5767168, 63438848, 0), (30715936768, 5767168, 69206016, 0), (30721703936, 5767168, 74973184, 0), (30710169600, 5767168, 80740352, 0), (31286886400, 5767168, 86507520, 0), (31292653568, 5767168, 92274688, 0), (31281119232, 5767168, 98041856, 0), (30750539776, 5767168, 103809024, 0), (30756306944, 5767168, 109576192, 0), (30744772608, 5767168, 115343360, 0), (31304187904, 5767168, 121110528, 0), (31309955072, 5767168, 126877696, 0), (31298420736, 5767168, 132644864, 0), (30785142784, 5767168, 138412032, 0), (30790909952, 5767168, 144179200, 0), (30779375616, 5767168, 149946368, 0), (31459901440, 5767168, 155713536, 0), (31465668608, 5767168, 161480704, 0), (31454134272, 5767168, 167247872, 0), (31477202944, 5767168, 173015040, 0), (31482970112, 5767168, 178782208, 0), (31471435776, 5767168, 184549376, 0), (31511805952, 5767168, 190316544, 0), (31517573120, 5767168, 196083712, 0), (31506038784, 5767168, 201850880, 0), (30975459328, 5767168, 207618048, 0), (30981226496, 5767168, 213385216, 0), (30969692160, 5767168, 219152384, 0), (31546408960, 5767168, 224919552, 0), (31552176128, 5767168, 230686720, 0), (31540641792, 5767168, 236453888, 0), (31010062336, 5767168, 242221056, 0), (31015829504, 5767168, 247988224, 0), (31004295168, 5767168, 253755392, 0), (31529107456, 5767168, 259522560, 0), (31534874624, 5767168, 265289728, 0), (31523340288, 5767168, 271056896, 0)], 2: [(31113871360, 5767168, 0, 0), (31119638528, 5767168, 5767168, 0), (31108104192, 5767168, 11534336, 0), (30577524736, 5767168, 17301504, 0), (30583291904, 5767168, 23068672, 0), (30571757568, 5767168, 28835840, 0), (31148474368, 5767168, 34603008, 0), (31154241536, 5767168, 40370176, 0), (31142707200, 5767168, 46137344, 0), (31234981888, 5767168, 51904512, 0), (31240749056, 5767168, 57671680, 0), (31229214720, 5767168, 63438848, 0), (31321489408, 5767168, 69206016, 0), (31327256576, 5767168, 74973184, 0), (31315722240, 5767168, 80740352, 0), (31338790912, 5767168, 86507520, 0), (31344558080, 5767168, 92274688, 0), (31333023744, 5767168, 98041856, 0), (30767841280, 5767168, 103809024, 0), (30773608448, 5767168, 109576192, 0), (30762074112, 5767168, 115343360, 0), (31356092416, 5767168, 121110528, 0), (31361859584, 5767168, 126877696, 0), (31350325248, 5767168, 132644864, 0), (31581011968, 5767168, 138412032, 0), (31586779136, 5767168, 144179200, 0), (31575244800, 5767168, 149946368, 0), (30854348800, 5767168, 155713536, 0), (30860115968, 5767168, 161480704, 0), (30848581632, 5767168, 167247872, 0), (30819745792, 5767168, 173015040, 0), (30825512960, 5767168, 178782208, 0), (30813978624, 5767168, 184549376, 0), (31442599936, 5767168, 190316544, 0), (31448367104, 5767168, 196083712, 0), (31436832768, 5767168, 201850880, 0), (30906253312, 5767168, 207618048, 0), (30912020480, 5767168, 213385216, 0), (30900486144, 5767168, 219152384, 0), (30992760832, 5767168, 224919552, 0), (30998528000, 5767168, 230686720, 0), (30986993664, 5767168, 236453888, 0), (31027363840, 5767168, 242221056, 0), (31033131008, 5767168, 247988224, 0), (31021596672, 5767168, 253755392, 0), (31079268352, 5767168, 259522560, 0), (31085035520, 5767168, 265289728, 0), (31073501184, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c0a5650>, 2: <capsule object NULL at 0x7a51b0646520>}
DEBUG 01-15 16:10:29.481690.481690 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:10:29.481362.481362 client.py:127] Model loaded
DEBUG 01-15 16:10:29.481510.481510 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.481010.481010 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 78f71c3d-d835-4524-807c-329e7e0a1bae
DEBUG 01-15 16:10:29.481921.481921 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:29.481527.481527 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.481870.481870 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.481151.481151 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:29.482973.482973 cuda_h.py:19] end restore2model cost 0.0003631114959716797 seconds
DEBUG 01-15 16:10:29.482888.482888 cuda_h.py:19] end sllm_worker_task cost 0.010477066040039062 seconds
INFO 01-15 16:10:29.482065.482065 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 78f71c3d-d835-4524-807c-329e7e0a1bae
DEBUG 01-15 16:10:29.482990.482990 cuda_h.py:19] end move_flatidxs cost 0.0008604526519775391 seconds
DEBUG 01-15 16:10:29.482972.482972 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:29.482131.482131 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004056453704833984 seconds
DEBUG 01-15 16:10:29.483464.483464 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.485780.485780 cuda_h.py:19] end restore2model cost 0.0024805068969726562 seconds
DEBUG 01-15 16:10:29.485908.485908 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0067577362060546875 seconds
DEBUG 01-15 16:10:29.485942.485942 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.485595.485595 cuda_h.py:19] end gpu_sexperts cost 0.000274658203125 seconds
DEBUG 01-15 16:10:29.485186.485186 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:29.486962.486962 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:29.486135.486135 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.486553.486553 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.486025.486025 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008091926574707031 seconds
DEBUG 01-15 16:10:29.486776.486776 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.487153.487153 cuda_h.py:19] end gpu_group_list cost 0.00017952919006347656 seconds
DEBUG 01-15 16:10:29.487450.487450 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.489411.489411 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011372566223144531 seconds
DEBUG 01-15 16:10:29.489413.489413 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.489745.489745 cuda_h.py:19] end gpu_group_list cost 0.00017952919006347656 seconds
DEBUG 01-15 16:10:29.490918.490918 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.490847.490847 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 78f71c3d-d835-4524-807c-329e7e0a1bae
DEBUG 01-15 16:10:29.493934.493934 cuda_h.py:19] end group_tensors cost 0.01028585433959961 seconds
DEBUG 01-15 16:10:29.493985.493985 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.497152.497152 cuda_h.py:19] end group pad cost 0.0033359527587890625 seconds
DEBUG 01-15 16:10:29.497419.497419 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:29.509081.509081 client.py:127] Model loaded
DEBUG 01-15 16:10:29.509748.509748 cuda_h.py:19] end wait_experts_multi_device cost 0.019189119338989258 seconds
DEBUG 01-15 16:10:29.509948.509948 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.514598.514598 cuda_h.py:19] end group_einsum cost 0.017525196075439453 seconds
DEBUG 01-15 16:10:29.515524.515524 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.518140.518140 cuda_h.py:19] end get_outputs_cpu1 cost 0.0029158592224121094 seconds
DEBUG 01-15 16:10:29.518802.518802 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03712129592895508 seconds
DEBUG 01-15 16:10:29.519751.519751 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.009866714477539062 seconds
DEBUG 01-15 16:10:29.519629.519629 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.521789.521789 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.521750.521750 cuda_h.py:19] end index_scatter cost 0.000110626220703125 seconds
DEBUG 01-15 16:10:29.521345.521345 cuda_h.py:19] end cpuoutputsdeal cost 0.0021505355834960938 seconds
DEBUG 01-15 16:10:29.521296.521296 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.521794.521794 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.521815.521815 cuda_h.py:19] end gpu_group_tensor cost 0.00017690658569335938 seconds
DEBUG 01-15 16:10:29.522890.522890 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.522022.522022 cuda_h.py:19] end gpu_group_tensor cost 0.00015854835510253906 seconds
DEBUG 01-15 16:10:29.522327.522327 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.523878.523878 cuda_h.py:19] end gpu_group_einsum cost 0.0013270378112792969 seconds
DEBUG 01-15 16:10:29.524653.524653 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.524993.524993 cuda_h.py:19] end gpu_group_einsum cost 0.0007445812225341797 seconds
DEBUG 01-15 16:10:29.525318.525318 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.525557.525557 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.525642.525642 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002968311309814453 seconds
DEBUG 01-15 16:10:29.525876.525876 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.525147.525147 cuda_h.py:19] end concat_expert_out cost 9.417533874511719e-05 seconds
DEBUG 01-15 16:10:29.526939.526939 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.526952.526952 cuda_h.py:19] end index_scatter cost 0.00011324882507324219 seconds
DEBUG 01-15 16:10:29.526292.526292 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0014998912811279297 seconds
DEBUG 01-15 16:10:29.526629.526629 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.527825.527825 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.527947.527947 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023674964904785156 seconds
DEBUG 01-15 16:10:29.527790.527790 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.527008.527008 cuda_h.py:19] end concat_expert_out cost 9.703636169433594e-05 seconds
DEBUG 01-15 16:10:29.527025.527025 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.527448.527448 cuda_h.py:19] end index_scatter cost 9.846687316894531e-05 seconds
DEBUG 01-15 16:10:29.527881.527881 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009171962738037109 seconds
DEBUG 01-15 16:10:29.528766.528766 cuda_h.py:19] end gpu_experts_multi_device cost 0.041936397552490234 seconds
DEBUG 01-15 16:10:29.528870.528870 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.05194687843322754 seconds
DEBUG 01-15 16:10:29.528644.528644 cuda_h.py:19] end prefill_layer cost 0.057596445083618164 seconds
DEBUG 01-15 16:10:29.528284.528284 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:10:29.528882.528882 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:29.528625.528625 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:10:29.529508.529508 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:29.529300.529300 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:29.529113.529113 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:29.533830.533830 cuda_h.py:19] end self_attn cost 0.003525257110595703 seconds
DEBUG 01-15 16:10:29.533707.533707 cuda_h.py:19] end iln_self_attn_paln cost 0.004517555236816406 seconds
DEBUG 01-15 16:10:29.533106.533106 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:10:29.533723.533723 cuda_h.py:10] start gate
DEBUG 01-15 16:10:29.534566.534566 cuda_h.py:19] end gate cost 0.000621795654296875 seconds
DEBUG 01-15 16:10:29.534873.534873 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:29.534115.534115 lmp.py:1912] 
DEBUG 01-15 16:10:29.534115.534115 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:29.534779.534779 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:29.534905.534905 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:29.534694.534694 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:29.534575.534575 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:29.534741.534741 lmp.py:1917] 
DEBUG 01-15 16:10:29.534741.534741 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:29.534861.534861 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:29.534465.534465 lmp.py:1935]   Expert 18 |     65 | CPU
DEBUG 01-15 16:10:29.534108.534108 lmp.py:1935]   Expert 47 |     69 | CPU
DEBUG 01-15 16:10:29.534035.534035 lmp.py:1935]   Expert 54 |     71 | CPU
DEBUG 01-15 16:10:29.534963.534963 lmp.py:1935]   Expert 23 |     77 | CPU
DEBUG 01-15 16:10:29.534891.534891 lmp.py:1935]   Expert 48 |     78 | CPU
DEBUG 01-15 16:10:29.534342.534342 lmp.py:1935]   Expert 44 |     85 | CPU
DEBUG 01-15 16:10:29.534462.534462 lmp.py:1935]   Expert 45 |     87 | CPU
DEBUG 01-15 16:10:29.534628.534628 lmp.py:1935]   Expert 20 |     92 | CPU
DEBUG 01-15 16:10:29.534794.534794 lmp.py:1935]   Expert 31 |     98 | CPU
DEBUG 01-15 16:10:29.535722.535722 lmp.py:1935]   Expert 36 |    107 | CPU
DEBUG 01-15 16:10:29.535364.535364 lmp.py:1935]   Expert 61 |    111 | CPU
DEBUG 01-15 16:10:29.535577.535577 lmp.py:1935]   Expert 42 |    118 | CPU
DEBUG 01-15 16:10:29.535789.535789 lmp.py:1935]   Expert 33 |    120 | CPU
DEBUG 01-15 16:10:29.535240.535240 lmp.py:1935]   Expert 10 |    122 | CPU
DEBUG 01-15 16:10:29.535214.535214 lmp.py:1935]   Expert 24 |    123 | CPU
DEBUG 01-15 16:10:29.535427.535427 lmp.py:1935]   Expert 43 |    123 | CPU
DEBUG 01-15 16:10:29.535639.535639 lmp.py:1935]   Expert 11 |    125 | CPU
DEBUG 01-15 16:10:29.535375.535375 lmp.py:1935]   Expert 49 |    127 | CPU
DEBUG 01-15 16:10:29.535587.535587 lmp.py:1935]   Expert 56 |    130 | CPU
DEBUG 01-15 16:10:29.535561.535561 lmp.py:1935]   Expert  6 |    134 | CPU
DEBUG 01-15 16:10:29.535774.535774 lmp.py:1935]   Expert 51 |    144 | CPU
DEBUG 01-15 16:10:29.535178.535178 lmp.py:1935]   Expert  0 |    150 | CPU
DEBUG 01-15 16:10:29.535106.535106 lmp.py:1935]   Expert 17 |    150 | CPU
DEBUG 01-15 16:10:29.535034.535034 lmp.py:1935]   Expert  5 |    153 | CPU
DEBUG 01-15 16:10:29.535200.535200 lmp.py:1935]   Expert 40 |    156 | CPU
DEBUG 01-15 16:10:29.535651.535651 lmp.py:1935]   Expert 55 |    161 | CPU
DEBUG 01-15 16:10:29.535625.535625 lmp.py:1935]   Expert 57 |    161 | CPU
DEBUG 01-15 16:10:29.535837.535837 lmp.py:1935]   Expert 26 |    163 | CPU
DEBUG 01-15 16:10:29.535335.535335 lmp.py:1935]   Expert 12 |    164 | CPU
DEBUG 01-15 16:10:29.535547.535547 lmp.py:1935]   Expert 59 |    164 | CPU
DEBUG 01-15 16:10:29.535760.535760 lmp.py:1935]   Expert 46 |    166 | CPU
DEBUG 01-15 16:10:29.535495.535495 lmp.py:1935]   Expert 38 |    168 | CPU
DEBUG 01-15 16:10:29.535615.535615 lmp.py:1935]   Expert 30 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535735.535735 lmp.py:1935]   Expert 58 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535093.535093 lmp.py:1935]   Expert 13 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535690.535690 lmp.py:1935]   Expert 35 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535571.535571 lmp.py:1935]   Expert 50 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535929.535929 lmp.py:1935]   Expert  7 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535572.535572 lmp.py:1935]   Expert 16 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535738.535738 lmp.py:1935]   Expert 15 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535905.535905 lmp.py:1935]   Expert 32 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535309.535309 lmp.py:1935]   Expert 14 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535237.535237 lmp.py:1935]   Expert  1 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535641.535641 lmp.py:1935]   Expert  3 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535331.535331 lmp.py:1935]   Expert  4 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535735.535735 lmp.py:1935]   Expert 39 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535901.535901 lmp.py:1935]   Expert 34 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535829.535829 lmp.py:1935]   Expert 52 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535757.535757 lmp.py:1935]   Expert 28 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535877.535877 lmp.py:1935]   Expert 25 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535996.535996 lmp.py:1935]   Expert 22 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535116.535116 lmp.py:1935]   Expert  2 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535998.535998 lmp.py:1935]   Expert 41 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535402.535402 lmp.py:1935]   Expert 21 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535330.535330 lmp.py:1935]   Expert 60 |    283 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535496.535496 lmp.py:1935]   Expert 63 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535662.535662 lmp.py:1935]   Expert 29 |    296 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535590.535590 lmp.py:1935]   Expert 62 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535518.535518 lmp.py:1935]   Expert 27 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535207.535207 lmp.py:1935]   Expert 37 |    326 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535373.535373 lmp.py:1935]   Expert 53 |    332 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535539.535539 lmp.py:1935]   Expert  8 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535897.535897 lmp.py:1935]   Expert 19 |    440 | GPU2(cuda:2)
DEBUG 01-15 16:10:29.535256.535256 lmp.py:1935]   Expert  9 |    612 | GPU1(cuda:1)
DEBUG 01-15 16:10:29.535707.535707 lmp.py:1937] 
DEBUG 01-15 16:10:29.535707.535707 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:29.535826.535826 lmp.py:1938]   CPU:   3962 tokens
DEBUG 01-15 16:10:29.535946.535946 lmp.py:1942]   cuda:1:   4191 tokens (16 experts)
DEBUG 01-15 16:10:29.536351.536351 lmp.py:1942]   cuda:2:   4135 tokens (16 experts)
DEBUG 01-15 16:10:29.536040.536040 lmp.py:1943]   Total GPU:   8326 tokens
DEBUG 01-15 16:10:29.536014.536014 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:29.536014.536014 lmp.py:1944] 
DEBUG 01-15 16:10:29.536664.536664 cuda_h.py:19] end experts_map_get cost 0.0016429424285888672 seconds
DEBUG 01-15 16:10:29.536421.536421 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:29.536522.536522 lmp.py:1953] 
DEBUG 01-15 16:10:29.536522.536522 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:29.536981.536981 cuda_h.py:19] end cpu_experts_submit cost 5.6743621826171875e-05 seconds
DEBUG 01-15 16:10:29.536975.536975 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:29.536758.536758 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:29.536728.536728 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.32.gate_proj.weight': 0, 'model.layers.27.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.28.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.28.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.28.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.13.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.13.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.13.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.27.mlp.experts.2.gate_proj.weight': 0, 'model.layers.27.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(32203866112, 5767168, 0, 0), (32209633280, 5767168, 5767168, 0), (32198098944, 5767168, 11534336, 0), (31667519488, 5767168, 17301504, 0), (31673286656, 5767168, 23068672, 0), (31661752320, 5767168, 28835840, 0), (32238469120, 5767168, 34603008, 0), (32244236288, 5767168, 40370176, 0), (32232701952, 5767168, 46137344, 0), (31719424000, 5767168, 51904512, 0), (31725191168, 5767168, 57671680, 0), (31713656832, 5767168, 63438848, 0), (31771328512, 5767168, 69206016, 0), (31777095680, 5767168, 74973184, 0), (31765561344, 5767168, 80740352, 0), (31805931520, 5767168, 86507520, 0), (31811698688, 5767168, 92274688, 0), (31800164352, 5767168, 98041856, 0), (32359579648, 5767168, 103809024, 0), (32365346816, 5767168, 109576192, 0), (32353812480, 5767168, 115343360, 0), (32134660096, 5767168, 121110528, 0), (32140427264, 5767168, 126877696, 0), (32128892928, 5767168, 132644864, 0), (31875137536, 5767168, 138412032, 0), (31880904704, 5767168, 144179200, 0), (31869370368, 5767168, 149946368, 0), (32515293184, 5767168, 155713536, 0), (32521060352, 5767168, 161480704, 0), (32509526016, 5767168, 167247872, 0), (32567197696, 5767168, 173015040, 0), (32572964864, 5767168, 178782208, 0), (32561430528, 5767168, 184549376, 0), (32030851072, 5767168, 190316544, 0), (32036618240, 5767168, 196083712, 0), (32025083904, 5767168, 201850880, 0), (32653705216, 5767168, 207618048, 0), (32659472384, 5767168, 213385216, 0), (32647938048, 5767168, 219152384, 0), (32117358592, 5767168, 224919552, 0), (32123125760, 5767168, 230686720, 0), (32111591424, 5767168, 236453888, 0), (32688308224, 5767168, 242221056, 0), (32694075392, 5767168, 247988224, 0), (32682541056, 5767168, 253755392, 0), (32722911232, 5767168, 259522560, 0), (32728678400, 5767168, 265289728, 0), (32717144064, 5767168, 271056896, 0)], 2: [(31684820992, 5767168, 0, 0), (31690588160, 5767168, 5767168, 0), (31679053824, 5767168, 11534336, 0), (31702122496, 5767168, 17301504, 0), (31707889664, 5767168, 23068672, 0), (31696355328, 5767168, 28835840, 0), (32255770624, 5767168, 34603008, 0), (32261537792, 5767168, 40370176, 0), (32250003456, 5767168, 46137344, 0), (32290373632, 5767168, 51904512, 0), (32296140800, 5767168, 57671680, 0), (32284606464, 5767168, 63438848, 0), (32324976640, 5767168, 69206016, 0), (32330743808, 5767168, 74973184, 0), (32319209472, 5767168, 80740352, 0), (31788630016, 5767168, 86507520, 0), (31794397184, 5767168, 92274688, 0), (31782862848, 5767168, 98041856, 0), (31892439040, 5767168, 103809024, 0), (31898206208, 5767168, 109576192, 0), (31886671872, 5767168, 115343360, 0), (31909740544, 5767168, 121110528, 0), (31915507712, 5767168, 126877696, 0), (31903973376, 5767168, 132644864, 0), (31927042048, 5767168, 138412032, 0), (31932809216, 5767168, 144179200, 0), (31921274880, 5767168, 149946368, 0), (31978946560, 5767168, 155713536, 0), (31984713728, 5767168, 161480704, 0), (31973179392, 5767168, 167247872, 0), (32549896192, 5767168, 173015040, 0), (32555663360, 5767168, 178782208, 0), (32544129024, 5767168, 184549376, 0), (32013549568, 5767168, 190316544, 0), (32019316736, 5767168, 196083712, 0), (32007782400, 5767168, 201850880, 0), (32082755584, 5767168, 207618048, 0), (32088522752, 5767168, 213385216, 0), (32076988416, 5767168, 219152384, 0), (32151961600, 5767168, 224919552, 0), (32157728768, 5767168, 230686720, 0), (32146194432, 5767168, 236453888, 0), (32169263104, 5767168, 242221056, 0), (32175030272, 5767168, 247988224, 0), (32163495936, 5767168, 253755392, 0), (32740212736, 5767168, 259522560, 0), (32745979904, 5767168, 265289728, 0), (32734445568, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4372220>, 2: <capsule object NULL at 0x7a51b8589f80>}
DEBUG 01-15 16:10:29.537289.537289 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:29.537702.537702 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:29.537219.537219 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ed54e1da-9af7-483a-9dc1-fca81c38c1f8
DEBUG 01-15 16:10:29.537942.537942 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:29.537260.537260 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:29.537793.537793 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:29.538168.538168 cuda_h.py:19] end move_flatidxs cost 0.0008511543273925781 seconds
DEBUG 01-15 16:10:29.538183.538183 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:29.540992.540992 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ed54e1da-9af7-483a-9dc1-fca81c38c1f8
DEBUG 01-15 16:10:29.540081.540081 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004278659820556641 seconds
DEBUG 01-15 16:10:29.540991.540991 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:29.544403.544403 cuda_h.py:19] end restore2model cost 0.0035643577575683594 seconds
DEBUG 01-15 16:10:29.544167.544167 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008086204528808594 seconds
DEBUG 01-15 16:10:29.543281.543281 cuda_h.py:19] end group_tensors cost 0.004880428314208984 seconds
DEBUG 01-15 16:10:29.544275.544275 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:29.544569.544569 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:29.544585.544585 cuda_h.py:19] end gpu_sexperts cost 0.0005033016204833984 seconds
DEBUG 01-15 16:10:29.545251.545251 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:29.545696.545696 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.546934.546934 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001489877700805664 seconds
DEBUG 01-15 16:10:29.546447.546447 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.547072.547072 cuda_h.py:19] end gpu_group_list cost 0.0002453327178955078 seconds
DEBUG 01-15 16:10:29.548555.548555 cuda_h.py:19] end group pad cost 0.0036950111389160156 seconds
DEBUG 01-15 16:10:29.548398.548398 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:29.548132.548132 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:29.550466.550466 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0019299983978271484 seconds
DEBUG 01-15 16:10:29.550694.550694 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:29.551814.551814 cuda_h.py:19] end gpu_group_list cost 0.00037479400634765625 seconds
DEBUG 01-15 16:10:29.552532.552532 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:29.553770.553770 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ed54e1da-9af7-483a-9dc1-fca81c38c1f8
INFO 01-15 16:10:29.567334.567334 client.py:127] Model loaded
DEBUG 01-15 16:10:29.567398.567398 cuda_h.py:19] end wait_experts_multi_device cost 0.014226675033569336 seconds
DEBUG 01-15 16:10:29.567459.567459 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:29.568917.568917 cuda_h.py:19] end group_einsum cost 0.02017807960510254 seconds
DEBUG 01-15 16:10:29.568915.568915 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:29.572387.572387 cuda_h.py:19] end get_outputs_cpu1 cost 0.003715038299560547 seconds
DEBUG 01-15 16:10:29.573794.573794 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0354456901550293 seconds
DEBUG 01-15 16:10:29.573269.573269 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00632929801940918 seconds
DEBUG 01-15 16:10:29.573319.573319 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:29.575243.575243 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.576820.576820 cuda_h.py:19] end index_scatter cost 0.00011420249938964844 seconds
DEBUG 01-15 16:10:29.576378.576378 cuda_h.py:19] end cpuoutputsdeal cost 0.0026798248291015625 seconds
DEBUG 01-15 16:10:29.576859.576859 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:29.576456.576456 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.577624.577624 cuda_h.py:19] end gpu_group_tensor cost 0.0002129077911376953 seconds
DEBUG 01-15 16:10:29.577984.577984 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:29.577224.577224 cuda_h.py:19] end gpu_group_tensor cost 0.0001976490020751953 seconds
DEBUG 01-15 16:10:29.577143.577143 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.578575.578575 cuda_h.py:19] end gpu_group_einsum cost 0.0007727146148681641 seconds
DEBUG 01-15 16:10:29.578290.578290 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:29.579254.579254 cuda_h.py:19] end gpu_group_einsum cost 0.0006322860717773438 seconds
DEBUG 01-15 16:10:29.579716.579716 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.579397.579397 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.579080.579080 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003712177276611328 seconds
DEBUG 01-15 16:10:29.579247.579247 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.580577.580577 cuda_h.py:19] end concat_expert_out cost 8.058547973632812e-05 seconds
DEBUG 01-15 16:10:29.580428.580428 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.580420.580420 cuda_h.py:19] end index_scatter cost 9.369850158691406e-05 seconds
DEBUG 01-15 16:10:29.580028.580028 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0013151168823242188 seconds
DEBUG 01-15 16:10:29.580185.580185 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:29.580400.580400 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:29.581868.581868 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002932548522949219 seconds
DEBUG 01-15 16:10:29.581406.581406 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:29.581928.581928 cuda_h.py:19] end concat_expert_out cost 8.749961853027344e-05 seconds
DEBUG 01-15 16:10:29.581202.581202 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:29.581022.581022 cuda_h.py:19] end index_scatter cost 9.059906005859375e-05 seconds
DEBUG 01-15 16:10:29.581481.581481 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008614063262939453 seconds
DEBUG 01-15 16:10:29.581908.581908 cuda_h.py:19] end gpu_experts_multi_device cost 0.036783695220947266 seconds
DEBUG 01-15 16:10:29.582363.582363 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.04836249351501465 seconds
DEBUG 01-15 16:10:29.582610.582610 cuda_h.py:19] end prefill_layer cost 0.053537845611572266 seconds
DEBUG 01-15 16:10:29.582916.582916 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:10:29.582526.582526 cuda_h.py:19] end prefill cost 1.530280351638794 seconds
DEBUG 01-15 16:10:31.807884.807884 cuda_h.py:10] start generate_input_ids
generate input ids cost 0.09206414222717285 s
DEBUG 01-15 16:10:32.179526.179526 cuda_h.py:19] end generate_input_ids cost 0.3717780113220215 seconds
DEBUG 01-15 16:10:32.180073.180073 cuda_h.py:10] start init_cache
DEBUG 01-15 16:10:32.180747.180747 cuda_h.py:19] end init_cache cost 0.00010061264038085938 seconds
DEBUG 01-15 16:10:34.801943.801943 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:10:34.803514.803514 cuda_h.py:19] end init_meta_layer cost 1.33514404296875e-05 seconds
DEBUG 01-15 16:10:34.803758.803758 cuda_h.py:10] start init_weights
DEBUG 01-15 16:10:34.803620.803620 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:34.803860.803860 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:34.804313.804313 cuda_h.py:19] end allocate_cuda_memory cost 0.0006537437438964844 seconds
DEBUG 01-15 16:10:34.804746.804746 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:34.804555.804555 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:34.804378.804378 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:34.804750.804750 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e92e2715-d312-4abe-80a8-e96714fabe36
DEBUG 01-15 16:10:34.804210.804210 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:34.806101.806101 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e92e2715-d312-4abe-80a8-e96714fabe36
DEBUG 01-15 16:10:34.806865.806865 cuda_h.py:19] end load_into_gpu_async cost 0.0021228790283203125 seconds
DEBUG 01-15 16:10:34.806806.806806 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:34.807981.807981 cuda_h.py:19] end restore_tensors2 cost 6.651878356933594e-05 seconds
DEBUG 01-15 16:10:34.807307.807307 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003081083297729492 seconds
DEBUG 01-15 16:10:34.807910.807910 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:34.807029.807029 cuda_h.py:19] end restore2model cost 0.00016808509826660156 seconds
INFO 01-15 16:10:34.807408.807408 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e92e2715-d312-4abe-80a8-e96714fabe36
INFO 01-15 16:10:34.882903.882903 client.py:127] Model loaded
DEBUG 01-15 16:10:34.882009.882009 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:10:34.882576.882576 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:34.882276.882276 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:34.882457.882457 cuda_h.py:19] end allocate_cuda_memory cost 0.0003533363342285156 seconds
DEBUG 01-15 16:10:34.883998.883998 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:34.883213.883213 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:34.883826.883826 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:34.883590.883590 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9ad4a231-96df-4399-b22a-7f0cabbbc033
DEBUG 01-15 16:10:34.883874.883874 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:34.885434.885434 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9ad4a231-96df-4399-b22a-7f0cabbbc033
DEBUG 01-15 16:10:34.885202.885202 cuda_h.py:19] end load_into_gpu_async cost 0.002229452133178711 seconds
DEBUG 01-15 16:10:34.885556.885556 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:34.885868.885868 cuda_h.py:19] end restore_tensors2 cost 0.0001475811004638672 seconds
DEBUG 01-15 16:10:34.885421.885421 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003381490707397461 seconds
INFO 01-15 16:10:34.885066.885066 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9ad4a231-96df-4399-b22a-7f0cabbbc033
INFO 01-15 16:10:34.900457.900457 client.py:127] Model loaded
DEBUG 01-15 16:10:34.900019.900019 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:34.901247.901247 cuda_h.py:19] end restore2model cost 0.0009720325469970703 seconds
DEBUG 01-15 16:10:34.902715.902715 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.01979827880859375 seconds
DEBUG 01-15 16:10:34.902414.902414 cuda_h.py:19] end init_weights cost 0.09826326370239258 seconds
DEBUG 01-15 16:10:34.902178.902178 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:10:35.669719.669719 cuda_h.py:19] end copy_emodel cost 0.7666630744934082 seconds
DEBUG 01-15 16:10:35.669195.669195 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:10:35.670744.670744 cuda_h.py:19] end init_inputs_tokens cost 0.0005340576171875 seconds
DEBUG 01-15 16:10:35.670435.670435 cuda_h.py:10] start prefill
DEBUG 01-15 16:10:35.670297.670297 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:35.670715.670715 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:10:35.670319.670319 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:35.670737.670737 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:35.670256.670256 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 3.647804260253906e-05 seconds
DEBUG 01-15 16:10:35.670720.670720 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:10:35.670317.670317 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:35.670683.670683 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:35.670958.670958 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:35.671971.671971 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:35.671927.671927 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:35.671876.671876 cuda_h.py:19] end allocate_cuda_memory cost 0.00022983551025390625 seconds
DEBUG 01-15 16:10:35.671469.671469 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:35.671802.671802 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.671076.671076 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.671368.671368 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 91fb3aff-3eb5-4fd5-beb7-6eeca66fa9f7
DEBUG 01-15 16:10:35.671684.671684 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.672027.672027 cuda_h.py:10] start self_attn
INFO 01-15 16:10:35.673077.673077 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 91fb3aff-3eb5-4fd5-beb7-6eeca66fa9f7
DEBUG 01-15 16:10:35.673192.673192 cuda_h.py:19] end load_into_gpu_async cost 0.0017611980438232422 seconds
DEBUG 01-15 16:10:35.673247.673247 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:35.673444.673444 cuda_h.py:19] end restore_tensors2 cost 9.34600830078125e-05 seconds
DEBUG 01-15 16:10:35.673069.673069 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024788379669189453 seconds
INFO 01-15 16:10:35.673608.673608 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 91fb3aff-3eb5-4fd5-beb7-6eeca66fa9f7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:35.677206.677206 cuda_h.py:19] end self_attn cost 0.00545811653137207 seconds
DEBUG 01-15 16:10:35.679596.679596 cuda_h.py:19] end iln_self_attn_paln cost 0.008321285247802734 seconds
DEBUG 01-15 16:10:35.679141.679141 cuda_h.py:10] start dense_mlp
INFO 01-15 16:10:35.681807.681807 client.py:127] Model loaded
DEBUG 01-15 16:10:35.681485.681485 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.682184.682184 cuda_h.py:19] end restore2model cost 0.0006322860717773438 seconds
DEBUG 01-15 16:10:35.682047.682047 cuda_h.py:19] end sllm_worker_task cost 0.011192798614501953 seconds
DEBUG 01-15 16:10:35.682752.682752 cuda_h.py:19] end dense_mlp cost 0.0031816959381103516 seconds
DEBUG 01-15 16:10:35.682406.682406 cuda_h.py:19] end prefill_layer cost 0.011916637420654297 seconds
DEBUG 01-15 16:10:35.682407.682407 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:10:35.682010.682010 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:35.682044.682044 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:10:35.682648.682648 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:35.682443.682443 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:35.682889.682889 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 2.2172927856445312e-05 seconds
DEBUG 01-15 16:10:35.682022.682022 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:35.682526.682526 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:35.682177.682177 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:35.682543.682543 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:35.682446.682446 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:35.682648.682648 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:35.683563.683563 cuda_h.py:19] end allocate_cuda_memory cost 0.00019550323486328125 seconds
DEBUG 01-15 16:10:35.683183.683183 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:35.683000.683000 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.683764.683764 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.683150.683150 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 780bf993-bc35-438f-a5a2-be9ee45ca712
DEBUG 01-15 16:10:35.683274.683274 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.683281.683281 cuda_h.py:10] start self_attn
INFO 01-15 16:10:35.684088.684088 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 780bf993-bc35-438f-a5a2-be9ee45ca712
DEBUG 01-15 16:10:35.684032.684032 cuda_h.py:19] end load_into_gpu_async cost 0.0013539791107177734 seconds
DEBUG 01-15 16:10:35.684419.684419 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:35.685852.685852 cuda_h.py:19] end restore_tensors2 cost 9.274482727050781e-05 seconds
DEBUG 01-15 16:10:35.685974.685974 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002268552780151367 seconds
INFO 01-15 16:10:35.685116.685116 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 780bf993-bc35-438f-a5a2-be9ee45ca712
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:35.687811.687811 cuda_h.py:19] end self_attn cost 0.00408172607421875 seconds
DEBUG 01-15 16:10:35.688842.688842 cuda_h.py:19] end iln_self_attn_paln cost 0.005643606185913086 seconds
DEBUG 01-15 16:10:35.688639.688639 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:10:35.688309.688309 cuda_h.py:10] start gate
DEBUG 01-15 16:10:35.689235.689235 cuda_h.py:19] end gate cost 0.0009267330169677734 seconds
DEBUG 01-15 16:10:35.689025.689025 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:35.689274.689274 lmp.py:1912] 
DEBUG 01-15 16:10:35.689274.689274 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:35.689321.689321 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:35.690070.690070 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:35.690720.690720 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:35.690747.690747 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:35.690298.690298 lmp.py:1917] 
DEBUG 01-15 16:10:35.690298.690298 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:35.690609.690609 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:35.690643.690643 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:10:35.690717.690717 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:10:35.690790.690790 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:35.690149.690149 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:10:35.690984.690984 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:10:35.690103.690103 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:10:35.690177.690177 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:10:35.690012.690012 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:10:35.690801.690801 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:10:35.690066.690066 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:10:35.690570.690570 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:10:35.690598.690598 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:10:35.690433.690433 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:10:35.690791.690791 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:10:35.690387.690387 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:10:35.690653.690653 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:10:35.690011.690011 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:10:35.690370.690370 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:10:35.690728.690728 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:10:35.690040.690040 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:10:35.690067.690067 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:10:35.690856.690856 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:10:35.690406.690406 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:10:35.690003.690003 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:10:35.690361.690361 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:10:35.690481.690481 lmp.py:1935]   Expert 34 |    161 | CPU
DEBUG 01-15 16:10:35.690316.690316 lmp.py:1935]   Expert 35 |    164 | CPU
DEBUG 01-15 16:10:35.690435.690435 lmp.py:1935]   Expert 36 |    168 | CPU
DEBUG 01-15 16:10:35.690032.690032 lmp.py:1935]   Expert 11 |    170 | CPU
DEBUG 01-15 16:10:35.690152.690152 lmp.py:1935]   Expert 17 |    170 | CPU
DEBUG 01-15 16:10:35.690749.690749 lmp.py:1935]   Expert 59 |    174 | CPU
DEBUG 01-15 16:10:35.690060.690060 lmp.py:1935]   Expert 10 |    180 | CPU
DEBUG 01-15 16:10:35.690995.690995 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.690453.690453 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.690241.690241 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.690553.690553 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.690342.690342 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.690416.690416 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.690727.690727 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.690039.690039 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.690351.690351 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.690054.690054 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.690128.690128 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.690632.690632 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.690089.690089 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.690262.690262 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691289.691289 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691601.691601 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.691152.691152 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691225.691225 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.691014.691014 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.691326.691326 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691876.691876 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.691426.691426 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691977.691977 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.691481.691481 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691223.691223 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.691966.691966 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.691231.691231 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691305.691305 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691332.691332 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.691405.691405 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691671.691671 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.691506.691506 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.691103.691103 lmp.py:1937] 
DEBUG 01-15 16:10:35.691103.691103 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:35.691176.691176 lmp.py:1938]   CPU:   4059 tokens
DEBUG 01-15 16:10:35.691442.691442 lmp.py:1942]   cuda:1:   4114 tokens (16 experts)
DEBUG 01-15 16:10:35.691469.691469 lmp.py:1942]   cuda:2:   4115 tokens (16 experts)
DEBUG 01-15 16:10:35.691973.691973 lmp.py:1943]   Total GPU:   8229 tokens
DEBUG 01-15 16:10:35.691761.691761 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:35.691761.691761 lmp.py:1944] 
DEBUG 01-15 16:10:35.691034.691034 cuda_h.py:19] end experts_map_get cost 0.0020122528076171875 seconds
DEBUG 01-15 16:10:35.691335.691335 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:35.691567.691567 lmp.py:1953] 
DEBUG 01-15 16:10:35.691567.691567 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:35.691404.691404 cuda_h.py:19] end cpu_experts_submit cost 5.459785461425781e-05 seconds
DEBUG 01-15 16:10:35.691074.691074 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:35.691903.691903 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:35.692775.692775 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.1.mlp.experts.33.gate_proj.weight': 0, 'model.layers.1.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.14.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.14.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.14.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.15.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.15.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.15.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.21.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.21.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.21.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.23.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.23.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.23.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.56.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3535273984, 5767168, 34603008, 0), (3541041152, 5767168, 40370176, 0), (3529506816, 5767168, 46137344, 0), (3016228864, 5767168, 51904512, 0), (3021996032, 5767168, 57671680, 0), (3010461696, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3690987520, 5767168, 86507520, 0), (3696754688, 5767168, 92274688, 0), (3685220352, 5767168, 98041856, 0), (3362258944, 5767168, 103809024, 0), (3368026112, 5767168, 109576192, 0), (3356491776, 5767168, 115343360, 0), (3725590528, 5767168, 121110528, 0), (3731357696, 5767168, 126877696, 0), (3719823360, 5767168, 132644864, 0), (3189243904, 5767168, 138412032, 0), (3195011072, 5767168, 144179200, 0), (3183476736, 5767168, 149946368, 0), (3206545408, 5767168, 155713536, 0), (3212312576, 5767168, 161480704, 0), (3200778240, 5767168, 167247872, 0), (3275751424, 5767168, 173015040, 0), (3281518592, 5767168, 178782208, 0), (3269984256, 5767168, 184549376, 0), (3846701056, 5767168, 190316544, 0), (3852468224, 5767168, 196083712, 0), (3840933888, 5767168, 201850880, 0), (3310354432, 5767168, 207618048, 0), (3316121600, 5767168, 213385216, 0), (3304587264, 5767168, 219152384, 0), (3915907072, 5767168, 224919552, 0), (3921674240, 5767168, 230686720, 0), (3910139904, 5767168, 236453888, 0), (3379560448, 5767168, 242221056, 0), (3385327616, 5767168, 247988224, 0), (3373793280, 5767168, 253755392, 0), (3950510080, 5767168, 259522560, 0), (3956277248, 5767168, 265289728, 0), (3944742912, 5767168, 271056896, 0)], 2: [(3431464960, 5767168, 0, 0), (3437232128, 5767168, 5767168, 0), (3425697792, 5767168, 11534336, 0), (2895118336, 5767168, 17301504, 0), (2900885504, 5767168, 23068672, 0), (2889351168, 5767168, 28835840, 0), (2929721344, 5767168, 34603008, 0), (2935488512, 5767168, 40370176, 0), (2923954176, 5767168, 46137344, 0), (2947022848, 5767168, 51904512, 0), (2952790016, 5767168, 57671680, 0), (2941255680, 5767168, 63438848, 0), (3552575488, 5767168, 69206016, 0), (3558342656, 5767168, 74973184, 0), (3546808320, 5767168, 80740352, 0), (2998927360, 5767168, 86507520, 0), (3004694528, 5767168, 92274688, 0), (2993160192, 5767168, 98041856, 0), (3587178496, 5767168, 103809024, 0), (3592945664, 5767168, 109576192, 0), (3581411328, 5767168, 115343360, 0), (3639083008, 5767168, 121110528, 0), (3644850176, 5767168, 126877696, 0), (3633315840, 5767168, 132644864, 0), (3656384512, 5767168, 138412032, 0), (3662151680, 5767168, 144179200, 0), (3650617344, 5767168, 149946368, 0), (3102736384, 5767168, 155713536, 0), (3108503552, 5767168, 161480704, 0), (3096969216, 5767168, 167247872, 0), (3137339392, 5767168, 173015040, 0), (3143106560, 5767168, 178782208, 0), (3131572224, 5767168, 184549376, 0), (3120037888, 5767168, 190316544, 0), (3125805056, 5767168, 196083712, 0), (3114270720, 5767168, 201850880, 0), (3777495040, 5767168, 207618048, 0), (3783262208, 5767168, 213385216, 0), (3771727872, 5767168, 219152384, 0), (3223846912, 5767168, 224919552, 0), (3229614080, 5767168, 230686720, 0), (3218079744, 5767168, 236453888, 0), (3258449920, 5767168, 242221056, 0), (3264217088, 5767168, 247988224, 0), (3252682752, 5767168, 253755392, 0), (3829399552, 5767168, 259522560, 0), (3835166720, 5767168, 265289728, 0), (3823632384, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e3464b7e0>, 2: <capsule object NULL at 0x7a4e3464b9f0>}
INFO 01-15 16:10:35.692580.692580 client.py:127] Model loaded
DEBUG 01-15 16:10:35.693016.693016 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.693212.693212 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.693305.693305 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:35.693531.693531 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.693613.693613 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4e1591b5-1be7-48ee-bab8-c02eeb0e63fd
DEBUG 01-15 16:10:35.693043.693043 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.693462.693462 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:35.694765.694765 cuda_h.py:19] end restore2model cost 0.0011358261108398438 seconds
DEBUG 01-15 16:10:35.694073.694073 cuda_h.py:19] end sllm_worker_task cost 0.011800289154052734 seconds
DEBUG 01-15 16:10:35.694084.694084 cuda_h.py:19] end move_flatidxs cost 0.0008676052093505859 seconds
DEBUG 01-15 16:10:35.694165.694165 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:35.694421.694421 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4e1591b5-1be7-48ee-bab8-c02eeb0e63fd
DEBUG 01-15 16:10:35.695503.695503 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036385059356689453 seconds
DEBUG 01-15 16:10:35.695559.695559 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.698273.698273 cuda_h.py:19] end restore2model cost 0.0030863285064697266 seconds
DEBUG 01-15 16:10:35.698699.698699 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006967067718505859 seconds
DEBUG 01-15 16:10:35.698614.698614 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:35.699645.699645 cuda_h.py:19] end gpu_sexperts cost 0.00030517578125 seconds
DEBUG 01-15 16:10:35.699382.699382 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:35.699165.699165 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8596649169921875e-05 seconds
DEBUG 01-15 16:10:35.699722.699722 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:35.699763.699763 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.700389.700389 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010602474212646484 seconds
DEBUG 01-15 16:10:35.700723.700723 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.700399.700399 cuda_h.py:19] end gpu_group_list cost 0.0001842975616455078 seconds
DEBUG 01-15 16:10:35.700515.700515 cuda_h.py:19] end group_tensors cost 0.006040811538696289 seconds
DEBUG 01-15 16:10:35.701638.701638 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:35.702987.702987 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.704069.704069 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0023458003997802734 seconds
DEBUG 01-15 16:10:35.704635.704635 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.705557.705557 cuda_h.py:19] end gpu_group_list cost 0.00021600723266601562 seconds
DEBUG 01-15 16:10:35.705899.705899 cuda_h.py:19] end group pad cost 0.004177570343017578 seconds
DEBUG 01-15 16:10:35.705265.705265 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:35.706408.706408 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:35.706913.706913 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4e1591b5-1be7-48ee-bab8-c02eeb0e63fd
INFO 01-15 16:10:35.721244.721244 client.py:127] Model loaded
DEBUG 01-15 16:10:35.721381.721381 cuda_h.py:19] end wait_experts_multi_device cost 0.015358686447143555 seconds
DEBUG 01-15 16:10:35.721681.721681 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:35.736224.736224 cuda_h.py:19] end group_einsum cost 0.030519485473632812 seconds
DEBUG 01-15 16:10:35.736057.736057 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:35.742871.742871 cuda_h.py:19] end get_outputs_cpu1 cost 0.0060405731201171875 seconds
DEBUG 01-15 16:10:35.743394.743394 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05003166198730469 seconds
DEBUG 01-15 16:10:35.744768.744768 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.022705554962158203 seconds
DEBUG 01-15 16:10:35.744040.744040 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:35.747015.747015 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.747661.747661 cuda_h.py:19] end index_scatter cost 0.00016379356384277344 seconds
DEBUG 01-15 16:10:35.748512.748512 cuda_h.py:19] end cpuoutputsdeal cost 0.003521442413330078 seconds
DEBUG 01-15 16:10:35.748410.748410 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:35.748512.748512 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.749534.749534 cuda_h.py:19] end gpu_group_tensor cost 0.0011603832244873047 seconds
DEBUG 01-15 16:10:35.749293.749293 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.750663.750663 cuda_h.py:19] end gpu_group_tensor cost 0.0010993480682373047 seconds
DEBUG 01-15 16:10:35.751367.751367 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.752132.752132 cuda_h.py:19] end gpu_group_einsum cost 0.0009953975677490234 seconds
DEBUG 01-15 16:10:35.752780.752780 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.753872.753872 cuda_h.py:19] end gpu_group_einsum cost 0.0012977123260498047 seconds
DEBUG 01-15 16:10:35.753089.753089 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.754721.754721 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.754932.754932 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002968311309814453 seconds
DEBUG 01-15 16:10:35.754901.754901 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.754726.754726 cuda_h.py:19] end concat_expert_out cost 0.00027680397033691406 seconds
DEBUG 01-15 16:10:35.755359.755359 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.755192.755192 cuda_h.py:19] end index_scatter cost 9.822845458984375e-05 seconds
DEBUG 01-15 16:10:35.755521.755521 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001550436019897461 seconds
DEBUG 01-15 16:10:35.755366.755366 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.755660.755660 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.755829.755829 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025463104248046875 seconds
DEBUG 01-15 16:10:35.756314.756314 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.756093.756093 cuda_h.py:19] end concat_expert_out cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:10:35.756778.756778 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.756947.756947 cuda_h.py:19] end index_scatter cost 7.700920104980469e-05 seconds
DEBUG 01-15 16:10:35.756175.756175 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007543563842773438 seconds
DEBUG 01-15 16:10:35.756396.756396 cuda_h.py:19] end gpu_experts_multi_device cost 0.05727982521057129 seconds
DEBUG 01-15 16:10:35.756983.756983 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.06819367408752441 seconds
DEBUG 01-15 16:10:35.756154.756154 cuda_h.py:19] end prefill_layer cost 0.07447576522827148 seconds
DEBUG 01-15 16:10:35.757455.757455 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:10:35.757933.757933 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:35.757126.757126 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:10:35.757035.757035 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:35.757520.757520 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:35.757344.757344 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 4.172325134277344e-05 seconds
DEBUG 01-15 16:10:35.757353.757353 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:35.757759.757759 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 0.0001723766326904297 seconds
DEBUG 01-15 16:10:35.757675.757675 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:35.757465.757465 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:35.757130.757130 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:35.757769.757769 cuda_h.py:19] end allocate_cuda_memory cost 0.00020194053649902344 seconds
DEBUG 01-15 16:10:35.758309.758309 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:35.758430.758430 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:35.758017.758017 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.758694.758694 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.758635.758635 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ae52d038-4323-4c83-91c8-84c49e25cc29
DEBUG 01-15 16:10:35.758234.758234 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.758904.758904 cuda_h.py:10] start self_attn
INFO 01-15 16:10:35.759769.759769 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ae52d038-4323-4c83-91c8-84c49e25cc29
DEBUG 01-15 16:10:35.759109.759109 cuda_h.py:19] end load_into_gpu_async cost 0.0013637542724609375 seconds
DEBUG 01-15 16:10:35.759402.759402 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:35.759870.759870 cuda_h.py:19] end restore_tensors2 cost 9.369850158691406e-05 seconds
DEBUG 01-15 16:10:35.759560.759560 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021898746490478516 seconds
INFO 01-15 16:10:35.759199.759199 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ae52d038-4323-4c83-91c8-84c49e25cc29
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:35.763553.763553 cuda_h.py:19] end self_attn cost 0.004677295684814453 seconds
DEBUG 01-15 16:10:35.763830.763830 cuda_h.py:19] end iln_self_attn_paln cost 0.006215095520019531 seconds
DEBUG 01-15 16:10:35.763467.763467 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:10:35.764753.764753 cuda_h.py:10] start gate
DEBUG 01-15 16:10:35.764077.764077 cuda_h.py:19] end gate cost 0.0007276535034179688 seconds
DEBUG 01-15 16:10:35.764151.764151 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:35.765625.765625 lmp.py:1912] 
DEBUG 01-15 16:10:35.765625.765625 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:35.765096.765096 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:35.765329.765329 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:35.765740.765740 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:35.765244.765244 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:35.765795.765795 lmp.py:1917] 
DEBUG 01-15 16:10:35.765795.765795 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:35.765106.765106 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:35.765617.765617 lmp.py:1935]   Expert 58 |     51 | CPU
DEBUG 01-15 16:10:35.765883.765883 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:10:35.765672.765672 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:35.765984.765984 lmp.py:1935]   Expert 17 |     84 | CPU
DEBUG 01-15 16:10:35.765819.765819 lmp.py:1935]   Expert 24 |     87 | CPU
DEBUG 01-15 16:10:35.765130.765130 lmp.py:1935]   Expert  0 |     89 | CPU
DEBUG 01-15 16:10:35.765966.765966 lmp.py:1935]   Expert 28 |    104 | CPU
DEBUG 01-15 16:10:35.765277.765277 lmp.py:1935]   Expert 34 |    115 | CPU
DEBUG 01-15 16:10:35.765020.765020 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:10:35.765809.765809 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:10:35.765836.765836 lmp.py:1935]   Expert  9 |    130 | CPU
DEBUG 01-15 16:10:35.765578.765578 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:10:35.765129.765129 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:10:35.765964.765964 lmp.py:1935]   Expert 23 |    135 | CPU
DEBUG 01-15 16:10:35.765037.765037 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:35.765872.765872 lmp.py:1935]   Expert 30 |    143 | CPU
DEBUG 01-15 16:10:35.765230.765230 lmp.py:1935]   Expert 45 |    146 | CPU
DEBUG 01-15 16:10:35.765065.765065 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:10:35.765662.765662 lmp.py:1935]   Expert 57 |    152 | CPU
DEBUG 01-15 16:10:35.765497.765497 lmp.py:1935]   Expert  1 |    153 | CPU
DEBUG 01-15 16:10:35.765571.765571 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:10:35.765121.765121 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:10:35.765194.765194 lmp.py:1935]   Expert 29 |    160 | CPU
DEBUG 01-15 16:10:35.765268.765268 lmp.py:1935]   Expert 25 |    165 | CPU
DEBUG 01-15 16:10:35.765626.765626 lmp.py:1935]   Expert 54 |    167 | CPU
DEBUG 01-15 16:10:35.765984.765984 lmp.py:1935]   Expert 49 |    170 | CPU
DEBUG 01-15 16:10:35.765819.765819 lmp.py:1935]   Expert  6 |    171 | CPU
DEBUG 01-15 16:10:35.766178.766178 lmp.py:1935]   Expert 48 |    172 | CPU
DEBUG 01-15 16:10:35.766536.766536 lmp.py:1935]   Expert 35 |    175 | CPU
DEBUG 01-15 16:10:35.766371.766371 lmp.py:1935]   Expert 12 |    176 | CPU
DEBUG 01-15 16:10:35.766491.766491 lmp.py:1935]   Expert 37 |    177 | CPU
DEBUG 01-15 16:10:35.766087.766087 lmp.py:1935]   Expert 60 |    186 | CPU
DEBUG 01-15 16:10:35.766068.766068 lmp.py:1935]   Expert 13 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766718.766718 lmp.py:1935]   Expert 33 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766460.766460 lmp.py:1935]   Expert 53 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766964.766964 lmp.py:1935]   Expert 10 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766515.766515 lmp.py:1935]   Expert 16 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766304.766304 lmp.py:1935]   Expert 21 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766092.766092 lmp.py:1935]   Expert 40 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766643.766643 lmp.py:1935]   Expert 43 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766431.766431 lmp.py:1935]   Expert 38 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766220.766220 lmp.py:1935]   Expert  5 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766532.766532 lmp.py:1935]   Expert 44 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766274.766274 lmp.py:1935]   Expert 50 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766540.766540 lmp.py:1935]   Expert 52 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766282.766282 lmp.py:1935]   Expert 19 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766263.766263 lmp.py:1935]   Expert 41 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766290.766290 lmp.py:1935]   Expert  4 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766079.766079 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766629.766629 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766180.766180 lmp.py:1935]   Expert 56 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766730.766730 lmp.py:1935]   Expert 31 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766042.766042 lmp.py:1935]   Expert 20 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766592.766592 lmp.py:1935]   Expert 39 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766096.766096 lmp.py:1935]   Expert 22 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766839.766839 lmp.py:1935]   Expert  2 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766866.766866 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766893.766893 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766682.766682 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766709.766709 lmp.py:1935]   Expert 18 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766782.766782 lmp.py:1935]   Expert 14 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766571.766571 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766645.766645 lmp.py:1935]   Expert 11 |    389 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.766957.766957 lmp.py:1935]   Expert 61 |    461 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.766076.766076 lmp.py:1937] 
DEBUG 01-15 16:10:35.766076.766076 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:35.766865.766865 lmp.py:1938]   CPU:   4338 tokens
DEBUG 01-15 16:10:35.766131.766131 lmp.py:1942]   cuda:1:   3975 tokens (16 experts)
DEBUG 01-15 16:10:35.766158.766158 lmp.py:1942]   cuda:2:   3975 tokens (16 experts)
DEBUG 01-15 16:10:35.766947.766947 lmp.py:1943]   Total GPU:   7950 tokens
DEBUG 01-15 16:10:35.766782.766782 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:35.766782.766782 lmp.py:1944] 
DEBUG 01-15 16:10:35.766577.766577 cuda_h.py:19] end experts_map_get cost 0.002003908157348633 seconds
INFO 01-15 16:10:35.766475.766475 client.py:127] Model loaded
DEBUG 01-15 16:10:35.767788.767788 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.767528.767528 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:35.767834.767834 lmp.py:1953] 
DEBUG 01-15 16:10:35.767834.767834 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:35.767578.767578 cuda_h.py:19] end cpu_experts_submit cost 5.6743621826171875e-05 seconds
DEBUG 01-15 16:10:35.767658.767658 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:35.767680.767680 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:35.767652.767652 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.33.gate_proj.weight': 0, 'model.layers.2.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.52.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.52.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.52.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.2.mlp.experts.2.gate_proj.weight': 0, 'model.layers.2.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.40.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.40.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.40.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.47.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.47.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.47.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.50.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.50.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.50.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.20.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.20.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.20.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(4538761216, 5767168, 0, 0), (4544528384, 5767168, 5767168, 0), (4532994048, 5767168, 11534336, 0), (4037017600, 5767168, 17301504, 0), (4042784768, 5767168, 23068672, 0), (4031250432, 5767168, 28835840, 0), (4625268736, 5767168, 34603008, 0), (4631035904, 5767168, 40370176, 0), (4619501568, 5767168, 46137344, 0), (4642570240, 5767168, 51904512, 0), (4648337408, 5767168, 57671680, 0), (4636803072, 5767168, 63438848, 0), (4694474752, 5767168, 69206016, 0), (4700241920, 5767168, 74973184, 0), (4688707584, 5767168, 80740352, 0), (4711776256, 5767168, 86507520, 0), (4717543424, 5767168, 92274688, 0), (4706009088, 5767168, 98041856, 0), (4210032640, 5767168, 103809024, 0), (4215799808, 5767168, 109576192, 0), (4204265472, 5767168, 115343360, 0), (4296540160, 5767168, 121110528, 0), (4302307328, 5767168, 126877696, 0), (4290772992, 5767168, 132644864, 0), (4867489792, 5767168, 138412032, 0), (4873256960, 5767168, 144179200, 0), (4861722624, 5767168, 149946368, 0), (4331143168, 5767168, 155713536, 0), (4336910336, 5767168, 161480704, 0), (4325376000, 5767168, 167247872, 0), (4348444672, 5767168, 173015040, 0), (4354211840, 5767168, 178782208, 0), (4342677504, 5767168, 184549376, 0), (4884791296, 5767168, 190316544, 0), (4890558464, 5767168, 196083712, 0), (4879024128, 5767168, 201850880, 0), (4936695808, 5767168, 207618048, 0), (4942462976, 5767168, 213385216, 0), (4930928640, 5767168, 219152384, 0), (4988600320, 5767168, 224919552, 0), (4994367488, 5767168, 230686720, 0), (4982833152, 5767168, 236453888, 0), (5023203328, 5767168, 242221056, 0), (5028970496, 5767168, 247988224, 0), (5017436160, 5767168, 253755392, 0), (5057806336, 5767168, 259522560, 0), (5063573504, 5767168, 265289728, 0), (5052039168, 5767168, 271056896, 0)], 2: [(4002414592, 5767168, 0, 0), (4008181760, 5767168, 5767168, 0), (3996647424, 5767168, 11534336, 0), (4054319104, 5767168, 17301504, 0), (4060086272, 5767168, 23068672, 0), (4048551936, 5767168, 28835840, 0), (4659871744, 5767168, 34603008, 0), (4665638912, 5767168, 40370176, 0), (4654104576, 5767168, 46137344, 0), (4677173248, 5767168, 51904512, 0), (4682940416, 5767168, 57671680, 0), (4671406080, 5767168, 63438848, 0), (4140826624, 5767168, 69206016, 0), (4146593792, 5767168, 74973184, 0), (4135059456, 5767168, 80740352, 0), (4158128128, 5767168, 86507520, 0), (4163895296, 5767168, 92274688, 0), (4152360960, 5767168, 98041856, 0), (4729077760, 5767168, 103809024, 0), (4734844928, 5767168, 109576192, 0), (4723310592, 5767168, 115343360, 0), (4192731136, 5767168, 121110528, 0), (4198498304, 5767168, 126877696, 0), (4186963968, 5767168, 132644864, 0), (4763680768, 5767168, 138412032, 0), (4769447936, 5767168, 144179200, 0), (4757913600, 5767168, 149946368, 0), (4780982272, 5767168, 155713536, 0), (4786749440, 5767168, 161480704, 0), (4775215104, 5767168, 167247872, 0), (4244635648, 5767168, 173015040, 0), (4250402816, 5767168, 178782208, 0), (4238868480, 5767168, 184549376, 0), (4279238656, 5767168, 190316544, 0), (4285005824, 5767168, 196083712, 0), (4273471488, 5767168, 201850880, 0), (4832886784, 5767168, 207618048, 0), (4838653952, 5767168, 213385216, 0), (4827119616, 5767168, 219152384, 0), (4313841664, 5767168, 224919552, 0), (4319608832, 5767168, 230686720, 0), (4308074496, 5767168, 236453888, 0), (4919394304, 5767168, 242221056, 0), (4925161472, 5767168, 247988224, 0), (4913627136, 5767168, 253755392, 0), (4504158208, 5767168, 259522560, 0), (4509925376, 5767168, 265289728, 0), (4498391040, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b03d5f80>, 2: <capsule object NULL at 0x7a4e3464b720>}
DEBUG 01-15 16:10:35.768056.768056 cuda_h.py:19] end restore2model cost 0.0014607906341552734 seconds
DEBUG 01-15 16:10:35.768165.768165 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.768301.768301 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:35.768008.768008 cuda_h.py:19] end sllm_worker_task cost 0.011138677597045898 seconds
DEBUG 01-15 16:10:35.768434.768434 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.768596.768596 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a87d7e06-99e7-4c69-af35-e67c907643bb
DEBUG 01-15 16:10:35.768178.768178 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.768470.768470 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:35.769342.769342 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a87d7e06-99e7-4c69-af35-e67c907643bb
DEBUG 01-15 16:10:35.769800.769800 cuda_h.py:19] end move_flatidxs cost 0.0008776187896728516 seconds
DEBUG 01-15 16:10:35.769344.769344 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:35.770285.770285 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030105113983154297 seconds
DEBUG 01-15 16:10:35.770771.770771 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.773274.773274 cuda_h.py:19] end restore2model cost 0.0031087398529052734 seconds
DEBUG 01-15 16:10:35.773700.773700 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006360054016113281 seconds
DEBUG 01-15 16:10:35.773456.773456 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:35.774971.774971 cuda_h.py:19] end gpu_sexperts cost 0.00030994415283203125 seconds
DEBUG 01-15 16:10:35.774231.774231 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:35.774438.774438 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:10:35.774564.774564 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:35.774890.774890 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.775298.775298 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010726451873779297 seconds
DEBUG 01-15 16:10:35.775002.775002 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.775671.775671 cuda_h.py:19] end gpu_group_list cost 0.0001823902130126953 seconds
DEBUG 01-15 16:10:35.776134.776134 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.776050.776050 cuda_h.py:19] end group_tensors cost 0.006414890289306641 seconds
DEBUG 01-15 16:10:35.777890.777890 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:35.778457.778457 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014634132385253906 seconds
DEBUG 01-15 16:10:35.778334.778334 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.778325.778325 cuda_h.py:19] end gpu_group_list cost 0.0002644062042236328 seconds
DEBUG 01-15 16:10:35.779106.779106 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:35.779566.779566 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a87d7e06-99e7-4c69-af35-e67c907643bb
DEBUG 01-15 16:10:35.781547.781547 cuda_h.py:19] end group pad cost 0.004705667495727539 seconds
DEBUG 01-15 16:10:35.782390.782390 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:35.796080.796080 client.py:127] Model loaded
DEBUG 01-15 16:10:35.796736.796736 cuda_h.py:19] end wait_experts_multi_device cost 0.016933917999267578 seconds
DEBUG 01-15 16:10:35.796188.796188 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:35.806576.806576 cuda_h.py:19] end group_einsum cost 0.024049997329711914 seconds
DEBUG 01-15 16:10:35.806906.806906 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:35.810974.810974 cuda_h.py:19] end get_outputs_cpu1 cost 0.004119396209716797 seconds
DEBUG 01-15 16:10:35.811374.811374 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.042467594146728516 seconds
DEBUG 01-15 16:10:35.811034.811034 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.014904499053955078 seconds
DEBUG 01-15 16:10:35.811806.811806 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:35.813706.813706 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.813858.813858 cuda_h.py:19] end index_scatter cost 8.869171142578125e-05 seconds
DEBUG 01-15 16:10:35.814174.814174 cuda_h.py:19] end cpuoutputsdeal cost 0.002224445343017578 seconds
DEBUG 01-15 16:10:35.814713.814713 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:35.814098.814098 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.814953.814953 cuda_h.py:19] end gpu_group_tensor cost 0.0001723766326904297 seconds
DEBUG 01-15 16:10:35.814100.814100 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.814033.814033 cuda_h.py:19] end gpu_group_tensor cost 0.00016188621520996094 seconds
DEBUG 01-15 16:10:35.814481.814481 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.815450.815450 cuda_h.py:19] end gpu_group_einsum cost 0.0007951259613037109 seconds
DEBUG 01-15 16:10:35.815091.815091 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.816161.816161 cuda_h.py:19] end gpu_group_einsum cost 0.0004534721374511719 seconds
DEBUG 01-15 16:10:35.816875.816875 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.816892.816892 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.816359.816359 cuda_h.py:19] end all_expert_outputs_slices cost 0.000263214111328125 seconds
DEBUG 01-15 16:10:35.816327.816327 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.817834.817834 cuda_h.py:19] end concat_expert_out cost 5.650520324707031e-05 seconds
DEBUG 01-15 16:10:35.817068.817068 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.817780.817780 cuda_h.py:19] end index_scatter cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:10:35.817266.817266 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009729862213134766 seconds
DEBUG 01-15 16:10:35.817694.817694 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.817689.817689 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.817353.817353 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020813941955566406 seconds
DEBUG 01-15 16:10:35.817255.817255 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.818430.818430 cuda_h.py:19] end concat_expert_out cost 6.008148193359375e-05 seconds
DEBUG 01-15 16:10:35.818234.818234 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.818462.818462 cuda_h.py:19] end index_scatter cost 6.222724914550781e-05 seconds
DEBUG 01-15 16:10:35.818993.818993 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005993843078613281 seconds
DEBUG 01-15 16:10:35.818440.818440 cuda_h.py:19] end gpu_experts_multi_device cost 0.04412436485290527 seconds
DEBUG 01-15 16:10:35.818807.818807 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.05442070960998535 seconds
DEBUG 01-15 16:10:35.818379.818379 cuda_h.py:19] end prefill_layer cost 0.061736345291137695 seconds
DEBUG 01-15 16:10:35.818851.818851 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:10:35.818700.818700 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:35.819456.819456 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:10:35.819974.819974 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:35.819114.819114 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:35.819454.819454 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 4.124641418457031e-05 seconds
DEBUG 01-15 16:10:35.819310.819310 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 8.130073547363281e-05 seconds
DEBUG 01-15 16:10:35.819344.819344 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:35.819637.819637 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:35.819735.819735 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:35.819849.819849 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:35.819132.819132 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:35.819777.819777 cuda_h.py:19] end allocate_cuda_memory cost 0.0002238750457763672 seconds
DEBUG 01-15 16:10:35.819767.819767 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:35.820536.820536 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.820624.820624 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.820288.820288 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f5d89928-463b-48db-9a01-6a81f768cf3b
DEBUG 01-15 16:10:35.820490.820490 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.820541.820541 cuda_h.py:10] start self_attn
INFO 01-15 16:10:35.822103.822103 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f5d89928-463b-48db-9a01-6a81f768cf3b
DEBUG 01-15 16:10:35.822888.822888 cuda_h.py:19] end load_into_gpu_async cost 0.002189159393310547 seconds
DEBUG 01-15 16:10:35.822174.822174 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:35.822760.822760 cuda_h.py:19] end restore_tensors2 cost 8.296966552734375e-05 seconds
DEBUG 01-15 16:10:35.822000.822000 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0028085708618164062 seconds
INFO 01-15 16:10:35.822287.822287 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f5d89928-463b-48db-9a01-6a81f768cf3b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:35.824208.824208 cuda_h.py:19] end self_attn cost 0.004244327545166016 seconds
DEBUG 01-15 16:10:35.825452.825452 cuda_h.py:19] end iln_self_attn_paln cost 0.005975008010864258 seconds
DEBUG 01-15 16:10:35.825149.825149 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:10:35.825343.825343 cuda_h.py:10] start gate
DEBUG 01-15 16:10:35.826193.826193 cuda_h.py:19] end gate cost 0.0008347034454345703 seconds
DEBUG 01-15 16:10:35.826983.826983 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:35.826847.826847 lmp.py:1912] 
DEBUG 01-15 16:10:35.826847.826847 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:35.826034.826034 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:35.826306.826306 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:35.826810.826810 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:35.826645.826645 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:35.826527.826527 lmp.py:1917] 
DEBUG 01-15 16:10:35.826527.826527 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:35.826031.826031 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:35.826018.826018 lmp.py:1935]   Expert  1 |     50 | CPU
DEBUG 01-15 16:10:35.826330.826330 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:10:35.826165.826165 lmp.py:1935]   Expert  7 |     75 | CPU
DEBUG 01-15 16:10:35.826762.826762 lmp.py:1935]   Expert 48 |     81 | CPU
DEBUG 01-15 16:10:35.826359.826359 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:10:35.827955.827955 lmp.py:1935]   Expert 30 |    109 | CPU
DEBUG 01-15 16:10:35.827314.827314 lmp.py:1935]   Expert 61 |    115 | CPU
DEBUG 01-15 16:10:35.827672.827672 lmp.py:1935]   Expert 18 |    118 | CPU
DEBUG 01-15 16:10:35.827792.827792 lmp.py:1935]   Expert 32 |    119 | CPU
DEBUG 01-15 16:10:35.827673.827673 lmp.py:1935]   Expert 45 |    119 | CPU
DEBUG 01-15 16:10:35.827793.827793 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:10:35.827866.827866 lmp.py:1935]   Expert 39 |    135 | CPU
DEBUG 01-15 16:10:35.827940.827940 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:35.827536.827536 lmp.py:1935]   Expert 36 |    138 | CPU
DEBUG 01-15 16:10:35.827133.827133 lmp.py:1935]   Expert 11 |    141 | CPU
DEBUG 01-15 16:10:35.827491.827491 lmp.py:1935]   Expert 59 |    142 | CPU
DEBUG 01-15 16:10:35.827373.827373 lmp.py:1935]   Expert  5 |    143 | CPU
DEBUG 01-15 16:10:35.827254.827254 lmp.py:1935]   Expert  6 |    144 | CPU
DEBUG 01-15 16:10:35.827612.827612 lmp.py:1935]   Expert 51 |    145 | CPU
DEBUG 01-15 16:10:35.827732.827732 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:10:35.827852.827852 lmp.py:1935]   Expert 49 |    156 | CPU
DEBUG 01-15 16:10:35.827972.827972 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:10:35.827853.827853 lmp.py:1935]   Expert  9 |    160 | CPU
DEBUG 01-15 16:10:35.827211.827211 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:10:35.827808.827808 lmp.py:1935]   Expert 56 |    167 | CPU
DEBUG 01-15 16:10:35.827643.827643 lmp.py:1935]   Expert 40 |    168 | CPU
DEBUG 01-15 16:10:35.827717.827717 lmp.py:1935]   Expert 52 |    168 | CPU
DEBUG 01-15 16:10:35.827843.827843 lmp.py:1935]   Expert 35 |    169 | CPU
DEBUG 01-15 16:10:35.827725.827725 lmp.py:1935]   Expert 16 |    172 | CPU
DEBUG 01-15 16:10:35.827891.827891 lmp.py:1935]   Expert  4 |    186 | CPU
DEBUG 01-15 16:10:35.827295.827295 lmp.py:1935]   Expert 37 |    190 | CPU
DEBUG 01-15 16:10:35.827461.827461 lmp.py:1935]   Expert 42 |    190 | CPU
DEBUG 01-15 16:10:35.827773.827773 lmp.py:1935]   Expert 13 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827085.827085 lmp.py:1935]   Expert 17 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827159.827159 lmp.py:1935]   Expert 38 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827517.827517 lmp.py:1935]   Expert 62 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827352.827352 lmp.py:1935]   Expert 21 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827187.827187 lmp.py:1935]   Expert  3 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827261.827261 lmp.py:1935]   Expert 44 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827572.827572 lmp.py:1935]   Expert 28 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827408.827408 lmp.py:1935]   Expert 58 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827289.827289 lmp.py:1935]   Expert 60 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827409.827409 lmp.py:1935]   Expert 47 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827290.827290 lmp.py:1935]   Expert 10 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827648.827648 lmp.py:1935]   Expert 53 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827530.827530 lmp.py:1935]   Expert 55 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827650.827650 lmp.py:1935]   Expert 20 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827531.827531 lmp.py:1935]   Expert 57 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827604.827604 lmp.py:1935]   Expert 33 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827678.827678 lmp.py:1935]   Expert 31 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827513.827513 lmp.py:1935]   Expert 46 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827110.827110 lmp.py:1935]   Expert  8 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827468.827468 lmp.py:1935]   Expert 19 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827349.827349 lmp.py:1935]   Expert 24 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827708.827708 lmp.py:1935]   Expert 14 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827589.827589 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827470.827470 lmp.py:1935]   Expert 29 |    273 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.827590.827590 lmp.py:1935]   Expert 12 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.827710.827710 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.828545.828545 lmp.py:1935]   Expert  0 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.828903.828903 lmp.py:1935]   Expert 43 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.828738.828738 lmp.py:1935]   Expert 54 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.828335.828335 lmp.py:1935]   Expert 41 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.828216.828216 lmp.py:1935]   Expert 25 |    411 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.828382.828382 lmp.py:1937] 
DEBUG 01-15 16:10:35.828382.828382 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:35.828264.828264 lmp.py:1938]   CPU:   4408 tokens
DEBUG 01-15 16:10:35.828384.828384 lmp.py:1942]   cuda:1:   3938 tokens (16 experts)
DEBUG 01-15 16:10:35.828265.828265 lmp.py:1942]   cuda:2:   3942 tokens (16 experts)
DEBUG 01-15 16:10:35.828954.828954 lmp.py:1943]   Total GPU:   7880 tokens
DEBUG 01-15 16:10:35.828882.828882 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:35.828882.828882 lmp.py:1944] 
DEBUG 01-15 16:10:35.828962.828962 cuda_h.py:19] end experts_map_get cost 0.0019197463989257812 seconds
DEBUG 01-15 16:10:35.828495.828495 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:35.828503.828503 lmp.py:1953] 
DEBUG 01-15 16:10:35.828503.828503 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:35.828485.828485 cuda_h.py:19] end cpu_experts_submit cost 5.626678466796875e-05 seconds
DEBUG 01-15 16:10:35.828333.828333 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:35.828408.828408 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:35.829412.829412 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.57.up_proj.weight': 271056896}, 2: {'model.layers.3.mlp.experts.38.gate_proj.weight': 0, 'model.layers.3.mlp.experts.38.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.38.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.41.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.41.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.41.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.12.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.12.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.12.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.28.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.28.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.28.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.46.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.46.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.46.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.47.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.47.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.47.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.60.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.60.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.60.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.20.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.20.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.20.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.53.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.53.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.53.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.54.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.54.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.54.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.31.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.31.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.31.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5646057472, 5767168, 17301504, 0), (5651824640, 5767168, 23068672, 0), (5640290304, 5767168, 28835840, 0), (5127012352, 5767168, 34603008, 0), (5132779520, 5767168, 40370176, 0), (5121245184, 5767168, 46137344, 0), (5213519872, 5767168, 51904512, 0), (5219287040, 5767168, 57671680, 0), (5207752704, 5767168, 63438848, 0), (5248122880, 5767168, 69206016, 0), (5253890048, 5767168, 74973184, 0), (5242355712, 5767168, 80740352, 0), (5819072512, 5767168, 86507520, 0), (5824839680, 5767168, 92274688, 0), (5813305344, 5767168, 98041856, 0), (5836374016, 5767168, 103809024, 0), (5842141184, 5767168, 109576192, 0), (5830606848, 5767168, 115343360, 0), (5300027392, 5767168, 121110528, 0), (5305794560, 5767168, 126877696, 0), (5294260224, 5767168, 132644864, 0), (5317328896, 5767168, 138412032, 0), (5323096064, 5767168, 144179200, 0), (5311561728, 5767168, 149946368, 0), (5369233408, 5767168, 155713536, 0), (5375000576, 5767168, 161480704, 0), (5363466240, 5767168, 167247872, 0), (5403836416, 5767168, 173015040, 0), (5409603584, 5767168, 178782208, 0), (5398069248, 5767168, 184549376, 0), (6026690560, 5767168, 190316544, 0), (6032457728, 5767168, 196083712, 0), (6020923392, 5767168, 201850880, 0), (5507645440, 5767168, 207618048, 0), (5513412608, 5767168, 213385216, 0), (5501878272, 5767168, 219152384, 0), (6078595072, 5767168, 224919552, 0), (6084362240, 5767168, 230686720, 0), (6072827904, 5767168, 236453888, 0), (5576851456, 5767168, 242221056, 0), (5582618624, 5767168, 247988224, 0), (5571084288, 5767168, 253755392, 0), (6061293568, 5767168, 259522560, 0), (6067060736, 5767168, 265289728, 0), (6055526400, 5767168, 271056896, 0)], 2: [(5732564992, 5767168, 0, 0), (5738332160, 5767168, 5767168, 0), (5726797824, 5767168, 11534336, 0), (5784469504, 5767168, 17301504, 0), (5790236672, 5767168, 23068672, 0), (5778702336, 5767168, 28835840, 0), (5282725888, 5767168, 34603008, 0), (5288493056, 5767168, 40370176, 0), (5276958720, 5767168, 46137344, 0), (5559549952, 5767168, 51904512, 0), (5565317120, 5767168, 57671680, 0), (5553782784, 5767168, 63438848, 0), (5870977024, 5767168, 69206016, 0), (5876744192, 5767168, 74973184, 0), (5865209856, 5767168, 80740352, 0), (5888278528, 5767168, 86507520, 0), (5894045696, 5767168, 92274688, 0), (5882511360, 5767168, 98041856, 0), (6113198080, 5767168, 103809024, 0), (6118965248, 5767168, 109576192, 0), (6107430912, 5767168, 115343360, 0), (5421137920, 5767168, 121110528, 0), (5426905088, 5767168, 126877696, 0), (5415370752, 5767168, 132644864, 0), (5992087552, 5767168, 138412032, 0), (5997854720, 5767168, 144179200, 0), (5986320384, 5767168, 149946368, 0), (6009389056, 5767168, 155713536, 0), (6015156224, 5767168, 161480704, 0), (6003621888, 5767168, 167247872, 0), (5455740928, 5767168, 173015040, 0), (5461508096, 5767168, 178782208, 0), (5449973760, 5767168, 184549376, 0), (5490343936, 5767168, 190316544, 0), (5496111104, 5767168, 196083712, 0), (5484576768, 5767168, 201850880, 0), (5438439424, 5767168, 207618048, 0), (5444206592, 5767168, 213385216, 0), (5432672256, 5767168, 219152384, 0), (5611454464, 5767168, 224919552, 0), (5617221632, 5767168, 230686720, 0), (5605687296, 5767168, 236453888, 0), (6147801088, 5767168, 242221056, 0), (6153568256, 5767168, 247988224, 0), (6142033920, 5767168, 253755392, 0), (6165102592, 5767168, 259522560, 0), (6170869760, 5767168, 265289728, 0), (6159335424, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4743d50>, 2: <capsule object NULL at 0x7a4ec4371f80>}
DEBUG 01-15 16:10:35.829961.829961 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.829903.829903 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.829182.829182 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f70225b8-2585-48d0-83c9-ebb3f9587326
DEBUG 01-15 16:10:35.829725.829725 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.829888.829888 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:35.829380.829380 client.py:127] Model loaded
DEBUG 01-15 16:10:35.830163.830163 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.830203.830203 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:35.830367.830367 cuda_h.py:19] end restore2model cost 0.0003285408020019531 seconds
DEBUG 01-15 16:10:35.830752.830752 cuda_h.py:19] end sllm_worker_task cost 0.01096796989440918 seconds
INFO 01-15 16:10:35.830186.830186 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f70225b8-2585-48d0-83c9-ebb3f9587326
DEBUG 01-15 16:10:35.831957.831957 cuda_h.py:19] end move_flatidxs cost 0.0009126663208007812 seconds
DEBUG 01-15 16:10:35.831933.831933 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:35.831065.831065 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002976655960083008 seconds
DEBUG 01-15 16:10:35.831658.831658 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.834326.834326 cuda_h.py:19] end restore2model cost 0.003121614456176758 seconds
DEBUG 01-15 16:10:35.834322.834322 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006357669830322266 seconds
DEBUG 01-15 16:10:35.834760.834760 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:35.835136.835136 cuda_h.py:19] end gpu_sexperts cost 0.0003142356872558594 seconds
DEBUG 01-15 16:10:35.835158.835158 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:35.835511.835511 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8835067749023438e-05 seconds
DEBUG 01-15 16:10:35.835690.835690 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:35.835208.835208 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.836238.836238 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010764598846435547 seconds
DEBUG 01-15 16:10:35.836466.836466 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.836804.836804 cuda_h.py:19] end gpu_group_list cost 0.00018286705017089844 seconds
DEBUG 01-15 16:10:35.837677.837677 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.837810.837810 cuda_h.py:19] end group_tensors cost 0.00597691535949707 seconds
DEBUG 01-15 16:10:35.838844.838844 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:35.838760.838760 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001222372055053711 seconds
DEBUG 01-15 16:10:35.839491.839491 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.839759.839759 cuda_h.py:19] end gpu_group_list cost 0.0002353191375732422 seconds
DEBUG 01-15 16:10:35.840377.840377 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:35.840605.840605 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f70225b8-2585-48d0-83c9-ebb3f9587326
DEBUG 01-15 16:10:35.842413.842413 cuda_h.py:19] end group pad cost 0.0042455196380615234 seconds
DEBUG 01-15 16:10:35.842177.842177 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:35.857731.857731 client.py:127] Model loaded
DEBUG 01-15 16:10:35.857159.857159 cuda_h.py:19] end wait_experts_multi_device cost 0.01714301109313965 seconds
DEBUG 01-15 16:10:35.857267.857267 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:35.865230.865230 cuda_h.py:19] end group_einsum cost 0.023050308227539062 seconds
DEBUG 01-15 16:10:35.865526.865526 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:35.869187.869187 cuda_h.py:19] end get_outputs_cpu1 cost 0.004031181335449219 seconds
DEBUG 01-15 16:10:35.870456.870456 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04061698913574219 seconds
DEBUG 01-15 16:10:35.871575.871575 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013505220413208008 seconds
DEBUG 01-15 16:10:35.871764.871764 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:35.872564.872564 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.872034.872034 cuda_h.py:19] end index_scatter cost 9.5367431640625e-05 seconds
DEBUG 01-15 16:10:35.872189.872189 cuda_h.py:19] end cpuoutputsdeal cost 0.0015723705291748047 seconds
DEBUG 01-15 16:10:35.872337.872337 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:35.872954.872954 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.873417.873417 cuda_h.py:19] end gpu_group_tensor cost 0.0001342296600341797 seconds
DEBUG 01-15 16:10:35.873272.873272 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.873152.873152 cuda_h.py:19] end gpu_group_tensor cost 0.0001270771026611328 seconds
DEBUG 01-15 16:10:35.873593.873593 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.873489.873489 cuda_h.py:19] end gpu_group_einsum cost 0.0006091594696044922 seconds
DEBUG 01-15 16:10:35.874917.874917 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.874995.874995 cuda_h.py:19] end gpu_group_einsum cost 0.0004892349243164062 seconds
DEBUG 01-15 16:10:35.874642.874642 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.874043.874043 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.875283.875283 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001990795135498047 seconds
DEBUG 01-15 16:10:35.875861.875861 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.875189.875189 cuda_h.py:19] end concat_expert_out cost 6.580352783203125e-05 seconds
DEBUG 01-15 16:10:35.875694.875694 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.875425.875425 cuda_h.py:19] end index_scatter cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:35.875691.875691 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008261203765869141 seconds
DEBUG 01-15 16:10:35.875290.875290 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.875326.875326 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.875066.875066 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012755393981933594 seconds
DEBUG 01-15 16:10:35.876391.876391 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.876454.876454 cuda_h.py:19] end concat_expert_out cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:35.876436.876436 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.876737.876737 cuda_h.py:19] end index_scatter cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:35.876691.876691 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004680156707763672 seconds
DEBUG 01-15 16:10:35.876263.876263 cuda_h.py:19] end gpu_experts_multi_device cost 0.04102301597595215 seconds
DEBUG 01-15 16:10:35.876525.876525 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.05112147331237793 seconds
DEBUG 01-15 16:10:35.876133.876133 cuda_h.py:19] end prefill_layer cost 0.0578000545501709 seconds
DEBUG 01-15 16:10:35.876453.876453 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:10:35.876918.876918 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:35.876097.876097 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:10:35.876231.876231 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:35.876987.876987 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:35.877274.877274 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 4.291534423828125e-05 seconds
DEBUG 01-15 16:10:35.877745.877745 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:10:35.877018.877018 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:35.877411.877411 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:35.877096.877096 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:35.877146.877146 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:35.877750.877750 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:35.877541.877541 cuda_h.py:19] end allocate_cuda_memory cost 0.00022864341735839844 seconds
DEBUG 01-15 16:10:35.877179.877179 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:35.877088.877088 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.877547.877547 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.877210.877210 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8676adb2-70fd-4da7-9fb1-fb1f5e893f10
DEBUG 01-15 16:10:35.878876.878876 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.878030.878030 cuda_h.py:10] start self_attn
INFO 01-15 16:10:35.879931.879931 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8676adb2-70fd-4da7-9fb1-fb1f5e893f10
DEBUG 01-15 16:10:35.879205.879205 cuda_h.py:19] end load_into_gpu_async cost 0.001911163330078125 seconds
DEBUG 01-15 16:10:35.879007.879007 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:35.879024.879024 cuda_h.py:19] end restore_tensors2 cost 8.535385131835938e-05 seconds
DEBUG 01-15 16:10:35.879132.879132 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002517223358154297 seconds
INFO 01-15 16:10:35.880320.880320 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8676adb2-70fd-4da7-9fb1-fb1f5e893f10
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:35.881623.881623 cuda_h.py:19] end self_attn cost 0.003211498260498047 seconds
DEBUG 01-15 16:10:35.881588.881588 cuda_h.py:19] end iln_self_attn_paln cost 0.00474095344543457 seconds
DEBUG 01-15 16:10:35.881610.881610 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:10:35.881895.881895 cuda_h.py:10] start gate
DEBUG 01-15 16:10:35.882151.882151 cuda_h.py:19] end gate cost 0.0006787776947021484 seconds
DEBUG 01-15 16:10:35.882749.882749 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:35.883961.883961 lmp.py:1912] 
DEBUG 01-15 16:10:35.883961.883961 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:35.883532.883532 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:35.883758.883758 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:35.883361.883361 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:35.883157.883157 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:35.883853.883853 lmp.py:1917] 
DEBUG 01-15 16:10:35.883853.883853 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:35.883311.883311 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:35.883159.883159 lmp.py:1935]   Expert 14 |     65 | CPU
DEBUG 01-15 16:10:35.883763.883763 lmp.py:1935]   Expert 57 |     72 | CPU
DEBUG 01-15 16:10:35.883605.883605 lmp.py:1935]   Expert 13 |     77 | CPU
DEBUG 01-15 16:10:35.883963.883963 lmp.py:1935]   Expert 26 |     81 | CPU
DEBUG 01-15 16:10:35.883367.883367 lmp.py:1935]   Expert 31 |     90 | CPU
DEBUG 01-15 16:10:35.883203.883203 lmp.py:1935]   Expert 11 |     94 | CPU
DEBUG 01-15 16:10:35.883276.883276 lmp.py:1935]   Expert 54 |     94 | CPU
DEBUG 01-15 16:10:35.883919.883919 lmp.py:1935]   Expert 45 |     96 | CPU
DEBUG 01-15 16:10:35.883039.883039 lmp.py:1935]   Expert 58 |    102 | CPU
DEBUG 01-15 16:10:35.883920.883920 lmp.py:1935]   Expert 30 |    107 | CPU
DEBUG 01-15 16:10:35.883325.883325 lmp.py:1935]   Expert 51 |    107 | CPU
DEBUG 01-15 16:10:35.883491.883491 lmp.py:1935]   Expert 36 |    112 | CPU
DEBUG 01-15 16:10:35.883895.883895 lmp.py:1935]   Expert 10 |    114 | CPU
DEBUG 01-15 16:10:35.883300.883300 lmp.py:1935]   Expert 32 |    114 | CPU
DEBUG 01-15 16:10:35.883466.883466 lmp.py:1935]   Expert 20 |    126 | CPU
DEBUG 01-15 16:10:35.883632.883632 lmp.py:1935]   Expert  8 |    133 | CPU
DEBUG 01-15 16:10:35.883798.883798 lmp.py:1935]   Expert  4 |    138 | CPU
DEBUG 01-15 16:10:35.883918.883918 lmp.py:1935]   Expert 63 |    138 | CPU
DEBUG 01-15 16:10:35.883038.883038 lmp.py:1935]   Expert 53 |    141 | CPU
DEBUG 01-15 16:10:35.883158.883158 lmp.py:1935]   Expert 34 |    144 | CPU
DEBUG 01-15 16:10:35.883039.883039 lmp.py:1935]   Expert 61 |    144 | CPU
DEBUG 01-15 16:10:35.883159.883159 lmp.py:1935]   Expert 16 |    147 | CPU
DEBUG 01-15 16:10:35.883802.883802 lmp.py:1935]   Expert 47 |    149 | CPU
DEBUG 01-15 16:10:35.883968.883968 lmp.py:1935]   Expert 60 |    158 | CPU
DEBUG 01-15 16:10:35.883373.883373 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:10:35.883539.883539 lmp.py:1935]   Expert 42 |    162 | CPU
DEBUG 01-15 16:10:35.883705.883705 lmp.py:1935]   Expert 17 |    163 | CPU
DEBUG 01-15 16:10:35.883633.883633 lmp.py:1935]   Expert 29 |    170 | CPU
DEBUG 01-15 16:10:35.883560.883560 lmp.py:1935]   Expert 44 |    172 | CPU
DEBUG 01-15 16:10:35.883919.883919 lmp.py:1935]   Expert  7 |    176 | CPU
DEBUG 01-15 16:10:35.883800.883800 lmp.py:1935]   Expert 27 |    176 | CPU
DEBUG 01-15 16:10:35.883443.883443 lmp.py:1935]   Expert 41 |    179 | CPU
DEBUG 01-15 16:10:35.883470.883470 lmp.py:1935]   Expert  9 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.883497.883497 lmp.py:1935]   Expert 48 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.883094.883094 lmp.py:1935]   Expert 56 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.883691.883691 lmp.py:1935]   Expert  3 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.883049.883049 lmp.py:1935]   Expert  2 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884407.884407 lmp.py:1935]   Expert 15 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884527.884527 lmp.py:1935]   Expert 24 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884885.884885 lmp.py:1935]   Expert  0 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884767.884767 lmp.py:1935]   Expert 18 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884078.884078 lmp.py:1935]   Expert 55 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884867.884867 lmp.py:1935]   Expert 40 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884544.884544 lmp.py:1935]   Expert 23 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884902.884902 lmp.py:1935]   Expert 38 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884784.884784 lmp.py:1935]   Expert 22 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884427.884427 lmp.py:1935]   Expert 37 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884070.884070 lmp.py:1935]   Expert  6 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884189.884189 lmp.py:1935]   Expert 46 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884594.884594 lmp.py:1935]   Expert 19 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884475.884475 lmp.py:1935]   Expert 39 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884118.884118 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884238.884238 lmp.py:1935]   Expert 12 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884120.884120 lmp.py:1935]   Expert 50 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884955.884955 lmp.py:1935]   Expert 62 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884790.884790 lmp.py:1935]   Expert 21 |    281 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884386.884386 lmp.py:1935]   Expert 35 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884619.884619 lmp.py:1935]   Expert 49 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884454.884454 lmp.py:1935]   Expert 33 |    299 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884813.884813 lmp.py:1935]   Expert 52 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884456.884456 lmp.py:1935]   Expert  1 |    347 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884098.884098 lmp.py:1935]   Expert  5 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884218.884218 lmp.py:1935]   Expert 43 |    437 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.884861.884861 lmp.py:1935]   Expert 59 |    586 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.884789.884789 lmp.py:1937] 
DEBUG 01-15 16:10:35.884789.884789 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:35.884955.884955 lmp.py:1938]   CPU:   4100 tokens
DEBUG 01-15 16:10:35.884598.884598 lmp.py:1942]   cuda:1:   4126 tokens (16 experts)
DEBUG 01-15 16:10:35.884241.884241 lmp.py:1942]   cuda:2:   4062 tokens (16 experts)
DEBUG 01-15 16:10:35.884930.884930 lmp.py:1943]   Total GPU:   8188 tokens
DEBUG 01-15 16:10:35.884381.884381 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:35.884381.884381 lmp.py:1944] 
DEBUG 01-15 16:10:35.884654.884654 cuda_h.py:19] end experts_map_get cost 0.0018620491027832031 seconds
DEBUG 01-15 16:10:35.884457.884457 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:35.884180.884180 lmp.py:1953] 
DEBUG 01-15 16:10:35.884180.884180 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:35.884394.884394 cuda_h.py:19] end cpu_experts_submit cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:35.884375.884375 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:35.884535.884535 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:35.887934.887934 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.38.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.38.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.38.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.39.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.39.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.39.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.40.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.40.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.40.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.9.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.9.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.9.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.12.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.12.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.12.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.46.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.46.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.46.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.56.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.56.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.56.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.25.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.25.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.25.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6217007104, 5767168, 17301504, 0), (6222774272, 5767168, 23068672, 0), (6211239936, 5767168, 28835840, 0), (6787956736, 5767168, 34603008, 0), (6793723904, 5767168, 40370176, 0), (6782189568, 5767168, 46137344, 0), (6822559744, 5767168, 51904512, 0), (6828326912, 5767168, 57671680, 0), (6816792576, 5767168, 63438848, 0), (6839861248, 5767168, 69206016, 0), (6845628416, 5767168, 74973184, 0), (6834094080, 5767168, 80740352, 0), (6857162752, 5767168, 86507520, 0), (6862929920, 5767168, 92274688, 0), (6851395584, 5767168, 98041856, 0), (6874464256, 5767168, 103809024, 0), (6880231424, 5767168, 109576192, 0), (6868697088, 5767168, 115343360, 0), (6338117632, 5767168, 121110528, 0), (6343884800, 5767168, 126877696, 0), (6332350464, 5767168, 132644864, 0), (6390022144, 5767168, 138412032, 0), (6395789312, 5767168, 144179200, 0), (6384254976, 5767168, 149946368, 0), (6978273280, 5767168, 155713536, 0), (6984040448, 5767168, 161480704, 0), (6972506112, 5767168, 167247872, 0), (6493831168, 5767168, 173015040, 0), (6499598336, 5767168, 178782208, 0), (6488064000, 5767168, 184549376, 0), (7082082304, 5767168, 190316544, 0), (7087849472, 5767168, 196083712, 0), (7076315136, 5767168, 201850880, 0), (6597640192, 5767168, 207618048, 0), (6603407360, 5767168, 213385216, 0), (6591873024, 5767168, 219152384, 0), (7151288320, 5767168, 224919552, 0), (7157055488, 5767168, 230686720, 0), (7145521152, 5767168, 236453888, 0), (7203192832, 5767168, 242221056, 0), (7208960000, 5767168, 247988224, 0), (7197425664, 5767168, 253755392, 0), (7255097344, 5767168, 259522560, 0), (7260864512, 5767168, 265289728, 0), (7249330176, 5767168, 271056896, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6753353728, 5767168, 17301504, 0), (6759120896, 5767168, 23068672, 0), (6747586560, 5767168, 28835840, 0), (6234308608, 5767168, 34603008, 0), (6240075776, 5767168, 40370176, 0), (6228541440, 5767168, 46137344, 0), (6268911616, 5767168, 51904512, 0), (6274678784, 5767168, 57671680, 0), (6263144448, 5767168, 63438848, 0), (6286213120, 5767168, 69206016, 0), (6291980288, 5767168, 74973184, 0), (6280445952, 5767168, 80740352, 0), (6926368768, 5767168, 86507520, 0), (6932135936, 5767168, 92274688, 0), (6920601600, 5767168, 98041856, 0), (6441926656, 5767168, 103809024, 0), (6447693824, 5767168, 109576192, 0), (6436159488, 5767168, 115343360, 0), (7012876288, 5767168, 121110528, 0), (7018643456, 5767168, 126877696, 0), (7007109120, 5767168, 132644864, 0), (7030177792, 5767168, 138412032, 0), (7035944960, 5767168, 144179200, 0), (7024410624, 5767168, 149946368, 0), (7047479296, 5767168, 155713536, 0), (7053246464, 5767168, 161480704, 0), (7041712128, 5767168, 167247872, 0), (6511132672, 5767168, 173015040, 0), (6516899840, 5767168, 178782208, 0), (6505365504, 5767168, 184549376, 0), (6545735680, 5767168, 190316544, 0), (6551502848, 5767168, 196083712, 0), (6539968512, 5767168, 201850880, 0), (6563037184, 5767168, 207618048, 0), (6568804352, 5767168, 213385216, 0), (6557270016, 5767168, 219152384, 0), (6580338688, 5767168, 224919552, 0), (6586105856, 5767168, 230686720, 0), (6574571520, 5767168, 236453888, 0), (7133986816, 5767168, 242221056, 0), (7139753984, 5767168, 247988224, 0), (7128219648, 5767168, 253755392, 0), (6614941696, 5767168, 259522560, 0), (6620708864, 5767168, 265289728, 0), (6609174528, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4683c90>, 2: <capsule object NULL at 0x7a4e545c06c0>}
DEBUG 01-15 16:10:35.887278.887278 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.887346.887346 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.887287.887287 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 449450e2-d72c-4fef-b24a-8fd01a3b5896
DEBUG 01-15 16:10:35.887532.887532 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.887375.887375 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:35.888749.888749 client.py:127] Model loaded
DEBUG 01-15 16:10:35.888201.888201 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.888352.888352 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:35.888326.888326 cuda_h.py:19] end restore2model cost 0.00033974647521972656 seconds
DEBUG 01-15 16:10:35.888049.888049 cuda_h.py:19] end sllm_worker_task cost 0.011174917221069336 seconds
DEBUG 01-15 16:10:35.889998.889998 cuda_h.py:19] end move_flatidxs cost 0.0008373260498046875 seconds
DEBUG 01-15 16:10:35.889681.889681 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:35.890372.890372 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 449450e2-d72c-4fef-b24a-8fd01a3b5896
DEBUG 01-15 16:10:35.890242.890242 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0056493282318115234 seconds
DEBUG 01-15 16:10:35.890682.890682 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.894039.894039 cuda_h.py:19] end group_tensors cost 0.004895210266113281 seconds
DEBUG 01-15 16:10:35.894065.894065 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:35.895697.895697 cuda_h.py:19] end restore2model cost 0.004948139190673828 seconds
DEBUG 01-15 16:10:35.895344.895344 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.010907411575317383 seconds
DEBUG 01-15 16:10:35.895080.895080 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:35.896829.896829 cuda_h.py:19] end gpu_sexperts cost 0.00037360191345214844 seconds
DEBUG 01-15 16:10:35.896911.896911 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:35.896483.896483 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.956390380859375e-05 seconds
DEBUG 01-15 16:10:35.896373.896373 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:35.896381.896381 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.897403.897403 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011944770812988281 seconds
DEBUG 01-15 16:10:35.897486.897486 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.898728.898728 cuda_h.py:19] end gpu_group_list cost 0.00028133392333984375 seconds
DEBUG 01-15 16:10:35.899282.899282 cuda_h.py:19] end group pad cost 0.0040400028228759766 seconds
DEBUG 01-15 16:10:35.899264.899264 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:35.899572.899572 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.903840.903840 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.004471778869628906 seconds
DEBUG 01-15 16:10:35.904251.904251 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.904219.904219 cuda_h.py:19] end gpu_group_list cost 0.0004763603210449219 seconds
DEBUG 01-15 16:10:35.906303.906303 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:35.906852.906852 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 449450e2-d72c-4fef-b24a-8fd01a3b5896
INFO 01-15 16:10:35.916733.916733 client.py:127] Model loaded
DEBUG 01-15 16:10:35.917482.917482 cuda_h.py:19] end wait_experts_multi_device cost 0.010485649108886719 seconds
DEBUG 01-15 16:10:35.917611.917611 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:35.919902.919902 cuda_h.py:19] end group_einsum cost 0.020749568939208984 seconds
DEBUG 01-15 16:10:35.920311.920311 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:35.924815.924815 cuda_h.py:19] end get_outputs_cpu1 cost 0.0038971900939941406 seconds
DEBUG 01-15 16:10:35.924845.924845 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.036730051040649414 seconds
DEBUG 01-15 16:10:35.925741.925741 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007947206497192383 seconds
DEBUG 01-15 16:10:35.925976.925976 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:35.926756.926756 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.926737.926737 cuda_h.py:19] end index_scatter cost 7.939338684082031e-05 seconds
DEBUG 01-15 16:10:35.927297.927297 cuda_h.py:19] end cpuoutputsdeal cost 0.0018100738525390625 seconds
DEBUG 01-15 16:10:35.927259.927259 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:35.927360.927360 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.927558.927558 cuda_h.py:19] end gpu_group_tensor cost 0.00014638900756835938 seconds
DEBUG 01-15 16:10:35.927228.927228 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.927077.927077 cuda_h.py:19] end gpu_group_tensor cost 0.00037384033203125 seconds
DEBUG 01-15 16:10:35.928068.928068 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.929834.929834 cuda_h.py:19] end gpu_group_einsum cost 0.0010716915130615234 seconds
DEBUG 01-15 16:10:35.929872.929872 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.929258.929258 cuda_h.py:19] end gpu_group_einsum cost 0.0004074573516845703 seconds
DEBUG 01-15 16:10:35.929951.929951 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.930948.930948 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.930240.930240 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017547607421875 seconds
DEBUG 01-15 16:10:35.930049.930049 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.930178.930178 cuda_h.py:19] end concat_expert_out cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:10:35.930226.930226 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.930475.930475 cuda_h.py:19] end index_scatter cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:10:35.930854.930854 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008287429809570312 seconds
DEBUG 01-15 16:10:35.930560.930560 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.930880.930880 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.931647.931647 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014662742614746094 seconds
DEBUG 01-15 16:10:35.931403.931403 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.931618.931618 cuda_h.py:19] end concat_expert_out cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:35.931461.931461 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.931530.931530 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:35.931531.931531 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005033016204833984 seconds
DEBUG 01-15 16:10:35.931765.931765 cuda_h.py:19] end gpu_experts_multi_device cost 0.0350191593170166 seconds
DEBUG 01-15 16:10:35.931344.931344 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.049596309661865234 seconds
DEBUG 01-15 16:10:35.931853.931853 cuda_h.py:19] end prefill_layer cost 0.05501198768615723 seconds
DEBUG 01-15 16:10:35.932108.932108 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:10:35.932195.932195 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:35.932567.932567 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:10:35.932415.932415 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:35.932456.932456 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:35.932121.932121 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 4.076957702636719e-05 seconds
DEBUG 01-15 16:10:35.932115.932115 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:10:35.932911.932911 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:35.932966.932966 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:35.932188.932188 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:35.932430.932430 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:35.932319.932319 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:35.932837.932837 cuda_h.py:19] end allocate_cuda_memory cost 0.0002028942108154297 seconds
DEBUG 01-15 16:10:35.932423.932423 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:35.932331.932331 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.933174.933174 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.933646.933646 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7a5aa5c0-fc7b-49fb-a437-8a1296725221
DEBUG 01-15 16:10:35.933795.933795 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.933632.933632 cuda_h.py:10] start self_attn
INFO 01-15 16:10:35.934148.934148 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7a5aa5c0-fc7b-49fb-a437-8a1296725221
DEBUG 01-15 16:10:35.934421.934421 cuda_h.py:19] end load_into_gpu_async cost 0.0018911361694335938 seconds
DEBUG 01-15 16:10:35.934508.934508 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:35.935333.935333 cuda_h.py:19] end restore_tensors2 cost 8.58306884765625e-05 seconds
DEBUG 01-15 16:10:35.935096.935096 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024580955505371094 seconds
INFO 01-15 16:10:35.935178.935178 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7a5aa5c0-fc7b-49fb-a437-8a1296725221
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:35.936728.936728 cuda_h.py:19] end self_attn cost 0.0031938552856445312 seconds
DEBUG 01-15 16:10:35.937692.937692 cuda_h.py:19] end iln_self_attn_paln cost 0.004721879959106445 seconds
DEBUG 01-15 16:10:35.937522.937522 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:10:35.937761.937761 cuda_h.py:10] start gate
DEBUG 01-15 16:10:35.937686.937686 cuda_h.py:19] end gate cost 0.0006771087646484375 seconds
DEBUG 01-15 16:10:35.937761.937761 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:35.938397.938397 lmp.py:1912] 
DEBUG 01-15 16:10:35.938397.938397 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:35.938683.938683 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:35.938286.938286 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:35.938313.938313 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:35.938148.938148 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:35.938791.938791 lmp.py:1917] 
DEBUG 01-15 16:10:35.938791.938791 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:35.938342.938342 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:35.938329.938329 lmp.py:1935]   Expert 34 |     24 | CPU
DEBUG 01-15 16:10:35.938403.938403 lmp.py:1935]   Expert 45 |     65 | CPU
DEBUG 01-15 16:10:35.938761.938761 lmp.py:1935]   Expert 22 |     74 | CPU
DEBUG 01-15 16:10:35.938404.938404 lmp.py:1935]   Expert 57 |     76 | CPU
DEBUG 01-15 16:10:35.938047.938047 lmp.py:1935]   Expert 17 |     94 | CPU
DEBUG 01-15 16:10:35.938928.938928 lmp.py:1935]   Expert 15 |    100 | CPU
DEBUG 01-15 16:10:35.938571.938571 lmp.py:1935]   Expert  4 |    101 | CPU
DEBUG 01-15 16:10:35.938214.938214 lmp.py:1935]   Expert 28 |    106 | CPU
DEBUG 01-15 16:10:35.938857.938857 lmp.py:1935]   Expert 32 |    112 | CPU
DEBUG 01-15 16:10:35.938023.938023 lmp.py:1935]   Expert 60 |    113 | CPU
DEBUG 01-15 16:10:35.938143.938143 lmp.py:1935]   Expert 36 |    125 | CPU
DEBUG 01-15 16:10:35.938978.938978 lmp.py:1935]   Expert 16 |    126 | CPU
DEBUG 01-15 16:10:35.938575.938575 lmp.py:1935]   Expert 14 |    127 | CPU
DEBUG 01-15 16:10:35.938410.938410 lmp.py:1935]   Expert 12 |    128 | CPU
DEBUG 01-15 16:10:35.938053.938053 lmp.py:1935]   Expert 52 |    130 | CPU
DEBUG 01-15 16:10:35.938696.938696 lmp.py:1935]   Expert 25 |    132 | CPU
DEBUG 01-15 16:10:35.938100.938100 lmp.py:1935]   Expert  8 |    134 | CPU
DEBUG 01-15 16:10:35.938505.938505 lmp.py:1935]   Expert  2 |    139 | CPU
DEBUG 01-15 16:10:35.938910.938910 lmp.py:1935]   Expert 35 |    141 | CPU
DEBUG 01-15 16:10:35.938314.938314 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:10:35.938480.938480 lmp.py:1935]   Expert 30 |    152 | CPU
DEBUG 01-15 16:10:35.938646.938646 lmp.py:1935]   Expert 23 |    154 | CPU
DEBUG 01-15 16:10:35.938812.938812 lmp.py:1935]   Expert 61 |    156 | CPU
DEBUG 01-15 16:10:35.938217.938217 lmp.py:1935]   Expert  0 |    157 | CPU
DEBUG 01-15 16:10:35.938575.938575 lmp.py:1935]   Expert 39 |    157 | CPU
DEBUG 01-15 16:10:35.938933.938933 lmp.py:1935]   Expert  3 |    169 | CPU
DEBUG 01-15 16:10:35.938815.938815 lmp.py:1935]   Expert 13 |    171 | CPU
DEBUG 01-15 16:10:35.938412.938412 lmp.py:1935]   Expert 42 |    171 | CPU
DEBUG 01-15 16:10:35.938055.938055 lmp.py:1935]   Expert 31 |    172 | CPU
DEBUG 01-15 16:10:35.938459.938459 lmp.py:1935]   Expert 44 |    174 | CPU
DEBUG 01-15 16:10:35.938625.938625 lmp.py:1935]   Expert 41 |    176 | CPU
DEBUG 01-15 16:10:35.938791.938791 lmp.py:1935]   Expert 46 |    176 | CPU
DEBUG 01-15 16:10:35.939342.939342 lmp.py:1935]   Expert  9 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939654.939654 lmp.py:1935]   Expert 43 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939489.939489 lmp.py:1935]   Expert 62 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939847.939847 lmp.py:1935]   Expert 18 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939920.939920 lmp.py:1935]   Expert 26 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939709.939709 lmp.py:1935]   Expert 50 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939882.939882 lmp.py:1935]   Expert 49 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939956.939956 lmp.py:1935]   Expert 51 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939598.939598 lmp.py:1935]   Expert 27 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939003.939003 lmp.py:1935]   Expert 11 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939408.939408 lmp.py:1935]   Expert 47 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939574.939574 lmp.py:1935]   Expert 19 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939501.939501 lmp.py:1935]   Expert 20 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939906.939906 lmp.py:1935]   Expert 63 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939264.939264 lmp.py:1935]   Expert 56 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939099.939099 lmp.py:1935]   Expert 55 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939411.939411 lmp.py:1935]   Expert 38 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939293.939293 lmp.py:1935]   Expert 48 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939936.939936 lmp.py:1935]   Expert  1 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939863.939863 lmp.py:1935]   Expert 10 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939281.939281 lmp.py:1935]   Expert 54 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939223.939223 lmp.py:1935]   Expert  7 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939435.939435 lmp.py:1935]   Expert 21 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939648.939648 lmp.py:1935]   Expert 33 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939291.939291 lmp.py:1935]   Expert 29 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939695.939695 lmp.py:1935]   Expert 40 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939100.939100 lmp.py:1935]   Expert 24 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939743.939743 lmp.py:1935]   Expert 59 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939432.939432 lmp.py:1935]   Expert 37 |    333 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939883.939883 lmp.py:1935]   Expert 58 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939334.939334 lmp.py:1935]   Expert  6 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:10:35.939546.939546 lmp.py:1935]   Expert 53 |    850 | GPU1(cuda:1)
DEBUG 01-15 16:10:35.939805.939805 lmp.py:1937] 
DEBUG 01-15 16:10:35.939805.939805 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:35.939541.939541 lmp.py:1938]   CPU:   4179 tokens
DEBUG 01-15 16:10:35.939753.939753 lmp.py:1942]   cuda:1:   3973 tokens (15 experts)
DEBUG 01-15 16:10:35.939727.939727 lmp.py:1942]   cuda:2:   4136 tokens (17 experts)
DEBUG 01-15 16:10:35.939986.939986 lmp.py:1943]   Total GPU:   8109 tokens
DEBUG 01-15 16:10:35.939768.939768 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:35.939768.939768 lmp.py:1944] 
DEBUG 01-15 16:10:35.939179.939179 cuda_h.py:19] end experts_map_get cost 0.001786947250366211 seconds
DEBUG 01-15 16:10:35.939360.939360 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:35.939447.939447 lmp.py:1953] 
DEBUG 01-15 16:10:35.939447.939447 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:35.939230.939230 cuda_h.py:19] end cpu_experts_submit cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:35.939993.939993 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:35.939300.939300 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:35.941212.941212 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.33.gate_proj.weight': 0, 'model.layers.5.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.40.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.40.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.40.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.48.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.48.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.48.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.56.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.56.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.56.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.38.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.38.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.38.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.51.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.51.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.51.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.27.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.27.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.27.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.29.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.62.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.62.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.62.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(7860649984, 5767168, 0, 0), (7866417152, 5767168, 5767168, 0), (7854882816, 5767168, 11534336, 0), (7410810880, 5767168, 17301504, 0), (7416578048, 5767168, 23068672, 0), (7405043712, 5767168, 28835840, 0), (7981760512, 5767168, 34603008, 0), (7987527680, 5767168, 40370176, 0), (7975993344, 5767168, 46137344, 0), (7462715392, 5767168, 51904512, 0), (7468482560, 5767168, 57671680, 0), (7456948224, 5767168, 63438848, 0), (7480016896, 5767168, 69206016, 0), (7485784064, 5767168, 74973184, 0), (7474249728, 5767168, 80740352, 0), (8033665024, 5767168, 86507520, 0), (8039432192, 5767168, 92274688, 0), (8027897856, 5767168, 98041856, 0), (8120172544, 5767168, 103809024, 0), (8125939712, 5767168, 109576192, 0), (8114405376, 5767168, 115343360, 0), (8137474048, 5767168, 121110528, 0), (8143241216, 5767168, 126877696, 0), (8131706880, 5767168, 132644864, 0), (7601127424, 5767168, 138412032, 0), (7606894592, 5767168, 144179200, 0), (7595360256, 5767168, 149946368, 0), (7618428928, 5767168, 155713536, 0), (7624196096, 5767168, 161480704, 0), (7612661760, 5767168, 167247872, 0), (8206680064, 5767168, 173015040, 0), (8212447232, 5767168, 178782208, 0), (8200912896, 5767168, 184549376, 0), (8241283072, 5767168, 190316544, 0), (8247050240, 5767168, 196083712, 0), (8235515904, 5767168, 201850880, 0), (7739539456, 5767168, 207618048, 0), (7745306624, 5767168, 213385216, 0), (7733772288, 5767168, 219152384, 0), (8310489088, 5767168, 224919552, 0), (8316256256, 5767168, 230686720, 0), (8304721920, 5767168, 236453888, 0), (8379695104, 5767168, 242221056, 0), (8385462272, 5767168, 247988224, 0), (8373927936, 5767168, 253755392, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (8258584576, 5767168, 17301504, 0), (8264351744, 5767168, 23068672, 0), (8252817408, 5767168, 28835840, 0), (7929856000, 5767168, 34603008, 0), (7935623168, 5767168, 40370176, 0), (7924088832, 5767168, 46137344, 0), (7393509376, 5767168, 51904512, 0), (7399276544, 5767168, 57671680, 0), (7387742208, 5767168, 63438848, 0), (7947157504, 5767168, 69206016, 0), (7952924672, 5767168, 74973184, 0), (7941390336, 5767168, 80740352, 0), (7445413888, 5767168, 86507520, 0), (7451181056, 5767168, 92274688, 0), (7439646720, 5767168, 98041856, 0), (8102871040, 5767168, 103809024, 0), (8108638208, 5767168, 109576192, 0), (8097103872, 5767168, 115343360, 0), (8154775552, 5767168, 121110528, 0), (8160542720, 5767168, 126877696, 0), (8149008384, 5767168, 132644864, 0), (8172077056, 5767168, 138412032, 0), (8177844224, 5767168, 144179200, 0), (8166309888, 5767168, 149946368, 0), (7635730432, 5767168, 155713536, 0), (7641497600, 5767168, 161480704, 0), (7629963264, 5767168, 167247872, 0), (7653031936, 5767168, 173015040, 0), (7658799104, 5767168, 178782208, 0), (7647264768, 5767168, 184549376, 0), (8223981568, 5767168, 190316544, 0), (8229748736, 5767168, 196083712, 0), (8218214400, 5767168, 201850880, 0), (7704936448, 5767168, 207618048, 0), (7710703616, 5767168, 213385216, 0), (7699169280, 5767168, 219152384, 0), (8293187584, 5767168, 224919552, 0), (8298954752, 5767168, 230686720, 0), (8287420416, 5767168, 236453888, 0), (7756840960, 5767168, 242221056, 0), (7762608128, 5767168, 247988224, 0), (7751073792, 5767168, 253755392, 0), (7791443968, 5767168, 259522560, 0), (7797211136, 5767168, 265289728, 0), (7785676800, 5767168, 271056896, 0), (8362393600, 5767168, 276824064, 0), (8368160768, 5767168, 282591232, 0), (8356626432, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec45cbbd0>, 2: <capsule object NULL at 0x7a4f2c0abc30>}
DEBUG 01-15 16:10:35.941898.941898 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:35.941058.941058 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:35.941622.941622 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 37ea6c6e-f0b7-4cbf-be59-27a974d3fb91
DEBUG 01-15 16:10:35.942489.942489 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:35.942350.942350 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:35.943678.943678 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:35.943102.943102 client.py:127] Model loaded
DEBUG 01-15 16:10:35.943501.943501 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.943944.943944 cuda_h.py:19] end restore2model cost 0.00032973289489746094 seconds
DEBUG 01-15 16:10:35.943806.943806 cuda_h.py:19] end sllm_worker_task cost 0.011243343353271484 seconds
DEBUG 01-15 16:10:35.944576.944576 cuda_h.py:19] end move_flatidxs cost 0.0011577606201171875 seconds
INFO 01-15 16:10:35.944011.944011 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 37ea6c6e-f0b7-4cbf-be59-27a974d3fb91
DEBUG 01-15 16:10:35.944343.944343 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:35.945032.945032 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005037069320678711 seconds
DEBUG 01-15 16:10:35.945856.945856 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:35.947716.947716 cuda_h.py:19] end restore2model cost 0.0024907588958740234 seconds
DEBUG 01-15 16:10:35.947936.947936 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007766008377075195 seconds
DEBUG 01-15 16:10:35.947970.947970 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:35.947185.947185 cuda_h.py:19] end gpu_sexperts cost 0.0002689361572265625 seconds
DEBUG 01-15 16:10:35.948154.948154 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:35.948738.948738 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:35.948149.948149 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:35.948713.948713 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.948700.948700 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000766754150390625 seconds
DEBUG 01-15 16:10:35.948729.948729 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.949947.949947 cuda_h.py:19] end gpu_group_list cost 0.0001678466796875 seconds
DEBUG 01-15 16:10:35.949481.949481 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:35.950321.950321 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009136199951171875 seconds
DEBUG 01-15 16:10:35.950708.950708 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:35.951947.951947 cuda_h.py:19] end gpu_group_list cost 0.0001811981201171875 seconds
DEBUG 01-15 16:10:35.951669.951669 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:35.951121.951121 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 37ea6c6e-f0b7-4cbf-be59-27a974d3fb91
DEBUG 01-15 16:10:35.962142.962142 cuda_h.py:19] end group_tensors cost 0.017406463623046875 seconds
DEBUG 01-15 16:10:35.963719.963719 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:35.968980.968980 cuda_h.py:19] end group pad cost 0.005340576171875 seconds
DEBUG 01-15 16:10:35.968860.968860 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:35.974118.974118 client.py:127] Model loaded
DEBUG 01-15 16:10:35.974019.974019 cuda_h.py:19] end wait_experts_multi_device cost 0.02282118797302246 seconds
DEBUG 01-15 16:10:35.974742.974742 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:35.987342.987342 cuda_h.py:19] end group_einsum cost 0.018642425537109375 seconds
DEBUG 01-15 16:10:35.987506.987506 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:35.991393.991393 cuda_h.py:19] end get_outputs_cpu1 cost 0.00405120849609375 seconds
DEBUG 01-15 16:10:35.992616.992616 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04980802536010742 seconds
DEBUG 01-15 16:10:35.993622.993622 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.018323183059692383 seconds
DEBUG 01-15 16:10:35.993381.993381 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:35.994162.994162 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.994842.994842 cuda_h.py:19] end index_scatter cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:10:35.994641.994641 cuda_h.py:19] end cpuoutputsdeal cost 0.0015909671783447266 seconds
DEBUG 01-15 16:10:35.994935.994935 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:35.994267.994267 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.995704.995704 cuda_h.py:19] end gpu_group_tensor cost 0.0003502368927001953 seconds
DEBUG 01-15 16:10:35.995348.995348 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:35.996627.996627 cuda_h.py:19] end gpu_group_tensor cost 0.0005865097045898438 seconds
DEBUG 01-15 16:10:35.996228.996228 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.996773.996773 cuda_h.py:19] end gpu_group_einsum cost 0.0008041858673095703 seconds
DEBUG 01-15 16:10:35.997731.997731 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:35.997869.997869 cuda_h.py:19] end gpu_group_einsum cost 0.0004892349243164062 seconds
DEBUG 01-15 16:10:35.997615.997615 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.997156.997156 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.998555.998555 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021219253540039062 seconds
DEBUG 01-15 16:10:35.998894.998894 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.998222.998222 cuda_h.py:19] end concat_expert_out cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:10:35.998635.998635 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.998889.998889 cuda_h.py:19] end index_scatter cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:35.998863.998863 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008313655853271484 seconds
DEBUG 01-15 16:10:35.998317.998317 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:35.998723.998723 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:35.999648.999648 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012040138244628906 seconds
DEBUG 01-15 16:10:35.999451.999451 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:35.999228.999228 cuda_h.py:19] end concat_expert_out cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:35.999018.999018 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:35.999127.999127 cuda_h.py:19] end index_scatter cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:10:35.999459.999459 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004565715789794922 seconds
DEBUG 01-15 16:10:35.999547.999547 cuda_h.py:19] end gpu_experts_multi_device cost 0.05121803283691406 seconds
DEBUG 01-15 16:10:35.999550.999550 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.06231212615966797 seconds
DEBUG 01-15 16:10:35.999184.999184 cuda_h.py:19] end prefill_layer cost 0.06769227981567383 seconds
DEBUG 01-15 16:10:35.999226.999226 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:10:35.999121.999121 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:35.999016.999016 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:10:35.999626.999626 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:35.999190.999190 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:36.000471.000471 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 3.8623809814453125e-05 seconds
DEBUG 01-15 16:10:36.000750.000750 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 7.176399230957031e-05 seconds
DEBUG 01-15 16:10:36.000069.000069 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.000687.000687 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.000387.000387 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.000401.000401 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.000883.000883 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.000991.000991 cuda_h.py:19] end allocate_cuda_memory cost 0.0002143383026123047 seconds
DEBUG 01-15 16:10:36.000729.000729 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.000306.000306 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.000182.000182 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.000700.000700 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9c4b3f06-14d4-405d-9746-f3e4aac0b1cc
DEBUG 01-15 16:10:36.001803.001803 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.001269.001269 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.002793.002793 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9c4b3f06-14d4-405d-9746-f3e4aac0b1cc
DEBUG 01-15 16:10:36.002544.002544 cuda_h.py:19] end load_into_gpu_async cost 0.0021414756774902344 seconds
DEBUG 01-15 16:10:36.003061.003061 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.003694.003694 cuda_h.py:19] end restore_tensors2 cost 8.368492126464844e-05 seconds
DEBUG 01-15 16:10:36.003172.003172 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0027306079864501953 seconds
INFO 01-15 16:10:36.003207.003207 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9c4b3f06-14d4-405d-9746-f3e4aac0b1cc
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.004436.004436 cuda_h.py:19] end self_attn cost 0.003142833709716797 seconds
DEBUG 01-15 16:10:36.004871.004871 cuda_h.py:19] end iln_self_attn_paln cost 0.004737138748168945 seconds
DEBUG 01-15 16:10:36.004700.004700 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:10:36.004039.004039 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.005249.005249 cuda_h.py:19] end gate cost 0.0006773471832275391 seconds
DEBUG 01-15 16:10:36.005224.005224 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.006129.006129 lmp.py:1912] 
DEBUG 01-15 16:10:36.006129.006129 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.006600.006600 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.006442.006442 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.006469.006469 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.006589.006589 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.006755.006755 lmp.py:1917] 
DEBUG 01-15 16:10:36.006755.006755 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.006398.006398 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.006763.006763 lmp.py:1935]   Expert  1 |     45 | CPU
DEBUG 01-15 16:10:36.006644.006644 lmp.py:1935]   Expert  7 |     59 | CPU
DEBUG 01-15 16:10:36.006857.006857 lmp.py:1935]   Expert 37 |     71 | CPU
DEBUG 01-15 16:10:36.006307.006307 lmp.py:1935]   Expert 17 |     75 | CPU
DEBUG 01-15 16:10:36.006758.006758 lmp.py:1935]   Expert 54 |     76 | CPU
DEBUG 01-15 16:10:36.006209.006209 lmp.py:1935]   Expert 18 |     84 | CPU
DEBUG 01-15 16:10:36.006660.006660 lmp.py:1935]   Expert 13 |     91 | CPU
DEBUG 01-15 16:10:36.006111.006111 lmp.py:1935]   Expert  9 |     92 | CPU
DEBUG 01-15 16:10:36.006800.006800 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:10:36.006443.006443 lmp.py:1935]   Expert 22 |    102 | CPU
DEBUG 01-15 16:10:36.006563.006563 lmp.py:1935]   Expert  0 |    109 | CPU
DEBUG 01-15 16:10:36.006929.006929 lmp.py:1935]   Expert 26 |    116 | CPU
DEBUG 01-15 16:10:36.006380.006380 lmp.py:1935]   Expert 16 |    119 | CPU
DEBUG 01-15 16:10:36.006354.006354 lmp.py:1935]   Expert 10 |    122 | CPU
DEBUG 01-15 16:10:36.006090.006090 lmp.py:1935]   Expert 63 |    128 | CPU
DEBUG 01-15 16:10:36.006793.006793 lmp.py:1935]   Expert 59 |    134 | CPU
DEBUG 01-15 16:10:36.006052.006052 lmp.py:1935]   Expert 62 |    140 | CPU
DEBUG 01-15 16:10:36.006072.006072 lmp.py:1935]   Expert 43 |    142 | CPU
DEBUG 01-15 16:10:36.006808.006808 lmp.py:1935]   Expert 28 |    145 | CPU
DEBUG 01-15 16:10:36.006166.006166 lmp.py:1935]   Expert 33 |    147 | CPU
DEBUG 01-15 16:10:36.006093.006093 lmp.py:1935]   Expert 29 |    149 | CPU
DEBUG 01-15 16:10:36.006021.006021 lmp.py:1935]   Expert  2 |    158 | CPU
DEBUG 01-15 16:10:36.006949.006949 lmp.py:1935]   Expert 51 |    164 | CPU
DEBUG 01-15 16:10:36.006400.006400 lmp.py:1935]   Expert 32 |    166 | CPU
DEBUG 01-15 16:10:36.006374.006374 lmp.py:1935]   Expert 11 |    167 | CPU
DEBUG 01-15 16:10:36.006586.006586 lmp.py:1935]   Expert 45 |    167 | CPU
DEBUG 01-15 16:10:36.006560.006560 lmp.py:1935]   Expert 55 |    167 | CPU
DEBUG 01-15 16:10:36.006011.006011 lmp.py:1935]   Expert  3 |    168 | CPU
DEBUG 01-15 16:10:36.006462.006462 lmp.py:1935]   Expert 40 |    168 | CPU
DEBUG 01-15 16:10:36.006675.006675 lmp.py:1935]   Expert 23 |    169 | CPU
DEBUG 01-15 16:10:36.006649.006649 lmp.py:1935]   Expert 53 |    169 | CPU
DEBUG 01-15 16:10:36.006100.006100 lmp.py:1935]   Expert 14 |    173 | CPU
DEBUG 01-15 16:10:36.006418.006418 lmp.py:1935]   Expert 34 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.006253.006253 lmp.py:1935]   Expert 52 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.006658.006658 lmp.py:1935]   Expert 41 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.006062.006062 lmp.py:1935]   Expert 42 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.006990.006990 lmp.py:1935]   Expert 21 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.006441.006441 lmp.py:1935]   Expert 57 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.006892.006892 lmp.py:1935]   Expert 30 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.006104.006104 lmp.py:1935]   Expert 15 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.006555.006555 lmp.py:1935]   Expert 35 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.006006.006006 lmp.py:1935]   Expert 12 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.006219.006219 lmp.py:1935]   Expert  4 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.006431.006431 lmp.py:1935]   Expert 46 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.006643.006643 lmp.py:1935]   Expert 50 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.006810.006810 lmp.py:1935]   Expert 19 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.007976.007976 lmp.py:1935]   Expert 24 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007903.007903 lmp.py:1935]   Expert  8 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007831.007831 lmp.py:1935]   Expert 44 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007282.007282 lmp.py:1935]   Expert 49 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.007733.007733 lmp.py:1935]   Expert 38 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.007945.007945 lmp.py:1935]   Expert  6 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007919.007919 lmp.py:1935]   Expert 47 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.007894.007894 lmp.py:1935]   Expert 31 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007106.007106 lmp.py:1935]   Expert 61 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.007080.007080 lmp.py:1935]   Expert 39 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007054.007054 lmp.py:1935]   Expert 36 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.007028.007028 lmp.py:1935]   Expert  5 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007241.007241 lmp.py:1935]   Expert 27 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.007930.007930 lmp.py:1935]   Expert 60 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007858.007858 lmp.py:1935]   Expert 20 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.007024.007024 lmp.py:1935]   Expert 48 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007951.007951 lmp.py:1935]   Expert 25 |    399 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.007879.007879 lmp.py:1935]   Expert 56 |    556 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.007853.007853 lmp.py:1937] 
DEBUG 01-15 16:10:36.007853.007853 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.007827.007827 lmp.py:1938]   CPU:   4083 tokens
DEBUG 01-15 16:10:36.007040.007040 lmp.py:1942]   cuda:1:   4117 tokens (16 experts)
DEBUG 01-15 16:10:36.007014.007014 lmp.py:1942]   cuda:2:   4088 tokens (16 experts)
DEBUG 01-15 16:10:36.007511.007511 lmp.py:1943]   Total GPU:   8205 tokens
DEBUG 01-15 16:10:36.007770.007770 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.007770.007770 lmp.py:1944] 
DEBUG 01-15 16:10:36.007989.007989 cuda_h.py:19] end experts_map_get cost 0.0016591548919677734 seconds
DEBUG 01-15 16:10:36.007309.007309 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.007204.007204 lmp.py:1953] 
DEBUG 01-15 16:10:36.007204.007204 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.007080.007080 cuda_h.py:19] end cpu_experts_submit cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:10:36.007392.007392 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.007791.007791 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.009906.009906 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.34.gate_proj.weight': 0, 'model.layers.6.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.38.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.38.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.38.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.6.mlp.experts.5.gate_proj.weight': 0, 'model.layers.6.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.57.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.57.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.57.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(8985247744, 5767168, 0, 0), (8991014912, 5767168, 5767168, 0), (8979480576, 5767168, 11534336, 0), (9002549248, 5767168, 17301504, 0), (9008316416, 5767168, 23068672, 0), (8996782080, 5767168, 28835840, 0), (9019850752, 5767168, 34603008, 0), (9025617920, 5767168, 40370176, 0), (9014083584, 5767168, 46137344, 0), (8466202624, 5767168, 51904512, 0), (8471969792, 5767168, 57671680, 0), (8460435456, 5767168, 63438848, 0), (9054453760, 5767168, 69206016, 0), (9060220928, 5767168, 74973184, 0), (9048686592, 5767168, 80740352, 0), (9106358272, 5767168, 86507520, 0), (9112125440, 5767168, 92274688, 0), (9100591104, 5767168, 98041856, 0), (9192865792, 5767168, 103809024, 0), (9198632960, 5767168, 109576192, 0), (9187098624, 5767168, 115343360, 0), (9210167296, 5767168, 121110528, 0), (9215934464, 5767168, 126877696, 0), (9204400128, 5767168, 132644864, 0), (9244770304, 5767168, 138412032, 0), (9250537472, 5767168, 144179200, 0), (9239003136, 5767168, 149946368, 0), (8725725184, 5767168, 155713536, 0), (8731492352, 5767168, 161480704, 0), (8719958016, 5767168, 167247872, 0), (8743026688, 5767168, 173015040, 0), (8748793856, 5767168, 178782208, 0), (8737259520, 5767168, 184549376, 0), (8760328192, 5767168, 190316544, 0), (8766095360, 5767168, 196083712, 0), (8754561024, 5767168, 201850880, 0), (9365880832, 5767168, 207618048, 0), (9371648000, 5767168, 213385216, 0), (9360113664, 5767168, 219152384, 0), (8864137216, 5767168, 224919552, 0), (8869904384, 5767168, 230686720, 0), (8858370048, 5767168, 236453888, 0), (9452388352, 5767168, 242221056, 0), (9458155520, 5767168, 247988224, 0), (9446621184, 5767168, 253755392, 0), (8916041728, 5767168, 259522560, 0), (8921808896, 5767168, 265289728, 0), (8910274560, 5767168, 271056896, 0)], 2: [(8483504128, 5767168, 0, 0), (8489271296, 5767168, 5767168, 0), (8477736960, 5767168, 11534336, 0), (8500805632, 5767168, 17301504, 0), (8506572800, 5767168, 23068672, 0), (8495038464, 5767168, 28835840, 0), (9071755264, 5767168, 34603008, 0), (9077522432, 5767168, 40370176, 0), (9065988096, 5767168, 46137344, 0), (8535408640, 5767168, 51904512, 0), (8541175808, 5767168, 57671680, 0), (8529641472, 5767168, 63438848, 0), (9123659776, 5767168, 69206016, 0), (9129426944, 5767168, 74973184, 0), (9117892608, 5767168, 80740352, 0), (9158262784, 5767168, 86507520, 0), (9164029952, 5767168, 92274688, 0), (9152495616, 5767168, 98041856, 0), (8604614656, 5767168, 103809024, 0), (8610381824, 5767168, 109576192, 0), (8598847488, 5767168, 115343360, 0), (9383182336, 5767168, 121110528, 0), (9388949504, 5767168, 126877696, 0), (9377415168, 5767168, 132644864, 0), (8656519168, 5767168, 138412032, 0), (8662286336, 5767168, 144179200, 0), (8650752000, 5767168, 149946368, 0), (9227468800, 5767168, 155713536, 0), (9233235968, 5767168, 161480704, 0), (9221701632, 5767168, 167247872, 0), (9262071808, 5767168, 173015040, 0), (9267838976, 5767168, 178782208, 0), (9256304640, 5767168, 184549376, 0), (9296674816, 5767168, 190316544, 0), (9302441984, 5767168, 196083712, 0), (9290907648, 5767168, 201850880, 0), (8812232704, 5767168, 207618048, 0), (8817999872, 5767168, 213385216, 0), (8806465536, 5767168, 219152384, 0), (8829534208, 5767168, 224919552, 0), (8835301376, 5767168, 230686720, 0), (8823767040, 5767168, 236453888, 0), (9435086848, 5767168, 242221056, 0), (9440854016, 5767168, 247988224, 0), (9429319680, 5767168, 253755392, 0), (8933343232, 5767168, 259522560, 0), (8939110400, 5767168, 265289728, 0), (8927576064, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e3464b750>, 2: <capsule object NULL at 0x7a4e3464bba0>}
DEBUG 01-15 16:10:36.009287.009287 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.009162.009162 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.009627.009627 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1237452f-1ad2-462e-b5ab-2bdf538b5cb1
DEBUG 01-15 16:10:36.010487.010487 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.010374.010374 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.011545.011545 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:36.011620.011620 client.py:127] Model loaded
DEBUG 01-15 16:10:36.011019.011019 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.011521.011521 cuda_h.py:19] end restore2model cost 0.0003275871276855469 seconds
DEBUG 01-15 16:10:36.011576.011576 cuda_h.py:19] end sllm_worker_task cost 0.011552810668945312 seconds
DEBUG 01-15 16:10:36.012834.012834 cuda_h.py:19] end move_flatidxs cost 0.0011622905731201172 seconds
INFO 01-15 16:10:36.012367.012367 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1237452f-1ad2-462e-b5ab-2bdf538b5cb1
DEBUG 01-15 16:10:36.012270.012270 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.012951.012951 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005349159240722656 seconds
DEBUG 01-15 16:10:36.013682.013682 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.015688.015688 cuda_h.py:19] end restore2model cost 0.002497434616088867 seconds
DEBUG 01-15 16:10:36.015239.015239 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008083105087280273 seconds
DEBUG 01-15 16:10:36.015319.015319 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.015475.015475 cuda_h.py:19] end gpu_sexperts cost 0.00025916099548339844 seconds
DEBUG 01-15 16:10:36.016966.016966 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.016359.016359 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4066696166992188e-05 seconds
DEBUG 01-15 16:10:36.016631.016631 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.016003.016003 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.016164.016164 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008254051208496094 seconds
DEBUG 01-15 16:10:36.017245.017245 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.017584.017584 cuda_h.py:19] end gpu_group_list cost 0.00018405914306640625 seconds
DEBUG 01-15 16:10:36.017887.017887 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.018441.018441 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008726119995117188 seconds
DEBUG 01-15 16:10:36.019874.019874 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.019153.019153 cuda_h.py:19] end gpu_group_list cost 0.0001761913299560547 seconds
DEBUG 01-15 16:10:36.019172.019172 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.019055.019055 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1237452f-1ad2-462e-b5ab-2bdf538b5cb1
DEBUG 01-15 16:10:36.026241.026241 cuda_h.py:19] end group_tensors cost 0.013852834701538086 seconds
DEBUG 01-15 16:10:36.027654.027654 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.031042.031042 cuda_h.py:19] end group pad cost 0.0042171478271484375 seconds
DEBUG 01-15 16:10:36.031715.031715 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.040573.040573 client.py:127] Model loaded
DEBUG 01-15 16:10:36.040095.040095 cuda_h.py:19] end wait_experts_multi_device cost 0.020697832107543945 seconds
DEBUG 01-15 16:10:36.040712.040712 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.050301.050301 cuda_h.py:19] end group_einsum cost 0.018394947052001953 seconds
DEBUG 01-15 16:10:36.050756.050756 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.054472.054472 cuda_h.py:19] end get_outputs_cpu1 cost 0.0038940906524658203 seconds
DEBUG 01-15 16:10:36.055919.055919 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04429054260253906 seconds
DEBUG 01-15 16:10:36.055740.055740 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.014550447463989258 seconds
DEBUG 01-15 16:10:36.055260.055260 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.056942.056942 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.056583.056583 cuda_h.py:19] end index_scatter cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:10:36.057150.057150 cuda_h.py:19] end cpuoutputsdeal cost 0.0016336441040039062 seconds
DEBUG 01-15 16:10:36.057921.057921 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.057776.057776 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.057908.057908 cuda_h.py:19] end gpu_group_tensor cost 0.00013589859008789062 seconds
DEBUG 01-15 16:10:36.057479.057479 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.057120.057120 cuda_h.py:19] end gpu_group_tensor cost 0.0001270771026611328 seconds
DEBUG 01-15 16:10:36.057116.057116 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.058432.058432 cuda_h.py:19] end gpu_group_einsum cost 0.00046634674072265625 seconds
DEBUG 01-15 16:10:36.058317.058317 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.059228.059228 cuda_h.py:19] end gpu_group_einsum cost 0.0004775524139404297 seconds
DEBUG 01-15 16:10:36.059220.059220 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.059714.059714 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.059239.059239 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001995563507080078 seconds
DEBUG 01-15 16:10:36.059862.059862 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.059475.059475 cuda_h.py:19] end concat_expert_out cost 6.175041198730469e-05 seconds
DEBUG 01-15 16:10:36.059364.059364 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.059493.059493 cuda_h.py:19] end index_scatter cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:10:36.060979.060979 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008456707000732422 seconds
DEBUG 01-15 16:10:36.060962.060962 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.060990.060990 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.060082.060082 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014090538024902344 seconds
DEBUG 01-15 16:10:36.060030.060030 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.060669.060669 cuda_h.py:19] end concat_expert_out cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:36.060704.060704 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.060720.060720 cuda_h.py:19] end index_scatter cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:36.060443.060443 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004868507385253906 seconds
DEBUG 01-15 16:10:36.060585.060585 cuda_h.py:19] end gpu_experts_multi_device cost 0.04467439651489258 seconds
DEBUG 01-15 16:10:36.060302.060302 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.0559239387512207 seconds
DEBUG 01-15 16:10:36.061513.061513 cuda_h.py:19] end prefill_layer cost 0.06131792068481445 seconds
DEBUG 01-15 16:10:36.061740.061740 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:10:36.061443.061443 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.061100.061100 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:10:36.061379.061379 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:36.061566.061566 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:36.061608.061608 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:10:36.061933.061933 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 7.009506225585938e-05 seconds
DEBUG 01-15 16:10:36.061775.061775 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.061546.061546 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.061939.061939 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.061823.061823 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.061566.061566 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.062741.062741 cuda_h.py:19] end allocate_cuda_memory cost 0.00023174285888671875 seconds
DEBUG 01-15 16:10:36.062287.062287 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.062242.062242 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.062548.062548 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.062973.062973 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ad5d2fd4-7d36-4974-ac41-0aaaa56929bc
DEBUG 01-15 16:10:36.062169.062169 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.062252.062252 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.064019.064019 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ad5d2fd4-7d36-4974-ac41-0aaaa56929bc
DEBUG 01-15 16:10:36.064803.064803 cuda_h.py:19] end load_into_gpu_async cost 0.0021321773529052734 seconds
DEBUG 01-15 16:10:36.064388.064388 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.064365.064365 cuda_h.py:19] end restore_tensors2 cost 8.940696716308594e-05 seconds
DEBUG 01-15 16:10:36.064274.064274 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0027577877044677734 seconds
INFO 01-15 16:10:36.064945.064945 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ad5d2fd4-7d36-4974-ac41-0aaaa56929bc
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.065394.065394 cuda_h.py:19] end self_attn cost 0.003195047378540039 seconds
DEBUG 01-15 16:10:36.066551.066551 cuda_h.py:19] end iln_self_attn_paln cost 0.004751920700073242 seconds
DEBUG 01-15 16:10:36.066811.066811 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:10:36.066574.066574 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.067929.067929 cuda_h.py:19] end gate cost 0.0006809234619140625 seconds
DEBUG 01-15 16:10:36.067765.067765 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.067454.067454 lmp.py:1912] 
DEBUG 01-15 16:10:36.067454.067454 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.067025.067025 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.067350.067350 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.067431.067431 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.067842.067842 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.067538.067538 lmp.py:1917] 
DEBUG 01-15 16:10:36.067538.067538 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.067757.067757 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.067652.067652 lmp.py:1935]   Expert 50 |     45 | CPU
DEBUG 01-15 16:10:36.067110.067110 lmp.py:1935]   Expert  3 |     53 | CPU
DEBUG 01-15 16:10:36.067091.067091 lmp.py:1935]   Expert 46 |     56 | CPU
DEBUG 01-15 16:10:36.067595.067595 lmp.py:1935]   Expert  1 |     76 | CPU
DEBUG 01-15 16:10:36.067337.067337 lmp.py:1935]   Expert  4 |     88 | CPU
DEBUG 01-15 16:10:36.067080.067080 lmp.py:1935]   Expert 29 |     89 | CPU
DEBUG 01-15 16:10:36.067776.067776 lmp.py:1935]   Expert 15 |     95 | CPU
DEBUG 01-15 16:10:36.067949.067949 lmp.py:1935]   Expert 40 |     97 | CPU
DEBUG 01-15 16:10:36.067883.067883 lmp.py:1935]   Expert  8 |    110 | CPU
DEBUG 01-15 16:10:36.067818.067818 lmp.py:1935]   Expert 28 |    110 | CPU
DEBUG 01-15 16:10:36.067176.067176 lmp.py:1935]   Expert 41 |    113 | CPU
DEBUG 01-15 16:10:36.067580.067580 lmp.py:1935]   Expert 27 |    126 | CPU
DEBUG 01-15 16:10:36.068985.068985 lmp.py:1935]   Expert 13 |    127 | CPU
DEBUG 01-15 16:10:36.068390.068390 lmp.py:1935]   Expert 16 |    127 | CPU
DEBUG 01-15 16:10:36.068794.068794 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:10:36.068960.068960 lmp.py:1935]   Expert 48 |    130 | CPU
DEBUG 01-15 16:10:36.068603.068603 lmp.py:1935]   Expert 54 |    132 | CPU
DEBUG 01-15 16:10:36.068769.068769 lmp.py:1935]   Expert  7 |    134 | CPU
DEBUG 01-15 16:10:36.068604.068604 lmp.py:1935]   Expert 51 |    138 | CPU
DEBUG 01-15 16:10:36.068963.068963 lmp.py:1935]   Expert 60 |    139 | CPU
DEBUG 01-15 16:10:36.068606.068606 lmp.py:1935]   Expert 39 |    140 | CPU
DEBUG 01-15 16:10:36.068772.068772 lmp.py:1935]   Expert 18 |    141 | CPU
DEBUG 01-15 16:10:36.068938.068938 lmp.py:1935]   Expert 43 |    146 | CPU
DEBUG 01-15 16:10:36.068343.068343 lmp.py:1935]   Expert 14 |    147 | CPU
DEBUG 01-15 16:10:36.068509.068509 lmp.py:1935]   Expert 56 |    148 | CPU
DEBUG 01-15 16:10:36.068675.068675 lmp.py:1935]   Expert 20 |    149 | CPU
DEBUG 01-15 16:10:36.068841.068841 lmp.py:1935]   Expert 52 |    149 | CPU
DEBUG 01-15 16:10:36.068007.068007 lmp.py:1935]   Expert 36 |    152 | CPU
DEBUG 01-15 16:10:36.068412.068412 lmp.py:1935]   Expert 55 |    153 | CPU
DEBUG 01-15 16:10:36.068339.068339 lmp.py:1935]   Expert 10 |    157 | CPU
DEBUG 01-15 16:10:36.068744.068744 lmp.py:1935]   Expert 45 |    157 | CPU
DEBUG 01-15 16:10:36.068148.068148 lmp.py:1935]   Expert 11 |    158 | CPU
DEBUG 01-15 16:10:36.068652.068652 lmp.py:1935]   Expert  5 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068680.068680 lmp.py:1935]   Expert 62 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068144.068144 lmp.py:1935]   Expert 57 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068555.068555 lmp.py:1935]   Expert 44 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068583.068583 lmp.py:1935]   Expert 33 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068894.068894 lmp.py:1935]   Expert 53 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068776.068776 lmp.py:1935]   Expert 58 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068180.068180 lmp.py:1935]   Expert 25 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068585.068585 lmp.py:1935]   Expert  2 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068990.068990 lmp.py:1935]   Expert 32 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068633.068633 lmp.py:1935]   Expert 31 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068037.068037 lmp.py:1935]   Expert 35 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068680.068680 lmp.py:1935]   Expert 21 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068323.068323 lmp.py:1935]   Expert 49 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068728.068728 lmp.py:1935]   Expert 63 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068324.068324 lmp.py:1935]   Expert 17 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068921.068921 lmp.py:1935]   Expert 42 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068325.068325 lmp.py:1935]   Expert 34 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068730.068730 lmp.py:1935]   Expert 37 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068896.068896 lmp.py:1935]   Expert 59 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068062.068062 lmp.py:1935]   Expert 22 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068705.068705 lmp.py:1935]   Expert  0 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068871.068871 lmp.py:1935]   Expert 19 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068276.068276 lmp.py:1935]   Expert 24 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068442.068442 lmp.py:1935]   Expert 61 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068608.068608 lmp.py:1935]   Expert 30 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068013.068013 lmp.py:1935]   Expert 47 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068417.068417 lmp.py:1935]   Expert 38 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068822.068822 lmp.py:1935]   Expert 26 |    375 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068703.068703 lmp.py:1935]   Expert 12 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068300.068300 lmp.py:1935]   Expert  9 |    682 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.068625.068625 lmp.py:1935]   Expert 23 |    701 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.068599.068599 lmp.py:1937] 
DEBUG 01-15 16:10:36.068599.068599 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.068289.068289 lmp.py:1938]   CPU:   3911 tokens
DEBUG 01-15 16:10:36.068455.068455 lmp.py:1942]   cuda:1:   4191 tokens (16 experts)
DEBUG 01-15 16:10:36.068906.068906 lmp.py:1942]   cuda:2:   4186 tokens (16 experts)
DEBUG 01-15 16:10:36.069880.069880 lmp.py:1943]   Total GPU:   8377 tokens
DEBUG 01-15 16:10:36.069377.069377 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.069377.069377 lmp.py:1944] 
DEBUG 01-15 16:10:36.069312.069312 cuda_h.py:19] end experts_map_get cost 0.0018432140350341797 seconds
DEBUG 01-15 16:10:36.069969.069969 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.069864.069864 lmp.py:1953] 
DEBUG 01-15 16:10:36.069864.069864 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.069840.069840 cuda_h.py:19] end cpu_experts_submit cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:36.069059.069059 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.069696.069696 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.070728.070728 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.071606.071606 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.071871.071871 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.2.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.2.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.2.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.38.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.38.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.38.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.53.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.53.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.53.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.7.mlp.experts.32.gate_proj.weight': 0, 'model.layers.7.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (10092544000, 5767168, 17301504, 0), (10098311168, 5767168, 23068672, 0), (10086776832, 5767168, 28835840, 0), (9538895872, 5767168, 34603008, 0), (9544663040, 5767168, 40370176, 0), (9533128704, 5767168, 46137344, 0), (10144448512, 5767168, 51904512, 0), (10150215680, 5767168, 57671680, 0), (10138681344, 5767168, 63438848, 0), (10161750016, 5767168, 69206016, 0), (10167517184, 5767168, 74973184, 0), (10155982848, 5767168, 80740352, 0), (9798418432, 5767168, 86507520, 0), (9804185600, 5767168, 92274688, 0), (9792651264, 5767168, 98041856, 0), (9833021440, 5767168, 103809024, 0), (9838788608, 5767168, 109576192, 0), (9827254272, 5767168, 115343360, 0), (9867624448, 5767168, 121110528, 0), (9873391616, 5767168, 126877696, 0), (9861857280, 5767168, 132644864, 0), (10421272576, 5767168, 138412032, 0), (10427039744, 5767168, 144179200, 0), (10415505408, 5767168, 149946368, 0), (9902227456, 5767168, 155713536, 0), (9907994624, 5767168, 161480704, 0), (9896460288, 5767168, 167247872, 0), (9936830464, 5767168, 173015040, 0), (9942597632, 5767168, 178782208, 0), (9931063296, 5767168, 184549376, 0), (9954131968, 5767168, 190316544, 0), (9959899136, 5767168, 196083712, 0), (9948364800, 5767168, 201850880, 0), (10490478592, 5767168, 207618048, 0), (10496245760, 5767168, 213385216, 0), (10484711424, 5767168, 219152384, 0), (10559684608, 5767168, 224919552, 0), (10565451776, 5767168, 230686720, 0), (10553917440, 5767168, 236453888, 0), (10576986112, 5767168, 242221056, 0), (10582753280, 5767168, 247988224, 0), (10571218944, 5767168, 253755392, 0), (10040639488, 5767168, 259522560, 0), (10046406656, 5767168, 265289728, 0), (10034872320, 5767168, 271056896, 0)], 2: [(10057940992, 5767168, 0, 0), (10063708160, 5767168, 5767168, 0), (10052173824, 5767168, 11534336, 0), (10075242496, 5767168, 17301504, 0), (10081009664, 5767168, 23068672, 0), (10069475328, 5767168, 28835840, 0), (10109845504, 5767168, 34603008, 0), (10115612672, 5767168, 40370176, 0), (10104078336, 5767168, 46137344, 0), (9590800384, 5767168, 51904512, 0), (9596567552, 5767168, 57671680, 0), (9585033216, 5767168, 63438848, 0), (9660006400, 5767168, 69206016, 0), (9665773568, 5767168, 74973184, 0), (9654239232, 5767168, 80740352, 0), (10230956032, 5767168, 86507520, 0), (10236723200, 5767168, 92274688, 0), (10225188864, 5767168, 98041856, 0), (9711910912, 5767168, 103809024, 0), (9717678080, 5767168, 109576192, 0), (9706143744, 5767168, 115343360, 0), (10265559040, 5767168, 121110528, 0), (10271326208, 5767168, 126877696, 0), (10259791872, 5767168, 132644864, 0), (10317463552, 5767168, 138412032, 0), (10323230720, 5767168, 144179200, 0), (10311696384, 5767168, 149946368, 0), (10352066560, 5767168, 155713536, 0), (10357833728, 5767168, 161480704, 0), (10346299392, 5767168, 167247872, 0), (9884925952, 5767168, 173015040, 0), (9890693120, 5767168, 178782208, 0), (9879158784, 5767168, 184549376, 0), (9919528960, 5767168, 190316544, 0), (9925296128, 5767168, 196083712, 0), (9913761792, 5767168, 201850880, 0), (10507780096, 5767168, 207618048, 0), (10513547264, 5767168, 213385216, 0), (10502012928, 5767168, 219152384, 0), (10525081600, 5767168, 224919552, 0), (10530848768, 5767168, 230686720, 0), (10519314432, 5767168, 236453888, 0), (10023337984, 5767168, 242221056, 0), (10029105152, 5767168, 247988224, 0), (10017570816, 5767168, 253755392, 0), (10594287616, 5767168, 259522560, 0), (10600054784, 5767168, 265289728, 0), (10588520448, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e3464bed0>, 2: <capsule object NULL at 0x7a51b8425f80>}
DEBUG 01-15 16:10:36.071140.071140 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.071626.071626 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.071144.071144 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1b34c08c-1230-4f9a-8528-423df1633d09
DEBUG 01-15 16:10:36.072772.072772 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:36.072145.072145 client.py:127] Model loaded
DEBUG 01-15 16:10:36.072498.072498 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.072869.072869 cuda_h.py:19] end move_flatidxs cost 0.0008692741394042969 seconds
DEBUG 01-15 16:10:36.072414.072414 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.072086.072086 cuda_h.py:19] end restore2model cost 0.0003306865692138672 seconds
DEBUG 01-15 16:10:36.073948.073948 cuda_h.py:19] end sllm_worker_task cost 0.011267662048339844 seconds
INFO 01-15 16:10:36.074097.074097 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1b34c08c-1230-4f9a-8528-423df1633d09
DEBUG 01-15 16:10:36.074005.074005 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005400896072387695 seconds
DEBUG 01-15 16:10:36.075961.075961 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.077290.077290 cuda_h.py:19] end restore2model cost 0.002454042434692383 seconds
DEBUG 01-15 16:10:36.077663.077663 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008323192596435547 seconds
DEBUG 01-15 16:10:36.077028.077028 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.077178.077178 cuda_h.py:19] end gpu_sexperts cost 0.0002815723419189453 seconds
DEBUG 01-15 16:10:36.077623.077623 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.077015.077015 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:36.078473.078473 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.078460.078460 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.077647.077647 cuda_h.py:19] end group_tensors cost 0.004732370376586914 seconds
DEBUG 01-15 16:10:36.078732.078732 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.079003.079003 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0015168190002441406 seconds
DEBUG 01-15 16:10:36.079821.079821 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.080514.080514 cuda_h.py:19] end gpu_group_list cost 0.0002796649932861328 seconds
DEBUG 01-15 16:10:36.081314.081314 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.082459.082459 cuda_h.py:19] end group pad cost 0.0040628910064697266 seconds
DEBUG 01-15 16:10:36.082587.082587 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:36.082983.082983 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001271963119506836 seconds
DEBUG 01-15 16:10:36.082781.082781 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.083817.083817 cuda_h.py:19] end gpu_group_list cost 0.0008914470672607422 seconds
DEBUG 01-15 16:10:36.086808.086808 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.087447.087447 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1b34c08c-1230-4f9a-8528-423df1633d09
DEBUG 01-15 16:10:36.102388.102388 cuda_h.py:19] end group_einsum cost 0.019672870635986328 seconds
DEBUG 01-15 16:10:36.102406.102406 cuda_h.py:10] start get_outputs_cpu1
INFO 01-15 16:10:36.102572.102572 client.py:127] Model loaded
DEBUG 01-15 16:10:36.102405.102405 cuda_h.py:19] end wait_experts_multi_device cost 0.015408992767333984 seconds
DEBUG 01-15 16:10:36.102957.102957 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.106568.106568 cuda_h.py:19] end get_outputs_cpu1 cost 0.003762960433959961 seconds
DEBUG 01-15 16:10:36.106564.106564 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03597402572631836 seconds
DEBUG 01-15 16:10:36.107019.107019 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0045452117919921875 seconds
DEBUG 01-15 16:10:36.107789.107789 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.108796.108796 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.108093.108093 cuda_h.py:19] end index_scatter cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:10:36.108149.108149 cuda_h.py:19] end cpuoutputsdeal cost 0.0015947818756103516 seconds
DEBUG 01-15 16:10:36.108674.108674 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.108007.108007 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.109569.109569 cuda_h.py:19] end gpu_group_tensor cost 0.0001366138458251953 seconds
DEBUG 01-15 16:10:36.109378.109378 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.109966.109966 cuda_h.py:19] end gpu_group_tensor cost 0.00012230873107910156 seconds
DEBUG 01-15 16:10:36.109532.109532 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.109787.109787 cuda_h.py:19] end gpu_group_einsum cost 0.0004584789276123047 seconds
DEBUG 01-15 16:10:36.110705.110705 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.110729.110729 cuda_h.py:19] end gpu_group_einsum cost 0.00045800209045410156 seconds
DEBUG 01-15 16:10:36.110521.110521 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.110962.110962 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.111255.111255 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020241737365722656 seconds
DEBUG 01-15 16:10:36.111786.111786 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.111836.111836 cuda_h.py:19] end concat_expert_out cost 5.626678466796875e-05 seconds
DEBUG 01-15 16:10:36.111057.111057 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.111503.111503 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:36.111082.111082 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008864402770996094 seconds
DEBUG 01-15 16:10:36.111397.111397 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.111955.111955 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.111033.111033 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001308917999267578 seconds
DEBUG 01-15 16:10:36.111405.111405 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.112229.112229 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:36.112496.112496 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.112942.112942 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:36.112228.112228 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00047278404235839844 seconds
DEBUG 01-15 16:10:36.112946.112946 cuda_h.py:19] end gpu_experts_multi_device cost 0.034279584884643555 seconds
DEBUG 01-15 16:10:36.112922.112922 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.046010494232177734 seconds
DEBUG 01-15 16:10:36.112723.112723 cuda_h.py:19] end prefill_layer cost 0.05143928527832031 seconds
DEBUG 01-15 16:10:36.112997.112997 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:10:36.112223.112223 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.112641.112641 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:10:36.112297.112297 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:36.112053.112053 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:36.113380.113380 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 3.719329833984375e-05 seconds
DEBUG 01-15 16:10:36.113659.113659 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:10:36.113554.113554 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.113279.113279 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.113156.113156 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.113470.113470 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.113021.113021 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.113441.113441 cuda_h.py:19] end allocate_cuda_memory cost 0.00023436546325683594 seconds
DEBUG 01-15 16:10:36.113563.113563 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.113141.113141 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.113732.113732 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.113919.113919 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c487a6f0-6bb1-45c9-9ae3-ade29578c157
DEBUG 01-15 16:10:36.113915.113915 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.114018.114018 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.115790.115790 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c487a6f0-6bb1-45c9-9ae3-ade29578c157
DEBUG 01-15 16:10:36.115202.115202 cuda_h.py:19] end load_into_gpu_async cost 0.0018360614776611328 seconds
DEBUG 01-15 16:10:36.115290.115290 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.115969.115969 cuda_h.py:19] end restore_tensors2 cost 8.273124694824219e-05 seconds
DEBUG 01-15 16:10:36.115984.115984 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002453327178955078 seconds
INFO 01-15 16:10:36.115065.115065 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c487a6f0-6bb1-45c9-9ae3-ade29578c157
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.117436.117436 cuda_h.py:19] end self_attn cost 0.0031173229217529297 seconds
DEBUG 01-15 16:10:36.117923.117923 cuda_h.py:19] end iln_self_attn_paln cost 0.0046634674072265625 seconds
DEBUG 01-15 16:10:36.117038.117038 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:10:36.117039.117039 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.118811.118811 cuda_h.py:19] end gate cost 0.0006725788116455078 seconds
DEBUG 01-15 16:10:36.118647.118647 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.119820.119820 lmp.py:1912] 
DEBUG 01-15 16:10:36.119820.119820 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.119629.119629 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.119431.119431 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.119273.119273 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.119923.119923 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.119619.119619 lmp.py:1917] 
DEBUG 01-15 16:10:36.119619.119619 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.119792.119792 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.119879.119879 lmp.py:1935]   Expert 38 |     12 | CPU
DEBUG 01-15 16:10:36.119098.119098 lmp.py:1935]   Expert 39 |     58 | CPU
DEBUG 01-15 16:10:36.119794.119794 lmp.py:1935]   Expert  7 |     72 | CPU
DEBUG 01-15 16:10:36.119014.119014 lmp.py:1935]   Expert 30 |     73 | CPU
DEBUG 01-15 16:10:36.119994.119994 lmp.py:1935]   Expert 14 |     94 | CPU
DEBUG 01-15 16:10:36.119406.119406 lmp.py:1935]   Expert 24 |     94 | CPU
DEBUG 01-15 16:10:36.119817.119817 lmp.py:1935]   Expert 27 |     94 | CPU
DEBUG 01-15 16:10:36.119752.119752 lmp.py:1935]   Expert 40 |     95 | CPU
DEBUG 01-15 16:10:36.119494.119494 lmp.py:1935]   Expert 36 |     98 | CPU
DEBUG 01-15 16:10:36.119998.119998 lmp.py:1935]   Expert 17 |    102 | CPU
DEBUG 01-15 16:10:36.119264.119264 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:10:36.119529.119529 lmp.py:1935]   Expert 32 |    106 | CPU
DEBUG 01-15 16:10:36.119795.119795 lmp.py:1935]   Expert 48 |    110 | CPU
DEBUG 01-15 16:10:36.119822.119822 lmp.py:1935]   Expert 18 |    112 | CPU
DEBUG 01-15 16:10:36.119849.119849 lmp.py:1935]   Expert 12 |    114 | CPU
DEBUG 01-15 16:10:36.119591.119591 lmp.py:1935]   Expert  1 |    115 | CPU
DEBUG 01-15 16:10:36.119049.119049 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:10:36.119222.119222 lmp.py:1935]   Expert 59 |    129 | CPU
DEBUG 01-15 16:10:36.119918.119918 lmp.py:1935]   Expert 42 |    135 | CPU
DEBUG 01-15 16:10:36.119661.119661 lmp.py:1935]   Expert  0 |    141 | CPU
DEBUG 01-15 16:10:36.119926.119926 lmp.py:1935]   Expert 53 |    144 | CPU
DEBUG 01-15 16:10:36.119192.119192 lmp.py:1935]   Expert 22 |    146 | CPU
DEBUG 01-15 16:10:36.119457.119457 lmp.py:1935]   Expert 51 |    152 | CPU
DEBUG 01-15 16:10:36.119915.119915 lmp.py:1935]   Expert  8 |    162 | CPU
DEBUG 01-15 16:10:36.119035.119035 lmp.py:1935]   Expert 15 |    166 | CPU
DEBUG 01-15 16:10:36.119724.119724 lmp.py:1935]   Expert 44 |    168 | CPU
DEBUG 01-15 16:10:36.119605.119605 lmp.py:1935]   Expert 60 |    168 | CPU
DEBUG 01-15 16:10:36.119248.119248 lmp.py:1935]   Expert 29 |    170 | CPU
DEBUG 01-15 16:10:36.119891.119891 lmp.py:1935]   Expert 54 |    176 | CPU
DEBUG 01-15 16:10:36.119581.119581 lmp.py:1935]   Expert 35 |    177 | CPU
DEBUG 01-15 16:10:36.119032.119032 lmp.py:1935]   Expert 34 |    180 | CPU
DEBUG 01-15 16:10:36.119244.119244 lmp.py:1935]   Expert 33 |    184 | CPU
DEBUG 01-15 16:10:36.119841.119841 lmp.py:1935]   Expert 47 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.119961.119961 lmp.py:1935]   Expert  9 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.119080.119080 lmp.py:1935]   Expert 19 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.119723.119723 lmp.py:1935]   Expert 56 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.119366.119366 lmp.py:1935]   Expert  3 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.119771.119771 lmp.py:1935]   Expert 21 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.119175.119175 lmp.py:1935]   Expert 46 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.119818.119818 lmp.py:1935]   Expert 45 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120700.120700 lmp.py:1935]   Expert 49 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120058.120058 lmp.py:1935]   Expert 20 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120939.120939 lmp.py:1935]   Expert 28 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120582.120582 lmp.py:1935]   Expert 57 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120987.120987 lmp.py:1935]   Expert  2 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120153.120153 lmp.py:1935]   Expert 13 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120319.120319 lmp.py:1935]   Expert 43 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120485.120485 lmp.py:1935]   Expert  4 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120413.120413 lmp.py:1935]   Expert 10 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120341.120341 lmp.py:1935]   Expert 41 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120507.120507 lmp.py:1935]   Expert 50 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120196.120196 lmp.py:1935]   Expert 26 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120362.120362 lmp.py:1935]   Expert 63 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120528.120528 lmp.py:1935]   Expert 37 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120887.120887 lmp.py:1935]   Expert 31 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120675.120675 lmp.py:1935]   Expert 61 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120226.120226 lmp.py:1935]   Expert 52 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120015.120015 lmp.py:1935]   Expert 58 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120611.120611 lmp.py:1935]   Expert 62 |    323 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120493.120493 lmp.py:1935]   Expert 55 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120612.120612 lmp.py:1935]   Expert 11 |    380 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120732.120732 lmp.py:1935]   Expert 23 |    382 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120826.120826 lmp.py:1935]   Expert 25 |    408 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.120900.120900 lmp.py:1935]   Expert  5 |    514 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.120542.120542 lmp.py:1937] 
DEBUG 01-15 16:10:36.120542.120542 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.120662.120662 lmp.py:1938]   CPU:   3978 tokens
DEBUG 01-15 16:10:36.120259.120259 lmp.py:1942]   cuda:1:   4171 tokens (16 experts)
DEBUG 01-15 16:10:36.120379.120379 lmp.py:1942]   cuda:2:   4139 tokens (16 experts)
DEBUG 01-15 16:10:36.120836.120836 lmp.py:1943]   Total GPU:   8310 tokens
DEBUG 01-15 16:10:36.120526.120526 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.120526.120526 lmp.py:1944] 
DEBUG 01-15 16:10:36.120414.120414 cuda_h.py:19] end experts_map_get cost 0.0019023418426513672 seconds
DEBUG 01-15 16:10:36.120999.120999 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.120325.120325 lmp.py:1953] 
DEBUG 01-15 16:10:36.120325.120325 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.120904.120904 cuda_h.py:19] end cpu_experts_submit cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:10:36.120243.120243 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.120549.120549 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.123630.123630 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.63.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.63.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.63.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.8.mlp.experts.26.gate_proj.weight': 0, 'model.layers.8.mlp.experts.26.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.26.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.49.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.49.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.49.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.57.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (10663493632, 5767168, 17301504, 0), (10669260800, 5767168, 23068672, 0), (10657726464, 5767168, 28835840, 0), (10698096640, 5767168, 34603008, 0), (10703863808, 5767168, 40370176, 0), (10692329472, 5767168, 46137344, 0), (10767302656, 5767168, 51904512, 0), (10773069824, 5767168, 57671680, 0), (10761535488, 5767168, 63438848, 0), (10784604160, 5767168, 69206016, 0), (10790371328, 5767168, 74973184, 0), (10778836992, 5767168, 80740352, 0), (10801905664, 5767168, 86507520, 0), (10807672832, 5767168, 92274688, 0), (10796138496, 5767168, 98041856, 0), (11355553792, 5767168, 103809024, 0), (11361320960, 5767168, 109576192, 0), (11349786624, 5767168, 115343360, 0), (11390156800, 5767168, 121110528, 0), (11395923968, 5767168, 126877696, 0), (11384389632, 5767168, 132644864, 0), (11424759808, 5767168, 138412032, 0), (11430526976, 5767168, 144179200, 0), (11418992640, 5767168, 149946368, 0), (11476664320, 5767168, 155713536, 0), (11482431488, 5767168, 161480704, 0), (11470897152, 5767168, 167247872, 0), (11511267328, 5767168, 173015040, 0), (11517034496, 5767168, 178782208, 0), (11505500160, 5767168, 184549376, 0), (10974920704, 5767168, 190316544, 0), (10980687872, 5767168, 196083712, 0), (10969153536, 5767168, 201850880, 0), (11096031232, 5767168, 207618048, 0), (11101798400, 5767168, 213385216, 0), (11090264064, 5767168, 219152384, 0), (11701583872, 5767168, 224919552, 0), (11707351040, 5767168, 230686720, 0), (11695816704, 5767168, 236453888, 0), (11684282368, 5767168, 242221056, 0), (11690049536, 5767168, 247988224, 0), (11678515200, 5767168, 253755392, 0), (11147935744, 5767168, 259522560, 0), (11153702912, 5767168, 265289728, 0), (11142168576, 5767168, 271056896, 0)], 2: [(11061428224, 5767168, 0, 0), (11067195392, 5767168, 5767168, 0), (11055661056, 5767168, 11534336, 0), (10680795136, 5767168, 17301504, 0), (10686562304, 5767168, 23068672, 0), (10675027968, 5767168, 28835840, 0), (11251744768, 5767168, 34603008, 0), (11257511936, 5767168, 40370176, 0), (11245977600, 5767168, 46137344, 0), (11320950784, 5767168, 51904512, 0), (11326717952, 5767168, 57671680, 0), (11315183616, 5767168, 63438848, 0), (10836508672, 5767168, 69206016, 0), (10842275840, 5767168, 74973184, 0), (10830741504, 5767168, 80740352, 0), (11407458304, 5767168, 86507520, 0), (11413225472, 5767168, 92274688, 0), (11401691136, 5767168, 98041856, 0), (11459362816, 5767168, 103809024, 0), (11465129984, 5767168, 109576192, 0), (11453595648, 5767168, 115343360, 0), (10940317696, 5767168, 121110528, 0), (10946084864, 5767168, 126877696, 0), (10934550528, 5767168, 132644864, 0), (11009523712, 5767168, 138412032, 0), (11015290880, 5767168, 144179200, 0), (11003756544, 5767168, 149946368, 0), (10957619200, 5767168, 155713536, 0), (10963386368, 5767168, 161480704, 0), (10951852032, 5767168, 167247872, 0), (11563171840, 5767168, 173015040, 0), (11568939008, 5767168, 178782208, 0), (11557404672, 5767168, 184549376, 0), (11580473344, 5767168, 190316544, 0), (11586240512, 5767168, 196083712, 0), (11574706176, 5767168, 201850880, 0), (11044126720, 5767168, 207618048, 0), (11049893888, 5767168, 213385216, 0), (11038359552, 5767168, 219152384, 0), (11615076352, 5767168, 224919552, 0), (11620843520, 5767168, 230686720, 0), (11609309184, 5767168, 236453888, 0), (11666980864, 5767168, 242221056, 0), (11672748032, 5767168, 247988224, 0), (11661213696, 5767168, 253755392, 0), (11597774848, 5767168, 259522560, 0), (11603542016, 5767168, 265289728, 0), (11592007680, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b06465e0>, 2: <capsule object NULL at 0x7a4e5420a160>}
DEBUG 01-15 16:10:36.123793.123793 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.123476.123476 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.123464.123464 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 83af0fe9-659d-45d8-a974-bb6b51f28061
DEBUG 01-15 16:10:36.123854.123854 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:36.124231.124231 client.py:127] Model loaded
DEBUG 01-15 16:10:36.124299.124299 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.124231.124231 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.124232.124232 cuda_h.py:19] end restore2model cost 0.0003399848937988281 seconds
DEBUG 01-15 16:10:36.124048.124048 cuda_h.py:19] end sllm_worker_task cost 0.01114511489868164 seconds
DEBUG 01-15 16:10:36.124003.124003 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.125788.125788 cuda_h.py:19] end move_flatidxs cost 0.0008320808410644531 seconds
DEBUG 01-15 16:10:36.125870.125870 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:36.125998.125998 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 83af0fe9-659d-45d8-a974-bb6b51f28061
DEBUG 01-15 16:10:36.126324.126324 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005527973175048828 seconds
DEBUG 01-15 16:10:36.126692.126692 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.129430.129430 cuda_h.py:19] end restore2model cost 0.0026092529296875 seconds
DEBUG 01-15 16:10:36.129101.129101 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008405923843383789 seconds
DEBUG 01-15 16:10:36.129135.129135 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.129920.129920 cuda_h.py:19] end gpu_sexperts cost 0.0002655982971191406 seconds
DEBUG 01-15 16:10:36.129173.129173 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.129565.129565 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:10:36.129738.129738 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.129487.129487 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.130755.130755 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008339881896972656 seconds
DEBUG 01-15 16:10:36.130042.130042 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.130658.130658 cuda_h.py:19] end gpu_group_list cost 0.000179290771484375 seconds
DEBUG 01-15 16:10:36.131371.131371 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.132629.132629 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008993148803710938 seconds
DEBUG 01-15 16:10:36.132737.132737 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.132354.132354 cuda_h.py:19] end gpu_group_list cost 0.0001780986785888672 seconds
DEBUG 01-15 16:10:36.133042.133042 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.133302.133302 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 83af0fe9-659d-45d8-a974-bb6b51f28061
DEBUG 01-15 16:10:36.135425.135425 cuda_h.py:19] end group_tensors cost 0.009814023971557617 seconds
DEBUG 01-15 16:10:36.136180.136180 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.139201.139201 cuda_h.py:19] end group pad cost 0.0035355091094970703 seconds
DEBUG 01-15 16:10:36.139707.139707 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.152101.152101 client.py:127] Model loaded
DEBUG 01-15 16:10:36.153006.153006 cuda_h.py:19] end wait_experts_multi_device cost 0.019640207290649414 seconds
DEBUG 01-15 16:10:36.153531.153531 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.160334.160334 cuda_h.py:19] end group_einsum cost 0.020813941955566406 seconds
DEBUG 01-15 16:10:36.161312.161312 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.164269.164269 cuda_h.py:19] end get_outputs_cpu1 cost 0.003585338592529297 seconds
DEBUG 01-15 16:10:36.165164.165164 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0408482551574707 seconds
DEBUG 01-15 16:10:36.165789.165789 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012599945068359375 seconds
DEBUG 01-15 16:10:36.165686.165686 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.167670.167670 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.167887.167887 cuda_h.py:19] end index_scatter cost 8.153915405273438e-05 seconds
DEBUG 01-15 16:10:36.167288.167288 cuda_h.py:19] end cpuoutputsdeal cost 0.0017001628875732422 seconds
DEBUG 01-15 16:10:36.167535.167535 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.167152.167152 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.167105.167105 cuda_h.py:19] end gpu_group_tensor cost 0.0001456737518310547 seconds
DEBUG 01-15 16:10:36.167961.167961 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.168463.168463 cuda_h.py:19] end gpu_group_tensor cost 0.00013113021850585938 seconds
DEBUG 01-15 16:10:36.168513.168513 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.168240.168240 cuda_h.py:19] end gpu_group_einsum cost 0.00069427490234375 seconds
DEBUG 01-15 16:10:36.169059.169059 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.169878.169878 cuda_h.py:19] end gpu_group_einsum cost 0.0004725456237792969 seconds
DEBUG 01-15 16:10:36.169201.169201 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.169317.169317 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.170507.170507 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002675056457519531 seconds
DEBUG 01-15 16:10:36.170654.170654 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.170651.170651 cuda_h.py:19] end concat_expert_out cost 6.270408630371094e-05 seconds
DEBUG 01-15 16:10:36.170898.170898 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.170246.170246 cuda_h.py:19] end index_scatter cost 7.462501525878906e-05 seconds
DEBUG 01-15 16:10:36.170242.170242 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000990152359008789 seconds
DEBUG 01-15 16:10:36.170882.170882 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.171077.171077 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.171344.171344 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022673606872558594 seconds
DEBUG 01-15 16:10:36.171107.171107 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.171634.171634 cuda_h.py:19] end concat_expert_out cost 6.794929504394531e-05 seconds
DEBUG 01-15 16:10:36.171212.171212 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.171461.171461 cuda_h.py:19] end index_scatter cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:10:36.171330.171330 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006697177886962891 seconds
DEBUG 01-15 16:10:36.171392.171392 cuda_h.py:19] end gpu_experts_multi_device cost 0.04206585884094238 seconds
DEBUG 01-15 16:10:36.171667.171667 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.053975820541381836 seconds
DEBUG 01-15 16:10:36.172819.172819 cuda_h.py:19] end prefill_layer cost 0.05932331085205078 seconds
DEBUG 01-15 16:10:36.172352.172352 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:10:36.172300.172300 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.172155.172155 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:10:36.172276.172276 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:36.172139.172139 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:36.172909.172909 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 4.100799560546875e-05 seconds
DEBUG 01-15 16:10:36.172341.172341 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 8.177757263183594e-05 seconds
DEBUG 01-15 16:10:36.172951.172951 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.172715.172715 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.172752.172752 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.172826.172826 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.173246.173246 cuda_h.py:19] end allocate_cuda_memory cost 0.00023555755615234375 seconds
DEBUG 01-15 16:10:36.173288.173288 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.173766.173766 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.173596.173596 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.173828.173828 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f341c997-4f2b-47c8-838c-4349050c8336
DEBUG 01-15 16:10:36.173070.173070 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.173248.173248 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.173918.173918 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.175595.175595 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f341c997-4f2b-47c8-838c-4349050c8336
DEBUG 01-15 16:10:36.175961.175961 cuda_h.py:19] end load_into_gpu_async cost 0.002074718475341797 seconds
DEBUG 01-15 16:10:36.175525.175525 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.175635.175635 cuda_h.py:19] end restore_tensors2 cost 8.416175842285156e-05 seconds
DEBUG 01-15 16:10:36.175683.175683 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0026559829711914062 seconds
INFO 01-15 16:10:36.175924.175924 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f341c997-4f2b-47c8-838c-4349050c8336
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.178208.178208 cuda_h.py:19] end self_attn cost 0.004041433334350586 seconds
DEBUG 01-15 16:10:36.178991.178991 cuda_h.py:19] end iln_self_attn_paln cost 0.005889892578125 seconds
DEBUG 01-15 16:10:36.178774.178774 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:10:36.178775.178775 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.179193.179193 cuda_h.py:19] end gate cost 0.0007619857788085938 seconds
DEBUG 01-15 16:10:36.179598.179598 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.179396.179396 lmp.py:1912] 
DEBUG 01-15 16:10:36.179396.179396 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.180358.180358 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.180107.180107 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.180565.180565 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.180638.180638 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.180996.180996 lmp.py:1917] 
DEBUG 01-15 16:10:36.180996.180996 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.180308.180308 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.180627.180627 lmp.py:1935]   Expert 24 |     39 | CPU
DEBUG 01-15 16:10:36.180462.180462 lmp.py:1935]   Expert  2 |     45 | CPU
DEBUG 01-15 16:10:36.180820.180820 lmp.py:1935]   Expert 26 |     60 | CPU
DEBUG 01-15 16:10:36.180179.180179 lmp.py:1935]   Expert 32 |     66 | CPU
DEBUG 01-15 16:10:36.180060.180060 lmp.py:1935]   Expert 19 |     68 | CPU
DEBUG 01-15 16:10:36.180418.180418 lmp.py:1935]   Expert 50 |     70 | CPU
DEBUG 01-15 16:10:36.180538.180538 lmp.py:1935]   Expert  4 |     79 | CPU
DEBUG 01-15 16:10:36.180658.180658 lmp.py:1935]   Expert 15 |     79 | CPU
DEBUG 01-15 16:10:36.180539.180539 lmp.py:1935]   Expert 28 |     82 | CPU
DEBUG 01-15 16:10:36.180659.180659 lmp.py:1935]   Expert 60 |     82 | CPU
DEBUG 01-15 16:10:36.180779.180779 lmp.py:1935]   Expert  7 |     83 | CPU
DEBUG 01-15 16:10:36.180899.180899 lmp.py:1935]   Expert 59 |     89 | CPU
DEBUG 01-15 16:10:36.180257.180257 lmp.py:1935]   Expert 49 |     97 | CPU
DEBUG 01-15 16:10:36.180569.180569 lmp.py:1935]   Expert  5 |    102 | CPU
DEBUG 01-15 16:10:36.180563.180563 lmp.py:1935]   Expert 23 |    102 | CPU
DEBUG 01-15 16:10:36.180147.180147 lmp.py:1935]   Expert 12 |    105 | CPU
DEBUG 01-15 16:10:36.180128.180128 lmp.py:1935]   Expert 10 |    110 | CPU
DEBUG 01-15 16:10:36.180725.180725 lmp.py:1935]   Expert 27 |    112 | CPU
DEBUG 01-15 16:10:36.180083.180083 lmp.py:1935]   Expert 41 |    122 | CPU
DEBUG 01-15 16:10:36.180441.180441 lmp.py:1935]   Expert  3 |    124 | CPU
DEBUG 01-15 16:10:36.180038.180038 lmp.py:1935]   Expert 25 |    127 | CPU
DEBUG 01-15 16:10:36.180396.180396 lmp.py:1935]   Expert 20 |    129 | CPU
DEBUG 01-15 16:10:36.180993.180993 lmp.py:1935]   Expert 40 |    130 | CPU
DEBUG 01-15 16:10:36.180590.180590 lmp.py:1935]   Expert 13 |    132 | CPU
DEBUG 01-15 16:10:36.180901.180901 lmp.py:1935]   Expert 16 |    133 | CPU
DEBUG 01-15 16:10:36.180213.180213 lmp.py:1935]   Expert 37 |    144 | CPU
DEBUG 01-15 16:10:36.180048.180048 lmp.py:1935]   Expert 35 |    147 | CPU
DEBUG 01-15 16:10:36.180122.180122 lmp.py:1935]   Expert 17 |    149 | CPU
DEBUG 01-15 16:10:36.180719.180719 lmp.py:1935]   Expert 47 |    152 | CPU
DEBUG 01-15 16:10:36.180077.180077 lmp.py:1935]   Expert 22 |    159 | CPU
DEBUG 01-15 16:10:36.180435.180435 lmp.py:1935]   Expert 53 |    169 | CPU
DEBUG 01-15 16:10:36.180793.180793 lmp.py:1935]   Expert 39 |    170 | CPU
DEBUG 01-15 16:10:36.180820.180820 lmp.py:1935]   Expert 38 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.180609.180609 lmp.py:1935]   Expert 44 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.180921.180921 lmp.py:1935]   Expert 36 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.180710.180710 lmp.py:1935]   Expert 52 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.180499.180499 lmp.py:1935]   Expert 58 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.180764.180764 lmp.py:1935]   Expert 18 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.180507.180507 lmp.py:1935]   Expert 62 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.180772.180772 lmp.py:1935]   Expert 48 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.180561.180561 lmp.py:1935]   Expert 11 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.180111.180111 lmp.py:1935]   Expert 14 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.180662.180662 lmp.py:1935]   Expert 30 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.180735.180735 lmp.py:1935]   Expert  1 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181285.181285 lmp.py:1935]   Expert 31 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181836.181836 lmp.py:1935]   Expert 42 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181863.181863 lmp.py:1935]   Expert 45 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181128.181128 lmp.py:1935]   Expert 51 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181917.181917 lmp.py:1935]   Expert  6 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181183.181183 lmp.py:1935]   Expert 29 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181733.181733 lmp.py:1935]   Expert 34 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181045.181045 lmp.py:1935]   Expert 33 |    275 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181595.181595 lmp.py:1935]   Expert 57 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181907.181907 lmp.py:1935]   Expert 61 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181219.181219 lmp.py:1935]   Expert 43 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181769.181769 lmp.py:1935]   Expert  0 |    320 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181081.181081 lmp.py:1935]   Expert 46 |    348 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181870.181870 lmp.py:1935]   Expert  8 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181182.181182 lmp.py:1935]   Expert  9 |    391 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181209.181209 lmp.py:1935]   Expert 56 |    392 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181998.181998 lmp.py:1935]   Expert 54 |    395 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181025.181025 lmp.py:1935]   Expert 63 |    407 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181291.181291 lmp.py:1935]   Expert 55 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.181079.181079 lmp.py:1935]   Expert 21 |    492 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.181199.181199 lmp.py:1937] 
DEBUG 01-15 16:10:36.181199.181199 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.181511.181511 lmp.py:1938]   CPU:   3457 tokens
DEBUG 01-15 16:10:36.181585.181585 lmp.py:1942]   cuda:1:   4416 tokens (16 experts)
DEBUG 01-15 16:10:36.181896.181896 lmp.py:1942]   cuda:2:   4415 tokens (16 experts)
DEBUG 01-15 16:10:36.181255.181255 lmp.py:1943]   Total GPU:   8831 tokens
DEBUG 01-15 16:10:36.181374.181374 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.181374.181374 lmp.py:1944] 
DEBUG 01-15 16:10:36.181124.181124 cuda_h.py:19] end experts_map_get cost 0.0020067691802978516 seconds
DEBUG 01-15 16:10:36.181272.181272 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.181458.181458 lmp.py:1953] 
DEBUG 01-15 16:10:36.181458.181458 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.181739.181739 cuda_h.py:19] end cpu_experts_submit cost 6.580352783203125e-05 seconds
DEBUG 01-15 16:10:36.181170.181170 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.182837.182837 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.182312.182312 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.182404.182404 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.33.gate_proj.weight': 0, 'model.layers.9.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.38.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.38.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.38.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.11.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.11.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.11.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.58.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.58.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.58.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.42.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.42.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.42.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.44.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.44.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.44.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.52.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.52.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.52.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.62.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.62.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.62.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(12289835008, 5767168, 0, 0), (12295602176, 5767168, 5767168, 0), (12284067840, 5767168, 11534336, 0), (11736186880, 5767168, 17301504, 0), (11741954048, 5767168, 23068672, 0), (11730419712, 5767168, 28835840, 0), (12341739520, 5767168, 34603008, 0), (12347506688, 5767168, 40370176, 0), (12335972352, 5767168, 46137344, 0), (11822694400, 5767168, 51904512, 0), (11828461568, 5767168, 57671680, 0), (11816927232, 5767168, 63438848, 0), (12376342528, 5767168, 69206016, 0), (12382109696, 5767168, 74973184, 0), (12370575360, 5767168, 80740352, 0), (11874598912, 5767168, 86507520, 0), (11880366080, 5767168, 92274688, 0), (11868831744, 5767168, 98041856, 0), (12462850048, 5767168, 103809024, 0), (12468617216, 5767168, 109576192, 0), (12457082880, 5767168, 115343360, 0), (11909201920, 5767168, 121110528, 0), (11914969088, 5767168, 126877696, 0), (11903434752, 5767168, 132644864, 0), (12514754560, 5767168, 138412032, 0), (12520521728, 5767168, 144179200, 0), (12508987392, 5767168, 149946368, 0), (12549357568, 5767168, 155713536, 0), (12555124736, 5767168, 161480704, 0), (12543590400, 5767168, 167247872, 0), (12601262080, 5767168, 173015040, 0), (12607029248, 5767168, 178782208, 0), (12595494912, 5767168, 184549376, 0), (12082216960, 5767168, 190316544, 0), (12087984128, 5767168, 196083712, 0), (12076449792, 5767168, 201850880, 0), (12653166592, 5767168, 207618048, 0), (12658933760, 5767168, 213385216, 0), (12647399424, 5767168, 219152384, 0), (12705071104, 5767168, 224919552, 0), (12710838272, 5767168, 230686720, 0), (12699303936, 5767168, 236453888, 0), (12722372608, 5767168, 242221056, 0), (12728139776, 5767168, 247988224, 0), (12716605440, 5767168, 253755392, 0), (12255232000, 5767168, 259522560, 0), (12260999168, 5767168, 265289728, 0), (12249464832, 5767168, 271056896, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (12307136512, 5767168, 17301504, 0), (12312903680, 5767168, 23068672, 0), (12301369344, 5767168, 28835840, 0), (11857297408, 5767168, 34603008, 0), (11863064576, 5767168, 40370176, 0), (11851530240, 5767168, 46137344, 0), (12445548544, 5767168, 51904512, 0), (12451315712, 5767168, 57671680, 0), (12439781376, 5767168, 63438848, 0), (12480151552, 5767168, 69206016, 0), (12485918720, 5767168, 74973184, 0), (12474384384, 5767168, 80740352, 0), (12497453056, 5767168, 86507520, 0), (12503220224, 5767168, 92274688, 0), (12491685888, 5767168, 98041856, 0), (11961106432, 5767168, 103809024, 0), (11966873600, 5767168, 109576192, 0), (11955339264, 5767168, 115343360, 0), (12220628992, 5767168, 121110528, 0), (12226396160, 5767168, 126877696, 0), (12214861824, 5767168, 132644864, 0), (12030312448, 5767168, 138412032, 0), (12036079616, 5767168, 144179200, 0), (12024545280, 5767168, 149946368, 0), (12618563584, 5767168, 155713536, 0), (12624330752, 5767168, 161480704, 0), (12612796416, 5767168, 167247872, 0), (12670468096, 5767168, 173015040, 0), (12676235264, 5767168, 178782208, 0), (12664700928, 5767168, 184549376, 0), (12687769600, 5767168, 190316544, 0), (12693536768, 5767168, 196083712, 0), (12682002432, 5767168, 201850880, 0), (12791578624, 5767168, 207618048, 0), (12797345792, 5767168, 213385216, 0), (12785811456, 5767168, 219152384, 0), (12774277120, 5767168, 224919552, 0), (12780044288, 5767168, 230686720, 0), (12768509952, 5767168, 236453888, 0), (12237930496, 5767168, 242221056, 0), (12243697664, 5767168, 247988224, 0), (12232163328, 5767168, 253755392, 0), (12808880128, 5767168, 259522560, 0), (12814647296, 5767168, 265289728, 0), (12803112960, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e5420a2b0>, 2: <capsule object NULL at 0x7a4e5420a190>}
DEBUG 01-15 16:10:36.182819.182819 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.182556.182556 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.182497.182497 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1048f4b4-4e4e-44e4-89db-164c2a6aacc9
DEBUG 01-15 16:10:36.183033.183033 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.183154.183154 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:36.183942.183942 client.py:127] Model loaded
DEBUG 01-15 16:10:36.183056.183056 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.184220.184220 cuda_h.py:19] end restore2model cost 0.00033593177795410156 seconds
DEBUG 01-15 16:10:36.184513.184513 cuda_h.py:19] end sllm_worker_task cost 0.011281490325927734 seconds
DEBUG 01-15 16:10:36.184535.184535 cuda_h.py:19] end move_flatidxs cost 0.0009717941284179688 seconds
DEBUG 01-15 16:10:36.184318.184318 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:36.185415.185415 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1048f4b4-4e4e-44e4-89db-164c2a6aacc9
DEBUG 01-15 16:10:36.185414.185414 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0035452842712402344 seconds
DEBUG 01-15 16:10:36.185013.185013 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.188208.188208 cuda_h.py:19] end restore2model cost 0.003019094467163086 seconds
DEBUG 01-15 16:10:36.188912.188912 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006827354431152344 seconds
DEBUG 01-15 16:10:36.188092.188092 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.189997.189997 cuda_h.py:19] end gpu_sexperts cost 0.0003178119659423828 seconds
DEBUG 01-15 16:10:36.189304.189304 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.189226.189226 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.811981201171875e-05 seconds
DEBUG 01-15 16:10:36.189829.189829 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.189916.189916 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.190060.190060 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010907649993896484 seconds
DEBUG 01-15 16:10:36.190440.190440 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.190864.190864 cuda_h.py:19] end gpu_group_list cost 0.00017762184143066406 seconds
DEBUG 01-15 16:10:36.191406.191406 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.193184.193184 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012028217315673828 seconds
DEBUG 01-15 16:10:36.193008.193008 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.193346.193346 cuda_h.py:19] end gpu_group_list cost 0.0001823902130126953 seconds
DEBUG 01-15 16:10:36.194340.194340 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.194560.194560 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1048f4b4-4e4e-44e4-89db-164c2a6aacc9
DEBUG 01-15 16:10:36.198375.198375 cuda_h.py:19] end group_tensors cost 0.013495206832885742 seconds
DEBUG 01-15 16:10:36.199710.199710 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.202844.202844 cuda_h.py:19] end group pad cost 0.0033698081970214844 seconds
DEBUG 01-15 16:10:36.202410.202410 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.211047.211047 client.py:127] Model loaded
DEBUG 01-15 16:10:36.211735.211735 cuda_h.py:19] end wait_experts_multi_device cost 0.01706242561340332 seconds
DEBUG 01-15 16:10:36.211823.211823 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.220081.220081 cuda_h.py:19] end group_einsum cost 0.01738905906677246 seconds
DEBUG 01-15 16:10:36.220796.220796 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.223961.223961 cuda_h.py:19] end get_outputs_cpu1 cost 0.0032846927642822266 seconds
DEBUG 01-15 16:10:36.224558.224558 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.041294097900390625 seconds
DEBUG 01-15 16:10:36.227805.227805 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.016254186630249023 seconds
DEBUG 01-15 16:10:36.228061.228061 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.231894.231894 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.231613.231613 cuda_h.py:19] end index_scatter cost 0.0001742839813232422 seconds
DEBUG 01-15 16:10:36.231861.231861 cuda_h.py:19] end cpuoutputsdeal cost 0.003623485565185547 seconds
DEBUG 01-15 16:10:36.232304.232304 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.232644.232644 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.232829.232829 cuda_h.py:19] end gpu_group_tensor cost 0.0003132820129394531 seconds
DEBUG 01-15 16:10:36.232600.232600 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.232558.232558 cuda_h.py:19] end gpu_group_tensor cost 0.0002551078796386719 seconds
DEBUG 01-15 16:10:36.233625.233625 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.234448.234448 cuda_h.py:19] end gpu_group_einsum cost 0.0013382434844970703 seconds
DEBUG 01-15 16:10:36.234099.234099 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.235213.235213 cuda_h.py:19] end gpu_group_einsum cost 0.0007576942443847656 seconds
DEBUG 01-15 16:10:36.235353.235353 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.236208.236208 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.236704.236704 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004916191101074219 seconds
DEBUG 01-15 16:10:36.236805.236805 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.236659.236659 cuda_h.py:19] end concat_expert_out cost 0.00010752677917480469 seconds
DEBUG 01-15 16:10:36.236412.236412 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.237531.237531 cuda_h.py:19] end index_scatter cost 0.0001201629638671875 seconds
DEBUG 01-15 16:10:36.237022.237022 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001697540283203125 seconds
DEBUG 01-15 16:10:36.237088.237088 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.237529.237529 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.238571.238571 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003848075866699219 seconds
DEBUG 01-15 16:10:36.238420.238420 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.238989.238989 cuda_h.py:19] end concat_expert_out cost 0.00010800361633300781 seconds
DEBUG 01-15 16:10:36.238397.238397 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.238343.238343 cuda_h.py:19] end index_scatter cost 0.00011134147644042969 seconds
DEBUG 01-15 16:10:36.238530.238530 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010623931884765625 seconds
DEBUG 01-15 16:10:36.239426.239426 cuda_h.py:19] end gpu_experts_multi_device cost 0.049643516540527344 seconds
DEBUG 01-15 16:10:36.239766.239766 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.06046175956726074 seconds
DEBUG 01-15 16:10:36.239268.239268 cuda_h.py:19] end prefill_layer cost 0.06703925132751465 seconds
DEBUG 01-15 16:10:36.239171.239171 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:10:36.239728.239728 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.239285.239285 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:10:36.239127.239127 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:36.239929.239929 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:36.239707.239707 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:10:36.239032.239032 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 8.797645568847656e-05 seconds
DEBUG 01-15 16:10:36.239106.239106 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.239837.239837 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.239661.239661 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.239636.239636 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.240042.240042 cuda_h.py:19] end allocate_cuda_memory cost 0.0002300739288330078 seconds
DEBUG 01-15 16:10:36.240291.240291 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.240644.240644 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.240589.240589 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.240299.240299 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.240910.240910 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f59bbd0d-6e3e-4ce8-9163-87d4ef12a280
DEBUG 01-15 16:10:36.240847.240847 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.241534.241534 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.242173.242173 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f59bbd0d-6e3e-4ce8-9163-87d4ef12a280
DEBUG 01-15 16:10:36.242347.242347 cuda_h.py:19] end load_into_gpu_async cost 0.001897573471069336 seconds
DEBUG 01-15 16:10:36.242508.242508 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.242790.242790 cuda_h.py:19] end restore_tensors2 cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:10:36.242307.242307 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002656698226928711 seconds
INFO 01-15 16:10:36.242495.242495 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f59bbd0d-6e3e-4ce8-9163-87d4ef12a280
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.245187.245187 cuda_h.py:19] end self_attn cost 0.004199981689453125 seconds
DEBUG 01-15 16:10:36.245303.245303 cuda_h.py:19] end iln_self_attn_paln cost 0.00587010383605957 seconds
DEBUG 01-15 16:10:36.245563.245563 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:10:36.245988.245988 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.246800.246800 cuda_h.py:19] end gate cost 0.0006680488586425781 seconds
DEBUG 01-15 16:10:36.246682.246682 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.246018.246018 lmp.py:1912] 
DEBUG 01-15 16:10:36.246018.246018 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.246204.246204 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.246569.246569 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.246073.246073 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.246193.246193 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.246598.246598 lmp.py:1917] 
DEBUG 01-15 16:10:36.246598.246598 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.246717.246717 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.246467.246467 lmp.py:1935]   Expert 43 |     17 | CPU
DEBUG 01-15 16:10:36.246871.246871 lmp.py:1935]   Expert 27 |     31 | CPU
DEBUG 01-15 16:10:36.246845.246845 lmp.py:1935]   Expert 26 |     52 | CPU
DEBUG 01-15 16:10:36.246058.246058 lmp.py:1935]   Expert 34 |     54 | CPU
DEBUG 01-15 16:10:36.247747.247747 lmp.py:1935]   Expert 56 |     55 | CPU
DEBUG 01-15 16:10:36.247105.247105 lmp.py:1935]   Expert  3 |     57 | CPU
DEBUG 01-15 16:10:36.247510.247510 lmp.py:1935]   Expert  4 |     65 | CPU
DEBUG 01-15 16:10:36.247676.247676 lmp.py:1935]   Expert 61 |     80 | CPU
DEBUG 01-15 16:10:36.247604.247604 lmp.py:1935]   Expert 14 |     95 | CPU
DEBUG 01-15 16:10:36.247770.247770 lmp.py:1935]   Expert 38 |    100 | CPU
DEBUG 01-15 16:10:36.247697.247697 lmp.py:1935]   Expert  2 |    112 | CPU
DEBUG 01-15 16:10:36.247387.247387 lmp.py:1935]   Expert 17 |    121 | CPU
DEBUG 01-15 16:10:36.247314.247314 lmp.py:1935]   Expert 22 |    121 | CPU
DEBUG 01-15 16:10:36.247242.247242 lmp.py:1935]   Expert 47 |    128 | CPU
DEBUG 01-15 16:10:36.247170.247170 lmp.py:1935]   Expert 55 |    131 | CPU
DEBUG 01-15 16:10:36.247528.247528 lmp.py:1935]   Expert 37 |    133 | CPU
DEBUG 01-15 16:10:36.247648.247648 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:10:36.247291.247291 lmp.py:1935]   Expert 54 |    136 | CPU
DEBUG 01-15 16:10:36.247934.247934 lmp.py:1935]   Expert  7 |    144 | CPU
DEBUG 01-15 16:10:36.247338.247338 lmp.py:1935]   Expert 15 |    145 | CPU
DEBUG 01-15 16:10:36.247505.247505 lmp.py:1935]   Expert 48 |    145 | CPU
DEBUG 01-15 16:10:36.247432.247432 lmp.py:1935]   Expert  5 |    148 | CPU
DEBUG 01-15 16:10:36.247122.247122 lmp.py:1935]   Expert 51 |    148 | CPU
DEBUG 01-15 16:10:36.247049.247049 lmp.py:1935]   Expert 45 |    149 | CPU
DEBUG 01-15 16:10:36.247977.247977 lmp.py:1935]   Expert 60 |    149 | CPU
DEBUG 01-15 16:10:36.247905.247905 lmp.py:1935]   Expert 12 |    154 | CPU
DEBUG 01-15 16:10:36.247356.247356 lmp.py:1935]   Expert 63 |    155 | CPU
DEBUG 01-15 16:10:36.247283.247283 lmp.py:1935]   Expert 19 |    156 | CPU
DEBUG 01-15 16:10:36.247973.247973 lmp.py:1935]   Expert  6 |    166 | CPU
DEBUG 01-15 16:10:36.247000.247000 lmp.py:1935]   Expert 57 |    169 | CPU
DEBUG 01-15 16:10:36.247643.247643 lmp.py:1935]   Expert 52 |    173 | CPU
DEBUG 01-15 16:10:36.247570.247570 lmp.py:1935]   Expert 50 |    177 | CPU
DEBUG 01-15 16:10:36.247644.247644 lmp.py:1935]   Expert 18 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247002.247002 lmp.py:1935]   Expert 44 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247645.247645 lmp.py:1935]   Expert 31 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247288.247288 lmp.py:1935]   Expert 30 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247693.247693 lmp.py:1935]   Expert 13 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247859.247859 lmp.py:1935]   Expert 23 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247025.247025 lmp.py:1935]   Expert 39 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247191.247191 lmp.py:1935]   Expert 53 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247357.247357 lmp.py:1935]   Expert 59 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247285.247285 lmp.py:1935]   Expert 20 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247451.247451 lmp.py:1935]   Expert 21 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247094.247094 lmp.py:1935]   Expert 29 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247975.247975 lmp.py:1935]   Expert 16 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247618.247618 lmp.py:1935]   Expert 36 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247261.247261 lmp.py:1935]   Expert 25 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247428.247428 lmp.py:1935]   Expert 41 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247355.247355 lmp.py:1935]   Expert 32 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247283.247283 lmp.py:1935]   Expert 49 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247211.247211 lmp.py:1935]   Expert 46 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247138.247138 lmp.py:1935]   Expert 10 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247828.247828 lmp.py:1935]   Expert 42 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247994.247994 lmp.py:1935]   Expert  8 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247922.247922 lmp.py:1935]   Expert 62 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247565.247565 lmp.py:1935]   Expert 35 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247208.247208 lmp.py:1935]   Expert 33 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247851.247851 lmp.py:1935]   Expert  9 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247493.247493 lmp.py:1935]   Expert 58 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247421.247421 lmp.py:1935]   Expert 40 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.247349.247349 lmp.py:1935]   Expert 11 |    420 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.247038.247038 lmp.py:1935]   Expert  0 |    428 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.248966.248966 lmp.py:1935]   Expert 24 |    566 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.248894.248894 lmp.py:1935]   Expert  1 |    649 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.248629.248629 lmp.py:1937] 
DEBUG 01-15 16:10:36.248629.248629 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.248842.248842 lmp.py:1938]   CPU:   3802 tokens
DEBUG 01-15 16:10:36.248769.248769 lmp.py:1942]   cuda:1:   4243 tokens (16 experts)
DEBUG 01-15 16:10:36.248174.248174 lmp.py:1942]   cuda:2:   4243 tokens (16 experts)
DEBUG 01-15 16:10:36.248625.248625 lmp.py:1943]   Total GPU:   8486 tokens
DEBUG 01-15 16:10:36.248837.248837 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.248837.248837 lmp.py:1944] 
DEBUG 01-15 16:10:36.248156.248156 cuda_h.py:19] end experts_map_get cost 0.0016531944274902344 seconds
DEBUG 01-15 16:10:36.248291.248291 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.248139.248139 lmp.py:1953] 
DEBUG 01-15 16:10:36.248139.248139 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.248313.248313 cuda_h.py:19] end cpu_experts_submit cost 5.7220458984375e-05 seconds
DEBUG 01-15 16:10:36.248771.248771 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.248601.248601 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.249384.249384 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.1.gate_proj.weight': 0, 'model.layers.10.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.32.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.32.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.32.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.40.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.40.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.40.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.23.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.23.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.23.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(12843483136, 5767168, 0, 0), (12849250304, 5767168, 5767168, 0), (12837715968, 5767168, 11534336, 0), (13431734272, 5767168, 17301504, 0), (13437501440, 5767168, 23068672, 0), (13425967104, 5767168, 28835840, 0), (12981895168, 5767168, 34603008, 0), (12987662336, 5767168, 40370176, 0), (12976128000, 5767168, 46137344, 0), (12999196672, 5767168, 51904512, 0), (13004963840, 5767168, 57671680, 0), (12993429504, 5767168, 63438848, 0), (13016498176, 5767168, 69206016, 0), (13022265344, 5767168, 74973184, 0), (13010731008, 5767168, 80740352, 0), (13552844800, 5767168, 86507520, 0), (13558611968, 5767168, 92274688, 0), (13547077632, 5767168, 98041856, 0), (13535543296, 5767168, 103809024, 0), (13541310464, 5767168, 109576192, 0), (13529776128, 5767168, 115343360, 0), (13051101184, 5767168, 121110528, 0), (13056868352, 5767168, 126877696, 0), (13045334016, 5767168, 132644864, 0), (13103005696, 5767168, 138412032, 0), (13108772864, 5767168, 144179200, 0), (13097238528, 5767168, 149946368, 0), (13673955328, 5767168, 155713536, 0), (13679722496, 5767168, 161480704, 0), (13668188160, 5767168, 167247872, 0), (13137608704, 5767168, 173015040, 0), (13143375872, 5767168, 178782208, 0), (13131841536, 5767168, 184549376, 0), (13189513216, 5767168, 190316544, 0), (13195280384, 5767168, 196083712, 0), (13183746048, 5767168, 201850880, 0), (13743161344, 5767168, 207618048, 0), (13748928512, 5767168, 213385216, 0), (13737394176, 5767168, 219152384, 0), (13829668864, 5767168, 224919552, 0), (13835436032, 5767168, 230686720, 0), (13823901696, 5767168, 236453888, 0), (13846970368, 5767168, 242221056, 0), (13852737536, 5767168, 247988224, 0), (13841203200, 5767168, 253755392, 0), (13362528256, 5767168, 259522560, 0), (13368295424, 5767168, 265289728, 0), (13356761088, 5767168, 271056896, 0)], 2: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (13397131264, 5767168, 17301504, 0), (13402898432, 5767168, 23068672, 0), (13391364096, 5767168, 28835840, 0), (13379829760, 5767168, 34603008, 0), (13385596928, 5767168, 40370176, 0), (13374062592, 5767168, 46137344, 0), (13449035776, 5767168, 51904512, 0), (13454802944, 5767168, 57671680, 0), (13443268608, 5767168, 63438848, 0), (13500940288, 5767168, 69206016, 0), (13506707456, 5767168, 74973184, 0), (13495173120, 5767168, 80740352, 0), (13518241792, 5767168, 86507520, 0), (13524008960, 5767168, 92274688, 0), (13512474624, 5767168, 98041856, 0), (12964593664, 5767168, 103809024, 0), (12970360832, 5767168, 109576192, 0), (12958826496, 5767168, 115343360, 0), (13587447808, 5767168, 121110528, 0), (13593214976, 5767168, 126877696, 0), (13581680640, 5767168, 132644864, 0), (13622050816, 5767168, 138412032, 0), (13627817984, 5767168, 144179200, 0), (13616283648, 5767168, 149946368, 0), (13172211712, 5767168, 155713536, 0), (13177978880, 5767168, 161480704, 0), (13166444544, 5767168, 167247872, 0), (13345226752, 5767168, 173015040, 0), (13350993920, 5767168, 178782208, 0), (13339459584, 5767168, 184549376, 0), (13224116224, 5767168, 190316544, 0), (13229883392, 5767168, 196083712, 0), (13218349056, 5767168, 201850880, 0), (13241417728, 5767168, 207618048, 0), (13247184896, 5767168, 213385216, 0), (13235650560, 5767168, 219152384, 0), (13258719232, 5767168, 224919552, 0), (13264486400, 5767168, 230686720, 0), (13252952064, 5767168, 236453888, 0), (13327925248, 5767168, 242221056, 0), (13333692416, 5767168, 247988224, 0), (13322158080, 5767168, 253755392, 0), (13898874880, 5767168, 259522560, 0), (13904642048, 5767168, 265289728, 0), (13893107712, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e5420a1f0>, 2: <capsule object NULL at 0x7a4e5420a4c0>}
DEBUG 01-15 16:10:36.249775.249775 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.249287.249287 sllm_store_c.py:29] call client load into gpu
INFO 01-15 16:10:36.249051.249051 client.py:127] Model loaded
DEBUG 01-15 16:10:36.249359.249359 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.249267.249267 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.250762.250762 cuda_h.py:19] end restore2model cost 0.0004901885986328125 seconds
DEBUG 01-15 16:10:36.250173.250173 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.249212.249212 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ac40d6ec-8fe2-4c5c-a81d-6ee4c52e78a2
DEBUG 01-15 16:10:36.250016.250016 cuda_h.py:19] end sllm_worker_task cost 0.010414838790893555 seconds
DEBUG 01-15 16:10:36.250625.250625 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.251225.251225 cuda_h.py:19] end move_flatidxs cost 0.0008730888366699219 seconds
DEBUG 01-15 16:10:36.251352.251352 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:36.251777.251777 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ac40d6ec-8fe2-4c5c-a81d-6ee4c52e78a2
DEBUG 01-15 16:10:36.252387.252387 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0036656856536865234 seconds
DEBUG 01-15 16:10:36.252734.252734 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.254726.254726 cuda_h.py:19] end restore2model cost 0.002485990524291992 seconds
DEBUG 01-15 16:10:36.254661.254661 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006392240524291992 seconds
DEBUG 01-15 16:10:36.254742.254742 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.255441.255441 cuda_h.py:19] end gpu_sexperts cost 0.0002722740173339844 seconds
DEBUG 01-15 16:10:36.255555.255555 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.255192.255192 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.9550323486328125e-05 seconds
DEBUG 01-15 16:10:36.255319.255319 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.255499.255499 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.256699.256699 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008182525634765625 seconds
DEBUG 01-15 16:10:36.256065.256065 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.256152.256152 cuda_h.py:19] end gpu_group_list cost 0.00017404556274414062 seconds
DEBUG 01-15 16:10:36.257455.257455 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.258824.258824 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008740425109863281 seconds
DEBUG 01-15 16:10:36.258581.258581 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.258244.258244 cuda_h.py:19] end gpu_group_list cost 0.0001766681671142578 seconds
DEBUG 01-15 16:10:36.258946.258946 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.258159.258159 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ac40d6ec-8fe2-4c5c-a81d-6ee4c52e78a2
DEBUG 01-15 16:10:36.261440.261440 cuda_h.py:19] end group_tensors cost 0.010691404342651367 seconds
DEBUG 01-15 16:10:36.262903.262903 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.267377.267377 cuda_h.py:19] end group pad cost 0.0044040679931640625 seconds
DEBUG 01-15 16:10:36.267313.267313 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.277724.277724 client.py:127] Model loaded
DEBUG 01-15 16:10:36.277664.277664 cuda_h.py:19] end wait_experts_multi_device cost 0.018992185592651367 seconds
DEBUG 01-15 16:10:36.277142.277142 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.285922.285922 cuda_h.py:19] end group_einsum cost 0.018306970596313477 seconds
DEBUG 01-15 16:10:36.285179.285179 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.289040.289040 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036776065826416016 seconds
DEBUG 01-15 16:10:36.290060.290060 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.040207624435424805 seconds
DEBUG 01-15 16:10:36.290137.290137 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012648344039916992 seconds
DEBUG 01-15 16:10:36.290001.290001 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.292662.292662 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.292291.292291 cuda_h.py:19] end index_scatter cost 9.679794311523438e-05 seconds
DEBUG 01-15 16:10:36.293210.293210 cuda_h.py:19] end cpuoutputsdeal cost 0.0022089481353759766 seconds
DEBUG 01-15 16:10:36.293524.293524 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.293916.293916 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.293685.293685 cuda_h.py:19] end gpu_group_tensor cost 0.0001723766326904297 seconds
DEBUG 01-15 16:10:36.293839.293839 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.293534.293534 cuda_h.py:19] end gpu_group_tensor cost 0.00015926361083984375 seconds
DEBUG 01-15 16:10:36.293796.293796 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.294275.294275 cuda_h.py:19] end gpu_group_einsum cost 0.0005767345428466797 seconds
DEBUG 01-15 16:10:36.294187.294187 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.295721.295721 cuda_h.py:19] end gpu_group_einsum cost 0.0004734992980957031 seconds
DEBUG 01-15 16:10:36.295759.295759 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.295021.295021 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.295595.295595 cuda_h.py:19] end all_expert_outputs_slices cost 0.00026869773864746094 seconds
DEBUG 01-15 16:10:36.295219.295219 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.295646.295646 cuda_h.py:19] end concat_expert_out cost 6.413459777832031e-05 seconds
DEBUG 01-15 16:10:36.295039.295039 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.296441.296441 cuda_h.py:19] end index_scatter cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:10:36.296914.296914 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009996891021728516 seconds
DEBUG 01-15 16:10:36.296521.296521 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.296861.296861 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.296698.296698 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002262592315673828 seconds
DEBUG 01-15 16:10:36.296984.296984 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.296656.296656 cuda_h.py:19] end concat_expert_out cost 6.985664367675781e-05 seconds
DEBUG 01-15 16:10:36.296003.296003 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.297067.297067 cuda_h.py:19] end index_scatter cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:36.297412.297412 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006804466247558594 seconds
DEBUG 01-15 16:10:36.297382.297382 cuda_h.py:19] end gpu_experts_multi_device cost 0.04201483726501465 seconds
DEBUG 01-15 16:10:36.297564.297564 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.05161881446838379 seconds
DEBUG 01-15 16:10:36.297326.297326 cuda_h.py:19] end prefill_layer cost 0.058168649673461914 seconds
DEBUG 01-15 16:10:36.297242.297242 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:10:36.297429.297429 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.297099.297099 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:10:36.297385.297385 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:36.297386.297386 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:36.298388.298388 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:10:36.298489.298489 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 7.987022399902344e-05 seconds
DEBUG 01-15 16:10:36.298053.298053 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.298247.298247 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.298853.298853 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.298551.298551 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.298686.298686 cuda_h.py:19] end allocate_cuda_memory cost 0.0002346038818359375 seconds
DEBUG 01-15 16:10:36.298404.298404 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.298671.298671 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.298735.298735 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.298564.298564 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.298797.298797 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f067aa5b-ff9a-4b3d-bb7c-979b1d32dc80
DEBUG 01-15 16:10:36.299331.299331 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.299398.299398 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.300514.300514 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f067aa5b-ff9a-4b3d-bb7c-979b1d32dc80
DEBUG 01-15 16:10:36.300026.300026 cuda_h.py:19] end load_into_gpu_async cost 0.001970529556274414 seconds
DEBUG 01-15 16:10:36.300020.300020 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.300845.300845 cuda_h.py:19] end restore_tensors2 cost 8.416175842285156e-05 seconds
DEBUG 01-15 16:10:36.301654.301654 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002733469009399414 seconds
INFO 01-15 16:10:36.301002.301002 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f067aa5b-ff9a-4b3d-bb7c-979b1d32dc80
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.304434.304434 cuda_h.py:19] end self_attn cost 0.004532814025878906 seconds
DEBUG 01-15 16:10:36.304434.304434 cuda_h.py:19] end iln_self_attn_paln cost 0.00638580322265625 seconds
DEBUG 01-15 16:10:36.304932.304932 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:10:36.304980.304980 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.305873.305873 cuda_h.py:19] end gate cost 0.0007216930389404297 seconds
DEBUG 01-15 16:10:36.305139.305139 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.305282.305282 lmp.py:1912] 
DEBUG 01-15 16:10:36.305282.305282 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.305852.305852 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.305840.305840 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.305536.305536 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.305040.305040 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.306829.306829 lmp.py:1917] 
DEBUG 01-15 16:10:36.306829.306829 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.306618.306618 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.306890.306890 lmp.py:1935]   Expert 39 |     16 | CPU
DEBUG 01-15 16:10:36.306679.306679 lmp.py:1935]   Expert 13 |     17 | CPU
DEBUG 01-15 16:10:36.306752.306752 lmp.py:1935]   Expert 49 |     37 | CPU
DEBUG 01-15 16:10:36.306587.306587 lmp.py:1935]   Expert 35 |     55 | CPU
DEBUG 01-15 16:10:36.306422.306422 lmp.py:1935]   Expert 19 |     63 | CPU
DEBUG 01-15 16:10:36.306211.306211 lmp.py:1935]   Expert 32 |     73 | CPU
DEBUG 01-15 16:10:36.306000.306000 lmp.py:1935]   Expert  9 |     74 | CPU
DEBUG 01-15 16:10:36.306789.306789 lmp.py:1935]   Expert 26 |     75 | CPU
DEBUG 01-15 16:10:36.306339.306339 lmp.py:1935]   Expert 41 |     77 | CPU
DEBUG 01-15 16:10:36.306412.306412 lmp.py:1935]   Expert 33 |     81 | CPU
DEBUG 01-15 16:10:36.306247.306247 lmp.py:1935]   Expert 23 |     87 | CPU
DEBUG 01-15 16:10:36.306082.306082 lmp.py:1935]   Expert 18 |     90 | CPU
DEBUG 01-15 16:10:36.306679.306679 lmp.py:1935]   Expert 31 |     90 | CPU
DEBUG 01-15 16:10:36.306514.306514 lmp.py:1935]   Expert 46 |     90 | CPU
DEBUG 01-15 16:10:36.306349.306349 lmp.py:1935]   Expert 38 |    100 | CPU
DEBUG 01-15 16:10:36.306184.306184 lmp.py:1935]   Expert  3 |    103 | CPU
DEBUG 01-15 16:10:36.306019.306019 lmp.py:1935]   Expert 17 |    104 | CPU
DEBUG 01-15 16:10:36.306854.306854 lmp.py:1935]   Expert  6 |    107 | CPU
DEBUG 01-15 16:10:36.306643.306643 lmp.py:1935]   Expert 20 |    117 | CPU
DEBUG 01-15 16:10:36.306955.306955 lmp.py:1935]   Expert 40 |    129 | CPU
DEBUG 01-15 16:10:36.306029.306029 lmp.py:1935]   Expert 61 |    131 | CPU
DEBUG 01-15 16:10:36.306579.306579 lmp.py:1935]   Expert 62 |    132 | CPU
DEBUG 01-15 16:10:36.306414.306414 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:10:36.306011.306011 lmp.py:1935]   Expert 43 |    135 | CPU
DEBUG 01-15 16:10:36.306369.306369 lmp.py:1935]   Expert 50 |    135 | CPU
DEBUG 01-15 16:10:36.306489.306489 lmp.py:1935]   Expert 16 |    137 | CPU
DEBUG 01-15 16:10:36.306085.306085 lmp.py:1935]   Expert 44 |    137 | CPU
DEBUG 01-15 16:10:36.306444.306444 lmp.py:1935]   Expert 59 |    138 | CPU
DEBUG 01-15 16:10:36.306040.306040 lmp.py:1935]   Expert 63 |    139 | CPU
DEBUG 01-15 16:10:36.306875.306875 lmp.py:1935]   Expert 42 |    144 | CPU
DEBUG 01-15 16:10:36.306710.306710 lmp.py:1935]   Expert  2 |    145 | CPU
DEBUG 01-15 16:10:36.306738.306738 lmp.py:1935]   Expert 36 |    151 | CPU
DEBUG 01-15 16:10:36.306957.306957 lmp.py:1935]   Expert 10 |    159 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.306176.306176 lmp.py:1935]   Expert  5 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.306680.306680 lmp.py:1935]   Expert 34 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.306946.306946 lmp.py:1935]   Expert 52 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.306496.306496 lmp.py:1935]   Expert 27 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.306285.306285 lmp.py:1935]   Expert 45 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.306597.306597 lmp.py:1935]   Expert 60 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.306670.306670 lmp.py:1935]   Expert 48 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.306459.306459 lmp.py:1935]   Expert 56 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.306532.306532 lmp.py:1935]   Expert 51 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.306083.306083 lmp.py:1935]   Expert 53 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.306633.306633 lmp.py:1935]   Expert 24 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.306183.306183 lmp.py:1935]   Expert  7 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.306210.306210 lmp.py:1935]   Expert  8 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.306999.306999 lmp.py:1935]   Expert 57 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.306788.306788 lmp.py:1935]   Expert 47 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.306338.306338 lmp.py:1935]   Expert 21 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.307889.307889 lmp.py:1935]   Expert 29 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.307962.307962 lmp.py:1935]   Expert 14 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.307274.307274 lmp.py:1935]   Expert  0 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.307586.307586 lmp.py:1935]   Expert  4 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.307898.307898 lmp.py:1935]   Expert 22 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.307971.307971 lmp.py:1935]   Expert 55 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.307283.307283 lmp.py:1935]   Expert 37 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.307787.307787 lmp.py:1935]   Expert 58 |    318 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.307814.307814 lmp.py:1935]   Expert  1 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.307318.307318 lmp.py:1935]   Expert 54 |    331 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.307822.307822 lmp.py:1935]   Expert 28 |    358 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.307611.307611 lmp.py:1935]   Expert 12 |    376 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.307923.307923 lmp.py:1935]   Expert 11 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.307235.307235 lmp.py:1935]   Expert 25 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.307547.307547 lmp.py:1935]   Expert 30 |    834 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.307667.307667 lmp.py:1937] 
DEBUG 01-15 16:10:36.307667.307667 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.307263.307263 lmp.py:1938]   CPU:   3244 tokens
DEBUG 01-15 16:10:36.307575.307575 lmp.py:1942]   cuda:1:   4599 tokens (16 experts)
DEBUG 01-15 16:10:36.307172.307172 lmp.py:1942]   cuda:2:   4445 tokens (16 experts)
DEBUG 01-15 16:10:36.307292.307292 lmp.py:1943]   Total GPU:   9044 tokens
DEBUG 01-15 16:10:36.307842.307842 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.307842.307842 lmp.py:1944] 
DEBUG 01-15 16:10:36.307498.307498 cuda_h.py:19] end experts_map_get cost 0.0019867420196533203 seconds
DEBUG 01-15 16:10:36.307693.307693 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.307687.307687 lmp.py:1953] 
DEBUG 01-15 16:10:36.307687.307687 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.307570.307570 cuda_h.py:19] end cpu_experts_submit cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:36.307312.307312 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.307957.307957 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.308348.308348 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.60.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.60.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.60.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.51.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.51.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.51.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.55.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.55.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.55.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.57.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.57.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.57.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.11.mlp.experts.4.gate_proj.weight': 0, 'model.layers.11.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.47.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.47.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.47.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.22.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.22.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.22.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.25.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.25.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.25.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (13950779392, 5767168, 17301504, 0), (13956546560, 5767168, 23068672, 0), (13945012224, 5767168, 28835840, 0), (14521729024, 5767168, 34603008, 0), (14527496192, 5767168, 40370176, 0), (14515961856, 5767168, 46137344, 0), (14106492928, 5767168, 51904512, 0), (14112260096, 5767168, 57671680, 0), (14100725760, 5767168, 63438848, 0), (14971568128, 5767168, 69206016, 0), (14977335296, 5767168, 74973184, 0), (14965800960, 5767168, 80740352, 0), (14763950080, 5767168, 86507520, 0), (14769717248, 5767168, 92274688, 0), (14758182912, 5767168, 98041856, 0), (14815854592, 5767168, 103809024, 0), (14821621760, 5767168, 109576192, 0), (14810087424, 5767168, 115343360, 0), (14296809472, 5767168, 121110528, 0), (14302576640, 5767168, 126877696, 0), (14291042304, 5767168, 132644864, 0), (14885060608, 5767168, 138412032, 0), (14890827776, 5767168, 144179200, 0), (14879293440, 5767168, 149946368, 0), (14348713984, 5767168, 155713536, 0), (14354481152, 5767168, 161480704, 0), (14342946816, 5767168, 167247872, 0), (14919663616, 5767168, 173015040, 0), (14925430784, 5767168, 178782208, 0), (14913896448, 5767168, 184549376, 0), (14936965120, 5767168, 190316544, 0), (14942732288, 5767168, 196083712, 0), (14931197952, 5767168, 201850880, 0), (14400618496, 5767168, 207618048, 0), (14406385664, 5767168, 213385216, 0), (14394851328, 5767168, 219152384, 0), (14417920000, 5767168, 224919552, 0), (14423687168, 5767168, 230686720, 0), (14412152832, 5767168, 236453888, 0), (14435221504, 5767168, 242221056, 0), (14440988672, 5767168, 247988224, 0), (14429454336, 5767168, 253755392, 0), (14452523008, 5767168, 259522560, 0), (14458290176, 5767168, 265289728, 0), (14446755840, 5767168, 271056896, 0)], 2: [(14002683904, 5767168, 0, 0), (14008451072, 5767168, 5767168, 0), (13996916736, 5767168, 11534336, 0), (14573633536, 5767168, 17301504, 0), (14579400704, 5767168, 23068672, 0), (14567866368, 5767168, 28835840, 0), (14019985408, 5767168, 34603008, 0), (14025752576, 5767168, 40370176, 0), (14014218240, 5767168, 46137344, 0), (14054588416, 5767168, 51904512, 0), (14060355584, 5767168, 57671680, 0), (14048821248, 5767168, 63438848, 0), (14071889920, 5767168, 69206016, 0), (14077657088, 5767168, 74973184, 0), (14066122752, 5767168, 80740352, 0), (14123794432, 5767168, 86507520, 0), (14129561600, 5767168, 92274688, 0), (14118027264, 5767168, 98041856, 0), (14141095936, 5767168, 103809024, 0), (14146863104, 5767168, 109576192, 0), (14135328768, 5767168, 115343360, 0), (14712045568, 5767168, 121110528, 0), (14717812736, 5767168, 126877696, 0), (14706278400, 5767168, 132644864, 0), (14175698944, 5767168, 138412032, 0), (14181466112, 5767168, 144179200, 0), (14169931776, 5767168, 149946368, 0), (14746648576, 5767168, 155713536, 0), (14752415744, 5767168, 161480704, 0), (14740881408, 5767168, 167247872, 0), (14833156096, 5767168, 173015040, 0), (14838923264, 5767168, 178782208, 0), (14827388928, 5767168, 184549376, 0), (14850457600, 5767168, 190316544, 0), (14856224768, 5767168, 196083712, 0), (14844690432, 5767168, 201850880, 0), (14867759104, 5767168, 207618048, 0), (14873526272, 5767168, 213385216, 0), (14861991936, 5767168, 219152384, 0), (14314110976, 5767168, 224919552, 0), (14319878144, 5767168, 230686720, 0), (14308343808, 5767168, 236453888, 0), (14902362112, 5767168, 242221056, 0), (14908129280, 5767168, 247988224, 0), (14896594944, 5767168, 253755392, 0), (14366015488, 5767168, 259522560, 0), (14371782656, 5767168, 265289728, 0), (14360248320, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e447ebe40>, 2: <capsule object NULL at 0x7a4ec424fcc0>}
INFO 01-15 16:10:36.308147.308147 client.py:127] Model loaded
DEBUG 01-15 16:10:36.308715.308715 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.308863.308863 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.308635.308635 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.308361.308361 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.309555.309555 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, aab8dafb-ad11-462c-9868-7e7efa5e60da
DEBUG 01-15 16:10:36.309724.309724 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.309767.309767 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.309360.309360 cuda_h.py:19] end restore2model cost 0.0006849765777587891 seconds
DEBUG 01-15 16:10:36.309043.309043 cuda_h.py:19] end sllm_worker_task cost 0.011420726776123047 seconds
DEBUG 01-15 16:10:36.310085.310085 cuda_h.py:19] end move_flatidxs cost 0.0008358955383300781 seconds
DEBUG 01-15 16:10:36.310053.310053 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:36.311505.311505 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, aab8dafb-ad11-462c-9868-7e7efa5e60da
DEBUG 01-15 16:10:36.312252.312252 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004425048828125 seconds
DEBUG 01-15 16:10:36.312228.312228 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.315780.315780 cuda_h.py:19] end restore2model cost 0.0030028820037841797 seconds
DEBUG 01-15 16:10:36.315478.315478 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007681608200073242 seconds
DEBUG 01-15 16:10:36.315750.315750 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.315642.315642 cuda_h.py:19] end gpu_sexperts cost 0.00030922889709472656 seconds
DEBUG 01-15 16:10:36.315187.315187 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.315970.315970 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.71661376953125e-05 seconds
DEBUG 01-15 16:10:36.315050.315050 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.315568.315568 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.316806.316806 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011243820190429688 seconds
DEBUG 01-15 16:10:36.317278.317278 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.317855.317855 cuda_h.py:19] end gpu_group_list cost 0.00018286705017089844 seconds
DEBUG 01-15 16:10:36.318517.318517 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.319578.319578 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011625289916992188 seconds
DEBUG 01-15 16:10:36.319826.319826 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.319985.319985 cuda_h.py:19] end gpu_group_list cost 0.00017642974853515625 seconds
DEBUG 01-15 16:10:36.320464.320464 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.320546.320546 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, aab8dafb-ad11-462c-9868-7e7efa5e60da
DEBUG 01-15 16:10:36.320295.320295 cuda_h.py:19] end group_tensors cost 0.009934663772583008 seconds
DEBUG 01-15 16:10:36.320460.320460 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.325737.325737 cuda_h.py:19] end group pad cost 0.004252910614013672 seconds
DEBUG 01-15 16:10:36.325957.325957 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.338074.338074 client.py:127] Model loaded
DEBUG 01-15 16:10:36.338271.338271 cuda_h.py:19] end wait_experts_multi_device cost 0.017897605895996094 seconds
DEBUG 01-15 16:10:36.338770.338770 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.344500.344500 cuda_h.py:19] end group_einsum cost 0.018890380859375 seconds
DEBUG 01-15 16:10:36.344731.344731 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.347959.347959 cuda_h.py:19] end get_outputs_cpu1 cost 0.0031855106353759766 seconds
DEBUG 01-15 16:10:36.348158.348158 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03929781913757324 seconds
DEBUG 01-15 16:10:36.349930.349930 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010265588760375977 seconds
DEBUG 01-15 16:10:36.349961.349961 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.351204.351204 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.351125.351125 cuda_h.py:19] end index_scatter cost 0.00011348724365234375 seconds
DEBUG 01-15 16:10:36.351775.351775 cuda_h.py:19] end cpuoutputsdeal cost 0.0026683807373046875 seconds
DEBUG 01-15 16:10:36.352296.352296 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.352729.352729 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.352847.352847 cuda_h.py:19] end gpu_group_tensor cost 0.0002961158752441406 seconds
DEBUG 01-15 16:10:36.352525.352525 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.352610.352610 cuda_h.py:19] end gpu_group_tensor cost 0.0002694129943847656 seconds
DEBUG 01-15 16:10:36.353948.353948 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.354447.354447 cuda_h.py:19] end gpu_group_einsum cost 0.0015604496002197266 seconds
DEBUG 01-15 16:10:36.354588.354588 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.355889.355889 cuda_h.py:19] end gpu_group_einsum cost 0.0007829666137695312 seconds
DEBUG 01-15 16:10:36.356612.356612 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.356215.356215 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.356069.356069 cuda_h.py:19] end all_expert_outputs_slices cost 0.0005080699920654297 seconds
DEBUG 01-15 16:10:36.356787.356787 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.357323.357323 cuda_h.py:19] end concat_expert_out cost 0.00010991096496582031 seconds
DEBUG 01-15 16:10:36.357029.357029 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.357731.357731 cuda_h.py:19] end index_scatter cost 0.00012445449829101562 seconds
DEBUG 01-15 16:10:36.357130.357130 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001722097396850586 seconds
DEBUG 01-15 16:10:36.358064.358064 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.358558.358558 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.358594.358594 cuda_h.py:19] end all_expert_outputs_slices cost 0.00041174888610839844 seconds
DEBUG 01-15 16:10:36.358443.358443 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.358542.358542 cuda_h.py:19] end concat_expert_out cost 0.0001125335693359375 seconds
DEBUG 01-15 16:10:36.359573.359573 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.359215.359215 cuda_h.py:19] end index_scatter cost 0.00011873245239257812 seconds
DEBUG 01-15 16:10:36.359178.359178 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011622905731201172 seconds
DEBUG 01-15 16:10:36.359123.359123 cuda_h.py:19] end gpu_experts_multi_device cost 0.04366326332092285 seconds
DEBUG 01-15 16:10:36.359493.359493 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.05507779121398926 seconds
DEBUG 01-15 16:10:36.360300.360300 cuda_h.py:19] end prefill_layer cost 0.06235241889953613 seconds
DEBUG 01-15 16:10:36.360243.360243 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:10:36.360246.360246 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.360665.360665 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:10:36.360462.360462 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:36.360027.360027 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:36.360541.360541 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 6.175041198730469e-05 seconds
DEBUG 01-15 16:10:36.360995.360995 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.360712.360712 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 0.00023031234741210938 seconds
DEBUG 01-15 16:10:36.360311.360311 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.361128.361128 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.361330.361330 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.361593.361593 cuda_h.py:19] end allocate_cuda_memory cost 0.00021004676818847656 seconds
DEBUG 01-15 16:10:36.361019.361019 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.361589.361589 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.361187.361187 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.361036.361036 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, aac0f9b6-4006-4950-a852-26dade8f8528
DEBUG 01-15 16:10:36.361781.361781 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.361144.361144 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.362119.362119 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.363678.363678 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, aac0f9b6-4006-4950-a852-26dade8f8528
DEBUG 01-15 16:10:36.363275.363275 cuda_h.py:19] end load_into_gpu_async cost 0.0016989707946777344 seconds
DEBUG 01-15 16:10:36.363025.363025 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.363399.363399 cuda_h.py:19] end restore_tensors2 cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:10:36.363440.363440 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0023157596588134766 seconds
INFO 01-15 16:10:36.363648.363648 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, aac0f9b6-4006-4950-a852-26dade8f8528
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.367674.367674 cuda_h.py:19] end self_attn cost 0.0046482086181640625 seconds
DEBUG 01-15 16:10:36.367204.367204 cuda_h.py:19] end iln_self_attn_paln cost 0.00625300407409668 seconds
DEBUG 01-15 16:10:36.367610.367610 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:10:36.367042.367042 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.368843.368843 cuda_h.py:19] end gate cost 0.00075531005859375 seconds
DEBUG 01-15 16:10:36.368885.368885 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.368572.368572 lmp.py:1912] 
DEBUG 01-15 16:10:36.368572.368572 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.368335.368335 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.369753.369753 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.369357.369357 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.369291.369291 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.369033.369033 lmp.py:1917] 
DEBUG 01-15 16:10:36.369033.369033 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.369014.369014 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.369817.369817 lmp.py:1935]   Expert 12 |     21 | CPU
DEBUG 01-15 16:10:36.369990.369990 lmp.py:1935]   Expert 47 |     24 | CPU
DEBUG 01-15 16:10:36.369209.369209 lmp.py:1935]   Expert 38 |     31 | CPU
DEBUG 01-15 16:10:36.369428.369428 lmp.py:1935]   Expert 27 |     35 | CPU
DEBUG 01-15 16:10:36.369932.369932 lmp.py:1935]   Expert 16 |     36 | CPU
DEBUG 01-15 16:10:36.369674.369674 lmp.py:1935]   Expert 52 |     38 | CPU
DEBUG 01-15 16:10:36.369178.369178 lmp.py:1935]   Expert 63 |     43 | CPU
DEBUG 01-15 16:10:36.369159.369159 lmp.py:1935]   Expert  4 |     59 | CPU
DEBUG 01-15 16:10:36.369147.369147 lmp.py:1935]   Expert 44 |     62 | CPU
DEBUG 01-15 16:10:36.369366.369366 lmp.py:1935]   Expert 43 |     64 | CPU
DEBUG 01-15 16:10:36.369347.369347 lmp.py:1935]   Expert 61 |     66 | CPU
DEBUG 01-15 16:10:36.369613.369613 lmp.py:1935]   Expert 34 |     77 | CPU
DEBUG 01-15 16:10:36.369163.369163 lmp.py:1935]   Expert 53 |     81 | CPU
DEBUG 01-15 16:10:36.369190.369190 lmp.py:1935]   Expert  0 |     88 | CPU
DEBUG 01-15 16:10:36.369217.369217 lmp.py:1935]   Expert 32 |     89 | CPU
DEBUG 01-15 16:10:36.369244.369244 lmp.py:1935]   Expert 37 |     90 | CPU
DEBUG 01-15 16:10:36.369795.369795 lmp.py:1935]   Expert 13 |    105 | CPU
DEBUG 01-15 16:10:36.369060.369060 lmp.py:1935]   Expert 39 |    116 | CPU
DEBUG 01-15 16:10:36.369087.369087 lmp.py:1935]   Expert 21 |    120 | CPU
DEBUG 01-15 16:10:36.369353.369353 lmp.py:1935]   Expert 11 |    121 | CPU
DEBUG 01-15 16:10:36.369334.369334 lmp.py:1935]   Expert 20 |    125 | CPU
DEBUG 01-15 16:10:36.369321.369321 lmp.py:1935]   Expert 60 |    130 | CPU
DEBUG 01-15 16:10:36.369541.369541 lmp.py:1935]   Expert  8 |    132 | CPU
DEBUG 01-15 16:10:36.369806.369806 lmp.py:1935]   Expert 14 |    138 | CPU
DEBUG 01-15 16:10:36.369357.369357 lmp.py:1935]   Expert 57 |    139 | CPU
DEBUG 01-15 16:10:36.369669.369669 lmp.py:1935]   Expert 22 |    140 | CPU
DEBUG 01-15 16:10:36.369219.369219 lmp.py:1935]   Expert 45 |    149 | CPU
DEBUG 01-15 16:10:36.369008.369008 lmp.py:1935]   Expert  2 |    155 | CPU
DEBUG 01-15 16:10:36.369558.369558 lmp.py:1935]   Expert 18 |    156 | CPU
DEBUG 01-15 16:10:36.369824.369824 lmp.py:1935]   Expert 23 |    158 | CPU
DEBUG 01-15 16:10:36.369302.369302 lmp.py:1935]   Expert 17 |    160 | CPU
DEBUG 01-15 16:10:36.369713.369713 lmp.py:1935]   Expert  7 |    161 | CPU
DEBUG 01-15 16:10:36.369886.369886 lmp.py:1935]   Expert 58 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.369105.369105 lmp.py:1935]   Expert 30 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.369086.369086 lmp.py:1935]   Expert 42 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.369067.369067 lmp.py:1935]   Expert 48 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.369571.369571 lmp.py:1935]   Expert 49 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.369790.369790 lmp.py:1935]   Expert 62 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.369009.369009 lmp.py:1935]   Expert 55 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.369520.369520 lmp.py:1935]   Expert 35 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.370614.370614 lmp.py:1935]   Expert 51 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.370469.370469 lmp.py:1935]   Expert 29 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.370133.370133 lmp.py:1935]   Expert  6 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.370988.370988 lmp.py:1935]   Expert 25 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.370512.370512 lmp.py:1935]   Expert 36 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.370262.370262 lmp.py:1935]   Expert  1 |    199 | GPU1(cuda:1)
INFO 01-15 16:10:36.370674.370674 client.py:127] Model loaded
DEBUG 01-15 16:10:36.370431.370431 lmp.py:1935]   Expert 31 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.370122.370122 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.370070.370070 lmp.py:1935]   Expert 28 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.370593.370593 lmp.py:1935]   Expert 54 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.370110.370110 lmp.py:1935]   Expert  5 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.370714.370714 lmp.py:1935]   Expert 41 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.370364.370364 lmp.py:1935]   Expert  9 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.370404.370404 lmp.py:1935]   Expert 19 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.370577.370577 lmp.py:1935]   Expert 24 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.370750.370750 lmp.py:1935]   Expert 50 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.370446.370446 lmp.py:1935]   Expert 46 |    308 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.370288.370288 lmp.py:1935]   Expert 59 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.370176.370176 lmp.py:1935]   Expert 56 |    378 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.371450.371450 lmp.py:1935]   Expert 26 |    405 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.371391.371391 lmp.py:1935]   Expert 33 |    424 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.371803.371803 lmp.py:1935]   Expert  3 |    589 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.371452.371452 lmp.py:1935]   Expert 10 |    644 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.371387.371387 lmp.py:1935]   Expert 15 |    645 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.371321.371321 lmp.py:1935]   Expert 40 |    790 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.371587.371587 lmp.py:1937] 
DEBUG 01-15 16:10:36.371587.371587 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.371760.371760 lmp.py:1938]   CPU:   3109 tokens
DEBUG 01-15 16:10:36.371933.371933 lmp.py:1942]   cuda:1:   4592 tokens (16 experts)
DEBUG 01-15 16:10:36.371391.371391 lmp.py:1942]   cuda:2:   4587 tokens (16 experts)
DEBUG 01-15 16:10:36.371895.371895 lmp.py:1943]   Total GPU:   9179 tokens
DEBUG 01-15 16:10:36.371544.371544 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.371544.371544 lmp.py:1944] 
DEBUG 01-15 16:10:36.371823.371823 cuda_h.py:19] end experts_map_get cost 0.002867460250854492 seconds
DEBUG 01-15 16:10:36.371059.371059 cuda_h.py:19] end restore2model cost 0.0009932518005371094 seconds
DEBUG 01-15 16:10:36.371842.371842 cuda_h.py:19] end sllm_worker_task cost 0.010516643524169922 seconds
DEBUG 01-15 16:10:36.371194.371194 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.371170.371170 lmp.py:1953] 
DEBUG 01-15 16:10:36.371170.371170 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.371464.371464 cuda_h.py:19] end cpu_experts_submit cost 6.532669067382812e-05 seconds
DEBUG 01-15 16:10:36.371160.371160 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.371135.371135 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.372970.372970 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.1.gate_proj.weight': 0, 'model.layers.12.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.58.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.58.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.58.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.40.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.40.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.40.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.12.mlp.experts.33.gate_proj.weight': 0, 'model.layers.12.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.54.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.54.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.54.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(15058075648, 5767168, 0, 0), (15063842816, 5767168, 5767168, 0), (15052308480, 5767168, 11534336, 0), (15092678656, 5767168, 17301504, 0), (15098445824, 5767168, 23068672, 0), (15086911488, 5767168, 28835840, 0), (16044261376, 5767168, 34603008, 0), (16050028544, 5767168, 40370176, 0), (16038494208, 5767168, 46137344, 0), (15127281664, 5767168, 51904512, 0), (15133048832, 5767168, 57671680, 0), (15121514496, 5767168, 63438848, 0), (15144583168, 5767168, 69206016, 0), (15150350336, 5767168, 74973184, 0), (15138816000, 5767168, 80740352, 0), (15732834304, 5767168, 86507520, 0), (15738601472, 5767168, 92274688, 0), (15727067136, 5767168, 98041856, 0), (15196487680, 5767168, 103809024, 0), (15202254848, 5767168, 109576192, 0), (15190720512, 5767168, 115343360, 0), (15871246336, 5767168, 121110528, 0), (15877013504, 5767168, 126877696, 0), (15865479168, 5767168, 132644864, 0), (15888547840, 5767168, 138412032, 0), (15894315008, 5767168, 144179200, 0), (15882780672, 5767168, 149946368, 0), (15905849344, 5767168, 155713536, 0), (15911616512, 5767168, 161480704, 0), (15900082176, 5767168, 167247872, 0), (15923150848, 5767168, 173015040, 0), (15928918016, 5767168, 178782208, 0), (15917383680, 5767168, 184549376, 0), (15456010240, 5767168, 190316544, 0), (15461777408, 5767168, 196083712, 0), (15450243072, 5767168, 201850880, 0), (15490613248, 5767168, 207618048, 0), (15496380416, 5767168, 213385216, 0), (15484846080, 5767168, 219152384, 0), (16061562880, 5767168, 224919552, 0), (16067330048, 5767168, 230686720, 0), (16055795712, 5767168, 236453888, 0), (15542517760, 5767168, 242221056, 0), (15548284928, 5767168, 247988224, 0), (15536750592, 5767168, 253755392, 0), (15577120768, 5767168, 259522560, 0), (15582887936, 5767168, 265289728, 0), (15571353600, 5767168, 271056896, 0)], 2: [(15611723776, 5767168, 0, 0), (15617490944, 5767168, 5767168, 0), (15605956608, 5767168, 11534336, 0), (15646326784, 5767168, 17301504, 0), (15652093952, 5767168, 23068672, 0), (15640559616, 5767168, 28835840, 0), (15663628288, 5767168, 34603008, 0), (15669395456, 5767168, 40370176, 0), (15657861120, 5767168, 46137344, 0), (15750135808, 5767168, 51904512, 0), (15755902976, 5767168, 57671680, 0), (15744368640, 5767168, 63438848, 0), (15213789184, 5767168, 69206016, 0), (15219556352, 5767168, 74973184, 0), (15208022016, 5767168, 80740352, 0), (15767437312, 5767168, 86507520, 0), (15773204480, 5767168, 92274688, 0), (15761670144, 5767168, 98041856, 0), (15836643328, 5767168, 103809024, 0), (15842410496, 5767168, 109576192, 0), (15830876160, 5767168, 115343360, 0), (15300296704, 5767168, 121110528, 0), (15306063872, 5767168, 126877696, 0), (15294529536, 5767168, 132644864, 0), (15369502720, 5767168, 138412032, 0), (15375269888, 5767168, 144179200, 0), (15363735552, 5767168, 149946368, 0), (15559819264, 5767168, 155713536, 0), (15565586432, 5767168, 161480704, 0), (15554052096, 5767168, 167247872, 0), (15975055360, 5767168, 173015040, 0), (15980822528, 5767168, 178782208, 0), (15969288192, 5767168, 184549376, 0), (15992356864, 5767168, 190316544, 0), (15998124032, 5767168, 196083712, 0), (15986589696, 5767168, 201850880, 0), (16009658368, 5767168, 207618048, 0), (16015425536, 5767168, 213385216, 0), (16003891200, 5767168, 219152384, 0), (15473311744, 5767168, 224919552, 0), (15479078912, 5767168, 230686720, 0), (15467544576, 5767168, 236453888, 0), (15525216256, 5767168, 242221056, 0), (15530983424, 5767168, 247988224, 0), (15519449088, 5767168, 253755392, 0), (16113467392, 5767168, 259522560, 0), (16119234560, 5767168, 265289728, 0), (16107700224, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4313c00>, 2: <capsule object NULL at 0x7a4e6c3aff00>}
DEBUG 01-15 16:10:36.372109.372109 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.372760.372760 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.372085.372085 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9949eaea-cbfa-4ee6-8083-e46085283b7b
DEBUG 01-15 16:10:36.373535.373535 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.373765.373765 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.373294.373294 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:36.374069.374069 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9949eaea-cbfa-4ee6-8083-e46085283b7b
DEBUG 01-15 16:10:36.374716.374716 cuda_h.py:19] end move_flatidxs cost 0.000865936279296875 seconds
DEBUG 01-15 16:10:36.374605.374605 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.374245.374245 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002838134765625 seconds
DEBUG 01-15 16:10:36.374930.374930 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.377861.377861 cuda_h.py:19] end restore2model cost 0.0030333995819091797 seconds
DEBUG 01-15 16:10:36.377512.377512 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0061342716217041016 seconds
DEBUG 01-15 16:10:36.377758.377758 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.378650.378650 cuda_h.py:19] end gpu_sexperts cost 0.0003097057342529297 seconds
DEBUG 01-15 16:10:36.378957.378957 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.378117.378117 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:10:36.378959.378959 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.378861.378861 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.379209.379209 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001065969467163086 seconds
DEBUG 01-15 16:10:36.379151.379151 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.379483.379483 cuda_h.py:19] end gpu_group_list cost 0.00017905235290527344 seconds
DEBUG 01-15 16:10:36.380138.380138 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.381544.381544 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011820793151855469 seconds
DEBUG 01-15 16:10:36.382407.382407 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.382866.382866 cuda_h.py:19] end gpu_group_list cost 0.00018525123596191406 seconds
DEBUG 01-15 16:10:36.383899.383899 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.383497.383497 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9949eaea-cbfa-4ee6-8083-e46085283b7b
DEBUG 01-15 16:10:36.384771.384771 cuda_h.py:19] end group_tensors cost 0.01025843620300293 seconds
DEBUG 01-15 16:10:36.385459.385459 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.390635.390635 cuda_h.py:19] end group pad cost 0.004605770111083984 seconds
DEBUG 01-15 16:10:36.390769.390769 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.400063.400063 client.py:127] Model loaded
DEBUG 01-15 16:10:36.401958.401958 cuda_h.py:19] end wait_experts_multi_device cost 0.0178983211517334 seconds
DEBUG 01-15 16:10:36.401411.401411 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.409375.409375 cuda_h.py:19] end group_einsum cost 0.01878643035888672 seconds
DEBUG 01-15 16:10:36.409824.409824 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.412554.412554 cuda_h.py:19] end get_outputs_cpu1 cost 0.003139019012451172 seconds
DEBUG 01-15 16:10:36.413235.413235 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039864540100097656 seconds
DEBUG 01-15 16:10:36.413034.413034 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012395381927490234 seconds
DEBUG 01-15 16:10:36.413468.413468 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.415910.415910 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.415372.415372 cuda_h.py:19] end index_scatter cost 8.392333984375e-05 seconds
DEBUG 01-15 16:10:36.415628.415628 cuda_h.py:19] end cpuoutputsdeal cost 0.0019714832305908203 seconds
DEBUG 01-15 16:10:36.415168.415168 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.415122.415122 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.416202.416202 cuda_h.py:19] end gpu_group_tensor cost 0.00016450881958007812 seconds
DEBUG 01-15 16:10:36.416442.416442 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.416362.416362 cuda_h.py:19] end gpu_group_tensor cost 0.00015044212341308594 seconds
DEBUG 01-15 16:10:36.416207.416207 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.417576.417576 cuda_h.py:19] end gpu_group_einsum cost 0.0006721019744873047 seconds
DEBUG 01-15 16:10:36.417032.417032 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.417653.417653 cuda_h.py:19] end gpu_group_einsum cost 0.0005028247833251953 seconds
DEBUG 01-15 16:10:36.417525.417525 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.418005.418005 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.418915.418915 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024199485778808594 seconds
DEBUG 01-15 16:10:36.418055.418055 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.418462.418462 cuda_h.py:19] end concat_expert_out cost 5.459785461425781e-05 seconds
DEBUG 01-15 16:10:36.418504.418504 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.418494.418494 cuda_h.py:19] end index_scatter cost 6.222724914550781e-05 seconds
DEBUG 01-15 16:10:36.418794.418794 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008897781372070312 seconds
DEBUG 01-15 16:10:36.419314.419314 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.419933.419933 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.419013.419013 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020003318786621094 seconds
DEBUG 01-15 16:10:36.419246.419246 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.419706.419706 cuda_h.py:19] end concat_expert_out cost 5.91278076171875e-05 seconds
DEBUG 01-15 16:10:36.419510.419510 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.419407.419407 cuda_h.py:19] end index_scatter cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:10:36.419554.419554 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005924701690673828 seconds
DEBUG 01-15 16:10:36.419670.419670 cuda_h.py:19] end gpu_experts_multi_device cost 0.04136967658996582 seconds
DEBUG 01-15 16:10:36.419593.419593 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.05227184295654297 seconds
DEBUG 01-15 16:10:36.420145.420145 cuda_h.py:19] end prefill_layer cost 0.05983853340148926 seconds
DEBUG 01-15 16:10:36.420028.420028 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:10:36.420592.420592 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.420752.420752 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:10:36.420985.420985 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:36.420556.420556 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:36.420088.420088 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 4.410743713378906e-05 seconds
DEBUG 01-15 16:10:36.420705.420705 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 8.082389831542969e-05 seconds
DEBUG 01-15 16:10:36.420693.420693 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.420887.420887 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.420501.420501 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.420993.420993 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.421985.421985 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.421697.421697 cuda_h.py:19] end allocate_cuda_memory cost 0.00023984909057617188 seconds
DEBUG 01-15 16:10:36.421600.421600 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.421316.421316 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.421953.421953 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.421186.421186 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7cb9d55b-85cc-4525-ac1d-1b97b5ed9f27
DEBUG 01-15 16:10:36.421475.421475 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.421194.421194 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.422338.422338 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7cb9d55b-85cc-4525-ac1d-1b97b5ed9f27
DEBUG 01-15 16:10:36.422943.422943 cuda_h.py:19] end load_into_gpu_async cost 0.0015153884887695312 seconds
DEBUG 01-15 16:10:36.422507.422507 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.423133.423133 cuda_h.py:19] end restore_tensors2 cost 7.963180541992188e-05 seconds
DEBUG 01-15 16:10:36.423704.423704 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.00209808349609375 seconds
INFO 01-15 16:10:36.423375.423375 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7cb9d55b-85cc-4525-ac1d-1b97b5ed9f27
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.425226.425226 cuda_h.py:19] end self_attn cost 0.003634929656982422 seconds
DEBUG 01-15 16:10:36.425422.425422 cuda_h.py:19] end iln_self_attn_paln cost 0.005306720733642578 seconds
DEBUG 01-15 16:10:36.426629.426629 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:10:36.426053.426053 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.426392.426392 cuda_h.py:19] end gate cost 0.0007779598236083984 seconds
DEBUG 01-15 16:10:36.426129.426129 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.427822.427822 lmp.py:1912] 
DEBUG 01-15 16:10:36.427822.427822 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.427532.427532 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.427658.427658 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.427209.427209 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.427090.427090 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.427256.427256 lmp.py:1917] 
DEBUG 01-15 16:10:36.427256.427256 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.427614.427614 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.427456.427456 lmp.py:1935]   Expert 42 |     23 | CPU
DEBUG 01-15 16:10:36.427099.427099 lmp.py:1935]   Expert 19 |     24 | CPU
DEBUG 01-15 16:10:36.427789.427789 lmp.py:1935]   Expert 30 |     27 | CPU
DEBUG 01-15 16:10:36.427001.427001 lmp.py:1935]   Expert 32 |     47 | CPU
DEBUG 01-15 16:10:36.427690.427690 lmp.py:1935]   Expert  6 |     58 | CPU
DEBUG 01-15 16:10:36.427903.427903 lmp.py:1935]   Expert 53 |     72 | CPU
DEBUG 01-15 16:10:36.427115.427115 lmp.py:1935]   Expert  5 |     73 | CPU
DEBUG 01-15 16:10:36.427566.427566 lmp.py:1935]   Expert  1 |     80 | CPU
DEBUG 01-15 16:10:36.427686.427686 lmp.py:1935]   Expert 13 |    120 | CPU
DEBUG 01-15 16:10:36.427852.427852 lmp.py:1935]   Expert  9 |    125 | CPU
DEBUG 01-15 16:10:36.427257.427257 lmp.py:1935]   Expert 34 |    128 | CPU
DEBUG 01-15 16:10:36.427906.427906 lmp.py:1935]   Expert 63 |    128 | CPU
DEBUG 01-15 16:10:36.427026.427026 lmp.py:1935]   Expert 58 |    130 | CPU
DEBUG 01-15 16:10:36.427669.427669 lmp.py:1935]   Expert 50 |    131 | CPU
DEBUG 01-15 16:10:36.427835.427835 lmp.py:1935]   Expert 11 |    136 | CPU
DEBUG 01-15 16:10:36.427001.427001 lmp.py:1935]   Expert 26 |    136 | CPU
DEBUG 01-15 16:10:36.427168.427168 lmp.py:1935]   Expert 18 |    137 | CPU
DEBUG 01-15 16:10:36.427095.427095 lmp.py:1935]   Expert 31 |    137 | CPU
DEBUG 01-15 16:10:36.427023.427023 lmp.py:1935]   Expert 59 |    141 | CPU
DEBUG 01-15 16:10:36.427189.427189 lmp.py:1935]   Expert 40 |    146 | CPU
DEBUG 01-15 16:10:36.427117.427117 lmp.py:1935]   Expert 46 |    146 | CPU
DEBUG 01-15 16:10:36.427806.427806 lmp.py:1935]   Expert 12 |    147 | CPU
DEBUG 01-15 16:10:36.427734.427734 lmp.py:1935]   Expert 56 |    151 | CPU
DEBUG 01-15 16:10:36.427092.427092 lmp.py:1935]   Expert 48 |    152 | CPU
DEBUG 01-15 16:10:36.427735.427735 lmp.py:1935]   Expert  4 |    153 | CPU
DEBUG 01-15 16:10:36.427378.427378 lmp.py:1935]   Expert 20 |    153 | CPU
DEBUG 01-15 16:10:36.427783.427783 lmp.py:1935]   Expert  2 |    154 | CPU
DEBUG 01-15 16:10:36.427949.427949 lmp.py:1935]   Expert 33 |    155 | CPU
DEBUG 01-15 16:10:36.427877.427877 lmp.py:1935]   Expert 61 |    157 | CPU
DEBUG 01-15 16:10:36.427327.427327 lmp.py:1935]   Expert 35 |    163 | CPU
DEBUG 01-15 16:10:36.427017.427017 lmp.py:1935]   Expert 10 |    168 | CPU
DEBUG 01-15 16:10:36.427944.427944 lmp.py:1935]   Expert 55 |    171 | CPU
DEBUG 01-15 16:10:36.427541.427541 lmp.py:1935]   Expert 51 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.427138.427138 lmp.py:1935]   Expert 36 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.427258.427258 lmp.py:1935]   Expert  8 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428854.428854 lmp.py:1935]   Expert 52 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428451.428451 lmp.py:1935]   Expert 37 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428809.428809 lmp.py:1935]   Expert  0 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428406.428406 lmp.py:1935]   Expert 57 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428764.428764 lmp.py:1935]   Expert 39 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428645.428645 lmp.py:1935]   Expert 25 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428288.428288 lmp.py:1935]   Expert 62 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428693.428693 lmp.py:1935]   Expert  7 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428336.428336 lmp.py:1935]   Expert 38 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428217.428217 lmp.py:1935]   Expert  3 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428099.428099 lmp.py:1935]   Expert 27 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428218.428218 lmp.py:1935]   Expert 24 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428815.428815 lmp.py:1935]   Expert 28 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428173.428173 lmp.py:1935]   Expert 49 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428055.428055 lmp.py:1935]   Expert 21 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428936.428936 lmp.py:1935]   Expert 60 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428056.428056 lmp.py:1935]   Expert 16 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428699.428699 lmp.py:1935]   Expert 43 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428342.428342 lmp.py:1935]   Expert 23 |    273 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428985.428985 lmp.py:1935]   Expert 29 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428389.428389 lmp.py:1935]   Expert 15 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428900.428900 lmp.py:1935]   Expert 22 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428543.428543 lmp.py:1935]   Expert 47 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428855.428855 lmp.py:1935]   Expert 41 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428975.428975 lmp.py:1935]   Expert 44 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428095.428095 lmp.py:1935]   Expert 54 |    356 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428691.428691 lmp.py:1935]   Expert 14 |    374 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428050.428050 lmp.py:1935]   Expert 17 |    408 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.428693.428693 lmp.py:1935]   Expert 45 |    454 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.428382.428382 lmp.py:1937] 
DEBUG 01-15 16:10:36.428382.428382 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.428786.428786 lmp.py:1938]   CPU:   3869 tokens
DEBUG 01-15 16:10:36.428668.428668 lmp.py:1942]   cuda:1:   4212 tokens (16 experts)
DEBUG 01-15 16:10:36.428311.428311 lmp.py:1942]   cuda:2:   4207 tokens (16 experts)
DEBUG 01-15 16:10:36.428762.428762 lmp.py:1943]   Total GPU:   8419 tokens
DEBUG 01-15 16:10:36.428451.428451 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.428451.428451 lmp.py:1944] 
DEBUG 01-15 16:10:36.428339.428339 cuda_h.py:19] end experts_map_get cost 0.0017061233520507812 seconds
DEBUG 01-15 16:10:36.428427.428427 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.428276.428276 lmp.py:1953] 
DEBUG 01-15 16:10:36.428276.428276 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.428821.428821 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:36.428371.428371 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.428724.428724 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.429677.429677 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.43.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.43.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.43.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.51.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.51.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.51.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.54.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.54.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.54.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.13.mlp.experts.3.gate_proj.weight': 0, 'model.layers.13.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.38.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.38.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.38.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.49.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.49.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.49.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.52.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.52.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.52.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.25.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.25.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.25.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.28.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.28.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.28.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16269180928, 5767168, 17301504, 0), (16274948096, 5767168, 23068672, 0), (16263413760, 5767168, 28835840, 0), (16286482432, 5767168, 34603008, 0), (16292249600, 5767168, 40370176, 0), (16280715264, 5767168, 46137344, 0), (16857432064, 5767168, 51904512, 0), (16863199232, 5767168, 57671680, 0), (16851664896, 5767168, 63438848, 0), (16892035072, 5767168, 69206016, 0), (16897802240, 5767168, 74973184, 0), (16886267904, 5767168, 80740352, 0), (16926638080, 5767168, 86507520, 0), (16932405248, 5767168, 92274688, 0), (16920870912, 5767168, 98041856, 0), (17030447104, 5767168, 103809024, 0), (17036214272, 5767168, 109576192, 0), (17024679936, 5767168, 115343360, 0), (16511401984, 5767168, 121110528, 0), (16517169152, 5767168, 126877696, 0), (16505634816, 5767168, 132644864, 0), (17082351616, 5767168, 138412032, 0), (17088118784, 5767168, 144179200, 0), (17076584448, 5767168, 149946368, 0), (16528703488, 5767168, 155713536, 0), (16534470656, 5767168, 161480704, 0), (16522936320, 5767168, 167247872, 0), (16563306496, 5767168, 173015040, 0), (16569073664, 5767168, 178782208, 0), (16557539328, 5767168, 184549376, 0), (17134256128, 5767168, 190316544, 0), (17140023296, 5767168, 196083712, 0), (17128488960, 5767168, 201850880, 0), (16615211008, 5767168, 207618048, 0), (16620978176, 5767168, 213385216, 0), (16609443840, 5767168, 219152384, 0), (17186160640, 5767168, 224919552, 0), (17191927808, 5767168, 230686720, 0), (17180393472, 5767168, 236453888, 0), (16649814016, 5767168, 242221056, 0), (16655581184, 5767168, 247988224, 0), (16644046848, 5767168, 253755392, 0), (17220763648, 5767168, 259522560, 0), (17226530816, 5767168, 265289728, 0), (17214996480, 5767168, 271056896, 0)], 2: [(16199974912, 5767168, 0, 0), (16205742080, 5767168, 5767168, 0), (16194207744, 5767168, 11534336, 0), (16770924544, 5767168, 17301504, 0), (16776691712, 5767168, 23068672, 0), (16765157376, 5767168, 28835840, 0), (16788226048, 5767168, 34603008, 0), (16793993216, 5767168, 40370176, 0), (16782458880, 5767168, 46137344, 0), (16805527552, 5767168, 51904512, 0), (16811294720, 5767168, 57671680, 0), (16799760384, 5767168, 63438848, 0), (16822829056, 5767168, 69206016, 0), (16828596224, 5767168, 74973184, 0), (16817061888, 5767168, 80740352, 0), (16909336576, 5767168, 86507520, 0), (16915103744, 5767168, 92274688, 0), (16903569408, 5767168, 98041856, 0), (16390291456, 5767168, 103809024, 0), (16396058624, 5767168, 109576192, 0), (16384524288, 5767168, 115343360, 0), (16407592960, 5767168, 121110528, 0), (16413360128, 5767168, 126877696, 0), (16401825792, 5767168, 132644864, 0), (16961241088, 5767168, 138412032, 0), (16967008256, 5767168, 144179200, 0), (16955473920, 5767168, 149946368, 0), (16442195968, 5767168, 155713536, 0), (16447963136, 5767168, 161480704, 0), (16436428800, 5767168, 167247872, 0), (16424894464, 5767168, 173015040, 0), (16430661632, 5767168, 178782208, 0), (16419127296, 5767168, 184549376, 0), (16995844096, 5767168, 190316544, 0), (17001611264, 5767168, 196083712, 0), (16990076928, 5767168, 201850880, 0), (17047748608, 5767168, 207618048, 0), (17053515776, 5767168, 213385216, 0), (17041981440, 5767168, 219152384, 0), (16546004992, 5767168, 224919552, 0), (16551772160, 5767168, 230686720, 0), (16540237824, 5767168, 236453888, 0), (16580608000, 5767168, 242221056, 0), (16586375168, 5767168, 247988224, 0), (16574840832, 5767168, 253755392, 0), (16632512512, 5767168, 259522560, 0), (16638279680, 5767168, 265289728, 0), (16626745344, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e3464b3f0>, 2: <capsule object NULL at 0x7a4e6c46bc30>}
DEBUG 01-15 16:10:36.430563.430563 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:10:36.430991.430991 client.py:127] Model loaded
DEBUG 01-15 16:10:36.430854.430854 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.430685.430685 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5f2b3824-369c-42ac-86d2-ac086ddcfe88
DEBUG 01-15 16:10:36.430203.430203 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.430387.430387 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.430306.430306 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.430013.430013 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.430423.430423 cuda_h.py:19] end restore2model cost 0.0003597736358642578 seconds
DEBUG 01-15 16:10:36.431292.431292 cuda_h.py:19] end sllm_worker_task cost 0.010134458541870117 seconds
DEBUG 01-15 16:10:36.431421.431421 cuda_h.py:19] end move_flatidxs cost 0.0008337497711181641 seconds
DEBUG 01-15 16:10:36.431343.431343 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:36.432329.432329 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5f2b3824-369c-42ac-86d2-ac086ddcfe88
DEBUG 01-15 16:10:36.433073.433073 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00447535514831543 seconds
DEBUG 01-15 16:10:36.433136.433136 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.435014.435014 cuda_h.py:19] end restore2model cost 0.0024738311767578125 seconds
DEBUG 01-15 16:10:36.436043.436043 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007179975509643555 seconds
DEBUG 01-15 16:10:36.436123.436123 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.436643.436643 cuda_h.py:19] end gpu_sexperts cost 0.0002818107604980469 seconds
DEBUG 01-15 16:10:36.436155.436155 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.436693.436693 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:10:36.436820.436820 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.436430.436430 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.437307.437307 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008242130279541016 seconds
DEBUG 01-15 16:10:36.437402.437402 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.437601.437601 cuda_h.py:19] end gpu_group_list cost 0.00018286705017089844 seconds
DEBUG 01-15 16:10:36.438726.438726 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.439499.439499 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008876323699951172 seconds
DEBUG 01-15 16:10:36.439409.439409 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.439403.439403 cuda_h.py:19] end gpu_group_list cost 0.00017547607421875 seconds
DEBUG 01-15 16:10:36.440218.440218 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.440200.440200 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5f2b3824-369c-42ac-86d2-ac086ddcfe88
DEBUG 01-15 16:10:36.440198.440198 cuda_h.py:19] end group_tensors cost 0.008878707885742188 seconds
DEBUG 01-15 16:10:36.441534.441534 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.445000.445000 cuda_h.py:19] end group pad cost 0.004183292388916016 seconds
DEBUG 01-15 16:10:36.445850.445850 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.458195.458195 client.py:127] Model loaded
DEBUG 01-15 16:10:36.458100.458100 cuda_h.py:19] end wait_experts_multi_device cost 0.018378496170043945 seconds
DEBUG 01-15 16:10:36.458678.458678 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.464515.464515 cuda_h.py:19] end group_einsum cost 0.01919102668762207 seconds
DEBUG 01-15 16:10:36.464017.464017 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.468977.468977 cuda_h.py:19] end get_outputs_cpu1 cost 0.0038652420043945312 seconds
DEBUG 01-15 16:10:36.469977.469977 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039095401763916016 seconds
DEBUG 01-15 16:10:36.469544.469544 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.011181116104125977 seconds
DEBUG 01-15 16:10:36.470979.470979 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.471600.471600 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.471645.471645 cuda_h.py:19] end index_scatter cost 8.654594421386719e-05 seconds
DEBUG 01-15 16:10:36.472981.472981 cuda_h.py:19] end cpuoutputsdeal cost 0.001989126205444336 seconds
DEBUG 01-15 16:10:36.472858.472858 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.472150.472150 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.472544.472544 cuda_h.py:19] end gpu_group_tensor cost 0.0002548694610595703 seconds
DEBUG 01-15 16:10:36.472506.472506 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.472803.472803 cuda_h.py:19] end gpu_group_tensor cost 0.00015115737915039062 seconds
DEBUG 01-15 16:10:36.472099.472099 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.473666.473666 cuda_h.py:19] end gpu_group_einsum cost 0.0006799697875976562 seconds
DEBUG 01-15 16:10:36.473460.473460 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.474121.474121 cuda_h.py:19] end gpu_group_einsum cost 0.0004928112030029297 seconds
DEBUG 01-15 16:10:36.474761.474761 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.474526.474526 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.474813.474813 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024271011352539062 seconds
DEBUG 01-15 16:10:36.474000.474000 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.474983.474983 cuda_h.py:19] end concat_expert_out cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:10:36.475456.475456 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.475161.475161 cuda_h.py:19] end index_scatter cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:10:36.475428.475428 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009002685546875 seconds
DEBUG 01-15 16:10:36.475948.475948 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.475036.475036 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.475025.475025 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002014636993408203 seconds
DEBUG 01-15 16:10:36.475496.475496 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.475532.475532 cuda_h.py:19] end concat_expert_out cost 6.365776062011719e-05 seconds
DEBUG 01-15 16:10:36.476667.476667 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.476319.476319 cuda_h.py:19] end index_scatter cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:10:36.476320.476320 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005879402160644531 seconds
DEBUG 01-15 16:10:36.476575.476575 cuda_h.py:19] end gpu_experts_multi_device cost 0.039716482162475586 seconds
DEBUG 01-15 16:10:36.476690.476690 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.05026078224182129 seconds
DEBUG 01-15 16:10:36.476184.476184 cuda_h.py:19] end prefill_layer cost 0.05638694763183594 seconds
DEBUG 01-15 16:10:36.476358.476358 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:10:36.476544.476544 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.476539.476539 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:10:36.476533.476533 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:36.477627.477627 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:36.477153.477153 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 4.076957702636719e-05 seconds
DEBUG 01-15 16:10:36.477718.477718 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.477415.477415 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 0.0001633167266845703 seconds
DEBUG 01-15 16:10:36.477676.477676 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.477089.477089 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.477436.477436 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.477487.477487 cuda_h.py:19] end allocate_cuda_memory cost 0.0002543926239013672 seconds
DEBUG 01-15 16:10:36.477119.477119 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.477743.477743 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.477526.477526 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.477236.477236 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 52f46642-761b-4807-887b-3a1ca928289e
DEBUG 01-15 16:10:36.478193.478193 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.478517.478517 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.478007.478007 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.479045.479045 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 52f46642-761b-4807-887b-3a1ca928289e
DEBUG 01-15 16:10:36.480033.480033 cuda_h.py:19] end load_into_gpu_async cost 0.0021448135375976562 seconds
DEBUG 01-15 16:10:36.480551.480551 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.480853.480853 cuda_h.py:19] end restore_tensors2 cost 8.487701416015625e-05 seconds
DEBUG 01-15 16:10:36.480854.480854 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0028281211853027344 seconds
INFO 01-15 16:10:36.480459.480459 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 52f46642-761b-4807-887b-3a1ca928289e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.482719.482719 cuda_h.py:19] end self_attn cost 0.0037984848022460938 seconds
DEBUG 01-15 16:10:36.482108.482108 cuda_h.py:19] end iln_self_attn_paln cost 0.005385637283325195 seconds
DEBUG 01-15 16:10:36.482938.482938 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:10:36.482985.482985 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.483442.483442 cuda_h.py:19] end gate cost 0.0007534027099609375 seconds
DEBUG 01-15 16:10:36.483279.483279 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.484759.484759 lmp.py:1912] 
DEBUG 01-15 16:10:36.484759.484759 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.484899.484899 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.484456.484456 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.484305.484305 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.484570.484570 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.484644.484644 lmp.py:1917] 
DEBUG 01-15 16:10:36.484644.484644 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.484479.484479 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.484559.484559 lmp.py:1935]   Expert 34 |     31 | CPU
DEBUG 01-15 16:10:36.484156.484156 lmp.py:1935]   Expert  7 |     33 | CPU
DEBUG 01-15 16:10:36.484275.484275 lmp.py:1935]   Expert 13 |     41 | CPU
DEBUG 01-15 16:10:36.484918.484918 lmp.py:1935]   Expert 54 |     74 | CPU
DEBUG 01-15 16:10:36.484561.484561 lmp.py:1935]   Expert 18 |     85 | CPU
DEBUG 01-15 16:10:36.484397.484397 lmp.py:1935]   Expert 39 |     88 | CPU
DEBUG 01-15 16:10:36.484755.484755 lmp.py:1935]   Expert 49 |     88 | CPU
DEBUG 01-15 16:10:36.484351.484351 lmp.py:1935]   Expert 59 |     99 | CPU
DEBUG 01-15 16:10:36.484425.484425 lmp.py:1935]   Expert 16 |    105 | CPU
DEBUG 01-15 16:10:36.484783.484783 lmp.py:1935]   Expert 21 |    108 | CPU
DEBUG 01-15 16:10:36.484664.484664 lmp.py:1935]   Expert  0 |    109 | CPU
DEBUG 01-15 16:10:36.484307.484307 lmp.py:1935]   Expert 15 |    118 | CPU
DEBUG 01-15 16:10:36.484950.484950 lmp.py:1935]   Expert 41 |    118 | CPU
DEBUG 01-15 16:10:36.484832.484832 lmp.py:1935]   Expert 22 |    121 | CPU
DEBUG 01-15 16:10:36.484475.484475 lmp.py:1935]   Expert 45 |    122 | CPU
DEBUG 01-15 16:10:36.484118.484118 lmp.py:1935]   Expert 17 |    128 | CPU
DEBUG 01-15 16:10:36.484191.484191 lmp.py:1935]   Expert 61 |    135 | CPU
DEBUG 01-15 16:10:36.484934.484934 lmp.py:1935]   Expert 52 |    136 | CPU
DEBUG 01-15 16:10:36.484484.484484 lmp.py:1935]   Expert  8 |    137 | CPU
DEBUG 01-15 16:10:36.484273.484273 lmp.py:1935]   Expert 35 |    139 | CPU
DEBUG 01-15 16:10:36.484108.484108 lmp.py:1935]   Expert 38 |    141 | CPU
DEBUG 01-15 16:10:36.484943.484943 lmp.py:1935]   Expert 12 |    143 | CPU
DEBUG 01-15 16:10:36.484778.484778 lmp.py:1935]   Expert 48 |    147 | CPU
DEBUG 01-15 16:10:36.484851.484851 lmp.py:1935]   Expert 31 |    150 | CPU
DEBUG 01-15 16:10:36.484687.484687 lmp.py:1935]   Expert 36 |    153 | CPU
DEBUG 01-15 16:10:36.484760.484760 lmp.py:1935]   Expert 53 |    155 | CPU
DEBUG 01-15 16:10:36.484118.484118 lmp.py:1935]   Expert 50 |    159 | CPU
DEBUG 01-15 16:10:36.484006.484006 lmp.py:1935]   Expert 60 |    161 | CPU
DEBUG 01-15 16:10:36.484080.484080 lmp.py:1935]   Expert 40 |    162 | CPU
DEBUG 01-15 16:10:36.485677.485677 lmp.py:1935]   Expert 27 |    175 | CPU
DEBUG 01-15 16:10:36.485757.485757 lmp.py:1935]   Expert 19 |    194 | CPU
DEBUG 01-15 16:10:36.485353.485353 lmp.py:1935]   Expert  4 |    196 | CPU
DEBUG 01-15 16:10:36.485665.485665 lmp.py:1935]   Expert 29 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485216.485216 lmp.py:1935]   Expert 30 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485051.485051 lmp.py:1935]   Expert 11 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485840.485840 lmp.py:1935]   Expert 26 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485436.485436 lmp.py:1935]   Expert 20 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485225.485225 lmp.py:1935]   Expert 57 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485252.485252 lmp.py:1935]   Expert  6 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485041.485041 lmp.py:1935]   Expert 46 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485353.485353 lmp.py:1935]   Expert 43 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485665.485665 lmp.py:1935]   Expert  2 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485261.485261 lmp.py:1935]   Expert 23 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485858.485858 lmp.py:1935]   Expert 33 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485693.485693 lmp.py:1935]   Expert 42 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485813.485813 lmp.py:1935]   Expert 55 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485409.485409 lmp.py:1935]   Expert 56 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485245.485245 lmp.py:1935]   Expert 32 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485603.485603 lmp.py:1935]   Expert  3 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485723.485723 lmp.py:1935]   Expert  9 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485034.485034 lmp.py:1935]   Expert 14 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485108.485108 lmp.py:1935]   Expert 28 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485943.485943 lmp.py:1935]   Expert 51 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485970.485970 lmp.py:1935]   Expert 44 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485567.485567 lmp.py:1935]   Expert  1 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485925.485925 lmp.py:1935]   Expert 58 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485999.485999 lmp.py:1935]   Expert 63 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485357.485357 lmp.py:1935]   Expert 47 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485953.485953 lmp.py:1935]   Expert 37 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485709.485709 lmp.py:1935]   Expert 24 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485783.485783 lmp.py:1935]   Expert 62 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485380.485380 lmp.py:1935]   Expert 10 |    313 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485930.485930 lmp.py:1935]   Expert 25 |    313 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.485811.485811 lmp.py:1935]   Expert  5 |    364 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.485739.485739 lmp.py:1937] 
DEBUG 01-15 16:10:36.485739.485739 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.485859.485859 lmp.py:1938]   CPU:   3951 tokens
DEBUG 01-15 16:10:36.485740.485740 lmp.py:1942]   cuda:1:   4169 tokens (16 experts)
DEBUG 01-15 16:10:36.485383.485383 lmp.py:1942]   cuda:2:   4168 tokens (16 experts)
DEBUG 01-15 16:10:36.485311.485311 lmp.py:1943]   Total GPU:   8337 tokens
DEBUG 01-15 16:10:36.485000.485000 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.485000.485000 lmp.py:1944] 
DEBUG 01-15 16:10:36.485080.485080 cuda_h.py:19] end experts_map_get cost 0.0019664764404296875 seconds
DEBUG 01-15 16:10:36.485819.485819 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.485436.485436 lmp.py:1953] 
DEBUG 01-15 16:10:36.485436.485436 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.485987.485987 cuda_h.py:19] end cpu_experts_submit cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:10:36.486445.486445 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.486943.486943 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.487843.487843 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.37.gate_proj.weight': 0, 'model.layers.14.mlp.experts.37.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.37.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.51.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.51.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.51.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.14.mlp.experts.32.gate_proj.weight': 0, 'model.layers.14.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.33.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.33.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.33.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.3.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.3.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.3.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.2.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.2.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.2.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.26.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.26.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.26.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.10.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.10.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.10.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.47.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.47.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.47.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.57.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(17895522304, 5767168, 0, 0), (17901289472, 5767168, 5767168, 0), (17889755136, 5767168, 11534336, 0), (17341874176, 5767168, 17301504, 0), (17347641344, 5767168, 23068672, 0), (17336107008, 5767168, 28835840, 0), (17359175680, 5767168, 34603008, 0), (17364942848, 5767168, 40370176, 0), (17353408512, 5767168, 46137344, 0), (17411080192, 5767168, 51904512, 0), (17416847360, 5767168, 57671680, 0), (17405313024, 5767168, 63438848, 0), (17982029824, 5767168, 69206016, 0), (17987796992, 5767168, 74973184, 0), (17976262656, 5767168, 80740352, 0), (17999331328, 5767168, 86507520, 0), (18005098496, 5767168, 92274688, 0), (17993564160, 5767168, 98041856, 0), (17445683200, 5767168, 103809024, 0), (17451450368, 5767168, 109576192, 0), (17439916032, 5767168, 115343360, 0), (17497587712, 5767168, 121110528, 0), (17503354880, 5767168, 126877696, 0), (17491820544, 5767168, 132644864, 0), (18137743360, 5767168, 138412032, 0), (18143510528, 5767168, 144179200, 0), (18131976192, 5767168, 149946368, 0), (17653301248, 5767168, 155713536, 0), (17659068416, 5767168, 161480704, 0), (17647534080, 5767168, 167247872, 0), (17601396736, 5767168, 173015040, 0), (17607163904, 5767168, 178782208, 0), (17595629568, 5767168, 184549376, 0), (17774411776, 5767168, 190316544, 0), (17780178944, 5767168, 196083712, 0), (17768644608, 5767168, 201850880, 0), (18206949376, 5767168, 207618048, 0), (18212716544, 5767168, 213385216, 0), (18201182208, 5767168, 219152384, 0), (18258853888, 5767168, 224919552, 0), (18264621056, 5767168, 230686720, 0), (18253086720, 5767168, 236453888, 0), (18328059904, 5767168, 242221056, 0), (18333827072, 5767168, 247988224, 0), (18322292736, 5767168, 253755392, 0), (18345361408, 5767168, 259522560, 0), (18351128576, 5767168, 265289728, 0), (18339594240, 5767168, 271056896, 0)], 2: [(17809014784, 5767168, 0, 0), (17814781952, 5767168, 5767168, 0), (17803247616, 5767168, 11534336, 0), (17272668160, 5767168, 17301504, 0), (17278435328, 5767168, 23068672, 0), (17266900992, 5767168, 28835840, 0), (17826316288, 5767168, 34603008, 0), (17832083456, 5767168, 40370176, 0), (17820549120, 5767168, 46137344, 0), (17307271168, 5767168, 51904512, 0), (17313038336, 5767168, 57671680, 0), (17301504000, 5767168, 63438848, 0), (17289969664, 5767168, 69206016, 0), (17295736832, 5767168, 74973184, 0), (17284202496, 5767168, 80740352, 0), (17705205760, 5767168, 86507520, 0), (17710972928, 5767168, 92274688, 0), (17699438592, 5767168, 98041856, 0), (17428381696, 5767168, 103809024, 0), (17434148864, 5767168, 109576192, 0), (17422614528, 5767168, 115343360, 0), (18016632832, 5767168, 121110528, 0), (18022400000, 5767168, 126877696, 0), (18010865664, 5767168, 132644864, 0), (18051235840, 5767168, 138412032, 0), (18057003008, 5767168, 144179200, 0), (18045468672, 5767168, 149946368, 0), (18068537344, 5767168, 155713536, 0), (18074304512, 5767168, 161480704, 0), (18062770176, 5767168, 167247872, 0), (17670602752, 5767168, 173015040, 0), (17676369920, 5767168, 178782208, 0), (17664835584, 5767168, 184549376, 0), (17687904256, 5767168, 190316544, 0), (17693671424, 5767168, 196083712, 0), (17682137088, 5767168, 201850880, 0), (18224250880, 5767168, 207618048, 0), (18230018048, 5767168, 213385216, 0), (18218483712, 5767168, 219152384, 0), (17739808768, 5767168, 224919552, 0), (17745575936, 5767168, 230686720, 0), (17734041600, 5767168, 236453888, 0), (17757110272, 5767168, 242221056, 0), (17762877440, 5767168, 247988224, 0), (17751343104, 5767168, 253755392, 0), (18241552384, 5767168, 259522560, 0), (18247319552, 5767168, 265289728, 0), (18235785216, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e5420a4f0>, 2: <capsule object NULL at 0x7a4e5420a340>}
DEBUG 01-15 16:10:36.487858.487858 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.487317.487317 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.487834.487834 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 18b45fab-77f0-4629-8667-2f7a9d119cf9
DEBUG 01-15 16:10:36.488668.488668 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.488101.488101 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:36.488860.488860 client.py:127] Model loaded
DEBUG 01-15 16:10:36.488451.488451 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.488664.488664 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.488106.488106 cuda_h.py:19] end restore2model cost 0.00034546852111816406 seconds
DEBUG 01-15 16:10:36.488352.488352 cuda_h.py:19] end sllm_worker_task cost 0.011501312255859375 seconds
DEBUG 01-15 16:10:36.489224.489224 cuda_h.py:19] end move_flatidxs cost 0.0008401870727539062 seconds
DEBUG 01-15 16:10:36.489623.489623 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:36.489941.489941 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 18b45fab-77f0-4629-8667-2f7a9d119cf9
DEBUG 01-15 16:10:36.489856.489856 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0039026737213134766 seconds
DEBUG 01-15 16:10:36.490979.490979 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.492043.492043 cuda_h.py:19] end restore2model cost 0.002470254898071289 seconds
DEBUG 01-15 16:10:36.492263.492263 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006619691848754883 seconds
DEBUG 01-15 16:10:36.492059.492059 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.492096.492096 cuda_h.py:19] end gpu_sexperts cost 0.0002732276916503906 seconds
DEBUG 01-15 16:10:36.493256.493256 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.493648.493648 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4543533325195312e-05 seconds
DEBUG 01-15 16:10:36.493298.493298 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.493001.493001 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.493765.493765 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008485317230224609 seconds
DEBUG 01-15 16:10:36.494092.494092 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.494504.494504 cuda_h.py:19] end gpu_group_list cost 0.0001995563507080078 seconds
DEBUG 01-15 16:10:36.494877.494877 cuda_h.py:19] end group_tensors cost 0.004995107650756836 seconds
DEBUG 01-15 16:10:36.495090.495090 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.495397.495397 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.497929.497929 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0020618438720703125 seconds
DEBUG 01-15 16:10:36.497842.497842 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.497039.497039 cuda_h.py:19] end gpu_group_list cost 0.00030994415283203125 seconds
DEBUG 01-15 16:10:36.498186.498186 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.498229.498229 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 18b45fab-77f0-4629-8667-2f7a9d119cf9
DEBUG 01-15 16:10:36.500105.500105 cuda_h.py:19] end group pad cost 0.004857063293457031 seconds
DEBUG 01-15 16:10:36.500994.500994 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.517415.517415 client.py:127] Model loaded
DEBUG 01-15 16:10:36.517389.517389 cuda_h.py:19] end wait_experts_multi_device cost 0.01881551742553711 seconds
DEBUG 01-15 16:10:36.517723.517723 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.523553.523553 cuda_h.py:19] end group_einsum cost 0.02309703826904297 seconds
DEBUG 01-15 16:10:36.523228.523228 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.527942.527942 cuda_h.py:19] end get_outputs_cpu1 cost 0.0038607120513916016 seconds
DEBUG 01-15 16:10:36.528088.528088 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039877891540527344 seconds
DEBUG 01-15 16:10:36.528933.528933 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010743141174316406 seconds
DEBUG 01-15 16:10:36.528414.528414 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.530935.530935 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.530266.530266 cuda_h.py:19] end index_scatter cost 8.535385131835938e-05 seconds
DEBUG 01-15 16:10:36.530270.530270 cuda_h.py:19] end cpuoutputsdeal cost 0.001993417739868164 seconds
DEBUG 01-15 16:10:36.530571.530571 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.530287.530287 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.531698.531698 cuda_h.py:19] end gpu_group_tensor cost 0.00016379356384277344 seconds
DEBUG 01-15 16:10:36.531984.531984 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.531043.531043 cuda_h.py:19] end gpu_group_tensor cost 0.00015091896057128906 seconds
DEBUG 01-15 16:10:36.531875.531875 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.532468.532468 cuda_h.py:19] end gpu_group_einsum cost 0.0006613731384277344 seconds
DEBUG 01-15 16:10:36.532685.532685 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.533648.533648 cuda_h.py:19] end gpu_group_einsum cost 0.0005943775177001953 seconds
DEBUG 01-15 16:10:36.533991.533991 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.533267.533267 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.533902.533902 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003113746643066406 seconds
DEBUG 01-15 16:10:36.533757.533757 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.533310.533310 cuda_h.py:19] end concat_expert_out cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:10:36.533113.533113 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.533103.533103 cuda_h.py:19] end index_scatter cost 6.29425048828125e-05 seconds
DEBUG 01-15 16:10:36.534563.534563 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009968280792236328 seconds
DEBUG 01-15 16:10:36.534374.534374 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.534330.534330 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.534795.534795 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020170211791992188 seconds
DEBUG 01-15 16:10:36.534743.534743 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.534680.534680 cuda_h.py:19] end concat_expert_out cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:10:36.534338.534338 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.534136.534136 cuda_h.py:19] end index_scatter cost 6.008148193359375e-05 seconds
DEBUG 01-15 16:10:36.534329.534329 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005953311920166016 seconds
DEBUG 01-15 16:10:36.535637.535637 cuda_h.py:19] end gpu_experts_multi_device cost 0.04193615913391113 seconds
DEBUG 01-15 16:10:36.535322.535322 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.052182674407958984 seconds
DEBUG 01-15 16:10:36.535066.535066 cuda_h.py:19] end prefill_layer cost 0.058670997619628906 seconds
DEBUG 01-15 16:10:36.535995.535995 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:10:36.535559.535559 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.535315.535315 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:10:36.535833.535833 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:36.535734.535734 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:36.535068.535068 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 3.933906555175781e-05 seconds
DEBUG 01-15 16:10:36.535784.535784 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:10:36.535487.535487 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.536212.536212 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.536548.536548 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.536470.536470 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.536507.536507 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.536273.536273 cuda_h.py:19] end allocate_cuda_memory cost 0.0002765655517578125 seconds
DEBUG 01-15 16:10:36.536991.536991 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.536469.536469 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.536821.536821 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.536816.536816 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5d27f48c-76d4-4757-893a-9a3a8fa0e36c
DEBUG 01-15 16:10:36.536680.536680 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.537191.537191 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.538553.538553 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5d27f48c-76d4-4757-893a-9a3a8fa0e36c
DEBUG 01-15 16:10:36.538350.538350 cuda_h.py:19] end load_into_gpu_async cost 0.0021944046020507812 seconds
DEBUG 01-15 16:10:36.538676.538676 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.539070.539070 cuda_h.py:19] end restore_tensors2 cost 8.344650268554688e-05 seconds
DEBUG 01-15 16:10:36.539787.539787 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0028285980224609375 seconds
INFO 01-15 16:10:36.539160.539160 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5d27f48c-76d4-4757-893a-9a3a8fa0e36c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.541256.541256 cuda_h.py:19] end self_attn cost 0.003833770751953125 seconds
DEBUG 01-15 16:10:36.541904.541904 cuda_h.py:19] end iln_self_attn_paln cost 0.005674123764038086 seconds
DEBUG 01-15 16:10:36.541733.541733 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:10:36.541019.541019 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.542701.542701 cuda_h.py:19] end gate cost 0.0007457733154296875 seconds
DEBUG 01-15 16:10:36.542107.542107 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.543580.543580 lmp.py:1912] 
DEBUG 01-15 16:10:36.543580.543580 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.543549.543549 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.543059.543059 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.543279.543279 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.543114.543114 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.543234.543234 lmp.py:1917] 
DEBUG 01-15 16:10:36.543234.543234 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.543353.543353 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.543195.543195 lmp.py:1935]   Expert 15 |     60 | CPU
DEBUG 01-15 16:10:36.543077.543077 lmp.py:1935]   Expert 41 |     73 | CPU
DEBUG 01-15 16:10:36.543720.543720 lmp.py:1935]   Expert  0 |     76 | CPU
DEBUG 01-15 16:10:36.543886.543886 lmp.py:1935]   Expert 63 |     76 | CPU
DEBUG 01-15 16:10:36.543529.543529 lmp.py:1935]   Expert 20 |     83 | CPU
DEBUG 01-15 16:10:36.543695.543695 lmp.py:1935]   Expert 45 |     91 | CPU
DEBUG 01-15 16:10:36.543623.543623 lmp.py:1935]   Expert  7 |     94 | CPU
DEBUG 01-15 16:10:36.543789.543789 lmp.py:1935]   Expert 28 |     99 | CPU
DEBUG 01-15 16:10:36.543716.543716 lmp.py:1935]   Expert 54 |    106 | CPU
DEBUG 01-15 16:10:36.543883.543883 lmp.py:1935]   Expert 12 |    109 | CPU
DEBUG 01-15 16:10:36.543479.543479 lmp.py:1935]   Expert 52 |    120 | CPU
DEBUG 01-15 16:10:36.543122.543122 lmp.py:1935]   Expert 40 |    121 | CPU
DEBUG 01-15 16:10:36.543765.543765 lmp.py:1935]   Expert 59 |    122 | CPU
DEBUG 01-15 16:10:36.543408.543408 lmp.py:1935]   Expert  5 |    125 | CPU
DEBUG 01-15 16:10:36.543336.543336 lmp.py:1935]   Expert  4 |    131 | CPU
DEBUG 01-15 16:10:36.543264.543264 lmp.py:1935]   Expert 34 |    134 | CPU
DEBUG 01-15 16:10:36.543191.543191 lmp.py:1935]   Expert 13 |    136 | CPU
DEBUG 01-15 16:10:36.543119.543119 lmp.py:1935]   Expert 55 |    137 | CPU
DEBUG 01-15 16:10:36.543285.543285 lmp.py:1935]   Expert 62 |    138 | CPU
DEBUG 01-15 16:10:36.543451.543451 lmp.py:1935]   Expert 61 |    139 | CPU
DEBUG 01-15 16:10:36.543617.543617 lmp.py:1935]   Expert 21 |    140 | CPU
DEBUG 01-15 16:10:36.543545.543545 lmp.py:1935]   Expert 42 |    140 | CPU
DEBUG 01-15 16:10:36.543473.543473 lmp.py:1935]   Expert 10 |    147 | CPU
DEBUG 01-15 16:10:36.543639.543639 lmp.py:1935]   Expert 14 |    147 | CPU
DEBUG 01-15 16:10:36.543567.543567 lmp.py:1935]   Expert 22 |    147 | CPU
DEBUG 01-15 16:10:36.543494.543494 lmp.py:1935]   Expert 51 |    154 | CPU
DEBUG 01-15 16:10:36.543661.543661 lmp.py:1935]   Expert 32 |    156 | CPU
DEBUG 01-15 16:10:36.543304.543304 lmp.py:1935]   Expert 25 |    166 | CPU
DEBUG 01-15 16:10:36.543708.543708 lmp.py:1935]   Expert 47 |    174 | CPU
DEBUG 01-15 16:10:36.543590.543590 lmp.py:1935]   Expert  1 |    175 | CPU
DEBUG 01-15 16:10:36.543994.543994 lmp.py:1935]   Expert 53 |    175 | CPU
DEBUG 01-15 16:10:36.543160.543160 lmp.py:1935]   Expert 26 |    176 | CPU
DEBUG 01-15 16:10:36.543711.543711 lmp.py:1935]   Expert 50 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.543546.543546 lmp.py:1935]   Expert 19 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.543142.543142 lmp.py:1935]   Expert  6 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.543262.543262 lmp.py:1935]   Expert  2 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.543382.543382 lmp.py:1935]   Expert 11 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.543025.543025 lmp.py:1935]   Expert 30 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.543145.543145 lmp.py:1935]   Expert 35 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.543788.543788 lmp.py:1935]   Expert 57 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.543100.543100 lmp.py:1935]   Expert 56 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.543935.543935 lmp.py:1935]   Expert 48 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.543531.543531 lmp.py:1935]   Expert 24 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.543651.543651 lmp.py:1935]   Expert 16 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544725.544725 lmp.py:1935]   Expert 44 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544321.544321 lmp.py:1935]   Expert 46 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544203.544203 lmp.py:1935]   Expert 18 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544846.544846 lmp.py:1935]   Expert 39 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544727.544727 lmp.py:1935]   Expert 29 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544608.544608 lmp.py:1935]   Expert 37 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544013.544013 lmp.py:1935]   Expert 31 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544656.544656 lmp.py:1935]   Expert 60 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544537.544537 lmp.py:1935]   Expert  3 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544372.544372 lmp.py:1935]   Expert 36 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544638.544638 lmp.py:1935]   Expert 38 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544427.544427 lmp.py:1935]   Expert  9 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544739.544739 lmp.py:1935]   Expert 17 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544050.544050 lmp.py:1935]   Expert 23 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544124.544124 lmp.py:1935]   Expert 27 |    348 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544197.544197 lmp.py:1935]   Expert 43 |    364 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544033.544033 lmp.py:1935]   Expert  8 |    399 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544629.544629 lmp.py:1935]   Expert 33 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544464.544464 lmp.py:1935]   Expert 58 |    444 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.544061.544061 lmp.py:1935]   Expert 49 |    543 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.544942.544942 lmp.py:1937] 
DEBUG 01-15 16:10:36.544942.544942 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.544122.544122 lmp.py:1938]   CPU:   4067 tokens
DEBUG 01-15 16:10:36.544176.544176 lmp.py:1942]   cuda:1:   4122 tokens (16 experts)
DEBUG 01-15 16:10:36.544601.544601 lmp.py:1942]   cuda:2:   4099 tokens (16 experts)
DEBUG 01-15 16:10:36.544105.544105 lmp.py:1943]   Total GPU:   8221 tokens
DEBUG 01-15 16:10:36.544702.544702 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.544702.544702 lmp.py:1944] 
DEBUG 01-15 16:10:36.544928.544928 cuda_h.py:19] end experts_map_get cost 0.0019423961639404297 seconds
DEBUG 01-15 16:10:36.544838.544838 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.544547.544547 lmp.py:1953] 
DEBUG 01-15 16:10:36.544547.544547 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.544668.544668 cuda_h.py:19] end cpu_experts_submit cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:10:36.544987.544987 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.544201.544201 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.546961.546961 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.2.gate_proj.weight': 0, 'model.layers.15.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.49.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.49.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.49.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.15.mlp.experts.33.gate_proj.weight': 0, 'model.layers.15.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.11.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.11.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.11.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.46.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.46.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.46.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.48.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.48.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.48.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.58.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.58.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.58.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.60.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.60.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.60.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(18397265920, 5767168, 0, 0), (18403033088, 5767168, 5767168, 0), (18391498752, 5767168, 11534336, 0), (18985517056, 5767168, 17301504, 0), (18991284224, 5767168, 23068672, 0), (18979749888, 5767168, 28835840, 0), (19020120064, 5767168, 34603008, 0), (19025887232, 5767168, 40370176, 0), (19014352896, 5767168, 46137344, 0), (18466471936, 5767168, 51904512, 0), (18472239104, 5767168, 57671680, 0), (18460704768, 5767168, 63438848, 0), (18501074944, 5767168, 69206016, 0), (18506842112, 5767168, 74973184, 0), (18495307776, 5767168, 80740352, 0), (18639486976, 5767168, 86507520, 0), (18645254144, 5767168, 92274688, 0), (18633719808, 5767168, 98041856, 0), (19210436608, 5767168, 103809024, 0), (19216203776, 5767168, 109576192, 0), (19204669440, 5767168, 115343360, 0), (18656788480, 5767168, 121110528, 0), (18662555648, 5767168, 126877696, 0), (18651021312, 5767168, 132644864, 0), (18674089984, 5767168, 138412032, 0), (18679857152, 5767168, 144179200, 0), (18668322816, 5767168, 149946368, 0), (19227738112, 5767168, 155713536, 0), (19233505280, 5767168, 161480704, 0), (19221970944, 5767168, 167247872, 0), (18777899008, 5767168, 173015040, 0), (18783666176, 5767168, 178782208, 0), (18772131840, 5767168, 184549376, 0), (19331547136, 5767168, 190316544, 0), (19337314304, 5767168, 196083712, 0), (19325779968, 5767168, 201850880, 0), (18829803520, 5767168, 207618048, 0), (18835570688, 5767168, 213385216, 0), (18824036352, 5767168, 219152384, 0), (18864406528, 5767168, 224919552, 0), (18870173696, 5767168, 230686720, 0), (18858639360, 5767168, 236453888, 0), (18881708032, 5767168, 242221056, 0), (18887475200, 5767168, 247988224, 0), (18875940864, 5767168, 253755392, 0), (18899009536, 5767168, 259522560, 0), (18904776704, 5767168, 265289728, 0), (18893242368, 5767168, 271056896, 0)], 2: [(18933612544, 5767168, 0, 0), (18939379712, 5767168, 5767168, 0), (18927845376, 5767168, 11534336, 0), (18414567424, 5767168, 17301504, 0), (18420334592, 5767168, 23068672, 0), (18408800256, 5767168, 28835840, 0), (18968215552, 5767168, 34603008, 0), (18973982720, 5767168, 40370176, 0), (18962448384, 5767168, 46137344, 0), (19002818560, 5767168, 51904512, 0), (19008585728, 5767168, 57671680, 0), (18997051392, 5767168, 63438848, 0), (19037421568, 5767168, 69206016, 0), (19043188736, 5767168, 74973184, 0), (19031654400, 5767168, 80740352, 0), (18518376448, 5767168, 86507520, 0), (18524143616, 5767168, 92274688, 0), (18512609280, 5767168, 98041856, 0), (19106627584, 5767168, 103809024, 0), (19112394752, 5767168, 109576192, 0), (19100860416, 5767168, 115343360, 0), (19123929088, 5767168, 121110528, 0), (19129696256, 5767168, 126877696, 0), (19118161920, 5767168, 132644864, 0), (18552979456, 5767168, 138412032, 0), (18558746624, 5767168, 144179200, 0), (18547212288, 5767168, 149946368, 0), (19158532096, 5767168, 155713536, 0), (19164299264, 5767168, 161480704, 0), (19152764928, 5767168, 167247872, 0), (19193135104, 5767168, 173015040, 0), (19198902272, 5767168, 178782208, 0), (19187367936, 5767168, 184549376, 0), (18691391488, 5767168, 190316544, 0), (18697158656, 5767168, 196083712, 0), (18685624320, 5767168, 201850880, 0), (18760597504, 5767168, 207618048, 0), (18766364672, 5767168, 213385216, 0), (18754830336, 5767168, 219152384, 0), (19348848640, 5767168, 224919552, 0), (19354615808, 5767168, 230686720, 0), (19343081472, 5767168, 236453888, 0), (19366150144, 5767168, 242221056, 0), (19371917312, 5767168, 247988224, 0), (19360382976, 5767168, 253755392, 0), (19400753152, 5767168, 259522560, 0), (19406520320, 5767168, 265289728, 0), (19394985984, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e5420a580>, 2: <capsule object NULL at 0x7a4e5420a610>}
DEBUG 01-15 16:10:36.546834.546834 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.546770.546770 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.546380.546380 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f765c397-95fd-422a-8f35-07c13dbe6ef7
DEBUG 01-15 16:10:36.546545.546545 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.547278.547278 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.547196.547196 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:36.548433.548433 client.py:127] Model loaded
DEBUG 01-15 16:10:36.548640.548640 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.548319.548319 cuda_h.py:19] end move_flatidxs cost 0.0008361339569091797 seconds
DEBUG 01-15 16:10:36.548764.548764 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.548427.548427 cuda_h.py:19] end restore2model cost 0.0003380775451660156 seconds
DEBUG 01-15 16:10:36.548150.548150 cuda_h.py:19] end sllm_worker_task cost 0.012418508529663086 seconds
INFO 01-15 16:10:36.550570.550570 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f765c397-95fd-422a-8f35-07c13dbe6ef7
DEBUG 01-15 16:10:36.550007.550007 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00565338134765625 seconds
DEBUG 01-15 16:10:36.550632.550632 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.553909.553909 cuda_h.py:19] end restore2model cost 0.0024871826171875 seconds
DEBUG 01-15 16:10:36.553169.553169 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008361339569091797 seconds
DEBUG 01-15 16:10:36.553487.553487 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.553133.553133 cuda_h.py:19] end gpu_sexperts cost 0.0002684593200683594 seconds
DEBUG 01-15 16:10:36.553055.553055 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.553163.553163 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4543533325195312e-05 seconds
DEBUG 01-15 16:10:36.553812.553812 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.553562.553562 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.553958.553958 cuda_h.py:19] end group_tensors cost 0.004950761795043945 seconds
DEBUG 01-15 16:10:36.554406.554406 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.554448.554448 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010492801666259766 seconds
DEBUG 01-15 16:10:36.554146.554146 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.555820.555820 cuda_h.py:19] end gpu_group_list cost 0.0003006458282470703 seconds
DEBUG 01-15 16:10:36.556636.556636 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.557585.557585 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013535022735595703 seconds
DEBUG 01-15 16:10:36.557126.557126 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.558692.558692 cuda_h.py:19] end group pad cost 0.0039463043212890625 seconds
DEBUG 01-15 16:10:36.558674.558674 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:36.558910.558910 cuda_h.py:19] end gpu_group_list cost 0.00025272369384765625 seconds
DEBUG 01-15 16:10:36.561757.561757 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.562588.562588 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f765c397-95fd-422a-8f35-07c13dbe6ef7
INFO 01-15 16:10:36.577747.577747 client.py:127] Model loaded
DEBUG 01-15 16:10:36.577170.577170 cuda_h.py:19] end wait_experts_multi_device cost 0.015445232391357422 seconds
DEBUG 01-15 16:10:36.577245.577245 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.578085.578085 cuda_h.py:19] end group_einsum cost 0.020693540573120117 seconds
DEBUG 01-15 16:10:36.579036.579036 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.583277.583277 cuda_h.py:19] end get_outputs_cpu1 cost 0.0041348934173583984 seconds
DEBUG 01-15 16:10:36.583021.583021 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03674459457397461 seconds
DEBUG 01-15 16:10:36.584287.584287 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006536245346069336 seconds
DEBUG 01-15 16:10:36.584948.584948 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.586944.586944 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.586659.586659 cuda_h.py:19] end index_scatter cost 8.7738037109375e-05 seconds
DEBUG 01-15 16:10:36.586405.586405 cuda_h.py:19] end cpuoutputsdeal cost 0.0021162033081054688 seconds
DEBUG 01-15 16:10:36.586898.586898 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.586237.586237 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.587761.587761 cuda_h.py:19] end gpu_group_tensor cost 0.00017380714416503906 seconds
DEBUG 01-15 16:10:36.587431.587431 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.587166.587166 cuda_h.py:19] end gpu_group_tensor cost 0.00015664100646972656 seconds
DEBUG 01-15 16:10:36.587044.587044 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.588915.588915 cuda_h.py:19] end gpu_group_einsum cost 0.0006589889526367188 seconds
DEBUG 01-15 16:10:36.588788.588788 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.591414.591414 cuda_h.py:19] end gpu_group_einsum cost 0.002803325653076172 seconds
DEBUG 01-15 16:10:36.591811.591811 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.591895.591895 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.591364.591364 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003032684326171875 seconds
DEBUG 01-15 16:10:36.591219.591219 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.591501.591501 cuda_h.py:19] end concat_expert_out cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:10:36.591940.591940 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.592189.592189 cuda_h.py:19] end index_scatter cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:10:36.592182.592182 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011327266693115234 seconds
DEBUG 01-15 16:10:36.592398.592398 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.592924.592924 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.592376.592376 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002257823944091797 seconds
DEBUG 01-15 16:10:36.592278.592278 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.593414.593414 cuda_h.py:19] end concat_expert_out cost 6.461143493652344e-05 seconds
DEBUG 01-15 16:10:36.593231.593231 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.593374.593374 cuda_h.py:19] end index_scatter cost 6.937980651855469e-05 seconds
DEBUG 01-15 16:10:36.593567.593567 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006456375122070312 seconds
DEBUG 01-15 16:10:36.593054.593054 cuda_h.py:19] end gpu_experts_multi_device cost 0.03983116149902344 seconds
DEBUG 01-15 16:10:36.593753.593753 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.0517730712890625 seconds
DEBUG 01-15 16:10:36.594538.594538 cuda_h.py:19] end prefill_layer cost 0.05865597724914551 seconds
DEBUG 01-15 16:10:36.594586.594586 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:10:36.594250.594250 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.594913.594913 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:10:36.594576.594576 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:36.594054.594054 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:36.594647.594647 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:10:36.594933.594933 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 9.131431579589844e-05 seconds
DEBUG 01-15 16:10:36.594351.594351 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.594062.594062 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.594992.594992 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.594603.594603 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.595413.595413 cuda_h.py:19] end allocate_cuda_memory cost 0.00041675567626953125 seconds
DEBUG 01-15 16:10:36.595351.595351 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.595724.595724 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.595702.595702 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.595446.595446 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.595586.595586 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e7b16261-e90d-4f2f-a6da-9e9beefbbee8
DEBUG 01-15 16:10:36.595471.595471 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.596207.596207 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.597263.597263 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e7b16261-e90d-4f2f-a6da-9e9beefbbee8
DEBUG 01-15 16:10:36.597053.597053 cuda_h.py:19] end load_into_gpu_async cost 0.0013856887817382812 seconds
DEBUG 01-15 16:10:36.597756.597756 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.597144.597144 cuda_h.py:19] end restore_tensors2 cost 8.225440979003906e-05 seconds
DEBUG 01-15 16:10:36.597284.597284 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0023584365844726562 seconds
INFO 01-15 16:10:36.597372.597372 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e7b16261-e90d-4f2f-a6da-9e9beefbbee8
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.600652.600652 cuda_h.py:19] end self_attn cost 0.004502058029174805 seconds
DEBUG 01-15 16:10:36.601598.601598 cuda_h.py:19] end iln_self_attn_paln cost 0.00657200813293457 seconds
DEBUG 01-15 16:10:36.601097.601097 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:10:36.601098.601098 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.602848.602848 cuda_h.py:19] end gate cost 0.0007977485656738281 seconds
DEBUG 01-15 16:10:36.602638.602638 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.602555.602555 lmp.py:1912] 
DEBUG 01-15 16:10:36.602555.602555 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.602762.602762 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.602227.602227 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.602923.602923 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.602996.602996 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.602593.602593 lmp.py:1917] 
DEBUG 01-15 16:10:36.602593.602593 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.602428.602428 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.603985.603985 lmp.py:1935]   Expert 58 |     33 | CPU
DEBUG 01-15 16:10:36.603820.603820 lmp.py:1935]   Expert 47 |     58 | CPU
DEBUG 01-15 16:10:36.603701.603701 lmp.py:1935]   Expert 31 |     59 | CPU
DEBUG 01-15 16:10:36.603344.603344 lmp.py:1935]   Expert 49 |     62 | CPU
DEBUG 01-15 16:10:36.603464.603464 lmp.py:1935]   Expert  4 |     64 | CPU
DEBUG 01-15 16:10:36.603346.603346 lmp.py:1935]   Expert 38 |     71 | CPU
DEBUG 01-15 16:10:36.603227.603227 lmp.py:1935]   Expert 45 |     74 | CPU
DEBUG 01-15 16:10:36.603539.603539 lmp.py:1935]   Expert 43 |     82 | CPU
DEBUG 01-15 16:10:36.603328.603328 lmp.py:1935]   Expert 41 |     83 | CPU
DEBUG 01-15 16:10:36.603686.603686 lmp.py:1935]   Expert 33 |     96 | CPU
DEBUG 01-15 16:10:36.603806.603806 lmp.py:1935]   Expert 57 |    105 | CPU
DEBUG 01-15 16:10:36.603687.603687 lmp.py:1935]   Expert 50 |    106 | CPU
DEBUG 01-15 16:10:36.603092.603092 lmp.py:1935]   Expert 11 |    108 | CPU
DEBUG 01-15 16:10:36.603973.603973 lmp.py:1935]   Expert  2 |    112 | CPU
DEBUG 01-15 16:10:36.603139.603139 lmp.py:1935]   Expert 51 |    116 | CPU
DEBUG 01-15 16:10:36.603259.603259 lmp.py:1935]   Expert  0 |    124 | CPU
DEBUG 01-15 16:10:36.603664.603664 lmp.py:1935]   Expert 14 |    124 | CPU
DEBUG 01-15 16:10:36.603260.603260 lmp.py:1935]   Expert 54 |    126 | CPU
DEBUG 01-15 16:10:36.603618.603618 lmp.py:1935]   Expert 26 |    141 | CPU
DEBUG 01-15 16:10:36.603500.603500 lmp.py:1935]   Expert 34 |    143 | CPU
DEBUG 01-15 16:10:36.603858.603858 lmp.py:1935]   Expert 56 |    147 | CPU
DEBUG 01-15 16:10:36.603739.603739 lmp.py:1935]   Expert 27 |    150 | CPU
DEBUG 01-15 16:10:36.603382.603382 lmp.py:1935]   Expert 55 |    157 | CPU
DEBUG 01-15 16:10:36.603025.603025 lmp.py:1935]   Expert 28 |    158 | CPU
DEBUG 01-15 16:10:36.603907.603907 lmp.py:1935]   Expert 10 |    164 | CPU
DEBUG 01-15 16:10:36.603550.603550 lmp.py:1935]   Expert 25 |    164 | CPU
DEBUG 01-15 16:10:36.603193.603193 lmp.py:1935]   Expert 13 |    179 | CPU
DEBUG 01-15 16:10:36.603597.603597 lmp.py:1935]   Expert  9 |    180 | CPU
DEBUG 01-15 16:10:36.603240.603240 lmp.py:1935]   Expert 61 |    190 | CPU
DEBUG 01-15 16:10:36.603360.603360 lmp.py:1935]   Expert  6 |    191 | CPU
DEBUG 01-15 16:10:36.603242.603242 lmp.py:1935]   Expert 48 |    191 | CPU
DEBUG 01-15 16:10:36.603123.603123 lmp.py:1935]   Expert  7 |    195 | CPU
DEBUG 01-15 16:10:36.603627.603627 lmp.py:1935]   Expert 46 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603939.603939 lmp.py:1935]   Expert 42 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603012.603012 lmp.py:1935]   Expert 24 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603847.603847 lmp.py:1935]   Expert 18 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603444.603444 lmp.py:1935]   Expert 40 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603802.603802 lmp.py:1935]   Expert 12 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603922.603922 lmp.py:1935]   Expert 63 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603280.603280 lmp.py:1935]   Expert 29 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603400.603400 lmp.py:1935]   Expert 59 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603758.603758 lmp.py:1935]   Expert 21 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603117.603117 lmp.py:1935]   Expert 22 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603713.603713 lmp.py:1935]   Expert 19 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603548.603548 lmp.py:1935]   Expert 32 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603860.603860 lmp.py:1935]   Expert 36 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603695.603695 lmp.py:1935]   Expert  3 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603530.603530 lmp.py:1935]   Expert 37 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603365.603365 lmp.py:1935]   Expert 16 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603724.603724 lmp.py:1935]   Expert  1 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603082.603082 lmp.py:1935]   Expert 20 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603632.603632 lmp.py:1935]   Expert  8 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.603182.603182 lmp.py:1935]   Expert  5 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.603256.603256 lmp.py:1935]   Expert 30 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.604806.604806 lmp.py:1935]   Expert 62 |    272 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.604549.604549 lmp.py:1935]   Expert 15 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.604053.604053 lmp.py:1935]   Expert 39 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.604318.604318 lmp.py:1935]   Expert 35 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.604584.604584 lmp.py:1935]   Expert 17 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.604373.604373 lmp.py:1935]   Expert 60 |    312 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.604923.604923 lmp.py:1935]   Expert 52 |    358 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.604235.604235 lmp.py:1935]   Expert 23 |    364 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.604547.604547 lmp.py:1935]   Expert 44 |    377 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.604620.604620 lmp.py:1935]   Expert 53 |    437 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.604978.604978 lmp.py:1937] 
DEBUG 01-15 16:10:36.604978.604978 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.604052.604052 lmp.py:1938]   CPU:   3953 tokens
DEBUG 01-15 16:10:36.604841.604841 lmp.py:1942]   cuda:1:   4167 tokens (16 experts)
DEBUG 01-15 16:10:36.604914.604914 lmp.py:1942]   cuda:2:   4168 tokens (16 experts)
DEBUG 01-15 16:10:36.604511.604511 lmp.py:1943]   Total GPU:   8335 tokens
DEBUG 01-15 16:10:36.604631.604631 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.604631.604631 lmp.py:1944] 
DEBUG 01-15 16:10:36.604426.604426 cuda_h.py:19] end experts_map_get cost 0.0019550323486328125 seconds
INFO 01-15 16:10:36.604608.604608 client.py:127] Model loaded
DEBUG 01-15 16:10:36.604630.604630 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.604199.604199 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.604313.604313 lmp.py:1953] 
DEBUG 01-15 16:10:36.604313.604313 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.604732.604732 cuda_h.py:19] end cpu_experts_submit cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:10:36.604097.604097 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.604933.604933 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.605102.605102 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.3.gate_proj.weight': 0, 'model.layers.16.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.52.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.52.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.52.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.16.mlp.experts.32.gate_proj.weight': 0, 'model.layers.16.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(19521863680, 5767168, 0, 0), (19527630848, 5767168, 5767168, 0), (19516096512, 5767168, 11534336, 0), (19556466688, 5767168, 17301504, 0), (19562233856, 5767168, 23068672, 0), (19550699520, 5767168, 28835840, 0), (20144717824, 5767168, 34603008, 0), (20150484992, 5767168, 40370176, 0), (20138950656, 5767168, 46137344, 0), (20162019328, 5767168, 51904512, 0), (20167786496, 5767168, 57671680, 0), (20156252160, 5767168, 63438848, 0), (20196622336, 5767168, 69206016, 0), (20202389504, 5767168, 74973184, 0), (20190855168, 5767168, 80740352, 0), (19746783232, 5767168, 86507520, 0), (19752550400, 5767168, 92274688, 0), (19741016064, 5767168, 98041856, 0), (19764084736, 5767168, 103809024, 0), (19769851904, 5767168, 109576192, 0), (19758317568, 5767168, 115343360, 0), (19798687744, 5767168, 121110528, 0), (19804454912, 5767168, 126877696, 0), (19792920576, 5767168, 132644864, 0), (20369637376, 5767168, 138412032, 0), (20375404544, 5767168, 144179200, 0), (20363870208, 5767168, 149946368, 0), (20386938880, 5767168, 155713536, 0), (20392706048, 5767168, 161480704, 0), (20381171712, 5767168, 167247872, 0), (19815989248, 5767168, 173015040, 0), (19821756416, 5767168, 178782208, 0), (19810222080, 5767168, 184549376, 0), (19850592256, 5767168, 190316544, 0), (19856359424, 5767168, 196083712, 0), (19844825088, 5767168, 201850880, 0), (19885195264, 5767168, 207618048, 0), (19890962432, 5767168, 213385216, 0), (19879428096, 5767168, 219152384, 0), (20490747904, 5767168, 224919552, 0), (20496515072, 5767168, 230686720, 0), (20484980736, 5767168, 236453888, 0), (20542652416, 5767168, 242221056, 0), (20548419584, 5767168, 247988224, 0), (20536885248, 5767168, 253755392, 0), (20559953920, 5767168, 259522560, 0), (20565721088, 5767168, 265289728, 0), (20554186752, 5767168, 271056896, 0)], 2: [(20023607296, 5767168, 0, 0), (20029374464, 5767168, 5767168, 0), (20017840128, 5767168, 11534336, 0), (19487260672, 5767168, 17301504, 0), (19493027840, 5767168, 23068672, 0), (19481493504, 5767168, 28835840, 0), (20075511808, 5767168, 34603008, 0), (20081278976, 5767168, 40370176, 0), (20069744640, 5767168, 46137344, 0), (20092813312, 5767168, 51904512, 0), (20098580480, 5767168, 57671680, 0), (20087046144, 5767168, 63438848, 0), (20110114816, 5767168, 69206016, 0), (20115881984, 5767168, 74973184, 0), (20104347648, 5767168, 80740352, 0), (19608371200, 5767168, 86507520, 0), (19614138368, 5767168, 92274688, 0), (19602604032, 5767168, 98041856, 0), (20231225344, 5767168, 103809024, 0), (20236992512, 5767168, 109576192, 0), (20225458176, 5767168, 115343360, 0), (19677577216, 5767168, 121110528, 0), (19683344384, 5767168, 126877696, 0), (19671810048, 5767168, 132644864, 0), (20265828352, 5767168, 138412032, 0), (20271595520, 5767168, 144179200, 0), (20260061184, 5767168, 149946368, 0), (19729481728, 5767168, 155713536, 0), (19735248896, 5767168, 161480704, 0), (19723714560, 5767168, 167247872, 0), (19781386240, 5767168, 173015040, 0), (19787153408, 5767168, 178782208, 0), (19775619072, 5767168, 184549376, 0), (19833290752, 5767168, 190316544, 0), (19839057920, 5767168, 196083712, 0), (19827523584, 5767168, 201850880, 0), (19867893760, 5767168, 207618048, 0), (19873660928, 5767168, 213385216, 0), (19862126592, 5767168, 219152384, 0), (20508049408, 5767168, 224919552, 0), (20513816576, 5767168, 230686720, 0), (20502282240, 5767168, 236453888, 0), (19971702784, 5767168, 242221056, 0), (19977469952, 5767168, 247988224, 0), (19965935616, 5767168, 253755392, 0), (19989004288, 5767168, 259522560, 0), (19994771456, 5767168, 265289728, 0), (19983237120, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e545c3c30>, 2: <capsule object NULL at 0x7a4e5420a0d0>}
DEBUG 01-15 16:10:36.606532.606532 cuda_h.py:19] end restore2model cost 0.0017864704132080078 seconds
DEBUG 01-15 16:10:36.606741.606741 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.606349.606349 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.606504.606504 cuda_h.py:19] end sllm_worker_task cost 0.011548042297363281 seconds
DEBUG 01-15 16:10:36.606460.606460 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.606582.606582 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, db800671-151e-4d3f-a062-39e5fe1d14a4
DEBUG 01-15 16:10:36.606467.606467 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.606722.606722 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.607531.607531 cuda_h.py:19] end move_flatidxs cost 0.0008547306060791016 seconds
DEBUG 01-15 16:10:36.607083.607083 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:36.607885.607885 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, db800671-151e-4d3f-a062-39e5fe1d14a4
DEBUG 01-15 16:10:36.608822.608822 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0035698413848876953 seconds
DEBUG 01-15 16:10:36.608123.608123 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.611699.611699 cuda_h.py:19] end restore2model cost 0.003125905990600586 seconds
DEBUG 01-15 16:10:36.611887.611887 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006946563720703125 seconds
DEBUG 01-15 16:10:36.611113.611113 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.612834.612834 cuda_h.py:19] end gpu_sexperts cost 0.00032258033752441406 seconds
DEBUG 01-15 16:10:36.612663.612663 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.612016.612016 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7642974853515625e-05 seconds
DEBUG 01-15 16:10:36.612381.612381 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.612229.612229 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.613333.613333 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001096963882446289 seconds
DEBUG 01-15 16:10:36.613706.613706 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.613607.613607 cuda_h.py:19] end gpu_group_list cost 0.00017714500427246094 seconds
DEBUG 01-15 16:10:36.614468.614468 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.615510.615510 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011982917785644531 seconds
DEBUG 01-15 16:10:36.615691.615691 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.616805.616805 cuda_h.py:19] end gpu_group_list cost 0.00017762184143066406 seconds
DEBUG 01-15 16:10:36.617461.617461 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.617343.617343 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, db800671-151e-4d3f-a062-39e5fe1d14a4
DEBUG 01-15 16:10:36.617136.617136 cuda_h.py:19] end group_tensors cost 0.010035991668701172 seconds
DEBUG 01-15 16:10:36.618577.618577 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.622575.622575 cuda_h.py:19] end group pad cost 0.0040531158447265625 seconds
DEBUG 01-15 16:10:36.622763.622763 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.634261.634261 client.py:127] Model loaded
DEBUG 01-15 16:10:36.634315.634315 cuda_h.py:19] end wait_experts_multi_device cost 0.017720937728881836 seconds
DEBUG 01-15 16:10:36.634436.634436 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.644377.644377 cuda_h.py:19] end group_einsum cost 0.0219881534576416 seconds
DEBUG 01-15 16:10:36.644726.644726 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.648204.648204 cuda_h.py:19] end get_outputs_cpu1 cost 0.003703594207763672 seconds
DEBUG 01-15 16:10:36.649032.649032 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04282951354980469 seconds
DEBUG 01-15 16:10:36.649277.649277 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.014946699142456055 seconds
DEBUG 01-15 16:10:36.649380.649380 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.651845.651845 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.651381.651381 cuda_h.py:19] end index_scatter cost 9.489059448242188e-05 seconds
DEBUG 01-15 16:10:36.652771.652771 cuda_h.py:19] end cpuoutputsdeal cost 0.002140045166015625 seconds
DEBUG 01-15 16:10:36.652032.652032 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.652132.652132 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.652611.652611 cuda_h.py:19] end gpu_group_tensor cost 0.00021147727966308594 seconds
DEBUG 01-15 16:10:36.652188.652188 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.652692.652692 cuda_h.py:19] end gpu_group_tensor cost 0.0001621246337890625 seconds
DEBUG 01-15 16:10:36.652716.652716 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.653458.653458 cuda_h.py:19] end gpu_group_einsum cost 0.0005648136138916016 seconds
DEBUG 01-15 16:10:36.653549.653549 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.654791.654791 cuda_h.py:19] end gpu_group_einsum cost 0.00044035911560058594 seconds
DEBUG 01-15 16:10:36.654497.654497 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.654931.654931 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.654591.654591 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002694129943847656 seconds
DEBUG 01-15 16:10:36.654652.654652 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.654728.654728 cuda_h.py:19] end concat_expert_out cost 5.602836608886719e-05 seconds
DEBUG 01-15 16:10:36.654485.654485 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.655720.655720 cuda_h.py:19] end index_scatter cost 6.699562072753906e-05 seconds
DEBUG 01-15 16:10:36.655941.655941 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009453296661376953 seconds
DEBUG 01-15 16:10:36.655845.655845 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.655318.655318 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.655981.655981 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020933151245117188 seconds
DEBUG 01-15 16:10:36.655214.655214 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.655966.655966 cuda_h.py:19] end concat_expert_out cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:10:36.655723.655723 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.656091.656091 cuda_h.py:19] end index_scatter cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:10:36.656761.656761 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006010532379150391 seconds
DEBUG 01-15 16:10:36.656068.656068 cuda_h.py:19] end gpu_experts_multi_device cost 0.04384589195251465 seconds
DEBUG 01-15 16:10:36.656045.656045 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.05477547645568848 seconds
DEBUG 01-15 16:10:36.656916.656916 cuda_h.py:19] end prefill_layer cost 0.06218147277832031 seconds
DEBUG 01-15 16:10:36.656766.656766 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:10:36.656806.656806 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.656847.656847 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:10:36.656841.656841 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:36.656458.656458 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:36.656030.656030 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 3.981590270996094e-05 seconds
DEBUG 01-15 16:10:36.657886.657886 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:10:36.657350.657350 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.657320.657320 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.657033.657033 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.657717.657717 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.657185.657185 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.657116.657116 cuda_h.py:19] end allocate_cuda_memory cost 0.00025844573974609375 seconds
DEBUG 01-15 16:10:36.657801.657801 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.657663.657663 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.657446.657446 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.657202.657202 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8121cf23-9a61-4242-b8b8-869b65e10ebe
DEBUG 01-15 16:10:36.658305.658305 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.658674.658674 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.659585.659585 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8121cf23-9a61-4242-b8b8-869b65e10ebe
DEBUG 01-15 16:10:36.659998.659998 cuda_h.py:19] end load_into_gpu_async cost 0.0012998580932617188 seconds
DEBUG 01-15 16:10:36.659800.659800 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.659864.659864 cuda_h.py:19] end restore_tensors2 cost 8.606910705566406e-05 seconds
DEBUG 01-15 16:10:36.659388.659388 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0019290447235107422 seconds
INFO 01-15 16:10:36.659113.659113 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8121cf23-9a61-4242-b8b8-869b65e10ebe
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.662655.662655 cuda_h.py:19] end self_attn cost 0.004513263702392578 seconds
DEBUG 01-15 16:10:36.663019.663019 cuda_h.py:19] end iln_self_attn_paln cost 0.006285905838012695 seconds
DEBUG 01-15 16:10:36.663955.663955 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:10:36.663525.663525 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.664929.664929 cuda_h.py:19] end gate cost 0.0007512569427490234 seconds
DEBUG 01-15 16:10:36.664666.664666 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.664597.664597 lmp.py:1912] 
DEBUG 01-15 16:10:36.664597.664597 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.664691.664691 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.664533.664533 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.664037.664037 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.664587.664587 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.664184.664184 lmp.py:1917] 
DEBUG 01-15 16:10:36.664184.664184 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.664019.664019 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.664291.664291 lmp.py:1935]   Expert  4 |      9 | CPU
DEBUG 01-15 16:10:36.664365.664365 lmp.py:1935]   Expert 28 |     28 | CPU
DEBUG 01-15 16:10:36.664246.664246 lmp.py:1935]   Expert  7 |     46 | CPU
DEBUG 01-15 16:10:36.665889.665889 lmp.py:1935]   Expert 53 |     57 | CPU
DEBUG 01-15 16:10:36.665294.665294 lmp.py:1935]   Expert 52 |     68 | CPU
DEBUG 01-15 16:10:36.665937.665937 lmp.py:1935]   Expert 43 |     72 | CPU
DEBUG 01-15 16:10:36.665818.665818 lmp.py:1935]   Expert 49 |     84 | CPU
DEBUG 01-15 16:10:36.665845.665845 lmp.py:1935]   Expert 12 |     87 | CPU
DEBUG 01-15 16:10:36.665157.665157 lmp.py:1935]   Expert 47 |    103 | CPU
DEBUG 01-15 16:10:36.665946.665946 lmp.py:1935]   Expert 24 |    107 | CPU
DEBUG 01-15 16:10:36.665496.665496 lmp.py:1935]   Expert 33 |    108 | CPU
DEBUG 01-15 16:10:36.665570.665570 lmp.py:1935]   Expert 50 |    109 | CPU
DEBUG 01-15 16:10:36.665643.665643 lmp.py:1935]   Expert  2 |    111 | CPU
DEBUG 01-15 16:10:36.665193.665193 lmp.py:1935]   Expert 60 |    111 | CPU
DEBUG 01-15 16:10:36.665267.665267 lmp.py:1935]   Expert 15 |    112 | CPU
DEBUG 01-15 16:10:36.665625.665625 lmp.py:1935]   Expert 39 |    114 | CPU
DEBUG 01-15 16:10:36.665745.665745 lmp.py:1935]   Expert 36 |    118 | CPU
DEBUG 01-15 16:10:36.665342.665342 lmp.py:1935]   Expert 25 |    122 | CPU
DEBUG 01-15 16:10:36.665461.665461 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:10:36.665820.665820 lmp.py:1935]   Expert 61 |    131 | CPU
DEBUG 01-15 16:10:36.665939.665939 lmp.py:1935]   Expert 59 |    135 | CPU
DEBUG 01-15 16:10:36.665298.665298 lmp.py:1935]   Expert  3 |    141 | CPU
DEBUG 01-15 16:10:36.665610.665610 lmp.py:1935]   Expert 27 |    144 | CPU
DEBUG 01-15 16:10:36.665683.665683 lmp.py:1935]   Expert 58 |    146 | CPU
DEBUG 01-15 16:10:36.665518.665518 lmp.py:1935]   Expert  8 |    148 | CPU
DEBUG 01-15 16:10:36.665353.665353 lmp.py:1935]   Expert 30 |    151 | CPU
DEBUG 01-15 16:10:36.665950.665950 lmp.py:1935]   Expert 31 |    153 | CPU
DEBUG 01-15 16:10:36.665547.665547 lmp.py:1935]   Expert 38 |    155 | CPU
DEBUG 01-15 16:10:36.665382.665382 lmp.py:1935]   Expert 10 |    156 | CPU
DEBUG 01-15 16:10:36.665740.665740 lmp.py:1935]   Expert 57 |    160 | CPU
DEBUG 01-15 16:10:36.665860.665860 lmp.py:1935]   Expert 40 |    161 | CPU
DEBUG 01-15 16:10:36.665456.665456 lmp.py:1935]   Expert 14 |    162 | CPU
DEBUG 01-15 16:10:36.665245.665245 lmp.py:1935]   Expert 41 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665987.665987 lmp.py:1935]   Expert 32 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665015.665015 lmp.py:1935]   Expert 37 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665803.665803 lmp.py:1935]   Expert 46 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665122.665122 lmp.py:1935]   Expert 54 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665388.665388 lmp.py:1935]   Expert 42 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665461.665461 lmp.py:1935]   Expert 19 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665773.665773 lmp.py:1935]   Expert 11 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665323.665323 lmp.py:1935]   Expert 34 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665397.665397 lmp.py:1935]   Expert 22 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665709.665709 lmp.py:1935]   Expert 18 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665305.665305 lmp.py:1935]   Expert 26 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665664.665664 lmp.py:1935]   Expert 56 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665783.665783 lmp.py:1935]   Expert  0 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665903.665903 lmp.py:1935]   Expert 44 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665261.665261 lmp.py:1935]   Expert  1 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665381.665381 lmp.py:1935]   Expert 51 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665216.665216 lmp.py:1935]   Expert 20 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665767.665767 lmp.py:1935]   Expert 29 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665555.665555 lmp.py:1935]   Expert 48 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665867.665867 lmp.py:1935]   Expert 45 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.665941.665941 lmp.py:1935]   Expert 21 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.665537.665537 lmp.py:1935]   Expert 35 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.666611.666611 lmp.py:1935]   Expert 55 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.666684.666684 lmp.py:1935]   Expert 16 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.666281.666281 lmp.py:1935]   Expert  5 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.666070.666070 lmp.py:1935]   Expert 23 |    372 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.666905.666905 lmp.py:1935]   Expert 13 |    382 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.666502.666502 lmp.py:1935]   Expert 17 |    436 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.666860.666860 lmp.py:1935]   Expert  9 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.666980.666980 lmp.py:1935]   Expert 63 |    460 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.666099.666099 lmp.py:1935]   Expert 62 |   1182 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.666504.666504 lmp.py:1937] 
DEBUG 01-15 16:10:36.666504.666504 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.666385.666385 lmp.py:1938]   CPU:   3636 tokens
DEBUG 01-15 16:10:36.666267.666267 lmp.py:1942]   cuda:1:   4287 tokens (15 experts)
DEBUG 01-15 16:10:36.666009.666009 lmp.py:1942]   cuda:2:   4365 tokens (17 experts)
DEBUG 01-15 16:10:36.666844.666844 lmp.py:1943]   Total GPU:   8652 tokens
DEBUG 01-15 16:10:36.666679.666679 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.666679.666679 lmp.py:1944] 
DEBUG 01-15 16:10:36.666428.666428 cuda_h.py:19] end experts_map_get cost 0.0019719600677490234 seconds
INFO 01-15 16:10:36.666808.666808 client.py:127] Model loaded
DEBUG 01-15 16:10:36.666499.666499 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.666193.666193 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.666261.666261 lmp.py:1953] 
DEBUG 01-15 16:10:36.666261.666261 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.666766.666766 cuda_h.py:19] end cpu_experts_submit cost 5.626678466796875e-05 seconds
DEBUG 01-15 16:10:36.666416.666416 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.667334.667334 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.667168.667168 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.667925.667925 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.62.up_proj.weight': 253755392}, 2: {'model.layers.17.mlp.experts.32.gate_proj.weight': 0, 'model.layers.17.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.41.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.41.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.41.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.19.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.19.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.19.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.29.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.63.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.63.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.63.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (20594556928, 5767168, 17301504, 0), (20600324096, 5767168, 23068672, 0), (20588789760, 5767168, 28835840, 0), (20663762944, 5767168, 34603008, 0), (20669530112, 5767168, 40370176, 0), (20657995776, 5767168, 46137344, 0), (21217411072, 5767168, 51904512, 0), (21223178240, 5767168, 57671680, 0), (21211643904, 5767168, 63438848, 0), (21303918592, 5767168, 69206016, 0), (21309685760, 5767168, 74973184, 0), (21298151424, 5767168, 80740352, 0), (20767571968, 5767168, 86507520, 0), (20773339136, 5767168, 92274688, 0), (20761804800, 5767168, 98041856, 0), (20802174976, 5767168, 103809024, 0), (20807942144, 5767168, 109576192, 0), (20796407808, 5767168, 115343360, 0), (21407727616, 5767168, 121110528, 0), (21413494784, 5767168, 126877696, 0), (21401960448, 5767168, 132644864, 0), (20923285504, 5767168, 138412032, 0), (20929052672, 5767168, 144179200, 0), (20917518336, 5767168, 149946368, 0), (20940587008, 5767168, 155713536, 0), (20946354176, 5767168, 161480704, 0), (20934819840, 5767168, 167247872, 0), (20957888512, 5767168, 173015040, 0), (20963655680, 5767168, 178782208, 0), (20952121344, 5767168, 184549376, 0), (21528838144, 5767168, 190316544, 0), (21534605312, 5767168, 196083712, 0), (21523070976, 5767168, 201850880, 0), (21511536640, 5767168, 207618048, 0), (21517303808, 5767168, 213385216, 0), (21505769472, 5767168, 219152384, 0), (21027094528, 5767168, 224919552, 0), (21032861696, 5767168, 230686720, 0), (21021327360, 5767168, 236453888, 0), (21649948672, 5767168, 242221056, 0), (21655715840, 5767168, 247988224, 0), (21644181504, 5767168, 253755392, 0)], 2: [(21130903552, 5767168, 0, 0), (21136670720, 5767168, 5767168, 0), (21125136384, 5767168, 11534336, 0), (21165506560, 5767168, 17301504, 0), (21171273728, 5767168, 23068672, 0), (21159739392, 5767168, 28835840, 0), (21182808064, 5767168, 34603008, 0), (21188575232, 5767168, 40370176, 0), (21177040896, 5767168, 46137344, 0), (20732968960, 5767168, 51904512, 0), (20738736128, 5767168, 57671680, 0), (20727201792, 5767168, 63438848, 0), (21286617088, 5767168, 69206016, 0), (21292384256, 5767168, 74973184, 0), (21280849920, 5767168, 80740352, 0), (21338521600, 5767168, 86507520, 0), (21344288768, 5767168, 92274688, 0), (21332754432, 5767168, 98041856, 0), (21355823104, 5767168, 103809024, 0), (21361590272, 5767168, 109576192, 0), (21350055936, 5767168, 115343360, 0), (21373124608, 5767168, 121110528, 0), (21378891776, 5767168, 126877696, 0), (21367357440, 5767168, 132644864, 0), (20854079488, 5767168, 138412032, 0), (20859846656, 5767168, 144179200, 0), (20848312320, 5767168, 149946368, 0), (20871380992, 5767168, 155713536, 0), (20877148160, 5767168, 161480704, 0), (20865613824, 5767168, 167247872, 0), (20888682496, 5767168, 173015040, 0), (20894449664, 5767168, 178782208, 0), (20882915328, 5767168, 184549376, 0), (21459632128, 5767168, 190316544, 0), (21465399296, 5767168, 196083712, 0), (21453864960, 5767168, 201850880, 0), (20905984000, 5767168, 207618048, 0), (20911751168, 5767168, 213385216, 0), (20900216832, 5767168, 219152384, 0), (20975190016, 5767168, 224919552, 0), (20980957184, 5767168, 230686720, 0), (20969422848, 5767168, 236453888, 0), (21546139648, 5767168, 242221056, 0), (21551906816, 5767168, 247988224, 0), (21540372480, 5767168, 253755392, 0), (21078999040, 5767168, 259522560, 0), (21084766208, 5767168, 265289728, 0), (21073231872, 5767168, 271056896, 0), (21667250176, 5767168, 276824064, 0), (21673017344, 5767168, 282591232, 0), (21661483008, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e54307d80>, 2: <capsule object NULL at 0x7a4ec4743e10>}
DEBUG 01-15 16:10:36.667967.667967 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.668841.668841 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.668368.668368 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.668224.668224 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a9ca6a31-03b5-4fd7-8c04-a2a907dbe45b
DEBUG 01-15 16:10:36.668827.668827 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.668482.668482 cuda_h.py:19] end move_flatidxs cost 0.0008676052093505859 seconds
DEBUG 01-15 16:10:36.668377.668377 cuda_h.py:19] end restore2model cost 0.0022618770599365234 seconds
DEBUG 01-15 16:10:36.668371.668371 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.668253.668253 cuda_h.py:19] end sllm_worker_task cost 0.011468172073364258 seconds
INFO 01-15 16:10:36.669139.669139 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a9ca6a31-03b5-4fd7-8c04-a2a907dbe45b
DEBUG 01-15 16:10:36.669354.669354 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0029230117797851562 seconds
DEBUG 01-15 16:10:36.670363.670363 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.673912.673912 cuda_h.py:19] end restore2model cost 0.003106355667114258 seconds
DEBUG 01-15 16:10:36.673576.673576 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0062825679779052734 seconds
DEBUG 01-15 16:10:36.673041.673041 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.673238.673238 cuda_h.py:19] end gpu_sexperts cost 0.0003159046173095703 seconds
DEBUG 01-15 16:10:36.673783.673783 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.673613.673613 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8358230590820312e-05 seconds
DEBUG 01-15 16:10:36.673693.673693 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.673303.673303 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.674564.674564 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010364055633544922 seconds
DEBUG 01-15 16:10:36.674891.674891 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.675970.675970 cuda_h.py:19] end gpu_group_list cost 0.00016832351684570312 seconds
DEBUG 01-15 16:10:36.676948.676948 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.677462.677462 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012280941009521484 seconds
DEBUG 01-15 16:10:36.677504.677504 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.677418.677418 cuda_h.py:19] end gpu_group_list cost 0.00018548965454101562 seconds
DEBUG 01-15 16:10:36.678731.678731 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.678852.678852 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a9ca6a31-03b5-4fd7-8c04-a2a907dbe45b
DEBUG 01-15 16:10:36.679653.679653 cuda_h.py:19] end group_tensors cost 0.010337352752685547 seconds
DEBUG 01-15 16:10:36.679838.679838 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.683902.683902 cuda_h.py:19] end group pad cost 0.004032611846923828 seconds
DEBUG 01-15 16:10:36.684553.684553 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.700633.700633 client.py:127] Model loaded
DEBUG 01-15 16:10:36.700581.700581 cuda_h.py:19] end wait_experts_multi_device cost 0.022348403930664062 seconds
DEBUG 01-15 16:10:36.701287.701287 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.703117.703117 cuda_h.py:19] end group_einsum cost 0.019153594970703125 seconds
DEBUG 01-15 16:10:36.703698.703698 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.707183.707183 cuda_h.py:19] end get_outputs_cpu1 cost 0.0035169124603271484 seconds
DEBUG 01-15 16:10:36.707329.707329 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04013371467590332 seconds
DEBUG 01-15 16:10:36.708850.708850 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007038593292236328 seconds
DEBUG 01-15 16:10:36.708455.708455 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.709735.709735 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.710820.710820 cuda_h.py:19] end index_scatter cost 8.726119995117188e-05 seconds
DEBUG 01-15 16:10:36.710367.710367 cuda_h.py:19] end cpuoutputsdeal cost 0.0020678043365478516 seconds
DEBUG 01-15 16:10:36.710052.710052 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.710914.710914 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.710762.710762 cuda_h.py:19] end gpu_group_tensor cost 0.0001685619354248047 seconds
DEBUG 01-15 16:10:36.710717.710717 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.710452.710452 cuda_h.py:19] end gpu_group_tensor cost 0.00015735626220703125 seconds
DEBUG 01-15 16:10:36.711800.711800 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.712014.712014 cuda_h.py:19] end gpu_group_einsum cost 0.0011916160583496094 seconds
DEBUG 01-15 16:10:36.712953.712953 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.713340.713340 cuda_h.py:19] end gpu_group_einsum cost 0.0006077289581298828 seconds
DEBUG 01-15 16:10:36.713875.713875 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.713959.713959 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.713322.713322 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003254413604736328 seconds
DEBUG 01-15 16:10:36.713986.713986 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.713154.713154 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:36.713243.713243 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.714663.714663 cuda_h.py:19] end index_scatter cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:10:36.714877.714877 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009982585906982422 seconds
DEBUG 01-15 16:10:36.714874.714874 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.714817.714817 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.714546.714546 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018715858459472656 seconds
DEBUG 01-15 16:10:36.714448.714448 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.714762.714762 cuda_h.py:19] end concat_expert_out cost 5.841255187988281e-05 seconds
DEBUG 01-15 16:10:36.714896.714896 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.714787.714787 cuda_h.py:19] end index_scatter cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:10:36.715371.715371 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005731582641601562 seconds
DEBUG 01-15 16:10:36.715394.715394 cuda_h.py:19] end gpu_experts_multi_device cost 0.04129338264465332 seconds
DEBUG 01-15 16:10:36.715463.715463 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.051748037338256836 seconds
DEBUG 01-15 16:10:36.715949.715949 cuda_h.py:19] end prefill_layer cost 0.058814048767089844 seconds
DEBUG 01-15 16:10:36.715600.715600 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:10:36.715687.715687 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.715443.715443 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:10:36.715722.715722 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:36.715101.715101 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:36.715911.715911 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 4.0531158447265625e-05 seconds
DEBUG 01-15 16:10:36.715098.715098 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 7.510185241699219e-05 seconds
DEBUG 01-15 16:10:36.715800.715800 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.716439.716439 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.716223.716223 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.716723.716723 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.716798.716798 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.716495.716495 cuda_h.py:19] end allocate_cuda_memory cost 0.00022912025451660156 seconds
DEBUG 01-15 16:10:36.716710.716710 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.716573.716573 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.716640.716640 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.716635.716635 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5cc6c983-343b-47d8-8c3f-1040855468aa
DEBUG 01-15 16:10:36.716546.716546 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.717167.717167 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.717534.717534 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5cc6c983-343b-47d8-8c3f-1040855468aa
DEBUG 01-15 16:10:36.717093.717093 cuda_h.py:19] end load_into_gpu_async cost 0.0012958049774169922 seconds
DEBUG 01-15 16:10:36.718134.718134 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.718005.718005 cuda_h.py:19] end restore_tensors2 cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:10:36.718529.718529 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.001897573471069336 seconds
INFO 01-15 16:10:36.718247.718247 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5cc6c983-343b-47d8-8c3f-1040855468aa
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.721969.721969 cuda_h.py:19] end self_attn cost 0.004482746124267578 seconds
DEBUG 01-15 16:10:36.722650.722650 cuda_h.py:19] end iln_self_attn_paln cost 0.006136894226074219 seconds
DEBUG 01-15 16:10:36.722241.722241 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:10:36.722858.722858 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.722929.722929 cuda_h.py:19] end gate cost 0.0007193088531494141 seconds
DEBUG 01-15 16:10:36.723759.723759 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.723172.723172 lmp.py:1912] 
DEBUG 01-15 16:10:36.723172.723172 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.723597.723597 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.723677.723677 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.723420.723420 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.723778.723778 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.723613.723613 lmp.py:1917] 
DEBUG 01-15 16:10:36.723613.723613 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.723210.723210 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.723290.723290 lmp.py:1935]   Expert 32 |     34 | CPU
DEBUG 01-15 16:10:36.723887.723887 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:10:36.723007.723007 lmp.py:1935]   Expert  5 |     54 | CPU
DEBUG 01-15 16:10:36.723365.723365 lmp.py:1935]   Expert 46 |     73 | CPU
DEBUG 01-15 16:10:36.723769.723769 lmp.py:1935]   Expert 40 |     89 | CPU
DEBUG 01-15 16:10:36.723697.723697 lmp.py:1935]   Expert  8 |     90 | CPU
DEBUG 01-15 16:10:36.723102.723102 lmp.py:1935]   Expert 12 |    100 | CPU
DEBUG 01-15 16:10:36.723506.723506 lmp.py:1935]   Expert 17 |    109 | CPU
DEBUG 01-15 16:10:36.723864.723864 lmp.py:1935]   Expert 60 |    112 | CPU
DEBUG 01-15 16:10:36.723984.723984 lmp.py:1935]   Expert 27 |    113 | CPU
DEBUG 01-15 16:10:36.723011.723011 lmp.py:1935]   Expert 58 |    115 | CPU
DEBUG 01-15 16:10:36.723085.723085 lmp.py:1935]   Expert 21 |    116 | CPU
DEBUG 01-15 16:10:36.723920.723920 lmp.py:1935]   Expert  3 |    117 | CPU
DEBUG 01-15 16:10:36.723040.723040 lmp.py:1935]   Expert 28 |    120 | CPU
DEBUG 01-15 16:10:36.723921.723921 lmp.py:1935]   Expert 29 |    121 | CPU
DEBUG 01-15 16:10:36.723802.723802 lmp.py:1935]   Expert 25 |    125 | CPU
DEBUG 01-15 16:10:36.723922.723922 lmp.py:1935]   Expert 41 |    126 | CPU
DEBUG 01-15 16:10:36.723804.723804 lmp.py:1935]   Expert 35 |    131 | CPU
DEBUG 01-15 16:10:36.723923.723923 lmp.py:1935]   Expert 19 |    136 | CPU
DEBUG 01-15 16:10:36.724282.724282 lmp.py:1935]   Expert  0 |    144 | CPU
DEBUG 01-15 16:10:36.724117.724117 lmp.py:1935]   Expert  6 |    146 | CPU
DEBUG 01-15 16:10:36.724475.724475 lmp.py:1935]   Expert 52 |    147 | CPU
DEBUG 01-15 16:10:36.724549.724549 lmp.py:1935]   Expert 56 |    148 | CPU
DEBUG 01-15 16:10:36.724907.724907 lmp.py:1935]   Expert 54 |    152 | CPU
DEBUG 01-15 16:10:36.724788.724788 lmp.py:1935]   Expert 37 |    153 | CPU
DEBUG 01-15 16:10:36.724431.724431 lmp.py:1935]   Expert 53 |    154 | CPU
DEBUG 01-15 16:10:36.724313.724313 lmp.py:1935]   Expert 63 |    157 | CPU
DEBUG 01-15 16:10:36.724194.724194 lmp.py:1935]   Expert 48 |    159 | CPU
DEBUG 01-15 16:10:36.724837.724837 lmp.py:1935]   Expert 36 |    164 | CPU
DEBUG 01-15 16:10:36.724480.724480 lmp.py:1935]   Expert 59 |    167 | CPU
DEBUG 01-15 16:10:36.724838.724838 lmp.py:1935]   Expert  9 |    183 | CPU
DEBUG 01-15 16:10:36.724958.724958 lmp.py:1935]   Expert  1 |    186 | CPU
DEBUG 01-15 16:10:36.724223.724223 lmp.py:1935]   Expert 39 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724774.724774 lmp.py:1935]   Expert 20 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724801.724801 lmp.py:1935]   Expert 43 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724113.724113 lmp.py:1935]   Expert 61 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724186.724186 lmp.py:1935]   Expert 11 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724498.724498 lmp.py:1935]   Expert  7 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724333.724333 lmp.py:1935]   Expert 42 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724189.724189 lmp.py:1935]   Expert 34 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724097.724097 lmp.py:1935]   Expert 47 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724078.724078 lmp.py:1935]   Expert 55 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724105.724105 lmp.py:1935]   Expert 13 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724656.724656 lmp.py:1935]   Expert 16 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724206.724206 lmp.py:1935]   Expert 57 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724525.724525 lmp.py:1935]   Expert 18 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724075.724075 lmp.py:1935]   Expert 15 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724148.724148 lmp.py:1935]   Expert  4 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724222.724222 lmp.py:1935]   Expert 22 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724580.724580 lmp.py:1935]   Expert 33 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724177.724177 lmp.py:1935]   Expert 50 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724012.724012 lmp.py:1935]   Expert 45 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724370.724370 lmp.py:1935]   Expert 31 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724728.724728 lmp.py:1935]   Expert 51 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724325.724325 lmp.py:1935]   Expert 49 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724445.724445 lmp.py:1935]   Expert 38 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724041.724041 lmp.py:1935]   Expert 26 |    283 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724830.724830 lmp.py:1935]   Expert 10 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724904.724904 lmp.py:1935]   Expert 44 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724692.724692 lmp.py:1935]   Expert  2 |    305 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724004.724004 lmp.py:1935]   Expert 24 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724316.724316 lmp.py:1935]   Expert 14 |    313 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724390.724390 lmp.py:1935]   Expert 23 |    402 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.724748.724748 lmp.py:1935]   Expert 62 |    671 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.724676.724676 lmp.py:1937] 
DEBUG 01-15 16:10:36.724676.724676 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.724795.724795 lmp.py:1938]   CPU:   3993 tokens
DEBUG 01-15 16:10:36.724346.724346 lmp.py:1942]   cuda:1:   4054 tokens (15 experts)
DEBUG 01-15 16:10:36.724419.724419 lmp.py:1942]   cuda:2:   4241 tokens (17 experts)
DEBUG 01-15 16:10:36.724539.724539 lmp.py:1943]   Total GPU:   8295 tokens
DEBUG 01-15 16:10:36.724659.724659 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.724659.724659 lmp.py:1944] 
DEBUG 01-15 16:10:36.725408.725408 cuda_h.py:19] end experts_map_get cost 0.0019593238830566406 seconds
INFO 01-15 16:10:36.725073.725073 client.py:127] Model loaded
DEBUG 01-15 16:10:36.725717.725717 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.725125.725125 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.725524.725524 lmp.py:1953] 
DEBUG 01-15 16:10:36.725524.725524 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.725268.725268 cuda_h.py:19] end cpu_experts_submit cost 5.698204040527344e-05 seconds
DEBUG 01-15 16:10:36.725156.725156 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.725323.725323 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.726609.726609 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.2.gate_proj.weight': 0, 'model.layers.18.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.42.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.42.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.42.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.43.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.43.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.43.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.13.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.13.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.13.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.15.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.15.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.15.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.57.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.57.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.57.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.61.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.61.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.61.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.62.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.62.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.62.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.31.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.31.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.31.up_proj.weight': 253755392}, 2: {'model.layers.18.mlp.experts.33.gate_proj.weight': 0, 'model.layers.18.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.38.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.38.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.38.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.39.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.39.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.39.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.10.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.10.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.10.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.11.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.11.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.11.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.45.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.45.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.45.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.14.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.14.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.14.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.51.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.51.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.51.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.20.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.20.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.20.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.23.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.23.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.23.up_proj.weight': 271056896, 'model.layers.18.mlp.experts.24.gate_proj.weight': 276824064, 'model.layers.18.mlp.experts.24.down_proj.weight': 282591232, 'model.layers.18.mlp.experts.24.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(21719154688, 5767168, 0, 0), (21724921856, 5767168, 5767168, 0), (21713387520, 5767168, 11534336, 0), (22411214848, 5767168, 17301504, 0), (22416982016, 5767168, 23068672, 0), (22405447680, 5767168, 28835840, 0), (22428516352, 5767168, 34603008, 0), (22434283520, 5767168, 40370176, 0), (22422749184, 5767168, 46137344, 0), (22445817856, 5767168, 51904512, 0), (22451585024, 5767168, 57671680, 0), (22440050688, 5767168, 63438848, 0), (21909471232, 5767168, 69206016, 0), (21915238400, 5767168, 74973184, 0), (21903704064, 5767168, 80740352, 0), (21944074240, 5767168, 86507520, 0), (21949841408, 5767168, 92274688, 0), (21938307072, 5767168, 98041856, 0), (22497722368, 5767168, 103809024, 0), (22503489536, 5767168, 109576192, 0), (22491955200, 5767168, 115343360, 0), (22532325376, 5767168, 121110528, 0), (22538092544, 5767168, 126877696, 0), (22526558208, 5767168, 132644864, 0), (22549626880, 5767168, 138412032, 0), (22555394048, 5767168, 144179200, 0), (22543859712, 5767168, 149946368, 0), (22065184768, 5767168, 155713536, 0), (22070951936, 5767168, 161480704, 0), (22059417600, 5767168, 167247872, 0), (22670737408, 5767168, 173015040, 0), (22676504576, 5767168, 178782208, 0), (22664970240, 5767168, 184549376, 0), (22134390784, 5767168, 190316544, 0), (22140157952, 5767168, 196083712, 0), (22128623616, 5767168, 201850880, 0), (22739943424, 5767168, 207618048, 0), (22745710592, 5767168, 213385216, 0), (22734176256, 5767168, 219152384, 0), (22757244928, 5767168, 224919552, 0), (22763012096, 5767168, 230686720, 0), (22751477760, 5767168, 236453888, 0), (22220898304, 5767168, 242221056, 0), (22226665472, 5767168, 247988224, 0), (22215131136, 5767168, 253755392, 0)], 2: [(22255501312, 5767168, 0, 0), (22261268480, 5767168, 5767168, 0), (22249734144, 5767168, 11534336, 0), (22272802816, 5767168, 17301504, 0), (22278569984, 5767168, 23068672, 0), (22267035648, 5767168, 28835840, 0), (21753757696, 5767168, 34603008, 0), (21759524864, 5767168, 40370176, 0), (21747990528, 5767168, 46137344, 0), (22342008832, 5767168, 51904512, 0), (22347776000, 5767168, 57671680, 0), (22336241664, 5767168, 63438848, 0), (21805662208, 5767168, 69206016, 0), (21811429376, 5767168, 74973184, 0), (21799895040, 5767168, 80740352, 0), (22359310336, 5767168, 86507520, 0), (22365077504, 5767168, 92274688, 0), (22353543168, 5767168, 98041856, 0), (21857566720, 5767168, 103809024, 0), (21863333888, 5767168, 109576192, 0), (21851799552, 5767168, 115343360, 0), (21874868224, 5767168, 121110528, 0), (21880635392, 5767168, 126877696, 0), (21869101056, 5767168, 132644864, 0), (22463119360, 5767168, 138412032, 0), (22468886528, 5767168, 144179200, 0), (22457352192, 5767168, 149946368, 0), (21926772736, 5767168, 155713536, 0), (21932539904, 5767168, 161480704, 0), (21921005568, 5767168, 167247872, 0), (21961375744, 5767168, 173015040, 0), (21967142912, 5767168, 178782208, 0), (21955608576, 5767168, 184549376, 0), (21995978752, 5767168, 190316544, 0), (22001745920, 5767168, 196083712, 0), (21990211584, 5767168, 201850880, 0), (22566928384, 5767168, 207618048, 0), (22572695552, 5767168, 213385216, 0), (22561161216, 5767168, 219152384, 0), (22030581760, 5767168, 224919552, 0), (22036348928, 5767168, 230686720, 0), (22024814592, 5767168, 236453888, 0), (22636134400, 5767168, 242221056, 0), (22641901568, 5767168, 247988224, 0), (22630367232, 5767168, 253755392, 0), (22082486272, 5767168, 259522560, 0), (22088253440, 5767168, 265289728, 0), (22076719104, 5767168, 271056896, 0), (22099787776, 5767168, 276824064, 0), (22105554944, 5767168, 282591232, 0), (22094020608, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e5420a100>, 2: <capsule object NULL at 0x7a4e5420a310>}
DEBUG 01-15 16:10:36.726859.726859 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.726500.726500 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.726071.726071 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 18d5caf6-8c4d-432b-a9a4-d1f7ed2abe9e
DEBUG 01-15 16:10:36.726286.726286 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.727236.727236 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.727633.727633 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.727786.727786 cuda_h.py:19] end restore2model cost 0.002011537551879883 seconds
DEBUG 01-15 16:10:36.727377.727377 cuda_h.py:19] end sllm_worker_task cost 0.01102304458618164 seconds
INFO 01-15 16:10:36.728309.728309 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 18d5caf6-8c4d-432b-a9a4-d1f7ed2abe9e
DEBUG 01-15 16:10:36.728392.728392 cuda_h.py:19] end move_flatidxs cost 0.0008463859558105469 seconds
DEBUG 01-15 16:10:36.728043.728043 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.728524.728524 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002991914749145508 seconds
DEBUG 01-15 16:10:36.728248.728248 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.731623.731623 cuda_h.py:19] end restore2model cost 0.003047943115234375 seconds
DEBUG 01-15 16:10:36.731943.731943 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006278514862060547 seconds
DEBUG 01-15 16:10:36.731215.731215 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.732459.732459 cuda_h.py:19] end gpu_sexperts cost 0.0003218650817871094 seconds
DEBUG 01-15 16:10:36.732527.732527 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.732502.732502 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.193450927734375e-05 seconds
DEBUG 01-15 16:10:36.732344.732344 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.732431.732431 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.733149.733149 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010213851928710938 seconds
DEBUG 01-15 16:10:36.733568.733568 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.733231.733231 cuda_h.py:19] end gpu_group_list cost 0.00017547607421875 seconds
DEBUG 01-15 16:10:36.734195.734195 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.734901.734901 cuda_h.py:19] end group_tensors cost 0.00599360466003418 seconds
DEBUG 01-15 16:10:36.735068.735068 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.736015.736015 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0016465187072753906 seconds
DEBUG 01-15 16:10:36.736510.736510 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.736406.736406 cuda_h.py:19] end gpu_group_list cost 0.00034499168395996094 seconds
DEBUG 01-15 16:10:36.738610.738610 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.738414.738414 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 18d5caf6-8c4d-432b-a9a4-d1f7ed2abe9e
DEBUG 01-15 16:10:36.739941.739941 cuda_h.py:19] end group pad cost 0.004057884216308594 seconds
DEBUG 01-15 16:10:36.739353.739353 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.757456.757456 client.py:127] Model loaded
DEBUG 01-15 16:10:36.758674.758674 cuda_h.py:19] end wait_experts_multi_device cost 0.019814729690551758 seconds
DEBUG 01-15 16:10:36.758610.758610 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.760349.760349 cuda_h.py:19] end group_einsum cost 0.02076268196105957 seconds
DEBUG 01-15 16:10:36.760023.760023 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.764016.764016 cuda_h.py:19] end get_outputs_cpu1 cost 0.003850221633911133 seconds
DEBUG 01-15 16:10:36.764310.764310 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03782987594604492 seconds
DEBUG 01-15 16:10:36.765960.765960 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007033824920654297 seconds
DEBUG 01-15 16:10:36.765096.765096 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.766018.766018 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.767918.767918 cuda_h.py:19] end index_scatter cost 8.940696716308594e-05 seconds
DEBUG 01-15 16:10:36.767393.767393 cuda_h.py:19] end cpuoutputsdeal cost 0.0021071434020996094 seconds
DEBUG 01-15 16:10:36.767648.767648 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.767795.767795 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.767696.767696 cuda_h.py:19] end gpu_group_tensor cost 0.00017213821411132812 seconds
DEBUG 01-15 16:10:36.767412.767412 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.768637.768637 cuda_h.py:19] end gpu_group_tensor cost 0.0001685619354248047 seconds
DEBUG 01-15 16:10:36.768270.768270 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.768390.768390 cuda_h.py:19] end gpu_group_einsum cost 0.0007352828979492188 seconds
DEBUG 01-15 16:10:36.769229.769229 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.769827.769827 cuda_h.py:19] end gpu_group_einsum cost 0.0005867481231689453 seconds
DEBUG 01-15 16:10:36.769409.769409 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.770823.770823 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.770645.770645 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003466606140136719 seconds
DEBUG 01-15 16:10:36.770546.770546 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.770768.770768 cuda_h.py:19] end concat_expert_out cost 5.817413330078125e-05 seconds
DEBUG 01-15 16:10:36.770048.770048 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.770522.770522 cuda_h.py:19] end index_scatter cost 6.723403930664062e-05 seconds
DEBUG 01-15 16:10:36.771928.771928 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001028299331665039 seconds
DEBUG 01-15 16:10:36.771309.771309 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.771874.771874 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.771803.771803 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019431114196777344 seconds
DEBUG 01-15 16:10:36.771751.771751 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.771449.771449 cuda_h.py:19] end concat_expert_out cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:10:36.771153.771153 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.771097.771097 cuda_h.py:19] end index_scatter cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:10:36.771290.771290 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005795955657958984 seconds
DEBUG 01-15 16:10:36.771399.771399 cuda_h.py:19] end gpu_experts_multi_device cost 0.03958725929260254 seconds
DEBUG 01-15 16:10:36.771561.771561 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.04967904090881348 seconds
DEBUG 01-15 16:10:36.772868.772868 cuda_h.py:19] end prefill_layer cost 0.0565943717956543 seconds
DEBUG 01-15 16:10:36.772030.772030 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:10:36.772309.772309 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.772873.772873 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:10:36.772106.772106 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:36.772007.772007 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:36.772579.772579 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 3.9577484130859375e-05 seconds
DEBUG 01-15 16:10:36.772256.772256 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:10:36.772298.772298 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.772419.772419 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.772449.772449 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.773088.773088 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.773482.773482 cuda_h.py:19] end allocate_cuda_memory cost 0.00025010108947753906 seconds
DEBUG 01-15 16:10:36.773009.773009 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.773230.773230 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.773863.773863 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.773712.773712 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.773137.773137 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 499937a7-b5e3-474b-a5b6-812cf6c63ff9
DEBUG 01-15 16:10:36.773426.773426 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.774800.774800 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.774149.774149 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 499937a7-b5e3-474b-a5b6-812cf6c63ff9
DEBUG 01-15 16:10:36.774323.774323 cuda_h.py:19] end load_into_gpu_async cost 0.0012884140014648438 seconds
DEBUG 01-15 16:10:36.774887.774887 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.775659.775659 cuda_h.py:19] end restore_tensors2 cost 8.225440979003906e-05 seconds
DEBUG 01-15 16:10:36.775660.775660 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0020623207092285156 seconds
INFO 01-15 16:10:36.775272.775272 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 499937a7-b5e3-474b-a5b6-812cf6c63ff9
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.778313.778313 cuda_h.py:19] end self_attn cost 0.004544496536254883 seconds
DEBUG 01-15 16:10:36.779061.779061 cuda_h.py:19] end iln_self_attn_paln cost 0.0061452388763427734 seconds
DEBUG 01-15 16:10:36.779844.779844 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:10:36.779607.779607 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.779002.779002 cuda_h.py:19] end gate cost 0.0007104873657226562 seconds
DEBUG 01-15 16:10:36.780216.780216 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.780226.780226 lmp.py:1912] 
DEBUG 01-15 16:10:36.780226.780226 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.780605.780605 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.780116.780116 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.780335.780335 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.780885.780885 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.780005.780005 lmp.py:1917] 
DEBUG 01-15 16:10:36.780005.780005 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.780840.780840 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.780159.780159 lmp.py:1935]   Expert 44 |     41 | CPU
DEBUG 01-15 16:10:36.780994.780994 lmp.py:1935]   Expert  1 |     45 | CPU
DEBUG 01-15 16:10:36.780114.780114 lmp.py:1935]   Expert 60 |     63 | CPU
DEBUG 01-15 16:10:36.780233.780233 lmp.py:1935]   Expert 28 |     71 | CPU
DEBUG 01-15 16:10:36.780876.780876 lmp.py:1935]   Expert 48 |     76 | CPU
DEBUG 01-15 16:10:36.780519.780519 lmp.py:1935]   Expert 27 |     88 | CPU
DEBUG 01-15 16:10:36.780639.780639 lmp.py:1935]   Expert  0 |    101 | CPU
DEBUG 01-15 16:10:36.780713.780713 lmp.py:1935]   Expert 62 |    107 | CPU
DEBUG 01-15 16:10:36.780932.780932 lmp.py:1935]   Expert 22 |    112 | CPU
DEBUG 01-15 16:10:36.780244.780244 lmp.py:1935]   Expert 30 |    114 | CPU
DEBUG 01-15 16:10:36.780748.780748 lmp.py:1935]   Expert 42 |    114 | CPU
DEBUG 01-15 16:10:36.780821.780821 lmp.py:1935]   Expert 59 |    115 | CPU
DEBUG 01-15 16:10:36.780418.780418 lmp.py:1935]   Expert 58 |    123 | CPU
DEBUG 01-15 16:10:36.780015.780015 lmp.py:1935]   Expert 12 |    125 | CPU
DEBUG 01-15 16:10:36.780088.780088 lmp.py:1935]   Expert  8 |    128 | CPU
DEBUG 01-15 16:10:36.780685.780685 lmp.py:1935]   Expert 16 |    128 | CPU
DEBUG 01-15 16:10:36.780281.780281 lmp.py:1935]   Expert 50 |    133 | CPU
DEBUG 01-15 16:10:36.780116.780116 lmp.py:1935]   Expert  5 |    143 | CPU
DEBUG 01-15 16:10:36.781713.781713 lmp.py:1935]   Expert 56 |    144 | CPU
DEBUG 01-15 16:10:36.781740.781740 lmp.py:1935]   Expert 55 |    151 | CPU
DEBUG 01-15 16:10:36.781291.781291 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:10:36.781602.781602 lmp.py:1935]   Expert 15 |    153 | CPU
DEBUG 01-15 16:10:36.781153.781153 lmp.py:1935]   Expert 26 |    154 | CPU
DEBUG 01-15 16:10:36.781749.781749 lmp.py:1935]   Expert 32 |    157 | CPU
DEBUG 01-15 16:10:36.781823.781823 lmp.py:1935]   Expert 34 |    158 | CPU
DEBUG 01-15 16:10:36.781181.781181 lmp.py:1935]   Expert 47 |    159 | CPU
DEBUG 01-15 16:10:36.781539.781539 lmp.py:1935]   Expert 24 |    161 | CPU
DEBUG 01-15 16:10:36.781136.781136 lmp.py:1935]   Expert  2 |    165 | CPU
DEBUG 01-15 16:10:36.781733.781733 lmp.py:1935]   Expert 52 |    166 | CPU
DEBUG 01-15 16:10:36.781329.781329 lmp.py:1935]   Expert  6 |    170 | CPU
DEBUG 01-15 16:10:36.781926.781926 lmp.py:1935]   Expert 13 |    170 | CPU
DEBUG 01-15 16:10:36.781761.781761 lmp.py:1935]   Expert 40 |    170 | CPU
DEBUG 01-15 16:10:36.781980.781980 lmp.py:1935]   Expert 18 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781723.781723 lmp.py:1935]   Expert 54 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781227.781227 lmp.py:1935]   Expert 41 |    176 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781492.781492 lmp.py:1935]   Expert  3 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781281.781281 lmp.py:1935]   Expert 19 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781593.781593 lmp.py:1935]   Expert 20 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781382.781382 lmp.py:1935]   Expert 37 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781932.781932 lmp.py:1935]   Expert 46 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781482.781482 lmp.py:1935]   Expert 25 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781794.781794 lmp.py:1935]   Expert 51 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781583.781583 lmp.py:1935]   Expert 17 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781133.781133 lmp.py:1935]   Expert 43 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781637.781637 lmp.py:1935]   Expert 35 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781664.781664 lmp.py:1935]   Expert 11 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781645.781645 lmp.py:1935]   Expert 31 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781672.781672 lmp.py:1935]   Expert 23 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781223.781223 lmp.py:1935]   Expert 49 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781773.781773 lmp.py:1935]   Expert 39 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781800.781800 lmp.py:1935]   Expert 53 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781112.781112 lmp.py:1935]   Expert 10 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781663.781663 lmp.py:1935]   Expert 33 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781213.781213 lmp.py:1935]   Expert 36 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781002.781002 lmp.py:1935]   Expert 38 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781267.781267 lmp.py:1935]   Expert  4 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781771.781771 lmp.py:1935]   Expert 21 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781275.781275 lmp.py:1935]   Expert 14 |    348 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781349.781349 lmp.py:1935]   Expert 63 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781661.781661 lmp.py:1935]   Expert 45 |    377 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781972.781972 lmp.py:1935]   Expert 61 |    390 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781284.781284 lmp.py:1935]   Expert  9 |    396 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781119.781119 lmp.py:1935]   Expert 29 |    493 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.781670.781670 lmp.py:1935]   Expert  7 |    512 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.781790.781790 lmp.py:1937] 
DEBUG 01-15 16:10:36.781790.781790 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.781863.781863 lmp.py:1938]   CPU:   4056 tokens
DEBUG 01-15 16:10:36.781367.781367 lmp.py:1942]   cuda:1:   4116 tokens (16 experts)
DEBUG 01-15 16:10:36.781633.781633 lmp.py:1942]   cuda:2:   4116 tokens (16 experts)
DEBUG 01-15 16:10:36.781468.781468 lmp.py:1943]   Total GPU:   8232 tokens
DEBUG 01-15 16:10:36.781826.781826 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.781826.781826 lmp.py:1944] 
DEBUG 01-15 16:10:36.782383.782383 cuda_h.py:19] end experts_map_get cost 0.0019888877868652344 seconds
INFO 01-15 16:10:36.782493.782493 client.py:127] Model loaded
DEBUG 01-15 16:10:36.782528.782528 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.782274.782274 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.782104.782104 lmp.py:1953] 
DEBUG 01-15 16:10:36.782104.782104 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.782039.782039 cuda_h.py:19] end cpu_experts_submit cost 5.936622619628906e-05 seconds
DEBUG 01-15 16:10:36.782120.782120 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.782241.782241 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.783911.783911 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.33.gate_proj.weight': 0, 'model.layers.19.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.49.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.49.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.49.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.51.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.51.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.51.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.31.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.31.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.31.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.19.mlp.experts.35.gate_proj.weight': 0, 'model.layers.19.mlp.experts.35.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.35.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.41.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.41.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.41.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.45.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.45.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.45.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.14.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.14.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.14.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.46.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.46.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.46.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.20.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.20.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.20.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.25.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.25.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.25.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.29.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(23362797568, 5767168, 0, 0), (23368564736, 5767168, 5767168, 0), (23357030400, 5767168, 11534336, 0), (22843752448, 5767168, 17301504, 0), (22849519616, 5767168, 23068672, 0), (22837985280, 5767168, 28835840, 0), (22861053952, 5767168, 34603008, 0), (22866821120, 5767168, 40370176, 0), (22855286784, 5767168, 46137344, 0), (23432003584, 5767168, 51904512, 0), (23437770752, 5767168, 57671680, 0), (23426236416, 5767168, 63438848, 0), (22912958464, 5767168, 69206016, 0), (22918725632, 5767168, 74973184, 0), (22907191296, 5767168, 80740352, 0), (22982164480, 5767168, 86507520, 0), (22987931648, 5767168, 92274688, 0), (22976397312, 5767168, 98041856, 0), (23639621632, 5767168, 103809024, 0), (23645388800, 5767168, 109576192, 0), (23633854464, 5767168, 115343360, 0), (23085973504, 5767168, 121110528, 0), (23091740672, 5767168, 126877696, 0), (23080206336, 5767168, 132644864, 0), (23674224640, 5767168, 138412032, 0), (23679991808, 5767168, 144179200, 0), (23668457472, 5767168, 149946368, 0), (23120576512, 5767168, 155713536, 0), (23126343680, 5767168, 161480704, 0), (23114809344, 5767168, 167247872, 0), (23155179520, 5767168, 173015040, 0), (23160946688, 5767168, 178782208, 0), (23149412352, 5767168, 184549376, 0), (23708827648, 5767168, 190316544, 0), (23714594816, 5767168, 196083712, 0), (23703060480, 5767168, 201850880, 0), (23726129152, 5767168, 207618048, 0), (23731896320, 5767168, 213385216, 0), (23720361984, 5767168, 219152384, 0), (23328194560, 5767168, 224919552, 0), (23333961728, 5767168, 230686720, 0), (23322427392, 5767168, 236453888, 0), (23847239680, 5767168, 242221056, 0), (23853006848, 5767168, 247988224, 0), (23841472512, 5767168, 253755392, 0), (23881842688, 5767168, 259522560, 0), (23887609856, 5767168, 265289728, 0), (23876075520, 5767168, 271056896, 0)], 2: [(23397400576, 5767168, 0, 0), (23403167744, 5767168, 5767168, 0), (23391633408, 5767168, 11534336, 0), (23414702080, 5767168, 17301504, 0), (23420469248, 5767168, 23068672, 0), (23408934912, 5767168, 28835840, 0), (23449305088, 5767168, 34603008, 0), (23455072256, 5767168, 40370176, 0), (23443537920, 5767168, 46137344, 0), (23466606592, 5767168, 51904512, 0), (23472373760, 5767168, 57671680, 0), (23460839424, 5767168, 63438848, 0), (22947561472, 5767168, 69206016, 0), (22953328640, 5767168, 74973184, 0), (22941794304, 5767168, 80740352, 0), (22964862976, 5767168, 86507520, 0), (22970630144, 5767168, 92274688, 0), (22959095808, 5767168, 98041856, 0), (23535812608, 5767168, 103809024, 0), (23541579776, 5767168, 109576192, 0), (23530045440, 5767168, 115343360, 0), (23501209600, 5767168, 121110528, 0), (23506976768, 5767168, 126877696, 0), (23495442432, 5767168, 132644864, 0), (23570415616, 5767168, 138412032, 0), (23576182784, 5767168, 144179200, 0), (23564648448, 5767168, 149946368, 0), (23034068992, 5767168, 155713536, 0), (23039836160, 5767168, 161480704, 0), (23028301824, 5767168, 167247872, 0), (23587717120, 5767168, 173015040, 0), (23593484288, 5767168, 178782208, 0), (23581949952, 5767168, 184549376, 0), (23103275008, 5767168, 190316544, 0), (23109042176, 5767168, 196083712, 0), (23097507840, 5767168, 201850880, 0), (23137878016, 5767168, 207618048, 0), (23143645184, 5767168, 213385216, 0), (23132110848, 5767168, 219152384, 0), (23189782528, 5767168, 224919552, 0), (23195549696, 5767168, 230686720, 0), (23184015360, 5767168, 236453888, 0), (23224385536, 5767168, 242221056, 0), (23230152704, 5767168, 247988224, 0), (23218618368, 5767168, 253755392, 0), (23293591552, 5767168, 259522560, 0), (23299358720, 5767168, 265289728, 0), (23287824384, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c0a1f80>, 2: <capsule object NULL at 0x7a4e5420a550>}
DEBUG 01-15 16:10:36.783813.783813 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.783988.783988 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.783028.783028 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 17426d32-53e3-45fe-9294-2c2c59443e33
DEBUG 01-15 16:10:36.783268.783268 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.784861.784861 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.785567.785567 cuda_h.py:19] end move_flatidxs cost 0.0008447170257568359 seconds
DEBUG 01-15 16:10:36.785443.785443 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.790662.790662 cuda_h.py:19] end group_tensors cost 0.004934787750244141 seconds
DEBUG 01-15 16:10:36.790091.790091 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.794673.794673 cuda_h.py:19] end group pad cost 0.003671884536743164 seconds
DEBUG 01-15 16:10:36.794463.794463 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:36.808250.808250 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.809519.809519 cuda_h.py:19] end restore2model cost 0.02732253074645996 seconds
DEBUG 01-15 16:10:36.809089.809089 cuda_h.py:19] end sllm_worker_task cost 0.03690910339355469 seconds
INFO 01-15 16:10:36.811737.811737 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 17426d32-53e3-45fe-9294-2c2c59443e33
DEBUG 01-15 16:10:36.812457.812457 cuda_h.py:19] end group_einsum cost 0.017362594604492188 seconds
DEBUG 01-15 16:10:36.812595.812595 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.812309.812309 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.029880046844482422 seconds
DEBUG 01-15 16:10:36.812811.812811 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.817849.817849 cuda_h.py:19] end get_outputs_cpu1 cost 0.004749774932861328 seconds
DEBUG 01-15 16:10:36.817981.817981 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03366804122924805 seconds
DEBUG 01-15 16:10:36.818651.818651 cuda_h.py:19] end restore2model cost 0.0056455135345458984 seconds
DEBUG 01-15 16:10:36.818224.818224 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0359344482421875 seconds
DEBUG 01-15 16:10:36.818510.818510 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.819922.819922 cuda_h.py:19] end gpu_sexperts cost 0.0005755424499511719 seconds
DEBUG 01-15 16:10:36.819348.819348 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.819489.819489 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.86102294921875e-05 seconds
DEBUG 01-15 16:10:36.819053.819053 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.819823.819823 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.820065.820065 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014722347259521484 seconds
DEBUG 01-15 16:10:36.820380.820380 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.821714.821714 cuda_h.py:19] end gpu_group_list cost 0.00024199485778808594 seconds
DEBUG 01-15 16:10:36.822704.822704 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.823669.823669 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014500617980957031 seconds
DEBUG 01-15 16:10:36.824784.824784 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.824813.824813 cuda_h.py:19] end gpu_group_list cost 0.00023031234741210938 seconds
DEBUG 01-15 16:10:36.825277.825277 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.825505.825505 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 17426d32-53e3-45fe-9294-2c2c59443e33
INFO 01-15 16:10:36.837593.837593 client.py:127] Model loaded
DEBUG 01-15 16:10:36.837005.837005 cuda_h.py:19] end wait_experts_multi_device cost 0.011996746063232422 seconds
DEBUG 01-15 16:10:36.837185.837185 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.837617.837617 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0003898143768310547 seconds
DEBUG 01-15 16:10:36.837883.837883 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.839969.839969 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.839544.839544 cuda_h.py:19] end index_scatter cost 8.130073547363281e-05 seconds
DEBUG 01-15 16:10:36.839506.839506 cuda_h.py:19] end cpuoutputsdeal cost 0.0015482902526855469 seconds
DEBUG 01-15 16:10:36.839714.839714 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.839099.839099 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.839172.839172 cuda_h.py:19] end gpu_group_tensor cost 0.0001575946807861328 seconds
DEBUG 01-15 16:10:36.839988.839988 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.840279.840279 cuda_h.py:19] end gpu_group_tensor cost 0.00014448165893554688 seconds
DEBUG 01-15 16:10:36.840891.840891 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.840796.840796 cuda_h.py:19] end gpu_group_einsum cost 0.0006425380706787109 seconds
DEBUG 01-15 16:10:36.840310.840310 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.841355.841355 cuda_h.py:19] end gpu_group_einsum cost 0.0005025863647460938 seconds
DEBUG 01-15 16:10:36.841240.841240 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.841820.841820 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.842406.842406 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002465248107910156 seconds
DEBUG 01-15 16:10:36.842361.842361 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.842948.842948 cuda_h.py:19] end concat_expert_out cost 7.939338684082031e-05 seconds
DEBUG 01-15 16:10:36.842142.842142 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.842252.842252 cuda_h.py:19] end index_scatter cost 7.557868957519531e-05 seconds
DEBUG 01-15 16:10:36.842744.842744 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009448528289794922 seconds
DEBUG 01-15 16:10:36.842470.842470 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.842657.842657 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.843421.843421 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020956993103027344 seconds
DEBUG 01-15 16:10:36.843799.843799 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.843366.843366 cuda_h.py:19] end concat_expert_out cost 6.532669067382812e-05 seconds
DEBUG 01-15 16:10:36.843414.843414 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.843749.843749 cuda_h.py:19] end index_scatter cost 6.699562072753906e-05 seconds
DEBUG 01-15 16:10:36.843757.843757 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006237030029296875 seconds
DEBUG 01-15 16:10:36.843243.843243 cuda_h.py:19] end gpu_experts_multi_device cost 0.024229764938354492 seconds
DEBUG 01-15 16:10:36.843749.843749 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.06439781188964844 seconds
DEBUG 01-15 16:10:36.843093.843093 cuda_h.py:19] end prefill_layer cost 0.07143425941467285 seconds
DEBUG 01-15 16:10:36.844313.844313 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:10:36.844546.844546 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.844170.844170 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:10:36.844595.844595 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:36.844496.844496 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:36.844777.844777 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 3.409385681152344e-05 seconds
DEBUG 01-15 16:10:36.844301.844301 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:10:36.844435.844435 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.844583.844583 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.844528.844528 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.844827.844827 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.844004.844004 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.844536.844536 cuda_h.py:19] end allocate_cuda_memory cost 0.00024080276489257812 seconds
DEBUG 01-15 16:10:36.845434.845434 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.845872.845872 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.845940.845940 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.845458.845458 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1773a159-d690-4ed4-9cc8-d55b8e1824e7
DEBUG 01-15 16:10:36.845971.845971 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.845002.845002 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.846880.846880 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1773a159-d690-4ed4-9cc8-d55b8e1824e7
DEBUG 01-15 16:10:36.846002.846002 cuda_h.py:19] end load_into_gpu_async cost 0.0011110305786132812 seconds
DEBUG 01-15 16:10:36.846295.846295 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.846657.846657 cuda_h.py:19] end restore_tensors2 cost 8.845329284667969e-05 seconds
DEBUG 01-15 16:10:36.846619.846619 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0017733573913574219 seconds
INFO 01-15 16:10:36.846244.846244 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1773a159-d690-4ed4-9cc8-d55b8e1824e7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.850803.850803 cuda_h.py:19] end self_attn cost 0.004453897476196289 seconds
DEBUG 01-15 16:10:36.850735.850735 cuda_h.py:19] end iln_self_attn_paln cost 0.0061528682708740234 seconds
DEBUG 01-15 16:10:36.850895.850895 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:10:36.850320.850320 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.851867.851867 cuda_h.py:19] end gate cost 0.0006840229034423828 seconds
DEBUG 01-15 16:10:36.851366.851366 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.851011.851011 lmp.py:1912] 
DEBUG 01-15 16:10:36.851011.851011 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.851482.851482 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.851324.851324 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.851305.851305 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.851617.851617 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.851498.851498 lmp.py:1917] 
DEBUG 01-15 16:10:36.851498.851498 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.851571.851571 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.851413.851413 lmp.py:1935]   Expert 54 |     21 | CPU
DEBUG 01-15 16:10:36.851156.851156 lmp.py:1935]   Expert  3 |     34 | CPU
DEBUG 01-15 16:10:36.851468.851468 lmp.py:1935]   Expert  8 |     41 | CPU
DEBUG 01-15 16:10:36.852587.852587 lmp.py:1935]   Expert 28 |     42 | CPU
DEBUG 01-15 16:10:36.852184.852184 lmp.py:1935]   Expert 43 |     54 | CPU
DEBUG 01-15 16:10:36.852496.852496 lmp.py:1935]   Expert 63 |     54 | CPU
DEBUG 01-15 16:10:36.852285.852285 lmp.py:1935]   Expert 36 |     76 | CPU
DEBUG 01-15 16:10:36.852120.852120 lmp.py:1935]   Expert 38 |     76 | CPU
DEBUG 01-15 16:10:36.852478.852478 lmp.py:1935]   Expert  6 |     80 | CPU
DEBUG 01-15 16:10:36.852836.852836 lmp.py:1935]   Expert 39 |     95 | CPU
DEBUG 01-15 16:10:36.852718.852718 lmp.py:1935]   Expert 57 |    100 | CPU
DEBUG 01-15 16:10:36.852361.852361 lmp.py:1935]   Expert 41 |    102 | CPU
DEBUG 01-15 16:10:36.852242.852242 lmp.py:1935]   Expert 12 |    109 | CPU
DEBUG 01-15 16:10:36.852362.852362 lmp.py:1935]   Expert 52 |    110 | CPU
DEBUG 01-15 16:10:36.852243.852243 lmp.py:1935]   Expert 19 |    120 | CPU
DEBUG 01-15 16:10:36.852886.852886 lmp.py:1935]   Expert 47 |    129 | CPU
DEBUG 01-15 16:10:36.852198.852198 lmp.py:1935]   Expert 13 |    133 | CPU
DEBUG 01-15 16:10:36.852748.852748 lmp.py:1935]   Expert 22 |    142 | CPU
DEBUG 01-15 16:10:36.852060.852060 lmp.py:1935]   Expert 46 |    145 | CPU
DEBUG 01-15 16:10:36.852849.852849 lmp.py:1935]   Expert 50 |    155 | CPU
DEBUG 01-15 16:10:36.852969.852969 lmp.py:1935]   Expert 24 |    163 | CPU
DEBUG 01-15 16:10:36.852612.852612 lmp.py:1935]   Expert 40 |    163 | CPU
DEBUG 01-15 16:10:36.852493.852493 lmp.py:1935]   Expert 20 |    164 | CPU
DEBUG 01-15 16:10:36.852812.852812 lmp.py:1935]   Expert 23 |    168 | CPU
DEBUG 01-15 16:10:36.852860.852860 lmp.py:1935]   Expert 55 |    168 | CPU
DEBUG 01-15 16:10:36.852218.852218 lmp.py:1935]   Expert  2 |    170 | CPU
DEBUG 01-15 16:10:36.852622.852622 lmp.py:1935]   Expert 37 |    173 | CPU
DEBUG 01-15 16:10:36.852219.852219 lmp.py:1935]   Expert 53 |    173 | CPU
DEBUG 01-15 16:10:36.852339.852339 lmp.py:1935]   Expert 21 |    176 | CPU
DEBUG 01-15 16:10:36.852174.852174 lmp.py:1935]   Expert 49 |    176 | CPU
DEBUG 01-15 16:10:36.852770.852770 lmp.py:1935]   Expert 61 |    177 | CPU
DEBUG 01-15 16:10:36.852175.852175 lmp.py:1935]   Expert 42 |    178 | CPU
DEBUG 01-15 16:10:36.852772.852772 lmp.py:1935]   Expert 33 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852560.852560 lmp.py:1935]   Expert 18 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852634.852634 lmp.py:1935]   Expert 32 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852992.852992 lmp.py:1935]   Expert  0 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852350.852350 lmp.py:1935]   Expert  5 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852470.852470 lmp.py:1935]   Expert 16 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852067.852067 lmp.py:1935]   Expert 30 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852425.852425 lmp.py:1935]   Expert  7 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852783.852783 lmp.py:1935]   Expert 14 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852142.852142 lmp.py:1935]   Expert 31 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852500.852500 lmp.py:1935]   Expert 34 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852858.852858 lmp.py:1935]   Expert 62 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852885.852885 lmp.py:1935]   Expert 59 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852243.852243 lmp.py:1935]   Expert 60 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852602.852602 lmp.py:1935]   Expert  9 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852721.852721 lmp.py:1935]   Expert 17 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852841.852841 lmp.py:1935]   Expert 10 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852961.852961 lmp.py:1935]   Expert 29 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852558.852558 lmp.py:1935]   Expert 15 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852254.852254 lmp.py:1935]   Expert 58 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852327.852327 lmp.py:1935]   Expert  4 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.852878.852878 lmp.py:1935]   Expert 26 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.852428.852428 lmp.py:1935]   Expert 51 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.853786.853786 lmp.py:1935]   Expert 11 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.853144.853144 lmp.py:1935]   Expert 44 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.853979.853979 lmp.py:1935]   Expert 56 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.853099.853099 lmp.py:1935]   Expert 27 |    289 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.853411.853411 lmp.py:1935]   Expert  1 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.853200.853200 lmp.py:1935]   Expert 45 |    364 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.853750.853750 lmp.py:1935]   Expert 25 |    462 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.853824.853824 lmp.py:1935]   Expert 35 |    516 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.853374.853374 lmp.py:1935]   Expert 48 |    637 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.853448.853448 lmp.py:1937] 
DEBUG 01-15 16:10:36.853448.853448 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.853190.853190 lmp.py:1938]   CPU:   3867 tokens
DEBUG 01-15 16:10:36.853409.853409 lmp.py:1942]   cuda:1:   4211 tokens (16 experts)
DEBUG 01-15 16:10:36.853960.853960 lmp.py:1942]   cuda:2:   4210 tokens (16 experts)
DEBUG 01-15 16:10:36.853318.853318 lmp.py:1943]   Total GPU:   8421 tokens
DEBUG 01-15 16:10:36.853438.853438 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.853438.853438 lmp.py:1944] 
DEBUG 01-15 16:10:36.853187.853187 cuda_h.py:19] end experts_map_get cost 0.0019443035125732422 seconds
INFO 01-15 16:10:36.853733.853733 client.py:127] Model loaded
DEBUG 01-15 16:10:36.853219.853219 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.853687.853687 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.853470.853470 lmp.py:1953] 
DEBUG 01-15 16:10:36.853470.853470 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.853022.853022 cuda_h.py:19] end cpu_experts_submit cost 5.626678466796875e-05 seconds
DEBUG 01-15 16:10:36.853148.853148 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.853216.853216 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.854660.854660 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.5.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.5.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.5.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.20.mlp.experts.32.gate_proj.weight': 0, 'model.layers.20.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.51.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.51.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.51.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (24470093824, 5767168, 17301504, 0), (24475860992, 5767168, 23068672, 0), (24464326656, 5767168, 28835840, 0), (24487395328, 5767168, 34603008, 0), (24493162496, 5767168, 40370176, 0), (24481628160, 5767168, 46137344, 0), (23968350208, 5767168, 51904512, 0), (23974117376, 5767168, 57671680, 0), (23962583040, 5767168, 63438848, 0), (23985651712, 5767168, 69206016, 0), (23991418880, 5767168, 74973184, 0), (23979884544, 5767168, 80740352, 0), (24072159232, 5767168, 86507520, 0), (24077926400, 5767168, 92274688, 0), (24066392064, 5767168, 98041856, 0), (24660410368, 5767168, 103809024, 0), (24666177536, 5767168, 109576192, 0), (24654643200, 5767168, 115343360, 0), (24677711872, 5767168, 121110528, 0), (24683479040, 5767168, 126877696, 0), (24671944704, 5767168, 132644864, 0), (24729616384, 5767168, 138412032, 0), (24735383552, 5767168, 144179200, 0), (24723849216, 5767168, 149946368, 0), (24193269760, 5767168, 155713536, 0), (24199036928, 5767168, 161480704, 0), (24187502592, 5767168, 167247872, 0), (24175968256, 5767168, 173015040, 0), (24181735424, 5767168, 178782208, 0), (24170201088, 5767168, 184549376, 0), (24868028416, 5767168, 190316544, 0), (24873795584, 5767168, 196083712, 0), (24862261248, 5767168, 201850880, 0), (24902631424, 5767168, 207618048, 0), (24908398592, 5767168, 213385216, 0), (24896864256, 5767168, 219152384, 0), (24366284800, 5767168, 224919552, 0), (24372051968, 5767168, 230686720, 0), (24360517632, 5767168, 236453888, 0), (24937234432, 5767168, 242221056, 0), (24943001600, 5767168, 247988224, 0), (24931467264, 5767168, 253755392, 0), (24435490816, 5767168, 259522560, 0), (24441257984, 5767168, 265289728, 0), (24429723648, 5767168, 271056896, 0)], 2: [(24452792320, 5767168, 0, 0), (24458559488, 5767168, 5767168, 0), (24447025152, 5767168, 11534336, 0), (23916445696, 5767168, 17301504, 0), (23922212864, 5767168, 23068672, 0), (23910678528, 5767168, 28835840, 0), (24504696832, 5767168, 34603008, 0), (24510464000, 5767168, 40370176, 0), (24498929664, 5767168, 46137344, 0), (24020254720, 5767168, 51904512, 0), (24026021888, 5767168, 57671680, 0), (24014487552, 5767168, 63438848, 0), (24054857728, 5767168, 69206016, 0), (24060624896, 5767168, 74973184, 0), (24049090560, 5767168, 80740352, 0), (24089460736, 5767168, 86507520, 0), (24095227904, 5767168, 92274688, 0), (24083693568, 5767168, 98041856, 0), (24141365248, 5767168, 103809024, 0), (24147132416, 5767168, 109576192, 0), (24135598080, 5767168, 115343360, 0), (24158666752, 5767168, 121110528, 0), (24164433920, 5767168, 126877696, 0), (24152899584, 5767168, 132644864, 0), (24210571264, 5767168, 138412032, 0), (24216338432, 5767168, 144179200, 0), (24204804096, 5767168, 149946368, 0), (24781520896, 5767168, 155713536, 0), (24787288064, 5767168, 161480704, 0), (24775753728, 5767168, 167247872, 0), (24418189312, 5767168, 173015040, 0), (24423956480, 5767168, 178782208, 0), (24412422144, 5767168, 184549376, 0), (24331681792, 5767168, 190316544, 0), (24337448960, 5767168, 196083712, 0), (24325914624, 5767168, 201850880, 0), (24348983296, 5767168, 207618048, 0), (24354750464, 5767168, 213385216, 0), (24343216128, 5767168, 219152384, 0), (24919932928, 5767168, 224919552, 0), (24925700096, 5767168, 230686720, 0), (24914165760, 5767168, 236453888, 0), (24400887808, 5767168, 242221056, 0), (24406654976, 5767168, 247988224, 0), (24395120640, 5767168, 253755392, 0), (24971837440, 5767168, 259522560, 0), (24977604608, 5767168, 265289728, 0), (24966070272, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e54733cc0>, 2: <capsule object NULL at 0x7a4e5420a6a0>}
DEBUG 01-15 16:10:36.855663.855663 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.855976.855976 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.855493.855493 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 90e92333-c15b-47b0-9b24-bac7606da383
DEBUG 01-15 16:10:36.855122.855122 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.855533.855533 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.855607.855607 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.855145.855145 cuda_h.py:19] end restore2model cost 0.002275228500366211 seconds
DEBUG 01-15 16:10:36.855538.855538 cuda_h.py:19] end sllm_worker_task cost 0.011346578598022461 seconds
INFO 01-15 16:10:36.856918.856918 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 90e92333-c15b-47b0-9b24-bac7606da383
DEBUG 01-15 16:10:36.856210.856210 cuda_h.py:19] end move_flatidxs cost 0.0009088516235351562 seconds
DEBUG 01-15 16:10:36.856504.856504 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.856263.856263 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003160715103149414 seconds
DEBUG 01-15 16:10:36.857358.857358 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.860718.860718 cuda_h.py:19] end restore2model cost 0.003004789352416992 seconds
DEBUG 01-15 16:10:36.860754.860754 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006390810012817383 seconds
DEBUG 01-15 16:10:36.860477.860477 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.860283.860283 cuda_h.py:19] end gpu_sexperts cost 0.0003147125244140625 seconds
DEBUG 01-15 16:10:36.860066.860066 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.860989.860989 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:10:36.860354.860354 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.860010.860010 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.861398.861398 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010540485382080078 seconds
DEBUG 01-15 16:10:36.861009.861009 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.862433.862433 cuda_h.py:19] end gpu_group_list cost 0.00017714500427246094 seconds
DEBUG 01-15 16:10:36.862717.862717 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.864500.864500 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011799335479736328 seconds
DEBUG 01-15 16:10:36.864854.864854 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.864273.864273 cuda_h.py:19] end gpu_group_list cost 0.00019311904907226562 seconds
DEBUG 01-15 16:10:36.865829.865829 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.865665.865665 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 90e92333-c15b-47b0-9b24-bac7606da383
DEBUG 01-15 16:10:36.870915.870915 cuda_h.py:19] end group_tensors cost 0.013800382614135742 seconds
DEBUG 01-15 16:10:36.871451.871451 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.875166.875166 cuda_h.py:19] end group pad cost 0.0040891170501708984 seconds
DEBUG 01-15 16:10:36.875969.875969 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.882138.882138 client.py:127] Model loaded
DEBUG 01-15 16:10:36.882719.882719 cuda_h.py:19] end wait_experts_multi_device cost 0.017017126083374023 seconds
DEBUG 01-15 16:10:36.882058.882058 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.896428.896428 cuda_h.py:19] end group_einsum cost 0.020923852920532227 seconds
DEBUG 01-15 16:10:36.896923.896923 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.900881.900881 cuda_h.py:19] end get_outputs_cpu1 cost 0.0037903785705566406 seconds
DEBUG 01-15 16:10:36.901472.901472 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.045914649963378906 seconds
DEBUG 01-15 16:10:36.901136.901136 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.019272565841674805 seconds
DEBUG 01-15 16:10:36.901809.901809 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.903450.903450 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.903919.903919 cuda_h.py:19] end index_scatter cost 8.988380432128906e-05 seconds
DEBUG 01-15 16:10:36.903486.903486 cuda_h.py:19] end cpuoutputsdeal cost 0.0020029544830322266 seconds
DEBUG 01-15 16:10:36.904886.904886 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.904411.904411 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.904391.904391 cuda_h.py:19] end gpu_group_tensor cost 0.0001633167266845703 seconds
DEBUG 01-15 16:10:36.904677.904677 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.904189.904189 cuda_h.py:19] end gpu_group_tensor cost 0.00023698806762695312 seconds
DEBUG 01-15 16:10:36.904399.904399 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.905220.905220 cuda_h.py:19] end gpu_group_einsum cost 0.0005571842193603516 seconds
DEBUG 01-15 16:10:36.905973.905973 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.905949.905949 cuda_h.py:19] end gpu_group_einsum cost 0.0004222393035888672 seconds
DEBUG 01-15 16:10:36.906350.906350 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.906361.906361 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.906245.906245 cuda_h.py:19] end all_expert_outputs_slices cost 0.00026226043701171875 seconds
DEBUG 01-15 16:10:36.906624.906624 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.906554.906554 cuda_h.py:19] end concat_expert_out cost 5.626678466796875e-05 seconds
DEBUG 01-15 16:10:36.906119.906119 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.906593.906593 cuda_h.py:19] end index_scatter cost 6.67572021484375e-05 seconds
DEBUG 01-15 16:10:36.907860.907860 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009245872497558594 seconds
DEBUG 01-15 16:10:36.907334.907334 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.907753.907753 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.907708.907708 cuda_h.py:19] end all_expert_outputs_slices cost 0.000213623046875 seconds
DEBUG 01-15 16:10:36.907418.907418 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.907686.907686 cuda_h.py:19] end concat_expert_out cost 5.817413330078125e-05 seconds
DEBUG 01-15 16:10:36.907582.907582 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.907711.907711 cuda_h.py:19] end index_scatter cost 6.0558319091796875e-05 seconds
DEBUG 01-15 16:10:36.907951.907951 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005931854248046875 seconds
DEBUG 01-15 16:10:36.907059.907059 cuda_h.py:19] end gpu_experts_multi_device cost 0.047159433364868164 seconds
DEBUG 01-15 16:10:36.907552.907552 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.057372331619262695 seconds
DEBUG 01-15 16:10:36.908535.908535 cuda_h.py:19] end prefill_layer cost 0.0642998218536377 seconds
DEBUG 01-15 16:10:36.908802.908802 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:10:36.908127.908127 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.908175.908175 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:10:36.908169.908169 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:36.908025.908025 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:36.908981.908981 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 4.172325134277344e-05 seconds
DEBUG 01-15 16:10:36.908883.908883 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:10:36.908870.908870 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.908197.908197 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.908273.908273 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.908540.908540 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.909311.909311 cuda_h.py:19] end allocate_cuda_memory cost 0.00025010108947753906 seconds
DEBUG 01-15 16:10:36.909268.909268 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.909012.909012 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.909929.909929 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.909905.909905 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.909899.909899 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ab2599ba-ba79-4274-9b02-eeca50f08d10
DEBUG 01-15 16:10:36.909810.909810 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.910784.910784 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.910030.910030 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ab2599ba-ba79-4274-9b02-eeca50f08d10
DEBUG 01-15 16:10:36.910920.910920 cuda_h.py:19] end load_into_gpu_async cost 0.001293182373046875 seconds
DEBUG 01-15 16:10:36.910722.910722 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.910123.910123 cuda_h.py:19] end restore_tensors2 cost 8.940696716308594e-05 seconds
DEBUG 01-15 16:10:36.910125.910125 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0020704269409179688 seconds
INFO 01-15 16:10:36.911034.911034 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ab2599ba-ba79-4274-9b02-eeca50f08d10
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.914614.914614 cuda_h.py:19] end self_attn cost 0.004751920700073242 seconds
DEBUG 01-15 16:10:36.915363.915363 cuda_h.py:19] end iln_self_attn_paln cost 0.006577968597412109 seconds
DEBUG 01-15 16:10:36.915716.915716 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:10:36.915525.915525 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.916219.916219 cuda_h.py:19] end gate cost 0.0007207393646240234 seconds
DEBUG 01-15 16:10:36.916148.916148 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.916119.916119 lmp.py:1912] 
DEBUG 01-15 16:10:36.916119.916119 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.916180.916180 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.916406.916406 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.916863.916863 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.916175.916175 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.916534.916534 lmp.py:1917] 
DEBUG 01-15 16:10:36.916534.916534 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.916130.916130 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.916972.916972 lmp.py:1935]   Expert 44 |     29 | CPU
DEBUG 01-15 16:10:36.916807.916807 lmp.py:1935]   Expert  9 |     36 | CPU
DEBUG 01-15 16:10:36.916689.916689 lmp.py:1935]   Expert 11 |     36 | CPU
DEBUG 01-15 16:10:36.916093.916093 lmp.py:1935]   Expert 56 |     58 | CPU
DEBUG 01-15 16:10:36.916975.916975 lmp.py:1935]   Expert 54 |     77 | CPU
DEBUG 01-15 16:10:36.916379.916379 lmp.py:1935]   Expert 62 |     90 | CPU
DEBUG 01-15 16:10:36.917453.917453 lmp.py:1935]   Expert  7 |     94 | CPU
DEBUG 01-15 16:10:36.917572.917572 lmp.py:1935]   Expert 47 |     97 | CPU
DEBUG 01-15 16:10:36.917692.917692 lmp.py:1935]   Expert 51 |    103 | CPU
DEBUG 01-15 16:10:36.917812.917812 lmp.py:1935]   Expert 60 |    107 | CPU
DEBUG 01-15 16:10:36.917217.917217 lmp.py:1935]   Expert 52 |    109 | CPU
DEBUG 01-15 16:10:36.917621.917621 lmp.py:1935]   Expert 22 |    111 | CPU
DEBUG 01-15 16:10:36.917787.917787 lmp.py:1935]   Expert 41 |    111 | CPU
DEBUG 01-15 16:10:36.917192.917192 lmp.py:1935]   Expert 53 |    111 | CPU
DEBUG 01-15 16:10:36.917596.917596 lmp.py:1935]   Expert 48 |    125 | CPU
DEBUG 01-15 16:10:36.917001.917001 lmp.py:1935]   Expert  8 |    126 | CPU
DEBUG 01-15 16:10:36.917644.917644 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:10:36.917810.917810 lmp.py:1935]   Expert  1 |    130 | CPU
DEBUG 01-15 16:10:36.917215.917215 lmp.py:1935]   Expert 32 |    130 | CPU
DEBUG 01-15 16:10:36.917858.917858 lmp.py:1935]   Expert  2 |    131 | CPU
DEBUG 01-15 16:10:36.917024.917024 lmp.py:1935]   Expert 35 |    137 | CPU
DEBUG 01-15 16:10:36.917428.917428 lmp.py:1935]   Expert 27 |    138 | CPU
DEBUG 01-15 16:10:36.917833.917833 lmp.py:1935]   Expert 23 |    141 | CPU
DEBUG 01-15 16:10:36.917191.917191 lmp.py:1935]   Expert 59 |    142 | CPU
DEBUG 01-15 16:10:36.917072.917072 lmp.py:1935]   Expert 39 |    144 | CPU
DEBUG 01-15 16:10:36.917954.917954 lmp.py:1935]   Expert 26 |    147 | CPU
DEBUG 01-15 16:10:36.917074.917074 lmp.py:1935]   Expert 50 |    152 | CPU
DEBUG 01-15 16:10:36.917717.917717 lmp.py:1935]   Expert 14 |    155 | CPU
DEBUG 01-15 16:10:36.917121.917121 lmp.py:1935]   Expert 46 |    164 | CPU
DEBUG 01-15 16:10:36.917526.917526 lmp.py:1935]   Expert 24 |    170 | CPU
DEBUG 01-15 16:10:36.917169.917169 lmp.py:1935]   Expert  0 |    171 | CPU
DEBUG 01-15 16:10:36.917335.917335 lmp.py:1935]   Expert  4 |    174 | CPU
DEBUG 01-15 16:10:36.917170.917170 lmp.py:1935]   Expert 34 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917436.917436 lmp.py:1935]   Expert 38 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917940.917940 lmp.py:1935]   Expert 49 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917728.917728 lmp.py:1935]   Expert 40 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917755.917755 lmp.py:1935]   Expert  5 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917544.917544 lmp.py:1935]   Expert 63 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917379.917379 lmp.py:1935]   Expert 19 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917976.917976 lmp.py:1935]   Expert 13 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917334.917334 lmp.py:1935]   Expert 43 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917408.917408 lmp.py:1935]   Expert 29 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917004.917004 lmp.py:1935]   Expert 61 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917601.917601 lmp.py:1935]   Expert 57 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917959.917959 lmp.py:1935]   Expert 33 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917509.917509 lmp.py:1935]   Expert 31 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917821.917821 lmp.py:1935]   Expert 16 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917656.917656 lmp.py:1935]   Expert 37 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917922.917922 lmp.py:1935]   Expert 20 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917426.917426 lmp.py:1935]   Expert  3 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917976.917976 lmp.py:1935]   Expert 15 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917288.917288 lmp.py:1935]   Expert 36 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917077.917077 lmp.py:1935]   Expert 18 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917866.917866 lmp.py:1935]   Expert 12 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917416.917416 lmp.py:1935]   Expert 28 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917490.917490 lmp.py:1935]   Expert 17 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917040.917040 lmp.py:1935]   Expert 55 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917544.917544 lmp.py:1935]   Expert 30 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.917286.917286 lmp.py:1935]   Expert 25 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.917837.917837 lmp.py:1935]   Expert 58 |    333 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.918864.918864 lmp.py:1935]   Expert 10 |    361 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.918652.918652 lmp.py:1935]   Expert 45 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.918441.918441 lmp.py:1935]   Expert 21 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.918230.918230 lmp.py:1935]   Expert 42 |    645 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.918827.918827 lmp.py:1937] 
DEBUG 01-15 16:10:36.918827.918827 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.918615.918615 lmp.py:1938]   CPU:   3773 tokens
DEBUG 01-15 16:10:36.918596.918596 lmp.py:1942]   cuda:1:   4338 tokens (16 experts)
DEBUG 01-15 16:10:36.918385.918385 lmp.py:1942]   cuda:2:   4177 tokens (16 experts)
DEBUG 01-15 16:10:36.918982.918982 lmp.py:1943]   Total GPU:   8515 tokens
DEBUG 01-15 16:10:36.918578.918578 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.918578.918578 lmp.py:1944] 
DEBUG 01-15 16:10:36.918327.918327 cuda_h.py:19] end experts_map_get cost 0.0019626617431640625 seconds
INFO 01-15 16:10:36.918522.918522 client.py:127] Model loaded
DEBUG 01-15 16:10:36.918643.918643 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.918243.918243 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.918881.918881 lmp.py:1953] 
DEBUG 01-15 16:10:36.918881.918881 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.918147.918147 cuda_h.py:19] end cpu_experts_submit cost 5.698204040527344e-05 seconds
DEBUG 01-15 16:10:36.918989.918989 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.918680.918680 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.919211.919211 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.33.gate_proj.weight': 0, 'model.layers.21.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.20.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.20.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.20.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.61.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.61.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.61.up_proj.weight': 271056896}, 2: {'model.layers.21.mlp.experts.3.gate_proj.weight': 0, 'model.layers.21.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.63.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.63.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.63.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(25577390080, 5767168, 0, 0), (25583157248, 5767168, 5767168, 0), (25571622912, 5767168, 11534336, 0), (25594691584, 5767168, 17301504, 0), (25600458752, 5767168, 23068672, 0), (25588924416, 5767168, 28835840, 0), (25646596096, 5767168, 34603008, 0), (25652363264, 5767168, 40370176, 0), (25640828928, 5767168, 46137344, 0), (25092947968, 5767168, 51904512, 0), (25098715136, 5767168, 57671680, 0), (25087180800, 5767168, 63438848, 0), (25733103616, 5767168, 69206016, 0), (25738870784, 5767168, 74973184, 0), (25727336448, 5767168, 80740352, 0), (25179455488, 5767168, 86507520, 0), (25185222656, 5767168, 92274688, 0), (25173688320, 5767168, 98041856, 0), (25750405120, 5767168, 103809024, 0), (25756172288, 5767168, 109576192, 0), (25744637952, 5767168, 115343360, 0), (25265963008, 5767168, 121110528, 0), (25271730176, 5767168, 126877696, 0), (25260195840, 5767168, 132644864, 0), (25300566016, 5767168, 138412032, 0), (25306333184, 5767168, 144179200, 0), (25294798848, 5767168, 149946368, 0), (25317867520, 5767168, 155713536, 0), (25323634688, 5767168, 161480704, 0), (25312100352, 5767168, 167247872, 0), (25335169024, 5767168, 173015040, 0), (25340936192, 5767168, 178782208, 0), (25329401856, 5767168, 184549376, 0), (25352470528, 5767168, 190316544, 0), (25358237696, 5767168, 196083712, 0), (25346703360, 5767168, 201850880, 0), (25854214144, 5767168, 207618048, 0), (25859981312, 5767168, 213385216, 0), (25848446976, 5767168, 219152384, 0), (25438978048, 5767168, 224919552, 0), (25444745216, 5767168, 230686720, 0), (25433210880, 5767168, 236453888, 0), (25490882560, 5767168, 242221056, 0), (25496649728, 5767168, 247988224, 0), (25485115392, 5767168, 253755392, 0), (26061832192, 5767168, 259522560, 0), (26067599360, 5767168, 265289728, 0), (26056065024, 5767168, 271056896, 0)], 2: [(25058344960, 5767168, 0, 0), (25064112128, 5767168, 5767168, 0), (25052577792, 5767168, 11534336, 0), (25629294592, 5767168, 17301504, 0), (25635061760, 5767168, 23068672, 0), (25623527424, 5767168, 28835840, 0), (25663897600, 5767168, 34603008, 0), (25669664768, 5767168, 40370176, 0), (25658130432, 5767168, 46137344, 0), (25698500608, 5767168, 51904512, 0), (25704267776, 5767168, 57671680, 0), (25692733440, 5767168, 63438848, 0), (25214058496, 5767168, 69206016, 0), (25219825664, 5767168, 74973184, 0), (25208291328, 5767168, 80740352, 0), (25785008128, 5767168, 86507520, 0), (25790775296, 5767168, 92274688, 0), (25779240960, 5767168, 98041856, 0), (25231360000, 5767168, 103809024, 0), (25237127168, 5767168, 109576192, 0), (25225592832, 5767168, 115343360, 0), (25283264512, 5767168, 121110528, 0), (25289031680, 5767168, 126877696, 0), (25277497344, 5767168, 132644864, 0), (26096435200, 5767168, 138412032, 0), (26102202368, 5767168, 144179200, 0), (26090668032, 5767168, 149946368, 0), (25369772032, 5767168, 155713536, 0), (25375539200, 5767168, 161480704, 0), (25364004864, 5767168, 167247872, 0), (25958023168, 5767168, 173015040, 0), (25963790336, 5767168, 178782208, 0), (25952256000, 5767168, 184549376, 0), (25992626176, 5767168, 190316544, 0), (25998393344, 5767168, 196083712, 0), (25986859008, 5767168, 201850880, 0), (26009927680, 5767168, 207618048, 0), (26015694848, 5767168, 213385216, 0), (26004160512, 5767168, 219152384, 0), (25508184064, 5767168, 224919552, 0), (25513951232, 5767168, 230686720, 0), (25502416896, 5767168, 236453888, 0), (25525485568, 5767168, 242221056, 0), (25531252736, 5767168, 247988224, 0), (25519718400, 5767168, 253755392, 0), (25542787072, 5767168, 259522560, 0), (25548554240, 5767168, 265289728, 0), (25537019904, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e3464bea0>, 2: <capsule object NULL at 0x7a4e5420a6d0>}
DEBUG 01-15 16:10:36.919580.919580 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.919178.919178 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.919027.919027 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, fa2c2140-d0b4-466b-9a1d-73bfeeee0c89
DEBUG 01-15 16:10:36.919960.919960 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.920528.920528 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.920551.920551 cuda_h.py:19] end restore2model cost 0.001966238021850586 seconds
DEBUG 01-15 16:10:36.920896.920896 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.920255.920255 cuda_h.py:19] end sllm_worker_task cost 0.01154780387878418 seconds
INFO 01-15 16:10:36.920121.920121 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, fa2c2140-d0b4-466b-9a1d-73bfeeee0c89
DEBUG 01-15 16:10:36.921416.921416 cuda_h.py:19] end move_flatidxs cost 0.0008401870727539062 seconds
DEBUG 01-15 16:10:36.921961.921961 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.921215.921215 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002713441848754883 seconds
DEBUG 01-15 16:10:36.921364.921364 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.924924.924924 cuda_h.py:19] end restore2model cost 0.0030448436737060547 seconds
DEBUG 01-15 16:10:36.924959.924959 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00599217414855957 seconds
DEBUG 01-15 16:10:36.924900.924900 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.924852.924852 cuda_h.py:19] end gpu_sexperts cost 0.0003159046173095703 seconds
DEBUG 01-15 16:10:36.925397.925397 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.925366.925366 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:36.925207.925207 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.925487.925487 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.926371.926371 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.00107574462890625 seconds
DEBUG 01-15 16:10:36.926983.926983 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.926838.926838 cuda_h.py:19] end gpu_group_list cost 0.00017786026000976562 seconds
DEBUG 01-15 16:10:36.927327.927327 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.928832.928832 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011858940124511719 seconds
DEBUG 01-15 16:10:36.928881.928881 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.929927.929927 cuda_h.py:19] end gpu_group_list cost 0.0001780986785888672 seconds
DEBUG 01-15 16:10:36.929517.929517 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.929207.929207 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, fa2c2140-d0b4-466b-9a1d-73bfeeee0c89
DEBUG 01-15 16:10:36.930264.930264 cuda_h.py:19] end group_tensors cost 0.009414196014404297 seconds
DEBUG 01-15 16:10:36.931110.931110 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.935293.935293 cuda_h.py:19] end group pad cost 0.004012584686279297 seconds
DEBUG 01-15 16:10:36.935659.935659 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:36.947981.947981 client.py:127] Model loaded
DEBUG 01-15 16:10:36.947153.947153 cuda_h.py:19] end wait_experts_multi_device cost 0.017352819442749023 seconds
DEBUG 01-15 16:10:36.947758.947758 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:36.954200.954200 cuda_h.py:19] end group_einsum cost 0.018854618072509766 seconds
DEBUG 01-15 16:10:36.954577.954577 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:36.958868.958868 cuda_h.py:19] end get_outputs_cpu1 cost 0.0038619041442871094 seconds
DEBUG 01-15 16:10:36.959001.959001 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039176225662231445 seconds
DEBUG 01-15 16:10:36.959164.959164 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012446403503417969 seconds
DEBUG 01-15 16:10:36.959598.959598 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:36.961314.961314 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.961406.961406 cuda_h.py:19] end index_scatter cost 8.7738037109375e-05 seconds
DEBUG 01-15 16:10:36.962616.962616 cuda_h.py:19] end cpuoutputsdeal cost 0.0020761489868164062 seconds
DEBUG 01-15 16:10:36.962109.962109 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:36.962110.962110 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.962706.962706 cuda_h.py:19] end gpu_group_tensor cost 0.0001609325408935547 seconds
DEBUG 01-15 16:10:36.962753.962753 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:36.962819.962819 cuda_h.py:19] end gpu_group_tensor cost 0.00015783309936523438 seconds
DEBUG 01-15 16:10:36.962783.962783 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.963260.963260 cuda_h.py:19] end gpu_group_einsum cost 0.00054931640625 seconds
DEBUG 01-15 16:10:36.963198.963198 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:36.964259.964259 cuda_h.py:19] end gpu_group_einsum cost 0.0005507469177246094 seconds
DEBUG 01-15 16:10:36.964310.964310 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.964347.964347 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.964705.964705 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003535747528076172 seconds
DEBUG 01-15 16:10:36.964998.964998 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.964750.964750 cuda_h.py:19] end concat_expert_out cost 6.246566772460938e-05 seconds
DEBUG 01-15 16:10:36.964745.964745 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.965980.965980 cuda_h.py:19] end index_scatter cost 6.818771362304688e-05 seconds
DEBUG 01-15 16:10:36.965201.965201 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001058340072631836 seconds
DEBUG 01-15 16:10:36.965059.965059 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:36.965909.965909 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:36.965520.965520 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020384788513183594 seconds
DEBUG 01-15 16:10:36.965991.965991 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:36.965689.965689 cuda_h.py:19] end concat_expert_out cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:10:36.965109.965109 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:36.965052.965052 cuda_h.py:19] end index_scatter cost 6.365776062011719e-05 seconds
DEBUG 01-15 16:10:36.966246.966246 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005903244018554688 seconds
DEBUG 01-15 16:10:36.966600.966600 cuda_h.py:19] end gpu_experts_multi_device cost 0.04095792770385742 seconds
DEBUG 01-15 16:10:36.966000.966000 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.05077528953552246 seconds
DEBUG 01-15 16:10:36.966883.966883 cuda_h.py:19] end prefill_layer cost 0.05814218521118164 seconds
DEBUG 01-15 16:10:36.966051.966051 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:10:36.966853.966853 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:36.966086.966086 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:10:36.966557.966557 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:36.966982.966982 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:36.966700.966700 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 4.1961669921875e-05 seconds
DEBUG 01-15 16:10:36.966840.966840 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:10:36.966020.966020 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:36.967850.967850 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:36.967012.967012 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:36.967465.967465 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:36.967970.967970 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:36.967669.967669 cuda_h.py:19] end allocate_cuda_memory cost 0.00026607513427734375 seconds
DEBUG 01-15 16:10:36.967162.967162 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:36.967309.967309 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.967331.967331 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.967040.967040 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 8fe3dc83-3363-4345-ade4-cc51c000193a
DEBUG 01-15 16:10:36.967806.967806 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.968427.968427 cuda_h.py:10] start self_attn
INFO 01-15 16:10:36.968789.968789 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 8fe3dc83-3363-4345-ade4-cc51c000193a
DEBUG 01-15 16:10:36.969824.969824 cuda_h.py:19] end load_into_gpu_async cost 0.0013275146484375 seconds
DEBUG 01-15 16:10:36.969626.969626 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:36.969259.969259 cuda_h.py:19] end restore_tensors2 cost 8.440017700195312e-05 seconds
DEBUG 01-15 16:10:36.969260.969260 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0019578933715820312 seconds
INFO 01-15 16:10:36.969052.969052 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 8fe3dc83-3363-4345-ade4-cc51c000193a
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:36.972125.972125 cuda_h.py:19] end self_attn cost 0.004540920257568359 seconds
DEBUG 01-15 16:10:36.973190.973190 cuda_h.py:19] end iln_self_attn_paln cost 0.006223440170288086 seconds
DEBUG 01-15 16:10:36.973947.973947 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:10:36.973710.973710 cuda_h.py:10] start gate
DEBUG 01-15 16:10:36.974212.974212 cuda_h.py:19] end gate cost 0.0007193088531494141 seconds
DEBUG 01-15 16:10:36.974141.974141 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:36.974329.974329 lmp.py:1912] 
DEBUG 01-15 16:10:36.974329.974329 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:36.974754.974754 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:36.974550.974550 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:36.974292.974292 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:36.974604.974604 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:36.974962.974962 lmp.py:1917] 
DEBUG 01-15 16:10:36.974962.974962 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:36.974559.974559 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:36.974593.974593 lmp.py:1935]   Expert 25 |     14 | CPU
DEBUG 01-15 16:10:36.974428.974428 lmp.py:1935]   Expert 48 |     32 | CPU
DEBUG 01-15 16:10:36.974309.974309 lmp.py:1935]   Expert 45 |     36 | CPU
DEBUG 01-15 16:10:36.974429.974429 lmp.py:1935]   Expert  9 |     62 | CPU
DEBUG 01-15 16:10:36.974549.974549 lmp.py:1935]   Expert 54 |     82 | CPU
DEBUG 01-15 16:10:36.974430.974430 lmp.py:1935]   Expert  0 |     83 | CPU
DEBUG 01-15 16:10:36.974458.974458 lmp.py:1935]   Expert 43 |     85 | CPU
DEBUG 01-15 16:10:36.974200.974200 lmp.py:1935]   Expert 20 |     87 | CPU
DEBUG 01-15 16:10:36.974989.974989 lmp.py:1935]   Expert 57 |     89 | CPU
DEBUG 01-15 16:10:36.974539.974539 lmp.py:1935]   Expert  6 |     90 | CPU
DEBUG 01-15 16:10:36.974613.974613 lmp.py:1935]   Expert 47 |     90 | CPU
DEBUG 01-15 16:10:36.975448.975448 lmp.py:1935]   Expert 36 |     94 | CPU
DEBUG 01-15 16:10:36.975806.975806 lmp.py:1935]   Expert 50 |    105 | CPU
DEBUG 01-15 16:10:36.975164.975164 lmp.py:1935]   Expert 13 |    106 | CPU
DEBUG 01-15 16:10:36.975999.975999 lmp.py:1935]   Expert 15 |    106 | CPU
DEBUG 01-15 16:10:36.975357.975357 lmp.py:1935]   Expert 62 |    106 | CPU
DEBUG 01-15 16:10:36.975477.975477 lmp.py:1935]   Expert 61 |    107 | CPU
DEBUG 01-15 16:10:36.975074.975074 lmp.py:1935]   Expert  1 |    108 | CPU
DEBUG 01-15 16:10:36.975863.975863 lmp.py:1935]   Expert 38 |    112 | CPU
DEBUG 01-15 16:10:36.975175.975175 lmp.py:1935]   Expert 37 |    116 | CPU
DEBUG 01-15 16:10:36.975010.975010 lmp.py:1935]   Expert 14 |    122 | CPU
DEBUG 01-15 16:10:36.975322.975322 lmp.py:1935]   Expert 46 |    124 | CPU
DEBUG 01-15 16:10:36.975157.975157 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:10:36.975753.975753 lmp.py:1935]   Expert  7 |    137 | CPU
DEBUG 01-15 16:10:36.975111.975111 lmp.py:1935]   Expert 21 |    138 | CPU
DEBUG 01-15 16:10:36.975470.975470 lmp.py:1935]   Expert 52 |    142 | CPU
DEBUG 01-15 16:10:36.975066.975066 lmp.py:1935]   Expert 44 |    145 | CPU
DEBUG 01-15 16:10:36.975663.975663 lmp.py:1935]   Expert 42 |    149 | CPU
DEBUG 01-15 16:10:36.975021.975021 lmp.py:1935]   Expert 24 |    150 | CPU
DEBUG 01-15 16:10:36.975333.975333 lmp.py:1935]   Expert 10 |    152 | CPU
DEBUG 01-15 16:10:36.975407.975407 lmp.py:1935]   Expert 11 |    159 | CPU
DEBUG 01-15 16:10:36.975719.975719 lmp.py:1935]   Expert  2 |    165 | CPU
DEBUG 01-15 16:10:36.975938.975938 lmp.py:1935]   Expert 26 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975203.975203 lmp.py:1935]   Expert 35 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975231.975231 lmp.py:1935]   Expert 31 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975019.975019 lmp.py:1935]   Expert  3 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975285.975285 lmp.py:1935]   Expert 19 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975312.975312 lmp.py:1935]   Expert 32 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975101.975101 lmp.py:1935]   Expert 12 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975651.975651 lmp.py:1935]   Expert 60 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975917.975917 lmp.py:1935]   Expert 56 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975335.975335 lmp.py:1935]   Expert 40 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975316.975316 lmp.py:1935]   Expert 41 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975581.975581 lmp.py:1935]   Expert 16 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975608.975608 lmp.py:1935]   Expert 23 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975920.975920 lmp.py:1935]   Expert 53 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975471.975471 lmp.py:1935]   Expert  8 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975259.975259 lmp.py:1935]   Expert 58 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975571.975571 lmp.py:1935]   Expert 51 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975360.975360 lmp.py:1935]   Expert 59 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975387.975387 lmp.py:1935]   Expert  4 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975176.975176 lmp.py:1935]   Expert 55 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975157.975157 lmp.py:1935]   Expert 49 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975184.975184 lmp.py:1935]   Expert 29 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975973.975973 lmp.py:1935]   Expert 18 |    284 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975046.975046 lmp.py:1935]   Expert 34 |    284 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975358.975358 lmp.py:1935]   Expert 63 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975670.975670 lmp.py:1935]   Expert 27 |    357 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975982.975982 lmp.py:1935]   Expert 39 |    381 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975201.975201 lmp.py:1935]   Expert 17 |    398 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.975467.975467 lmp.py:1935]   Expert 22 |    426 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.975686.975686 lmp.py:1935]   Expert 33 |    449 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.976713.976713 lmp.py:1935]   Expert 30 |    454 | GPU2(cuda:2)
DEBUG 01-15 16:10:36.976263.976263 lmp.py:1935]   Expert  5 |    711 | GPU1(cuda:1)
DEBUG 01-15 16:10:36.976582.976582 lmp.py:1937] 
DEBUG 01-15 16:10:36.976582.976582 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:36.976132.976132 lmp.py:1938]   CPU:   3429 tokens
DEBUG 01-15 16:10:36.976921.976921 lmp.py:1942]   cuda:1:   4467 tokens (16 experts)
DEBUG 01-15 16:10:36.976233.976233 lmp.py:1942]   cuda:2:   4392 tokens (16 experts)
DEBUG 01-15 16:10:36.976307.976307 lmp.py:1943]   Total GPU:   8859 tokens
DEBUG 01-15 16:10:36.976426.976426 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:36.976426.976426 lmp.py:1944] 
DEBUG 01-15 16:10:36.976983.976983 cuda_h.py:19] end experts_map_get cost 0.0019965171813964844 seconds
INFO 01-15 16:10:36.976735.976735 client.py:127] Model loaded
DEBUG 01-15 16:10:36.976426.976426 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.976332.976332 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:36.976923.976923 lmp.py:1953] 
DEBUG 01-15 16:10:36.976923.976923 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:36.976998.976998 cuda_h.py:19] end cpu_experts_submit cost 5.507469177246094e-05 seconds
DEBUG 01-15 16:10:36.976171.976171 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:36.976292.976292 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:36.977532.977532 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.34.gate_proj.weight': 0, 'model.layers.22.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.35.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.35.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.35.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.39.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.39.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.39.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.41.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.41.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.41.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.12.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.12.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.12.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.49.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.49.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.49.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.31.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.31.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.31.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.22.mlp.experts.32.gate_proj.weight': 0, 'model.layers.22.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.26.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.26.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.26.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.17.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.17.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.17.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.18.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.18.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.18.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.51.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.51.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.51.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.53.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.53.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.53.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(26701987840, 5767168, 0, 0), (26707755008, 5767168, 5767168, 0), (26696220672, 5767168, 11534336, 0), (26165641216, 5767168, 17301504, 0), (26171408384, 5767168, 23068672, 0), (26159874048, 5767168, 28835840, 0), (26182942720, 5767168, 34603008, 0), (26188709888, 5767168, 40370176, 0), (26177175552, 5767168, 46137344, 0), (26200244224, 5767168, 51904512, 0), (26206011392, 5767168, 57671680, 0), (26194477056, 5767168, 63438848, 0), (26719289344, 5767168, 69206016, 0), (26725056512, 5767168, 74973184, 0), (26713522176, 5767168, 80740352, 0), (26788495360, 5767168, 86507520, 0), (26794262528, 5767168, 92274688, 0), (26782728192, 5767168, 98041856, 0), (26252148736, 5767168, 103809024, 0), (26257915904, 5767168, 109576192, 0), (26246381568, 5767168, 115343360, 0), (26823098368, 5767168, 121110528, 0), (26828865536, 5767168, 126877696, 0), (26817331200, 5767168, 132644864, 0), (26321354752, 5767168, 138412032, 0), (26327121920, 5767168, 144179200, 0), (26315587584, 5767168, 149946368, 0), (26390560768, 5767168, 155713536, 0), (26396327936, 5767168, 161480704, 0), (26384793600, 5767168, 167247872, 0), (26961510400, 5767168, 173015040, 0), (26967277568, 5767168, 178782208, 0), (26955743232, 5767168, 184549376, 0), (26494369792, 5767168, 190316544, 0), (26500136960, 5767168, 196083712, 0), (26488602624, 5767168, 201850880, 0), (27082620928, 5767168, 207618048, 0), (27088388096, 5767168, 213385216, 0), (27076853760, 5767168, 219152384, 0), (27134525440, 5767168, 224919552, 0), (27140292608, 5767168, 230686720, 0), (27128758272, 5767168, 236453888, 0), (26650083328, 5767168, 242221056, 0), (26655850496, 5767168, 247988224, 0), (26644316160, 5767168, 253755392, 0), (27203731456, 5767168, 259522560, 0), (27209498624, 5767168, 265289728, 0), (27197964288, 5767168, 271056896, 0)], 2: [(26667384832, 5767168, 0, 0), (26673152000, 5767168, 5767168, 0), (26661617664, 5767168, 11534336, 0), (26684686336, 5767168, 17301504, 0), (26690453504, 5767168, 23068672, 0), (26678919168, 5767168, 28835840, 0), (26563575808, 5767168, 34603008, 0), (26569342976, 5767168, 40370176, 0), (26557808640, 5767168, 46137344, 0), (26805796864, 5767168, 51904512, 0), (26811564032, 5767168, 57671680, 0), (26800029696, 5767168, 63438848, 0), (26407862272, 5767168, 69206016, 0), (26413629440, 5767168, 74973184, 0), (26402095104, 5767168, 80740352, 0), (26425163776, 5767168, 86507520, 0), (26430930944, 5767168, 92274688, 0), (26419396608, 5767168, 98041856, 0), (26996113408, 5767168, 103809024, 0), (27001880576, 5767168, 109576192, 0), (26990346240, 5767168, 115343360, 0), (26511671296, 5767168, 121110528, 0), (26517438464, 5767168, 126877696, 0), (26505904128, 5767168, 132644864, 0), (27030716416, 5767168, 138412032, 0), (27036483584, 5767168, 144179200, 0), (27024949248, 5767168, 149946368, 0), (26442465280, 5767168, 155713536, 0), (26448232448, 5767168, 161480704, 0), (26436698112, 5767168, 167247872, 0), (27065319424, 5767168, 173015040, 0), (27071086592, 5767168, 178782208, 0), (27059552256, 5767168, 184549376, 0), (27117223936, 5767168, 190316544, 0), (27122991104, 5767168, 196083712, 0), (27111456768, 5767168, 201850880, 0), (26580877312, 5767168, 207618048, 0), (26586644480, 5767168, 213385216, 0), (26575110144, 5767168, 219152384, 0), (27151826944, 5767168, 224919552, 0), (27157594112, 5767168, 230686720, 0), (27146059776, 5767168, 236453888, 0), (26615480320, 5767168, 242221056, 0), (26621247488, 5767168, 247988224, 0), (26609713152, 5767168, 253755392, 0), (26632781824, 5767168, 259522560, 0), (26638548992, 5767168, 265289728, 0), (26627014656, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e5420a1c0>, 2: <capsule object NULL at 0x7a4e5420a490>}
DEBUG 01-15 16:10:36.977033.977033 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:36.977108.977108 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:36.977387.977387 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bd0c6caa-f18f-4c2f-ad59-f153156f6836
DEBUG 01-15 16:10:36.977314.977314 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:36.977210.977210 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:36.978835.978835 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:36.978255.978255 cuda_h.py:19] end restore2model cost 0.0019783973693847656 seconds
DEBUG 01-15 16:10:36.978853.978853 cuda_h.py:19] end sllm_worker_task cost 0.011178731918334961 seconds
INFO 01-15 16:10:36.978545.978545 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bd0c6caa-f18f-4c2f-ad59-f153156f6836
DEBUG 01-15 16:10:36.979998.979998 cuda_h.py:19] end move_flatidxs cost 0.0008318424224853516 seconds
DEBUG 01-15 16:10:36.979158.979158 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:36.979256.979256 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002642393112182617 seconds
DEBUG 01-15 16:10:36.979689.979689 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:36.982303.982303 cuda_h.py:19] end restore2model cost 0.003086566925048828 seconds
DEBUG 01-15 16:10:36.982716.982716 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005959033966064453 seconds
DEBUG 01-15 16:10:36.982439.982439 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:36.983768.983768 cuda_h.py:19] end gpu_sexperts cost 0.0003151893615722656 seconds
DEBUG 01-15 16:10:36.983790.983790 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:36.983474.983474 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:10:36.983792.983792 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:36.983118.983118 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.984691.984691 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001056671142578125 seconds
DEBUG 01-15 16:10:36.984203.984203 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.984502.984502 cuda_h.py:19] end gpu_group_list cost 0.00018930435180664062 seconds
DEBUG 01-15 16:10:36.985759.985759 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:36.986119.986119 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011849403381347656 seconds
DEBUG 01-15 16:10:36.986545.986545 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:36.987711.987711 cuda_h.py:19] end gpu_group_list cost 0.0001811981201171875 seconds
DEBUG 01-15 16:10:36.987062.987062 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:36.987706.987706 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bd0c6caa-f18f-4c2f-ad59-f153156f6836
DEBUG 01-15 16:10:36.988762.988762 cuda_h.py:19] end group_tensors cost 0.009289264678955078 seconds
DEBUG 01-15 16:10:36.989689.989689 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:36.993269.993269 cuda_h.py:19] end group pad cost 0.004026174545288086 seconds
DEBUG 01-15 16:10:36.993159.993159 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:37.005026.005026 client.py:127] Model loaded
DEBUG 01-15 16:10:37.005814.005814 cuda_h.py:19] end wait_experts_multi_device cost 0.017437458038330078 seconds
DEBUG 01-15 16:10:37.005465.005465 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:37.012606.012606 cuda_h.py:19] end group_einsum cost 0.019189834594726562 seconds
DEBUG 01-15 16:10:37.012101.012101 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:37.016375.016375 cuda_h.py:19] end get_outputs_cpu1 cost 0.003363370895385742 seconds
DEBUG 01-15 16:10:37.016906.016906 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03893280029296875 seconds
DEBUG 01-15 16:10:37.017758.017758 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012054204940795898 seconds
DEBUG 01-15 16:10:37.017855.017855 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:37.019939.019939 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.019594.019594 cuda_h.py:19] end index_scatter cost 8.702278137207031e-05 seconds
DEBUG 01-15 16:10:37.019227.019227 cuda_h.py:19] end cpuoutputsdeal cost 0.001988649368286133 seconds
DEBUG 01-15 16:10:37.019197.019197 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:37.019152.019152 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.019417.019417 cuda_h.py:19] end gpu_group_tensor cost 0.0001614093780517578 seconds
DEBUG 01-15 16:10:37.020941.020941 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.020007.020007 cuda_h.py:19] end gpu_group_tensor cost 0.0001518726348876953 seconds
DEBUG 01-15 16:10:37.020686.020686 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.021223.021223 cuda_h.py:19] end gpu_group_einsum cost 0.0007615089416503906 seconds
DEBUG 01-15 16:10:37.021494.021494 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.021791.021791 cuda_h.py:19] end gpu_group_einsum cost 0.0005083084106445312 seconds
DEBUG 01-15 16:10:37.021616.021616 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.022342.022342 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.022775.022775 cuda_h.py:19] end all_expert_outputs_slices cost 0.000244140625 seconds
DEBUG 01-15 16:10:37.022869.022869 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.022753.022753 cuda_h.py:19] end concat_expert_out cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:10:37.022649.022649 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.022076.022076 cuda_h.py:19] end index_scatter cost 6.747245788574219e-05 seconds
DEBUG 01-15 16:10:37.022244.022244 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009045600891113281 seconds
DEBUG 01-15 16:10:37.023910.023910 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.023760.023760 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.023086.023086 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020647048950195312 seconds
DEBUG 01-15 16:10:37.023034.023034 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.023494.023494 cuda_h.py:19] end concat_expert_out cost 5.9604644775390625e-05 seconds
DEBUG 01-15 16:10:37.023198.023198 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.023049.023049 cuda_h.py:19] end index_scatter cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:10:37.023719.023719 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005929470062255859 seconds
DEBUG 01-15 16:10:37.023258.023258 cuda_h.py:19] end gpu_experts_multi_device cost 0.04059147834777832 seconds
DEBUG 01-15 16:10:37.023135.023135 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.05049300193786621 seconds
DEBUG 01-15 16:10:37.024767.024767 cuda_h.py:19] end prefill_layer cost 0.05752205848693848 seconds
DEBUG 01-15 16:10:37.024332.024332 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:10:37.024817.024817 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:37.024811.024811 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:10:37.024329.024329 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:37.024237.024237 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:37.024624.024624 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 4.220008850097656e-05 seconds
DEBUG 01-15 16:10:37.024956.024956 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 7.867813110351562e-05 seconds
DEBUG 01-15 16:10:37.024229.024229 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:37.024993.024993 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:37.024944.024944 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:37.024913.024913 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:37.025858.025858 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:37.025265.025265 cuda_h.py:19] end allocate_cuda_memory cost 0.0002613067626953125 seconds
DEBUG 01-15 16:10:37.025096.025096 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:37.025196.025196 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:37.025093.025093 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:37.025233.025233 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7aedd77a-7ec8-4cfe-b75a-e349faa7ff1d
DEBUG 01-15 16:10:37.025667.025667 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:37.025359.025359 cuda_h.py:10] start self_attn
INFO 01-15 16:10:37.026656.026656 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7aedd77a-7ec8-4cfe-b75a-e349faa7ff1d
DEBUG 01-15 16:10:37.026684.026684 cuda_h.py:19] end load_into_gpu_async cost 0.0013074874877929688 seconds
DEBUG 01-15 16:10:37.026533.026533 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:37.026265.026265 cuda_h.py:19] end restore_tensors2 cost 8.702278137207031e-05 seconds
DEBUG 01-15 16:10:37.026313.026313 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0019397735595703125 seconds
INFO 01-15 16:10:37.027023.027023 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7aedd77a-7ec8-4cfe-b75a-e349faa7ff1d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:37.030791.030791 cuda_h.py:19] end self_attn cost 0.0045278072357177734 seconds
DEBUG 01-15 16:10:37.030771.030771 cuda_h.py:19] end iln_self_attn_paln cost 0.006262540817260742 seconds
DEBUG 01-15 16:10:37.030554.030554 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:10:37.031648.031648 cuda_h.py:10] start gate
DEBUG 01-15 16:10:37.031819.031819 cuda_h.py:19] end gate cost 0.0007221698760986328 seconds
DEBUG 01-15 16:10:37.031509.031509 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:37.032784.032784 lmp.py:1912] 
DEBUG 01-15 16:10:37.032784.032784 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:37.032016.032016 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:37.032362.032362 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:37.032986.032986 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:37.032060.032060 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:37.032179.032179 lmp.py:1917] 
DEBUG 01-15 16:10:37.032179.032179 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:37.032253.032253 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:37.032572.032572 lmp.py:1935]   Expert  5 |     13 | CPU
DEBUG 01-15 16:10:37.032168.032168 lmp.py:1935]   Expert 56 |     33 | CPU
DEBUG 01-15 16:10:37.032050.032050 lmp.py:1935]   Expert 16 |     85 | CPU
DEBUG 01-15 16:10:37.032169.032169 lmp.py:1935]   Expert 27 |     86 | CPU
DEBUG 01-15 16:10:37.032005.032005 lmp.py:1935]   Expert 17 |     89 | CPU
DEBUG 01-15 16:10:37.032124.032124 lmp.py:1935]   Expert 40 |     94 | CPU
DEBUG 01-15 16:10:37.032628.032628 lmp.py:1935]   Expert 63 |    103 | CPU
DEBUG 01-15 16:10:37.032179.032179 lmp.py:1935]   Expert 49 |    105 | CPU
DEBUG 01-15 16:10:37.032537.032537 lmp.py:1935]   Expert 53 |    105 | CPU
DEBUG 01-15 16:10:37.032895.032895 lmp.py:1935]   Expert 51 |    106 | CPU
DEBUG 01-15 16:10:37.032492.032492 lmp.py:1935]   Expert 28 |    107 | CPU
DEBUG 01-15 16:10:37.032088.032088 lmp.py:1935]   Expert  7 |    112 | CPU
DEBUG 01-15 16:10:37.032924.032924 lmp.py:1935]   Expert 47 |    118 | CPU
DEBUG 01-15 16:10:37.032474.032474 lmp.py:1935]   Expert 38 |    120 | CPU
DEBUG 01-15 16:10:37.032786.032786 lmp.py:1935]   Expert 62 |    123 | CPU
DEBUG 01-15 16:10:37.032859.032859 lmp.py:1935]   Expert 37 |    125 | CPU
DEBUG 01-15 16:10:37.032886.032886 lmp.py:1935]   Expert 11 |    129 | CPU
DEBUG 01-15 16:10:37.032721.032721 lmp.py:1935]   Expert 58 |    131 | CPU
DEBUG 01-15 16:10:37.032318.032318 lmp.py:1935]   Expert 57 |    138 | CPU
DEBUG 01-15 16:10:37.032438.032438 lmp.py:1935]   Expert  1 |    147 | CPU
DEBUG 01-15 16:10:37.032796.032796 lmp.py:1935]   Expert 39 |    149 | CPU
DEBUG 01-15 16:10:37.032393.032393 lmp.py:1935]   Expert 52 |    151 | CPU
DEBUG 01-15 16:10:37.032751.032751 lmp.py:1935]   Expert 14 |    152 | CPU
DEBUG 01-15 16:10:37.032871.032871 lmp.py:1935]   Expert 25 |    152 | CPU
DEBUG 01-15 16:10:37.032468.032468 lmp.py:1935]   Expert 23 |    157 | CPU
DEBUG 01-15 16:10:37.032779.032779 lmp.py:1935]   Expert 33 |    160 | CPU
DEBUG 01-15 16:10:37.033091.033091 lmp.py:1935]   Expert 21 |    164 | CPU
DEBUG 01-15 16:10:37.033165.033165 lmp.py:1935]   Expert  6 |    170 | CPU
DEBUG 01-15 16:10:37.033238.033238 lmp.py:1935]   Expert 60 |    172 | CPU
DEBUG 01-15 16:10:37.033597.033597 lmp.py:1935]   Expert 45 |    175 | CPU
DEBUG 01-15 16:10:37.033716.033716 lmp.py:1935]   Expert  4 |    182 | CPU
DEBUG 01-15 16:10:37.033075.033075 lmp.py:1935]   Expert 19 |    182 | CPU
DEBUG 01-15 16:10:37.033340.033340 lmp.py:1935]   Expert 44 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033890.033890 lmp.py:1935]   Expert 12 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033441.033441 lmp.py:1935]   Expert 30 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033753.033753 lmp.py:1935]   Expert 31 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033065.033065 lmp.py:1935]   Expert 55 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033615.033615 lmp.py:1935]   Expert  3 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033642.033642 lmp.py:1935]   Expert 36 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033146.033146 lmp.py:1935]   Expert  9 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033173.033173 lmp.py:1935]   Expert  0 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033631.033631 lmp.py:1935]   Expert 34 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033658.033658 lmp.py:1935]   Expert 22 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033208.033208 lmp.py:1935]   Expert 41 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033759.033759 lmp.py:1935]   Expert 26 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033594.033594 lmp.py:1935]   Expert 54 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033144.033144 lmp.py:1935]   Expert 43 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033694.033694 lmp.py:1935]   Expert 59 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033006.033006 lmp.py:1935]   Expert 18 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033795.033795 lmp.py:1935]   Expert 13 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033061.033061 lmp.py:1935]   Expert 20 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033326.033326 lmp.py:1935]   Expert 50 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033115.033115 lmp.py:1935]   Expert 15 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033904.033904 lmp.py:1935]   Expert 24 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033454.033454 lmp.py:1935]   Expert 29 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033766.033766 lmp.py:1935]   Expert 42 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033555.033555 lmp.py:1935]   Expert 61 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033628.033628 lmp.py:1935]   Expert 35 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033940.033940 lmp.py:1935]   Expert 32 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033729.033729 lmp.py:1935]   Expert  2 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033802.033802 lmp.py:1935]   Expert  8 |    340 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033114.033114 lmp.py:1935]   Expert 10 |    344 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033665.033665 lmp.py:1935]   Expert 46 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.033738.033738 lmp.py:1935]   Expert 48 |    445 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.033858.033858 lmp.py:1937] 
DEBUG 01-15 16:10:37.033858.033858 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:37.033931.033931 lmp.py:1938]   CPU:   4035 tokens
DEBUG 01-15 16:10:37.033243.033243 lmp.py:1942]   cuda:1:   4127 tokens (16 experts)
DEBUG 01-15 16:10:37.033608.033608 lmp.py:1942]   cuda:2:   4126 tokens (16 experts)
DEBUG 01-15 16:10:37.033966.033966 lmp.py:1943]   Total GPU:   8253 tokens
DEBUG 01-15 16:10:37.033325.033325 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:37.033325.033325 lmp.py:1944] 
DEBUG 01-15 16:10:37.033643.033643 cuda_h.py:19] end experts_map_get cost 0.002000570297241211 seconds
INFO 01-15 16:10:37.033354.033354 client.py:127] Model loaded
DEBUG 01-15 16:10:37.033568.033568 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:37.034077.034077 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:37.034429.034429 lmp.py:1953] 
DEBUG 01-15 16:10:37.034429.034429 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:37.034266.034266 cuda_h.py:19] end cpu_experts_submit cost 5.507469177246094e-05 seconds
DEBUG 01-15 16:10:37.034392.034392 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:37.034321.034321 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:37.034660.034660 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.32.gate_proj.weight': 0, 'model.layers.23.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.0.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.0.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.0.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.22.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.22.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.22.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.23.mlp.experts.2.gate_proj.weight': 0, 'model.layers.23.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.9.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.9.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.9.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.44.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.44.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.44.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.20.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.20.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.20.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.54.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.54.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.54.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(27774681088, 5767168, 0, 0), (27780448256, 5767168, 5767168, 0), (27768913920, 5767168, 11534336, 0), (27221032960, 5767168, 17301504, 0), (27226800128, 5767168, 23068672, 0), (27215265792, 5767168, 28835840, 0), (27826585600, 5767168, 34603008, 0), (27832352768, 5767168, 40370176, 0), (27820818432, 5767168, 46137344, 0), (27843887104, 5767168, 51904512, 0), (27849654272, 5767168, 57671680, 0), (27838119936, 5767168, 63438848, 0), (27359444992, 5767168, 69206016, 0), (27365212160, 5767168, 74973184, 0), (27353677824, 5767168, 80740352, 0), (27428651008, 5767168, 86507520, 0), (27434418176, 5767168, 92274688, 0), (27422883840, 5767168, 98041856, 0), (27445952512, 5767168, 103809024, 0), (27451719680, 5767168, 109576192, 0), (27440185344, 5767168, 115343360, 0), (28051505152, 5767168, 121110528, 0), (28057272320, 5767168, 126877696, 0), (28045737984, 5767168, 132644864, 0), (28086108160, 5767168, 138412032, 0), (28091875328, 5767168, 144179200, 0), (28080340992, 5767168, 149946368, 0), (27601666048, 5767168, 155713536, 0), (27607433216, 5767168, 161480704, 0), (27595898880, 5767168, 167247872, 0), (28172615680, 5767168, 173015040, 0), (28178382848, 5767168, 178782208, 0), (28166848512, 5767168, 184549376, 0), (27636269056, 5767168, 190316544, 0), (27642036224, 5767168, 196083712, 0), (27630501888, 5767168, 201850880, 0), (27670872064, 5767168, 207618048, 0), (27676639232, 5767168, 213385216, 0), (27665104896, 5767168, 219152384, 0), (28241821696, 5767168, 224919552, 0), (28247588864, 5767168, 230686720, 0), (28236054528, 5767168, 236453888, 0), (27722776576, 5767168, 242221056, 0), (27728543744, 5767168, 247988224, 0), (27717009408, 5767168, 253755392, 0), (27740078080, 5767168, 259522560, 0), (27745845248, 5767168, 265289728, 0), (27734310912, 5767168, 271056896, 0)], 2: [(27255635968, 5767168, 0, 0), (27261403136, 5767168, 5767168, 0), (27249868800, 5767168, 11534336, 0), (27809284096, 5767168, 17301504, 0), (27815051264, 5767168, 23068672, 0), (27803516928, 5767168, 28835840, 0), (27272937472, 5767168, 34603008, 0), (27278704640, 5767168, 40370176, 0), (27267170304, 5767168, 46137344, 0), (27930394624, 5767168, 51904512, 0), (27936161792, 5767168, 57671680, 0), (27924627456, 5767168, 63438848, 0), (27394048000, 5767168, 69206016, 0), (27399815168, 5767168, 74973184, 0), (27388280832, 5767168, 80740352, 0), (27947696128, 5767168, 86507520, 0), (27953463296, 5767168, 92274688, 0), (27941928960, 5767168, 98041856, 0), (27964997632, 5767168, 103809024, 0), (27970764800, 5767168, 109576192, 0), (27959230464, 5767168, 115343360, 0), (27376746496, 5767168, 121110528, 0), (27382513664, 5767168, 126877696, 0), (27370979328, 5767168, 132644864, 0), (28016902144, 5767168, 138412032, 0), (28022669312, 5767168, 144179200, 0), (28011134976, 5767168, 149946368, 0), (27480555520, 5767168, 155713536, 0), (27486322688, 5767168, 161480704, 0), (27474788352, 5767168, 167247872, 0), (27982299136, 5767168, 173015040, 0), (27988066304, 5767168, 178782208, 0), (27976531968, 5767168, 184549376, 0), (27532460032, 5767168, 190316544, 0), (27538227200, 5767168, 196083712, 0), (27526692864, 5767168, 201850880, 0), (27567063040, 5767168, 207618048, 0), (27572830208, 5767168, 213385216, 0), (27561295872, 5767168, 219152384, 0), (28155314176, 5767168, 224919552, 0), (28161081344, 5767168, 230686720, 0), (28149547008, 5767168, 236453888, 0), (28276424704, 5767168, 242221056, 0), (28282191872, 5767168, 247988224, 0), (28270657536, 5767168, 253755392, 0), (27757379584, 5767168, 259522560, 0), (27763146752, 5767168, 265289728, 0), (27751612416, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b0644270>, 2: <capsule object NULL at 0x7a4e5420a3a0>}
DEBUG 01-15 16:10:37.035122.035122 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:37.035581.035581 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:37.035052.035052 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 541e5240-1497-4382-a20b-1bcf6c3e993f
DEBUG 01-15 16:10:37.035238.035238 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:37.035302.035302 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:37.036716.036716 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:37.036723.036723 cuda_h.py:19] end restore2model cost 0.0020058155059814453 seconds
DEBUG 01-15 16:10:37.036082.036082 cuda_h.py:19] end sllm_worker_task cost 0.011197328567504883 seconds
INFO 01-15 16:10:37.036631.036631 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 541e5240-1497-4382-a20b-1bcf6c3e993f
DEBUG 01-15 16:10:37.036461.036461 cuda_h.py:19] end move_flatidxs cost 0.0008380413055419922 seconds
DEBUG 01-15 16:10:37.036244.036244 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:37.037441.037441 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0027577877044677734 seconds
DEBUG 01-15 16:10:37.037113.037113 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:37.040852.040852 cuda_h.py:19] end restore2model cost 0.003074169158935547 seconds
DEBUG 01-15 16:10:37.040550.040550 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006060600280761719 seconds
DEBUG 01-15 16:10:37.040882.040882 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:37.040887.040887 cuda_h.py:19] end gpu_sexperts cost 0.00031113624572753906 seconds
DEBUG 01-15 16:10:37.040962.040962 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:37.040122.040122 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:10:37.040249.040249 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:37.040005.040005 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.042453.042453 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010688304901123047 seconds
DEBUG 01-15 16:10:37.042534.042534 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.042389.042389 cuda_h.py:19] end gpu_group_list cost 0.00017786026000976562 seconds
DEBUG 01-15 16:10:37.043912.043912 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.044470.044470 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011906623840332031 seconds
DEBUG 01-15 16:10:37.044896.044896 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.044135.044135 cuda_h.py:19] end gpu_group_list cost 0.00017976760864257812 seconds
DEBUG 01-15 16:10:37.045009.045009 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:37.045607.045607 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 541e5240-1497-4382-a20b-1bcf6c3e993f
DEBUG 01-15 16:10:37.046518.046518 cuda_h.py:19] end group_tensors cost 0.009321451187133789 seconds
DEBUG 01-15 16:10:37.047723.047723 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:37.051686.051686 cuda_h.py:19] end group pad cost 0.003991365432739258 seconds
DEBUG 01-15 16:10:37.051053.051053 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:37.062642.062642 client.py:127] Model loaded
DEBUG 01-15 16:10:37.062635.062635 cuda_h.py:19] end wait_experts_multi_device cost 0.01723647117614746 seconds
DEBUG 01-15 16:10:37.062657.062657 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:37.071496.071496 cuda_h.py:19] end group_einsum cost 0.01969432830810547 seconds
DEBUG 01-15 16:10:37.071283.071283 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:37.075064.075064 cuda_h.py:19] end get_outputs_cpu1 cost 0.0038728713989257812 seconds
DEBUG 01-15 16:10:37.075039.075039 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03993487358093262 seconds
DEBUG 01-15 16:10:37.076222.076222 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01323556900024414 seconds
DEBUG 01-15 16:10:37.076987.076987 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:37.077059.077059 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.078873.078873 cuda_h.py:19] end index_scatter cost 9.107589721679688e-05 seconds
DEBUG 01-15 16:10:37.078778.078778 cuda_h.py:19] end cpuoutputsdeal cost 0.0019991397857666016 seconds
DEBUG 01-15 16:10:37.078648.078648 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:37.078649.078649 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.078536.078536 cuda_h.py:19] end gpu_group_tensor cost 0.0001609325408935547 seconds
DEBUG 01-15 16:10:37.078822.078822 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.078358.078358 cuda_h.py:19] end gpu_group_tensor cost 0.00015211105346679688 seconds
DEBUG 01-15 16:10:37.079230.079230 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.079930.079930 cuda_h.py:19] end gpu_group_einsum cost 0.0006735324859619141 seconds
DEBUG 01-15 16:10:37.079147.079147 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.080710.080710 cuda_h.py:19] end gpu_group_einsum cost 0.0005297660827636719 seconds
DEBUG 01-15 16:10:37.080879.080879 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.080651.080651 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.081622.081622 cuda_h.py:19] end all_expert_outputs_slices cost 0.00025343894958496094 seconds
DEBUG 01-15 16:10:37.081762.081762 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.081977.081977 cuda_h.py:19] end concat_expert_out cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:10:37.081634.081634 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.081439.081439 cuda_h.py:19] end index_scatter cost 6.532669067382812e-05 seconds
DEBUG 01-15 16:10:37.081660.081660 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000911712646484375 seconds
DEBUG 01-15 16:10:37.081803.081803 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.081513.081513 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.082555.082555 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002071857452392578 seconds
DEBUG 01-15 16:10:37.082264.082264 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.082486.082486 cuda_h.py:19] end concat_expert_out cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:10:37.082283.082283 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.082180.082180 cuda_h.py:19] end index_scatter cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:10:37.082373.082373 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005927085876464844 seconds
DEBUG 01-15 16:10:37.082535.082535 cuda_h.py:19] end gpu_experts_multi_device cost 0.04154706001281738 seconds
DEBUG 01-15 16:10:37.082604.082604 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.051516056060791016 seconds
DEBUG 01-15 16:10:37.083858.083858 cuda_h.py:19] end prefill_layer cost 0.0585637092590332 seconds
DEBUG 01-15 16:10:37.083708.083708 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:10:37.083272.083272 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:37.083505.083505 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:10:37.083022.083022 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:37.083163.083163 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:37.083887.083887 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 4.601478576660156e-05 seconds
DEBUG 01-15 16:10:37.083743.083743 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 8.249282836914062e-05 seconds
DEBUG 01-15 16:10:37.083445.083445 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:37.083064.083064 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:37.083539.083539 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:37.083315.083315 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:37.083975.083975 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:37.084442.084442 cuda_h.py:19] end allocate_cuda_memory cost 0.00026869773864746094 seconds
DEBUG 01-15 16:10:37.084180.084180 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:37.084566.084566 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:37.084872.084872 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:37.084628.084628 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 364ba9ab-ad1c-4b22-b25e-6c91b4dccc60
DEBUG 01-15 16:10:37.084016.084016 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:37.084848.084848 cuda_h.py:10] start self_attn
INFO 01-15 16:10:37.085788.085788 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 364ba9ab-ad1c-4b22-b25e-6c91b4dccc60
DEBUG 01-15 16:10:37.085969.085969 cuda_h.py:19] end load_into_gpu_async cost 0.0015599727630615234 seconds
DEBUG 01-15 16:10:37.085295.085295 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:37.085543.085543 cuda_h.py:19] end restore_tensors2 cost 8.225440979003906e-05 seconds
DEBUG 01-15 16:10:37.085783.085783 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002201080322265625 seconds
INFO 01-15 16:10:37.085401.085401 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 364ba9ab-ad1c-4b22-b25e-6c91b4dccc60
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:37.089243.089243 cuda_h.py:19] end self_attn cost 0.004391670227050781 seconds
DEBUG 01-15 16:10:37.089740.089740 cuda_h.py:19] end iln_self_attn_paln cost 0.006170034408569336 seconds
DEBUG 01-15 16:10:37.089616.089616 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:10:37.089186.089186 cuda_h.py:10] start gate
DEBUG 01-15 16:10:37.090960.090960 cuda_h.py:19] end gate cost 0.0007085800170898438 seconds
DEBUG 01-15 16:10:37.090889.090889 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:37.090256.090256 lmp.py:1912] 
DEBUG 01-15 16:10:37.090256.090256 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:37.090204.090204 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:37.090522.090522 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:37.090265.090265 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:37.091862.091862 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:37.091743.091743 lmp.py:1917] 
DEBUG 01-15 16:10:37.091743.091743 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:37.091101.091101 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:37.091705.091705 lmp.py:1935]   Expert 36 |     21 | CPU
DEBUG 01-15 16:10:37.091540.091540 lmp.py:1935]   Expert 35 |     30 | CPU
DEBUG 01-15 16:10:37.091944.091944 lmp.py:1935]   Expert 25 |     45 | CPU
DEBUG 01-15 16:10:37.091349.091349 lmp.py:1935]   Expert 46 |     45 | CPU
DEBUG 01-15 16:10:37.091515.091515 lmp.py:1935]   Expert 51 |     52 | CPU
DEBUG 01-15 16:10:37.091681.091681 lmp.py:1935]   Expert 30 |     60 | CPU
DEBUG 01-15 16:10:37.091086.091086 lmp.py:1935]   Expert 16 |     61 | CPU
DEBUG 01-15 16:10:37.091490.091490 lmp.py:1935]   Expert  0 |     65 | CPU
DEBUG 01-15 16:10:37.091656.091656 lmp.py:1935]   Expert 43 |     69 | CPU
DEBUG 01-15 16:10:37.091822.091822 lmp.py:1935]   Expert 47 |     72 | CPU
DEBUG 01-15 16:10:37.091227.091227 lmp.py:1935]   Expert 44 |     73 | CPU
DEBUG 01-15 16:10:37.091062.091062 lmp.py:1935]   Expert 55 |     74 | CPU
DEBUG 01-15 16:10:37.091182.091182 lmp.py:1935]   Expert 39 |     76 | CPU
DEBUG 01-15 16:10:37.091540.091540 lmp.py:1935]   Expert 42 |     79 | CPU
DEBUG 01-15 16:10:37.091660.091660 lmp.py:1935]   Expert  2 |     81 | CPU
DEBUG 01-15 16:10:37.091303.091303 lmp.py:1935]   Expert  4 |    108 | CPU
DEBUG 01-15 16:10:37.091707.091707 lmp.py:1935]   Expert 33 |    117 | CPU
DEBUG 01-15 16:10:37.091874.091874 lmp.py:1935]   Expert 48 |    119 | CPU
DEBUG 01-15 16:10:37.091040.091040 lmp.py:1935]   Expert  6 |    122 | CPU
DEBUG 01-15 16:10:37.091206.091206 lmp.py:1935]   Expert 13 |    124 | CPU
DEBUG 01-15 16:10:37.091134.091134 lmp.py:1935]   Expert 24 |    125 | CPU
DEBUG 01-15 16:10:37.091061.091061 lmp.py:1935]   Expert 61 |    125 | CPU
DEBUG 01-15 16:10:37.091989.091989 lmp.py:1935]   Expert 29 |    130 | CPU
DEBUG 01-15 16:10:37.091917.091917 lmp.py:1935]   Expert 56 |    130 | CPU
DEBUG 01-15 16:10:37.091844.091844 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:10:37.091680.091680 lmp.py:1935]   Expert 38 |    140 | CPU
DEBUG 01-15 16:10:37.091561.091561 lmp.py:1935]   Expert 54 |    141 | CPU
DEBUG 01-15 16:10:37.091158.091158 lmp.py:1935]   Expert  9 |    143 | CPU
DEBUG 01-15 16:10:37.091754.091754 lmp.py:1935]   Expert  7 |    145 | CPU
DEBUG 01-15 16:10:37.091066.091066 lmp.py:1935]   Expert 20 |    148 | CPU
DEBUG 01-15 16:10:37.091901.091901 lmp.py:1935]   Expert 59 |    149 | CPU
DEBUG 01-15 16:10:37.091259.091259 lmp.py:1935]   Expert 19 |    159 | CPU
DEBUG 01-15 16:10:37.091525.091525 lmp.py:1935]   Expert 45 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.091599.091599 lmp.py:1935]   Expert 62 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.091910.091910 lmp.py:1935]   Expert 34 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.091984.091984 lmp.py:1935]   Expert 57 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.091057.091057 lmp.py:1935]   Expert 50 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.091892.091892 lmp.py:1935]   Expert 31 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.091966.091966 lmp.py:1935]   Expert 10 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.091993.091993 lmp.py:1935]   Expert 23 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.091543.091543 lmp.py:1935]   Expert  8 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.091094.091094 lmp.py:1935]   Expert 18 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.091121.091121 lmp.py:1935]   Expert 60 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.091433.091433 lmp.py:1935]   Expert 22 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.091268.091268 lmp.py:1935]   Expert 53 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.091341.091341 lmp.py:1935]   Expert 52 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.091938.091938 lmp.py:1935]   Expert 37 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.091773.091773 lmp.py:1935]   Expert  5 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.091370.091370 lmp.py:1935]   Expert 17 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.091966.091966 lmp.py:1935]   Expert 11 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.091755.091755 lmp.py:1935]   Expert  1 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.092305.092305 lmp.py:1935]   Expert 49 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.092617.092617 lmp.py:1935]   Expert 41 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.092168.092168 lmp.py:1935]   Expert 28 |    287 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.092241.092241 lmp.py:1935]   Expert 26 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.092766.092766 lmp.py:1935]   Expert 32 |    296 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.092316.092316 lmp.py:1935]   Expert 58 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.092628.092628 lmp.py:1935]   Expert 40 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.092701.092701 lmp.py:1935]   Expert 14 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.092775.092775 lmp.py:1935]   Expert 12 |    329 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.092087.092087 lmp.py:1935]   Expert 63 |    338 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.092399.092399 lmp.py:1935]   Expert 21 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.092472.092472 lmp.py:1935]   Expert 27 |    662 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.092214.092214 lmp.py:1935]   Expert  3 |   1016 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.092288.092288 lmp.py:1937] 
DEBUG 01-15 16:10:37.092288.092288 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:37.092600.092600 lmp.py:1938]   CPU:   3163 tokens
DEBUG 01-15 16:10:37.092389.092389 lmp.py:1942]   cuda:1:   4497 tokens (15 experts)
DEBUG 01-15 16:10:37.092462.092462 lmp.py:1942]   cuda:2:   4628 tokens (17 experts)
DEBUG 01-15 16:10:37.092820.092820 lmp.py:1943]   Total GPU:   9125 tokens
DEBUG 01-15 16:10:37.092940.092940 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:37.092940.092940 lmp.py:1944] 
DEBUG 01-15 16:10:37.092842.092842 cuda_h.py:19] end experts_map_get cost 0.001936197280883789 seconds
DEBUG 01-15 16:10:37.092129.092129 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:37.092123.092123 lmp.py:1953] 
DEBUG 01-15 16:10:37.092123.092123 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:37.092960.092960 cuda_h.py:19] end cpu_experts_submit cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:37.092802.092802 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:37.092399.092399 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:37.093387.093387 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.34.gate_proj.weight': 0, 'model.layers.24.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.58.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.58.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.58.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.31.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.31.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.31.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.24.mlp.experts.32.gate_proj.weight': 0, 'model.layers.24.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.60.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.60.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.60.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.27.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.27.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.27.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.28.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.28.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.28.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.62.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.62.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.62.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(28916580352, 5767168, 0, 0), (28922347520, 5767168, 5767168, 0), (28910813184, 5767168, 11534336, 0), (28380233728, 5767168, 17301504, 0), (28386000896, 5767168, 23068672, 0), (28374466560, 5767168, 28835840, 0), (29331816448, 5767168, 34603008, 0), (29337583616, 5767168, 40370176, 0), (29326049280, 5767168, 46137344, 0), (28968484864, 5767168, 51904512, 0), (28974252032, 5767168, 57671680, 0), (28962717696, 5767168, 63438848, 0), (28466741248, 5767168, 69206016, 0), (28472508416, 5767168, 74973184, 0), (28460974080, 5767168, 80740352, 0), (29037690880, 5767168, 86507520, 0), (29043458048, 5767168, 92274688, 0), (29031923712, 5767168, 98041856, 0), (28518645760, 5767168, 103809024, 0), (28524412928, 5767168, 109576192, 0), (28512878592, 5767168, 115343360, 0), (28570550272, 5767168, 121110528, 0), (28576317440, 5767168, 126877696, 0), (28564783104, 5767168, 132644864, 0), (28622454784, 5767168, 138412032, 0), (28628221952, 5767168, 144179200, 0), (28616687616, 5767168, 149946368, 0), (28639756288, 5767168, 155713536, 0), (28645523456, 5767168, 161480704, 0), (28633989120, 5767168, 167247872, 0), (28708962304, 5767168, 173015040, 0), (28714729472, 5767168, 178782208, 0), (28703195136, 5767168, 184549376, 0), (29314514944, 5767168, 190316544, 0), (29320282112, 5767168, 196083712, 0), (29308747776, 5767168, 201850880, 0), (28778168320, 5767168, 207618048, 0), (28783935488, 5767168, 213385216, 0), (28772401152, 5767168, 219152384, 0), (28864675840, 5767168, 224919552, 0), (28870443008, 5767168, 230686720, 0), (28858908672, 5767168, 236453888, 0), (29418323968, 5767168, 242221056, 0), (29424091136, 5767168, 247988224, 0), (29412556800, 5767168, 253755392, 0)], 2: [(28881977344, 5767168, 0, 0), (28887744512, 5767168, 5767168, 0), (28876210176, 5767168, 11534336, 0), (28345630720, 5767168, 17301504, 0), (28351397888, 5767168, 23068672, 0), (28339863552, 5767168, 28835840, 0), (28414836736, 5767168, 34603008, 0), (28420603904, 5767168, 40370176, 0), (28409069568, 5767168, 46137344, 0), (29020389376, 5767168, 51904512, 0), (29026156544, 5767168, 57671680, 0), (29014622208, 5767168, 63438848, 0), (28501344256, 5767168, 69206016, 0), (28507111424, 5767168, 74973184, 0), (28495577088, 5767168, 80740352, 0), (28535947264, 5767168, 86507520, 0), (28541714432, 5767168, 92274688, 0), (28530180096, 5767168, 98041856, 0), (29106896896, 5767168, 103809024, 0), (29112664064, 5767168, 109576192, 0), (29101129728, 5767168, 115343360, 0), (29366419456, 5767168, 121110528, 0), (29372186624, 5767168, 126877696, 0), (29360652288, 5767168, 132644864, 0), (29176102912, 5767168, 138412032, 0), (29181870080, 5767168, 144179200, 0), (29170335744, 5767168, 149946368, 0), (29193404416, 5767168, 155713536, 0), (29199171584, 5767168, 161480704, 0), (29187637248, 5767168, 167247872, 0), (29228007424, 5767168, 173015040, 0), (29233774592, 5767168, 178782208, 0), (29222240256, 5767168, 184549376, 0), (28691660800, 5767168, 190316544, 0), (28697427968, 5767168, 196083712, 0), (28685893632, 5767168, 201850880, 0), (29245308928, 5767168, 207618048, 0), (29251076096, 5767168, 213385216, 0), (29239541760, 5767168, 219152384, 0), (28726263808, 5767168, 224919552, 0), (28732030976, 5767168, 230686720, 0), (28720496640, 5767168, 236453888, 0), (28795469824, 5767168, 242221056, 0), (28801236992, 5767168, 247988224, 0), (28789702656, 5767168, 253755392, 0), (28812771328, 5767168, 259522560, 0), (28818538496, 5767168, 265289728, 0), (28807004160, 5767168, 271056896, 0), (29401022464, 5767168, 276824064, 0), (29406789632, 5767168, 282591232, 0), (29395255296, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e541f2070>, 2: <capsule object NULL at 0x7a4f2c275b30>}
DEBUG 01-15 16:10:37.093564.093564 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:37.093831.093831 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:37.093110.093110 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 12df6de9-e6f9-4df2-8933-d6b345956409
DEBUG 01-15 16:10:37.093376.093376 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:37.093834.093834 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:37.094938.094938 client.py:127] Model loaded
DEBUG 01-15 16:10:37.094351.094351 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:37.094645.094645 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:37.094542.094542 cuda_h.py:19] end restore2model cost 0.0003514289855957031 seconds
DEBUG 01-15 16:10:37.094835.094835 cuda_h.py:19] end sllm_worker_task cost 0.010999441146850586 seconds
INFO 01-15 16:10:37.094399.094399 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 12df6de9-e6f9-4df2-8933-d6b345956409
DEBUG 01-15 16:10:37.095608.095608 cuda_h.py:19] end move_flatidxs cost 0.000827789306640625 seconds
DEBUG 01-15 16:10:37.095815.095815 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:37.095428.095428 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002796173095703125 seconds
DEBUG 01-15 16:10:37.095808.095808 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:37.098853.098853 cuda_h.py:19] end restore2model cost 0.0030863285064697266 seconds
DEBUG 01-15 16:10:37.098934.098934 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006111860275268555 seconds
DEBUG 01-15 16:10:37.098207.098207 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:37.099106.099106 cuda_h.py:19] end gpu_sexperts cost 0.00031280517578125 seconds
DEBUG 01-15 16:10:37.099889.099889 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:37.099765.099765 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:10:37.099845.099845 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:37.099409.099409 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.100927.100927 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010159015655517578 seconds
DEBUG 01-15 16:10:37.100777.100777 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.100678.100678 cuda_h.py:19] end gpu_group_list cost 0.00016927719116210938 seconds
DEBUG 01-15 16:10:37.101445.101445 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.102085.102085 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001251220703125 seconds
DEBUG 01-15 16:10:37.102180.102180 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.103810.103810 cuda_h.py:19] end gpu_group_list cost 0.00018715858459472656 seconds
DEBUG 01-15 16:10:37.103653.103653 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:37.104250.104250 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 12df6de9-e6f9-4df2-8933-d6b345956409
DEBUG 01-15 16:10:37.104582.104582 cuda_h.py:19] end group_tensors cost 0.009024858474731445 seconds
DEBUG 01-15 16:10:37.105039.105039 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:37.110878.110878 cuda_h.py:19] end group pad cost 0.005410909652709961 seconds
DEBUG 01-15 16:10:37.110259.110259 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:37.124914.124914 client.py:127] Model loaded
DEBUG 01-15 16:10:37.124535.124535 cuda_h.py:19] end wait_experts_multi_device cost 0.020349502563476562 seconds
DEBUG 01-15 16:10:37.124233.124233 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:37.130461.130461 cuda_h.py:19] end group_einsum cost 0.01997995376586914 seconds
DEBUG 01-15 16:10:37.130009.130009 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:37.134508.134508 cuda_h.py:19] end get_outputs_cpu1 cost 0.0033464431762695312 seconds
DEBUG 01-15 16:10:37.134846.134846 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04085254669189453 seconds
DEBUG 01-15 16:10:37.135595.135595 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010739326477050781 seconds
DEBUG 01-15 16:10:37.135115.135115 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:37.136475.136475 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.136546.136546 cuda_h.py:19] end index_scatter cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:10:37.137530.137530 cuda_h.py:19] end cpuoutputsdeal cost 0.0016698837280273438 seconds
DEBUG 01-15 16:10:37.137685.137685 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:37.137971.137971 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.137632.137632 cuda_h.py:19] end gpu_group_tensor cost 0.0001385211944580078 seconds
DEBUG 01-15 16:10:37.137395.137395 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.137189.137189 cuda_h.py:19] end gpu_group_tensor cost 0.00013256072998046875 seconds
DEBUG 01-15 16:10:37.137523.137523 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.138103.138103 cuda_h.py:19] end gpu_group_einsum cost 0.0010418891906738281 seconds
DEBUG 01-15 16:10:37.138432.138432 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.139332.139332 cuda_h.py:19] end gpu_group_einsum cost 0.0005004405975341797 seconds
DEBUG 01-15 16:10:37.139860.139860 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.139830.139830 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.140873.140873 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022912025451660156 seconds
DEBUG 01-15 16:10:37.140166.140166 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.140963.140963 cuda_h.py:19] end concat_expert_out cost 5.8650970458984375e-05 seconds
DEBUG 01-15 16:10:37.140376.140376 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.140498.140498 cuda_h.py:19] end index_scatter cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:10:37.140567.140567 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008881092071533203 seconds
DEBUG 01-15 16:10:37.140497.140497 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.140432.140432 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.140934.140934 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012564659118652344 seconds
DEBUG 01-15 16:10:37.140167.140167 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.140753.140753 cuda_h.py:19] end concat_expert_out cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:37.141165.141165 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.141188.141188 cuda_h.py:19] end index_scatter cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:10:37.141328.141328 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004725456237792969 seconds
DEBUG 01-15 16:10:37.141569.141569 cuda_h.py:19] end gpu_experts_multi_device cost 0.0419769287109375 seconds
DEBUG 01-15 16:10:37.141294.141294 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.05166339874267578 seconds
DEBUG 01-15 16:10:37.141836.141836 cuda_h.py:19] end prefill_layer cost 0.058527231216430664 seconds
DEBUG 01-15 16:10:37.141155.141155 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:10:37.141858.141858 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:37.141561.141561 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:10:37.141741.141741 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:37.141543.141543 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:37.141870.141870 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:10:37.141911.141911 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:10:37.141276.141276 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:37.142378.142378 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:37.142394.142394 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:37.142483.142483 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:37.142704.142704 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:37.142335.142335 cuda_h.py:19] end allocate_cuda_memory cost 0.00021696090698242188 seconds
DEBUG 01-15 16:10:37.142834.142834 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:37.142743.142743 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:37.142334.142334 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:37.142567.142567 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a023bd61-bf3f-4519-835a-d39b84f432f9
DEBUG 01-15 16:10:37.142285.142285 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:37.143175.143175 cuda_h.py:10] start self_attn
INFO 01-15 16:10:37.144874.144874 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a023bd61-bf3f-4519-835a-d39b84f432f9
DEBUG 01-15 16:10:37.144593.144593 cuda_h.py:19] end load_into_gpu_async cost 0.0014667510986328125 seconds
DEBUG 01-15 16:10:37.144502.144502 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:37.144519.144519 cuda_h.py:19] end restore_tensors2 cost 8.535385131835938e-05 seconds
DEBUG 01-15 16:10:37.144805.144805 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002064228057861328 seconds
INFO 01-15 16:10:37.144656.144656 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a023bd61-bf3f-4519-835a-d39b84f432f9
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:37.146945.146945 cuda_h.py:19] end self_attn cost 0.003728151321411133 seconds
DEBUG 01-15 16:10:37.147558.147558 cuda_h.py:19] end iln_self_attn_paln cost 0.0052318572998046875 seconds
DEBUG 01-15 16:10:37.147626.147626 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:10:37.147627.147627 cuda_h.py:10] start gate
DEBUG 01-15 16:10:37.148847.148847 cuda_h.py:19] end gate cost 0.0007903575897216797 seconds
DEBUG 01-15 16:10:37.148014.148014 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:37.148841.148841 lmp.py:1912] 
DEBUG 01-15 16:10:37.148841.148841 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:37.148127.148127 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:37.148207.148207 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:37.148234.148234 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:37.148354.148354 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:37.148759.148759 lmp.py:1917] 
DEBUG 01-15 16:10:37.148759.148759 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:37.148594.148594 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:37.148197.148197 lmp.py:1935]   Expert 13 |     28 | CPU
DEBUG 01-15 16:10:37.148079.148079 lmp.py:1935]   Expert 44 |     39 | CPU
DEBUG 01-15 16:10:37.148768.148768 lmp.py:1935]   Expert  9 |     40 | CPU
DEBUG 01-15 16:10:37.148457.148457 lmp.py:1935]   Expert 25 |     42 | CPU
DEBUG 01-15 16:10:37.148147.148147 lmp.py:1935]   Expert 16 |     46 | CPU
DEBUG 01-15 16:10:37.148597.148597 lmp.py:1935]   Expert 38 |     48 | CPU
DEBUG 01-15 16:10:37.148810.148810 lmp.py:1935]   Expert  2 |     50 | CPU
DEBUG 01-15 16:10:37.148738.148738 lmp.py:1935]   Expert 22 |     52 | CPU
DEBUG 01-15 16:10:37.148142.148142 lmp.py:1935]   Expert 33 |     56 | CPU
DEBUG 01-15 16:10:37.148308.148308 lmp.py:1935]   Expert 42 |     61 | CPU
DEBUG 01-15 16:10:37.148713.148713 lmp.py:1935]   Expert  5 |     69 | CPU
DEBUG 01-15 16:10:37.148356.148356 lmp.py:1935]   Expert 23 |     74 | CPU
DEBUG 01-15 16:10:37.148045.148045 lmp.py:1935]   Expert 24 |     81 | CPU
DEBUG 01-15 16:10:37.148496.148496 lmp.py:1935]   Expert 10 |     85 | CPU
DEBUG 01-15 16:10:37.148708.148708 lmp.py:1935]   Expert 59 |    102 | CPU
DEBUG 01-15 16:10:37.148921.148921 lmp.py:1935]   Expert 21 |    107 | CPU
DEBUG 01-15 16:10:37.148372.148372 lmp.py:1935]   Expert 55 |    114 | CPU
DEBUG 01-15 16:10:37.148584.148584 lmp.py:1935]   Expert 46 |    117 | CPU
DEBUG 01-15 16:10:37.149466.149466 lmp.py:1935]   Expert 61 |    117 | CPU
DEBUG 01-15 16:10:37.149585.149585 lmp.py:1935]   Expert 45 |    120 | CPU
DEBUG 01-15 16:10:37.149613.149613 lmp.py:1935]   Expert 31 |    130 | CPU
DEBUG 01-15 16:10:37.149256.149256 lmp.py:1935]   Expert 51 |    140 | CPU
DEBUG 01-15 16:10:37.149375.149375 lmp.py:1935]   Expert 36 |    141 | CPU
DEBUG 01-15 16:10:37.149780.149780 lmp.py:1935]   Expert  6 |    142 | CPU
DEBUG 01-15 16:10:37.149661.149661 lmp.py:1935]   Expert 43 |    146 | CPU
DEBUG 01-15 16:10:37.149258.149258 lmp.py:1935]   Expert  8 |    148 | CPU
DEBUG 01-15 16:10:37.149616.149616 lmp.py:1935]   Expert  0 |    150 | CPU
DEBUG 01-15 16:10:37.149498.149498 lmp.py:1935]   Expert  3 |    153 | CPU
DEBUG 01-15 16:10:37.149856.149856 lmp.py:1935]   Expert 18 |    158 | CPU
DEBUG 01-15 16:10:37.149453.149453 lmp.py:1935]   Expert 26 |    160 | CPU
DEBUG 01-15 16:10:37.149334.149334 lmp.py:1935]   Expert 48 |    163 | CPU
DEBUG 01-15 16:10:37.149977.149977 lmp.py:1935]   Expert 41 |    165 | CPU
DEBUG 01-15 16:10:37.149196.149196 lmp.py:1935]   Expert 12 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149985.149985 lmp.py:1935]   Expert  7 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149582.149582 lmp.py:1935]   Expert 20 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149775.149775 lmp.py:1935]   Expert 28 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149656.149656 lmp.py:1935]   Expert 56 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149299.149299 lmp.py:1935]   Expert 27 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149657.149657 lmp.py:1935]   Expert 34 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149016.149016 lmp.py:1935]   Expert  1 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149374.149374 lmp.py:1935]   Expert 47 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149732.149732 lmp.py:1935]   Expert 11 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149044.149044 lmp.py:1935]   Expert 32 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149925.149925 lmp.py:1935]   Expert 40 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149568.149568 lmp.py:1935]   Expert 53 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149211.149211 lmp.py:1935]   Expert 49 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149616.149616 lmp.py:1935]   Expert 63 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149497.149497 lmp.py:1935]   Expert 15 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149108.149108 lmp.py:1935]   Expert 29 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149466.149466 lmp.py:1935]   Expert 50 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149347.149347 lmp.py:1935]   Expert 30 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149229.149229 lmp.py:1935]   Expert  4 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149110.149110 lmp.py:1935]   Expert 14 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149276.149276 lmp.py:1935]   Expert 35 |    275 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149442.149442 lmp.py:1935]   Expert 37 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149847.149847 lmp.py:1935]   Expert 52 |    337 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149251.149251 lmp.py:1935]   Expert 17 |    359 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149179.149179 lmp.py:1935]   Expert 54 |    380 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149345.149345 lmp.py:1935]   Expert 39 |    388 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149465.149465 lmp.py:1935]   Expert 57 |    409 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149585.149585 lmp.py:1935]   Expert 60 |    456 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149705.149705 lmp.py:1935]   Expert 62 |    462 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149586.149586 lmp.py:1935]   Expert 19 |    542 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.149467.149467 lmp.py:1935]   Expert 58 |    577 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.149680.149680 lmp.py:1937] 
DEBUG 01-15 16:10:37.149680.149680 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:37.149608.149608 lmp.py:1938]   CPU:   3244 tokens
DEBUG 01-15 16:10:37.149489.149489 lmp.py:1942]   cuda:1:   4523 tokens (16 experts)
DEBUG 01-15 16:10:37.149894.149894 lmp.py:1942]   cuda:2:   4521 tokens (16 experts)
DEBUG 01-15 16:10:37.149106.149106 lmp.py:1943]   Total GPU:   9044 tokens
DEBUG 01-15 16:10:37.149557.149557 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:37.149557.149557 lmp.py:1944] 
DEBUG 01-15 16:10:37.149491.149491 cuda_h.py:19] end experts_map_get cost 0.0017597675323486328 seconds
DEBUG 01-15 16:10:37.150295.150295 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:37.150859.150859 lmp.py:1953] 
DEBUG 01-15 16:10:37.150859.150859 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:37.150019.150019 cuda_h.py:19] end cpu_experts_submit cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:10:37.150762.150762 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:37.150281.150281 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:37.150097.150097 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.1.gate_proj.weight': 0, 'model.layers.25.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.47.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.47.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.47.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.52.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.52.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.52.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.53.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.53.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.53.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.58.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.58.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.58.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.27.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.27.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.27.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.60.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.60.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.60.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.25.mlp.experts.32.gate_proj.weight': 0, 'model.layers.25.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(29452926976, 5767168, 0, 0), (29458694144, 5767168, 5767168, 0), (29447159808, 5767168, 11534336, 0), (30041178112, 5767168, 17301504, 0), (30046945280, 5767168, 23068672, 0), (30035410944, 5767168, 28835840, 0), (30110384128, 5767168, 34603008, 0), (30116151296, 5767168, 40370176, 0), (30104616960, 5767168, 46137344, 0), (29556736000, 5767168, 51904512, 0), (29562503168, 5767168, 57671680, 0), (29550968832, 5767168, 63438848, 0), (29625942016, 5767168, 69206016, 0), (29631709184, 5767168, 74973184, 0), (29620174848, 5767168, 80740352, 0), (30248796160, 5767168, 86507520, 0), (30254563328, 5767168, 92274688, 0), (30243028992, 5767168, 98041856, 0), (29729751040, 5767168, 103809024, 0), (29735518208, 5767168, 109576192, 0), (29723983872, 5767168, 115343360, 0), (30335303680, 5767168, 121110528, 0), (30341070848, 5767168, 126877696, 0), (30329536512, 5767168, 132644864, 0), (30352605184, 5767168, 138412032, 0), (30358372352, 5767168, 144179200, 0), (30346838016, 5767168, 149946368, 0), (29781655552, 5767168, 155713536, 0), (29787422720, 5767168, 161480704, 0), (29775888384, 5767168, 167247872, 0), (30439112704, 5767168, 173015040, 0), (30444879872, 5767168, 178782208, 0), (30433345536, 5767168, 184549376, 0), (29902766080, 5767168, 190316544, 0), (29908533248, 5767168, 196083712, 0), (29896998912, 5767168, 201850880, 0), (30473715712, 5767168, 207618048, 0), (30479482880, 5767168, 213385216, 0), (30467948544, 5767168, 219152384, 0), (29937369088, 5767168, 224919552, 0), (29943136256, 5767168, 230686720, 0), (29931601920, 5767168, 236453888, 0), (29954670592, 5767168, 242221056, 0), (29960437760, 5767168, 247988224, 0), (29948903424, 5767168, 253755392, 0), (30525620224, 5767168, 259522560, 0), (30531387392, 5767168, 265289728, 0), (30519853056, 5767168, 271056896, 0)], 2: [(29989273600, 5767168, 0, 0), (29995040768, 5767168, 5767168, 0), (29983506432, 5767168, 11534336, 0), (30023876608, 5767168, 17301504, 0), (30029643776, 5767168, 23068672, 0), (30018109440, 5767168, 28835840, 0), (29504831488, 5767168, 34603008, 0), (29510598656, 5767168, 40370176, 0), (29499064320, 5767168, 46137344, 0), (30075781120, 5767168, 51904512, 0), (30081548288, 5767168, 57671680, 0), (30070013952, 5767168, 63438848, 0), (30127685632, 5767168, 69206016, 0), (30133452800, 5767168, 74973184, 0), (30121918464, 5767168, 80740352, 0), (29643243520, 5767168, 86507520, 0), (29649010688, 5767168, 92274688, 0), (29637476352, 5767168, 98041856, 0), (29677846528, 5767168, 103809024, 0), (29683613696, 5767168, 109576192, 0), (29672079360, 5767168, 115343360, 0), (29695148032, 5767168, 121110528, 0), (29700915200, 5767168, 126877696, 0), (29689380864, 5767168, 132644864, 0), (30283399168, 5767168, 138412032, 0), (30289166336, 5767168, 144179200, 0), (30277632000, 5767168, 149946368, 0), (30300700672, 5767168, 155713536, 0), (30306467840, 5767168, 161480704, 0), (30294933504, 5767168, 167247872, 0), (29764354048, 5767168, 173015040, 0), (29770121216, 5767168, 178782208, 0), (29758586880, 5767168, 184549376, 0), (30369906688, 5767168, 190316544, 0), (30375673856, 5767168, 196083712, 0), (30364139520, 5767168, 201850880, 0), (30404509696, 5767168, 207618048, 0), (30410276864, 5767168, 213385216, 0), (30398742528, 5767168, 219152384, 0), (30421811200, 5767168, 224919552, 0), (30427578368, 5767168, 230686720, 0), (30416044032, 5767168, 236453888, 0), (29920067584, 5767168, 242221056, 0), (29925834752, 5767168, 247988224, 0), (29914300416, 5767168, 253755392, 0), (30508318720, 5767168, 259522560, 0), (30514085888, 5767168, 265289728, 0), (30502551552, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4197d20>, 2: <capsule object NULL at 0x7a4e6c2f3d20>}
INFO 01-15 16:10:37.151599.151599 client.py:127] Model loaded
DEBUG 01-15 16:10:37.151464.151464 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:37.151274.151274 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:37.151779.151779 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:37.151069.151069 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:37.151626.151626 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 62f34901-4c1a-4d13-ba69-c71e6d1e8b7b
DEBUG 01-15 16:10:37.151032.151032 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:37.151721.151721 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:37.152806.152806 cuda_h.py:19] end restore2model cost 0.0007383823394775391 seconds
DEBUG 01-15 16:10:37.152443.152443 cuda_h.py:19] end sllm_worker_task cost 0.01009058952331543 seconds
DEBUG 01-15 16:10:37.152831.152831 cuda_h.py:19] end move_flatidxs cost 0.0008416175842285156 seconds
INFO 01-15 16:10:37.152295.152295 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 62f34901-4c1a-4d13-ba69-c71e6d1e8b7b
DEBUG 01-15 16:10:37.152706.152706 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:37.153667.153667 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002991199493408203 seconds
DEBUG 01-15 16:10:37.153384.153384 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:37.155098.155098 cuda_h.py:19] end restore2model cost 0.002496004104614258 seconds
DEBUG 01-15 16:10:37.155126.155126 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005707740783691406 seconds
DEBUG 01-15 16:10:37.155730.155730 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:37.156607.156607 cuda_h.py:19] end gpu_sexperts cost 0.0002663135528564453 seconds
DEBUG 01-15 16:10:37.156622.156622 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:37.156061.156061 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:10:37.156518.156518 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:37.156075.156075 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.157469.157469 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008575916290283203 seconds
DEBUG 01-15 16:10:37.157319.157319 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.157121.157121 cuda_h.py:19] end gpu_group_list cost 0.0001766681671142578 seconds
DEBUG 01-15 16:10:37.158868.158868 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.159925.159925 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.00086212158203125 seconds
DEBUG 01-15 16:10:37.159491.159491 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.159047.159047 cuda_h.py:19] end gpu_group_list cost 0.00017070770263671875 seconds
DEBUG 01-15 16:10:37.160344.160344 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:37.160505.160505 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 62f34901-4c1a-4d13-ba69-c71e6d1e8b7b
DEBUG 01-15 16:10:37.163227.163227 cuda_h.py:19] end group_tensors cost 0.010560750961303711 seconds
DEBUG 01-15 16:10:37.164798.164798 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:37.167962.167962 cuda_h.py:19] end group pad cost 0.0036487579345703125 seconds
DEBUG 01-15 16:10:37.168467.168467 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:37.179668.179668 client.py:127] Model loaded
DEBUG 01-15 16:10:37.179835.179835 cuda_h.py:19] end wait_experts_multi_device cost 0.01931476593017578 seconds
DEBUG 01-15 16:10:37.179148.179148 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:37.187404.187404 cuda_h.py:19] end group_einsum cost 0.0191805362701416 seconds
DEBUG 01-15 16:10:37.187190.187190 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:37.190154.190154 cuda_h.py:19] end get_outputs_cpu1 cost 0.00337982177734375 seconds
DEBUG 01-15 16:10:37.191658.191658 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039869070053100586 seconds
DEBUG 01-15 16:10:37.192025.192025 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.012479543685913086 seconds
DEBUG 01-15 16:10:37.192459.192459 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:37.193384.193384 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.193031.193031 cuda_h.py:19] end index_scatter cost 7.772445678710938e-05 seconds
DEBUG 01-15 16:10:37.193506.193506 cuda_h.py:19] end cpuoutputsdeal cost 0.0017483234405517578 seconds
DEBUG 01-15 16:10:37.193468.193468 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:37.194993.194993 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.194992.194992 cuda_h.py:19] end gpu_group_tensor cost 0.00014257431030273438 seconds
DEBUG 01-15 16:10:37.194424.194424 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.194264.194264 cuda_h.py:19] end gpu_group_tensor cost 0.00013184547424316406 seconds
DEBUG 01-15 16:10:37.194936.194936 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.195382.195382 cuda_h.py:19] end gpu_group_einsum cost 0.0005939006805419922 seconds
DEBUG 01-15 16:10:37.195618.195618 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.195337.195337 cuda_h.py:19] end gpu_group_einsum cost 0.00042629241943359375 seconds
DEBUG 01-15 16:10:37.195347.195347 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.196390.196390 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.196020.196020 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017833709716796875 seconds
DEBUG 01-15 16:10:37.196352.196352 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.196368.196368 cuda_h.py:19] end concat_expert_out cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:37.196185.196185 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.196553.196553 cuda_h.py:19] end index_scatter cost 5.7220458984375e-05 seconds
DEBUG 01-15 16:10:37.196455.196455 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008032321929931641 seconds
DEBUG 01-15 16:10:37.196074.196074 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.196540.196540 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.197493.197493 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014162063598632812 seconds
DEBUG 01-15 16:10:37.197918.197918 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.197563.197563 cuda_h.py:19] end concat_expert_out cost 5.626678466796875e-05 seconds
DEBUG 01-15 16:10:37.197314.197314 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.197674.197674 cuda_h.py:19] end index_scatter cost 5.507469177246094e-05 seconds
DEBUG 01-15 16:10:37.197676.197676 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005080699920654297 seconds
DEBUG 01-15 16:10:37.197685.197685 cuda_h.py:19] end gpu_experts_multi_device cost 0.04114699363708496 seconds
DEBUG 01-15 16:10:37.197840.197840 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.050264835357666016 seconds
DEBUG 01-15 16:10:37.197594.197594 cuda_h.py:19] end prefill_layer cost 0.05616450309753418 seconds
DEBUG 01-15 16:10:37.198868.198868 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:10:37.198193.198193 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:37.198757.198757 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:10:37.198275.198275 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:37.198938.198938 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:37.198272.198272 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 4.029273986816406e-05 seconds
DEBUG 01-15 16:10:37.198173.198173 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 7.557868957519531e-05 seconds
DEBUG 01-15 16:10:37.198068.198068 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:37.198608.198608 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:37.198869.198869 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:37.198375.198375 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:37.198218.198218 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:37.198587.198587 cuda_h.py:19] end allocate_cuda_memory cost 0.000301361083984375 seconds
DEBUG 01-15 16:10:37.199524.199524 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:37.199532.199532 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:37.199176.199176 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:37.199555.199555 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b2a476b8-6b29-47c5-9147-70de83b8551c
DEBUG 01-15 16:10:37.199373.199373 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:37.199569.199569 cuda_h.py:10] start self_attn
INFO 01-15 16:10:37.200812.200812 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b2a476b8-6b29-47c5-9147-70de83b8551c
DEBUG 01-15 16:10:37.200908.200908 cuda_h.py:19] end load_into_gpu_async cost 0.0015397071838378906 seconds
DEBUG 01-15 16:10:37.200287.200287 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:37.200026.200026 cuda_h.py:19] end restore_tensors2 cost 9.059906005859375e-05 seconds
DEBUG 01-15 16:10:37.200696.200696 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002237558364868164 seconds
INFO 01-15 16:10:37.200924.200924 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b2a476b8-6b29-47c5-9147-70de83b8551c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:37.203543.203543 cuda_h.py:19] end self_attn cost 0.003876209259033203 seconds
DEBUG 01-15 16:10:37.203248.203248 cuda_h.py:19] end iln_self_attn_paln cost 0.005540370941162109 seconds
DEBUG 01-15 16:10:37.203747.203747 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:10:37.203271.203271 cuda_h.py:10] start gate
DEBUG 01-15 16:10:37.204103.204103 cuda_h.py:19] end gate cost 0.0006799697875976562 seconds
DEBUG 01-15 16:10:37.204085.204085 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:37.205575.205575 lmp.py:1912] 
DEBUG 01-15 16:10:37.205575.205575 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:37.205100.205100 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:37.205279.205279 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:37.205360.205360 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:37.205010.205010 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:37.205706.205706 lmp.py:1917] 
DEBUG 01-15 16:10:37.205706.205706 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:37.205925.205925 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:37.205058.205058 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:10:37.205139.205139 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:10:37.205742.205742 lmp.py:1935]   Expert 11 |     29 | CPU
DEBUG 01-15 16:10:37.205107.205107 lmp.py:1935]   Expert  7 |     41 | CPU
DEBUG 01-15 16:10:37.205280.205280 lmp.py:1935]   Expert 62 |     43 | CPU
DEBUG 01-15 16:10:37.205645.205645 lmp.py:1935]   Expert  3 |     46 | CPU
DEBUG 01-15 16:10:37.205295.205295 lmp.py:1935]   Expert 51 |     48 | CPU
DEBUG 01-15 16:10:37.205176.205176 lmp.py:1935]   Expert 30 |     51 | CPU
DEBUG 01-15 16:10:37.205819.205819 lmp.py:1935]   Expert 17 |     52 | CPU
DEBUG 01-15 16:10:37.205462.205462 lmp.py:1935]   Expert 29 |     55 | CPU
DEBUG 01-15 16:10:37.205105.205105 lmp.py:1935]   Expert  6 |     61 | CPU
DEBUG 01-15 16:10:37.205225.205225 lmp.py:1935]   Expert  9 |     66 | CPU
DEBUG 01-15 16:10:37.205868.205868 lmp.py:1935]   Expert 38 |     77 | CPU
DEBUG 01-15 16:10:37.205749.205749 lmp.py:1935]   Expert 63 |     78 | CPU
DEBUG 01-15 16:10:37.205631.205631 lmp.py:1935]   Expert 55 |     83 | CPU
DEBUG 01-15 16:10:37.205035.205035 lmp.py:1935]   Expert 59 |     85 | CPU
DEBUG 01-15 16:10:37.205440.205440 lmp.py:1935]   Expert 48 |     92 | CPU
DEBUG 01-15 16:10:37.205606.205606 lmp.py:1935]   Expert  8 |     93 | CPU
DEBUG 01-15 16:10:37.205010.205010 lmp.py:1935]   Expert 19 |     97 | CPU
DEBUG 01-15 16:10:37.205892.205892 lmp.py:1935]   Expert 22 |    101 | CPU
DEBUG 01-15 16:10:37.205820.205820 lmp.py:1935]   Expert 49 |    102 | CPU
DEBUG 01-15 16:10:37.205224.205224 lmp.py:1935]   Expert 24 |    110 | CPU
DEBUG 01-15 16:10:37.205629.205629 lmp.py:1935]   Expert 34 |    113 | CPU
DEBUG 01-15 16:10:37.205225.205225 lmp.py:1935]   Expert 36 |    114 | CPU
DEBUG 01-15 16:10:37.205345.205345 lmp.py:1935]   Expert 42 |    117 | CPU
DEBUG 01-15 16:10:37.205465.205465 lmp.py:1935]   Expert 50 |    120 | CPU
DEBUG 01-15 16:10:37.205108.205108 lmp.py:1935]   Expert 39 |    123 | CPU
DEBUG 01-15 16:10:37.205228.205228 lmp.py:1935]   Expert  4 |    134 | CPU
DEBUG 01-15 16:10:37.205155.205155 lmp.py:1935]   Expert 37 |    145 | CPU
DEBUG 01-15 16:10:37.205322.205322 lmp.py:1935]   Expert 41 |    146 | CPU
DEBUG 01-15 16:10:37.205965.205965 lmp.py:1935]   Expert 15 |    150 | CPU
DEBUG 01-15 16:10:37.205131.205131 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:10:37.205681.205681 lmp.py:1935]   Expert 56 |    162 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.205516.205516 lmp.py:1935]   Expert 60 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.205782.205782 lmp.py:1935]   Expert 16 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.205332.205332 lmp.py:1935]   Expert 44 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.205167.205167 lmp.py:1935]   Expert  1 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.205240.205240 lmp.py:1935]   Expert 21 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.205837.205837 lmp.py:1935]   Expert 43 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206672.206672 lmp.py:1935]   Expert 53 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206554.206554 lmp.py:1935]   Expert 47 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206912.206912 lmp.py:1935]   Expert 33 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206032.206032 lmp.py:1935]   Expert 12 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206628.206628 lmp.py:1935]   Expert 13 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206987.206987 lmp.py:1935]   Expert 32 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206106.206106 lmp.py:1935]   Expert 28 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206418.206418 lmp.py:1935]   Expert  0 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206253.206253 lmp.py:1935]   Expert 31 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206327.206327 lmp.py:1935]   Expert 54 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206162.206162 lmp.py:1935]   Expert 26 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206520.206520 lmp.py:1935]   Expert 10 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206117.206117 lmp.py:1935]   Expert 18 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206237.206237 lmp.py:1935]   Expert 57 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206356.206356 lmp.py:1935]   Expert  2 |    281 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206715.206715 lmp.py:1935]   Expert 58 |    298 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206073.206073 lmp.py:1935]   Expert 40 |    338 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206193.206193 lmp.py:1935]   Expert 45 |    362 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206551.206551 lmp.py:1935]   Expert 25 |    364 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206386.206386 lmp.py:1935]   Expert  5 |    442 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206221.206221 lmp.py:1935]   Expert 35 |    464 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206818.206818 lmp.py:1935]   Expert 27 |    484 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206620.206620 lmp.py:1935]   Expert 46 |    549 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206469.206469 lmp.py:1935]   Expert 52 |    589 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.206873.206873 lmp.py:1935]   Expert 14 |    888 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.206324.206324 lmp.py:1937] 
DEBUG 01-15 16:10:37.206324.206324 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:37.206490.206490 lmp.py:1938]   CPU:   2749 tokens
DEBUG 01-15 16:10:37.206895.206895 lmp.py:1942]   cuda:1:   4831 tokens (16 experts)
DEBUG 01-15 16:10:37.206299.206299 lmp.py:1942]   cuda:2:   4708 tokens (16 experts)
DEBUG 01-15 16:10:37.206512.206512 lmp.py:1943]   Total GPU:   9539 tokens
DEBUG 01-15 16:10:37.206963.206963 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:37.206963.206963 lmp.py:1944] 
DEBUG 01-15 16:10:37.206851.206851 cuda_h.py:19] end experts_map_get cost 0.001867055892944336 seconds
DEBUG 01-15 16:10:37.206462.206462 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:37.206265.206265 lmp.py:1953] 
DEBUG 01-15 16:10:37.206265.206265 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:37.206902.206902 cuda_h.py:19] end cpu_experts_submit cost 4.76837158203125e-05 seconds
DEBUG 01-15 16:10:37.206406.206406 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:37.206567.206567 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:37.207357.207357 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.32.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.32.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.32.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.1.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.1.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.1.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.47.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.47.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.47.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.26.mlp.experts.33.gate_proj.weight': 0, 'model.layers.26.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.44.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.44.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.44.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.52.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.52.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.52.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.54.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.54.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.54.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.26.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.26.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.26.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.28.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.28.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.28.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (31096569856, 5767168, 17301504, 0), (31102337024, 5767168, 23068672, 0), (31090802688, 5767168, 28835840, 0), (30560223232, 5767168, 34603008, 0), (30565990400, 5767168, 40370176, 0), (30554456064, 5767168, 46137344, 0), (30629429248, 5767168, 51904512, 0), (30635196416, 5767168, 57671680, 0), (30623662080, 5767168, 63438848, 0), (30715936768, 5767168, 69206016, 0), (30721703936, 5767168, 74973184, 0), (30710169600, 5767168, 80740352, 0), (31286886400, 5767168, 86507520, 0), (31292653568, 5767168, 92274688, 0), (31281119232, 5767168, 98041856, 0), (30750539776, 5767168, 103809024, 0), (30756306944, 5767168, 109576192, 0), (30744772608, 5767168, 115343360, 0), (31321489408, 5767168, 121110528, 0), (31327256576, 5767168, 126877696, 0), (31315722240, 5767168, 132644864, 0), (30785142784, 5767168, 138412032, 0), (30790909952, 5767168, 144179200, 0), (30779375616, 5767168, 149946368, 0), (31356092416, 5767168, 155713536, 0), (31361859584, 5767168, 161480704, 0), (31350325248, 5767168, 167247872, 0), (30819745792, 5767168, 173015040, 0), (30825512960, 5767168, 178782208, 0), (30813978624, 5767168, 184549376, 0), (31529107456, 5767168, 190316544, 0), (31534874624, 5767168, 196083712, 0), (31523340288, 5767168, 201850880, 0), (31546408960, 5767168, 207618048, 0), (31552176128, 5767168, 213385216, 0), (31540641792, 5767168, 219152384, 0), (31010062336, 5767168, 224919552, 0), (31015829504, 5767168, 230686720, 0), (31004295168, 5767168, 236453888, 0), (31581011968, 5767168, 242221056, 0), (31586779136, 5767168, 247988224, 0), (31575244800, 5767168, 253755392, 0), (31079268352, 5767168, 259522560, 0), (31085035520, 5767168, 265289728, 0), (31073501184, 5767168, 271056896, 0)], 2: [(31113871360, 5767168, 0, 0), (31119638528, 5767168, 5767168, 0), (31108104192, 5767168, 11534336, 0), (30577524736, 5767168, 17301504, 0), (30583291904, 5767168, 23068672, 0), (30571757568, 5767168, 28835840, 0), (31148474368, 5767168, 34603008, 0), (31154241536, 5767168, 40370176, 0), (31142707200, 5767168, 46137344, 0), (31234981888, 5767168, 51904512, 0), (31240749056, 5767168, 57671680, 0), (31229214720, 5767168, 63438848, 0), (31304187904, 5767168, 69206016, 0), (31309955072, 5767168, 74973184, 0), (31298420736, 5767168, 80740352, 0), (30767841280, 5767168, 86507520, 0), (30773608448, 5767168, 92274688, 0), (30762074112, 5767168, 98041856, 0), (31338790912, 5767168, 103809024, 0), (31344558080, 5767168, 109576192, 0), (31333023744, 5767168, 115343360, 0), (30854348800, 5767168, 121110528, 0), (30860115968, 5767168, 126877696, 0), (30848581632, 5767168, 132644864, 0), (31442599936, 5767168, 138412032, 0), (31448367104, 5767168, 144179200, 0), (31436832768, 5767168, 149946368, 0), (31459901440, 5767168, 155713536, 0), (31465668608, 5767168, 161480704, 0), (31454134272, 5767168, 167247872, 0), (31477202944, 5767168, 173015040, 0), (31482970112, 5767168, 178782208, 0), (31471435776, 5767168, 184549376, 0), (30906253312, 5767168, 190316544, 0), (30912020480, 5767168, 196083712, 0), (30900486144, 5767168, 201850880, 0), (31511805952, 5767168, 207618048, 0), (31517573120, 5767168, 213385216, 0), (31506038784, 5767168, 219152384, 0), (30975459328, 5767168, 224919552, 0), (30981226496, 5767168, 230686720, 0), (30969692160, 5767168, 236453888, 0), (30992760832, 5767168, 242221056, 0), (30998528000, 5767168, 247988224, 0), (30986993664, 5767168, 253755392, 0), (31027363840, 5767168, 259522560, 0), (31033131008, 5767168, 265289728, 0), (31021596672, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e6c1fe070>, 2: <capsule object NULL at 0x7a4e5420a8b0>}
DEBUG 01-15 16:10:37.207434.207434 sllm_store_c.py:27] get device uuid map
INFO 01-15 16:10:37.207789.207789 client.py:127] Model loaded
DEBUG 01-15 16:10:37.207037.207037 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:37.208291.208291 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c4c6f28d-2a1e-4f82-a01c-65af9612106f
DEBUG 01-15 16:10:37.208399.208399 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:37.208763.208763 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:37.208436.208436 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:37.208491.208491 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:37.208919.208919 cuda_h.py:19] end restore2model cost 0.0003619194030761719 seconds
DEBUG 01-15 16:10:37.208026.208026 cuda_h.py:19] end sllm_worker_task cost 0.010355472564697266 seconds
INFO 01-15 16:10:37.209613.209613 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c4c6f28d-2a1e-4f82-a01c-65af9612106f
DEBUG 01-15 16:10:37.209899.209899 cuda_h.py:19] end move_flatidxs cost 0.0008325576782226562 seconds
DEBUG 01-15 16:10:37.209344.209344 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:37.209853.209853 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0027780532836914062 seconds
DEBUG 01-15 16:10:37.209471.209471 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:37.212311.212311 cuda_h.py:19] end restore2model cost 0.0025174617767333984 seconds
DEBUG 01-15 16:10:37.212055.212055 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005512237548828125 seconds
DEBUG 01-15 16:10:37.212089.212089 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:37.212325.212325 cuda_h.py:19] end gpu_sexperts cost 0.0002830028533935547 seconds
DEBUG 01-15 16:10:37.212200.212200 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:37.212785.212785 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:10:37.212911.212911 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:37.212137.212137 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.213285.213285 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008149147033691406 seconds
DEBUG 01-15 16:10:37.213896.213896 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.213274.213274 cuda_h.py:19] end gpu_group_list cost 0.000179290771484375 seconds
DEBUG 01-15 16:10:37.214399.214399 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.215060.215060 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008840560913085938 seconds
DEBUG 01-15 16:10:37.215294.215294 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.215810.215810 cuda_h.py:19] end gpu_group_list cost 0.00017547607421875 seconds
DEBUG 01-15 16:10:37.216420.216420 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:37.216110.216110 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c4c6f28d-2a1e-4f82-a01c-65af9612106f
DEBUG 01-15 16:10:37.218748.218748 cuda_h.py:19] end group_tensors cost 0.009073495864868164 seconds
DEBUG 01-15 16:10:37.219675.219675 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:37.223238.223238 cuda_h.py:19] end group pad cost 0.003699064254760742 seconds
DEBUG 01-15 16:10:37.223889.223889 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:37.239070.239070 client.py:127] Model loaded
DEBUG 01-15 16:10:37.239425.239425 cuda_h.py:19] end wait_experts_multi_device cost 0.022811412811279297 seconds
DEBUG 01-15 16:10:37.239475.239475 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:37.242947.242947 cuda_h.py:19] end group_einsum cost 0.01883840560913086 seconds
DEBUG 01-15 16:10:37.242317.242317 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:37.245314.245314 cuda_h.py:19] end get_outputs_cpu1 cost 0.003014802932739258 seconds
DEBUG 01-15 16:10:37.245096.245096 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03777503967285156 seconds
DEBUG 01-15 16:10:37.246872.246872 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0068454742431640625 seconds
DEBUG 01-15 16:10:37.246253.246253 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:37.247042.247042 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.247438.247438 cuda_h.py:19] end index_scatter cost 7.319450378417969e-05 seconds
DEBUG 01-15 16:10:37.248111.248111 cuda_h.py:19] end cpuoutputsdeal cost 0.0016453266143798828 seconds
DEBUG 01-15 16:10:37.248935.248935 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:37.248506.248506 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.248912.248912 cuda_h.py:19] end gpu_group_tensor cost 0.0002300739288330078 seconds
DEBUG 01-15 16:10:37.248397.248397 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.248230.248230 cuda_h.py:19] end gpu_group_tensor cost 0.00012636184692382812 seconds
DEBUG 01-15 16:10:37.248187.248187 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.249277.249277 cuda_h.py:19] end gpu_group_einsum cost 0.0008540153503417969 seconds
DEBUG 01-15 16:10:37.249381.249381 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.250452.250452 cuda_h.py:19] end gpu_group_einsum cost 0.0004825592041015625 seconds
DEBUG 01-15 16:10:37.250106.250106 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.250216.250216 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.250833.250833 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019741058349609375 seconds
DEBUG 01-15 16:10:37.251695.251695 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.251361.251361 cuda_h.py:19] end concat_expert_out cost 5.7697296142578125e-05 seconds
DEBUG 01-15 16:10:37.251727.251727 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.251512.251512 cuda_h.py:19] end index_scatter cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:37.251447.251447 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008301734924316406 seconds
DEBUG 01-15 16:10:37.251305.251305 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.251148.251148 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.251272.251272 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013017654418945312 seconds
DEBUG 01-15 16:10:37.251028.251028 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.251044.251044 cuda_h.py:19] end concat_expert_out cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:37.251934.251934 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.252155.252155 cuda_h.py:19] end index_scatter cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:37.252011.252011 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004832744598388672 seconds
DEBUG 01-15 16:10:37.252907.252907 cuda_h.py:19] end gpu_experts_multi_device cost 0.039405107498168945 seconds
DEBUG 01-15 16:10:37.252161.252161 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.04834270477294922 seconds
DEBUG 01-15 16:10:37.252756.252756 cuda_h.py:19] end prefill_layer cost 0.05455970764160156 seconds
DEBUG 01-15 16:10:37.252984.252984 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:10:37.252163.252163 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:37.252343.252343 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:10:37.252284.252284 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:37.252770.252770 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:37.253548.253548 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:37.255076.255076 cuda_h.py:19] end self_attn cost 0.002651214599609375 seconds
DEBUG 01-15 16:10:37.256113.256113 cuda_h.py:19] end iln_self_attn_paln cost 0.0033516883850097656 seconds
DEBUG 01-15 16:10:37.256896.256896 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:10:37.256897.256897 cuda_h.py:10] start gate
DEBUG 01-15 16:10:37.256503.256503 cuda_h.py:19] end gate cost 0.0006542205810546875 seconds
DEBUG 01-15 16:10:37.256770.256770 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:37.257883.257883 lmp.py:1912] 
DEBUG 01-15 16:10:37.257883.257883 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:37.257215.257215 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:37.257395.257395 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:37.257475.257475 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:37.257410.257410 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:37.257629.257629 lmp.py:1917] 
DEBUG 01-15 16:10:37.257629.257629 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:37.257848.257848 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:37.257220.257220 lmp.py:1935]   Expert 18 |     66 | CPU
DEBUG 01-15 16:10:37.257154.257154 lmp.py:1935]   Expert 47 |     70 | CPU
DEBUG 01-15 16:10:37.257374.257374 lmp.py:1935]   Expert 54 |     70 | CPU
DEBUG 01-15 16:10:37.257593.257593 lmp.py:1935]   Expert 23 |     76 | CPU
DEBUG 01-15 16:10:37.257289.257289 lmp.py:1935]   Expert 48 |     76 | CPU
DEBUG 01-15 16:10:37.257985.257985 lmp.py:1935]   Expert 44 |     84 | CPU
DEBUG 01-15 16:10:37.257920.257920 lmp.py:1935]   Expert 45 |     86 | CPU
DEBUG 01-15 16:10:37.257900.257900 lmp.py:1935]   Expert 20 |     92 | CPU
DEBUG 01-15 16:10:37.257643.257643 lmp.py:1935]   Expert 31 |     99 | CPU
DEBUG 01-15 16:10:37.257385.257385 lmp.py:1935]   Expert 36 |    107 | CPU
DEBUG 01-15 16:10:37.257889.257889 lmp.py:1935]   Expert 61 |    113 | CPU
DEBUG 01-15 16:10:37.257393.257393 lmp.py:1935]   Expert 42 |    119 | CPU
DEBUG 01-15 16:10:37.257659.257659 lmp.py:1935]   Expert 33 |    120 | CPU
DEBUG 01-15 16:10:37.257163.257163 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:10:37.257859.257859 lmp.py:1935]   Expert 24 |    123 | CPU
DEBUG 01-15 16:10:37.257317.257317 lmp.py:1935]   Expert 43 |    124 | CPU
DEBUG 01-15 16:10:37.257013.257013 lmp.py:1935]   Expert 11 |    127 | CPU
DEBUG 01-15 16:10:37.257947.257947 lmp.py:1935]   Expert 49 |    129 | CPU
DEBUG 01-15 16:10:37.257690.257690 lmp.py:1935]   Expert 56 |    129 | CPU
DEBUG 01-15 16:10:37.257194.257194 lmp.py:1935]   Expert  6 |    137 | CPU
DEBUG 01-15 16:10:37.257698.257698 lmp.py:1935]   Expert 51 |    142 | CPU
DEBUG 01-15 16:10:37.258678.258678 lmp.py:1935]   Expert  0 |    148 | CPU
DEBUG 01-15 16:10:37.258182.258182 lmp.py:1935]   Expert 17 |    148 | CPU
DEBUG 01-15 16:10:37.258686.258686 lmp.py:1935]   Expert  5 |    151 | CPU
DEBUG 01-15 16:10:37.258144.258144 lmp.py:1935]   Expert 12 |    158 | CPU
DEBUG 01-15 16:10:37.258840.258840 lmp.py:1935]   Expert 40 |    158 | CPU
DEBUG 01-15 16:10:37.258013.258013 lmp.py:1935]   Expert 55 |    159 | CPU
DEBUG 01-15 16:10:37.258471.258471 lmp.py:1935]   Expert 57 |    159 | CPU
DEBUG 01-15 16:10:37.258213.258213 lmp.py:1935]   Expert 26 |    164 | CPU
DEBUG 01-15 16:10:37.258479.258479 lmp.py:1935]   Expert 59 |    164 | CPU
DEBUG 01-15 16:10:37.258744.258744 lmp.py:1935]   Expert 46 |    166 | CPU
DEBUG 01-15 16:10:37.258772.258772 lmp.py:1935]   Expert 38 |    167 | CPU
DEBUG 01-15 16:10:37.258421.258421 lmp.py:1935]   Expert 13 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258786.258786 lmp.py:1935]   Expert 50 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258436.258436 lmp.py:1935]   Expert 58 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258563.258563 lmp.py:1935]   Expert 30 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258212.258212 lmp.py:1935]   Expert 35 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258862.258862 lmp.py:1935]   Expert  7 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258558.258558 lmp.py:1935]   Expert 16 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258016.258016 lmp.py:1935]   Expert 15 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258712.258712 lmp.py:1935]   Expert 32 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258170.258170 lmp.py:1935]   Expert 14 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258866.258866 lmp.py:1935]   Expert  1 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258324.258324 lmp.py:1935]   Expert  3 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258496.258496 lmp.py:1935]   Expert  4 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258669.258669 lmp.py:1935]   Expert 39 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258842.258842 lmp.py:1935]   Expert 34 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258538.258538 lmp.py:1935]   Expert 28 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258519.258519 lmp.py:1935]   Expert 52 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258500.258500 lmp.py:1935]   Expert 25 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258481.258481 lmp.py:1935]   Expert 22 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258700.258700 lmp.py:1935]   Expert  2 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258681.258681 lmp.py:1935]   Expert 21 |    280 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258139.258139 lmp.py:1935]   Expert 41 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258934.258934 lmp.py:1935]   Expert 60 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258822.258822 lmp.py:1935]   Expert 63 |    292 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258472.258472 lmp.py:1935]   Expert 29 |    299 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258168.258168 lmp.py:1935]   Expert 62 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258387.258387 lmp.py:1935]   Expert 27 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258607.258607 lmp.py:1935]   Expert 37 |    331 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258826.258826 lmp.py:1935]   Expert  8 |    333 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258045.258045 lmp.py:1935]   Expert 53 |    333 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258264.258264 lmp.py:1935]   Expert 19 |    442 | GPU2(cuda:2)
DEBUG 01-15 16:10:37.258153.258153 lmp.py:1935]   Expert  9 |    619 | GPU1(cuda:1)
DEBUG 01-15 16:10:37.258372.258372 lmp.py:1937] 
DEBUG 01-15 16:10:37.258372.258372 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:37.258306.258306 lmp.py:1938]   CPU:   3952 tokens
DEBUG 01-15 16:10:37.258956.258956 lmp.py:1942]   cuda:1:   4203 tokens (16 experts)
DEBUG 01-15 16:10:37.259937.259937 lmp.py:1942]   cuda:2:   4133 tokens (16 experts)
DEBUG 01-15 16:10:37.259441.259441 lmp.py:1943]   Total GPU:   8336 tokens
DEBUG 01-15 16:10:37.259991.259991 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:37.259991.259991 lmp.py:1944] 
DEBUG 01-15 16:10:37.259741.259741 cuda_h.py:19] end experts_map_get cost 0.0020525455474853516 seconds
DEBUG 01-15 16:10:37.259829.259829 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:37.259254.259254 lmp.py:1953] 
DEBUG 01-15 16:10:37.259254.259254 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:37.259243.259243 cuda_h.py:19] end cpu_experts_submit cost 6.0558319091796875e-05 seconds
DEBUG 01-15 16:10:37.259707.259707 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:37.259835.259835 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:37.259981.259981 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:37.260560.260560 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:37.260090.260090 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.1.gate_proj.weight': 0, 'model.layers.27.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.52.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.52.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.52.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.27.mlp.experts.32.gate_proj.weight': 0, 'model.layers.27.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.7.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.7.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.7.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(31667519488, 5767168, 0, 0), (31673286656, 5767168, 5767168, 0), (31661752320, 5767168, 11534336, 0), (32238469120, 5767168, 17301504, 0), (32244236288, 5767168, 23068672, 0), (32232701952, 5767168, 28835840, 0), (32255770624, 5767168, 34603008, 0), (32261537792, 5767168, 40370176, 0), (32250003456, 5767168, 46137344, 0), (31719424000, 5767168, 51904512, 0), (31725191168, 5767168, 57671680, 0), (31713656832, 5767168, 63438848, 0), (31805931520, 5767168, 69206016, 0), (31811698688, 5767168, 74973184, 0), (31800164352, 5767168, 80740352, 0), (31875137536, 5767168, 86507520, 0), (31880904704, 5767168, 92274688, 0), (31869370368, 5767168, 98041856, 0), (31909740544, 5767168, 103809024, 0), (31915507712, 5767168, 109576192, 0), (31903973376, 5767168, 115343360, 0), (31927042048, 5767168, 121110528, 0), (31932809216, 5767168, 126877696, 0), (31921274880, 5767168, 132644864, 0), (32549896192, 5767168, 138412032, 0), (32555663360, 5767168, 144179200, 0), (32544129024, 5767168, 149946368, 0), (32567197696, 5767168, 155713536, 0), (32572964864, 5767168, 161480704, 0), (32561430528, 5767168, 167247872, 0), (32013549568, 5767168, 173015040, 0), (32019316736, 5767168, 178782208, 0), (32007782400, 5767168, 184549376, 0), (32030851072, 5767168, 190316544, 0), (32036618240, 5767168, 196083712, 0), (32025083904, 5767168, 201850880, 0), (32653705216, 5767168, 207618048, 0), (32659472384, 5767168, 213385216, 0), (32647938048, 5767168, 219152384, 0), (32117358592, 5767168, 224919552, 0), (32123125760, 5767168, 230686720, 0), (32111591424, 5767168, 236453888, 0), (32688308224, 5767168, 242221056, 0), (32694075392, 5767168, 247988224, 0), (32682541056, 5767168, 253755392, 0), (32722911232, 5767168, 259522560, 0), (32728678400, 5767168, 265289728, 0), (32717144064, 5767168, 271056896, 0)], 2: [(32203866112, 5767168, 0, 0), (32209633280, 5767168, 5767168, 0), (32198098944, 5767168, 11534336, 0), (31684820992, 5767168, 17301504, 0), (31690588160, 5767168, 23068672, 0), (31679053824, 5767168, 28835840, 0), (31702122496, 5767168, 34603008, 0), (31707889664, 5767168, 40370176, 0), (31696355328, 5767168, 46137344, 0), (32290373632, 5767168, 51904512, 0), (32296140800, 5767168, 57671680, 0), (32284606464, 5767168, 63438848, 0), (32324976640, 5767168, 69206016, 0), (32330743808, 5767168, 74973184, 0), (32319209472, 5767168, 80740352, 0), (31788630016, 5767168, 86507520, 0), (31794397184, 5767168, 92274688, 0), (31782862848, 5767168, 98041856, 0), (32359579648, 5767168, 103809024, 0), (32365346816, 5767168, 109576192, 0), (32353812480, 5767168, 115343360, 0), (31771328512, 5767168, 121110528, 0), (31777095680, 5767168, 126877696, 0), (31765561344, 5767168, 132644864, 0), (31892439040, 5767168, 138412032, 0), (31898206208, 5767168, 144179200, 0), (31886671872, 5767168, 149946368, 0), (32515293184, 5767168, 155713536, 0), (32521060352, 5767168, 161480704, 0), (32509526016, 5767168, 167247872, 0), (31978946560, 5767168, 173015040, 0), (31984713728, 5767168, 178782208, 0), (31973179392, 5767168, 184549376, 0), (32082755584, 5767168, 190316544, 0), (32088522752, 5767168, 196083712, 0), (32076988416, 5767168, 201850880, 0), (32134660096, 5767168, 207618048, 0), (32140427264, 5767168, 213385216, 0), (32128892928, 5767168, 219152384, 0), (32151961600, 5767168, 224919552, 0), (32157728768, 5767168, 230686720, 0), (32146194432, 5767168, 236453888, 0), (32169263104, 5767168, 242221056, 0), (32175030272, 5767168, 247988224, 0), (32163495936, 5767168, 253755392, 0), (32740212736, 5767168, 259522560, 0), (32745979904, 5767168, 265289728, 0), (32734445568, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c24a370>, 2: <capsule object NULL at 0x7a4e447ea310>}
DEBUG 01-15 16:10:37.260524.260524 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:37.260830.260830 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:37.260938.260938 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9f57531c-e98f-41a4-b2b9-659cd82e5c87
DEBUG 01-15 16:10:37.260482.260482 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:37.261544.261544 cuda_h.py:19] end move_flatidxs cost 0.0008382797241210938 seconds
DEBUG 01-15 16:10:37.261227.261227 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:37.261874.261874 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9f57531c-e98f-41a4-b2b9-659cd82e5c87
DEBUG 01-15 16:10:37.262695.262695 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0026733875274658203 seconds
DEBUG 01-15 16:10:37.262121.262121 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:37.264898.264898 cuda_h.py:19] end restore2model cost 0.0025730133056640625 seconds
DEBUG 01-15 16:10:37.265264.265264 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005720853805541992 seconds
DEBUG 01-15 16:10:37.265298.265298 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:37.265811.265811 cuda_h.py:19] end gpu_sexperts cost 0.0002772808074951172 seconds
DEBUG 01-15 16:10:37.265495.265495 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:37.265920.265920 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.266100.266100 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008029937744140625 seconds
DEBUG 01-15 16:10:37.266182.266182 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.266513.266513 cuda_h.py:19] end gpu_group_list cost 0.0001804828643798828 seconds
DEBUG 01-15 16:10:37.266961.266961 cuda_h.py:19] end group_tensors cost 0.005066871643066406 seconds
DEBUG 01-15 16:10:37.266164.266164 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:37.267474.267474 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:37.269782.269782 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013456344604492188 seconds
DEBUG 01-15 16:10:37.269785.269785 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:37.269426.269426 cuda_h.py:19] end gpu_group_list cost 0.00029468536376953125 seconds
DEBUG 01-15 16:10:37.270739.270739 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:37.270246.270246 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9f57531c-e98f-41a4-b2b9-659cd82e5c87
DEBUG 01-15 16:10:37.271267.271267 cuda_h.py:19] end group pad cost 0.004027605056762695 seconds
DEBUG 01-15 16:10:37.271488.271488 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:37.288394.288394 client.py:127] Model loaded
DEBUG 01-15 16:10:37.289036.289036 cuda_h.py:19] end wait_experts_multi_device cost 0.018648147583007812 seconds
DEBUG 01-15 16:10:37.289157.289157 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:37.290074.290074 cuda_h.py:19] end group_einsum cost 0.019792795181274414 seconds
DEBUG 01-15 16:10:37.291986.291986 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:37.295238.295238 cuda_h.py:19] end get_outputs_cpu1 cost 0.0038657188415527344 seconds
DEBUG 01-15 16:10:37.295485.295485 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03577256202697754 seconds
DEBUG 01-15 16:10:37.296504.296504 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.0069391727447509766 seconds
DEBUG 01-15 16:10:37.296335.296335 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:37.297785.297785 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.297181.297181 cuda_h.py:19] end index_scatter cost 7.176399230957031e-05 seconds
DEBUG 01-15 16:10:37.297993.297993 cuda_h.py:19] end cpuoutputsdeal cost 0.0016057491302490234 seconds
DEBUG 01-15 16:10:37.297902.297902 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:37.297566.297566 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.298220.298220 cuda_h.py:19] end gpu_group_tensor cost 0.00013709068298339844 seconds
DEBUG 01-15 16:10:37.298599.298599 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:37.298233.298233 cuda_h.py:19] end gpu_group_tensor cost 0.00012183189392089844 seconds
DEBUG 01-15 16:10:37.298846.298846 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.299847.299847 cuda_h.py:19] end gpu_group_einsum cost 0.0005824565887451172 seconds
DEBUG 01-15 16:10:37.299322.299322 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:37.299657.299657 cuda_h.py:19] end gpu_group_einsum cost 0.00046706199645996094 seconds
DEBUG 01-15 16:10:37.299834.299834 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.299944.299944 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.300038.300038 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019860267639160156 seconds
DEBUG 01-15 16:10:37.300616.300616 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.300043.300043 cuda_h.py:19] end concat_expert_out cost 5.7220458984375e-05 seconds
DEBUG 01-15 16:10:37.300271.300271 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.300725.300725 cuda_h.py:19] end index_scatter cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:37.300037.300037 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008690357208251953 seconds
DEBUG 01-15 16:10:37.300835.300835 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:37.300771.300771 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:37.301319.301319 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012683868408203125 seconds
DEBUG 01-15 16:10:37.301075.301075 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:37.301614.301614 cuda_h.py:19] end concat_expert_out cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:37.301073.301073 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:37.301851.301851 cuda_h.py:19] end index_scatter cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:10:37.301421.301421 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004658699035644531 seconds
DEBUG 01-15 16:10:37.301563.301563 cuda_h.py:19] end gpu_experts_multi_device cost 0.03601360321044922 seconds
DEBUG 01-15 16:10:37.301472.301472 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.045266151428222656 seconds
DEBUG 01-15 16:10:37.301810.301810 cuda_h.py:19] end prefill_layer cost 0.04914665222167969 seconds
DEBUG 01-15 16:10:37.301130.301130 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:10:37.301648.301648 cuda_h.py:19] end prefill cost 1.6315197944641113 seconds
DEBUG 01-15 16:10:39.522729.522729 cuda_h.py:10] start generate_input_ids
generate input ids cost 0.09097862243652344 s
DEBUG 01-15 16:10:39.879822.879822 cuda_h.py:19] end generate_input_ids cost 0.35536813735961914 seconds
DEBUG 01-15 16:10:39.879352.879352 cuda_h.py:10] start init_cache
DEBUG 01-15 16:10:39.879727.879727 cuda_h.py:19] end init_cache cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:10:42.261564.261564 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:10:42.262148.262148 cuda_h.py:19] end init_meta_layer cost 1.2636184692382812e-05 seconds
DEBUG 01-15 16:10:42.262788.262788 cuda_h.py:10] start init_weights
DEBUG 01-15 16:10:42.262312.262312 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:42.262307.262307 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:42.263568.263568 cuda_h.py:19] end allocate_cuda_memory cost 0.0006563663482666016 seconds
DEBUG 01-15 16:10:42.263941.263941 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:42.263697.263697 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:42.263513.263513 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:42.263117.263117 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d2db7a6a-148c-42fe-a781-94aa56ec99cc
DEBUG 01-15 16:10:42.263656.263656 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:42.264760.264760 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d2db7a6a-148c-42fe-a781-94aa56ec99cc
DEBUG 01-15 16:10:42.265550.265550 cuda_h.py:19] end load_into_gpu_async cost 0.0013306140899658203 seconds
DEBUG 01-15 16:10:42.265584.265584 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:42.265547.265547 cuda_h.py:19] end restore_tensors2 cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:10:42.265488.265488 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002249479293823242 seconds
DEBUG 01-15 16:10:42.265184.265184 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:42.265322.265322 cuda_h.py:19] end restore2model cost 0.0001494884490966797 seconds
INFO 01-15 16:10:42.265462.265462 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d2db7a6a-148c-42fe-a781-94aa56ec99cc
INFO 01-15 16:10:42.341286.341286 client.py:127] Model loaded
DEBUG 01-15 16:10:42.341775.341775 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:10:42.341468.341468 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:42.341492.341492 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:42.341017.341017 cuda_h.py:19] end allocate_cuda_memory cost 0.00036787986755371094 seconds
DEBUG 01-15 16:10:42.342830.342830 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:42.342131.342131 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:42.342922.342922 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:42.342302.342302 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6ad40307-7da8-4e74-988a-eee29f9aa8a5
DEBUG 01-15 16:10:42.342241.342241 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:42.344765.344765 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6ad40307-7da8-4e74-988a-eee29f9aa8a5
DEBUG 01-15 16:10:42.344353.344353 cuda_h.py:19] end load_into_gpu_async cost 0.002086162567138672 seconds
DEBUG 01-15 16:10:42.344323.344323 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:42.344895.344895 cuda_h.py:19] end restore_tensors2 cost 0.00020241737365722656 seconds
DEBUG 01-15 16:10:42.344839.344839 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0032835006713867188 seconds
INFO 01-15 16:10:42.344649.344649 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6ad40307-7da8-4e74-988a-eee29f9aa8a5
INFO 01-15 16:10:42.359382.359382 client.py:127] Model loaded
DEBUG 01-15 16:10:42.359612.359612 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:42.360967.360967 cuda_h.py:19] end restore2model cost 0.0008323192596435547 seconds
DEBUG 01-15 16:10:42.360098.360098 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.01963186264038086 seconds
DEBUG 01-15 16:10:42.361882.361882 cuda_h.py:19] end init_weights cost 0.09810066223144531 seconds
DEBUG 01-15 16:10:42.361686.361686 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:10:43.105792.105792 cuda_h.py:19] end copy_emodel cost 0.7441964149475098 seconds
DEBUG 01-15 16:10:43.106202.106202 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:10:43.106001.106001 cuda_h.py:19] end init_inputs_tokens cost 0.0004715919494628906 seconds
DEBUG 01-15 16:10:43.106261.106261 cuda_h.py:10] start prefill
DEBUG 01-15 16:10:43.106308.106308 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.106958.106958 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:10:43.106892.106892 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:43.106496.106496 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:43.106577.106577 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 3.337860107421875e-05 seconds
DEBUG 01-15 16:10:43.106301.106301 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 8.058547973632812e-05 seconds
DEBUG 01-15 16:10:43.106142.106142 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.106217.106217 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.107094.107094 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.107305.107305 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.107201.107201 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.107527.107527 cuda_h.py:19] end allocate_cuda_memory cost 0.00019741058349609375 seconds
DEBUG 01-15 16:10:43.107272.107272 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.107518.107518 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.107255.107255 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.107587.107587 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3a085a6c-4890-496b-8647-c5783761a9c5
DEBUG 01-15 16:10:43.107160.107160 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.108117.108117 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.109281.109281 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3a085a6c-4890-496b-8647-c5783761a9c5
DEBUG 01-15 16:10:43.109370.109370 cuda_h.py:19] end load_into_gpu_async cost 0.0015044212341308594 seconds
DEBUG 01-15 16:10:43.109556.109556 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.109573.109573 cuda_h.py:19] end restore_tensors2 cost 8.296966552734375e-05 seconds
DEBUG 01-15 16:10:43.109701.109701 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002107858657836914 seconds
INFO 01-15 16:10:43.109200.109200 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3a085a6c-4890-496b-8647-c5783761a9c5
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.111343.111343 cuda_h.py:19] end self_attn cost 0.0037186145782470703 seconds
DEBUG 01-15 16:10:43.112108.112108 cuda_h.py:19] end iln_self_attn_paln cost 0.00587916374206543 seconds
DEBUG 01-15 16:10:43.112938.112938 cuda_h.py:10] start dense_mlp
INFO 01-15 16:10:43.116473.116473 client.py:127] Model loaded
DEBUG 01-15 16:10:43.116945.116945 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.117661.117661 cuda_h.py:19] end restore2model cost 0.0005464553833007812 seconds
DEBUG 01-15 16:10:43.117795.117795 cuda_h.py:19] end sllm_worker_task cost 0.009917497634887695 seconds
DEBUG 01-15 16:10:43.117329.117329 cuda_h.py:19] end dense_mlp cost 0.00432133674621582 seconds
DEBUG 01-15 16:10:43.117995.117995 cuda_h.py:19] end prefill_layer cost 0.01059269905090332 seconds
DEBUG 01-15 16:10:43.117850.117850 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:10:43.117785.117785 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.117388.117388 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:10:43.117038.117038 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:43.117357.117357 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:43.117179.117179 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 2.09808349609375e-05 seconds
DEBUG 01-15 16:10:43.117213.117213 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:43.117287.117287 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.117229.117229 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.117973.117973 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.117107.117107 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.117288.117288 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.118872.118872 cuda_h.py:19] end allocate_cuda_memory cost 0.00017189979553222656 seconds
DEBUG 01-15 16:10:43.118286.118286 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.118400.118400 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.118780.118780 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.118503.118503 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 820a7d5d-7af0-4479-98f6-89acff681658
DEBUG 01-15 16:10:43.118646.118646 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.118010.118010 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.119174.119174 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 820a7d5d-7af0-4479-98f6-89acff681658
DEBUG 01-15 16:10:43.119919.119919 cuda_h.py:19] end load_into_gpu_async cost 0.0012562274932861328 seconds
DEBUG 01-15 16:10:43.119788.119788 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.119196.119196 cuda_h.py:19] end restore_tensors2 cost 7.963180541992188e-05 seconds
DEBUG 01-15 16:10:43.119946.119946 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0018749237060546875 seconds
INFO 01-15 16:10:43.119710.119710 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 820a7d5d-7af0-4479-98f6-89acff681658
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.121687.121687 cuda_h.py:19] end self_attn cost 0.0031430721282958984 seconds
DEBUG 01-15 16:10:43.122245.122245 cuda_h.py:19] end iln_self_attn_paln cost 0.0044710636138916016 seconds
DEBUG 01-15 16:10:43.122691.122691 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:10:43.122400.122400 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.122683.122683 cuda_h.py:19] end gate cost 0.0007026195526123047 seconds
DEBUG 01-15 16:10:43.122943.122943 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.123450.123450 lmp.py:1912] 
DEBUG 01-15 16:10:43.123450.123450 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.123398.123398 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.123286.123286 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.123552.123552 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.123387.123387 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.123553.123553 lmp.py:1917] 
DEBUG 01-15 16:10:43.123553.123553 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.123434.123434 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.123846.123846 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:10:43.123250.123250 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:10:43.123178.123178 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:43.123867.123867 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:10:43.123318.123318 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:10:43.123531.123531 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:10:43.123982.123982 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:10:43.123671.123671 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:10:43.123360.123360 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:10:43.123811.123811 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:10:43.123692.123692 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:10:43.123097.123097 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:10:43.123978.123978 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:10:43.123906.123906 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:10:43.123119.123119 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:10:43.123046.123046 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:10:43.123736.123736 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:10:43.123663.123663 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:10:43.123353.123353 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:10:43.123565.123565 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:10:43.123016.123016 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:10:43.123705.123705 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:10:43.123156.123156 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:10:43.123369.123369 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:10:43.123820.123820 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:10:43.123747.123747 lmp.py:1935]   Expert 34 |    161 | CPU
DEBUG 01-15 16:10:43.123913.123913 lmp.py:1935]   Expert 35 |    164 | CPU
DEBUG 01-15 16:10:43.123318.123318 lmp.py:1935]   Expert 36 |    168 | CPU
DEBUG 01-15 16:10:43.123723.123723 lmp.py:1935]   Expert 11 |    170 | CPU
DEBUG 01-15 16:10:43.123365.123365 lmp.py:1935]   Expert 17 |    170 | CPU
DEBUG 01-15 16:10:43.123293.123293 lmp.py:1935]   Expert 59 |    174 | CPU
DEBUG 01-15 16:10:43.123506.123506 lmp.py:1935]   Expert 10 |    180 | CPU
DEBUG 01-15 16:10:43.123102.123102 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.123984.123984 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.123388.123388 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.123793.123793 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.123197.123197 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124754.124754 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124397.124397 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124471.124471 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124068.124068 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124903.124903 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124784.124784 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124427.124427 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124593.124593 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124236.124236 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124879.124879 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124522.124522 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124688.124688 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124331.124331 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124497.124497 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124140.124140 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124545.124545 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124380.124380 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124977.124977 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124858.124858 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124216.124216 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124098.124098 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124277.124277 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124371.124371 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124253.124253 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124372.124372 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124015.124015 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.124897.124897 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.124824.124824 lmp.py:1937] 
DEBUG 01-15 16:10:43.124824.124824 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.124706.124706 lmp.py:1938]   CPU:   4059 tokens
DEBUG 01-15 16:10:43.124541.124541 lmp.py:1942]   cuda:1:   4114 tokens (16 experts)
DEBUG 01-15 16:10:43.124661.124661 lmp.py:1942]   cuda:2:   4115 tokens (16 experts)
DEBUG 01-15 16:10:43.124781.124781 lmp.py:1943]   Total GPU:   8229 tokens
DEBUG 01-15 16:10:43.124662.124662 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.124662.124662 lmp.py:1944] 
DEBUG 01-15 16:10:43.124312.124312 cuda_h.py:19] end experts_map_get cost 0.0016834735870361328 seconds
DEBUG 01-15 16:10:43.124168.124168 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.124494.124494 lmp.py:1953] 
DEBUG 01-15 16:10:43.124494.124494 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.124469.124469 cuda_h.py:19] end cpu_experts_submit cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:43.124496.124496 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.124180.124180 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.126657.126657 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.1.mlp.experts.33.gate_proj.weight': 0, 'model.layers.1.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.14.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.14.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.14.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.15.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.15.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.15.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.21.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.21.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.21.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.23.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.23.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.23.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.56.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3535273984, 5767168, 34603008, 0), (3541041152, 5767168, 40370176, 0), (3529506816, 5767168, 46137344, 0), (3016228864, 5767168, 51904512, 0), (3021996032, 5767168, 57671680, 0), (3010461696, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3690987520, 5767168, 86507520, 0), (3696754688, 5767168, 92274688, 0), (3685220352, 5767168, 98041856, 0), (3362258944, 5767168, 103809024, 0), (3368026112, 5767168, 109576192, 0), (3356491776, 5767168, 115343360, 0), (3725590528, 5767168, 121110528, 0), (3731357696, 5767168, 126877696, 0), (3719823360, 5767168, 132644864, 0), (3189243904, 5767168, 138412032, 0), (3195011072, 5767168, 144179200, 0), (3183476736, 5767168, 149946368, 0), (3206545408, 5767168, 155713536, 0), (3212312576, 5767168, 161480704, 0), (3200778240, 5767168, 167247872, 0), (3275751424, 5767168, 173015040, 0), (3281518592, 5767168, 178782208, 0), (3269984256, 5767168, 184549376, 0), (3846701056, 5767168, 190316544, 0), (3852468224, 5767168, 196083712, 0), (3840933888, 5767168, 201850880, 0), (3310354432, 5767168, 207618048, 0), (3316121600, 5767168, 213385216, 0), (3304587264, 5767168, 219152384, 0), (3915907072, 5767168, 224919552, 0), (3921674240, 5767168, 230686720, 0), (3910139904, 5767168, 236453888, 0), (3379560448, 5767168, 242221056, 0), (3385327616, 5767168, 247988224, 0), (3373793280, 5767168, 253755392, 0), (3950510080, 5767168, 259522560, 0), (3956277248, 5767168, 265289728, 0), (3944742912, 5767168, 271056896, 0)], 2: [(3431464960, 5767168, 0, 0), (3437232128, 5767168, 5767168, 0), (3425697792, 5767168, 11534336, 0), (2895118336, 5767168, 17301504, 0), (2900885504, 5767168, 23068672, 0), (2889351168, 5767168, 28835840, 0), (2929721344, 5767168, 34603008, 0), (2935488512, 5767168, 40370176, 0), (2923954176, 5767168, 46137344, 0), (2947022848, 5767168, 51904512, 0), (2952790016, 5767168, 57671680, 0), (2941255680, 5767168, 63438848, 0), (3552575488, 5767168, 69206016, 0), (3558342656, 5767168, 74973184, 0), (3546808320, 5767168, 80740352, 0), (2998927360, 5767168, 86507520, 0), (3004694528, 5767168, 92274688, 0), (2993160192, 5767168, 98041856, 0), (3587178496, 5767168, 103809024, 0), (3592945664, 5767168, 109576192, 0), (3581411328, 5767168, 115343360, 0), (3639083008, 5767168, 121110528, 0), (3644850176, 5767168, 126877696, 0), (3633315840, 5767168, 132644864, 0), (3656384512, 5767168, 138412032, 0), (3662151680, 5767168, 144179200, 0), (3650617344, 5767168, 149946368, 0), (3102736384, 5767168, 155713536, 0), (3108503552, 5767168, 161480704, 0), (3096969216, 5767168, 167247872, 0), (3137339392, 5767168, 173015040, 0), (3143106560, 5767168, 178782208, 0), (3131572224, 5767168, 184549376, 0), (3120037888, 5767168, 190316544, 0), (3125805056, 5767168, 196083712, 0), (3114270720, 5767168, 201850880, 0), (3777495040, 5767168, 207618048, 0), (3783262208, 5767168, 213385216, 0), (3771727872, 5767168, 219152384, 0), (3223846912, 5767168, 224919552, 0), (3229614080, 5767168, 230686720, 0), (3218079744, 5767168, 236453888, 0), (3258449920, 5767168, 242221056, 0), (3264217088, 5767168, 247988224, 0), (3252682752, 5767168, 253755392, 0), (3829399552, 5767168, 259522560, 0), (3835166720, 5767168, 265289728, 0), (3823632384, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c2c1d40>, 2: <capsule object NULL at 0x7a4f2c2c25b0>}
DEBUG 01-15 16:10:43.126847.126847 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.126054.126054 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.126472.126472 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5af6d4a6-e289-4fd5-9787-ddc21e564e28
DEBUG 01-15 16:10:43.126279.126279 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:43.126321.126321 client.py:127] Model loaded
DEBUG 01-15 16:10:43.127259.127259 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.127258.127258 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:43.127105.127105 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.127988.127988 cuda_h.py:19] end restore2model cost 0.0007402896881103516 seconds
INFO 01-15 16:10:43.127237.127237 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5af6d4a6-e289-4fd5-9787-ddc21e564e28
DEBUG 01-15 16:10:43.128353.128353 cuda_h.py:19] end sllm_worker_task cost 0.010286808013916016 seconds
DEBUG 01-15 16:10:43.128269.128269 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003603696823120117 seconds
DEBUG 01-15 16:10:43.128082.128082 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.128568.128568 cuda_h.py:19] end move_flatidxs cost 0.0008902549743652344 seconds
DEBUG 01-15 16:10:43.128656.128656 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.131050.131050 cuda_h.py:19] end restore2model cost 0.0025720596313476562 seconds
DEBUG 01-15 16:10:43.131469.131469 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006487369537353516 seconds
DEBUG 01-15 16:10:43.131000.131000 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.131739.131739 cuda_h.py:19] end gpu_sexperts cost 0.0002675056457519531 seconds
DEBUG 01-15 16:10:43.131615.131615 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.131292.131292 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.33514404296875e-05 seconds
DEBUG 01-15 16:10:43.131226.131226 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.131883.131883 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.132805.132805 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000823974609375 seconds
DEBUG 01-15 16:10:43.132602.132602 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.132119.132119 cuda_h.py:19] end gpu_group_list cost 0.0001761913299560547 seconds
DEBUG 01-15 16:10:43.133826.133826 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.135116.135116 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014867782592773438 seconds
DEBUG 01-15 16:10:43.135635.135635 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.134380.134380 cuda_h.py:19] end group_tensors cost 0.006139516830444336 seconds
DEBUG 01-15 16:10:43.135649.135649 cuda_h.py:19] end gpu_group_list cost 0.00017595291137695312 seconds
DEBUG 01-15 16:10:43.135067.135067 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.136311.136311 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.136684.136684 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5af6d4a6-e289-4fd5-9787-ddc21e564e28
DEBUG 01-15 16:10:43.146718.146718 cuda_h.py:19] end group pad cost 0.01042628288269043 seconds
DEBUG 01-15 16:10:43.146985.146985 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.154448.154448 client.py:127] Model loaded
DEBUG 01-15 16:10:43.154162.154162 cuda_h.py:19] end wait_experts_multi_device cost 0.01761460304260254 seconds
DEBUG 01-15 16:10:43.154640.154640 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.167693.167693 cuda_h.py:19] end group_einsum cost 0.020649433135986328 seconds
DEBUG 01-15 16:10:43.167394.167394 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.171463.171463 cuda_h.py:19] end get_outputs_cpu1 cost 0.004152059555053711 seconds
DEBUG 01-15 16:10:43.172921.172921 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04483509063720703 seconds
DEBUG 01-15 16:10:43.172703.172703 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.018249034881591797 seconds
DEBUG 01-15 16:10:43.172322.172322 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.174470.174470 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.174740.174740 cuda_h.py:19] end index_scatter cost 8.440017700195312e-05 seconds
DEBUG 01-15 16:10:43.174393.174393 cuda_h.py:19] end cpuoutputsdeal cost 0.0016608238220214844 seconds
DEBUG 01-15 16:10:43.174309.174309 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.174834.174834 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.175982.175982 cuda_h.py:19] end gpu_group_tensor cost 0.0006289482116699219 seconds
DEBUG 01-15 16:10:43.175017.175017 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.176973.176973 cuda_h.py:19] end gpu_group_tensor cost 0.0006246566772460938 seconds
DEBUG 01-15 16:10:43.176488.176488 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.176070.176070 cuda_h.py:19] end gpu_group_einsum cost 0.0007140636444091797 seconds
DEBUG 01-15 16:10:43.177485.177485 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.178618.178618 cuda_h.py:19] end gpu_group_einsum cost 0.0009584426879882812 seconds
DEBUG 01-15 16:10:43.178112.178112 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.178371.178371 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.178471.178471 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016307830810546875 seconds
DEBUG 01-15 16:10:43.178327.178327 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.178937.178937 cuda_h.py:19] end concat_expert_out cost 0.00020432472229003906 seconds
DEBUG 01-15 16:10:43.179576.179576 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.179480.179480 cuda_h.py:19] end index_scatter cost 6.580352783203125e-05 seconds
DEBUG 01-15 16:10:43.179952.179952 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010988712310791016 seconds
DEBUG 01-15 16:10:43.179412.179412 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.179917.179917 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.179996.179996 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001316070556640625 seconds
DEBUG 01-15 16:10:43.179321.179321 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.179099.179099 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:43.179796.179796 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.179673.179673 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:43.179482.179482 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00047206878662109375 seconds
DEBUG 01-15 16:10:43.180762.180762 cuda_h.py:19] end gpu_experts_multi_device cost 0.04822993278503418 seconds
DEBUG 01-15 16:10:43.180573.180573 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.05796074867248535 seconds
DEBUG 01-15 16:10:43.180624.180624 cuda_h.py:19] end prefill_layer cost 0.06302022933959961 seconds
DEBUG 01-15 16:10:43.180997.180997 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:10:43.180415.180415 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.180641.180641 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:10:43.180344.180344 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:43.180431.180431 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:43.180950.180950 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 3.886222839355469e-05 seconds
DEBUG 01-15 16:10:43.180898.180898 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 7.319450378417969e-05 seconds
DEBUG 01-15 16:10:43.180694.180694 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.180689.180689 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.180919.180919 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.180980.180980 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.181011.181011 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.181085.181085 cuda_h.py:19] end allocate_cuda_memory cost 0.0001888275146484375 seconds
DEBUG 01-15 16:10:43.181955.181955 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.181625.181625 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.181978.181978 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.181926.181926 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b836ef8d-7f53-451c-8982-d8b30758777f
DEBUG 01-15 16:10:43.181545.181545 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.181105.181105 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.183237.183237 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b836ef8d-7f53-451c-8982-d8b30758777f
DEBUG 01-15 16:10:43.183816.183816 cuda_h.py:19] end load_into_gpu_async cost 0.0019299983978271484 seconds
DEBUG 01-15 16:10:43.183764.183764 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.183867.183867 cuda_h.py:19] end restore_tensors2 cost 7.963180541992188e-05 seconds
DEBUG 01-15 16:10:43.183961.183961 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.00247955322265625 seconds
INFO 01-15 16:10:43.183612.183612 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b836ef8d-7f53-451c-8982-d8b30758777f
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.185988.185988 cuda_h.py:19] end self_attn cost 0.0031456947326660156 seconds
DEBUG 01-15 16:10:43.185143.185143 cuda_h.py:19] end iln_self_attn_paln cost 0.004688262939453125 seconds
DEBUG 01-15 16:10:43.185019.185019 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:10:43.185159.185159 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.186607.186607 cuda_h.py:19] end gate cost 0.0006809234619140625 seconds
DEBUG 01-15 16:10:43.186821.186821 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.186341.186341 lmp.py:1912] 
DEBUG 01-15 16:10:43.186341.186341 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.186097.186097 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.186701.186701 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.186774.186774 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.186179.186179 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.186106.186106 lmp.py:1917] 
DEBUG 01-15 16:10:43.186106.186106 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.186703.186703 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.186591.186591 lmp.py:1935]   Expert 58 |     51 | CPU
DEBUG 01-15 16:10:43.186996.186996 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:10:43.186208.186208 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:43.186421.186421 lmp.py:1935]   Expert 17 |     84 | CPU
DEBUG 01-15 16:10:43.186872.186872 lmp.py:1935]   Expert 24 |     87 | CPU
DEBUG 01-15 16:10:43.186084.186084 lmp.py:1935]   Expert  0 |     89 | CPU
DEBUG 01-15 16:10:43.186296.186296 lmp.py:1935]   Expert 28 |    105 | CPU
DEBUG 01-15 16:10:43.186509.186509 lmp.py:1935]   Expert 34 |    115 | CPU
DEBUG 01-15 16:10:43.186721.186721 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:10:43.186457.186457 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:10:43.186431.186431 lmp.py:1935]   Expert  9 |    130 | CPU
DEBUG 01-15 16:10:43.186312.186312 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:10:43.186240.186240 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:10:43.186883.186883 lmp.py:1935]   Expert 23 |    135 | CPU
DEBUG 01-15 16:10:43.187288.187288 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:43.187692.187692 lmp.py:1935]   Expert 30 |    143 | CPU
DEBUG 01-15 16:10:43.187335.187335 lmp.py:1935]   Expert 45 |    146 | CPU
DEBUG 01-15 16:10:43.187263.187263 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:10:43.187952.187952 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:10:43.187642.187642 lmp.py:1935]   Expert  1 |    153 | CPU
DEBUG 01-15 16:10:43.187808.187808 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:10:43.187497.187497 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:10:43.187186.187186 lmp.py:1935]   Expert 29 |    161 | CPU
DEBUG 01-15 16:10:43.187114.187114 lmp.py:1935]   Expert 25 |    164 | CPU
DEBUG 01-15 16:10:43.187042.187042 lmp.py:1935]   Expert 54 |    167 | CPU
DEBUG 01-15 16:10:43.187969.187969 lmp.py:1935]   Expert  6 |    171 | CPU
DEBUG 01-15 16:10:43.187659.187659 lmp.py:1935]   Expert 48 |    172 | CPU
DEBUG 01-15 16:10:43.187302.187302 lmp.py:1935]   Expert 49 |    172 | CPU
DEBUG 01-15 16:10:43.187468.187468 lmp.py:1935]   Expert 12 |    175 | CPU
DEBUG 01-15 16:10:43.187872.187872 lmp.py:1935]   Expert 35 |    175 | CPU
DEBUG 01-15 16:10:43.187800.187800 lmp.py:1935]   Expert 37 |    178 | CPU
DEBUG 01-15 16:10:43.187013.187013 lmp.py:1935]   Expert 60 |    185 | CPU
DEBUG 01-15 16:10:43.187086.187086 lmp.py:1935]   Expert 13 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187921.187921 lmp.py:1935]   Expert 33 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187041.187041 lmp.py:1935]   Expert 53 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187399.187399 lmp.py:1935]   Expert 10 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187042.187042 lmp.py:1935]   Expert 16 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187924.187924 lmp.py:1935]   Expert 21 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187805.187805 lmp.py:1935]   Expert 40 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187448.187448 lmp.py:1935]   Expert 43 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187091.187091 lmp.py:1935]   Expert 38 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187211.187211 lmp.py:1935]   Expert  5 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187569.187569 lmp.py:1935]   Expert 44 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187450.187450 lmp.py:1935]   Expert 50 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187570.187570 lmp.py:1935]   Expert 52 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187213.187213 lmp.py:1935]   Expert 19 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187379.187379 lmp.py:1935]   Expert 41 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187784.187784 lmp.py:1935]   Expert  4 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187188.187188 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187831.187831 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187998.187998 lmp.py:1935]   Expert 56 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187879.187879 lmp.py:1935]   Expert 31 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187283.187283 lmp.py:1935]   Expert 20 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187403.187403 lmp.py:1935]   Expert 39 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187523.187523 lmp.py:1935]   Expert 22 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187643.187643 lmp.py:1935]   Expert  2 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187001.187001 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187598.187598 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187241.187241 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187645.187645 lmp.py:1935]   Expert 18 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187050.187050 lmp.py:1935]   Expert 14 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187693.187693 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187097.187097 lmp.py:1935]   Expert 11 |    389 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.187264.187264 lmp.py:1935]   Expert 61 |    461 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.187476.187476 lmp.py:1937] 
DEBUG 01-15 16:10:43.187476.187476 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.187881.187881 lmp.py:1938]   CPU:   4339 tokens
DEBUG 01-15 16:10:43.187716.187716 lmp.py:1942]   cuda:1:   3974 tokens (16 experts)
DEBUG 01-15 16:10:43.187835.187835 lmp.py:1942]   cuda:2:   3975 tokens (16 experts)
DEBUG 01-15 16:10:43.187002.187002 lmp.py:1943]   Total GPU:   7949 tokens
DEBUG 01-15 16:10:43.187168.187168 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.187168.187168 lmp.py:1944] 
DEBUG 01-15 16:10:43.187341.187341 cuda_h.py:19] end experts_map_get cost 0.0016541481018066406 seconds
DEBUG 01-15 16:10:43.188045.188045 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.188655.188655 lmp.py:1953] 
DEBUG 01-15 16:10:43.188655.188655 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.188677.188677 cuda_h.py:19] end cpu_experts_submit cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:43.188134.188134 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.188772.188772 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.189641.189641 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.33.gate_proj.weight': 0, 'model.layers.2.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.2.mlp.experts.2.gate_proj.weight': 0, 'model.layers.2.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.40.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.40.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.40.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.47.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.47.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.47.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.20.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.20.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.20.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(4538761216, 5767168, 0, 0), (4544528384, 5767168, 5767168, 0), (4532994048, 5767168, 11534336, 0), (4037017600, 5767168, 17301504, 0), (4042784768, 5767168, 23068672, 0), (4031250432, 5767168, 28835840, 0), (4625268736, 5767168, 34603008, 0), (4631035904, 5767168, 40370176, 0), (4619501568, 5767168, 46137344, 0), (4642570240, 5767168, 51904512, 0), (4648337408, 5767168, 57671680, 0), (4636803072, 5767168, 63438848, 0), (4694474752, 5767168, 69206016, 0), (4700241920, 5767168, 74973184, 0), (4688707584, 5767168, 80740352, 0), (4711776256, 5767168, 86507520, 0), (4717543424, 5767168, 92274688, 0), (4706009088, 5767168, 98041856, 0), (4210032640, 5767168, 103809024, 0), (4215799808, 5767168, 109576192, 0), (4204265472, 5767168, 115343360, 0), (4832886784, 5767168, 121110528, 0), (4838653952, 5767168, 126877696, 0), (4827119616, 5767168, 132644864, 0), (4296540160, 5767168, 138412032, 0), (4302307328, 5767168, 144179200, 0), (4290772992, 5767168, 149946368, 0), (4331143168, 5767168, 155713536, 0), (4336910336, 5767168, 161480704, 0), (4325376000, 5767168, 167247872, 0), (4348444672, 5767168, 173015040, 0), (4354211840, 5767168, 178782208, 0), (4342677504, 5767168, 184549376, 0), (4884791296, 5767168, 190316544, 0), (4890558464, 5767168, 196083712, 0), (4879024128, 5767168, 201850880, 0), (4936695808, 5767168, 207618048, 0), (4942462976, 5767168, 213385216, 0), (4930928640, 5767168, 219152384, 0), (4988600320, 5767168, 224919552, 0), (4994367488, 5767168, 230686720, 0), (4982833152, 5767168, 236453888, 0), (5023203328, 5767168, 242221056, 0), (5028970496, 5767168, 247988224, 0), (5017436160, 5767168, 253755392, 0), (5057806336, 5767168, 259522560, 0), (5063573504, 5767168, 265289728, 0), (5052039168, 5767168, 271056896, 0)], 2: [(4002414592, 5767168, 0, 0), (4008181760, 5767168, 5767168, 0), (3996647424, 5767168, 11534336, 0), (4054319104, 5767168, 17301504, 0), (4060086272, 5767168, 23068672, 0), (4048551936, 5767168, 28835840, 0), (4659871744, 5767168, 34603008, 0), (4665638912, 5767168, 40370176, 0), (4654104576, 5767168, 46137344, 0), (4677173248, 5767168, 51904512, 0), (4682940416, 5767168, 57671680, 0), (4671406080, 5767168, 63438848, 0), (4140826624, 5767168, 69206016, 0), (4146593792, 5767168, 74973184, 0), (4135059456, 5767168, 80740352, 0), (4158128128, 5767168, 86507520, 0), (4163895296, 5767168, 92274688, 0), (4152360960, 5767168, 98041856, 0), (4729077760, 5767168, 103809024, 0), (4734844928, 5767168, 109576192, 0), (4723310592, 5767168, 115343360, 0), (4192731136, 5767168, 121110528, 0), (4198498304, 5767168, 126877696, 0), (4186963968, 5767168, 132644864, 0), (4763680768, 5767168, 138412032, 0), (4769447936, 5767168, 144179200, 0), (4757913600, 5767168, 149946368, 0), (4780982272, 5767168, 155713536, 0), (4786749440, 5767168, 161480704, 0), (4775215104, 5767168, 167247872, 0), (4244635648, 5767168, 173015040, 0), (4250402816, 5767168, 178782208, 0), (4238868480, 5767168, 184549376, 0), (4279238656, 5767168, 190316544, 0), (4285005824, 5767168, 196083712, 0), (4273471488, 5767168, 201850880, 0), (4313841664, 5767168, 207618048, 0), (4319608832, 5767168, 213385216, 0), (4308074496, 5767168, 219152384, 0), (4867489792, 5767168, 224919552, 0), (4873256960, 5767168, 230686720, 0), (4861722624, 5767168, 236453888, 0), (4919394304, 5767168, 242221056, 0), (4925161472, 5767168, 247988224, 0), (4913627136, 5767168, 253755392, 0), (4504158208, 5767168, 259522560, 0), (4509925376, 5767168, 265289728, 0), (4498391040, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a5afdc8dfb0>, 2: <capsule object NULL at 0x7a4e543ce070>}
DEBUG 01-15 16:10:43.189002.189002 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.190640.190640 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.190296.190296 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a0e8bd85-fff0-4662-88a3-684947de3759
DEBUG 01-15 16:10:43.190601.190601 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.191942.191942 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:43.191034.191034 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:43.191167.191167 client.py:127] Model loaded
DEBUG 01-15 16:10:43.191096.191096 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.191599.191599 cuda_h.py:19] end restore2model cost 0.00033783912658691406 seconds
DEBUG 01-15 16:10:43.191845.191845 cuda_h.py:19] end sllm_worker_task cost 0.011007547378540039 seconds
DEBUG 01-15 16:10:43.192840.192840 cuda_h.py:19] end move_flatidxs cost 0.0008819103240966797 seconds
DEBUG 01-15 16:10:43.192816.192816 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:43.193073.193073 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a0e8bd85-fff0-4662-88a3-684947de3759
DEBUG 01-15 16:10:43.193948.193948 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00543522834777832 seconds
DEBUG 01-15 16:10:43.193964.193964 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.196257.196257 cuda_h.py:19] end restore2model cost 0.002562999725341797 seconds
DEBUG 01-15 16:10:43.196219.196219 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008255481719970703 seconds
DEBUG 01-15 16:10:43.196492.196492 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.196383.196383 cuda_h.py:19] end gpu_sexperts cost 0.00026917457580566406 seconds
DEBUG 01-15 16:10:43.196543.196543 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.196505.196505 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4543533325195312e-05 seconds
DEBUG 01-15 16:10:43.196440.196440 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.196666.196666 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.197230.197230 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008056163787841797 seconds
DEBUG 01-15 16:10:43.197696.197696 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.198650.198650 cuda_h.py:19] end gpu_group_list cost 0.00018358230590820312 seconds
DEBUG 01-15 16:10:43.198708.198708 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.198459.198459 cuda_h.py:19] end group_tensors cost 0.0059125423431396484 seconds
DEBUG 01-15 16:10:43.199007.199007 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.199106.199106 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001142740249633789 seconds
DEBUG 01-15 16:10:43.200607.200607 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.200049.200049 cuda_h.py:19] end gpu_group_list cost 0.00030803680419921875 seconds
DEBUG 01-15 16:10:43.201213.201213 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.201454.201454 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a0e8bd85-fff0-4662-88a3-684947de3759
DEBUG 01-15 16:10:43.203870.203870 cuda_h.py:19] end group pad cost 0.003945350646972656 seconds
DEBUG 01-15 16:10:43.203282.203282 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.220810.220810 client.py:127] Model loaded
DEBUG 01-15 16:10:43.220507.220507 cuda_h.py:19] end wait_experts_multi_device cost 0.019350290298461914 seconds
DEBUG 01-15 16:10:43.220291.220291 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.223336.223336 cuda_h.py:19] end group_einsum cost 0.02010941505432129 seconds
DEBUG 01-15 16:10:43.223540.223540 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.230418.230418 cuda_h.py:19] end get_outputs_cpu1 cost 0.006783962249755859 seconds
DEBUG 01-15 16:10:43.231857.231857 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03992605209350586 seconds
DEBUG 01-15 16:10:43.231587.231587 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010532379150390625 seconds
DEBUG 01-15 16:10:43.231789.231789 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.232796.232796 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.233146.233146 cuda_h.py:19] end index_scatter cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:43.233023.233023 cuda_h.py:19] end cpuoutputsdeal cost 0.0016083717346191406 seconds
DEBUG 01-15 16:10:43.233264.233264 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.233927.233927 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.233774.233774 cuda_h.py:19] end gpu_group_tensor cost 0.0001373291015625 seconds
DEBUG 01-15 16:10:43.233437.233437 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.233933.233933 cuda_h.py:19] end gpu_group_tensor cost 0.00012421607971191406 seconds
DEBUG 01-15 16:10:43.233360.233360 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.234007.234007 cuda_h.py:19] end gpu_group_einsum cost 0.0006723403930664062 seconds
DEBUG 01-15 16:10:43.234567.234567 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.235085.235085 cuda_h.py:19] end gpu_group_einsum cost 0.0003666877746582031 seconds
DEBUG 01-15 16:10:43.235181.235181 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.235548.235548 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.235588.235588 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016045570373535156 seconds
DEBUG 01-15 16:10:43.235536.235536 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.235215.235215 cuda_h.py:19] end concat_expert_out cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:10:43.235058.235058 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.235173.235173 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:43.236956.236956 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007369518280029297 seconds
DEBUG 01-15 16:10:43.236223.236223 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.236206.236206 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.236708.236708 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001285076141357422 seconds
DEBUG 01-15 16:10:43.236079.236079 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.236930.236930 cuda_h.py:19] end concat_expert_out cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:10:43.236866.236866 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.236313.236313 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:43.236883.236883 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00048542022705078125 seconds
DEBUG 01-15 16:10:43.236124.236124 cuda_h.py:19] end gpu_experts_multi_device cost 0.039808034896850586 seconds
DEBUG 01-15 16:10:43.236557.236557 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.051244258880615234 seconds
DEBUG 01-15 16:10:43.237761.237761 cuda_h.py:19] end prefill_layer cost 0.0565800666809082 seconds
DEBUG 01-15 16:10:43.237127.237127 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:10:43.237830.237830 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.237248.237248 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:10:43.237143.237143 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:43.237184.237184 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:43.237226.237226 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 3.7670135498046875e-05 seconds
DEBUG 01-15 16:10:43.237028.237028 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 6.985664367675781e-05 seconds
DEBUG 01-15 16:10:43.237117.237117 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.237993.237993 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.237731.237731 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.237165.237165 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.237465.237465 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.238397.238397 cuda_h.py:19] end allocate_cuda_memory cost 0.0002262592315673828 seconds
DEBUG 01-15 16:10:43.238897.238897 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.238282.238282 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.238371.238371 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.238226.238226 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 83696d42-96aa-4d1b-b2cb-4eae54e2aa8e
DEBUG 01-15 16:10:43.238375.238375 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.238198.238198 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.240789.240789 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 83696d42-96aa-4d1b-b2cb-4eae54e2aa8e
DEBUG 01-15 16:10:43.240394.240394 cuda_h.py:19] end load_into_gpu_async cost 0.0019423961639404297 seconds
DEBUG 01-15 16:10:43.240434.240434 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.240160.240160 cuda_h.py:19] end restore_tensors2 cost 8.058547973632812e-05 seconds
DEBUG 01-15 16:10:43.240507.240507 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0026557445526123047 seconds
INFO 01-15 16:10:43.240034.240034 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 83696d42-96aa-4d1b-b2cb-4eae54e2aa8e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.242158.242158 cuda_h.py:19] end self_attn cost 0.0032706260681152344 seconds
DEBUG 01-15 16:10:43.242738.242738 cuda_h.py:19] end iln_self_attn_paln cost 0.004721879959106445 seconds
DEBUG 01-15 16:10:43.242568.242568 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:10:43.242331.242331 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.243355.243355 cuda_h.py:19] end gate cost 0.0006818771362304688 seconds
DEBUG 01-15 16:10:43.243290.243290 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.243172.243172 lmp.py:1912] 
DEBUG 01-15 16:10:43.243172.243172 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.243504.243504 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.243346.243346 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.243850.243850 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.243969.243969 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.243136.243136 lmp.py:1917] 
DEBUG 01-15 16:10:43.243136.243136 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.243732.243732 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.243336.243336 lmp.py:1935]   Expert  1 |     50 | CPU
DEBUG 01-15 16:10:43.243740.243740 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:10:43.243668.243668 lmp.py:1935]   Expert  7 |     75 | CPU
DEBUG 01-15 16:10:43.243119.243119 lmp.py:1935]   Expert 48 |     81 | CPU
DEBUG 01-15 16:10:43.243716.243716 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:10:43.243074.243074 lmp.py:1935]   Expert 30 |    108 | CPU
DEBUG 01-15 16:10:43.243432.243432 lmp.py:1935]   Expert 61 |    116 | CPU
DEBUG 01-15 16:10:43.243313.243313 lmp.py:1935]   Expert 18 |    118 | CPU
DEBUG 01-15 16:10:43.243672.243672 lmp.py:1935]   Expert 32 |    119 | CPU
DEBUG 01-15 16:10:43.243076.243076 lmp.py:1935]   Expert 45 |    119 | CPU
DEBUG 01-15 16:10:43.243242.243242 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:10:43.244647.244647 lmp.py:1935]   Expert 39 |    134 | CPU
DEBUG 01-15 16:10:43.244575.244575 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:43.244741.244741 lmp.py:1935]   Expert 36 |    138 | CPU
DEBUG 01-15 16:10:43.244668.244668 lmp.py:1935]   Expert 11 |    139 | CPU
DEBUG 01-15 16:10:43.244788.244788 lmp.py:1935]   Expert  5 |    142 | CPU
DEBUG 01-15 16:10:43.244431.244431 lmp.py:1935]   Expert 59 |    143 | CPU
DEBUG 01-15 16:10:43.244313.244313 lmp.py:1935]   Expert  6 |    144 | CPU
DEBUG 01-15 16:10:43.244956.244956 lmp.py:1935]   Expert 51 |    145 | CPU
DEBUG 01-15 16:10:43.244599.244599 lmp.py:1935]   Expert 49 |    155 | CPU
DEBUG 01-15 16:10:43.244003.244003 lmp.py:1935]   Expert 23 |    156 | CPU
DEBUG 01-15 16:10:43.244169.244169 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:10:43.244335.244335 lmp.py:1935]   Expert  9 |    158 | CPU
DEBUG 01-15 16:10:43.244025.244025 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:10:43.244191.244191 lmp.py:1935]   Expert 52 |    168 | CPU
DEBUG 01-15 16:10:43.244357.244357 lmp.py:1935]   Expert 56 |    168 | CPU
DEBUG 01-15 16:10:43.244285.244285 lmp.py:1935]   Expert 40 |    169 | CPU
DEBUG 01-15 16:10:43.244451.244451 lmp.py:1935]   Expert 16 |    172 | CPU
DEBUG 01-15 16:10:43.244617.244617 lmp.py:1935]   Expert 35 |    172 | CPU
DEBUG 01-15 16:10:43.244260.244260 lmp.py:1935]   Expert  4 |    186 | CPU
DEBUG 01-15 16:10:43.244664.244664 lmp.py:1935]   Expert 42 |    190 | CPU
DEBUG 01-15 16:10:43.244307.244307 lmp.py:1935]   Expert 13 |    191 | CPU
DEBUG 01-15 16:10:43.244295.244295 lmp.py:1935]   Expert 37 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244322.244322 lmp.py:1935]   Expert 17 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244634.244634 lmp.py:1935]   Expert 38 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244708.244708 lmp.py:1935]   Expert 62 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244066.244066 lmp.py:1935]   Expert 21 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244424.244424 lmp.py:1935]   Expert  3 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244782.244782 lmp.py:1935]   Expert 44 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244141.244141 lmp.py:1935]   Expert 28 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244022.244022 lmp.py:1935]   Expert 58 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244619.244619 lmp.py:1935]   Expert 60 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244454.244454 lmp.py:1935]   Expert 47 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244289.244289 lmp.py:1935]   Expert 10 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244885.244885 lmp.py:1935]   Expert 53 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244720.244720 lmp.py:1935]   Expert 55 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244079.244079 lmp.py:1935]   Expert 20 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244437.244437 lmp.py:1935]   Expert 57 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244557.244557 lmp.py:1935]   Expert 33 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244438.244438 lmp.py:1935]   Expert 31 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244081.244081 lmp.py:1935]   Expert 46 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244439.244439 lmp.py:1935]   Expert  8 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244321.244321 lmp.py:1935]   Expert 19 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244917.244917 lmp.py:1935]   Expert 24 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244276.244276 lmp.py:1935]   Expert 14 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244628.244628 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244510.244510 lmp.py:1935]   Expert 29 |    273 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244629.244629 lmp.py:1935]   Expert 12 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244272.244272 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244869.244869 lmp.py:1935]   Expert  0 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244512.244512 lmp.py:1935]   Expert 43 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.244917.244917 lmp.py:1935]   Expert 54 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244321.244321 lmp.py:1935]   Expert 41 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.244726.244726 lmp.py:1935]   Expert 25 |    412 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.245177.245177 lmp.py:1937] 
DEBUG 01-15 16:10:43.245177.245177 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.245581.245581 lmp.py:1938]   CPU:   4409 tokens
DEBUG 01-15 16:10:43.245178.245178 lmp.py:1942]   cuda:1:   3938 tokens (16 experts)
DEBUG 01-15 16:10:43.245536.245536 lmp.py:1942]   cuda:2:   3941 tokens (16 experts)
DEBUG 01-15 16:10:43.245464.245464 lmp.py:1943]   Total GPU:   7879 tokens
DEBUG 01-15 16:10:43.245391.245391 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.245391.245391 lmp.py:1944] 
DEBUG 01-15 16:10:43.245280.245280 cuda_h.py:19] end experts_map_get cost 0.0018208026885986328 seconds
DEBUG 01-15 16:10:43.245937.245937 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.245594.245594 lmp.py:1953] 
DEBUG 01-15 16:10:43.245594.245594 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.245662.245662 cuda_h.py:19] end cpu_experts_submit cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:43.245881.245881 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.245850.245850 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.247763.247763 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.43.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.43.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.43.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.44.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.44.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.44.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.3.mlp.experts.33.gate_proj.weight': 0, 'model.layers.3.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.38.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.38.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.38.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.55.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.55.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.55.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.54.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.54.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.54.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.57.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.57.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.57.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5127012352, 5767168, 17301504, 0), (5132779520, 5767168, 23068672, 0), (5121245184, 5767168, 28835840, 0), (5715263488, 5767168, 34603008, 0), (5721030656, 5767168, 40370176, 0), (5709496320, 5767168, 46137344, 0), (5819072512, 5767168, 51904512, 0), (5824839680, 5767168, 57671680, 0), (5813305344, 5767168, 63438848, 0), (5836374016, 5767168, 69206016, 0), (5842141184, 5767168, 74973184, 0), (5830606848, 5767168, 80740352, 0), (5317328896, 5767168, 86507520, 0), (5323096064, 5767168, 92274688, 0), (5311561728, 5767168, 98041856, 0), (5870977024, 5767168, 103809024, 0), (5876744192, 5767168, 109576192, 0), (5865209856, 5767168, 115343360, 0), (5888278528, 5767168, 121110528, 0), (5894045696, 5767168, 126877696, 0), (5882511360, 5767168, 132644864, 0), (5369233408, 5767168, 138412032, 0), (5375000576, 5767168, 144179200, 0), (5363466240, 5767168, 149946368, 0), (5403836416, 5767168, 155713536, 0), (5409603584, 5767168, 161480704, 0), (5398069248, 5767168, 167247872, 0), (5421137920, 5767168, 173015040, 0), (5426905088, 5767168, 178782208, 0), (5415370752, 5767168, 184549376, 0), (5992087552, 5767168, 190316544, 0), (5997854720, 5767168, 196083712, 0), (5986320384, 5767168, 201850880, 0), (5507645440, 5767168, 207618048, 0), (5513412608, 5767168, 213385216, 0), (5501878272, 5767168, 219152384, 0), (6113198080, 5767168, 224919552, 0), (6118965248, 5767168, 230686720, 0), (6107430912, 5767168, 236453888, 0), (5576851456, 5767168, 242221056, 0), (5582618624, 5767168, 247988224, 0), (5571084288, 5767168, 253755392, 0), (5611454464, 5767168, 259522560, 0), (5617221632, 5767168, 265289728, 0), (5605687296, 5767168, 271056896, 0)], 2: [(5646057472, 5767168, 0, 0), (5651824640, 5767168, 5767168, 0), (5640290304, 5767168, 11534336, 0), (5732564992, 5767168, 17301504, 0), (5738332160, 5767168, 23068672, 0), (5726797824, 5767168, 28835840, 0), (5213519872, 5767168, 34603008, 0), (5219287040, 5767168, 40370176, 0), (5207752704, 5767168, 46137344, 0), (5784469504, 5767168, 51904512, 0), (5790236672, 5767168, 57671680, 0), (5778702336, 5767168, 63438848, 0), (5248122880, 5767168, 69206016, 0), (5253890048, 5767168, 74973184, 0), (5242355712, 5767168, 80740352, 0), (5282725888, 5767168, 86507520, 0), (5288493056, 5767168, 92274688, 0), (5276958720, 5767168, 98041856, 0), (6026690560, 5767168, 103809024, 0), (6032457728, 5767168, 109576192, 0), (6020923392, 5767168, 115343360, 0), (6009389056, 5767168, 121110528, 0), (6015156224, 5767168, 126877696, 0), (6003621888, 5767168, 132644864, 0), (5455740928, 5767168, 138412032, 0), (5461508096, 5767168, 144179200, 0), (5449973760, 5767168, 149946368, 0), (5490343936, 5767168, 155713536, 0), (5496111104, 5767168, 161480704, 0), (5484576768, 5767168, 167247872, 0), (6061293568, 5767168, 173015040, 0), (6067060736, 5767168, 178782208, 0), (6055526400, 5767168, 184549376, 0), (6078595072, 5767168, 190316544, 0), (6084362240, 5767168, 196083712, 0), (6072827904, 5767168, 201850880, 0), (5438439424, 5767168, 207618048, 0), (5444206592, 5767168, 213385216, 0), (5432672256, 5767168, 219152384, 0), (5559549952, 5767168, 224919552, 0), (5565317120, 5767168, 230686720, 0), (5553782784, 5767168, 236453888, 0), (6147801088, 5767168, 242221056, 0), (6153568256, 5767168, 247988224, 0), (6142033920, 5767168, 253755392, 0), (6165102592, 5767168, 259522560, 0), (6170869760, 5767168, 265289728, 0), (6159335424, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a5b00d72250>, 2: <capsule object NULL at 0x7a4f2c2c2400>}
DEBUG 01-15 16:10:43.247051.247051 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.247166.247166 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.247345.247345 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 134321f3-d191-4f00-8cbf-8b81cf891931
DEBUG 01-15 16:10:43.248490.248490 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:43.248479.248479 client.py:127] Model loaded
DEBUG 01-15 16:10:43.248547.248547 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.248377.248377 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:43.248512.248512 cuda_h.py:19] end restore2model cost 0.00032806396484375 seconds
DEBUG 01-15 16:10:43.248613.248613 cuda_h.py:19] end sllm_worker_task cost 0.01133871078491211 seconds
DEBUG 01-15 16:10:43.249678.249678 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.250768.250768 cuda_h.py:19] end move_flatidxs cost 0.0008485317230224609 seconds
DEBUG 01-15 16:10:43.250644.250644 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:43.250861.250861 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 134321f3-d191-4f00-8cbf-8b81cf891931
DEBUG 01-15 16:10:43.250572.250572 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005522251129150391 seconds
DEBUG 01-15 16:10:43.250157.250157 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.253702.253702 cuda_h.py:19] end restore2model cost 0.002568960189819336 seconds
DEBUG 01-15 16:10:43.253982.253982 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008332014083862305 seconds
DEBUG 01-15 16:10:43.253493.253493 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.253609.253609 cuda_h.py:19] end gpu_sexperts cost 0.0002651214599609375 seconds
DEBUG 01-15 16:10:43.254339.254339 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.254254.254254 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:10:43.254997.254997 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.254938.254938 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.254230.254230 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000782012939453125 seconds
DEBUG 01-15 16:10:43.254735.254735 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.255868.255868 cuda_h.py:19] end gpu_group_list cost 0.00017523765563964844 seconds
DEBUG 01-15 16:10:43.255469.255469 cuda_h.py:19] end group_tensors cost 0.005029916763305664 seconds
DEBUG 01-15 16:10:43.255777.255777 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.255946.255946 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.257734.257734 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014872550964355469 seconds
DEBUG 01-15 16:10:43.257751.257751 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.257192.257192 cuda_h.py:19] end gpu_group_list cost 0.0002868175506591797 seconds
DEBUG 01-15 16:10:43.258149.258149 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.258066.258066 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 134321f3-d191-4f00-8cbf-8b81cf891931
DEBUG 01-15 16:10:43.260799.260799 cuda_h.py:19] end group pad cost 0.004176616668701172 seconds
DEBUG 01-15 16:10:43.260973.260973 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.277448.277448 client.py:127] Model loaded
DEBUG 01-15 16:10:43.277286.277286 cuda_h.py:19] end wait_experts_multi_device cost 0.01903080940246582 seconds
DEBUG 01-15 16:10:43.278827.278827 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.281476.281476 cuda_h.py:19] end group_einsum cost 0.021070241928100586 seconds
DEBUG 01-15 16:10:43.281170.281170 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.285836.285836 cuda_h.py:19] end get_outputs_cpu1 cost 0.004158973693847656 seconds
DEBUG 01-15 16:10:43.286429.286429 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03750729560852051 seconds
DEBUG 01-15 16:10:43.286763.286763 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008679628372192383 seconds
DEBUG 01-15 16:10:43.286713.286713 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.288449.288449 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.288640.288640 cuda_h.py:19] end index_scatter cost 8.893013000488281e-05 seconds
DEBUG 01-15 16:10:43.288468.288468 cuda_h.py:19] end cpuoutputsdeal cost 0.0017430782318115234 seconds
DEBUG 01-15 16:10:43.288292.288292 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.288863.288863 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.289716.289716 cuda_h.py:19] end gpu_group_tensor cost 0.00014066696166992188 seconds
DEBUG 01-15 16:10:43.289764.289764 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.289405.289405 cuda_h.py:19] end gpu_group_tensor cost 0.00012612342834472656 seconds
DEBUG 01-15 16:10:43.289779.289779 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.289068.289068 cuda_h.py:19] end gpu_group_einsum cost 0.0004832744598388672 seconds
DEBUG 01-15 16:10:43.289278.289278 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.290495.290495 cuda_h.py:19] end gpu_group_einsum cost 0.0004937648773193359 seconds
DEBUG 01-15 16:10:43.290148.290148 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.290066.290066 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.290928.290928 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002033710479736328 seconds
DEBUG 01-15 16:10:43.291791.291791 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.291734.291734 cuda_h.py:19] end concat_expert_out cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:10:43.291432.291432 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.291355.291355 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:43.291144.291144 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008206367492675781 seconds
DEBUG 01-15 16:10:43.291743.291743 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.291725.291725 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.291419.291419 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012969970703125 seconds
DEBUG 01-15 16:10:43.291983.291983 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.291522.291522 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:43.291312.291312 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.292090.292090 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:43.292853.292853 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046706199645996094 seconds
DEBUG 01-15 16:10:43.292372.292372 cuda_h.py:19] end gpu_experts_multi_device cost 0.038068532943725586 seconds
DEBUG 01-15 16:10:43.292089.292089 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.04975605010986328 seconds
DEBUG 01-15 16:10:43.292533.292533 cuda_h.py:19] end prefill_layer cost 0.05536651611328125 seconds
DEBUG 01-15 16:10:43.292767.292767 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:10:43.292470.292470 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.292126.292126 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:10:43.292498.292498 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:43.292254.292254 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:43.292018.292018 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 4.363059997558594e-05 seconds
DEBUG 01-15 16:10:43.292351.292351 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 8.0108642578125e-05 seconds
DEBUG 01-15 16:10:43.292716.292716 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.293361.293361 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.293576.293576 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.293381.293381 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.293807.293807 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.293339.293339 cuda_h.py:19] end allocate_cuda_memory cost 0.0002391338348388672 seconds
DEBUG 01-15 16:10:43.293230.293230 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.293199.293199 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.293532.293532 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.293110.293110 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5e2d440c-4106-4bb4-9c10-f3d8a200077e
DEBUG 01-15 16:10:43.293312.293312 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.294725.294725 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.295701.295701 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5e2d440c-4106-4bb4-9c10-f3d8a200077e
DEBUG 01-15 16:10:43.295034.295034 cuda_h.py:19] end load_into_gpu_async cost 0.002020120620727539 seconds
DEBUG 01-15 16:10:43.295512.295512 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.295411.295411 cuda_h.py:19] end restore_tensors2 cost 8.559226989746094e-05 seconds
DEBUG 01-15 16:10:43.295234.295234 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002691030502319336 seconds
INFO 01-15 16:10:43.296037.296037 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5e2d440c-4106-4bb4-9c10-f3d8a200077e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.297960.297960 cuda_h.py:19] end self_attn cost 0.0031943321228027344 seconds
DEBUG 01-15 16:10:43.297925.297925 cuda_h.py:19] end iln_self_attn_paln cost 0.0048160552978515625 seconds
DEBUG 01-15 16:10:43.297708.297708 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:10:43.297232.297232 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.298879.298879 cuda_h.py:19] end gate cost 0.0006835460662841797 seconds
DEBUG 01-15 16:10:43.298622.298622 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.299166.299166 lmp.py:1912] 
DEBUG 01-15 16:10:43.299166.299166 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.299260.299260 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.299201.299201 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.299281.299281 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.299169.299169 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.299912.299912 lmp.py:1917] 
DEBUG 01-15 16:10:43.299912.299912 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.299131.299131 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.299264.299264 lmp.py:1935]   Expert 14 |     62 | CPU
DEBUG 01-15 16:10:43.299199.299199 lmp.py:1935]   Expert 57 |     73 | CPU
DEBUG 01-15 16:10:43.299372.299372 lmp.py:1935]   Expert 13 |     75 | CPU
DEBUG 01-15 16:10:43.299684.299684 lmp.py:1935]   Expert 26 |     81 | CPU
DEBUG 01-15 16:10:43.299373.299373 lmp.py:1935]   Expert 31 |     90 | CPU
DEBUG 01-15 16:10:43.299824.299824 lmp.py:1935]   Expert 54 |     91 | CPU
DEBUG 01-15 16:10:43.299275.299275 lmp.py:1935]   Expert 45 |     93 | CPU
DEBUG 01-15 16:10:43.299964.299964 lmp.py:1935]   Expert 11 |     94 | CPU
DEBUG 01-15 16:10:43.299892.299892 lmp.py:1935]   Expert 58 |    103 | CPU
DEBUG 01-15 16:10:43.299257.299257 lmp.py:1935]   Expert 30 |    106 | CPU
DEBUG 01-15 16:10:43.299092.299092 lmp.py:1935]   Expert 51 |    109 | CPU
DEBUG 01-15 16:10:43.299689.299689 lmp.py:1935]   Expert 36 |    112 | CPU
DEBUG 01-15 16:10:43.299808.299808 lmp.py:1935]   Expert 10 |    114 | CPU
DEBUG 01-15 16:10:43.299074.299074 lmp.py:1935]   Expert 32 |    114 | CPU
DEBUG 01-15 16:10:43.299816.299816 lmp.py:1935]   Expert 20 |    128 | CPU
DEBUG 01-15 16:10:43.299698.299698 lmp.py:1935]   Expert  8 |    135 | CPU
DEBUG 01-15 16:10:43.299864.299864 lmp.py:1935]   Expert  4 |    137 | CPU
DEBUG 01-15 16:10:43.299030.299030 lmp.py:1935]   Expert 63 |    138 | CPU
DEBUG 01-15 16:10:43.299673.299673 lmp.py:1935]   Expert 53 |    140 | CPU
DEBUG 01-15 16:10:43.299078.299078 lmp.py:1935]   Expert 34 |    144 | CPU
DEBUG 01-15 16:10:43.299482.299482 lmp.py:1935]   Expert 61 |    144 | CPU
DEBUG 01-15 16:10:43.299887.299887 lmp.py:1935]   Expert 47 |    148 | CPU
DEBUG 01-15 16:10:43.299483.299483 lmp.py:1935]   Expert 16 |    149 | CPU
DEBUG 01-15 16:10:43.299842.299842 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:10:43.299723.299723 lmp.py:1935]   Expert 60 |    159 | CPU
DEBUG 01-15 16:10:43.299843.299843 lmp.py:1935]   Expert 17 |    160 | CPU
DEBUG 01-15 16:10:43.299009.299009 lmp.py:1935]   Expert 42 |    160 | CPU
DEBUG 01-15 16:10:43.299652.299652 lmp.py:1935]   Expert 29 |    170 | CPU
DEBUG 01-15 16:10:43.299295.299295 lmp.py:1935]   Expert 44 |    172 | CPU
DEBUG 01-15 16:10:43.299699.299699 lmp.py:1935]   Expert  7 |    175 | CPU
DEBUG 01-15 16:10:43.299342.299342 lmp.py:1935]   Expert 27 |    175 | CPU
DEBUG 01-15 16:10:43.299747.299747 lmp.py:1935]   Expert 41 |    179 | CPU
DEBUG 01-15 16:10:43.299820.299820 lmp.py:1935]   Expert 48 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.299894.299894 lmp.py:1935]   Expert  9 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.299729.299729 lmp.py:1935]   Expert 56 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.299326.299326 lmp.py:1935]   Expert  3 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.299922.299922 lmp.py:1935]   Expert  2 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.299280.299280 lmp.py:1935]   Expert 15 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.299639.299639 lmp.py:1935]   Expert 24 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.299474.299474 lmp.py:1935]   Expert  0 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.299594.299594 lmp.py:1935]   Expert 18 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300667.300667 lmp.py:1935]   Expert 55 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300979.300979 lmp.py:1935]   Expert 40 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300768.300768 lmp.py:1935]   Expert 22 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300556.300556 lmp.py:1935]   Expert 38 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300392.300392 lmp.py:1935]   Expert 23 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300750.300750 lmp.py:1935]   Expert  6 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300108.300108 lmp.py:1935]   Expert 37 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300705.300705 lmp.py:1935]   Expert 46 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300063.300063 lmp.py:1935]   Expert 19 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300660.300660 lmp.py:1935]   Expert 39 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300733.300733 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300091.300091 lmp.py:1935]   Expert 12 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300072.300072 lmp.py:1935]   Expert 50 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300338.300338 lmp.py:1935]   Expert 62 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300126.300126 lmp.py:1935]   Expert 21 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300690.300690 lmp.py:1935]   Expert 35 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300810.300810 lmp.py:1935]   Expert 49 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300930.300930 lmp.py:1935]   Expert 52 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300573.300573 lmp.py:1935]   Expert 33 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300454.300454 lmp.py:1935]   Expert  1 |    348 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300727.300727 lmp.py:1935]   Expert  5 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300846.300846 lmp.py:1935]   Expert 43 |    438 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.300966.300966 lmp.py:1935]   Expert 59 |    586 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.300417.300417 lmp.py:1937] 
DEBUG 01-15 16:10:43.300417.300417 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.300014.300014 lmp.py:1938]   CPU:   4089 tokens
DEBUG 01-15 16:10:43.300326.300326 lmp.py:1942]   cuda:1:   4131 tokens (16 experts)
DEBUG 01-15 16:10:43.300684.300684 lmp.py:1942]   cuda:2:   4068 tokens (16 experts)
DEBUG 01-15 16:10:43.300089.300089 lmp.py:1943]   Total GPU:   8199 tokens
DEBUG 01-15 16:10:43.300970.300970 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.300970.300970 lmp.py:1944] 
DEBUG 01-15 16:10:43.300573.300573 cuda_h.py:19] end experts_map_get cost 0.0018634796142578125 seconds
DEBUG 01-15 16:10:43.300192.300192 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.300802.300802 lmp.py:1953] 
DEBUG 01-15 16:10:43.300802.300802 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.300393.300393 cuda_h.py:19] end cpu_experts_submit cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:43.300135.300135 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.300104.300104 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.303730.303730 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.2.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.2.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.2.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.3.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.3.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.3.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.5.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.5.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.5.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.37.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.37.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.37.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.38.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.38.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.38.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.43.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.43.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.43.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.56.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.56.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.56.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.25.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.25.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.25.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6787956736, 5767168, 17301504, 0), (6793723904, 5767168, 23068672, 0), (6782189568, 5767168, 28835840, 0), (6286213120, 5767168, 34603008, 0), (6291980288, 5767168, 40370176, 0), (6280445952, 5767168, 46137344, 0), (6857162752, 5767168, 51904512, 0), (6862929920, 5767168, 57671680, 0), (6851395584, 5767168, 63438848, 0), (6874464256, 5767168, 69206016, 0), (6880231424, 5767168, 74973184, 0), (6868697088, 5767168, 80740352, 0), (6338117632, 5767168, 86507520, 0), (6343884800, 5767168, 92274688, 0), (6332350464, 5767168, 98041856, 0), (6390022144, 5767168, 103809024, 0), (6395789312, 5767168, 109576192, 0), (6384254976, 5767168, 115343360, 0), (6978273280, 5767168, 121110528, 0), (6984040448, 5767168, 126877696, 0), (6972506112, 5767168, 132644864, 0), (6441926656, 5767168, 138412032, 0), (6447693824, 5767168, 144179200, 0), (6436159488, 5767168, 149946368, 0), (7012876288, 5767168, 155713536, 0), (7018643456, 5767168, 161480704, 0), (7007109120, 5767168, 167247872, 0), (6493831168, 5767168, 173015040, 0), (6499598336, 5767168, 178782208, 0), (6488064000, 5767168, 184549376, 0), (7082082304, 5767168, 190316544, 0), (7087849472, 5767168, 196083712, 0), (7076315136, 5767168, 201850880, 0), (6563037184, 5767168, 207618048, 0), (6568804352, 5767168, 213385216, 0), (6557270016, 5767168, 219152384, 0), (6597640192, 5767168, 224919552, 0), (6603407360, 5767168, 230686720, 0), (6591873024, 5767168, 236453888, 0), (7203192832, 5767168, 242221056, 0), (7208960000, 5767168, 247988224, 0), (7197425664, 5767168, 253755392, 0), (7255097344, 5767168, 259522560, 0), (7260864512, 5767168, 265289728, 0), (7249330176, 5767168, 271056896, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6753353728, 5767168, 17301504, 0), (6759120896, 5767168, 23068672, 0), (6747586560, 5767168, 28835840, 0), (6217007104, 5767168, 34603008, 0), (6222774272, 5767168, 40370176, 0), (6211239936, 5767168, 46137344, 0), (6234308608, 5767168, 51904512, 0), (6240075776, 5767168, 57671680, 0), (6228541440, 5767168, 63438848, 0), (6268911616, 5767168, 69206016, 0), (6274678784, 5767168, 74973184, 0), (6263144448, 5767168, 80740352, 0), (6822559744, 5767168, 86507520, 0), (6828326912, 5767168, 92274688, 0), (6816792576, 5767168, 98041856, 0), (6839861248, 5767168, 103809024, 0), (6845628416, 5767168, 109576192, 0), (6834094080, 5767168, 115343360, 0), (6926368768, 5767168, 121110528, 0), (6932135936, 5767168, 126877696, 0), (6920601600, 5767168, 132644864, 0), (7030177792, 5767168, 138412032, 0), (7035944960, 5767168, 144179200, 0), (7024410624, 5767168, 149946368, 0), (7047479296, 5767168, 155713536, 0), (7053246464, 5767168, 161480704, 0), (7041712128, 5767168, 167247872, 0), (6511132672, 5767168, 173015040, 0), (6516899840, 5767168, 178782208, 0), (6505365504, 5767168, 184549376, 0), (6545735680, 5767168, 190316544, 0), (6551502848, 5767168, 196083712, 0), (6539968512, 5767168, 201850880, 0), (7133986816, 5767168, 207618048, 0), (7139753984, 5767168, 213385216, 0), (7128219648, 5767168, 219152384, 0), (6580338688, 5767168, 224919552, 0), (6586105856, 5767168, 230686720, 0), (6574571520, 5767168, 236453888, 0), (7151288320, 5767168, 242221056, 0), (7157055488, 5767168, 247988224, 0), (7145521152, 5767168, 253755392, 0), (6614941696, 5767168, 259522560, 0), (6620708864, 5767168, 265289728, 0), (6609174528, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c0a6af0>, 2: <capsule object NULL at 0x7a4f2c2c2100>}
DEBUG 01-15 16:10:43.303502.303502 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.303107.303107 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.303432.303432 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 21997811-c1bf-4258-a54c-bb232e4a776f
DEBUG 01-15 16:10:43.303207.303207 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:43.303493.303493 client.py:127] Model loaded
DEBUG 01-15 16:10:43.304682.304682 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.304882.304882 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:43.304025.304025 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.304580.304580 cuda_h.py:19] end restore2model cost 0.0006325244903564453 seconds
DEBUG 01-15 16:10:43.304066.304066 cuda_h.py:19] end sllm_worker_task cost 0.011572599411010742 seconds
DEBUG 01-15 16:10:43.305962.305962 cuda_h.py:19] end move_flatidxs cost 0.0008385181427001953 seconds
DEBUG 01-15 16:10:43.305076.305076 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:43.305997.305997 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 21997811-c1bf-4258-a54c-bb232e4a776f
DEBUG 01-15 16:10:43.306958.306958 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005515098571777344 seconds
DEBUG 01-15 16:10:43.306206.306206 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.308518.308518 cuda_h.py:19] end restore2model cost 0.002543926239013672 seconds
DEBUG 01-15 16:10:43.309314.309314 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008291482925415039 seconds
DEBUG 01-15 16:10:43.309633.309633 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.309941.309941 cuda_h.py:19] end gpu_sexperts cost 0.0002655982971191406 seconds
DEBUG 01-15 16:10:43.309386.309386 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.309255.309255 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:10:43.309951.309951 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.309939.309939 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.310332.310332 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008211135864257812 seconds
DEBUG 01-15 16:10:43.310851.310851 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.310083.310083 cuda_h.py:19] end gpu_group_list cost 0.00017762184143066406 seconds
DEBUG 01-15 16:10:43.311425.311425 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.311573.311573 cuda_h.py:19] end group_tensors cost 0.005486249923706055 seconds
DEBUG 01-15 16:10:43.311850.311850 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.312761.312761 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0014240741729736328 seconds
DEBUG 01-15 16:10:43.313944.313944 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.313233.313233 cuda_h.py:19] end gpu_group_list cost 0.00027751922607421875 seconds
DEBUG 01-15 16:10:43.314654.314654 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.314862.314862 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 21997811-c1bf-4258-a54c-bb232e4a776f
DEBUG 01-15 16:10:43.315438.315438 cuda_h.py:19] end group pad cost 0.004061698913574219 seconds
DEBUG 01-15 16:10:43.315904.315904 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.332465.332465 client.py:127] Model loaded
DEBUG 01-15 16:10:43.333727.333727 cuda_h.py:19] end wait_experts_multi_device cost 0.018856287002563477 seconds
DEBUG 01-15 16:10:43.333193.333193 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.336364.336364 cuda_h.py:19] end group_einsum cost 0.019977092742919922 seconds
DEBUG 01-15 16:10:43.336065.336065 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.340143.340143 cuda_h.py:19] end get_outputs_cpu1 cost 0.003996372222900391 seconds
DEBUG 01-15 16:10:43.341054.341054 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.036916494369506836 seconds
DEBUG 01-15 16:10:43.341120.341120 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008216619491577148 seconds
DEBUG 01-15 16:10:43.341164.341164 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.342985.342985 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.343554.343554 cuda_h.py:19] end index_scatter cost 9.250640869140625e-05 seconds
DEBUG 01-15 16:10:43.343591.343591 cuda_h.py:19] end cpuoutputsdeal cost 0.0016431808471679688 seconds
DEBUG 01-15 16:10:43.343746.343746 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.343363.343363 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.343878.343878 cuda_h.py:19] end gpu_group_tensor cost 0.0001380443572998047 seconds
DEBUG 01-15 16:10:43.343449.343449 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.344231.344231 cuda_h.py:19] end gpu_group_tensor cost 0.0003612041473388672 seconds
DEBUG 01-15 16:10:43.344023.344023 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.345689.345689 cuda_h.py:19] end gpu_group_einsum cost 0.0008587837219238281 seconds
DEBUG 01-15 16:10:43.345641.345641 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.345569.345569 cuda_h.py:19] end gpu_group_einsum cost 0.00035381317138671875 seconds
DEBUG 01-15 16:10:43.345042.345042 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.345986.345986 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.346635.346635 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001614093780517578 seconds
DEBUG 01-15 16:10:43.346868.346868 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.346639.346639 cuda_h.py:19] end concat_expert_out cost 4.649162292480469e-05 seconds
DEBUG 01-15 16:10:43.346859.346859 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.346113.346113 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:43.346963.346963 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007367134094238281 seconds
DEBUG 01-15 16:10:43.346323.346323 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.346590.346590 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.346854.346854 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012755393981933594 seconds
DEBUG 01-15 16:10:43.346464.346464 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.347241.347241 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:43.347846.347846 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.347577.347577 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:43.347148.347148 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046825408935546875 seconds
DEBUG 01-15 16:10:43.347144.347144 cuda_h.py:19] end gpu_experts_multi_device cost 0.03775954246520996 seconds
DEBUG 01-15 16:10:43.347623.347623 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.04944920539855957 seconds
DEBUG 01-15 16:10:43.347589.347589 cuda_h.py:19] end prefill_layer cost 0.054937124252319336 seconds
DEBUG 01-15 16:10:43.347763.347763 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:10:43.347942.347942 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.347361.347361 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:10:43.347494.347494 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:43.347343.347343 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:43.347524.347524 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 3.62396240234375e-05 seconds
DEBUG 01-15 16:10:43.347518.347518 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 6.914138793945312e-05 seconds
DEBUG 01-15 16:10:43.347883.347883 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.348415.348415 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.348888.348888 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.348965.348965 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.348093.348093 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.348598.348598 cuda_h.py:19] end allocate_cuda_memory cost 0.00022840499877929688 seconds
DEBUG 01-15 16:10:43.348568.348568 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.348953.348953 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.348068.348068 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.348208.348208 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, aedab1b9-94bb-434f-8f85-36bc3af82cad
DEBUG 01-15 16:10:43.348072.348072 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.349639.349639 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.350943.350943 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, aedab1b9-94bb-434f-8f85-36bc3af82cad
DEBUG 01-15 16:10:43.350561.350561 cuda_h.py:19] end load_into_gpu_async cost 0.001535654067993164 seconds
DEBUG 01-15 16:10:43.350363.350363 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.350473.350473 cuda_h.py:19] end restore_tensors2 cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:10:43.350236.350236 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002130746841430664 seconds
INFO 01-15 16:10:43.350040.350040 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, aedab1b9-94bb-434f-8f85-36bc3af82cad
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.352261.352261 cuda_h.py:19] end self_attn cost 0.0036394596099853516 seconds
DEBUG 01-15 16:10:43.353424.353424 cuda_h.py:19] end iln_self_attn_paln cost 0.0052471160888671875 seconds
DEBUG 01-15 16:10:43.353399.353399 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:10:43.353116.353116 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.354607.354607 cuda_h.py:19] end gate cost 0.0007810592651367188 seconds
DEBUG 01-15 16:10:43.354059.354059 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.354632.354632 lmp.py:1912] 
DEBUG 01-15 16:10:43.354632.354632 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.354057.354057 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.354899.354899 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.354165.354165 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.354000.354000 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.354643.354643 lmp.py:1917] 
DEBUG 01-15 16:10:43.354643.354643 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.354478.354478 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.354366.354366 lmp.py:1935]   Expert 34 |     24 | CPU
DEBUG 01-15 16:10:43.354247.354247 lmp.py:1935]   Expert 45 |     65 | CPU
DEBUG 01-15 16:10:43.354413.354413 lmp.py:1935]   Expert 22 |     74 | CPU
DEBUG 01-15 16:10:43.354103.354103 lmp.py:1935]   Expert 57 |     75 | CPU
DEBUG 01-15 16:10:43.354031.354031 lmp.py:1935]   Expert 17 |     95 | CPU
DEBUG 01-15 16:10:43.354958.354958 lmp.py:1935]   Expert  4 |    100 | CPU
DEBUG 01-15 16:10:43.354886.354886 lmp.py:1935]   Expert 15 |    100 | CPU
DEBUG 01-15 16:10:43.354814.354814 lmp.py:1935]   Expert 28 |    106 | CPU
DEBUG 01-15 16:10:43.354218.354218 lmp.py:1935]   Expert 32 |    112 | CPU
DEBUG 01-15 16:10:43.354669.354669 lmp.py:1935]   Expert 60 |    113 | CPU
DEBUG 01-15 16:10:43.354835.354835 lmp.py:1935]   Expert 36 |    124 | CPU
DEBUG 01-15 16:10:43.354578.354578 lmp.py:1935]   Expert 16 |    126 | CPU
DEBUG 01-15 16:10:43.354188.354188 lmp.py:1935]   Expert 14 |    127 | CPU
DEBUG 01-15 16:10:43.354275.354275 lmp.py:1935]   Expert 12 |    128 | CPU
DEBUG 01-15 16:10:43.354156.354156 lmp.py:1935]   Expert 52 |    130 | CPU
DEBUG 01-15 16:10:43.354846.354846 lmp.py:1935]   Expert 25 |    132 | CPU
DEBUG 01-15 16:10:43.354058.354058 lmp.py:1935]   Expert  8 |    134 | CPU
DEBUG 01-15 16:10:43.355747.355747 lmp.py:1935]   Expert  2 |    139 | CPU
DEBUG 01-15 16:10:43.355437.355437 lmp.py:1935]   Expert 35 |    142 | CPU
DEBUG 01-15 16:10:43.355888.355888 lmp.py:1935]   Expert  5 |    146 | CPU
DEBUG 01-15 16:10:43.355577.355577 lmp.py:1935]   Expert 30 |    152 | CPU
DEBUG 01-15 16:10:43.355789.355789 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:10:43.355479.355479 lmp.py:1935]   Expert 39 |    156 | CPU
DEBUG 01-15 16:10:43.355691.355691 lmp.py:1935]   Expert 61 |    157 | CPU
DEBUG 01-15 16:10:43.355096.355096 lmp.py:1935]   Expert  0 |    159 | CPU
DEBUG 01-15 16:10:43.355023.355023 lmp.py:1935]   Expert 13 |    170 | CPU
DEBUG 01-15 16:10:43.355666.355666 lmp.py:1935]   Expert  3 |    171 | CPU
DEBUG 01-15 16:10:43.355309.355309 lmp.py:1935]   Expert 42 |    171 | CPU
DEBUG 01-15 16:10:43.355999.355999 lmp.py:1935]   Expert 31 |    172 | CPU
DEBUG 01-15 16:10:43.355211.355211 lmp.py:1935]   Expert 41 |    175 | CPU
DEBUG 01-15 16:10:43.355662.355662 lmp.py:1935]   Expert 46 |    175 | CPU
DEBUG 01-15 16:10:43.355874.355874 lmp.py:1935]   Expert 44 |    176 | CPU
DEBUG 01-15 16:10:43.355471.355471 lmp.py:1935]   Expert  9 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355352.355352 lmp.py:1935]   Expert 43 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355995.355995 lmp.py:1935]   Expert 26 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355638.355638 lmp.py:1935]   Expert 18 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355281.355281 lmp.py:1935]   Expert 50 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355878.355878 lmp.py:1935]   Expert 62 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355236.355236 lmp.py:1935]   Expert 27 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355356.355356 lmp.py:1935]   Expert 51 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355191.355191 lmp.py:1935]   Expert 49 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355834.355834 lmp.py:1935]   Expert 11 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355239.355239 lmp.py:1935]   Expert 47 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355882.355882 lmp.py:1935]   Expert 19 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355286.355286 lmp.py:1935]   Expert 20 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355691.355691 lmp.py:1935]   Expert 63 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355857.355857 lmp.py:1935]   Expert 55 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355261.355261 lmp.py:1935]   Expert 56 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355904.355904 lmp.py:1935]   Expert 38 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355309.355309 lmp.py:1935]   Expert 48 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355952.355952 lmp.py:1935]   Expert  1 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355357.355357 lmp.py:1935]   Expert 10 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355238.355238 lmp.py:1935]   Expert 54 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355596.355596 lmp.py:1935]   Expert  7 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355716.355716 lmp.py:1935]   Expert 21 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355074.355074 lmp.py:1935]   Expert 33 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355194.355194 lmp.py:1935]   Expert 29 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355837.355837 lmp.py:1935]   Expert 40 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355480.355480 lmp.py:1935]   Expert 24 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355885.355885 lmp.py:1935]   Expert 59 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355289.355289 lmp.py:1935]   Expert 37 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355170.355170 lmp.py:1935]   Expert 58 |    366 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355575.355575 lmp.py:1935]   Expert  6 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.355980.355980 lmp.py:1935]   Expert 53 |    855 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.355384.355384 lmp.py:1937] 
DEBUG 01-15 16:10:43.355384.355384 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.355789.355789 lmp.py:1938]   CPU:   4181 tokens
DEBUG 01-15 16:10:43.355909.355909 lmp.py:1942]   cuda:1:   3974 tokens (15 experts)
DEBUG 01-15 16:10:43.355552.355552 lmp.py:1942]   cuda:2:   4133 tokens (17 experts)
DEBUG 01-15 16:10:43.355956.355956 lmp.py:1943]   Total GPU:   8107 tokens
DEBUG 01-15 16:10:43.355407.355407 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.355407.355407 lmp.py:1944] 
DEBUG 01-15 16:10:43.355057.355057 cuda_h.py:19] end experts_map_get cost 0.0016896724700927734 seconds
DEBUG 01-15 16:10:43.356953.356953 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.356563.356563 lmp.py:1953] 
DEBUG 01-15 16:10:43.356563.356563 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.356108.356108 cuda_h.py:19] end cpu_experts_submit cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:43.356063.356063 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.356700.356700 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.356336.356336 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.33.gate_proj.weight': 0, 'model.layers.5.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.40.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.40.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.40.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.27.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.27.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.27.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.48.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.48.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.48.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.59.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.59.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.59.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.62.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.62.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.62.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.56.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.56.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.56.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.26.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.26.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.26.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.7.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.7.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.7.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.38.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.38.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.38.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.9.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.9.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.9.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.51.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.51.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.51.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.54.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.54.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.54.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.24.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.24.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.24.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.58.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.58.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.58.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.29.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.29.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.29.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(7860649984, 5767168, 0, 0), (7866417152, 5767168, 5767168, 0), (7854882816, 5767168, 11534336, 0), (7981760512, 5767168, 17301504, 0), (7987527680, 5767168, 23068672, 0), (7975993344, 5767168, 28835840, 0), (7756840960, 5767168, 34603008, 0), (7762608128, 5767168, 40370176, 0), (7751073792, 5767168, 46137344, 0), (7462715392, 5767168, 51904512, 0), (7468482560, 5767168, 57671680, 0), (7456948224, 5767168, 63438848, 0), (7480016896, 5767168, 69206016, 0), (7485784064, 5767168, 74973184, 0), (7474249728, 5767168, 80740352, 0), (8033665024, 5767168, 86507520, 0), (8039432192, 5767168, 92274688, 0), (8027897856, 5767168, 98041856, 0), (8120172544, 5767168, 103809024, 0), (8125939712, 5767168, 109576192, 0), (8114405376, 5767168, 115343360, 0), (7601127424, 5767168, 121110528, 0), (7606894592, 5767168, 126877696, 0), (7595360256, 5767168, 132644864, 0), (7635730432, 5767168, 138412032, 0), (7641497600, 5767168, 144179200, 0), (7629963264, 5767168, 149946368, 0), (7653031936, 5767168, 155713536, 0), (7658799104, 5767168, 161480704, 0), (7647264768, 5767168, 167247872, 0), (8206680064, 5767168, 173015040, 0), (8212447232, 5767168, 178782208, 0), (8200912896, 5767168, 184549376, 0), (8241283072, 5767168, 190316544, 0), (8247050240, 5767168, 196083712, 0), (8235515904, 5767168, 201850880, 0), (8310489088, 5767168, 207618048, 0), (8316256256, 5767168, 213385216, 0), (8304721920, 5767168, 219152384, 0), (8362393600, 5767168, 224919552, 0), (8368160768, 5767168, 230686720, 0), (8356626432, 5767168, 236453888, 0), (8379695104, 5767168, 242221056, 0), (8385462272, 5767168, 247988224, 0), (8373927936, 5767168, 253755392, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (8258584576, 5767168, 17301504, 0), (8264351744, 5767168, 23068672, 0), (8252817408, 5767168, 28835840, 0), (7739539456, 5767168, 34603008, 0), (7745306624, 5767168, 40370176, 0), (7733772288, 5767168, 46137344, 0), (7929856000, 5767168, 51904512, 0), (7935623168, 5767168, 57671680, 0), (7924088832, 5767168, 63438848, 0), (7393509376, 5767168, 69206016, 0), (7399276544, 5767168, 74973184, 0), (7387742208, 5767168, 80740352, 0), (7410810880, 5767168, 86507520, 0), (7416578048, 5767168, 92274688, 0), (7405043712, 5767168, 98041856, 0), (7947157504, 5767168, 103809024, 0), (7952924672, 5767168, 109576192, 0), (7941390336, 5767168, 115343360, 0), (7445413888, 5767168, 121110528, 0), (7451181056, 5767168, 126877696, 0), (7439646720, 5767168, 132644864, 0), (8102871040, 5767168, 138412032, 0), (8108638208, 5767168, 144179200, 0), (8097103872, 5767168, 149946368, 0), (8137474048, 5767168, 155713536, 0), (8143241216, 5767168, 161480704, 0), (8131706880, 5767168, 167247872, 0), (8154775552, 5767168, 173015040, 0), (8160542720, 5767168, 178782208, 0), (8149008384, 5767168, 184549376, 0), (7618428928, 5767168, 190316544, 0), (7624196096, 5767168, 196083712, 0), (7612661760, 5767168, 201850880, 0), (8172077056, 5767168, 207618048, 0), (8177844224, 5767168, 213385216, 0), (8166309888, 5767168, 219152384, 0), (8223981568, 5767168, 224919552, 0), (8229748736, 5767168, 230686720, 0), (8218214400, 5767168, 236453888, 0), (7704936448, 5767168, 242221056, 0), (7710703616, 5767168, 247988224, 0), (7699169280, 5767168, 253755392, 0), (8293187584, 5767168, 259522560, 0), (8298954752, 5767168, 265289728, 0), (8287420416, 5767168, 271056896, 0), (7791443968, 5767168, 276824064, 0), (7797211136, 5767168, 282591232, 0), (7785676800, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e34594d20>, 2: <capsule object NULL at 0x7a4e5420a250>}
DEBUG 01-15 16:10:43.357188.357188 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.357594.357594 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.357012.357012 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 90521f4e-e77e-4bc3-b94b-6073231af834
DEBUG 01-15 16:10:43.357502.357502 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.357472.357472 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:43.357607.357607 client.py:127] Model loaded
DEBUG 01-15 16:10:43.357198.357198 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.357064.357064 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.358224.358224 cuda_h.py:19] end restore2model cost 0.0003390312194824219 seconds
DEBUG 01-15 16:10:43.358040.358040 cuda_h.py:19] end sllm_worker_task cost 0.009967565536499023 seconds
INFO 01-15 16:10:43.358604.358604 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 90521f4e-e77e-4bc3-b94b-6073231af834
DEBUG 01-15 16:10:43.358122.358122 cuda_h.py:19] end move_flatidxs cost 0.0008547306060791016 seconds
DEBUG 01-15 16:10:43.358865.358865 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.359896.359896 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0028526782989501953 seconds
DEBUG 01-15 16:10:43.359190.359190 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.361946.361946 cuda_h.py:19] end restore2model cost 0.002559185028076172 seconds
DEBUG 01-15 16:10:43.361928.361928 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005636930465698242 seconds
DEBUG 01-15 16:10:43.361485.361485 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.362998.362998 cuda_h.py:19] end gpu_sexperts cost 0.0002772808074951172 seconds
DEBUG 01-15 16:10:43.362636.362636 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.362697.362697 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:10:43.362393.362393 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.362858.362858 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.363997.363997 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007741451263427734 seconds
DEBUG 01-15 16:10:43.363463.363463 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.363867.363867 cuda_h.py:19] end gpu_group_list cost 0.00016379356384277344 seconds
DEBUG 01-15 16:10:43.364513.364513 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.365832.365832 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009438991546630859 seconds
DEBUG 01-15 16:10:43.365656.365656 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.365756.365756 cuda_h.py:19] end gpu_group_list cost 0.0001838207244873047 seconds
DEBUG 01-15 16:10:43.365491.365491 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.366566.366566 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 90521f4e-e77e-4bc3-b94b-6073231af834
DEBUG 01-15 16:10:43.369887.369887 cuda_h.py:19] end group_tensors cost 0.010677576065063477 seconds
DEBUG 01-15 16:10:43.370529.370529 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.374691.374691 cuda_h.py:19] end group pad cost 0.004000663757324219 seconds
DEBUG 01-15 16:10:43.374243.374243 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.389994.389994 client.py:127] Model loaded
DEBUG 01-15 16:10:43.389893.389893 cuda_h.py:19] end wait_experts_multi_device cost 0.023591041564941406 seconds
DEBUG 01-15 16:10:43.389136.389136 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.393203.393203 cuda_h.py:19] end group_einsum cost 0.019306421279907227 seconds
DEBUG 01-15 16:10:43.394698.394698 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.398827.398827 cuda_h.py:19] end get_outputs_cpu1 cost 0.00416111946105957 seconds
DEBUG 01-15 16:10:43.398332.398332 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04125714302062988 seconds
DEBUG 01-15 16:10:43.399300.399300 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.009583234786987305 seconds
DEBUG 01-15 16:10:43.399350.399350 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.400214.400214 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.400431.400431 cuda_h.py:19] end index_scatter cost 7.534027099609375e-05 seconds
DEBUG 01-15 16:10:43.401336.401336 cuda_h.py:19] end cpuoutputsdeal cost 0.0017049312591552734 seconds
DEBUG 01-15 16:10:43.401014.401014 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.401346.401346 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.401413.401413 cuda_h.py:19] end gpu_group_tensor cost 0.0003600120544433594 seconds
DEBUG 01-15 16:10:43.401110.401110 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.402741.402741 cuda_h.py:19] end gpu_group_tensor cost 0.0005986690521240234 seconds
DEBUG 01-15 16:10:43.402825.402825 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.403437.403437 cuda_h.py:19] end gpu_group_einsum cost 0.0008165836334228516 seconds
DEBUG 01-15 16:10:43.403249.403249 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.404487.404487 cuda_h.py:19] end gpu_group_einsum cost 0.0005002021789550781 seconds
DEBUG 01-15 16:10:43.404517.404517 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.404317.404317 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.404100.404100 cuda_h.py:19] end all_expert_outputs_slices cost 0.000213623046875 seconds
DEBUG 01-15 16:10:43.404870.404870 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.404436.404436 cuda_h.py:19] end concat_expert_out cost 6.341934204101562e-05 seconds
DEBUG 01-15 16:10:43.404610.404610 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.404249.404249 cuda_h.py:19] end index_scatter cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:43.405330.405330 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008590221405029297 seconds
DEBUG 01-15 16:10:43.405121.405121 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.405533.405533 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.405737.405737 cuda_h.py:19] end all_expert_outputs_slices cost 0.00011992454528808594 seconds
DEBUG 01-15 16:10:43.405016.405016 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.405509.405509 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:43.405776.405776 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.405315.405315 cuda_h.py:19] end index_scatter cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:43.405362.405362 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00045871734619140625 seconds
DEBUG 01-15 16:10:43.405835.405835 cuda_h.py:19] end gpu_experts_multi_device cost 0.04363131523132324 seconds
DEBUG 01-15 16:10:43.405645.405645 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.05260586738586426 seconds
DEBUG 01-15 16:10:43.406479.406479 cuda_h.py:19] end prefill_layer cost 0.058510541915893555 seconds
DEBUG 01-15 16:10:43.406037.406037 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:10:43.406170.406170 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.406589.406589 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:10:43.406437.406437 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:43.406955.406955 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:43.406520.406520 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 3.7670135498046875e-05 seconds
DEBUG 01-15 16:10:43.406276.406276 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 7.05718994140625e-05 seconds
DEBUG 01-15 16:10:43.406641.406641 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.406842.406842 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.406620.406620 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.406358.406358 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.406148.406148 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.407527.407527 cuda_h.py:19] end allocate_cuda_memory cost 0.0002071857452392578 seconds
DEBUG 01-15 16:10:43.407411.407411 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.407511.407511 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.407056.407056 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.407289.407289 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d3b494d2-46ee-4a38-aa2a-b4a839b977a2
DEBUG 01-15 16:10:43.407485.407485 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.407381.407381 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.409469.409469 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d3b494d2-46ee-4a38-aa2a-b4a839b977a2
DEBUG 01-15 16:10:43.409604.409604 cuda_h.py:19] end load_into_gpu_async cost 0.0021746158599853516 seconds
DEBUG 01-15 16:10:43.409360.409360 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.409801.409801 cuda_h.py:19] end restore_tensors2 cost 8.249282836914062e-05 seconds
DEBUG 01-15 16:10:43.409802.409802 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002751588821411133 seconds
INFO 01-15 16:10:43.409314.409314 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d3b494d2-46ee-4a38-aa2a-b4a839b977a2
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.410171.410171 cuda_h.py:19] end self_attn cost 0.0031392574310302734 seconds
DEBUG 01-15 16:10:43.411950.411950 cuda_h.py:19] end iln_self_attn_paln cost 0.0046539306640625 seconds
DEBUG 01-15 16:10:43.411925.411925 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:10:43.411841.411841 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.412925.412925 cuda_h.py:19] end gate cost 0.0006875991821289062 seconds
DEBUG 01-15 16:10:43.412569.412569 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.412726.412726 lmp.py:1912] 
DEBUG 01-15 16:10:43.412726.412726 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.412296.412296 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.412423.412423 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.412927.412927 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.412762.412762 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.412882.412882 lmp.py:1917] 
DEBUG 01-15 16:10:43.412882.412882 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.412002.412002 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.412559.412559 lmp.py:1935]   Expert  1 |     45 | CPU
DEBUG 01-15 16:10:43.412917.412917 lmp.py:1935]   Expert  7 |     59 | CPU
DEBUG 01-15 16:10:43.412845.412845 lmp.py:1935]   Expert 37 |     70 | CPU
DEBUG 01-15 16:10:43.412534.412534 lmp.py:1935]   Expert 17 |     76 | CPU
DEBUG 01-15 16:10:43.412985.412985 lmp.py:1935]   Expert 54 |     76 | CPU
DEBUG 01-15 16:10:43.412674.412674 lmp.py:1935]   Expert 18 |     85 | CPU
DEBUG 01-15 16:10:43.412887.412887 lmp.py:1935]   Expert 13 |     89 | CPU
DEBUG 01-15 16:10:43.412576.412576 lmp.py:1935]   Expert  9 |     90 | CPU
DEBUG 01-15 16:10:43.412265.412265 lmp.py:1935]   Expert 58 |    101 | CPU
DEBUG 01-15 16:10:43.412955.412955 lmp.py:1935]   Expert 22 |    102 | CPU
DEBUG 01-15 16:10:43.412644.412644 lmp.py:1935]   Expert  0 |    108 | CPU
DEBUG 01-15 16:10:43.412572.412572 lmp.py:1935]   Expert 26 |    118 | CPU
DEBUG 01-15 16:10:43.412215.412215 lmp.py:1935]   Expert 16 |    119 | CPU
DEBUG 01-15 16:10:43.412334.412334 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:10:43.412500.412500 lmp.py:1935]   Expert 59 |    128 | CPU
DEBUG 01-15 16:10:43.412713.412713 lmp.py:1935]   Expert 63 |    129 | CPU
DEBUG 01-15 16:10:43.412402.412402 lmp.py:1935]   Expert 62 |    138 | CPU
DEBUG 01-15 16:10:43.412568.412568 lmp.py:1935]   Expert 43 |    143 | CPU
DEBUG 01-15 16:10:43.412781.412781 lmp.py:1935]   Expert 28 |    145 | CPU
DEBUG 01-15 16:10:43.412961.412961 lmp.py:1935]   Expert 29 |    147 | CPU
DEBUG 01-15 16:10:43.413696.413696 lmp.py:1935]   Expert 33 |    147 | CPU
DEBUG 01-15 16:10:43.413054.413054 lmp.py:1935]   Expert  2 |    158 | CPU
DEBUG 01-15 16:10:43.413459.413459 lmp.py:1935]   Expert 51 |    164 | CPU
DEBUG 01-15 16:10:43.413387.413387 lmp.py:1935]   Expert 11 |    167 | CPU
DEBUG 01-15 16:10:43.413076.413076 lmp.py:1935]   Expert 23 |    167 | CPU
DEBUG 01-15 16:10:43.413242.413242 lmp.py:1935]   Expert 45 |    167 | CPU
DEBUG 01-15 16:10:43.413647.413647 lmp.py:1935]   Expert 55 |    167 | CPU
DEBUG 01-15 16:10:43.413813.413813 lmp.py:1935]   Expert 32 |    168 | CPU
DEBUG 01-15 16:10:43.413933.413933 lmp.py:1935]   Expert 53 |    168 | CPU
DEBUG 01-15 16:10:43.413529.413529 lmp.py:1935]   Expert  3 |    169 | CPU
DEBUG 01-15 16:10:43.413888.413888 lmp.py:1935]   Expert 40 |    169 | CPU
DEBUG 01-15 16:10:43.413292.413292 lmp.py:1935]   Expert 34 |    175 | CPU
DEBUG 01-15 16:10:43.413889.413889 lmp.py:1935]   Expert 14 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413962.413962 lmp.py:1935]   Expert 41 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413797.413797 lmp.py:1935]   Expert 52 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413679.413679 lmp.py:1935]   Expert 42 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413798.413798 lmp.py:1935]   Expert 21 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413680.413680 lmp.py:1935]   Expert 57 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413561.413561 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413681.413681 lmp.py:1935]   Expert 30 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413755.413755 lmp.py:1935]   Expert 35 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413828.413828 lmp.py:1935]   Expert  4 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413378.413378 lmp.py:1935]   Expert 12 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413498.413498 lmp.py:1935]   Expert 19 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413141.413141 lmp.py:1935]   Expert 50 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413023.413023 lmp.py:1935]   Expert 46 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413666.413666 lmp.py:1935]   Expert 24 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413785.413785 lmp.py:1935]   Expert 44 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413190.413190 lmp.py:1935]   Expert 49 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413833.413833 lmp.py:1935]   Expert  8 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413728.413728 lmp.py:1935]   Expert 38 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413132.413132 lmp.py:1935]   Expert  6 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413491.413491 lmp.py:1935]   Expert 47 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413372.413372 lmp.py:1935]   Expert 31 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413777.413777 lmp.py:1935]   Expert 61 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413612.413612 lmp.py:1935]   Expert 39 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413016.413016 lmp.py:1935]   Expert 36 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413659.413659 lmp.py:1935]   Expert  5 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413587.413587 lmp.py:1935]   Expert 27 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413991.413991 lmp.py:1935]   Expert 60 |    332 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413442.413442 lmp.py:1935]   Expert 20 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413847.413847 lmp.py:1935]   Expert 48 |    369 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413775.413775 lmp.py:1935]   Expert 25 |    399 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.413941.413941 lmp.py:1935]   Expert 56 |    556 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.413153.413153 lmp.py:1937] 
DEBUG 01-15 16:10:43.413153.413153 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.413319.413319 lmp.py:1938]   CPU:   4075 tokens
DEBUG 01-15 16:10:43.413777.413777 lmp.py:1942]   cuda:1:   4122 tokens (16 experts)
DEBUG 01-15 16:10:43.413420.413420 lmp.py:1942]   cuda:2:   4091 tokens (16 experts)
DEBUG 01-15 16:10:43.413109.413109 lmp.py:1943]   Total GPU:   8213 tokens
DEBUG 01-15 16:10:43.413845.413845 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.413845.413845 lmp.py:1944] 
DEBUG 01-15 16:10:43.413779.413779 cuda_h.py:19] end experts_map_get cost 0.0017061233520507812 seconds
DEBUG 01-15 16:10:43.413530.413530 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.414663.414663 lmp.py:1953] 
DEBUG 01-15 16:10:43.414663.414663 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.414493.414493 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:43.414712.414712 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.414634.414634 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.415377.415377 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.35.gate_proj.weight': 0, 'model.layers.6.mlp.experts.35.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.35.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.6.mlp.experts.4.gate_proj.weight': 0, 'model.layers.6.mlp.experts.4.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.4.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.57.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.57.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.57.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.48.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.48.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.48.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.50.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.50.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.50.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.52.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.52.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.52.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(9002549248, 5767168, 0, 0), (9008316416, 5767168, 5767168, 0), (8996782080, 5767168, 11534336, 0), (9019850752, 5767168, 17301504, 0), (9025617920, 5767168, 23068672, 0), (9014083584, 5767168, 28835840, 0), (9054453760, 5767168, 34603008, 0), (9060220928, 5767168, 40370176, 0), (9048686592, 5767168, 46137344, 0), (9106358272, 5767168, 51904512, 0), (9112125440, 5767168, 57671680, 0), (9100591104, 5767168, 63438848, 0), (8604614656, 5767168, 69206016, 0), (8610381824, 5767168, 74973184, 0), (8598847488, 5767168, 80740352, 0), (8639217664, 5767168, 86507520, 0), (8644984832, 5767168, 92274688, 0), (8633450496, 5767168, 98041856, 0), (9210167296, 5767168, 103809024, 0), (9215934464, 5767168, 109576192, 0), (9204400128, 5767168, 115343360, 0), (9244770304, 5767168, 121110528, 0), (9250537472, 5767168, 126877696, 0), (9239003136, 5767168, 132644864, 0), (8725725184, 5767168, 138412032, 0), (8731492352, 5767168, 144179200, 0), (8719958016, 5767168, 149946368, 0), (8743026688, 5767168, 155713536, 0), (8748793856, 5767168, 161480704, 0), (8737259520, 5767168, 167247872, 0), (8760328192, 5767168, 173015040, 0), (8766095360, 5767168, 178782208, 0), (8754561024, 5767168, 184549376, 0), (9365880832, 5767168, 190316544, 0), (9371648000, 5767168, 196083712, 0), (9360113664, 5767168, 201850880, 0), (8812232704, 5767168, 207618048, 0), (8817999872, 5767168, 213385216, 0), (8806465536, 5767168, 219152384, 0), (8864137216, 5767168, 224919552, 0), (8869904384, 5767168, 230686720, 0), (8858370048, 5767168, 236453888, 0), (9452388352, 5767168, 242221056, 0), (9458155520, 5767168, 247988224, 0), (9446621184, 5767168, 253755392, 0), (8916041728, 5767168, 259522560, 0), (8921808896, 5767168, 265289728, 0), (8910274560, 5767168, 271056896, 0)], 2: [(8466202624, 5767168, 0, 0), (8471969792, 5767168, 5767168, 0), (8460435456, 5767168, 11534336, 0), (8483504128, 5767168, 17301504, 0), (8489271296, 5767168, 23068672, 0), (8477736960, 5767168, 28835840, 0), (8500805632, 5767168, 34603008, 0), (8506572800, 5767168, 40370176, 0), (8495038464, 5767168, 46137344, 0), (9071755264, 5767168, 51904512, 0), (9077522432, 5767168, 57671680, 0), (9065988096, 5767168, 63438848, 0), (8535408640, 5767168, 69206016, 0), (8541175808, 5767168, 74973184, 0), (8529641472, 5767168, 80740352, 0), (9123659776, 5767168, 86507520, 0), (9129426944, 5767168, 92274688, 0), (9117892608, 5767168, 98041856, 0), (9158262784, 5767168, 103809024, 0), (9164029952, 5767168, 109576192, 0), (9152495616, 5767168, 115343360, 0), (9383182336, 5767168, 121110528, 0), (9388949504, 5767168, 126877696, 0), (9377415168, 5767168, 132644864, 0), (9192865792, 5767168, 138412032, 0), (9198632960, 5767168, 144179200, 0), (9187098624, 5767168, 149946368, 0), (8656519168, 5767168, 155713536, 0), (8662286336, 5767168, 161480704, 0), (8650752000, 5767168, 167247872, 0), (9227468800, 5767168, 173015040, 0), (9233235968, 5767168, 178782208, 0), (9221701632, 5767168, 184549376, 0), (9262071808, 5767168, 190316544, 0), (9267838976, 5767168, 196083712, 0), (9256304640, 5767168, 201850880, 0), (9296674816, 5767168, 207618048, 0), (9302441984, 5767168, 213385216, 0), (9290907648, 5767168, 219152384, 0), (8829534208, 5767168, 224919552, 0), (8835301376, 5767168, 230686720, 0), (8823767040, 5767168, 236453888, 0), (9435086848, 5767168, 242221056, 0), (9440854016, 5767168, 247988224, 0), (9429319680, 5767168, 253755392, 0), (8933343232, 5767168, 259522560, 0), (8939110400, 5767168, 265289728, 0), (8927576064, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e443a6e50>, 2: <capsule object NULL at 0x7a4f2c2c1e90>}
DEBUG 01-15 16:10:43.416535.416535 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.416225.416225 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.416511.416511 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e786fde6-3559-4097-94f1-bd0b922962b5
DEBUG 01-15 16:10:43.416954.416954 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.416794.416794 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:43.417927.417927 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:43.417827.417827 client.py:127] Model loaded
DEBUG 01-15 16:10:43.417464.417464 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.418039.418039 cuda_h.py:19] end restore2model cost 0.0003237724304199219 seconds
DEBUG 01-15 16:10:43.418140.418140 cuda_h.py:19] end sllm_worker_task cost 0.011294364929199219 seconds
DEBUG 01-15 16:10:43.418566.418566 cuda_h.py:19] end move_flatidxs cost 0.0011653900146484375 seconds
DEBUG 01-15 16:10:43.418611.418611 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:43.418899.418899 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e786fde6-3559-4097-94f1-bd0b922962b5
DEBUG 01-15 16:10:43.419569.419569 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0051534175872802734 seconds
DEBUG 01-15 16:10:43.419578.419578 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.421180.421180 cuda_h.py:19] end restore2model cost 0.002516031265258789 seconds
DEBUG 01-15 16:10:43.421831.421831 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007895231246948242 seconds
DEBUG 01-15 16:10:43.422150.422150 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.422981.422981 cuda_h.py:19] end gpu_sexperts cost 0.0002665519714355469 seconds
DEBUG 01-15 16:10:43.422427.422427 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.422819.422819 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:43.422515.422515 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.422787.422787 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.423100.423100 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007977485656738281 seconds
DEBUG 01-15 16:10:43.423512.423512 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.423168.423168 cuda_h.py:19] end gpu_group_list cost 0.00017404556274414062 seconds
DEBUG 01-15 16:10:43.424530.424530 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.425383.425383 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008876323699951172 seconds
DEBUG 01-15 16:10:43.425180.425180 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.425796.425796 cuda_h.py:19] end gpu_group_list cost 0.00017547607421875 seconds
DEBUG 01-15 16:10:43.426028.426028 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.426811.426811 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e786fde6-3559-4097-94f1-bd0b922962b5
DEBUG 01-15 16:10:43.436703.436703 cuda_h.py:19] end group_tensors cost 0.017879247665405273 seconds
DEBUG 01-15 16:10:43.437817.437817 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.441280.441280 cuda_h.py:19] end group pad cost 0.0036857128143310547 seconds
DEBUG 01-15 16:10:43.441401.441401 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.445524.445524 client.py:127] Model loaded
DEBUG 01-15 16:10:43.446604.446604 cuda_h.py:19] end wait_experts_multi_device cost 0.019909143447875977 seconds
DEBUG 01-15 16:10:43.446864.446864 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.461143.461143 cuda_h.py:19] end group_einsum cost 0.019890308380126953 seconds
DEBUG 01-15 16:10:43.461657.461657 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.465993.465993 cuda_h.py:19] end get_outputs_cpu1 cost 0.003830432891845703 seconds
DEBUG 01-15 16:10:43.466205.466205 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04921293258666992 seconds
DEBUG 01-15 16:10:43.466069.466069 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.020485877990722656 seconds
DEBUG 01-15 16:10:43.466781.466781 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.467762.467762 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.468641.468641 cuda_h.py:19] end index_scatter cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:10:43.468732.468732 cuda_h.py:19] end cpuoutputsdeal cost 0.001676797866821289 seconds
DEBUG 01-15 16:10:43.468371.468371 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.468326.468326 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.468186.468186 cuda_h.py:19] end gpu_group_tensor cost 0.00014472007751464844 seconds
DEBUG 01-15 16:10:43.468141.468141 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.468073.468073 cuda_h.py:19] end gpu_group_tensor cost 0.00012922286987304688 seconds
DEBUG 01-15 16:10:43.469170.469170 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.469340.469340 cuda_h.py:19] end gpu_group_einsum cost 0.0004832744598388672 seconds
DEBUG 01-15 16:10:43.469841.469841 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.470676.470676 cuda_h.py:19] end gpu_group_einsum cost 0.00035500526428222656 seconds
DEBUG 01-15 16:10:43.470057.470057 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.470908.470908 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.470802.470802 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016546249389648438 seconds
DEBUG 01-15 16:10:43.470466.470466 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.470243.470243 cuda_h.py:19] end concat_expert_out cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:43.470563.470563 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.470493.470493 cuda_h.py:19] end index_scatter cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:10:43.471104.471104 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007593631744384766 seconds
DEBUG 01-15 16:10:43.471041.471041 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.471553.471553 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.471446.471446 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013303756713867188 seconds
DEBUG 01-15 16:10:43.471440.471440 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.471509.471509 cuda_h.py:19] end concat_expert_out cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:10:43.471875.471875 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.471183.471183 cuda_h.py:19] end index_scatter cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:43.471946.471946 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004889965057373047 seconds
DEBUG 01-15 16:10:43.471041.471041 cuda_h.py:19] end gpu_experts_multi_device cost 0.04931211471557617 seconds
DEBUG 01-15 16:10:43.471858.471858 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.06044411659240723 seconds
DEBUG 01-15 16:10:43.472527.472527 cuda_h.py:19] end prefill_layer cost 0.06578636169433594 seconds
DEBUG 01-15 16:10:43.472661.472661 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:10:43.472033.472033 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.472358.472358 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:10:43.472399.472399 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:43.472819.472819 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:43.472358.472358 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:43.472544.472544 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 8.58306884765625e-05 seconds
DEBUG 01-15 16:10:43.472055.472055 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.472713.472713 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.472955.472955 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.472309.472309 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.472075.472075 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.473839.473839 cuda_h.py:19] end allocate_cuda_memory cost 0.00024056434631347656 seconds
DEBUG 01-15 16:10:43.473571.473571 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.473910.473910 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.473455.473455 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.473641.473641 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b5969f80-f332-4923-b890-9f7025d9e8fe
DEBUG 01-15 16:10:43.473552.473552 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.473225.473225 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.474124.474124 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b5969f80-f332-4923-b890-9f7025d9e8fe
DEBUG 01-15 16:10:43.474504.474504 cuda_h.py:19] end load_into_gpu_async cost 0.00152587890625 seconds
DEBUG 01-15 16:10:43.474022.474022 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.475701.475701 cuda_h.py:19] end restore_tensors2 cost 8.296966552734375e-05 seconds
DEBUG 01-15 16:10:43.475464.475464 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021491050720214844 seconds
INFO 01-15 16:10:43.475459.475459 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b5969f80-f332-4923-b890-9f7025d9e8fe
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.477286.477286 cuda_h.py:19] end self_attn cost 0.0031328201293945312 seconds
DEBUG 01-15 16:10:43.477834.477834 cuda_h.py:19] end iln_self_attn_paln cost 0.0047380924224853516 seconds
DEBUG 01-15 16:10:43.477577.477577 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:10:43.477770.477770 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.478523.478523 cuda_h.py:19] end gate cost 0.0006861686706542969 seconds
DEBUG 01-15 16:10:43.478651.478651 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.478863.478863 lmp.py:1912] 
DEBUG 01-15 16:10:43.478863.478863 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.478103.478103 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.478998.478998 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.478793.478793 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.478443.478443 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.478855.478855 lmp.py:1917] 
DEBUG 01-15 16:10:43.478855.478855 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.478028.478028 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.478307.478307 lmp.py:1935]   Expert 50 |     44 | CPU
DEBUG 01-15 16:10:43.478718.478718 lmp.py:1935]   Expert  3 |     53 | CPU
DEBUG 01-15 16:10:43.478891.478891 lmp.py:1935]   Expert 46 |     56 | CPU
DEBUG 01-15 16:10:43.478587.478587 lmp.py:1935]   Expert  1 |     75 | CPU
DEBUG 01-15 16:10:43.479522.479522 lmp.py:1935]   Expert 29 |     87 | CPU
DEBUG 01-15 16:10:43.479648.479648 lmp.py:1935]   Expert  4 |     88 | CPU
DEBUG 01-15 16:10:43.479536.479536 lmp.py:1935]   Expert 40 |     95 | CPU
DEBUG 01-15 16:10:43.479425.479425 lmp.py:1935]   Expert 15 |     96 | CPU
DEBUG 01-15 16:10:43.479227.479227 lmp.py:1935]   Expert  8 |    111 | CPU
DEBUG 01-15 16:10:43.479161.479161 lmp.py:1935]   Expert 28 |    111 | CPU
DEBUG 01-15 16:10:43.479381.479381 lmp.py:1935]   Expert 41 |    113 | CPU
DEBUG 01-15 16:10:43.479838.479838 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:10:43.479534.479534 lmp.py:1935]   Expert 48 |    126 | CPU
DEBUG 01-15 16:10:43.479992.479992 lmp.py:1935]   Expert 27 |    127 | CPU
DEBUG 01-15 16:10:43.479211.479211 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:10:43.479907.479907 lmp.py:1935]   Expert 13 |    130 | CPU
DEBUG 01-15 16:10:43.479842.479842 lmp.py:1935]   Expert 54 |    131 | CPU
DEBUG 01-15 16:10:43.479776.479776 lmp.py:1935]   Expert  7 |    134 | CPU
DEBUG 01-15 16:10:43.479949.479949 lmp.py:1935]   Expert 51 |    137 | CPU
DEBUG 01-15 16:10:43.479599.479599 lmp.py:1935]   Expert 18 |    139 | CPU
DEBUG 01-15 16:10:43.479010.479010 lmp.py:1935]   Expert 39 |    139 | CPU
DEBUG 01-15 16:10:43.479707.479707 lmp.py:1935]   Expert 60 |    140 | CPU
DEBUG 01-15 16:10:43.479118.479118 lmp.py:1935]   Expert 14 |    147 | CPU
DEBUG 01-15 16:10:43.479291.479291 lmp.py:1935]   Expert 20 |    147 | CPU
DEBUG 01-15 16:10:43.479464.479464 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:10:43.479868.479868 lmp.py:1935]   Expert 52 |    148 | CPU
DEBUG 01-15 16:10:43.479034.479034 lmp.py:1935]   Expert 56 |    149 | CPU
DEBUG 01-15 16:10:43.479439.479439 lmp.py:1935]   Expert 55 |    153 | CPU
DEBUG 01-15 16:10:43.479082.479082 lmp.py:1935]   Expert 36 |    154 | CPU
DEBUG 01-15 16:10:43.479963.479963 lmp.py:1935]   Expert 10 |    156 | CPU
DEBUG 01-15 16:10:43.479322.479322 lmp.py:1935]   Expert 11 |    158 | CPU
DEBUG 01-15 16:10:43.479203.479203 lmp.py:1935]   Expert  5 |    160 | CPU
DEBUG 01-15 16:10:43.479707.479707 lmp.py:1935]   Expert 45 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479019.479019 lmp.py:1935]   Expert 62 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479854.479854 lmp.py:1935]   Expert 57 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479212.479212 lmp.py:1935]   Expert 44 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479047.479047 lmp.py:1935]   Expert 33 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479405.479405 lmp.py:1935]   Expert 25 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479764.479764 lmp.py:1935]   Expert 53 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479360.479360 lmp.py:1935]   Expert 58 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479957.479957 lmp.py:1935]   Expert  2 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479077.479077 lmp.py:1935]   Expert 32 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479104.479104 lmp.py:1935]   Expert 35 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479177.479177 lmp.py:1935]   Expert 31 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479489.479489 lmp.py:1935]   Expert 21 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479801.479801 lmp.py:1935]   Expert 49 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479398.479398 lmp.py:1935]   Expert 63 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479147.479147 lmp.py:1935]   Expert 17 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479744.479744 lmp.py:1935]   Expert 42 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479864.479864 lmp.py:1935]   Expert 34 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479983.479983 lmp.py:1935]   Expert 37 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479342.479342 lmp.py:1935]   Expert 59 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.479938.479938 lmp.py:1935]   Expert  0 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.479058.479058 lmp.py:1935]   Expert 22 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.480655.480655 lmp.py:1935]   Expert 19 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.480536.480536 lmp.py:1935]   Expert 24 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.480656.480656 lmp.py:1935]   Expert 61 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.480253.480253 lmp.py:1935]   Expert 30 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.480326.480326 lmp.py:1935]   Expert 47 |    321 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.480923.480923 lmp.py:1935]   Expert 38 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.480235.480235 lmp.py:1935]   Expert 26 |    377 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.480831.480831 lmp.py:1935]   Expert 12 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.480190.480190 lmp.py:1935]   Expert  9 |    679 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.480786.480786 lmp.py:1935]   Expert 23 |    701 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.480952.480952 lmp.py:1937] 
DEBUG 01-15 16:10:43.480952.480952 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.480834.480834 lmp.py:1938]   CPU:   3905 tokens
DEBUG 01-15 16:10:43.480192.480192 lmp.py:1942]   cuda:1:   4192 tokens (16 experts)
DEBUG 01-15 16:10:43.480073.480073 lmp.py:1942]   cuda:2:   4191 tokens (16 experts)
DEBUG 01-15 16:10:43.480239.480239 lmp.py:1943]   Total GPU:   8383 tokens
DEBUG 01-15 16:10:43.480406.480406 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.480406.480406 lmp.py:1944] 
DEBUG 01-15 16:10:43.480771.480771 cuda_h.py:19] end experts_map_get cost 0.0019521713256835938 seconds
DEBUG 01-15 16:10:43.480667.480667 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.480575.480575 lmp.py:1953] 
DEBUG 01-15 16:10:43.480575.480575 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.480120.480120 cuda_h.py:19] end cpu_experts_submit cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:43.480339.480339 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.480838.480838 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.482667.482667 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.32.gate_proj.weight': 0, 'model.layers.7.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.53.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.53.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.53.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.22.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.22.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.22.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.59.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.59.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.59.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.31.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.31.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.31.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.2.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.2.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.2.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.35.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.35.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.35.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.62.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.62.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.62.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.58.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.58.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.58.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(10057940992, 5767168, 0, 0), (10063708160, 5767168, 5767168, 0), (10052173824, 5767168, 11534336, 0), (10092544000, 5767168, 17301504, 0), (10098311168, 5767168, 23068672, 0), (10086776832, 5767168, 28835840, 0), (10161750016, 5767168, 34603008, 0), (10167517184, 5767168, 40370176, 0), (10155982848, 5767168, 46137344, 0), (10265559040, 5767168, 51904512, 0), (10271326208, 5767168, 57671680, 0), (10259791872, 5767168, 63438848, 0), (10282860544, 5767168, 69206016, 0), (10288627712, 5767168, 74973184, 0), (10277093376, 5767168, 80740352, 0), (9798418432, 5767168, 86507520, 0), (9804185600, 5767168, 92274688, 0), (9792651264, 5767168, 98041856, 0), (9833021440, 5767168, 103809024, 0), (9838788608, 5767168, 109576192, 0), (9827254272, 5767168, 115343360, 0), (10421272576, 5767168, 121110528, 0), (10427039744, 5767168, 126877696, 0), (10415505408, 5767168, 132644864, 0), (9884925952, 5767168, 138412032, 0), (9890693120, 5767168, 144179200, 0), (9879158784, 5767168, 149946368, 0), (9902227456, 5767168, 155713536, 0), (9907994624, 5767168, 161480704, 0), (9896460288, 5767168, 167247872, 0), (9936830464, 5767168, 173015040, 0), (9942597632, 5767168, 178782208, 0), (9931063296, 5767168, 184549376, 0), (9954131968, 5767168, 190316544, 0), (9959899136, 5767168, 196083712, 0), (9948364800, 5767168, 201850880, 0), (10525081600, 5767168, 207618048, 0), (10530848768, 5767168, 213385216, 0), (10519314432, 5767168, 219152384, 0), (10040639488, 5767168, 224919552, 0), (10046406656, 5767168, 230686720, 0), (10034872320, 5767168, 236453888, 0), (10559684608, 5767168, 242221056, 0), (10565451776, 5767168, 247988224, 0), (10553917440, 5767168, 253755392, 0), (10594287616, 5767168, 259522560, 0), (10600054784, 5767168, 265289728, 0), (10588520448, 5767168, 271056896, 0)], 2: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (10075242496, 5767168, 17301504, 0), (10081009664, 5767168, 23068672, 0), (10069475328, 5767168, 28835840, 0), (9538895872, 5767168, 34603008, 0), (9544663040, 5767168, 40370176, 0), (9533128704, 5767168, 46137344, 0), (10109845504, 5767168, 51904512, 0), (10115612672, 5767168, 57671680, 0), (10104078336, 5767168, 63438848, 0), (10144448512, 5767168, 69206016, 0), (10150215680, 5767168, 74973184, 0), (10138681344, 5767168, 80740352, 0), (9660006400, 5767168, 86507520, 0), (9665773568, 5767168, 92274688, 0), (9654239232, 5767168, 98041856, 0), (10230956032, 5767168, 103809024, 0), (10236723200, 5767168, 109576192, 0), (10225188864, 5767168, 115343360, 0), (9711910912, 5767168, 121110528, 0), (9717678080, 5767168, 126877696, 0), (9706143744, 5767168, 132644864, 0), (10317463552, 5767168, 138412032, 0), (10323230720, 5767168, 144179200, 0), (10311696384, 5767168, 149946368, 0), (10352066560, 5767168, 155713536, 0), (10357833728, 5767168, 161480704, 0), (10346299392, 5767168, 167247872, 0), (9867624448, 5767168, 173015040, 0), (9873391616, 5767168, 178782208, 0), (9861857280, 5767168, 184549376, 0), (10576986112, 5767168, 190316544, 0), (10582753280, 5767168, 196083712, 0), (10571218944, 5767168, 201850880, 0), (9919528960, 5767168, 207618048, 0), (9925296128, 5767168, 213385216, 0), (9913761792, 5767168, 219152384, 0), (10490478592, 5767168, 224919552, 0), (10496245760, 5767168, 230686720, 0), (10484711424, 5767168, 236453888, 0), (10507780096, 5767168, 242221056, 0), (10513547264, 5767168, 247988224, 0), (10502012928, 5767168, 253755392, 0), (10023337984, 5767168, 259522560, 0), (10029105152, 5767168, 265289728, 0), (10017570816, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e345952c0>, 2: <capsule object NULL at 0x7a4f2c0a72d0>}
DEBUG 01-15 16:10:43.482453.482453 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.482044.482044 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.482177.482177 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d5152059-bb16-456b-8423-d6947aeddeab
DEBUG 01-15 16:10:43.482852.482852 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:43.482363.482363 client.py:127] Model loaded
DEBUG 01-15 16:10:43.483305.483305 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.483236.483236 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:43.483801.483801 cuda_h.py:19] end restore2model cost 0.0003299713134765625 seconds
DEBUG 01-15 16:10:43.483140.483140 cuda_h.py:19] end sllm_worker_task cost 0.01059865951538086 seconds
INFO 01-15 16:10:43.483769.483769 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d5152059-bb16-456b-8423-d6947aeddeab
DEBUG 01-15 16:10:43.483850.483850 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.484982.484982 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003477811813354492 seconds
DEBUG 01-15 16:10:43.484283.484283 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.484061.484061 cuda_h.py:19] end move_flatidxs cost 0.0008819103240966797 seconds
DEBUG 01-15 16:10:43.484196.484196 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.486923.486923 cuda_h.py:19] end restore2model cost 0.002475261688232422 seconds
DEBUG 01-15 16:10:43.486474.486474 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006186962127685547 seconds
DEBUG 01-15 16:10:43.486747.486747 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.487677.487677 cuda_h.py:19] end gpu_sexperts cost 0.0002684593200683594 seconds
DEBUG 01-15 16:10:43.487076.487076 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.487330.487330 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:10:43.487026.487026 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.487298.487298 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.488730.488730 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008134841918945312 seconds
DEBUG 01-15 16:10:43.488428.488428 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.488375.488375 cuda_h.py:19] end gpu_group_list cost 0.00017881393432617188 seconds
DEBUG 01-15 16:10:43.489195.489195 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.489252.489252 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008597373962402344 seconds
DEBUG 01-15 16:10:43.489333.489333 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.490466.490466 cuda_h.py:19] end gpu_group_list cost 0.00017380714416503906 seconds
DEBUG 01-15 16:10:43.490200.490200 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.490414.490414 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d5152059-bb16-456b-8423-d6947aeddeab
DEBUG 01-15 16:10:43.490134.490134 cuda_h.py:19] end group_tensors cost 0.0062215328216552734 seconds
DEBUG 01-15 16:10:43.491123.491123 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.496234.496234 cuda_h.py:19] end group pad cost 0.004453897476196289 seconds
DEBUG 01-15 16:10:43.496825.496825 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.511121.511121 client.py:127] Model loaded
DEBUG 01-15 16:10:43.512353.512353 cuda_h.py:19] end wait_experts_multi_device cost 0.021245479583740234 seconds
DEBUG 01-15 16:10:43.512529.512529 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.515572.515572 cuda_h.py:19] end group_einsum cost 0.019039630889892578 seconds
DEBUG 01-15 16:10:43.515384.515384 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.519682.519682 cuda_h.py:19] end get_outputs_cpu1 cost 0.0036618709564208984 seconds
DEBUG 01-15 16:10:43.519840.519840 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03634500503540039 seconds
DEBUG 01-15 16:10:43.520888.520888 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00786900520324707 seconds
DEBUG 01-15 16:10:43.520362.520362 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.521992.521992 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.521720.521720 cuda_h.py:19] end index_scatter cost 9.1552734375e-05 seconds
DEBUG 01-15 16:10:43.522339.522339 cuda_h.py:19] end cpuoutputsdeal cost 0.0016629695892333984 seconds
DEBUG 01-15 16:10:43.522494.522494 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.522588.522588 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.522727.522727 cuda_h.py:19] end gpu_group_tensor cost 0.0001418590545654297 seconds
DEBUG 01-15 16:10:43.522774.522774 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.522607.522607 cuda_h.py:19] end gpu_group_tensor cost 0.00012755393981933594 seconds
DEBUG 01-15 16:10:43.522286.522286 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.523694.523694 cuda_h.py:19] end gpu_group_einsum cost 0.0004639625549316406 seconds
DEBUG 01-15 16:10:43.523281.523281 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.523500.523500 cuda_h.py:19] end gpu_group_einsum cost 0.00035834312438964844 seconds
DEBUG 01-15 16:10:43.523027.523027 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.523494.523494 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.524249.524249 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016832351684570312 seconds
DEBUG 01-15 16:10:43.524912.524912 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.524067.524067 cuda_h.py:19] end concat_expert_out cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:10:43.524056.524056 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.524271.524271 cuda_h.py:19] end index_scatter cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:10:43.524173.524173 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007638931274414062 seconds
DEBUG 01-15 16:10:43.524487.524487 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.524662.524662 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.524786.524786 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001304149627685547 seconds
DEBUG 01-15 16:10:43.524112.524112 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.524320.524320 cuda_h.py:19] end concat_expert_out cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:43.525501.525501 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.525000.525000 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:43.525194.525194 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004832744598388672 seconds
DEBUG 01-15 16:10:43.525342.525342 cuda_h.py:19] end gpu_experts_multi_device cost 0.03809690475463867 seconds
DEBUG 01-15 16:10:43.525557.525557 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.04780888557434082 seconds
DEBUG 01-15 16:10:43.525524.525524 cuda_h.py:19] end prefill_layer cost 0.053382158279418945 seconds
DEBUG 01-15 16:10:43.525149.525149 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:10:43.525090.525090 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.525985.525985 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:10:43.525880.525880 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:43.525397.525397 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:43.525247.525247 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 3.695487976074219e-05 seconds
DEBUG 01-15 16:10:43.526003.526003 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:10:43.526991.526991 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.526000.526000 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.526752.526752 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.526159.526159 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.526472.526472 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.526892.526892 cuda_h.py:19] end allocate_cuda_memory cost 0.00023698806762695312 seconds
DEBUG 01-15 16:10:43.526391.526391 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.526777.526777 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.526368.526368 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.526124.526124 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bb6ffacd-412d-418d-a2a0-59aef95c691f
DEBUG 01-15 16:10:43.526180.526180 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.527475.527475 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
INFO 01-15 16:10:43.531216.531216 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bb6ffacd-412d-418d-a2a0-59aef95c691f
DEBUG 01-15 16:10:43.531604.531604 cuda_h.py:19] end load_into_gpu_async cost 0.005129098892211914 seconds
DEBUG 01-15 16:10:43.531413.531413 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.532928.532928 cuda_h.py:19] end restore_tensors2 cost 9.918212890625e-05 seconds
DEBUG 01-15 16:10:43.532360.532360 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.005768299102783203 seconds
INFO 01-15 16:10:43.532151.532151 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bb6ffacd-412d-418d-a2a0-59aef95c691f
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.533763.533763 cuda_h.py:19] end self_attn cost 0.005970954895019531 seconds
DEBUG 01-15 16:10:43.533591.533591 cuda_h.py:19] end iln_self_attn_paln cost 0.00760650634765625 seconds
DEBUG 01-15 16:10:43.533089.533089 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:10:43.533422.533422 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.534834.534834 cuda_h.py:19] end gate cost 0.0007965564727783203 seconds
DEBUG 01-15 16:10:43.534955.534955 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.535219.535219 lmp.py:1912] 
DEBUG 01-15 16:10:43.535219.535219 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.535690.535690 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.535340.535340 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.535413.535413 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.535341.535341 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.535792.535792 lmp.py:1917] 
DEBUG 01-15 16:10:43.535792.535792 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.535720.535720 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.535416.535416 lmp.py:1935]   Expert 38 |     13 | CPU
DEBUG 01-15 16:10:43.535867.535867 lmp.py:1935]   Expert 39 |     60 | CPU
DEBUG 01-15 16:10:43.535364.535364 lmp.py:1935]   Expert  7 |     72 | CPU
DEBUG 01-15 16:10:43.535099.535099 lmp.py:1935]   Expert 30 |     74 | CPU
DEBUG 01-15 16:10:43.535073.535073 lmp.py:1935]   Expert 14 |     93 | CPU
DEBUG 01-15 16:10:43.535047.535047 lmp.py:1935]   Expert 24 |     94 | CPU
DEBUG 01-15 16:10:43.535545.535545 lmp.py:1935]   Expert 27 |     94 | CPU
DEBUG 01-15 16:10:43.535042.535042 lmp.py:1935]   Expert 36 |     98 | CPU
DEBUG 01-15 16:10:43.535539.535539 lmp.py:1935]   Expert 40 |     98 | CPU
DEBUG 01-15 16:10:43.535228.535228 lmp.py:1935]   Expert 17 |    100 | CPU
DEBUG 01-15 16:10:43.535679.535679 lmp.py:1935]   Expert 16 |    104 | CPU
DEBUG 01-15 16:10:43.535653.535653 lmp.py:1935]   Expert 32 |    107 | CPU
DEBUG 01-15 16:10:43.535104.535104 lmp.py:1935]   Expert 18 |    110 | CPU
DEBUG 01-15 16:10:43.535555.535555 lmp.py:1935]   Expert 48 |    110 | CPU
DEBUG 01-15 16:10:43.535052.535052 lmp.py:1935]   Expert  1 |    115 | CPU
DEBUG 01-15 16:10:43.535788.535788 lmp.py:1935]   Expert 12 |    115 | CPU
DEBUG 01-15 16:10:43.535524.535524 lmp.py:1935]   Expert  6 |    125 | CPU
DEBUG 01-15 16:10:43.535021.535021 lmp.py:1935]   Expert 59 |    129 | CPU
DEBUG 01-15 16:10:43.535518.535518 lmp.py:1935]   Expert 42 |    136 | CPU
DEBUG 01-15 16:10:43.535492.535492 lmp.py:1935]   Expert  0 |    141 | CPU
DEBUG 01-15 16:10:43.535751.535751 lmp.py:1935]   Expert 22 |    145 | CPU
DEBUG 01-15 16:10:43.535486.535486 lmp.py:1935]   Expert 53 |    147 | CPU
DEBUG 01-15 16:10:43.535984.535984 lmp.py:1935]   Expert 51 |    151 | CPU
DEBUG 01-15 16:10:43.535719.535719 lmp.py:1935]   Expert  8 |    161 | CPU
DEBUG 01-15 16:10:43.535077.535077 lmp.py:1935]   Expert 44 |    166 | CPU
DEBUG 01-15 16:10:43.535482.535482 lmp.py:1935]   Expert 60 |    168 | CPU
DEBUG 01-15 16:10:43.535648.535648 lmp.py:1935]   Expert 15 |    169 | CPU
DEBUG 01-15 16:10:43.535291.535291 lmp.py:1935]   Expert 29 |    171 | CPU
DEBUG 01-15 16:10:43.535980.535980 lmp.py:1935]   Expert 54 |    172 | CPU
DEBUG 01-15 16:10:43.535431.535431 lmp.py:1935]   Expert 35 |    177 | CPU
DEBUG 01-15 16:10:43.535644.535644 lmp.py:1935]   Expert 34 |    180 | CPU
DEBUG 01-15 16:10:43.535095.535095 lmp.py:1935]   Expert 33 |    183 | CPU
DEBUG 01-15 16:10:43.535214.535214 lmp.py:1935]   Expert 47 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.535573.535573 lmp.py:1935]   Expert 19 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.535454.535454 lmp.py:1935]   Expert  9 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.535859.535859 lmp.py:1935]   Expert 21 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.535455.535455 lmp.py:1935]   Expert 46 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.535052.535052 lmp.py:1935]   Expert  3 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.535887.535887 lmp.py:1935]   Expert 20 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.535245.535245 lmp.py:1935]   Expert 45 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.535127.535127 lmp.py:1935]   Expert 56 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.535770.535770 lmp.py:1935]   Expert 49 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.535174.535174 lmp.py:1935]   Expert 28 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.535579.535579 lmp.py:1935]   Expert 57 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.535983.535983 lmp.py:1935]   Expert 13 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.535865.535865 lmp.py:1935]   Expert  2 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.535269.535269 lmp.py:1935]   Expert 43 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.535674.535674 lmp.py:1935]   Expert  4 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.535555.535555 lmp.py:1935]   Expert 10 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.535675.535675 lmp.py:1935]   Expert 50 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.536795.536795 lmp.py:1935]   Expert 41 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.536915.536915 lmp.py:1935]   Expert 26 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.536796.536796 lmp.py:1935]   Expert 63 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.536201.536201 lmp.py:1935]   Expert 37 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.536605.536605 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.536771.536771 lmp.py:1935]   Expert 31 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.536699.536699 lmp.py:1935]   Expert 52 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.536865.536865 lmp.py:1935]   Expert 58 |    322 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.536031.536031 lmp.py:1935]   Expert 62 |    323 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.536436.536436 lmp.py:1935]   Expert 55 |    337 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.536032.536032 lmp.py:1935]   Expert 11 |    382 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.536391.536391 lmp.py:1935]   Expert 23 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.536749.536749 lmp.py:1935]   Expert 25 |    403 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.536630.536630 lmp.py:1935]   Expert  5 |    519 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.536796.536796 lmp.py:1937] 
DEBUG 01-15 16:10:43.536796.536796 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.536916.536916 lmp.py:1938]   CPU:   3978 tokens
DEBUG 01-15 16:10:43.536559.536559 lmp.py:1942]   cuda:1:   4176 tokens (16 experts)
DEBUG 01-15 16:10:43.536964.536964 lmp.py:1942]   cuda:2:   4134 tokens (16 experts)
DEBUG 01-15 16:10:43.536415.536415 lmp.py:1943]   Total GPU:   8310 tokens
DEBUG 01-15 16:10:43.536627.536627 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.536627.536627 lmp.py:1944] 
DEBUG 01-15 16:10:43.536277.536277 cuda_h.py:19] end experts_map_get cost 0.0016837120056152344 seconds
DEBUG 01-15 16:10:43.536836.536836 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.536069.536069 lmp.py:1953] 
DEBUG 01-15 16:10:43.536069.536069 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.536012.536012 cuda_h.py:19] end cpu_experts_submit cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:10:43.536516.536516 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.536438.536438 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.539144.539144 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.5.gate_proj.weight': 0, 'model.layers.8.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.41.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.41.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.41.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.43.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.43.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.43.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.9.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.9.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.9.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.47.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.47.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.47.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.26.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.26.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.26.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.3.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.3.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.3.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.49.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.49.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.49.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.50.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.50.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.50.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.57.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.57.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.57.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(10698096640, 5767168, 0, 0), (10703863808, 5767168, 5767168, 0), (10692329472, 5767168, 11534336, 0), (11320950784, 5767168, 17301504, 0), (11326717952, 5767168, 23068672, 0), (11315183616, 5767168, 28835840, 0), (10784604160, 5767168, 34603008, 0), (10790371328, 5767168, 40370176, 0), (10778836992, 5767168, 46137344, 0), (10801905664, 5767168, 51904512, 0), (10807672832, 5767168, 57671680, 0), (10796138496, 5767168, 63438848, 0), (11355553792, 5767168, 69206016, 0), (11361320960, 5767168, 74973184, 0), (11349786624, 5767168, 80740352, 0), (10836508672, 5767168, 86507520, 0), (10842275840, 5767168, 92274688, 0), (10830741504, 5767168, 98041856, 0), (11390156800, 5767168, 103809024, 0), (11395923968, 5767168, 109576192, 0), (11384389632, 5767168, 115343360, 0), (11407458304, 5767168, 121110528, 0), (11413225472, 5767168, 126877696, 0), (11401691136, 5767168, 132644864, 0), (10767302656, 5767168, 138412032, 0), (10773069824, 5767168, 144179200, 0), (10761535488, 5767168, 149946368, 0), (11424759808, 5767168, 155713536, 0), (11430526976, 5767168, 161480704, 0), (11418992640, 5767168, 167247872, 0), (11511267328, 5767168, 173015040, 0), (11517034496, 5767168, 178782208, 0), (11505500160, 5767168, 184549376, 0), (11580473344, 5767168, 190316544, 0), (11586240512, 5767168, 196083712, 0), (11574706176, 5767168, 201850880, 0), (11096031232, 5767168, 207618048, 0), (11101798400, 5767168, 213385216, 0), (11090264064, 5767168, 219152384, 0), (11666980864, 5767168, 224919552, 0), (11672748032, 5767168, 230686720, 0), (11661213696, 5767168, 236453888, 0), (11684282368, 5767168, 242221056, 0), (11690049536, 5767168, 247988224, 0), (11678515200, 5767168, 253755392, 0), (11701583872, 5767168, 259522560, 0), (11707351040, 5767168, 265289728, 0), (11695816704, 5767168, 271056896, 0)], 2: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (11061428224, 5767168, 17301504, 0), (11067195392, 5767168, 23068672, 0), (11055661056, 5767168, 28835840, 0), (10680795136, 5767168, 34603008, 0), (10686562304, 5767168, 40370176, 0), (10675027968, 5767168, 46137344, 0), (11251744768, 5767168, 51904512, 0), (11257511936, 5767168, 57671680, 0), (11245977600, 5767168, 63438848, 0), (10663493632, 5767168, 69206016, 0), (10669260800, 5767168, 74973184, 0), (10657726464, 5767168, 80740352, 0), (11459362816, 5767168, 86507520, 0), (11465129984, 5767168, 92274688, 0), (11453595648, 5767168, 98041856, 0), (11476664320, 5767168, 103809024, 0), (11482431488, 5767168, 109576192, 0), (11470897152, 5767168, 115343360, 0), (10940317696, 5767168, 121110528, 0), (10946084864, 5767168, 126877696, 0), (10934550528, 5767168, 132644864, 0), (11009523712, 5767168, 138412032, 0), (11015290880, 5767168, 144179200, 0), (11003756544, 5767168, 149946368, 0), (10957619200, 5767168, 155713536, 0), (10963386368, 5767168, 161480704, 0), (10951852032, 5767168, 167247872, 0), (10974920704, 5767168, 173015040, 0), (10980687872, 5767168, 178782208, 0), (10969153536, 5767168, 184549376, 0), (11563171840, 5767168, 190316544, 0), (11568939008, 5767168, 196083712, 0), (11557404672, 5767168, 201850880, 0), (11044126720, 5767168, 207618048, 0), (11049893888, 5767168, 213385216, 0), (11038359552, 5767168, 219152384, 0), (11615076352, 5767168, 224919552, 0), (11620843520, 5767168, 230686720, 0), (11609309184, 5767168, 236453888, 0), (11597774848, 5767168, 242221056, 0), (11603542016, 5767168, 247988224, 0), (11592007680, 5767168, 253755392, 0), (11147935744, 5767168, 259522560, 0), (11153702912, 5767168, 265289728, 0), (11142168576, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e547ae0d0>, 2: <capsule object NULL at 0x7a4e547ae340>}
DEBUG 01-15 16:10:43.539635.539635 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.539432.539432 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.539996.539996 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c7912162-6f89-4c7c-abba-ea6fd29bcc58
DEBUG 01-15 16:10:43.540116.540116 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.540802.540802 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:43.540526.540526 client.py:127] Model loaded
DEBUG 01-15 16:10:43.540117.540117 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.540797.540797 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.540198.540198 cuda_h.py:19] end restore2model cost 0.0004124641418457031 seconds
DEBUG 01-15 16:10:43.540266.540266 cuda_h.py:19] end sllm_worker_task cost 0.014481782913208008 seconds
INFO 01-15 16:10:43.541401.541401 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c7912162-6f89-4c7c-abba-ea6fd29bcc58
DEBUG 01-15 16:10:43.541580.541580 cuda_h.py:19] end move_flatidxs cost 0.0009338855743408203 seconds
DEBUG 01-15 16:10:43.541973.541973 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.542455.542455 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0058100223541259766 seconds
DEBUG 01-15 16:10:43.542431.542431 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.545422.545422 cuda_h.py:19] end restore2model cost 0.0026607513427734375 seconds
DEBUG 01-15 16:10:43.545524.545524 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008727073669433594 seconds
DEBUG 01-15 16:10:43.545319.545319 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.545516.545516 cuda_h.py:19] end gpu_sexperts cost 0.00028443336486816406 seconds
DEBUG 01-15 16:10:43.545630.545630 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.545598.545598 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:10:43.545533.545533 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.545189.545189 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.547294.547294 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0013089179992675781 seconds
DEBUG 01-15 16:10:43.547998.547998 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.547422.547422 cuda_h.py:19] end gpu_group_list cost 0.0001742839813232422 seconds
DEBUG 01-15 16:10:43.548733.548733 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.549462.549462 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000934600830078125 seconds
DEBUG 01-15 16:10:43.549736.549736 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.549736.549736 cuda_h.py:19] end gpu_group_list cost 0.0001773834228515625 seconds
DEBUG 01-15 16:10:43.550412.550412 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.550924.550924 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c7912162-6f89-4c7c-abba-ea6fd29bcc58
DEBUG 01-15 16:10:43.553436.553436 cuda_h.py:19] end group_tensors cost 0.012182950973510742 seconds
DEBUG 01-15 16:10:43.554142.554142 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.558329.558329 cuda_h.py:19] end group pad cost 0.0033512115478515625 seconds
DEBUG 01-15 16:10:43.558920.558920 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.569662.569662 client.py:127] Model loaded
DEBUG 01-15 16:10:43.569767.569767 cuda_h.py:19] end wait_experts_multi_device cost 0.01902604103088379 seconds
DEBUG 01-15 16:10:43.569352.569352 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.578664.578664 cuda_h.py:19] end group_einsum cost 0.020158767700195312 seconds
DEBUG 01-15 16:10:43.578589.578589 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.582932.582932 cuda_h.py:19] end get_outputs_cpu1 cost 0.003839731216430664 seconds
DEBUG 01-15 16:10:43.583044.583044 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04271292686462402 seconds
DEBUG 01-15 16:10:43.583239.583239 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01418447494506836 seconds
DEBUG 01-15 16:10:43.583044.583044 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.584437.584437 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.585761.585761 cuda_h.py:19] end index_scatter cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:10:43.585672.585672 cuda_h.py:19] end cpuoutputsdeal cost 0.0017228126525878906 seconds
DEBUG 01-15 16:10:43.585397.585397 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.585491.585491 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.585258.585258 cuda_h.py:19] end gpu_group_tensor cost 0.0001480579376220703 seconds
DEBUG 01-15 16:10:43.585829.585829 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.585477.585477 cuda_h.py:19] end gpu_group_tensor cost 0.00013113021850585938 seconds
DEBUG 01-15 16:10:43.585666.585666 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.586122.586122 cuda_h.py:19] end gpu_group_einsum cost 0.0005359649658203125 seconds
DEBUG 01-15 16:10:43.586100.586100 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.587948.587948 cuda_h.py:19] end gpu_group_einsum cost 0.00036644935607910156 seconds
DEBUG 01-15 16:10:43.587005.587005 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.587803.587803 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.587380.587380 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017690658569335938 seconds
DEBUG 01-15 16:10:43.587805.587805 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.587575.587575 cuda_h.py:19] end concat_expert_out cost 4.57763671875e-05 seconds
DEBUG 01-15 16:10:43.587749.587749 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.587825.587825 cuda_h.py:19] end index_scatter cost 5.650520324707031e-05 seconds
DEBUG 01-15 16:10:43.587436.587436 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000762939453125 seconds
DEBUG 01-15 16:10:43.588896.588896 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.588309.588309 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.588149.588149 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001316070556640625 seconds
DEBUG 01-15 16:10:43.588057.588057 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.588061.588061 cuda_h.py:19] end concat_expert_out cost 7.462501525878906e-05 seconds
DEBUG 01-15 16:10:43.588857.588857 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.588602.588602 cuda_h.py:19] end index_scatter cost 5.793571472167969e-05 seconds
DEBUG 01-15 16:10:43.588127.588127 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005197525024414062 seconds
DEBUG 01-15 16:10:43.588752.588752 cuda_h.py:19] end gpu_experts_multi_device cost 0.04290485382080078 seconds
DEBUG 01-15 16:10:43.588900.588900 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.05502581596374512 seconds
DEBUG 01-15 16:10:43.589648.589648 cuda_h.py:19] end prefill_layer cost 0.06330013275146484 seconds
DEBUG 01-15 16:10:43.589782.589782 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:10:43.589869.589869 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.589526.589526 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:10:43.589659.589659 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:43.589461.589461 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:43.589742.589742 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 3.933906555175781e-05 seconds
DEBUG 01-15 16:10:43.589259.589259 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:43.589624.589624 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.589210.589210 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.589398.589398 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.589958.589958 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.589179.589179 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.590857.590857 cuda_h.py:19] end allocate_cuda_memory cost 0.00025081634521484375 seconds
DEBUG 01-15 16:10:43.590112.590112 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.590544.590544 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.590472.590472 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.590705.590705 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 34f48ba0-3238-4736-a310-ae1c4134b4c5
DEBUG 01-15 16:10:43.590762.590762 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.590156.590156 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.591056.591056 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 34f48ba0-3238-4736-a310-ae1c4134b4c5
DEBUG 01-15 16:10:43.591045.591045 cuda_h.py:19] end load_into_gpu_async cost 0.0015273094177246094 seconds
DEBUG 01-15 16:10:43.591847.591847 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.591434.591434 cuda_h.py:19] end restore_tensors2 cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:10:43.591674.591674 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002143383026123047 seconds
INFO 01-15 16:10:43.592305.592305 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 34f48ba0-3238-4736-a310-ae1c4134b4c5
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.594069.594069 cuda_h.py:19] end self_attn cost 0.0038673877716064453 seconds
DEBUG 01-15 16:10:43.594577.594577 cuda_h.py:19] end iln_self_attn_paln cost 0.0054798126220703125 seconds
DEBUG 01-15 16:10:43.595268.595268 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:10:43.595554.595554 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.595381.595381 cuda_h.py:19] end gate cost 0.0007493495941162109 seconds
DEBUG 01-15 16:10:43.595310.595310 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.596831.596831 lmp.py:1912] 
DEBUG 01-15 16:10:43.596831.596831 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.596302.596302 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.596667.596667 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.596933.596933 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.596814.596814 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.596980.596980 lmp.py:1917] 
DEBUG 01-15 16:10:43.596980.596980 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.596100.596100 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.596419.596419 lmp.py:1935]   Expert 24 |     39 | CPU
DEBUG 01-15 16:10:43.596314.596314 lmp.py:1935]   Expert  2 |     47 | CPU
DEBUG 01-15 16:10:43.596433.596433 lmp.py:1935]   Expert 26 |     63 | CPU
DEBUG 01-15 16:10:43.596553.596553 lmp.py:1935]   Expert 32 |     64 | CPU
DEBUG 01-15 16:10:43.596435.596435 lmp.py:1935]   Expert 19 |     67 | CPU
DEBUG 01-15 16:10:43.596078.596078 lmp.py:1935]   Expert 50 |     71 | CPU
DEBUG 01-15 16:10:43.596721.596721 lmp.py:1935]   Expert 15 |     79 | CPU
DEBUG 01-15 16:10:43.596032.596032 lmp.py:1935]   Expert  4 |     81 | CPU
DEBUG 01-15 16:10:43.596391.596391 lmp.py:1935]   Expert 28 |     82 | CPU
DEBUG 01-15 16:10:43.596464.596464 lmp.py:1935]   Expert 60 |     82 | CPU
DEBUG 01-15 16:10:43.596822.596822 lmp.py:1935]   Expert  7 |     83 | CPU
DEBUG 01-15 16:10:43.596942.596942 lmp.py:1935]   Expert 59 |     91 | CPU
DEBUG 01-15 16:10:43.596347.596347 lmp.py:1935]   Expert 23 |     98 | CPU
DEBUG 01-15 16:10:43.596751.596751 lmp.py:1935]   Expert 49 |     99 | CPU
DEBUG 01-15 16:10:43.596156.596156 lmp.py:1935]   Expert  5 |    104 | CPU
DEBUG 01-15 16:10:43.596799.596799 lmp.py:1935]   Expert 12 |    106 | CPU
DEBUG 01-15 16:10:43.596965.596965 lmp.py:1935]   Expert 10 |    110 | CPU
DEBUG 01-15 16:10:43.596370.596370 lmp.py:1935]   Expert 27 |    112 | CPU
DEBUG 01-15 16:10:43.596774.596774 lmp.py:1935]   Expert 41 |    123 | CPU
DEBUG 01-15 16:10:43.596894.596894 lmp.py:1935]   Expert  3 |    124 | CPU
DEBUG 01-15 16:10:43.596298.596298 lmp.py:1935]   Expert 25 |    127 | CPU
DEBUG 01-15 16:10:43.596703.596703 lmp.py:1935]   Expert 16 |    128 | CPU
DEBUG 01-15 16:10:43.596790.596790 lmp.py:1935]   Expert 20 |    128 | CPU
DEBUG 01-15 16:10:43.596671.596671 lmp.py:1935]   Expert 40 |    130 | CPU
DEBUG 01-15 16:10:43.596076.596076 lmp.py:1935]   Expert 13 |    131 | CPU
DEBUG 01-15 16:10:43.596957.596957 lmp.py:1935]   Expert 37 |    145 | CPU
DEBUG 01-15 16:10:43.596124.596124 lmp.py:1935]   Expert 17 |    147 | CPU
DEBUG 01-15 16:10:43.596290.596290 lmp.py:1935]   Expert 35 |    147 | CPU
DEBUG 01-15 16:10:43.596217.596217 lmp.py:1935]   Expert 47 |    152 | CPU
DEBUG 01-15 16:10:43.596145.596145 lmp.py:1935]   Expert 22 |    157 | CPU
DEBUG 01-15 16:10:43.596834.596834 lmp.py:1935]   Expert 53 |    169 | CPU
DEBUG 01-15 16:10:43.596716.596716 lmp.py:1935]   Expert 39 |    173 | CPU
DEBUG 01-15 16:10:43.596505.596505 lmp.py:1935]   Expert 38 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.596816.596816 lmp.py:1935]   Expert 44 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597128.597128 lmp.py:1935]   Expert 36 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597679.597679 lmp.py:1935]   Expert 52 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597514.597514 lmp.py:1935]   Expert 58 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597872.597872 lmp.py:1935]   Expert 18 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597469.597469 lmp.py:1935]   Expert 62 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597827.597827 lmp.py:1935]   Expert 11 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597947.597947 lmp.py:1935]   Expert 48 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597066.597066 lmp.py:1935]   Expert 14 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597186.597186 lmp.py:1935]   Expert 30 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597021.597021 lmp.py:1935]   Expert  1 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597618.597618 lmp.py:1935]   Expert 45 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597168.597168 lmp.py:1935]   Expert 31 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597765.597765 lmp.py:1935]   Expert 42 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597838.597838 lmp.py:1935]   Expert  6 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597197.597197 lmp.py:1935]   Expert 51 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597317.597317 lmp.py:1935]   Expert 29 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597959.597959 lmp.py:1935]   Expert 34 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597079.597079 lmp.py:1935]   Expert 33 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597199.597199 lmp.py:1935]   Expert 57 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597319.597319 lmp.py:1935]   Expert 61 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597962.597962 lmp.py:1935]   Expert 43 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597320.597320 lmp.py:1935]   Expert  0 |    321 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597202.597202 lmp.py:1935]   Expert 46 |    352 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597560.597560 lmp.py:1935]   Expert  8 |    382 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597203.597203 lmp.py:1935]   Expert 56 |    391 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597084.597084 lmp.py:1935]   Expert  9 |    392 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597966.597966 lmp.py:1935]   Expert 54 |    394 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597608.597608 lmp.py:1935]   Expert 63 |    408 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597728.597728 lmp.py:1935]   Expert 55 |    425 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.597802.597802 lmp.py:1935]   Expert 21 |    491 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.597445.597445 lmp.py:1937] 
DEBUG 01-15 16:10:43.597445.597445 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.597565.597565 lmp.py:1938]   CPU:   3459 tokens
DEBUG 01-15 16:10:43.597175.597175 lmp.py:1942]   cuda:1:   4416 tokens (16 experts)
DEBUG 01-15 16:10:43.597056.597056 lmp.py:1942]   cuda:2:   4413 tokens (16 experts)
DEBUG 01-15 16:10:43.597746.597746 lmp.py:1943]   Total GPU:   8829 tokens
DEBUG 01-15 16:10:43.597196.597196 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.597196.597196 lmp.py:1944] 
DEBUG 01-15 16:10:43.597846.597846 cuda_h.py:19] end experts_map_get cost 0.0017347335815429688 seconds
DEBUG 01-15 16:10:43.597550.597550 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.597876.597876 lmp.py:1953] 
DEBUG 01-15 16:10:43.597876.597876 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.597752.597752 cuda_h.py:19] end cpu_experts_submit cost 4.76837158203125e-05 seconds
DEBUG 01-15 16:10:43.597468.597468 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.597436.597436 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.598034.598034 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.34.gate_proj.weight': 0, 'model.layers.9.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.42.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.42.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.42.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.43.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.43.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.43.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.54.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.54.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.54.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.62.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.62.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.62.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.1.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.1.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.1.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.9.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.9.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.9.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.11.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.11.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.11.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.31.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.31.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.31.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(12307136512, 5767168, 0, 0), (12312903680, 5767168, 5767168, 0), (12301369344, 5767168, 11534336, 0), (12445548544, 5767168, 17301504, 0), (12451315712, 5767168, 23068672, 0), (12439781376, 5767168, 28835840, 0), (12462850048, 5767168, 34603008, 0), (12468617216, 5767168, 40370176, 0), (12457082880, 5767168, 46137344, 0), (12480151552, 5767168, 51904512, 0), (12485918720, 5767168, 57671680, 0), (12474384384, 5767168, 63438848, 0), (12497453056, 5767168, 69206016, 0), (12503220224, 5767168, 74973184, 0), (12491685888, 5767168, 80740352, 0), (12514754560, 5767168, 86507520, 0), (12520521728, 5767168, 92274688, 0), (12508987392, 5767168, 98041856, 0), (11961106432, 5767168, 103809024, 0), (11966873600, 5767168, 109576192, 0), (11955339264, 5767168, 115343360, 0), (12030312448, 5767168, 121110528, 0), (12036079616, 5767168, 126877696, 0), (12024545280, 5767168, 132644864, 0), (12082216960, 5767168, 138412032, 0), (12087984128, 5767168, 144179200, 0), (12076449792, 5767168, 149946368, 0), (12653166592, 5767168, 155713536, 0), (12658933760, 5767168, 161480704, 0), (12647399424, 5767168, 167247872, 0), (12791578624, 5767168, 173015040, 0), (12797345792, 5767168, 178782208, 0), (12785811456, 5767168, 184549376, 0), (12687769600, 5767168, 190316544, 0), (12693536768, 5767168, 196083712, 0), (12682002432, 5767168, 201850880, 0), (12705071104, 5767168, 207618048, 0), (12710838272, 5767168, 213385216, 0), (12699303936, 5767168, 219152384, 0), (12722372608, 5767168, 224919552, 0), (12728139776, 5767168, 230686720, 0), (12716605440, 5767168, 236453888, 0), (12220628992, 5767168, 242221056, 0), (12226396160, 5767168, 247988224, 0), (12214861824, 5767168, 253755392, 0), (12237930496, 5767168, 259522560, 0), (12243697664, 5767168, 265289728, 0), (12232163328, 5767168, 271056896, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (12289835008, 5767168, 17301504, 0), (12295602176, 5767168, 23068672, 0), (12284067840, 5767168, 28835840, 0), (11736186880, 5767168, 34603008, 0), (11741954048, 5767168, 40370176, 0), (11730419712, 5767168, 46137344, 0), (12341739520, 5767168, 51904512, 0), (12347506688, 5767168, 57671680, 0), (12335972352, 5767168, 63438848, 0), (11822694400, 5767168, 69206016, 0), (11828461568, 5767168, 74973184, 0), (11816927232, 5767168, 80740352, 0), (12376342528, 5767168, 86507520, 0), (12382109696, 5767168, 92274688, 0), (12370575360, 5767168, 98041856, 0), (11857297408, 5767168, 103809024, 0), (11863064576, 5767168, 109576192, 0), (11851530240, 5767168, 115343360, 0), (11874598912, 5767168, 121110528, 0), (11880366080, 5767168, 126877696, 0), (11868831744, 5767168, 132644864, 0), (11909201920, 5767168, 138412032, 0), (11914969088, 5767168, 144179200, 0), (11903434752, 5767168, 149946368, 0), (12549357568, 5767168, 155713536, 0), (12555124736, 5767168, 161480704, 0), (12543590400, 5767168, 167247872, 0), (12601262080, 5767168, 173015040, 0), (12607029248, 5767168, 178782208, 0), (12595494912, 5767168, 184549376, 0), (12618563584, 5767168, 190316544, 0), (12624330752, 5767168, 196083712, 0), (12612796416, 5767168, 201850880, 0), (12670468096, 5767168, 207618048, 0), (12676235264, 5767168, 213385216, 0), (12664700928, 5767168, 219152384, 0), (12255232000, 5767168, 224919552, 0), (12260999168, 5767168, 230686720, 0), (12249464832, 5767168, 236453888, 0), (12774277120, 5767168, 242221056, 0), (12780044288, 5767168, 247988224, 0), (12768509952, 5767168, 253755392, 0), (12808880128, 5767168, 259522560, 0), (12814647296, 5767168, 265289728, 0), (12803112960, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c2c2490>, 2: <capsule object NULL at 0x7a4e447ea370>}
DEBUG 01-15 16:10:43.598398.598398 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.598804.598804 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.599937.599937 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 24527c63-caaa-48cd-99ad-5d2e7c2d6b25
DEBUG 01-15 16:10:43.599711.599711 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.599925.599925 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:43.599618.599618 client.py:127] Model loaded
DEBUG 01-15 16:10:43.599163.599163 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.599379.599379 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.599639.599639 cuda_h.py:19] end restore2model cost 0.0003552436828613281 seconds
DEBUG 01-15 16:10:43.599601.599601 cuda_h.py:19] end sllm_worker_task cost 0.010103702545166016 seconds
INFO 01-15 16:10:43.600499.600499 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 24527c63-caaa-48cd-99ad-5d2e7c2d6b25
DEBUG 01-15 16:10:43.600985.600985 cuda_h.py:19] end move_flatidxs cost 0.0008442401885986328 seconds
DEBUG 01-15 16:10:43.600146.600146 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.600678.600678 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0030252933502197266 seconds
DEBUG 01-15 16:10:43.601754.601754 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.603947.603947 cuda_h.py:19] end restore2model cost 0.0025637149810791016 seconds
DEBUG 01-15 16:10:43.603320.603320 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005836963653564453 seconds
DEBUG 01-15 16:10:43.603592.603592 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.604530.604530 cuda_h.py:19] end gpu_sexperts cost 0.0002734661102294922 seconds
DEBUG 01-15 16:10:43.604736.604736 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.604367.604367 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5497207641601562e-05 seconds
DEBUG 01-15 16:10:43.604586.604586 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.604289.604289 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.605796.605796 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008690357208251953 seconds
DEBUG 01-15 16:10:43.605878.605878 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.605864.605864 cuda_h.py:19] end gpu_group_list cost 0.00017309188842773438 seconds
DEBUG 01-15 16:10:43.606923.606923 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.605557.605557 cuda_h.py:19] end group_tensors cost 0.005313873291015625 seconds
DEBUG 01-15 16:10:43.606164.606164 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.607464.607464 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012826919555664062 seconds
DEBUG 01-15 16:10:43.607648.607648 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.607274.607274 cuda_h.py:19] end gpu_group_list cost 0.0002751350402832031 seconds
DEBUG 01-15 16:10:43.608603.608603 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.608824.608824 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 24527c63-caaa-48cd-99ad-5d2e7c2d6b25
DEBUG 01-15 16:10:43.610417.610417 cuda_h.py:19] end group pad cost 0.0035407543182373047 seconds
DEBUG 01-15 16:10:43.610054.610054 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.626257.626257 client.py:127] Model loaded
DEBUG 01-15 16:10:43.626912.626912 cuda_h.py:19] end wait_experts_multi_device cost 0.017826080322265625 seconds
DEBUG 01-15 16:10:43.626211.626211 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.627436.627436 cuda_h.py:19] end group_einsum cost 0.017648935317993164 seconds
DEBUG 01-15 16:10:43.628342.628342 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.631655.631655 cuda_h.py:19] end get_outputs_cpu1 cost 0.003561735153198242 seconds
DEBUG 01-15 16:10:43.632826.632826 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.032971858978271484 seconds
DEBUG 01-15 16:10:43.632558.632558 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.005961179733276367 seconds
DEBUG 01-15 16:10:43.632097.632097 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.634099.634099 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.634913.634913 cuda_h.py:19] end index_scatter cost 9.894371032714844e-05 seconds
DEBUG 01-15 16:10:43.634817.634817 cuda_h.py:19] end cpuoutputsdeal cost 0.0016837120056152344 seconds
DEBUG 01-15 16:10:43.634827.634827 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.634159.634159 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.634132.634132 cuda_h.py:19] end gpu_group_tensor cost 0.00015783309936523438 seconds
DEBUG 01-15 16:10:43.634418.634418 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.635556.635556 cuda_h.py:19] end gpu_group_tensor cost 0.00014019012451171875 seconds
DEBUG 01-15 16:10:43.635176.635176 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.635734.635734 cuda_h.py:19] end gpu_group_einsum cost 0.0006072521209716797 seconds
DEBUG 01-15 16:10:43.636116.636116 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.636057.636057 cuda_h.py:19] end gpu_group_einsum cost 0.00052642822265625 seconds
DEBUG 01-15 16:10:43.636710.636710 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.636297.636297 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.637855.637855 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022363662719726562 seconds
DEBUG 01-15 16:10:43.637387.637387 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.637807.637807 cuda_h.py:19] end concat_expert_out cost 6.103515625e-05 seconds
DEBUG 01-15 16:10:43.637081.637081 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.637633.637633 cuda_h.py:19] end index_scatter cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:10:43.637649.637649 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008838176727294922 seconds
DEBUG 01-15 16:10:43.637586.637586 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.637283.637283 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.637414.637414 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001354217529296875 seconds
DEBUG 01-15 16:10:43.638455.638455 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.638233.638233 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:43.638645.638645 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.638913.638913 cuda_h.py:19] end index_scatter cost 5.817413330078125e-05 seconds
DEBUG 01-15 16:10:43.638914.638914 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004863739013671875 seconds
DEBUG 01-15 16:10:43.638731.638731 cuda_h.py:19] end gpu_experts_multi_device cost 0.03417849540710449 seconds
DEBUG 01-15 16:10:43.638264.638264 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.04336380958557129 seconds
DEBUG 01-15 16:10:43.638893.638893 cuda_h.py:19] end prefill_layer cost 0.049529314041137695 seconds
DEBUG 01-15 16:10:43.638604.638604 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:10:43.638499.638499 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.638394.638394 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:10:43.638766.638766 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:43.639283.639283 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:43.639279.639279 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 3.886222839355469e-05 seconds
DEBUG 01-15 16:10:43.639750.639750 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 7.2479248046875e-05 seconds
DEBUG 01-15 16:10:43.639261.639261 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.639131.639131 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.639127.639127 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.639396.639396 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.639861.639861 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.639466.639466 cuda_h.py:19] end allocate_cuda_memory cost 0.00040650367736816406 seconds
DEBUG 01-15 16:10:43.640188.640188 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.640911.640911 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.640045.640045 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.640312.640312 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1941b2c0-4392-47ee-8879-b9153bb1c943
DEBUG 01-15 16:10:43.640985.640985 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.640135.640135 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.641787.641787 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1941b2c0-4392-47ee-8879-b9153bb1c943
DEBUG 01-15 16:10:43.641961.641961 cuda_h.py:19] end load_into_gpu_async cost 0.0015115737915039062 seconds
DEBUG 01-15 16:10:43.641572.641572 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.642258.642258 cuda_h.py:19] end restore_tensors2 cost 8.893013000488281e-05 seconds
DEBUG 01-15 16:10:43.642067.642067 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002602100372314453 seconds
INFO 01-15 16:10:43.642738.642738 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1941b2c0-4392-47ee-8879-b9153bb1c943
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.644206.644206 cuda_h.py:19] end self_attn cost 0.003629922866821289 seconds
DEBUG 01-15 16:10:43.644515.644515 cuda_h.py:19] end iln_self_attn_paln cost 0.0057697296142578125 seconds
DEBUG 01-15 16:10:43.644490.644490 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:10:43.645968.645968 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.645321.645321 cuda_h.py:19] end gate cost 0.0008184909820556641 seconds
DEBUG 01-15 16:10:43.645449.645449 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.646261.646261 lmp.py:1912] 
DEBUG 01-15 16:10:43.646261.646261 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.646971.646971 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.646859.646859 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.646409.646409 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.646529.646529 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.646410.646410 lmp.py:1917] 
DEBUG 01-15 16:10:43.646410.646410 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.646769.646769 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.646657.646657 lmp.py:1935]   Expert 43 |     15 | CPU
DEBUG 01-15 16:10:43.646300.646300 lmp.py:1935]   Expert 27 |     32 | CPU
DEBUG 01-15 16:10:43.646433.646433 lmp.py:1935]   Expert 26 |     51 | CPU
DEBUG 01-15 16:10:43.646407.646407 lmp.py:1935]   Expert 34 |     53 | CPU
DEBUG 01-15 16:10:43.646858.646858 lmp.py:1935]   Expert 56 |     57 | CPU
DEBUG 01-15 16:10:43.646740.646740 lmp.py:1935]   Expert  3 |     58 | CPU
DEBUG 01-15 16:10:43.646144.646144 lmp.py:1935]   Expert  4 |     67 | CPU
DEBUG 01-15 16:10:43.646264.646264 lmp.py:1935]   Expert 61 |     80 | CPU
DEBUG 01-15 16:10:43.646384.646384 lmp.py:1935]   Expert 14 |     94 | CPU
DEBUG 01-15 16:10:43.646027.646027 lmp.py:1935]   Expert 38 |    100 | CPU
DEBUG 01-15 16:10:43.646146.646146 lmp.py:1935]   Expert  2 |    111 | CPU
DEBUG 01-15 16:10:43.646313.646313 lmp.py:1935]   Expert 17 |    121 | CPU
DEBUG 01-15 16:10:43.646479.646479 lmp.py:1935]   Expert 22 |    123 | CPU
DEBUG 01-15 16:10:43.646406.646406 lmp.py:1935]   Expert 47 |    128 | CPU
DEBUG 01-15 16:10:43.646334.646334 lmp.py:1935]   Expert 37 |    130 | CPU
DEBUG 01-15 16:10:43.646024.646024 lmp.py:1935]   Expert 55 |    132 | CPU
DEBUG 01-15 16:10:43.646190.646190 lmp.py:1935]   Expert 28 |    135 | CPU
DEBUG 01-15 16:10:43.646117.646117 lmp.py:1935]   Expert 54 |    135 | CPU
DEBUG 01-15 16:10:43.646283.646283 lmp.py:1935]   Expert  7 |    141 | CPU
DEBUG 01-15 16:10:43.646642.646642 lmp.py:1935]   Expert 15 |    143 | CPU
DEBUG 01-15 16:10:43.646000.646000 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:10:43.646358.646358 lmp.py:1935]   Expert 48 |    148 | CPU
DEBUG 01-15 16:10:43.646763.646763 lmp.py:1935]   Expert 45 |    149 | CPU
DEBUG 01-15 16:10:43.646167.646167 lmp.py:1935]   Expert 51 |    149 | CPU
DEBUG 01-15 16:10:43.646585.646585 lmp.py:1935]   Expert 60 |    151 | CPU
DEBUG 01-15 16:10:43.646036.646036 lmp.py:1935]   Expert 12 |    153 | CPU
DEBUG 01-15 16:10:43.646249.646249 lmp.py:1935]   Expert 63 |    155 | CPU
DEBUG 01-15 16:10:43.646461.646461 lmp.py:1935]   Expert 19 |    157 | CPU
DEBUG 01-15 16:10:43.646912.646912 lmp.py:1935]   Expert  6 |    167 | CPU
DEBUG 01-15 16:10:43.646125.646125 lmp.py:1935]   Expert 57 |    171 | CPU
DEBUG 01-15 16:10:43.646575.646575 lmp.py:1935]   Expert 52 |    178 | CPU
DEBUG 01-15 16:10:43.646026.646026 lmp.py:1935]   Expert 50 |    180 | CPU
DEBUG 01-15 16:10:43.646623.646623 lmp.py:1935]   Expert 18 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647696.647696 lmp.py:1935]   Expert 44 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647816.647816 lmp.py:1935]   Expert 31 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647459.647459 lmp.py:1935]   Expert 13 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647102.647102 lmp.py:1935]   Expert 23 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647507.647507 lmp.py:1935]   Expert 30 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647150.647150 lmp.py:1935]   Expert 39 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647316.647316 lmp.py:1935]   Expert 53 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647720.647720 lmp.py:1935]   Expert 59 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647887.647887 lmp.py:1935]   Expert 20 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647291.647291 lmp.py:1935]   Expert 29 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647457.647457 lmp.py:1935]   Expert 21 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647577.647577 lmp.py:1935]   Expert 16 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647459.647459 lmp.py:1935]   Expert 36 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647294.647294 lmp.py:1935]   Expert 41 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647413.647413 lmp.py:1935]   Expert 25 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647056.647056 lmp.py:1935]   Expert 32 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647461.647461 lmp.py:1935]   Expert 49 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647865.647865 lmp.py:1935]   Expert 46 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647032.647032 lmp.py:1935]   Expert  8 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647436.647436 lmp.py:1935]   Expert 42 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647841.647841 lmp.py:1935]   Expert 10 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647007.647007 lmp.py:1935]   Expert 62 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647173.647173 lmp.py:1935]   Expert 35 |    280 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647293.647293 lmp.py:1935]   Expert 33 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647174.647174 lmp.py:1935]   Expert  9 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647294.647294 lmp.py:1935]   Expert 58 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647414.647414 lmp.py:1935]   Expert 40 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647818.647818 lmp.py:1935]   Expert 11 |    416 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647223.647223 lmp.py:1935]   Expert  0 |    429 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647389.647389 lmp.py:1935]   Expert 24 |    560 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.647317.647317 lmp.py:1935]   Expert  1 |    650 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.647291.647291 lmp.py:1937] 
DEBUG 01-15 16:10:43.647291.647291 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.647980.647980 lmp.py:1938]   CPU:   3811 tokens
DEBUG 01-15 16:10:43.647146.647146 lmp.py:1942]   cuda:1:   4238 tokens (16 experts)
DEBUG 01-15 16:10:43.647312.647312 lmp.py:1942]   cuda:2:   4239 tokens (16 experts)
DEBUG 01-15 16:10:43.647240.647240 lmp.py:1943]   Total GPU:   8477 tokens
DEBUG 01-15 16:10:43.647691.647691 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.647691.647691 lmp.py:1944] 
DEBUG 01-15 16:10:43.647579.647579 cuda_h.py:19] end experts_map_get cost 0.0016810894012451172 seconds
DEBUG 01-15 16:10:43.647052.647052 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.647900.647900 lmp.py:1953] 
DEBUG 01-15 16:10:43.647900.647900 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.647015.647015 cuda_h.py:19] end cpu_experts_submit cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:10:43.647208.647208 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.647560.647560 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.649083.649083 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.1.gate_proj.weight': 0, 'model.layers.10.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.32.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.32.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.32.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.10.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.10.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.10.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.42.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.42.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.42.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.44.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.44.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.44.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.13.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.13.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.13.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(12843483136, 5767168, 0, 0), (12849250304, 5767168, 5767168, 0), (12837715968, 5767168, 11534336, 0), (13431734272, 5767168, 17301504, 0), (13437501440, 5767168, 23068672, 0), (13425967104, 5767168, 28835840, 0), (13449035776, 5767168, 34603008, 0), (13454802944, 5767168, 40370176, 0), (13443268608, 5767168, 46137344, 0), (12964593664, 5767168, 51904512, 0), (12970360832, 5767168, 57671680, 0), (12958826496, 5767168, 63438848, 0), (12981895168, 5767168, 69206016, 0), (12987662336, 5767168, 74973184, 0), (12976128000, 5767168, 80740352, 0), (13016498176, 5767168, 86507520, 0), (13022265344, 5767168, 92274688, 0), (13010731008, 5767168, 98041856, 0), (13622050816, 5767168, 103809024, 0), (13627817984, 5767168, 109576192, 0), (13616283648, 5767168, 115343360, 0), (13673955328, 5767168, 121110528, 0), (13679722496, 5767168, 126877696, 0), (13668188160, 5767168, 132644864, 0), (13137608704, 5767168, 138412032, 0), (13143375872, 5767168, 144179200, 0), (13131841536, 5767168, 149946368, 0), (13172211712, 5767168, 155713536, 0), (13177978880, 5767168, 161480704, 0), (13166444544, 5767168, 167247872, 0), (13189513216, 5767168, 173015040, 0), (13195280384, 5767168, 178782208, 0), (13183746048, 5767168, 184549376, 0), (13258719232, 5767168, 190316544, 0), (13264486400, 5767168, 196083712, 0), (13252952064, 5767168, 201850880, 0), (13829668864, 5767168, 207618048, 0), (13835436032, 5767168, 213385216, 0), (13823901696, 5767168, 219152384, 0), (13846970368, 5767168, 224919552, 0), (13852737536, 5767168, 230686720, 0), (13841203200, 5767168, 236453888, 0), (13345226752, 5767168, 242221056, 0), (13350993920, 5767168, 247988224, 0), (13339459584, 5767168, 253755392, 0), (13362528256, 5767168, 259522560, 0), (13368295424, 5767168, 265289728, 0), (13356761088, 5767168, 271056896, 0)], 2: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (13397131264, 5767168, 17301504, 0), (13402898432, 5767168, 23068672, 0), (13391364096, 5767168, 28835840, 0), (13379829760, 5767168, 34603008, 0), (13385596928, 5767168, 40370176, 0), (13374062592, 5767168, 46137344, 0), (13500940288, 5767168, 51904512, 0), (13506707456, 5767168, 57671680, 0), (13495173120, 5767168, 63438848, 0), (13518241792, 5767168, 69206016, 0), (13524008960, 5767168, 74973184, 0), (13512474624, 5767168, 80740352, 0), (13535543296, 5767168, 86507520, 0), (13541310464, 5767168, 92274688, 0), (13529776128, 5767168, 98041856, 0), (12999196672, 5767168, 103809024, 0), (13004963840, 5767168, 109576192, 0), (12993429504, 5767168, 115343360, 0), (13552844800, 5767168, 121110528, 0), (13558611968, 5767168, 126877696, 0), (13547077632, 5767168, 132644864, 0), (13587447808, 5767168, 138412032, 0), (13593214976, 5767168, 144179200, 0), (13581680640, 5767168, 149946368, 0), (13051101184, 5767168, 155713536, 0), (13056868352, 5767168, 161480704, 0), (13045334016, 5767168, 167247872, 0), (13103005696, 5767168, 173015040, 0), (13108772864, 5767168, 178782208, 0), (13097238528, 5767168, 184549376, 0), (13743161344, 5767168, 190316544, 0), (13748928512, 5767168, 196083712, 0), (13737394176, 5767168, 201850880, 0), (13224116224, 5767168, 207618048, 0), (13229883392, 5767168, 213385216, 0), (13218349056, 5767168, 219152384, 0), (13241417728, 5767168, 224919552, 0), (13247184896, 5767168, 230686720, 0), (13235650560, 5767168, 236453888, 0), (13327925248, 5767168, 242221056, 0), (13333692416, 5767168, 247988224, 0), (13322158080, 5767168, 253755392, 0), (13898874880, 5767168, 259522560, 0), (13904642048, 5767168, 265289728, 0), (13893107712, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e44507030>, 2: <capsule object NULL at 0x7a4f2c23b330>}
DEBUG 01-15 16:10:43.649858.649858 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.649455.649455 sllm_store_c.py:29] call client load into gpu
INFO 01-15 16:10:43.649405.649405 client.py:127] Model loaded
DEBUG 01-15 16:10:43.649626.649626 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.649009.649009 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2ac7fd8b-1a6e-4962-868d-2950bbe55941
DEBUG 01-15 16:10:43.650009.650009 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.650561.650561 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:43.650941.650941 cuda_h.py:19] end restore2model cost 0.0006480216979980469 seconds
DEBUG 01-15 16:10:43.650340.650340 cuda_h.py:19] end sllm_worker_task cost 0.010993003845214844 seconds
DEBUG 01-15 16:10:43.650593.650593 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.651314.651314 cuda_h.py:19] end move_flatidxs cost 0.0008678436279296875 seconds
DEBUG 01-15 16:10:43.651018.651018 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:43.651872.651872 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2ac7fd8b-1a6e-4962-868d-2950bbe55941
DEBUG 01-15 16:10:43.652895.652895 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00421595573425293 seconds
DEBUG 01-15 16:10:43.652070.652070 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.654601.654601 cuda_h.py:19] end restore2model cost 0.0025675296783447266 seconds
DEBUG 01-15 16:10:43.654127.654127 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007044553756713867 seconds
DEBUG 01-15 16:10:43.654161.654161 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.655091.655091 cuda_h.py:19] end gpu_sexperts cost 0.00026798248291015625 seconds
DEBUG 01-15 16:10:43.655444.655444 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.655459.655459 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:10:43.655632.655632 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.655573.655573 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.656171.656171 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008301734924316406 seconds
DEBUG 01-15 16:10:43.656776.656776 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.656637.656637 cuda_h.py:19] end gpu_group_list cost 0.0001850128173828125 seconds
DEBUG 01-15 16:10:43.657265.657265 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.658474.658474 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010406970977783203 seconds
DEBUG 01-15 16:10:43.658490.658490 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.658272.658272 cuda_h.py:19] end gpu_group_list cost 0.0001780986785888672 seconds
DEBUG 01-15 16:10:43.659405.659405 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.659387.659387 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2ac7fd8b-1a6e-4962-868d-2950bbe55941
DEBUG 01-15 16:10:43.664114.664114 cuda_h.py:19] end group_tensors cost 0.01311039924621582 seconds
DEBUG 01-15 16:10:43.665727.665727 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.668368.668368 cuda_h.py:19] end group pad cost 0.003475666046142578 seconds
DEBUG 01-15 16:10:43.668483.668483 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.678564.678564 client.py:127] Model loaded
DEBUG 01-15 16:10:43.678853.678853 cuda_h.py:19] end wait_experts_multi_device cost 0.019016504287719727 seconds
DEBUG 01-15 16:10:43.678705.678705 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.688560.688560 cuda_h.py:19] end group_einsum cost 0.01925349235534668 seconds
DEBUG 01-15 16:10:43.688776.688776 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.692147.692147 cuda_h.py:19] end get_outputs_cpu1 cost 0.003673076629638672 seconds
DEBUG 01-15 16:10:43.692850.692850 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04257631301879883 seconds
DEBUG 01-15 16:10:43.693182.693182 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.014550209045410156 seconds
DEBUG 01-15 16:10:43.693510.693510 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.694483.694483 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.694985.694985 cuda_h.py:19] end index_scatter cost 8.082389831542969e-05 seconds
DEBUG 01-15 16:10:43.694677.694677 cuda_h.py:19] end cpuoutputsdeal cost 0.0015943050384521484 seconds
DEBUG 01-15 16:10:43.694017.694017 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.694111.694111 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.695004.695004 cuda_h.py:19] end gpu_group_tensor cost 0.00013589859008789062 seconds
DEBUG 01-15 16:10:43.695959.695959 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.695845.695845 cuda_h.py:19] end gpu_group_tensor cost 0.00013136863708496094 seconds
DEBUG 01-15 16:10:43.695365.695365 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.696566.696566 cuda_h.py:19] end gpu_group_einsum cost 0.0005891323089599609 seconds
DEBUG 01-15 16:10:43.696332.696332 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.696080.696080 cuda_h.py:19] end gpu_group_einsum cost 0.0005276203155517578 seconds
DEBUG 01-15 16:10:43.696449.696449 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.697227.697227 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.697018.697018 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021910667419433594 seconds
DEBUG 01-15 16:10:43.697311.697311 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.697493.697493 cuda_h.py:19] end concat_expert_out cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:10:43.697674.697674 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.697127.697127 cuda_h.py:19] end index_scatter cost 5.340576171875e-05 seconds
DEBUG 01-15 16:10:43.697830.697830 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008604526519775391 seconds
DEBUG 01-15 16:10:43.697290.697290 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.698226.698226 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.698013.698013 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012683868408203125 seconds
DEBUG 01-15 16:10:43.698623.698623 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.698162.698162 cuda_h.py:19] end concat_expert_out cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:43.698482.698482 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.698074.698074 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:43.698599.698599 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004718303680419922 seconds
DEBUG 01-15 16:10:43.698601.698601 cuda_h.py:19] end gpu_experts_multi_device cost 0.043177127838134766 seconds
DEBUG 01-15 16:10:43.698465.698465 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.053600311279296875 seconds
DEBUG 01-15 16:10:43.699272.699272 cuda_h.py:19] end prefill_layer cost 0.060053348541259766 seconds
DEBUG 01-15 16:10:43.699890.699890 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:10:43.699070.699070 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.699727.699727 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:10:43.699860.699860 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:43.699378.699378 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:43.699135.699135 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 3.933906555175781e-05 seconds
DEBUG 01-15 16:10:43.699222.699222 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 7.104873657226562e-05 seconds
DEBUG 01-15 16:10:43.699587.699587 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.699543.699543 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.699349.699349 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.699463.699463 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.699369.699369 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.700339.700339 cuda_h.py:19] end allocate_cuda_memory cost 0.00024366378784179688 seconds
DEBUG 01-15 16:10:43.700852.700852 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.700959.700959 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.700365.700365 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.700221.700221 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, cf5a9ffa-5224-4f4e-823e-4c4b5fe2b2b7
DEBUG 01-15 16:10:43.700370.700370 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.700168.700168 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.701253.701253 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, cf5a9ffa-5224-4f4e-823e-4c4b5fe2b2b7
DEBUG 01-15 16:10:43.701156.701156 cuda_h.py:19] end load_into_gpu_async cost 0.001512289047241211 seconds
DEBUG 01-15 16:10:43.701872.701872 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.701929.701929 cuda_h.py:19] end restore_tensors2 cost 7.653236389160156e-05 seconds
DEBUG 01-15 16:10:43.701745.701745 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021703243255615234 seconds
INFO 01-15 16:10:43.701992.701992 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, cf5a9ffa-5224-4f4e-823e-4c4b5fe2b2b7
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.703072.703072 cuda_h.py:19] end self_attn cost 0.0031943321228027344 seconds
DEBUG 01-15 16:10:43.704666.704666 cuda_h.py:19] end iln_self_attn_paln cost 0.004905223846435547 seconds
DEBUG 01-15 16:10:43.704973.704973 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:10:43.704689.704689 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.705782.705782 cuda_h.py:19] end gate cost 0.000766754150390625 seconds
DEBUG 01-15 16:10:43.705718.705718 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.705215.705215 lmp.py:1912] 
DEBUG 01-15 16:10:43.705215.705215 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.705170.705170 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.705542.705542 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.705052.705052 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.705417.705417 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.705782.705782 lmp.py:1917] 
DEBUG 01-15 16:10:43.705782.705782 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.705432.705432 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.705466.705466 lmp.py:1935]   Expert 13 |     15 | CPU
DEBUG 01-15 16:10:43.705586.705586 lmp.py:1935]   Expert 39 |     16 | CPU
DEBUG 01-15 16:10:43.705229.705229 lmp.py:1935]   Expert 49 |     38 | CPU
DEBUG 01-15 16:10:43.705872.705872 lmp.py:1935]   Expert 35 |     54 | CPU
DEBUG 01-15 16:10:43.705276.705276 lmp.py:1935]   Expert 19 |     63 | CPU
DEBUG 01-15 16:10:43.705919.705919 lmp.py:1935]   Expert  9 |     72 | CPU
DEBUG 01-15 16:10:43.705278.705278 lmp.py:1935]   Expert 26 |     73 | CPU
DEBUG 01-15 16:10:43.705397.705397 lmp.py:1935]   Expert 32 |     75 | CPU
DEBUG 01-15 16:10:43.705517.705517 lmp.py:1935]   Expert 41 |     77 | CPU
DEBUG 01-15 16:10:43.705399.705399 lmp.py:1935]   Expert 33 |     82 | CPU
DEBUG 01-15 16:10:43.705042.705042 lmp.py:1935]   Expert 23 |     86 | CPU
DEBUG 01-15 16:10:43.705446.705446 lmp.py:1935]   Expert 46 |     87 | CPU
DEBUG 01-15 16:10:43.706089.706089 lmp.py:1935]   Expert 18 |     91 | CPU
DEBUG 01-15 16:10:43.706494.706494 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:10:43.706137.706137 lmp.py:1935]   Expert 38 |    101 | CPU
DEBUG 01-15 16:10:43.706541.706541 lmp.py:1935]   Expert  3 |    102 | CPU
DEBUG 01-15 16:10:43.706946.706946 lmp.py:1935]   Expert 17 |    104 | CPU
DEBUG 01-15 16:10:43.706350.706350 lmp.py:1935]   Expert  6 |    105 | CPU
DEBUG 01-15 16:10:43.706993.706993 lmp.py:1935]   Expert 20 |    119 | CPU
DEBUG 01-15 16:10:43.706319.706319 lmp.py:1935]   Expert 40 |    128 | CPU
DEBUG 01-15 16:10:43.706962.706962 lmp.py:1935]   Expert 61 |    129 | CPU
DEBUG 01-15 16:10:43.706366.706366 lmp.py:1935]   Expert 44 |    133 | CPU
DEBUG 01-15 16:10:43.706532.706532 lmp.py:1935]   Expert 62 |    133 | CPU
DEBUG 01-15 16:10:43.706222.706222 lmp.py:1935]   Expert 15 |    134 | CPU
DEBUG 01-15 16:10:43.706149.706149 lmp.py:1935]   Expert 43 |    136 | CPU
DEBUG 01-15 16:10:43.706839.706839 lmp.py:1935]   Expert 50 |    137 | CPU
DEBUG 01-15 16:10:43.706528.706528 lmp.py:1935]   Expert 16 |    138 | CPU
DEBUG 01-15 16:10:43.706456.706456 lmp.py:1935]   Expert 63 |    139 | CPU
DEBUG 01-15 16:10:43.706622.706622 lmp.py:1935]   Expert 59 |    141 | CPU
DEBUG 01-15 16:10:43.706550.706550 lmp.py:1935]   Expert 42 |    143 | CPU
DEBUG 01-15 16:10:43.706477.706477 lmp.py:1935]   Expert  2 |    147 | CPU
DEBUG 01-15 16:10:43.706120.706120 lmp.py:1935]   Expert 36 |    153 | CPU
DEBUG 01-15 16:10:43.706432.706432 lmp.py:1935]   Expert 10 |    160 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706221.706221 lmp.py:1935]   Expert  5 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706771.706771 lmp.py:1935]   Expert 34 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706891.706891 lmp.py:1935]   Expert 52 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706395.706395 lmp.py:1935]   Expert 27 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706561.706561 lmp.py:1935]   Expert 45 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706966.706966 lmp.py:1935]   Expert 60 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706132.706132 lmp.py:1935]   Expert 48 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706730.706730 lmp.py:1935]   Expert 56 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706757.706757 lmp.py:1935]   Expert 51 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706606.706606 lmp.py:1935]   Expert 24 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706202.706202 lmp.py:1935]   Expert  7 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706322.706322 lmp.py:1935]   Expert 53 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706203.706203 lmp.py:1935]   Expert  8 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706562.706562 lmp.py:1935]   Expert 57 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706443.706443 lmp.py:1935]   Expert 47 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706325.706325 lmp.py:1935]   Expert 29 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706206.706206 lmp.py:1935]   Expert 21 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706087.706087 lmp.py:1935]   Expert  4 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706446.706446 lmp.py:1935]   Expert 14 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706996.706996 lmp.py:1935]   Expert  0 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706785.706785 lmp.py:1935]   Expert 22 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706858.706858 lmp.py:1935]   Expert 58 |    314 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706455.706455 lmp.py:1935]   Expert 37 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706098.706098 lmp.py:1935]   Expert 55 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706979.706979 lmp.py:1935]   Expert  1 |    321 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706622.706622 lmp.py:1935]   Expert 54 |    333 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706027.706027 lmp.py:1935]   Expert 28 |    355 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706670.706670 lmp.py:1935]   Expert 12 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706551.706551 lmp.py:1935]   Expert 11 |    398 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706956.706956 lmp.py:1935]   Expert 25 |    400 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.706837.706837 lmp.py:1935]   Expert 30 |    836 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.706480.706480 lmp.py:1937] 
DEBUG 01-15 16:10:43.706480.706480 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.707838.707838 lmp.py:1938]   CPU:   3242 tokens
DEBUG 01-15 16:10:43.707435.707435 lmp.py:1942]   cuda:1:   4603 tokens (16 experts)
DEBUG 01-15 16:10:43.707793.707793 lmp.py:1942]   cuda:2:   4443 tokens (16 experts)
DEBUG 01-15 16:10:43.707959.707959 lmp.py:1943]   Total GPU:   9046 tokens
DEBUG 01-15 16:10:43.707887.707887 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.707887.707887 lmp.py:1944] 
DEBUG 01-15 16:10:43.707775.707775 cuda_h.py:19] end experts_map_get cost 0.001870870590209961 seconds
DEBUG 01-15 16:10:43.707102.707102 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.707474.707474 lmp.py:1953] 
DEBUG 01-15 16:10:43.707474.707474 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.707018.707018 cuda_h.py:19] end cpu_experts_submit cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:43.707953.707953 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.707305.707305 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.708867.708867 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.1.gate_proj.weight': 0, 'model.layers.11.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.56.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.56.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.56.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.52.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.52.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.52.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.21.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.21.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.21.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.53.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.53.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.53.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.55.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.55.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.55.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.27.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.27.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.27.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.60.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.60.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.60.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(13950779392, 5767168, 0, 0), (13956546560, 5767168, 5767168, 0), (13945012224, 5767168, 11534336, 0), (14902362112, 5767168, 17301504, 0), (14908129280, 5767168, 23068672, 0), (14896594944, 5767168, 28835840, 0), (14002683904, 5767168, 34603008, 0), (14008451072, 5767168, 40370176, 0), (13996916736, 5767168, 46137344, 0), (14019985408, 5767168, 51904512, 0), (14025752576, 5767168, 57671680, 0), (14014218240, 5767168, 63438848, 0), (14106492928, 5767168, 69206016, 0), (14112260096, 5767168, 74973184, 0), (14100725760, 5767168, 80740352, 0), (14712045568, 5767168, 86507520, 0), (14717812736, 5767168, 92274688, 0), (14706278400, 5767168, 98041856, 0), (14833156096, 5767168, 103809024, 0), (14838923264, 5767168, 109576192, 0), (14827388928, 5767168, 115343360, 0), (14296809472, 5767168, 121110528, 0), (14302576640, 5767168, 126877696, 0), (14291042304, 5767168, 132644864, 0), (14850457600, 5767168, 138412032, 0), (14856224768, 5767168, 144179200, 0), (14844690432, 5767168, 149946368, 0), (14885060608, 5767168, 155713536, 0), (14890827776, 5767168, 161480704, 0), (14879293440, 5767168, 167247872, 0), (14348713984, 5767168, 173015040, 0), (14354481152, 5767168, 178782208, 0), (14342946816, 5767168, 184549376, 0), (14919663616, 5767168, 190316544, 0), (14925430784, 5767168, 196083712, 0), (14913896448, 5767168, 201850880, 0), (14936965120, 5767168, 207618048, 0), (14942732288, 5767168, 213385216, 0), (14931197952, 5767168, 219152384, 0), (14417920000, 5767168, 224919552, 0), (14423687168, 5767168, 230686720, 0), (14412152832, 5767168, 236453888, 0), (14435221504, 5767168, 242221056, 0), (14440988672, 5767168, 247988224, 0), (14429454336, 5767168, 253755392, 0), (14452523008, 5767168, 259522560, 0), (14458290176, 5767168, 265289728, 0), (14446755840, 5767168, 271056896, 0)], 2: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (14521729024, 5767168, 17301504, 0), (14527496192, 5767168, 23068672, 0), (14515961856, 5767168, 28835840, 0), (14573633536, 5767168, 34603008, 0), (14579400704, 5767168, 40370176, 0), (14567866368, 5767168, 46137344, 0), (14054588416, 5767168, 51904512, 0), (14060355584, 5767168, 57671680, 0), (14048821248, 5767168, 63438848, 0), (14071889920, 5767168, 69206016, 0), (14077657088, 5767168, 74973184, 0), (14066122752, 5767168, 80740352, 0), (14123794432, 5767168, 86507520, 0), (14129561600, 5767168, 92274688, 0), (14118027264, 5767168, 98041856, 0), (14141095936, 5767168, 103809024, 0), (14146863104, 5767168, 109576192, 0), (14135328768, 5767168, 115343360, 0), (14175698944, 5767168, 121110528, 0), (14181466112, 5767168, 126877696, 0), (14169931776, 5767168, 132644864, 0), (14746648576, 5767168, 138412032, 0), (14752415744, 5767168, 144179200, 0), (14740881408, 5767168, 149946368, 0), (14763950080, 5767168, 155713536, 0), (14769717248, 5767168, 161480704, 0), (14758182912, 5767168, 167247872, 0), (14815854592, 5767168, 173015040, 0), (14821621760, 5767168, 178782208, 0), (14810087424, 5767168, 184549376, 0), (14867759104, 5767168, 190316544, 0), (14873526272, 5767168, 196083712, 0), (14861991936, 5767168, 201850880, 0), (14314110976, 5767168, 207618048, 0), (14319878144, 5767168, 213385216, 0), (14308343808, 5767168, 219152384, 0), (14366015488, 5767168, 224919552, 0), (14371782656, 5767168, 230686720, 0), (14360248320, 5767168, 236453888, 0), (14400618496, 5767168, 242221056, 0), (14406385664, 5767168, 247988224, 0), (14394851328, 5767168, 253755392, 0), (14971568128, 5767168, 259522560, 0), (14977335296, 5767168, 265289728, 0), (14965800960, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e44507090>, 2: <capsule object NULL at 0x7a4e3470e490>}
DEBUG 01-15 16:10:43.709766.709766 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.709410.709410 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.709828.709828 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d2a47192-6571-4595-bad6-f3b3c1f9367f
DEBUG 01-15 16:10:43.709464.709464 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:43.709158.709158 client.py:127] Model loaded
DEBUG 01-15 16:10:43.709948.709948 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.709403.709403 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:43.710276.710276 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.710987.710987 cuda_h.py:19] end restore2model cost 0.00034499168395996094 seconds
DEBUG 01-15 16:10:43.710147.710147 cuda_h.py:19] end sllm_worker_task cost 0.010555267333984375 seconds
INFO 01-15 16:10:43.710281.710281 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d2a47192-6571-4595-bad6-f3b3c1f9367f
DEBUG 01-15 16:10:43.710587.710587 cuda_h.py:19] end move_flatidxs cost 0.0008878707885742188 seconds
DEBUG 01-15 16:10:43.711337.711337 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.711689.711689 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0039033889770507812 seconds
DEBUG 01-15 16:10:43.711897.711897 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.713388.713388 cuda_h.py:19] end restore2model cost 0.0025746822357177734 seconds
DEBUG 01-15 16:10:43.714669.714669 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006720304489135742 seconds
DEBUG 01-15 16:10:43.714226.714226 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.714150.714150 cuda_h.py:19] end gpu_sexperts cost 0.0002644062042236328 seconds
DEBUG 01-15 16:10:43.714833.714833 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.714464.714464 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5020370483398438e-05 seconds
DEBUG 01-15 16:10:43.714683.714683 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.714432.714432 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.715480.715480 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008113384246826172 seconds
DEBUG 01-15 16:10:43.715655.715655 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.715139.715139 cuda_h.py:19] end gpu_group_list cost 0.0001881122589111328 seconds
DEBUG 01-15 16:10:43.716481.716481 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.717004.717004 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009233951568603516 seconds
DEBUG 01-15 16:10:43.717729.717729 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.717498.717498 cuda_h.py:19] end gpu_group_list cost 0.00017523765563964844 seconds
DEBUG 01-15 16:10:43.718477.718477 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.718499.718499 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d2a47192-6571-4595-bad6-f3b3c1f9367f
DEBUG 01-15 16:10:43.722890.722890 cuda_h.py:19] end group_tensors cost 0.011904478073120117 seconds
DEBUG 01-15 16:10:43.723265.723265 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.727615.727615 cuda_h.py:19] end group pad cost 0.0034673213958740234 seconds
DEBUG 01-15 16:10:43.727590.727590 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.738303.738303 client.py:127] Model loaded
DEBUG 01-15 16:10:43.738706.738706 cuda_h.py:19] end wait_experts_multi_device cost 0.020003557205200195 seconds
DEBUG 01-15 16:10:43.738138.738138 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.746430.746430 cuda_h.py:19] end group_einsum cost 0.01945972442626953 seconds
DEBUG 01-15 16:10:43.746977.746977 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.750779.750779 cuda_h.py:19] end get_outputs_cpu1 cost 0.00330352783203125 seconds
DEBUG 01-15 16:10:43.750009.750009 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04115033149719238 seconds
DEBUG 01-15 16:10:43.751640.751640 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01335906982421875 seconds
DEBUG 01-15 16:10:43.751441.751441 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.754127.754127 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.754203.754203 cuda_h.py:19] end index_scatter cost 0.00014328956604003906 seconds
DEBUG 01-15 16:10:43.755240.755240 cuda_h.py:19] end cpuoutputsdeal cost 0.003401517868041992 seconds
DEBUG 01-15 16:10:43.755358.755358 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.755142.755142 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.756154.756154 cuda_h.py:19] end gpu_group_tensor cost 0.0002796649932861328 seconds
DEBUG 01-15 16:10:43.756184.756184 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.756467.756467 cuda_h.py:19] end gpu_group_tensor cost 0.0002682209014892578 seconds
DEBUG 01-15 16:10:43.756574.756574 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.758548.758548 cuda_h.py:19] end gpu_group_einsum cost 0.0018656253814697266 seconds
DEBUG 01-15 16:10:43.758431.758431 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.759886.759886 cuda_h.py:19] end gpu_group_einsum cost 0.0008091926574707031 seconds
DEBUG 01-15 16:10:43.760788.760788 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.760557.760557 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.760772.760772 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003781318664550781 seconds
DEBUG 01-15 16:10:43.760365.760365 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.761530.761530 cuda_h.py:19] end concat_expert_out cost 0.00011515617370605469 seconds
DEBUG 01-15 16:10:43.761442.761442 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.761966.761966 cuda_h.py:19] end index_scatter cost 0.00012373924255371094 seconds
DEBUG 01-15 16:10:43.761359.761359 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0017082691192626953 seconds
DEBUG 01-15 16:10:43.762982.762982 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.762404.762404 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.762059.762059 cuda_h.py:19] end all_expert_outputs_slices cost 0.00029659271240234375 seconds
DEBUG 01-15 16:10:43.762399.762399 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.762280.762280 cuda_h.py:19] end concat_expert_out cost 0.00011801719665527344 seconds
DEBUG 01-15 16:10:43.763039.763039 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.763947.763947 cuda_h.py:19] end index_scatter cost 0.00012540817260742188 seconds
DEBUG 01-15 16:10:43.763261.763261 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011296272277832031 seconds
DEBUG 01-15 16:10:43.763472.763472 cuda_h.py:19] end gpu_experts_multi_device cost 0.049037933349609375 seconds
DEBUG 01-15 16:10:43.763478.763478 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.059349775314331055 seconds
DEBUG 01-15 16:10:43.764967.764967 cuda_h.py:19] end prefill_layer cost 0.06491804122924805 seconds
DEBUG 01-15 16:10:43.764546.764546 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:10:43.764109.764109 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.764435.764435 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:10:43.764237.764237 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:43.764947.764947 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:43.764380.764380 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 3.8623809814453125e-05 seconds
DEBUG 01-15 16:10:43.764613.764613 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 7.748603820800781e-05 seconds
DEBUG 01-15 16:10:43.764761.764761 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.764650.764650 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.764376.764376 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.764334.764334 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.765194.765194 cuda_h.py:19] end allocate_cuda_memory cost 0.0002906322479248047 seconds
DEBUG 01-15 16:10:43.765655.765655 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.765181.765181 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.765066.765066 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.765015.765015 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.765030.765030 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 78bbdc54-a699-4b2d-81d4-7c4a70968d14
DEBUG 01-15 16:10:43.765664.765664 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.766987.766987 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.767208.767208 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 78bbdc54-a699-4b2d-81d4-7c4a70968d14
DEBUG 01-15 16:10:43.767145.767145 cuda_h.py:19] end load_into_gpu_async cost 0.0017764568328857422 seconds
DEBUG 01-15 16:10:43.767783.767783 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.767556.767556 cuda_h.py:19] end restore_tensors2 cost 0.00010228157043457031 seconds
DEBUG 01-15 16:10:43.767028.767028 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002750873565673828 seconds
INFO 01-15 16:10:43.767680.767680 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 78bbdc54-a699-4b2d-81d4-7c4a70968d14
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.770903.770903 cuda_h.py:19] end self_attn cost 0.003940105438232422 seconds
DEBUG 01-15 16:10:43.770278.770278 cuda_h.py:19] end iln_self_attn_paln cost 0.0057184696197509766 seconds
DEBUG 01-15 16:10:43.770453.770453 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:10:43.770461.770461 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.771620.771620 cuda_h.py:19] end gate cost 0.0007777214050292969 seconds
DEBUG 01-15 16:10:43.771132.771132 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.771847.771847 lmp.py:1912] 
DEBUG 01-15 16:10:43.771847.771847 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.771948.771948 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.771750.771750 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.771261.771261 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.771017.771017 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.771097.771097 lmp.py:1917] 
DEBUG 01-15 16:10:43.771097.771097 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.771747.771747 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.771073.771073 lmp.py:1935]   Expert 12 |     18 | CPU
DEBUG 01-15 16:10:43.771484.771484 lmp.py:1935]   Expert 47 |     24 | CPU
DEBUG 01-15 16:10:43.772942.772942 lmp.py:1935]   Expert 38 |     31 | CPU
DEBUG 01-15 16:10:43.772545.772545 lmp.py:1935]   Expert 27 |     34 | CPU
DEBUG 01-15 16:10:43.772195.772195 lmp.py:1935]   Expert 16 |     38 | CPU
DEBUG 01-15 16:10:43.772752.772752 lmp.py:1935]   Expert 52 |     40 | CPU
DEBUG 01-15 16:10:43.772163.772163 lmp.py:1935]   Expert 63 |     44 | CPU
DEBUG 01-15 16:10:43.772052.772052 lmp.py:1935]   Expert  4 |     58 | CPU
DEBUG 01-15 16:10:43.772701.772701 lmp.py:1935]   Expert 43 |     63 | CPU
DEBUG 01-15 16:10:43.772589.772589 lmp.py:1935]   Expert 61 |     63 | CPU
DEBUG 01-15 16:10:43.772762.772762 lmp.py:1935]   Expert 44 |     65 | CPU
DEBUG 01-15 16:10:43.772697.772697 lmp.py:1935]   Expert 34 |     77 | CPU
DEBUG 01-15 16:10:43.772631.772631 lmp.py:1935]   Expert 53 |     83 | CPU
DEBUG 01-15 16:10:43.772566.772566 lmp.py:1935]   Expert  0 |     88 | CPU
DEBUG 01-15 16:10:43.772500.772500 lmp.py:1935]   Expert 32 |     90 | CPU
DEBUG 01-15 16:10:43.772912.772912 lmp.py:1935]   Expert 37 |     91 | CPU
DEBUG 01-15 16:10:43.772846.772846 lmp.py:1935]   Expert 13 |    101 | CPU
DEBUG 01-15 16:10:43.772781.772781 lmp.py:1935]   Expert 39 |    111 | CPU
DEBUG 01-15 16:10:43.772192.772192 lmp.py:1935]   Expert 21 |    119 | CPU
DEBUG 01-15 16:10:43.772994.772994 lmp.py:1935]   Expert 11 |    120 | CPU
DEBUG 01-15 16:10:43.772075.772075 lmp.py:1935]   Expert 20 |    126 | CPU
DEBUG 01-15 16:10:43.772678.772678 lmp.py:1935]   Expert 60 |    131 | CPU
DEBUG 01-15 16:10:43.772328.772328 lmp.py:1935]   Expert  8 |    132 | CPU
DEBUG 01-15 16:10:43.772501.772501 lmp.py:1935]   Expert 14 |    137 | CPU
DEBUG 01-15 16:10:43.772435.772435 lmp.py:1935]   Expert 57 |    140 | CPU
DEBUG 01-15 16:10:43.772847.772847 lmp.py:1935]   Expert 22 |    142 | CPU
DEBUG 01-15 16:10:43.772020.772020 lmp.py:1935]   Expert 45 |    153 | CPU
DEBUG 01-15 16:10:43.772954.772954 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:10:43.772127.772127 lmp.py:1935]   Expert 18 |    157 | CPU
DEBUG 01-15 16:10:43.772538.772538 lmp.py:1935]   Expert 23 |    159 | CPU
DEBUG 01-15 16:10:43.772950.772950 lmp.py:1935]   Expert 17 |    160 | CPU
DEBUG 01-15 16:10:43.772646.772646 lmp.py:1935]   Expert  7 |    163 | CPU
DEBUG 01-15 16:10:43.772203.772203 lmp.py:1935]   Expert 30 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.772760.772760 lmp.py:1935]   Expert 58 |    163 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.772364.772364 lmp.py:1935]   Expert 42 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.772490.772490 lmp.py:1935]   Expert 48 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.772094.772094 lmp.py:1935]   Expert 49 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.772174.772174 lmp.py:1935]   Expert 55 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.772254.772254 lmp.py:1935]   Expert 62 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.772096.772096 lmp.py:1935]   Expert 35 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.772560.772560 lmp.py:1935]   Expert 51 |    186 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.772594.772594 lmp.py:1935]   Expert 29 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.772628.772628 lmp.py:1935]   Expert 25 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.772709.772709 lmp.py:1935]   Expert  6 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.772789.772789 lmp.py:1935]   Expert 36 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773392.773392 lmp.py:1935]   Expert  1 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773996.773996 lmp.py:1935]   Expert 31 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773122.773122 lmp.py:1935]   Expert 28 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773487.773487 lmp.py:1935]   Expert  5 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773329.773329 lmp.py:1935]   Expert 41 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773933.773933 lmp.py:1935]   Expert 54 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773775.773775 lmp.py:1935]   Expert  9 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773378.773378 lmp.py:1935]   Expert 19 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773220.773220 lmp.py:1935]   Expert 24 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773585.773585 lmp.py:1935]   Expert 50 |    289 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773188.773188 lmp.py:1935]   Expert 46 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773077.773077 lmp.py:1935]   Expert 59 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773918.773918 lmp.py:1935]   Expert 56 |    377 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773522.773522 lmp.py:1935]   Expert 26 |    406 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773887.773887 lmp.py:1935]   Expert 33 |    424 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773967.773967 lmp.py:1935]   Expert  3 |    590 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773286.773286 lmp.py:1935]   Expert 10 |    640 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773412.773412 lmp.py:1935]   Expert 15 |    646 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.773254.773254 lmp.py:1935]   Expert 40 |    791 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.773427.773427 lmp.py:1937] 
DEBUG 01-15 16:10:43.773427.773427 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.773077.773077 lmp.py:1938]   CPU:   3114 tokens
DEBUG 01-15 16:10:43.773919.773919 lmp.py:1942]   cuda:1:   4586 tokens (16 experts)
DEBUG 01-15 16:10:43.773522.773522 lmp.py:1942]   cuda:2:   4588 tokens (16 experts)
DEBUG 01-15 16:10:43.773695.773695 lmp.py:1943]   Total GPU:   9174 tokens
DEBUG 01-15 16:10:43.773630.773630 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.773630.773630 lmp.py:1944] 
DEBUG 01-15 16:10:43.773717.773717 cuda_h.py:19] end experts_map_get cost 0.0021848678588867188 seconds
DEBUG 01-15 16:10:43.773633.773633 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.773827.773827 lmp.py:1953] 
DEBUG 01-15 16:10:43.773827.773827 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.773815.773815 cuda_h.py:19] end cpu_experts_submit cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:10:43.773803.773803 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.773838.773838 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.774026.774026 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.3.gate_proj.weight': 0, 'model.layers.12.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.41.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.41.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.41.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.48.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.48.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.48.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.51.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.51.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.51.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.55.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.55.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.55.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.12.mlp.experts.33.gate_proj.weight': 0, 'model.layers.12.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.54.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.54.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.54.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(15092678656, 5767168, 0, 0), (15098445824, 5767168, 5767168, 0), (15086911488, 5767168, 11534336, 0), (15663628288, 5767168, 17301504, 0), (15669395456, 5767168, 23068672, 0), (15657861120, 5767168, 28835840, 0), (15127281664, 5767168, 34603008, 0), (15133048832, 5767168, 40370176, 0), (15121514496, 5767168, 46137344, 0), (15732834304, 5767168, 51904512, 0), (15738601472, 5767168, 57671680, 0), (15727067136, 5767168, 63438848, 0), (15750135808, 5767168, 69206016, 0), (15755902976, 5767168, 74973184, 0), (15744368640, 5767168, 80740352, 0), (15767437312, 5767168, 86507520, 0), (15773204480, 5767168, 92274688, 0), (15761670144, 5767168, 98041856, 0), (15871246336, 5767168, 103809024, 0), (15877013504, 5767168, 109576192, 0), (15865479168, 5767168, 115343360, 0), (15905849344, 5767168, 121110528, 0), (15911616512, 5767168, 126877696, 0), (15900082176, 5767168, 132644864, 0), (15923150848, 5767168, 138412032, 0), (15928918016, 5767168, 144179200, 0), (15917383680, 5767168, 149946368, 0), (15992356864, 5767168, 155713536, 0), (15998124032, 5767168, 161480704, 0), (15986589696, 5767168, 167247872, 0), (15456010240, 5767168, 173015040, 0), (15461777408, 5767168, 178782208, 0), (15450243072, 5767168, 184549376, 0), (15473311744, 5767168, 190316544, 0), (15479078912, 5767168, 196083712, 0), (15467544576, 5767168, 201850880, 0), (15490613248, 5767168, 207618048, 0), (15496380416, 5767168, 213385216, 0), (15484846080, 5767168, 219152384, 0), (16061562880, 5767168, 224919552, 0), (16067330048, 5767168, 230686720, 0), (16055795712, 5767168, 236453888, 0), (15525216256, 5767168, 242221056, 0), (15530983424, 5767168, 247988224, 0), (15519449088, 5767168, 253755392, 0), (15559819264, 5767168, 259522560, 0), (15565586432, 5767168, 265289728, 0), (15554052096, 5767168, 271056896, 0)], 2: [(15611723776, 5767168, 0, 0), (15617490944, 5767168, 5767168, 0), (15605956608, 5767168, 11534336, 0), (15058075648, 5767168, 17301504, 0), (15063842816, 5767168, 23068672, 0), (15052308480, 5767168, 28835840, 0), (15646326784, 5767168, 34603008, 0), (15652093952, 5767168, 40370176, 0), (15640559616, 5767168, 46137344, 0), (15144583168, 5767168, 51904512, 0), (15150350336, 5767168, 57671680, 0), (15138816000, 5767168, 63438848, 0), (15196487680, 5767168, 69206016, 0), (15202254848, 5767168, 74973184, 0), (15190720512, 5767168, 80740352, 0), (15213789184, 5767168, 86507520, 0), (15219556352, 5767168, 92274688, 0), (15208022016, 5767168, 98041856, 0), (15836643328, 5767168, 103809024, 0), (15842410496, 5767168, 109576192, 0), (15830876160, 5767168, 115343360, 0), (15300296704, 5767168, 121110528, 0), (15306063872, 5767168, 126877696, 0), (15294529536, 5767168, 132644864, 0), (15888547840, 5767168, 138412032, 0), (15894315008, 5767168, 144179200, 0), (15882780672, 5767168, 149946368, 0), (15369502720, 5767168, 155713536, 0), (15375269888, 5767168, 161480704, 0), (15363735552, 5767168, 167247872, 0), (15975055360, 5767168, 173015040, 0), (15980822528, 5767168, 178782208, 0), (15969288192, 5767168, 184549376, 0), (16009658368, 5767168, 190316544, 0), (16015425536, 5767168, 196083712, 0), (16003891200, 5767168, 201850880, 0), (16044261376, 5767168, 207618048, 0), (16050028544, 5767168, 213385216, 0), (16038494208, 5767168, 219152384, 0), (15542517760, 5767168, 224919552, 0), (15548284928, 5767168, 230686720, 0), (15536750592, 5767168, 236453888, 0), (16113467392, 5767168, 242221056, 0), (16119234560, 5767168, 247988224, 0), (16107700224, 5767168, 253755392, 0), (15577120768, 5767168, 259522560, 0), (15582887936, 5767168, 265289728, 0), (15571353600, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e547ae220>, 2: <capsule object NULL at 0x7a4e547ae5b0>}
INFO 01-15 16:10:43.774696.774696 client.py:127] Model loaded
DEBUG 01-15 16:10:43.775860.775860 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.775943.775943 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.775925.775925 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.775064.775064 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 75c3678f-5f0f-45a5-9d96-5954c7278efe
DEBUG 01-15 16:10:43.775003.775003 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:43.775508.775508 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.775077.775077 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.776986.776986 cuda_h.py:19] end move_flatidxs cost 0.0009138584136962891 seconds
DEBUG 01-15 16:10:43.776449.776449 cuda_h.py:19] end restore2model cost 0.0015139579772949219 seconds
DEBUG 01-15 16:10:43.776379.776379 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.777275.777275 cuda_h.py:19] end sllm_worker_task cost 0.012430191040039062 seconds
INFO 01-15 16:10:43.777580.777580 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 75c3678f-5f0f-45a5-9d96-5954c7278efe
DEBUG 01-15 16:10:43.778191.778191 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004368305206298828 seconds
DEBUG 01-15 16:10:43.778061.778061 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.780930.780930 cuda_h.py:19] end restore2model cost 0.0025551319122314453 seconds
DEBUG 01-15 16:10:43.781396.781396 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007187604904174805 seconds
DEBUG 01-15 16:10:43.781668.781668 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.781360.781360 cuda_h.py:19] end gpu_sexperts cost 0.0002675056457519531 seconds
DEBUG 01-15 16:10:43.781474.781474 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.781774.781774 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:10:43.781185.781185 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.781080.781080 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.782068.782068 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008027553558349609 seconds
DEBUG 01-15 16:10:43.782911.782911 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.782733.782733 cuda_h.py:19] end gpu_group_list cost 0.00019168853759765625 seconds
DEBUG 01-15 16:10:43.783970.783970 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.784842.784842 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008649826049804688 seconds
DEBUG 01-15 16:10:43.784169.784169 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.784766.784766 cuda_h.py:19] end gpu_group_list cost 0.00020074844360351562 seconds
DEBUG 01-15 16:10:43.785560.785560 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.785390.785390 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 75c3678f-5f0f-45a5-9d96-5954c7278efe
DEBUG 01-15 16:10:43.789309.789309 cuda_h.py:19] end group_tensors cost 0.012058496475219727 seconds
DEBUG 01-15 16:10:43.789540.789540 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.793526.793526 cuda_h.py:19] end group pad cost 0.003695249557495117 seconds
DEBUG 01-15 16:10:43.793640.793640 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.805179.805179 client.py:127] Model loaded
DEBUG 01-15 16:10:43.805539.805539 cuda_h.py:19] end wait_experts_multi_device cost 0.020641088485717773 seconds
DEBUG 01-15 16:10:43.805920.805920 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.815638.815638 cuda_h.py:19] end group_einsum cost 0.021618127822875977 seconds
DEBUG 01-15 16:10:43.815470.815470 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.818254.818254 cuda_h.py:19] end get_outputs_cpu1 cost 0.003361940383911133 seconds
DEBUG 01-15 16:10:43.819436.819436 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04405641555786133 seconds
DEBUG 01-15 16:10:43.820815.820815 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013991355895996094 seconds
DEBUG 01-15 16:10:43.820620.820620 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.821822.821822 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.821423.821423 cuda_h.py:19] end index_scatter cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:10:43.821599.821599 cuda_h.py:19] end cpuoutputsdeal cost 0.0017011165618896484 seconds
DEBUG 01-15 16:10:43.821323.821323 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.822464.822464 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.822880.822880 cuda_h.py:19] end gpu_group_tensor cost 0.00013637542724609375 seconds
DEBUG 01-15 16:10:43.822735.822735 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.822422.822422 cuda_h.py:19] end gpu_group_tensor cost 0.00012612342834472656 seconds
DEBUG 01-15 16:10:43.822797.822797 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.822767.822767 cuda_h.py:19] end gpu_group_einsum cost 0.00045943260192871094 seconds
DEBUG 01-15 16:10:43.823447.823447 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.823076.823076 cuda_h.py:19] end gpu_group_einsum cost 0.0003445148468017578 seconds
DEBUG 01-15 16:10:43.823457.823457 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.823731.823731 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.823519.823519 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015783309936523438 seconds
DEBUG 01-15 16:10:43.823752.823752 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.823761.823761 cuda_h.py:19] end concat_expert_out cost 4.601478576660156e-05 seconds
DEBUG 01-15 16:10:43.824982.824982 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.824620.824620 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:43.824410.824410 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000728607177734375 seconds
DEBUG 01-15 16:10:43.824864.824864 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.824608.824608 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.824626.824626 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001246929168701172 seconds
DEBUG 01-15 16:10:43.824429.824429 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.824776.824776 cuda_h.py:19] end concat_expert_out cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:43.824380.824380 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.824780.824780 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:43.824636.824636 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046181678771972656 seconds
DEBUG 01-15 16:10:43.825016.825016 cuda_h.py:19] end gpu_experts_multi_device cost 0.043566226959228516 seconds
DEBUG 01-15 16:10:43.825833.825833 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.054633378982543945 seconds
DEBUG 01-15 16:10:43.825700.825700 cuda_h.py:19] end prefill_layer cost 0.06132173538208008 seconds
DEBUG 01-15 16:10:43.825775.825775 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:10:43.825716.825716 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.825373.825373 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:10:43.825221.825221 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:43.825739.825739 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:43.825543.825543 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 3.6716461181640625e-05 seconds
DEBUG 01-15 16:10:43.825914.825914 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 6.866455078125e-05 seconds
DEBUG 01-15 16:10:43.825518.825518 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.825978.825978 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.826428.826428 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.826623.826623 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.826750.826750 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.826012.826012 cuda_h.py:19] end allocate_cuda_memory cost 0.0003914833068847656 seconds
DEBUG 01-15 16:10:43.827229.827229 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.827140.827140 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.827263.827263 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.827471.827471 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d296beca-5903-4260-8365-e9ddc86b3e1d
DEBUG 01-15 16:10:43.827604.827604 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.827619.827619 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.829042.829042 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d296beca-5903-4260-8365-e9ddc86b3e1d
DEBUG 01-15 16:10:43.829860.829860 cuda_h.py:19] end load_into_gpu_async cost 0.0021202564239501953 seconds
DEBUG 01-15 16:10:43.829953.829953 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.829832.829832 cuda_h.py:19] end restore_tensors2 cost 8.797645568847656e-05 seconds
DEBUG 01-15 16:10:43.829548.829548 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0031347274780273438 seconds
INFO 01-15 16:10:43.829775.829775 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d296beca-5903-4260-8365-e9ddc86b3e1d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.831981.831981 cuda_h.py:19] end self_attn cost 0.003208637237548828 seconds
DEBUG 01-15 16:10:43.831952.831952 cuda_h.py:19] end iln_self_attn_paln cost 0.005625009536743164 seconds
DEBUG 01-15 16:10:43.831404.831404 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:10:43.831644.831644 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.832848.832848 cuda_h.py:19] end gate cost 0.0007090568542480469 seconds
DEBUG 01-15 16:10:43.832982.832982 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.832708.832708 lmp.py:1912] 
DEBUG 01-15 16:10:43.832708.832708 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.832610.832610 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.832452.832452 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.832718.832718 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.832599.832599 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.832242.832242 lmp.py:1917] 
DEBUG 01-15 16:10:43.832242.832242 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.832839.832839 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.832442.832442 lmp.py:1935]   Expert 42 |     21 | CPU
DEBUG 01-15 16:10:43.832708.832708 lmp.py:1935]   Expert 19 |     23 | CPU
DEBUG 01-15 16:10:43.832828.832828 lmp.py:1935]   Expert 30 |     26 | CPU
DEBUG 01-15 16:10:43.832709.832709 lmp.py:1935]   Expert 32 |     44 | CPU
DEBUG 01-15 16:10:43.832590.832590 lmp.py:1935]   Expert  6 |     58 | CPU
DEBUG 01-15 16:10:43.832233.832233 lmp.py:1935]   Expert  5 |     74 | CPU
DEBUG 01-15 16:10:43.832115.832115 lmp.py:1935]   Expert 53 |     74 | CPU
DEBUG 01-15 16:10:43.832996.832996 lmp.py:1935]   Expert  1 |     80 | CPU
DEBUG 01-15 16:10:43.832116.832116 lmp.py:1935]   Expert 13 |    118 | CPU
DEBUG 01-15 16:10:43.833236.833236 lmp.py:1935]   Expert  9 |    123 | CPU
DEBUG 01-15 16:10:43.833356.833356 lmp.py:1935]   Expert 63 |    125 | CPU
DEBUG 01-15 16:10:43.833237.833237 lmp.py:1935]   Expert 34 |    127 | CPU
DEBUG 01-15 16:10:43.833880.833880 lmp.py:1935]   Expert 58 |    130 | CPU
DEBUG 01-15 16:10:43.833046.833046 lmp.py:1935]   Expert 50 |    131 | CPU
DEBUG 01-15 16:10:43.833689.833689 lmp.py:1935]   Expert 18 |    136 | CPU
DEBUG 01-15 16:10:43.833094.833094 lmp.py:1935]   Expert 11 |    137 | CPU
DEBUG 01-15 16:10:43.833260.833260 lmp.py:1935]   Expert 31 |    137 | CPU
DEBUG 01-15 16:10:43.833426.833426 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:43.833830.833830 lmp.py:1935]   Expert 59 |    142 | CPU
DEBUG 01-15 16:10:43.833473.833473 lmp.py:1935]   Expert 40 |    144 | CPU
DEBUG 01-15 16:10:43.833355.833355 lmp.py:1935]   Expert 12 |    149 | CPU
DEBUG 01-15 16:10:43.833236.833236 lmp.py:1935]   Expert 46 |    151 | CPU
DEBUG 01-15 16:10:43.833118.833118 lmp.py:1935]   Expert 48 |    151 | CPU
DEBUG 01-15 16:10:43.833999.833999 lmp.py:1935]   Expert 20 |    152 | CPU
DEBUG 01-15 16:10:43.833165.833165 lmp.py:1935]   Expert  2 |    154 | CPU
DEBUG 01-15 16:10:43.833570.833570 lmp.py:1935]   Expert  4 |    154 | CPU
DEBUG 01-15 16:10:43.833736.833736 lmp.py:1935]   Expert 33 |    154 | CPU
DEBUG 01-15 16:10:43.833140.833140 lmp.py:1935]   Expert 56 |    154 | CPU
DEBUG 01-15 16:10:43.833545.833545 lmp.py:1935]   Expert 61 |    156 | CPU
DEBUG 01-15 16:10:43.833949.833949 lmp.py:1935]   Expert 35 |    163 | CPU
DEBUG 01-15 16:10:43.833116.833116 lmp.py:1935]   Expert 10 |    169 | CPU
DEBUG 01-15 16:10:43.833235.833235 lmp.py:1935]   Expert 55 |    170 | CPU
DEBUG 01-15 16:10:43.833263.833263 lmp.py:1935]   Expert 51 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833290.833290 lmp.py:1935]   Expert  8 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833363.833363 lmp.py:1935]   Expert 36 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833198.833198 lmp.py:1935]   Expert 52 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833033.833033 lmp.py:1935]   Expert 37 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833868.833868 lmp.py:1935]   Expert  0 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833465.833465 lmp.py:1935]   Expert 57 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833096.833096 lmp.py:1935]   Expert 39 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833977.833977 lmp.py:1935]   Expert 25 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833812.833812 lmp.py:1935]   Expert 62 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833647.833647 lmp.py:1935]   Expert 38 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833257.833257 lmp.py:1935]   Expert  3 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833377.833377 lmp.py:1935]   Expert  7 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833782.833782 lmp.py:1935]   Expert 24 |    250 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833425.833425 lmp.py:1935]   Expert 27 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833829.833829 lmp.py:1935]   Expert 28 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833234.833234 lmp.py:1935]   Expert 60 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833638.833638 lmp.py:1935]   Expert 21 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833758.833758 lmp.py:1935]   Expert 49 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833640.833640 lmp.py:1935]   Expert 16 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833521.833521 lmp.py:1935]   Expert 43 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833164.833164 lmp.py:1935]   Expert 23 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833807.833807 lmp.py:1935]   Expert 29 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833735.833735 lmp.py:1935]   Expert 22 |    291 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833901.833901 lmp.py:1935]   Expert 15 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833305.833305 lmp.py:1935]   Expert 47 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833472.833472 lmp.py:1935]   Expert 41 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833638.833638 lmp.py:1935]   Expert 44 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833042.833042 lmp.py:1935]   Expert 54 |    355 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.833447.833447 lmp.py:1935]   Expert 14 |    375 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.833567.833567 lmp.py:1935]   Expert 17 |    408 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.834210.834210 lmp.py:1935]   Expert 45 |    452 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.834137.834137 lmp.py:1937] 
DEBUG 01-15 16:10:43.834137.834137 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.834019.834019 lmp.py:1938]   CPU:   3865 tokens
DEBUG 01-15 16:10:43.834662.834662 lmp.py:1942]   cuda:1:   4213 tokens (16 experts)
DEBUG 01-15 16:10:43.834066.834066 lmp.py:1942]   cuda:2:   4210 tokens (16 experts)
DEBUG 01-15 16:10:43.834517.834517 lmp.py:1943]   Total GPU:   8423 tokens
DEBUG 01-15 16:10:43.834491.834491 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.834491.834491 lmp.py:1944] 
DEBUG 01-15 16:10:43.834664.834664 cuda_h.py:19] end experts_map_get cost 0.0017273426055908203 seconds
DEBUG 01-15 16:10:43.834752.834752 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.834839.834839 lmp.py:1953] 
DEBUG 01-15 16:10:43.834839.834839 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.834907.834907 cuda_h.py:19] end cpu_experts_submit cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:43.834955.834955 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.834115.834115 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.836967.836967 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.28.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.28.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.28.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.51.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.51.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.51.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.54.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.54.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.54.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.13.mlp.experts.3.gate_proj.weight': 0, 'model.layers.13.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.17.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.17.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.17.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.22.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.22.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.22.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.25.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.25.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.25.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.27.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.27.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.27.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16770924544, 5767168, 17301504, 0), (16776691712, 5767168, 23068672, 0), (16765157376, 5767168, 28835840, 0), (16805527552, 5767168, 34603008, 0), (16811294720, 5767168, 40370176, 0), (16799760384, 5767168, 46137344, 0), (16857432064, 5767168, 51904512, 0), (16863199232, 5767168, 57671680, 0), (16851664896, 5767168, 63438848, 0), (16926638080, 5767168, 69206016, 0), (16932405248, 5767168, 74973184, 0), (16920870912, 5767168, 80740352, 0), (16632512512, 5767168, 86507520, 0), (16638279680, 5767168, 92274688, 0), (16626745344, 5767168, 98041856, 0), (16407592960, 5767168, 103809024, 0), (16413360128, 5767168, 109576192, 0), (16401825792, 5767168, 115343360, 0), (16995844096, 5767168, 121110528, 0), (17001611264, 5767168, 126877696, 0), (16990076928, 5767168, 132644864, 0), (17030447104, 5767168, 138412032, 0), (17036214272, 5767168, 144179200, 0), (17024679936, 5767168, 149946368, 0), (17082351616, 5767168, 155713536, 0), (17088118784, 5767168, 161480704, 0), (17076584448, 5767168, 167247872, 0), (16546004992, 5767168, 173015040, 0), (16551772160, 5767168, 178782208, 0), (16540237824, 5767168, 184549376, 0), (16563306496, 5767168, 190316544, 0), (16569073664, 5767168, 196083712, 0), (16557539328, 5767168, 201850880, 0), (17134256128, 5767168, 207618048, 0), (17140023296, 5767168, 213385216, 0), (17128488960, 5767168, 219152384, 0), (17186160640, 5767168, 224919552, 0), (17191927808, 5767168, 230686720, 0), (17180393472, 5767168, 236453888, 0), (16649814016, 5767168, 242221056, 0), (16655581184, 5767168, 247988224, 0), (16644046848, 5767168, 253755392, 0), (17220763648, 5767168, 259522560, 0), (17226530816, 5767168, 265289728, 0), (17214996480, 5767168, 271056896, 0)], 2: [(16199974912, 5767168, 0, 0), (16205742080, 5767168, 5767168, 0), (16194207744, 5767168, 11534336, 0), (16788226048, 5767168, 17301504, 0), (16793993216, 5767168, 23068672, 0), (16782458880, 5767168, 28835840, 0), (16269180928, 5767168, 34603008, 0), (16274948096, 5767168, 40370176, 0), (16263413760, 5767168, 46137344, 0), (16822829056, 5767168, 51904512, 0), (16828596224, 5767168, 57671680, 0), (16817061888, 5767168, 63438848, 0), (16286482432, 5767168, 69206016, 0), (16292249600, 5767168, 74973184, 0), (16280715264, 5767168, 80740352, 0), (16892035072, 5767168, 86507520, 0), (16897802240, 5767168, 92274688, 0), (16886267904, 5767168, 98041856, 0), (16909336576, 5767168, 103809024, 0), (16915103744, 5767168, 109576192, 0), (16903569408, 5767168, 115343360, 0), (16390291456, 5767168, 121110528, 0), (16396058624, 5767168, 126877696, 0), (16384524288, 5767168, 132644864, 0), (16961241088, 5767168, 138412032, 0), (16967008256, 5767168, 144179200, 0), (16955473920, 5767168, 149946368, 0), (16424894464, 5767168, 155713536, 0), (16430661632, 5767168, 161480704, 0), (16419127296, 5767168, 167247872, 0), (16442195968, 5767168, 173015040, 0), (16447963136, 5767168, 178782208, 0), (16436428800, 5767168, 184549376, 0), (17047748608, 5767168, 190316544, 0), (17053515776, 5767168, 196083712, 0), (17041981440, 5767168, 201850880, 0), (16511401984, 5767168, 207618048, 0), (16517169152, 5767168, 213385216, 0), (16505634816, 5767168, 219152384, 0), (16528703488, 5767168, 224919552, 0), (16534470656, 5767168, 230686720, 0), (16522936320, 5767168, 236453888, 0), (16580608000, 5767168, 242221056, 0), (16586375168, 5767168, 247988224, 0), (16574840832, 5767168, 253755392, 0), (16615211008, 5767168, 259522560, 0), (16620978176, 5767168, 265289728, 0), (16609443840, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c23adc0>, 2: <capsule object NULL at 0x7a4e547ae730>}
DEBUG 01-15 16:10:43.836058.836058 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.836179.836179 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.836074.836074 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 42bceeec-67ba-40e5-b2d1-06db2f37498b
DEBUG 01-15 16:10:43.837856.837856 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.837666.837666 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:43.837159.837159 client.py:127] Model loaded
DEBUG 01-15 16:10:43.837750.837750 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.837537.837537 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.837597.837597 cuda_h.py:19] end restore2model cost 0.0003478527069091797 seconds
DEBUG 01-15 16:10:43.837651.837651 cuda_h.py:19] end sllm_worker_task cost 0.011566400527954102 seconds
INFO 01-15 16:10:43.838696.838696 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 42bceeec-67ba-40e5-b2d1-06db2f37498b
DEBUG 01-15 16:10:43.838189.838189 cuda_h.py:19] end move_flatidxs cost 0.0008425712585449219 seconds
DEBUG 01-15 16:10:43.838588.838588 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.838903.838903 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00430750846862793 seconds
DEBUG 01-15 16:10:43.838773.838773 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.841972.841972 cuda_h.py:19] end restore2model cost 0.0025343894958496094 seconds
DEBUG 01-15 16:10:43.841815.841815 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007072925567626953 seconds
DEBUG 01-15 16:10:43.841564.841564 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.841972.841972 cuda_h.py:19] end gpu_sexperts cost 0.00026917457580566406 seconds
DEBUG 01-15 16:10:43.841563.841563 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.841908.841908 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.4543533325195312e-05 seconds
DEBUG 01-15 16:10:43.841472.841472 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.841414.841414 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.842362.842362 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008089542388916016 seconds
DEBUG 01-15 16:10:43.842590.842590 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.842206.842206 cuda_h.py:19] end gpu_group_list cost 0.00017952919006347656 seconds
DEBUG 01-15 16:10:43.843065.843065 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.844905.844905 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009105205535888672 seconds
DEBUG 01-15 16:10:43.844445.844445 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.844392.844392 cuda_h.py:19] end gpu_group_list cost 0.00017571449279785156 seconds
DEBUG 01-15 16:10:43.845372.845372 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.845916.845916 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 42bceeec-67ba-40e5-b2d1-06db2f37498b
DEBUG 01-15 16:10:43.847737.847737 cuda_h.py:19] end group_tensors cost 0.009337663650512695 seconds
DEBUG 01-15 16:10:43.848578.848578 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.852693.852693 cuda_h.py:19] end group pad cost 0.0037708282470703125 seconds
DEBUG 01-15 16:10:43.852444.852444 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.865257.865257 client.py:127] Model loaded
DEBUG 01-15 16:10:43.866071.866071 cuda_h.py:19] end wait_experts_multi_device cost 0.02041482925415039 seconds
DEBUG 01-15 16:10:43.866147.866147 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.871164.871164 cuda_h.py:19] end group_einsum cost 0.019446372985839844 seconds
DEBUG 01-15 16:10:43.872519.872519 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.875806.875806 cuda_h.py:19] end get_outputs_cpu1 cost 0.0035610198974609375 seconds
DEBUG 01-15 16:10:43.876509.876509 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03935647010803223 seconds
DEBUG 01-15 16:10:43.876323.876323 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010711669921875 seconds
DEBUG 01-15 16:10:43.877751.877751 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.878067.878067 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.878648.878648 cuda_h.py:19] end index_scatter cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:43.878678.878678 cuda_h.py:19] end cpuoutputsdeal cost 0.001558542251586914 seconds
DEBUG 01-15 16:10:43.878356.878356 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.878735.878735 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.878251.878251 cuda_h.py:19] end gpu_group_tensor cost 0.0001399517059326172 seconds
DEBUG 01-15 16:10:43.878821.878821 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.879555.879555 cuda_h.py:19] end gpu_group_tensor cost 0.00012540817260742188 seconds
DEBUG 01-15 16:10:43.879360.879360 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.879984.879984 cuda_h.py:19] end gpu_group_einsum cost 0.0005757808685302734 seconds
DEBUG 01-15 16:10:43.880750.880750 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.880681.880681 cuda_h.py:19] end gpu_group_einsum cost 0.0004553794860839844 seconds
DEBUG 01-15 16:10:43.880254.880254 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.880099.880099 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.880410.880410 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015735626220703125 seconds
DEBUG 01-15 16:10:43.880642.880642 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.881844.881844 cuda_h.py:19] end concat_expert_out cost 4.76837158203125e-05 seconds
DEBUG 01-15 16:10:43.881349.881349 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.881988.881988 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:43.881929.881929 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007328987121582031 seconds
DEBUG 01-15 16:10:43.881674.881674 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.881133.881133 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.881258.881258 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012969970703125 seconds
DEBUG 01-15 16:10:43.881060.881060 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.881030.881030 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:43.881204.881204 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.881028.881028 cuda_h.py:19] end index_scatter cost 4.839897155761719e-05 seconds
DEBUG 01-15 16:10:43.882883.882883 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004699230194091797 seconds
DEBUG 01-15 16:10:43.882879.882879 cuda_h.py:19] end gpu_experts_multi_device cost 0.04019927978515625 seconds
DEBUG 01-15 16:10:43.882219.882219 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.05060291290283203 seconds
DEBUG 01-15 16:10:43.882318.882318 cuda_h.py:19] end prefill_layer cost 0.056906938552856445 seconds
DEBUG 01-15 16:10:43.882770.882770 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:10:43.882188.882188 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.882845.882845 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:10:43.882455.882455 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:43.882781.882781 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:43.882445.882445 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 4.076957702636719e-05 seconds
DEBUG 01-15 16:10:43.882817.882817 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:43.882513.882513 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.882900.882900 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.883572.883572 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.883684.883684 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.883266.883266 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.883316.883316 cuda_h.py:19] end allocate_cuda_memory cost 0.00040078163146972656 seconds
DEBUG 01-15 16:10:43.884945.884945 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.884796.884796 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.884920.884920 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.884049.884049 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c55b43ae-48fb-4efa-883f-d5ca9270e36d
DEBUG 01-15 16:10:43.884150.884150 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.884563.884563 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.886974.886974 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c55b43ae-48fb-4efa-883f-d5ca9270e36d
DEBUG 01-15 16:10:43.886534.886534 cuda_h.py:19] end load_into_gpu_async cost 0.0026557445526123047 seconds
DEBUG 01-15 16:10:43.886264.886264 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.886414.886414 cuda_h.py:19] end restore_tensors2 cost 8.845329284667969e-05 seconds
DEBUG 01-15 16:10:43.887582.887582 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003754854202270508 seconds
INFO 01-15 16:10:43.887101.887101 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c55b43ae-48fb-4efa-883f-d5ca9270e36d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.888669.888669 cuda_h.py:19] end self_attn cost 0.0033278465270996094 seconds
DEBUG 01-15 16:10:43.888124.888124 cuda_h.py:19] end iln_self_attn_paln cost 0.005755186080932617 seconds
DEBUG 01-15 16:10:43.888815.888815 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:10:43.888293.888293 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.889058.889058 cuda_h.py:19] end gate cost 0.0006661415100097656 seconds
DEBUG 01-15 16:10:43.889994.889994 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.889395.889395 lmp.py:1912] 
DEBUG 01-15 16:10:43.889395.889395 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.889013.889013 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.889378.889378 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.889120.889120 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.889955.889955 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.889075.889075 lmp.py:1917] 
DEBUG 01-15 16:10:43.889075.889075 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.889148.889148 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.889229.889229 lmp.py:1935]   Expert 34 |     27 | CPU
DEBUG 01-15 16:10:43.889587.889587 lmp.py:1935]   Expert  7 |     33 | CPU
DEBUG 01-15 16:10:43.889230.889230 lmp.py:1935]   Expert 13 |     42 | CPU
DEBUG 01-15 16:10:43.890873.890873 lmp.py:1935]   Expert 54 |     77 | CPU
DEBUG 01-15 16:10:43.890754.890754 lmp.py:1935]   Expert 18 |     84 | CPU
DEBUG 01-15 16:10:43.890397.890397 lmp.py:1935]   Expert 49 |     87 | CPU
DEBUG 01-15 16:10:43.890279.890279 lmp.py:1935]   Expert 39 |     89 | CPU
DEBUG 01-15 16:10:43.890637.890637 lmp.py:1935]   Expert 59 |    102 | CPU
DEBUG 01-15 16:10:43.890995.890995 lmp.py:1935]   Expert  0 |    108 | CPU
DEBUG 01-15 16:10:43.890353.890353 lmp.py:1935]   Expert 16 |    108 | CPU
DEBUG 01-15 16:10:43.890711.890711 lmp.py:1935]   Expert 21 |    108 | CPU
DEBUG 01-15 16:10:43.890831.890831 lmp.py:1935]   Expert 41 |    116 | CPU
DEBUG 01-15 16:10:43.890918.890918 lmp.py:1935]   Expert 22 |    121 | CPU
DEBUG 01-15 16:10:43.890561.890561 lmp.py:1935]   Expert 45 |    121 | CPU
DEBUG 01-15 16:10:43.890727.890727 lmp.py:1935]   Expert 15 |    122 | CPU
DEBUG 01-15 16:10:43.890132.890132 lmp.py:1935]   Expert 17 |    126 | CPU
DEBUG 01-15 16:10:43.890537.890537 lmp.py:1935]   Expert 61 |    133 | CPU
DEBUG 01-15 16:10:43.890703.890703 lmp.py:1935]   Expert 52 |    136 | CPU
DEBUG 01-15 16:10:43.890869.890869 lmp.py:1935]   Expert  8 |    137 | CPU
DEBUG 01-15 16:10:43.890558.890558 lmp.py:1935]   Expert 35 |    137 | CPU
DEBUG 01-15 16:10:43.890963.890963 lmp.py:1935]   Expert 38 |    137 | CPU
DEBUG 01-15 16:10:43.890367.890367 lmp.py:1935]   Expert 12 |    142 | CPU
DEBUG 01-15 16:10:43.890725.890725 lmp.py:1935]   Expert 48 |    148 | CPU
DEBUG 01-15 16:10:43.890607.890607 lmp.py:1935]   Expert 31 |    149 | CPU
DEBUG 01-15 16:10:43.890488.890488 lmp.py:1935]   Expert 53 |    151 | CPU
DEBUG 01-15 16:10:43.890416.890416 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:10:43.890582.890582 lmp.py:1935]   Expert 50 |    160 | CPU
DEBUG 01-15 16:10:43.890748.890748 lmp.py:1935]   Expert 40 |    161 | CPU
DEBUG 01-15 16:10:43.890153.890153 lmp.py:1935]   Expert 60 |    161 | CPU
DEBUG 01-15 16:10:43.890319.890319 lmp.py:1935]   Expert 27 |    175 | CPU
DEBUG 01-15 16:10:43.890724.890724 lmp.py:1935]   Expert 19 |    195 | CPU
DEBUG 01-15 16:10:43.890890.890890 lmp.py:1935]   Expert  4 |    200 | CPU
DEBUG 01-15 16:10:43.890963.890963 lmp.py:1935]   Expert 29 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890275.890275 lmp.py:1935]   Expert 30 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890110.890110 lmp.py:1935]   Expert 11 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890945.890945 lmp.py:1935]   Expert 26 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890456.890456 lmp.py:1935]   Expert 20 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890099.890099 lmp.py:1935]   Expert 57 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890265.890265 lmp.py:1935]   Expert  6 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890193.890193 lmp.py:1935]   Expert 46 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890882.890882 lmp.py:1935]   Expert 43 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890333.890333 lmp.py:1935]   Expert 23 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890784.890784 lmp.py:1935]   Expert  2 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890473.890473 lmp.py:1935]   Expert 33 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890162.890162 lmp.py:1935]   Expert 42 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890044.890044 lmp.py:1935]   Expert 55 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890448.890448 lmp.py:1935]   Expert 32 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890376.890376 lmp.py:1935]   Expert 56 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890781.890781 lmp.py:1935]   Expert  9 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890708.890708 lmp.py:1935]   Expert  3 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890159.890159 lmp.py:1935]   Expert 28 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890849.890849 lmp.py:1935]   Expert 14 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890299.890299 lmp.py:1935]   Expert  1 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890750.890750 lmp.py:1935]   Expert 51 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890724.890724 lmp.py:1935]   Expert 58 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890175.890175 lmp.py:1935]   Expert 44 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890865.890865 lmp.py:1935]   Expert 63 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.890700.890700 lmp.py:1935]   Expert 37 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.890343.890343 lmp.py:1935]   Expert 47 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.891986.891986 lmp.py:1935]   Expert 24 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.891867.891867 lmp.py:1935]   Expert 62 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.891556.891556 lmp.py:1935]   Expert 10 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.891246.891246 lmp.py:1935]   Expert 25 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.891696.891696 lmp.py:1935]   Expert  5 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.891194.891194 lmp.py:1937] 
DEBUG 01-15 16:10:43.891194.891194 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.891645.891645 lmp.py:1938]   CPU:   3948 tokens
DEBUG 01-15 16:10:43.891334.891334 lmp.py:1942]   cuda:1:   4170 tokens (16 experts)
DEBUG 01-15 16:10:43.891262.891262 lmp.py:1942]   cuda:2:   4170 tokens (16 experts)
DEBUG 01-15 16:10:43.891951.891951 lmp.py:1943]   Total GPU:   8340 tokens
DEBUG 01-15 16:10:43.891687.891687 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.891687.891687 lmp.py:1944] 
DEBUG 01-15 16:10:43.891098.891098 cuda_h.py:19] end experts_map_get cost 0.0016963481903076172 seconds
DEBUG 01-15 16:10:43.891994.891994 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.891366.891366 lmp.py:1953] 
DEBUG 01-15 16:10:43.891366.891366 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.891434.891434 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:43.891673.891673 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.891311.891311 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.893025.893025 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.2.gate_proj.weight': 0, 'model.layers.14.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.51.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.51.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.51.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.56.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.56.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.56.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.14.mlp.experts.32.gate_proj.weight': 0, 'model.layers.14.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.33.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.33.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.33.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.3.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.3.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.3.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.57.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(17289969664, 5767168, 0, 0), (17295736832, 5767168, 5767168, 0), (17284202496, 5767168, 11534336, 0), (17341874176, 5767168, 17301504, 0), (17347641344, 5767168, 23068672, 0), (17336107008, 5767168, 28835840, 0), (17359175680, 5767168, 34603008, 0), (17364942848, 5767168, 40370176, 0), (17353408512, 5767168, 46137344, 0), (17411080192, 5767168, 51904512, 0), (17416847360, 5767168, 57671680, 0), (17405313024, 5767168, 63438848, 0), (17982029824, 5767168, 69206016, 0), (17987796992, 5767168, 74973184, 0), (17976262656, 5767168, 80740352, 0), (17999331328, 5767168, 86507520, 0), (18005098496, 5767168, 92274688, 0), (17993564160, 5767168, 98041856, 0), (17445683200, 5767168, 103809024, 0), (17451450368, 5767168, 109576192, 0), (17439916032, 5767168, 115343360, 0), (18068537344, 5767168, 121110528, 0), (18074304512, 5767168, 126877696, 0), (18062770176, 5767168, 132644864, 0), (18137743360, 5767168, 138412032, 0), (18143510528, 5767168, 144179200, 0), (18131976192, 5767168, 149946368, 0), (17601396736, 5767168, 155713536, 0), (17607163904, 5767168, 161480704, 0), (17595629568, 5767168, 167247872, 0), (18224250880, 5767168, 173015040, 0), (18230018048, 5767168, 178782208, 0), (18218483712, 5767168, 184549376, 0), (18258853888, 5767168, 190316544, 0), (18264621056, 5767168, 196083712, 0), (18253086720, 5767168, 201850880, 0), (17739808768, 5767168, 207618048, 0), (17745575936, 5767168, 213385216, 0), (17734041600, 5767168, 219152384, 0), (17757110272, 5767168, 224919552, 0), (17762877440, 5767168, 230686720, 0), (17751343104, 5767168, 236453888, 0), (18328059904, 5767168, 242221056, 0), (18333827072, 5767168, 247988224, 0), (18322292736, 5767168, 253755392, 0), (18345361408, 5767168, 259522560, 0), (18351128576, 5767168, 265289728, 0), (18339594240, 5767168, 271056896, 0)], 2: [(17809014784, 5767168, 0, 0), (17814781952, 5767168, 5767168, 0), (17803247616, 5767168, 11534336, 0), (17272668160, 5767168, 17301504, 0), (17278435328, 5767168, 23068672, 0), (17266900992, 5767168, 28835840, 0), (17826316288, 5767168, 34603008, 0), (17832083456, 5767168, 40370176, 0), (17820549120, 5767168, 46137344, 0), (17307271168, 5767168, 51904512, 0), (17313038336, 5767168, 57671680, 0), (17301504000, 5767168, 63438848, 0), (17895522304, 5767168, 69206016, 0), (17901289472, 5767168, 74973184, 0), (17889755136, 5767168, 80740352, 0), (17428381696, 5767168, 86507520, 0), (17434148864, 5767168, 92274688, 0), (17422614528, 5767168, 98041856, 0), (18016632832, 5767168, 103809024, 0), (18022400000, 5767168, 109576192, 0), (18010865664, 5767168, 115343360, 0), (17497587712, 5767168, 121110528, 0), (17503354880, 5767168, 126877696, 0), (17491820544, 5767168, 132644864, 0), (18051235840, 5767168, 138412032, 0), (18057003008, 5767168, 144179200, 0), (18045468672, 5767168, 149946368, 0), (17653301248, 5767168, 155713536, 0), (17659068416, 5767168, 161480704, 0), (17647534080, 5767168, 167247872, 0), (18206949376, 5767168, 173015040, 0), (18212716544, 5767168, 178782208, 0), (18201182208, 5767168, 184549376, 0), (17670602752, 5767168, 190316544, 0), (17676369920, 5767168, 196083712, 0), (17664835584, 5767168, 201850880, 0), (17687904256, 5767168, 207618048, 0), (17693671424, 5767168, 213385216, 0), (17682137088, 5767168, 219152384, 0), (17705205760, 5767168, 224919552, 0), (17710972928, 5767168, 230686720, 0), (17699438592, 5767168, 236453888, 0), (17774411776, 5767168, 242221056, 0), (17780178944, 5767168, 247988224, 0), (17768644608, 5767168, 253755392, 0), (18241552384, 5767168, 259522560, 0), (18247319552, 5767168, 265289728, 0), (18235785216, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c2c2880>, 2: <capsule object NULL at 0x7a4e547ae190>}
DEBUG 01-15 16:10:43.893413.893413 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.894056.894056 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.894580.894580 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 923871f4-ee13-46df-a64f-c950e8c53728
DEBUG 01-15 16:10:43.894269.894269 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.894624.894624 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:43.894155.894155 client.py:127] Model loaded
DEBUG 01-15 16:10:43.894660.894660 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.894311.894311 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.895653.895653 cuda_h.py:19] end restore2model cost 0.00034737586975097656 seconds
DEBUG 01-15 16:10:43.895899.895899 cuda_h.py:19] end sllm_worker_task cost 0.011981964111328125 seconds
INFO 01-15 16:10:43.895605.895605 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 923871f4-ee13-46df-a64f-c950e8c53728
DEBUG 01-15 16:10:43.895554.895554 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004466533660888672 seconds
DEBUG 01-15 16:10:43.895886.895886 cuda_h.py:19] end move_flatidxs cost 0.0008747577667236328 seconds
DEBUG 01-15 16:10:43.895947.895947 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.896577.896577 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.898662.898662 cuda_h.py:19] end restore2model cost 0.0025255680084228516 seconds
DEBUG 01-15 16:10:43.898440.898440 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007249593734741211 seconds
DEBUG 01-15 16:10:43.898712.898712 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.898835.898835 cuda_h.py:19] end gpu_sexperts cost 0.00026917457580566406 seconds
DEBUG 01-15 16:10:43.899426.899426 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.899487.899487 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:43.899945.899945 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.899933.899933 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.899637.899637 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008394718170166016 seconds
DEBUG 01-15 16:10:43.900547.900547 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.900872.900872 cuda_h.py:19] end gpu_group_list cost 0.00017571449279785156 seconds
DEBUG 01-15 16:10:43.901103.901103 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.901139.901139 cuda_h.py:19] end group_tensors cost 0.005284547805786133 seconds
DEBUG 01-15 16:10:43.901777.901777 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008828639984130859 seconds
DEBUG 01-15 16:10:43.902136.902136 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.902687.902687 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.902321.902321 cuda_h.py:19] end gpu_group_list cost 0.0002951622009277344 seconds
DEBUG 01-15 16:10:43.903818.903818 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.903535.903535 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 923871f4-ee13-46df-a64f-c950e8c53728
DEBUG 01-15 16:10:43.905018.905018 cuda_h.py:19] end group pad cost 0.0037186145782470703 seconds
DEBUG 01-15 16:10:43.905324.905324 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.923828.923828 client.py:127] Model loaded
DEBUG 01-15 16:10:43.923341.923341 cuda_h.py:19] end wait_experts_multi_device cost 0.02025628089904785 seconds
DEBUG 01-15 16:10:43.924106.924106 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.927099.927099 cuda_h.py:19] end group_einsum cost 0.021277427673339844 seconds
DEBUG 01-15 16:10:43.927745.927745 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.930145.930145 cuda_h.py:19] end get_outputs_cpu1 cost 0.0035593509674072266 seconds
DEBUG 01-15 16:10:43.931013.931013 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03685474395751953 seconds
DEBUG 01-15 16:10:43.931530.931530 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00784611701965332 seconds
DEBUG 01-15 16:10:43.932474.932474 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.933368.933368 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.933539.933539 cuda_h.py:19] end index_scatter cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:10:43.933053.933053 cuda_h.py:19] end cpuoutputsdeal cost 0.0016160011291503906 seconds
DEBUG 01-15 16:10:43.933823.933823 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.933679.933679 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.934526.934526 cuda_h.py:19] end gpu_group_tensor cost 0.00013756752014160156 seconds
DEBUG 01-15 16:10:43.934904.934904 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.934062.934062 cuda_h.py:19] end gpu_group_tensor cost 0.00012254714965820312 seconds
DEBUG 01-15 16:10:43.934118.934118 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.934675.934675 cuda_h.py:19] end gpu_group_einsum cost 0.0005700588226318359 seconds
DEBUG 01-15 16:10:43.935442.935442 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.935577.935577 cuda_h.py:19] end gpu_group_einsum cost 0.00041794776916503906 seconds
DEBUG 01-15 16:10:43.935813.935813 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.935657.935657 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.935206.935206 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001583099365234375 seconds
DEBUG 01-15 16:10:43.935962.935962 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.936780.936780 cuda_h.py:19] end concat_expert_out cost 4.553794860839844e-05 seconds
DEBUG 01-15 16:10:43.936285.936285 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.936447.936447 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:43.936309.936309 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007421970367431641 seconds
DEBUG 01-15 16:10:43.936915.936915 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.936705.936705 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.936869.936869 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012540817260742188 seconds
DEBUG 01-15 16:10:43.936910.936910 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.936164.936164 cuda_h.py:19] end concat_expert_out cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:43.936292.936292 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.937262.937262 cuda_h.py:19] end index_scatter cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:43.937402.937402 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004649162292480469 seconds
DEBUG 01-15 16:10:43.937835.937835 cuda_h.py:19] end gpu_experts_multi_device cost 0.03802847862243652 seconds
DEBUG 01-15 16:10:43.937460.937460 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.04851198196411133 seconds
DEBUG 01-15 16:10:43.937943.937943 cuda_h.py:19] end prefill_layer cost 0.054949045181274414 seconds
DEBUG 01-15 16:10:43.937634.937634 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:10:43.937337.937337 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.937278.937278 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:10:43.937173.937173 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:43.937214.937214 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:43.937209.937209 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 3.886222839355469e-05 seconds
DEBUG 01-15 16:10:43.937965.937965 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:43.937377.937377 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.937624.937624 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.938851.938851 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.938677.938677 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.938943.938943 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.938284.938284 cuda_h.py:19] end allocate_cuda_memory cost 0.00024700164794921875 seconds
DEBUG 01-15 16:10:43.938147.938147 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.938963.938963 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.938078.938078 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.938171.938171 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c566b635-ea5d-472b-8a10-ce8b6372c5e9
DEBUG 01-15 16:10:43.938884.938884 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.939390.939390 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.940615.940615 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c566b635-ea5d-472b-8a10-ce8b6372c5e9
DEBUG 01-15 16:10:43.940286.940286 cuda_h.py:19] end load_into_gpu_async cost 0.001537322998046875 seconds
DEBUG 01-15 16:10:43.940665.940665 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.940066.940066 cuda_h.py:19] end restore_tensors2 cost 8.749961853027344e-05 seconds
DEBUG 01-15 16:10:43.940021.940021 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021512508392333984 seconds
INFO 01-15 16:10:43.940169.940169 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c566b635-ea5d-472b-8a10-ce8b6372c5e9
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.942056.942056 cuda_h.py:19] end self_attn cost 0.003795146942138672 seconds
DEBUG 01-15 16:10:43.943074.943074 cuda_h.py:19] end iln_self_attn_paln cost 0.0053615570068359375 seconds
DEBUG 01-15 16:10:43.943334.943334 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:10:43.943573.943573 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.944778.944778 cuda_h.py:19] end gate cost 0.0007448196411132812 seconds
DEBUG 01-15 16:10:43.944237.944237 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.944673.944673 lmp.py:1912] 
DEBUG 01-15 16:10:43.944673.944673 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.944721.944721 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.944039.944039 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.944782.944782 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.944140.944140 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.944260.944260 lmp.py:1917] 
DEBUG 01-15 16:10:43.944260.944260 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.944141.944141 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.944268.944268 lmp.py:1935]   Expert 15 |     64 | CPU
DEBUG 01-15 16:10:43.944672.944672 lmp.py:1935]   Expert 41 |     72 | CPU
DEBUG 01-15 16:10:43.944838.944838 lmp.py:1935]   Expert  0 |     75 | CPU
DEBUG 01-15 16:10:43.944528.944528 lmp.py:1935]   Expert 63 |     76 | CPU
DEBUG 01-15 16:10:43.944217.944217 lmp.py:1935]   Expert 20 |     84 | CPU
DEBUG 01-15 16:10:43.944814.944814 lmp.py:1935]   Expert 45 |     91 | CPU
DEBUG 01-15 16:10:43.944649.944649 lmp.py:1935]   Expert  7 |     92 | CPU
DEBUG 01-15 16:10:43.944769.944769 lmp.py:1935]   Expert 28 |     98 | CPU
DEBUG 01-15 16:10:43.944080.944080 lmp.py:1935]   Expert 12 |    106 | CPU
DEBUG 01-15 16:10:43.944439.944439 lmp.py:1935]   Expert 54 |    108 | CPU
DEBUG 01-15 16:10:43.944082.944082 lmp.py:1935]   Expert 40 |    122 | CPU
DEBUG 01-15 16:10:43.944725.944725 lmp.py:1935]   Expert 52 |    123 | CPU
DEBUG 01-15 16:10:43.944368.944368 lmp.py:1935]   Expert  5 |    124 | CPU
DEBUG 01-15 16:10:43.944011.944011 lmp.py:1935]   Expert 59 |    124 | CPU
DEBUG 01-15 16:10:43.944415.944415 lmp.py:1935]   Expert  4 |    131 | CPU
DEBUG 01-15 16:10:43.944820.944820 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:10:43.945463.945463 lmp.py:1935]   Expert 61 |    134 | CPU
DEBUG 01-15 16:10:43.945583.945583 lmp.py:1935]   Expert 13 |    137 | CPU
DEBUG 01-15 16:10:43.945941.945941 lmp.py:1935]   Expert 62 |    137 | CPU
DEBUG 01-15 16:10:43.945061.945061 lmp.py:1935]   Expert 55 |    139 | CPU
DEBUG 01-15 16:10:43.945180.945180 lmp.py:1935]   Expert 21 |    140 | CPU
DEBUG 01-15 16:10:43.945539.945539 lmp.py:1935]   Expert 42 |    141 | CPU
DEBUG 01-15 16:10:43.945943.945943 lmp.py:1935]   Expert 10 |    144 | CPU
DEBUG 01-15 16:10:43.945586.945586 lmp.py:1935]   Expert 14 |    146 | CPU
DEBUG 01-15 16:10:43.945991.945991 lmp.py:1935]   Expert 22 |    146 | CPU
DEBUG 01-15 16:10:43.945157.945157 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:10:43.945561.945561 lmp.py:1935]   Expert 32 |    156 | CPU
DEBUG 01-15 16:10:43.945966.945966 lmp.py:1935]   Expert 25 |    167 | CPU
DEBUG 01-15 16:10:43.945370.945370 lmp.py:1935]   Expert  1 |    173 | CPU
DEBUG 01-15 16:10:43.945775.945775 lmp.py:1935]   Expert 47 |    174 | CPU
DEBUG 01-15 16:10:43.945941.945941 lmp.py:1935]   Expert 50 |    175 | CPU
DEBUG 01-15 16:10:43.945107.945107 lmp.py:1935]   Expert 53 |    178 | CPU
DEBUG 01-15 16:10:43.945671.945671 lmp.py:1935]   Expert 26 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945029.945029 lmp.py:1935]   Expert 19 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945149.945149 lmp.py:1935]   Expert  6 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945269.945269 lmp.py:1935]   Expert  2 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945912.945912 lmp.py:1935]   Expert 11 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945986.945986 lmp.py:1935]   Expert 35 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945821.945821 lmp.py:1935]   Expert 30 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945940.945940 lmp.py:1935]   Expert 56 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945060.945060 lmp.py:1935]   Expert 57 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945180.945180 lmp.py:1935]   Expert 48 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945300.945300 lmp.py:1935]   Expert 16 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945943.945943 lmp.py:1935]   Expert 24 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945586.945586 lmp.py:1935]   Expert 44 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945229.945229 lmp.py:1935]   Expert 46 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945872.945872 lmp.py:1935]   Expert 39 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945276.945276 lmp.py:1935]   Expert 18 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945635.945635 lmp.py:1935]   Expert 29 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945993.945993 lmp.py:1935]   Expert 37 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945113.945113 lmp.py:1935]   Expert 31 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945232.945232 lmp.py:1935]   Expert 36 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945114.945114 lmp.py:1935]   Expert 60 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945995.945995 lmp.py:1935]   Expert  3 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945115.945115 lmp.py:1935]   Expert 38 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945758.945758 lmp.py:1935]   Expert  9 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945401.945401 lmp.py:1935]   Expert 17 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945044.945044 lmp.py:1935]   Expert 23 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945687.945687 lmp.py:1935]   Expert 27 |    345 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945284.945284 lmp.py:1935]   Expert 43 |    366 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945403.945403 lmp.py:1935]   Expert  8 |    396 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945762.945762 lmp.py:1935]   Expert 33 |    396 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945881.945881 lmp.py:1935]   Expert 58 |    447 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.945240.945240 lmp.py:1935]   Expert 49 |    543 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.945167.945167 lmp.py:1937] 
DEBUG 01-15 16:10:43.945167.945167 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.945572.945572 lmp.py:1938]   CPU:   4065 tokens
DEBUG 01-15 16:10:43.945930.945930 lmp.py:1942]   cuda:1:   4123 tokens (16 experts)
DEBUG 01-15 16:10:43.945335.945335 lmp.py:1942]   cuda:2:   4100 tokens (16 experts)
DEBUG 01-15 16:10:43.945262.945262 lmp.py:1943]   Total GPU:   8223 tokens
DEBUG 01-15 16:10:43.945952.945952 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.945952.945952 lmp.py:1944] 
DEBUG 01-15 16:10:43.945078.945078 cuda_h.py:19] end experts_map_get cost 0.0017473697662353516 seconds
DEBUG 01-15 16:10:43.946028.946028 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.946068.946068 lmp.py:1953] 
DEBUG 01-15 16:10:43.946068.946068 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.946328.946328 cuda_h.py:19] end cpu_experts_submit cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:43.946640.946640 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.946039.946039 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:43.946095.946095 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.33.gate_proj.weight': 0, 'model.layers.15.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.26.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.26.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.26.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.56.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.56.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.56.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.27.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.27.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.27.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.60.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.60.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.60.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.15.mlp.experts.2.gate_proj.weight': 0, 'model.layers.15.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.35.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.35.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.35.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.43.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.43.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.43.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.48.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.48.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.48.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.19.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.19.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.19.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.57.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.57.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.57.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.58.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.58.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.58.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(18933612544, 5767168, 0, 0), (18939379712, 5767168, 5767168, 0), (18927845376, 5767168, 11534336, 0), (18812502016, 5767168, 17301504, 0), (18818269184, 5767168, 23068672, 0), (18806734848, 5767168, 28835840, 0), (19020120064, 5767168, 34603008, 0), (19025887232, 5767168, 40370176, 0), (19014352896, 5767168, 46137344, 0), (19037421568, 5767168, 51904512, 0), (19043188736, 5767168, 57671680, 0), (19031654400, 5767168, 63438848, 0), (18466471936, 5767168, 69206016, 0), (18472239104, 5767168, 74973184, 0), (18460704768, 5767168, 80740352, 0), (18552979456, 5767168, 86507520, 0), (18558746624, 5767168, 92274688, 0), (18547212288, 5767168, 98041856, 0), (19123929088, 5767168, 103809024, 0), (19129696256, 5767168, 109576192, 0), (19118161920, 5767168, 115343360, 0), (19210436608, 5767168, 121110528, 0), (19216203776, 5767168, 126877696, 0), (19204669440, 5767168, 132644864, 0), (18656788480, 5767168, 138412032, 0), (18662555648, 5767168, 144179200, 0), (18651021312, 5767168, 149946368, 0), (18777899008, 5767168, 155713536, 0), (18783666176, 5767168, 161480704, 0), (18772131840, 5767168, 167247872, 0), (19331547136, 5767168, 173015040, 0), (19337314304, 5767168, 178782208, 0), (19325779968, 5767168, 184549376, 0), (18829803520, 5767168, 190316544, 0), (18835570688, 5767168, 196083712, 0), (18824036352, 5767168, 201850880, 0), (19400753152, 5767168, 207618048, 0), (19406520320, 5767168, 213385216, 0), (19394985984, 5767168, 219152384, 0), (18864406528, 5767168, 224919552, 0), (18870173696, 5767168, 230686720, 0), (18858639360, 5767168, 236453888, 0), (18881708032, 5767168, 242221056, 0), (18887475200, 5767168, 247988224, 0), (18875940864, 5767168, 253755392, 0), (18899009536, 5767168, 259522560, 0), (18904776704, 5767168, 265289728, 0), (18893242368, 5767168, 271056896, 0)], 2: [(18397265920, 5767168, 0, 0), (18403033088, 5767168, 5767168, 0), (18391498752, 5767168, 11534336, 0), (18414567424, 5767168, 17301504, 0), (18420334592, 5767168, 23068672, 0), (18408800256, 5767168, 28835840, 0), (18985517056, 5767168, 34603008, 0), (18991284224, 5767168, 40370176, 0), (18979749888, 5767168, 46137344, 0), (19002818560, 5767168, 51904512, 0), (19008585728, 5767168, 57671680, 0), (18997051392, 5767168, 63438848, 0), (18968215552, 5767168, 69206016, 0), (18973982720, 5767168, 74973184, 0), (18962448384, 5767168, 80740352, 0), (18501074944, 5767168, 86507520, 0), (18506842112, 5767168, 92274688, 0), (18495307776, 5767168, 98041856, 0), (18518376448, 5767168, 103809024, 0), (18524143616, 5767168, 109576192, 0), (18512609280, 5767168, 115343360, 0), (19106627584, 5767168, 121110528, 0), (19112394752, 5767168, 126877696, 0), (19100860416, 5767168, 132644864, 0), (19158532096, 5767168, 138412032, 0), (19164299264, 5767168, 144179200, 0), (19152764928, 5767168, 149946368, 0), (18639486976, 5767168, 155713536, 0), (18645254144, 5767168, 161480704, 0), (18633719808, 5767168, 167247872, 0), (19193135104, 5767168, 173015040, 0), (19198902272, 5767168, 178782208, 0), (19187367936, 5767168, 184549376, 0), (18674089984, 5767168, 190316544, 0), (18679857152, 5767168, 196083712, 0), (18668322816, 5767168, 201850880, 0), (18691391488, 5767168, 207618048, 0), (18697158656, 5767168, 213385216, 0), (18685624320, 5767168, 219152384, 0), (18760597504, 5767168, 224919552, 0), (18766364672, 5767168, 230686720, 0), (18754830336, 5767168, 236453888, 0), (19348848640, 5767168, 242221056, 0), (19354615808, 5767168, 247988224, 0), (19343081472, 5767168, 253755392, 0), (19366150144, 5767168, 259522560, 0), (19371917312, 5767168, 265289728, 0), (19360382976, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e5420a640>, 2: <capsule object NULL at 0x7a4e547adf80>}
DEBUG 01-15 16:10:43.947690.947690 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.947049.947049 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.947421.947421 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 29d92d96-faa7-4f25-81c7-21091da062c9
DEBUG 01-15 16:10:43.947825.947825 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.947213.947213 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:43.947943.947943 client.py:127] Model loaded
DEBUG 01-15 16:10:43.947203.947203 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.947959.947959 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:43.948474.948474 cuda_h.py:19] end restore2model cost 0.00034427642822265625 seconds
DEBUG 01-15 16:10:43.948243.948243 cuda_h.py:19] end sllm_worker_task cost 0.010015010833740234 seconds
INFO 01-15 16:10:43.948168.948168 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 29d92d96-faa7-4f25-81c7-21091da062c9
DEBUG 01-15 16:10:43.948104.948104 cuda_h.py:19] end move_flatidxs cost 0.0008740425109863281 seconds
DEBUG 01-15 16:10:43.948654.948654 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0027043819427490234 seconds
DEBUG 01-15 16:10:43.948616.948616 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:43.949862.949862 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:43.951014.951014 cuda_h.py:19] end restore2model cost 0.0025358200073242188 seconds
DEBUG 01-15 16:10:43.951957.951957 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0054781436920166016 seconds
DEBUG 01-15 16:10:43.951944.951944 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:43.951882.951882 cuda_h.py:19] end gpu_sexperts cost 0.0002722740173339844 seconds
DEBUG 01-15 16:10:43.952519.952519 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:43.952064.952064 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.1457672119140625e-05 seconds
DEBUG 01-15 16:10:43.952714.952714 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:43.952893.952893 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.952670.952670 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008225440979003906 seconds
DEBUG 01-15 16:10:43.953182.953182 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.953746.953746 cuda_h.py:19] end gpu_group_list cost 0.00017547607421875 seconds
DEBUG 01-15 16:10:43.954870.954870 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:43.954173.954173 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000858306884765625 seconds
DEBUG 01-15 16:10:43.955242.955242 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:43.955474.955474 cuda_h.py:19] end gpu_group_list cost 0.00017523765563964844 seconds
DEBUG 01-15 16:10:43.955606.955606 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:43.955111.955111 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 29d92d96-faa7-4f25-81c7-21091da062c9
DEBUG 01-15 16:10:43.955835.955835 cuda_h.py:19] end group_tensors cost 0.006493568420410156 seconds
DEBUG 01-15 16:10:43.956638.956638 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:43.960818.960818 cuda_h.py:19] end group pad cost 0.004308938980102539 seconds
DEBUG 01-15 16:10:43.960952.960952 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:43.976979.976979 client.py:127] Model loaded
DEBUG 01-15 16:10:43.976071.976071 cuda_h.py:19] end wait_experts_multi_device cost 0.020472288131713867 seconds
DEBUG 01-15 16:10:43.976777.976777 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:43.980224.980224 cuda_h.py:19] end group_einsum cost 0.019475221633911133 seconds
DEBUG 01-15 16:10:43.980004.980004 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:43.984214.984214 cuda_h.py:19] end get_outputs_cpu1 cost 0.004018306732177734 seconds
DEBUG 01-15 16:10:43.985809.985809 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.037511587142944336 seconds
DEBUG 01-15 16:10:43.985679.985679 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.009039163589477539 seconds
DEBUG 01-15 16:10:43.985530.985530 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:43.986198.986198 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.987170.987170 cuda_h.py:19] end index_scatter cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:10:43.987691.987691 cuda_h.py:19] end cpuoutputsdeal cost 0.0015888214111328125 seconds
DEBUG 01-15 16:10:43.987415.987415 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:43.987555.987555 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.987448.987448 cuda_h.py:19] end gpu_group_tensor cost 0.0001354217529296875 seconds
DEBUG 01-15 16:10:43.987019.987019 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:43.987223.987223 cuda_h.py:19] end gpu_group_tensor cost 0.00012111663818359375 seconds
DEBUG 01-15 16:10:43.987504.987504 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.988509.988509 cuda_h.py:19] end gpu_group_einsum cost 0.0004830360412597656 seconds
DEBUG 01-15 16:10:43.988195.988195 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:43.989248.989248 cuda_h.py:19] end gpu_group_einsum cost 0.0003421306610107422 seconds
DEBUG 01-15 16:10:43.989006.989006 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.989042.989042 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.989467.989467 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017118453979492188 seconds
DEBUG 01-15 16:10:43.989892.989892 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.989623.989623 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:43.989512.989512 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.989111.989111 cuda_h.py:19] end index_scatter cost 5.602836608886719e-05 seconds
DEBUG 01-15 16:10:43.989669.989669 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007603168487548828 seconds
DEBUG 01-15 16:10:43.990314.990314 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:43.990442.990442 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:43.990858.990858 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013375282287597656 seconds
DEBUG 01-15 16:10:43.990660.990660 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:43.990345.990345 cuda_h.py:19] end concat_expert_out cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:43.990566.990566 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:43.990112.990112 cuda_h.py:19] end index_scatter cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:10:43.990113.990113 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00048279762268066406 seconds
DEBUG 01-15 16:10:43.990685.990685 cuda_h.py:19] end gpu_experts_multi_device cost 0.03847336769104004 seconds
DEBUG 01-15 16:10:43.990370.990370 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.04732537269592285 seconds
DEBUG 01-15 16:10:43.991992.991992 cuda_h.py:19] end prefill_layer cost 0.05336642265319824 seconds
DEBUG 01-15 16:10:43.991683.991683 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:10:43.991385.991385 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:43.991327.991327 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:10:43.991460.991460 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:43.991693.991693 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:43.991781.991781 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 3.743171691894531e-05 seconds
DEBUG 01-15 16:10:43.991491.991491 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 7.128715515136719e-05 seconds
DEBUG 01-15 16:10:43.991856.991856 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:43.991488.991488 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:43.991226.991226 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:43.991401.991401 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:43.991098.991098 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:43.991307.991307 cuda_h.py:19] end allocate_cuda_memory cost 0.0002551078796386719 seconds
DEBUG 01-15 16:10:43.992640.992640 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:43.992403.992403 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:43.992517.992517 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:43.992227.992227 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b6f317d2-4ab1-45d6-afec-75693f13576c
DEBUG 01-15 16:10:43.992661.992661 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:43.992766.992766 cuda_h.py:10] start self_attn
INFO 01-15 16:10:43.993671.993671 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b6f317d2-4ab1-45d6-afec-75693f13576c
DEBUG 01-15 16:10:43.993090.993090 cuda_h.py:19] end load_into_gpu_async cost 0.001524209976196289 seconds
DEBUG 01-15 16:10:43.993892.993892 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:43.993764.993764 cuda_h.py:19] end restore_tensors2 cost 8.416175842285156e-05 seconds
DEBUG 01-15 16:10:43.993765.993765 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021305084228515625 seconds
INFO 01-15 16:10:43.993198.993198 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b6f317d2-4ab1-45d6-afec-75693f13576c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:43.996386.996386 cuda_h.py:19] end self_attn cost 0.0036149024963378906 seconds
DEBUG 01-15 16:10:43.996252.996252 cuda_h.py:19] end iln_self_attn_paln cost 0.0052793025970458984 seconds
DEBUG 01-15 16:10:43.996420.996420 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:10:43.996659.996659 cuda_h.py:10] start gate
DEBUG 01-15 16:10:43.997315.997315 cuda_h.py:19] end gate cost 0.0007603168487548828 seconds
DEBUG 01-15 16:10:43.997489.997489 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:43.997057.997057 lmp.py:1912] 
DEBUG 01-15 16:10:43.997057.997057 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:43.998198.998198 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:43.998278.998278 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:43.998543.998543 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:43.998902.998902 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:43.998545.998545 lmp.py:1917] 
DEBUG 01-15 16:10:43.998545.998545 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:43.998664.998664 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:43.998506.998506 lmp.py:1935]   Expert 58 |     34 | CPU
DEBUG 01-15 16:10:43.998865.998865 lmp.py:1935]   Expert 31 |     60 | CPU
DEBUG 01-15 16:10:43.998031.998031 lmp.py:1935]   Expert 49 |     61 | CPU
DEBUG 01-15 16:10:43.998435.998435 lmp.py:1935]   Expert 47 |     62 | CPU
DEBUG 01-15 16:10:43.998363.998363 lmp.py:1935]   Expert  4 |     65 | CPU
DEBUG 01-15 16:10:43.998052.998052 lmp.py:1935]   Expert 38 |     68 | CPU
DEBUG 01-15 16:10:43.998980.998980 lmp.py:1935]   Expert 45 |     69 | CPU
DEBUG 01-15 16:10:43.998861.998861 lmp.py:1935]   Expert 41 |     83 | CPU
DEBUG 01-15 16:10:43.998173.998173 lmp.py:1935]   Expert 43 |     84 | CPU
DEBUG 01-15 16:10:43.998531.998531 lmp.py:1935]   Expert 33 |     95 | CPU
DEBUG 01-15 16:10:43.998367.998367 lmp.py:1935]   Expert 57 |    102 | CPU
DEBUG 01-15 16:10:43.998248.998248 lmp.py:1935]   Expert 11 |    105 | CPU
DEBUG 01-15 16:10:43.998414.998414 lmp.py:1935]   Expert 50 |    105 | CPU
DEBUG 01-15 16:10:43.998295.998295 lmp.py:1935]   Expert  2 |    110 | CPU
DEBUG 01-15 16:10:43.998700.998700 lmp.py:1935]   Expert 51 |    115 | CPU
DEBUG 01-15 16:10:43.998343.998343 lmp.py:1935]   Expert 14 |    122 | CPU
DEBUG 01-15 16:10:43.998748.998748 lmp.py:1935]   Expert  0 |    124 | CPU
DEBUG 01-15 16:10:43.998152.998152 lmp.py:1935]   Expert 54 |    131 | CPU
DEBUG 01-15 16:10:43.998557.998557 lmp.py:1935]   Expert 26 |    141 | CPU
DEBUG 01-15 16:10:43.998153.998153 lmp.py:1935]   Expert 34 |    142 | CPU
DEBUG 01-15 16:10:43.998035.998035 lmp.py:1935]   Expert 56 |    143 | CPU
DEBUG 01-15 16:10:43.998393.998393 lmp.py:1935]   Expert 27 |    152 | CPU
DEBUG 01-15 16:10:43.998513.998513 lmp.py:1935]   Expert 55 |    157 | CPU
DEBUG 01-15 16:10:43.998394.998394 lmp.py:1935]   Expert 28 |    160 | CPU
DEBUG 01-15 16:10:43.998799.998799 lmp.py:1935]   Expert 25 |    164 | CPU
DEBUG 01-15 16:10:43.998203.998203 lmp.py:1935]   Expert 10 |    166 | CPU
DEBUG 01-15 16:10:43.998529.998529 lmp.py:1935]   Expert  9 |    179 | CPU
DEBUG 01-15 16:10:43.998218.998218 lmp.py:1935]   Expert 13 |    179 | CPU
DEBUG 01-15 16:10:43.998146.998146 lmp.py:1935]   Expert 61 |    187 | CPU
DEBUG 01-15 16:10:43.998074.998074 lmp.py:1935]   Expert 48 |    191 | CPU
DEBUG 01-15 16:10:43.998240.998240 lmp.py:1935]   Expert  6 |    193 | CPU
DEBUG 01-15 16:10:43.998167.998167 lmp.py:1935]   Expert  7 |    194 | CPU
DEBUG 01-15 16:10:43.998241.998241 lmp.py:1935]   Expert 46 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.998076.998076 lmp.py:1935]   Expert 42 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.998911.998911 lmp.py:1935]   Expert 18 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.998984.998984 lmp.py:1935]   Expert 24 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.998820.998820 lmp.py:1935]   Expert 40 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.998701.998701 lmp.py:1935]   Expert 63 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.998059.998059 lmp.py:1935]   Expert 12 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.998179.998179 lmp.py:1935]   Expert 29 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.998060.998060 lmp.py:1935]   Expert 22 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.998657.998657 lmp.py:1935]   Expert 59 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.998492.998492 lmp.py:1935]   Expert 21 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.998327.998327 lmp.py:1935]   Expert 32 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.998209.998209 lmp.py:1935]   Expert 19 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.998328.998328 lmp.py:1935]   Expert 36 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.998462.998462 lmp.py:1935]   Expert  3 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.998582.998582 lmp.py:1935]   Expert 37 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.999940.999940 lmp.py:1935]   Expert  1 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.999060.999060 lmp.py:1935]   Expert 16 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.999418.999418 lmp.py:1935]   Expert 20 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.999061.999061 lmp.py:1935]   Expert  5 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.999704.999704 lmp.py:1935]   Expert  8 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.999585.999585 lmp.py:1935]   Expert 30 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.999751.999751 lmp.py:1935]   Expert 62 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.999394.999394 lmp.py:1935]   Expert 15 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.999799.999799 lmp.py:1935]   Expert 39 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.999965.999965 lmp.py:1935]   Expert 35 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.999899.999899 lmp.py:1935]   Expert 17 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.999542.999542 lmp.py:1935]   Expert 60 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.999947.999947 lmp.py:1935]   Expert 52 |    353 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.999590.999590 lmp.py:1935]   Expert 23 |    362 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.999233.999233 lmp.py:1935]   Expert 44 |    378 | GPU2(cuda:2)
DEBUG 01-15 16:10:43.999684.999684 lmp.py:1935]   Expert 53 |    435 | GPU1(cuda:1)
DEBUG 01-15 16:10:43.999419.999419 lmp.py:1937] 
DEBUG 01-15 16:10:43.999419.999419 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:43.999870.999870 lmp.py:1938]   CPU:   3943 tokens
DEBUG 01-15 16:10:43.999321.999321 lmp.py:1942]   cuda:1:   4176 tokens (16 experts)
DEBUG 01-15 16:10:43.999772.999772 lmp.py:1942]   cuda:2:   4169 tokens (16 experts)
DEBUG 01-15 16:10:43.999792.999792 lmp.py:1943]   Total GPU:   8345 tokens
DEBUG 01-15 16:10:43.999290.999290 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:43.999290.999290 lmp.py:1944] 
DEBUG 01-15 16:10:43.999039.999039 cuda_h.py:19] end experts_map_get cost 0.0017390251159667969 seconds
DEBUG 01-15 16:10:43.999770.999770 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:43.999288.999288 lmp.py:1953] 
DEBUG 01-15 16:10:43.999288.999288 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:43.999501.999501 cuda_h.py:19] end cpu_experts_submit cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:43.999005.999005 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:43.999404.999404 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.000607.000607 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.1.gate_proj.weight': 0, 'model.layers.16.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.52.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.52.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.52.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.20.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.20.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.20.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.22.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.22.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.22.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.24.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.24.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.24.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.16.mlp.experts.32.gate_proj.weight': 0, 'model.layers.16.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.23.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.23.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.23.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.59.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.59.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.59.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.60.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.60.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.60.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(19487260672, 5767168, 0, 0), (19493027840, 5767168, 5767168, 0), (19481493504, 5767168, 11534336, 0), (19521863680, 5767168, 17301504, 0), (19527630848, 5767168, 23068672, 0), (19516096512, 5767168, 28835840, 0), (20144717824, 5767168, 34603008, 0), (20150484992, 5767168, 40370176, 0), (20138950656, 5767168, 46137344, 0), (19608371200, 5767168, 51904512, 0), (19614138368, 5767168, 57671680, 0), (19602604032, 5767168, 63438848, 0), (20162019328, 5767168, 69206016, 0), (20167786496, 5767168, 74973184, 0), (20156252160, 5767168, 80740352, 0), (19677577216, 5767168, 86507520, 0), (19683344384, 5767168, 92274688, 0), (19671810048, 5767168, 98041856, 0), (20265828352, 5767168, 103809024, 0), (20271595520, 5767168, 109576192, 0), (20260061184, 5767168, 115343360, 0), (19764084736, 5767168, 121110528, 0), (19769851904, 5767168, 126877696, 0), (19758317568, 5767168, 132644864, 0), (19798687744, 5767168, 138412032, 0), (19804454912, 5767168, 144179200, 0), (19792920576, 5767168, 149946368, 0), (20369637376, 5767168, 155713536, 0), (20375404544, 5767168, 161480704, 0), (20363870208, 5767168, 167247872, 0), (20386938880, 5767168, 173015040, 0), (20392706048, 5767168, 178782208, 0), (20381171712, 5767168, 184549376, 0), (19815989248, 5767168, 190316544, 0), (19821756416, 5767168, 196083712, 0), (19810222080, 5767168, 201850880, 0), (19833290752, 5767168, 207618048, 0), (19839057920, 5767168, 213385216, 0), (19827523584, 5767168, 219152384, 0), (19850592256, 5767168, 224919552, 0), (19856359424, 5767168, 230686720, 0), (19844825088, 5767168, 236453888, 0), (19885195264, 5767168, 242221056, 0), (19890962432, 5767168, 247988224, 0), (19879428096, 5767168, 253755392, 0), (20542652416, 5767168, 259522560, 0), (20548419584, 5767168, 265289728, 0), (20536885248, 5767168, 271056896, 0)], 2: [(20023607296, 5767168, 0, 0), (20029374464, 5767168, 5767168, 0), (20017840128, 5767168, 11534336, 0), (20075511808, 5767168, 17301504, 0), (20081278976, 5767168, 23068672, 0), (20069744640, 5767168, 28835840, 0), (20092813312, 5767168, 34603008, 0), (20098580480, 5767168, 40370176, 0), (20087046144, 5767168, 46137344, 0), (19556466688, 5767168, 51904512, 0), (19562233856, 5767168, 57671680, 0), (19550699520, 5767168, 63438848, 0), (20110114816, 5767168, 69206016, 0), (20115881984, 5767168, 74973184, 0), (20104347648, 5767168, 80740352, 0), (20196622336, 5767168, 86507520, 0), (20202389504, 5767168, 92274688, 0), (20190855168, 5767168, 98041856, 0), (20231225344, 5767168, 103809024, 0), (20236992512, 5767168, 109576192, 0), (20225458176, 5767168, 115343360, 0), (19729481728, 5767168, 121110528, 0), (19735248896, 5767168, 126877696, 0), (19723714560, 5767168, 132644864, 0), (19746783232, 5767168, 138412032, 0), (19752550400, 5767168, 144179200, 0), (19741016064, 5767168, 149946368, 0), (19781386240, 5767168, 155713536, 0), (19787153408, 5767168, 161480704, 0), (19775619072, 5767168, 167247872, 0), (19867893760, 5767168, 173015040, 0), (19873660928, 5767168, 178782208, 0), (19862126592, 5767168, 184549376, 0), (20490747904, 5767168, 190316544, 0), (20496515072, 5767168, 196083712, 0), (20484980736, 5767168, 201850880, 0), (20508049408, 5767168, 207618048, 0), (20513816576, 5767168, 213385216, 0), (20502282240, 5767168, 219152384, 0), (19971702784, 5767168, 224919552, 0), (19977469952, 5767168, 230686720, 0), (19965935616, 5767168, 236453888, 0), (19989004288, 5767168, 242221056, 0), (19994771456, 5767168, 247988224, 0), (19983237120, 5767168, 253755392, 0), (20559953920, 5767168, 259522560, 0), (20565721088, 5767168, 265289728, 0), (20554186752, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a5afab83060>, 2: <capsule object NULL at 0x7a4f2c363000>}
DEBUG 01-15 16:10:44.000256.000256 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.000344.000344 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.001954.001954 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6ee22254-02ea-4c1c-ab9b-2685a2a030a1
DEBUG 01-15 16:10:44.001564.001564 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.001981.001981 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:44.001172.001172 client.py:127] Model loaded
DEBUG 01-15 16:10:44.001717.001717 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.001520.001520 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.001180.001180 cuda_h.py:19] end restore2model cost 0.0003457069396972656 seconds
DEBUG 01-15 16:10:44.001188.001188 cuda_h.py:19] end sllm_worker_task cost 0.010244607925415039 seconds
INFO 01-15 16:10:44.002270.002270 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6ee22254-02ea-4c1c-ab9b-2685a2a030a1
DEBUG 01-15 16:10:44.002875.002875 cuda_h.py:19] end move_flatidxs cost 0.0008335113525390625 seconds
DEBUG 01-15 16:10:44.002651.002651 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:44.002378.002378 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0031447410583496094 seconds
DEBUG 01-15 16:10:44.002633.002633 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.005971.005971 cuda_h.py:19] end restore2model cost 0.002566099166870117 seconds
DEBUG 01-15 16:10:44.005921.005921 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0059545040130615234 seconds
DEBUG 01-15 16:10:44.005432.005432 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.005184.005184 cuda_h.py:19] end gpu_sexperts cost 0.0002765655517578125 seconds
DEBUG 01-15 16:10:44.005536.005536 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.005551.005551 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 01-15 16:10:44.006247.006247 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.006427.006427 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.006648.006648 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008339881896972656 seconds
DEBUG 01-15 16:10:44.006789.006789 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.007359.007359 cuda_h.py:19] end gpu_group_list cost 0.00018095970153808594 seconds
DEBUG 01-15 16:10:44.007623.007623 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.008635.008635 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008885860443115234 seconds
DEBUG 01-15 16:10:44.008684.008684 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.009784.009784 cuda_h.py:19] end gpu_group_list cost 0.00018453598022460938 seconds
DEBUG 01-15 16:10:44.009300.009300 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.009567.009567 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6ee22254-02ea-4c1c-ab9b-2685a2a030a1
DEBUG 01-15 16:10:44.011856.011856 cuda_h.py:19] end group_tensors cost 0.009243965148925781 seconds
DEBUG 01-15 16:10:44.012153.012153 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.016155.016155 cuda_h.py:19] end group pad cost 0.004160404205322266 seconds
DEBUG 01-15 16:10:44.016561.016561 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.029380.029380 client.py:127] Model loaded
DEBUG 01-15 16:10:44.029737.029737 cuda_h.py:19] end wait_experts_multi_device cost 0.019696712493896484 seconds
DEBUG 01-15 16:10:44.029951.029951 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.037688.037688 cuda_h.py:19] end group_einsum cost 0.021112680435180664 seconds
DEBUG 01-15 16:10:44.038700.038700 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.041829.041829 cuda_h.py:19] end get_outputs_cpu1 cost 0.0037848949432373047 seconds
DEBUG 01-15 16:10:44.042160.042160 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04128599166870117 seconds
DEBUG 01-15 16:10:44.043990.043990 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013380289077758789 seconds
DEBUG 01-15 16:10:44.043557.043557 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.044550.044550 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.044128.044128 cuda_h.py:19] end index_scatter cost 7.963180541992188e-05 seconds
DEBUG 01-15 16:10:44.044503.044503 cuda_h.py:19] end cpuoutputsdeal cost 0.0017118453979492188 seconds
DEBUG 01-15 16:10:44.044181.044181 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.045798.045798 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.045168.045168 cuda_h.py:19] end gpu_group_tensor cost 0.0001380443572998047 seconds
DEBUG 01-15 16:10:44.045785.045785 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.045896.045896 cuda_h.py:19] end gpu_group_tensor cost 0.0001232624053955078 seconds
DEBUG 01-15 16:10:44.045323.045323 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.046410.046410 cuda_h.py:19] end gpu_group_einsum cost 0.0005762577056884766 seconds
DEBUG 01-15 16:10:44.046899.046899 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.046387.046387 cuda_h.py:19] end gpu_group_einsum cost 0.00047326087951660156 seconds
DEBUG 01-15 16:10:44.046285.046285 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.047111.047111 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.047378.047378 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002200603485107422 seconds
DEBUG 01-15 16:10:44.047671.047671 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.047184.047184 cuda_h.py:19] end concat_expert_out cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:10:44.047596.047596 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.047096.047096 cuda_h.py:19] end index_scatter cost 5.459785461425781e-05 seconds
DEBUG 01-15 16:10:44.047077.047077 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008456707000732422 seconds
DEBUG 01-15 16:10:44.047676.047676 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.047612.047612 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.048021.048021 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013136863708496094 seconds
DEBUG 01-15 16:10:44.048108.048108 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.048555.048555 cuda_h.py:19] end concat_expert_out cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:44.048014.048014 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.048937.048937 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:44.048482.048482 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00048804283142089844 seconds
DEBUG 01-15 16:10:44.048299.048299 cuda_h.py:19] end gpu_experts_multi_device cost 0.04246687889099121 seconds
DEBUG 01-15 16:10:44.048070.048070 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.05181527137756348 seconds
DEBUG 01-15 16:10:44.048189.048189 cuda_h.py:19] end prefill_layer cost 0.05779433250427246 seconds
DEBUG 01-15 16:10:44.049357.049357 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:10:44.049059.049059 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.049239.049239 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:10:44.049849.049849 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:44.049082.049082 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:44.049793.049793 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 3.910064697265625e-05 seconds
DEBUG 01-15 16:10:44.049788.049788 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:10:44.049868.049868 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.049261.049261 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.049853.049853 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.049314.049314 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.049534.049534 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.049749.049749 cuda_h.py:19] end allocate_cuda_memory cost 0.0002617835998535156 seconds
DEBUG 01-15 16:10:44.049057.049057 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.050919.050919 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.050080.050080 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.050312.050312 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 04d0f52d-0851-4dc0-a7b9-90c26d8e518c
DEBUG 01-15 16:10:44.050800.050800 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.050612.050612 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.051590.051590 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 04d0f52d-0851-4dc0-a7b9-90c26d8e518c
DEBUG 01-15 16:10:44.051910.051910 cuda_h.py:19] end load_into_gpu_async cost 0.0015027523040771484 seconds
DEBUG 01-15 16:10:44.051520.051520 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.051729.051729 cuda_h.py:19] end restore_tensors2 cost 8.726119995117188e-05 seconds
DEBUG 01-15 16:10:44.051777.051777 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021347999572753906 seconds
INFO 01-15 16:10:44.051356.051356 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 04d0f52d-0851-4dc0-a7b9-90c26d8e518c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.054260.054260 cuda_h.py:19] end self_attn cost 0.0034592151641845703 seconds
DEBUG 01-15 16:10:44.054192.054192 cuda_h.py:19] end iln_self_attn_paln cost 0.005086421966552734 seconds
DEBUG 01-15 16:10:44.054405.054405 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:10:44.054645.054645 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.055722.055722 cuda_h.py:19] end gate cost 0.0006861686706542969 seconds
DEBUG 01-15 16:10:44.055373.055373 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.055491.055491 lmp.py:1912] 
DEBUG 01-15 16:10:44.055491.055491 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.055347.055347 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.055950.055950 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.055977.055977 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.055097.055097 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.055740.055740 lmp.py:1917] 
DEBUG 01-15 16:10:44.055740.055740 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.055575.055575 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.055179.055179 lmp.py:1935]   Expert  4 |     10 | CPU
DEBUG 01-15 16:10:44.055060.055060 lmp.py:1935]   Expert 28 |     27 | CPU
DEBUG 01-15 16:10:44.055988.055988 lmp.py:1935]   Expert  7 |     46 | CPU
DEBUG 01-15 16:10:44.055915.055915 lmp.py:1935]   Expert 53 |     57 | CPU
DEBUG 01-15 16:10:44.055366.055366 lmp.py:1935]   Expert 52 |     68 | CPU
DEBUG 01-15 16:10:44.055294.055294 lmp.py:1935]   Expert 43 |     71 | CPU
DEBUG 01-15 16:10:44.055745.055745 lmp.py:1935]   Expert 49 |     81 | CPU
DEBUG 01-15 16:10:44.055739.055739 lmp.py:1935]   Expert 12 |     88 | CPU
DEBUG 01-15 16:10:44.055952.055952 lmp.py:1935]   Expert 47 |    105 | CPU
DEBUG 01-15 16:10:44.055164.055164 lmp.py:1935]   Expert 33 |    107 | CPU
DEBUG 01-15 16:10:44.055900.055900 lmp.py:1935]   Expert 50 |    107 | CPU
DEBUG 01-15 16:10:44.055112.055112 lmp.py:1935]   Expert  2 |    109 | CPU
DEBUG 01-15 16:10:44.055325.055325 lmp.py:1935]   Expert 24 |    109 | CPU
DEBUG 01-15 16:10:44.056299.056299 lmp.py:1935]   Expert 60 |    112 | CPU
DEBUG 01-15 16:10:44.056273.056273 lmp.py:1935]   Expert 15 |    113 | CPU
DEBUG 01-15 16:10:44.056962.056962 lmp.py:1935]   Expert 39 |    113 | CPU
DEBUG 01-15 16:10:44.056890.056890 lmp.py:1935]   Expert 36 |    119 | CPU
DEBUG 01-15 16:10:44.056056.056056 lmp.py:1935]   Expert 25 |    122 | CPU
DEBUG 01-15 16:10:44.056984.056984 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:10:44.056435.056435 lmp.py:1935]   Expert 61 |    129 | CPU
DEBUG 01-15 16:10:44.056886.056886 lmp.py:1935]   Expert 59 |    135 | CPU
DEBUG 01-15 16:10:44.056005.056005 lmp.py:1935]   Expert  3 |    143 | CPU
DEBUG 01-15 16:10:44.056410.056410 lmp.py:1935]   Expert 27 |    143 | CPU
DEBUG 01-15 16:10:44.056338.056338 lmp.py:1935]   Expert 58 |    146 | CPU
DEBUG 01-15 16:10:44.056265.056265 lmp.py:1935]   Expert  8 |    149 | CPU
DEBUG 01-15 16:10:44.056670.056670 lmp.py:1935]   Expert 30 |    150 | CPU
DEBUG 01-15 16:10:44.056836.056836 lmp.py:1935]   Expert 31 |    154 | CPU
DEBUG 01-15 16:10:44.056002.056002 lmp.py:1935]   Expert 10 |    156 | CPU
DEBUG 01-15 16:10:44.056692.056692 lmp.py:1935]   Expert 40 |    158 | CPU
DEBUG 01-15 16:10:44.056573.056573 lmp.py:1935]   Expert 38 |    159 | CPU
DEBUG 01-15 16:10:44.056977.056977 lmp.py:1935]   Expert 14 |    160 | CPU
DEBUG 01-15 16:10:44.056349.056349 lmp.py:1935]   Expert 41 |    162 | CPU
DEBUG 01-15 16:10:44.056184.056184 lmp.py:1935]   Expert 54 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056781.056781 lmp.py:1935]   Expert 57 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056007.056007 lmp.py:1935]   Expert 37 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056578.056578 lmp.py:1935]   Expert 46 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056698.056698 lmp.py:1935]   Expert 32 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056579.056579 lmp.py:1935]   Expert 42 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056699.056699 lmp.py:1935]   Expert 19 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056295.056295 lmp.py:1935]   Expert 11 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056892.056892 lmp.py:1935]   Expert 22 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056204.056204 lmp.py:1935]   Expert 34 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056562.056562 lmp.py:1935]   Expert 26 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056159.056159 lmp.py:1935]   Expert 18 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056755.056755 lmp.py:1935]   Expert  0 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056114.056114 lmp.py:1935]   Expert 56 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056757.056757 lmp.py:1935]   Expert  1 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056115.056115 lmp.py:1935]   Expert 44 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056758.056758 lmp.py:1935]   Expert 51 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056401.056401 lmp.py:1935]   Expert 20 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056282.056282 lmp.py:1935]   Expert 29 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056640.056640 lmp.py:1935]   Expert 48 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056237.056237 lmp.py:1935]   Expert 45 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056834.056834 lmp.py:1935]   Expert 21 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056954.056954 lmp.py:1935]   Expert 55 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056597.056597 lmp.py:1935]   Expert 35 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056001.056001 lmp.py:1935]   Expert 16 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056883.056883 lmp.py:1935]   Expert  5 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056002.056002 lmp.py:1935]   Expert 23 |    370 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056407.056407 lmp.py:1935]   Expert 13 |    379 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056288.056288 lmp.py:1935]   Expert 17 |    435 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056931.056931 lmp.py:1935]   Expert  9 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056574.056574 lmp.py:1935]   Expert 63 |    464 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.056979.056979 lmp.py:1935]   Expert 62 |   1174 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.056860.056860 lmp.py:1937] 
DEBUG 01-15 16:10:44.056860.056860 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.056742.056742 lmp.py:1938]   CPU:   3637 tokens
DEBUG 01-15 16:10:44.056623.056623 lmp.py:1942]   cuda:1:   4277 tokens (15 experts)
DEBUG 01-15 16:10:44.057220.057220 lmp.py:1942]   cuda:2:   4374 tokens (17 experts)
DEBUG 01-15 16:10:44.057909.057909 lmp.py:1943]   Total GPU:   8651 tokens
DEBUG 01-15 16:10:44.057883.057883 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.057883.057883 lmp.py:1944] 
DEBUG 01-15 16:10:44.057533.057533 cuda_h.py:19] end experts_map_get cost 0.0017709732055664062 seconds
DEBUG 01-15 16:10:44.057336.057336 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.057185.057185 lmp.py:1953] 
DEBUG 01-15 16:10:44.057185.057185 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.057683.057683 cuda_h.py:19] end cpu_experts_submit cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:44.057141.057141 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.057613.057613 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.058834.058834 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.1.gate_proj.weight': 0, 'model.layers.17.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.37.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.37.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.37.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.56.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.56.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.56.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.62.up_proj.weight': 253755392}, 2: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.32.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.32.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.32.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.9.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.9.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.9.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.51.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.51.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.51.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.26.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.26.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.26.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.29.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.63.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.63.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.63.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(20594556928, 5767168, 0, 0), (20600324096, 5767168, 5767168, 0), (20588789760, 5767168, 11534336, 0), (21165506560, 5767168, 17301504, 0), (21171273728, 5767168, 23068672, 0), (21159739392, 5767168, 28835840, 0), (21182808064, 5767168, 34603008, 0), (21188575232, 5767168, 40370176, 0), (21177040896, 5767168, 46137344, 0), (20663762944, 5767168, 51904512, 0), (20669530112, 5767168, 57671680, 0), (20657995776, 5767168, 63438848, 0), (21217411072, 5767168, 69206016, 0), (21223178240, 5767168, 74973184, 0), (21211643904, 5767168, 80740352, 0), (21303918592, 5767168, 86507520, 0), (21309685760, 5767168, 92274688, 0), (21298151424, 5767168, 98041856, 0), (20767571968, 5767168, 103809024, 0), (20773339136, 5767168, 109576192, 0), (20761804800, 5767168, 115343360, 0), (20802174976, 5767168, 121110528, 0), (20807942144, 5767168, 126877696, 0), (20796407808, 5767168, 132644864, 0), (21407727616, 5767168, 138412032, 0), (21413494784, 5767168, 144179200, 0), (21401960448, 5767168, 149946368, 0), (20888682496, 5767168, 155713536, 0), (20894449664, 5767168, 161480704, 0), (20882915328, 5767168, 167247872, 0), (20923285504, 5767168, 173015040, 0), (20929052672, 5767168, 178782208, 0), (20917518336, 5767168, 184549376, 0), (20940587008, 5767168, 190316544, 0), (20946354176, 5767168, 196083712, 0), (20934819840, 5767168, 201850880, 0), (21511536640, 5767168, 207618048, 0), (21517303808, 5767168, 213385216, 0), (21505769472, 5767168, 219152384, 0), (21546139648, 5767168, 224919552, 0), (21551906816, 5767168, 230686720, 0), (21540372480, 5767168, 236453888, 0), (21649948672, 5767168, 242221056, 0), (21655715840, 5767168, 247988224, 0), (21644181504, 5767168, 253755392, 0)], 2: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (21130903552, 5767168, 17301504, 0), (21136670720, 5767168, 23068672, 0), (21125136384, 5767168, 28835840, 0), (20732968960, 5767168, 34603008, 0), (20738736128, 5767168, 40370176, 0), (20727201792, 5767168, 46137344, 0), (21338521600, 5767168, 51904512, 0), (21344288768, 5767168, 57671680, 0), (21332754432, 5767168, 63438848, 0), (21355823104, 5767168, 69206016, 0), (21361590272, 5767168, 74973184, 0), (21350055936, 5767168, 80740352, 0), (21373124608, 5767168, 86507520, 0), (21378891776, 5767168, 92274688, 0), (21367357440, 5767168, 98041856, 0), (20854079488, 5767168, 103809024, 0), (20859846656, 5767168, 109576192, 0), (20848312320, 5767168, 115343360, 0), (20871380992, 5767168, 121110528, 0), (20877148160, 5767168, 126877696, 0), (20865613824, 5767168, 132644864, 0), (21459632128, 5767168, 138412032, 0), (21465399296, 5767168, 144179200, 0), (21453864960, 5767168, 149946368, 0), (20905984000, 5767168, 155713536, 0), (20911751168, 5767168, 161480704, 0), (20900216832, 5767168, 167247872, 0), (21528838144, 5767168, 173015040, 0), (21534605312, 5767168, 178782208, 0), (21523070976, 5767168, 184549376, 0), (20957888512, 5767168, 190316544, 0), (20963655680, 5767168, 196083712, 0), (20952121344, 5767168, 201850880, 0), (20975190016, 5767168, 207618048, 0), (20980957184, 5767168, 213385216, 0), (20969422848, 5767168, 219152384, 0), (21563441152, 5767168, 224919552, 0), (21569208320, 5767168, 230686720, 0), (21557673984, 5767168, 236453888, 0), (21027094528, 5767168, 242221056, 0), (21032861696, 5767168, 247988224, 0), (21021327360, 5767168, 253755392, 0), (21078999040, 5767168, 259522560, 0), (21084766208, 5767168, 265289728, 0), (21073231872, 5767168, 271056896, 0), (21667250176, 5767168, 276824064, 0), (21673017344, 5767168, 282591232, 0), (21661483008, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e443a7480>, 2: <capsule object NULL at 0x7a51b87d2f40>}
DEBUG 01-15 16:10:44.058355.058355 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.058668.058668 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.058563.058563 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c709cb08-d501-4895-85af-f8da6039e25f
DEBUG 01-15 16:10:44.058173.058173 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.059760.059760 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:44.059827.059827 client.py:127] Model loaded
DEBUG 01-15 16:10:44.059564.059564 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.059869.059869 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.059027.059027 cuda_h.py:19] end restore2model cost 0.0003445148468017578 seconds
DEBUG 01-15 16:10:44.059274.059274 cuda_h.py:19] end sllm_worker_task cost 0.010091781616210938 seconds
INFO 01-15 16:10:44.060268.060268 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c709cb08-d501-4895-85af-f8da6039e25f
DEBUG 01-15 16:10:44.060350.060350 cuda_h.py:19] end move_flatidxs cost 0.0008544921875 seconds
DEBUG 01-15 16:10:44.060663.060663 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:44.060487.060487 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003105640411376953 seconds
DEBUG 01-15 16:10:44.060211.060211 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.063835.063835 cuda_h.py:19] end restore2model cost 0.0025675296783447266 seconds
DEBUG 01-15 16:10:44.063069.063069 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005910396575927734 seconds
DEBUG 01-15 16:10:44.063626.063626 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.063371.063371 cuda_h.py:19] end gpu_sexperts cost 0.0002722740173339844 seconds
DEBUG 01-15 16:10:44.063201.063201 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.063407.063407 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8596649169921875e-05 seconds
DEBUG 01-15 16:10:44.063104.063104 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.063283.063283 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.064212.064212 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007929801940917969 seconds
DEBUG 01-15 16:10:44.064446.064446 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.064101.064101 cuda_h.py:19] end gpu_group_list cost 0.0001735687255859375 seconds
DEBUG 01-15 16:10:44.065072.065072 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.066033.066033 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009253025054931641 seconds
DEBUG 01-15 16:10:44.066558.066558 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.066380.066380 cuda_h.py:19] end gpu_group_list cost 0.00018906593322753906 seconds
DEBUG 01-15 16:10:44.067632.067632 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.067323.067323 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c709cb08-d501-4895-85af-f8da6039e25f
DEBUG 01-15 16:10:44.070210.070210 cuda_h.py:19] end group_tensors cost 0.010566234588623047 seconds
DEBUG 01-15 16:10:44.071944.071944 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.075785.075785 cuda_h.py:19] end group pad cost 0.0040819644927978516 seconds
DEBUG 01-15 16:10:44.075190.075190 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.091545.091545 client.py:127] Model loaded
DEBUG 01-15 16:10:44.092758.092758 cuda_h.py:19] end wait_experts_multi_device cost 0.024588584899902344 seconds
DEBUG 01-15 16:10:44.092697.092697 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.094327.094327 cuda_h.py:19] end group_einsum cost 0.018829345703125 seconds
DEBUG 01-15 16:10:44.094955.094955 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.098058.098058 cuda_h.py:19] end get_outputs_cpu1 cost 0.003799915313720703 seconds
DEBUG 01-15 16:10:44.099368.099368 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04026937484741211 seconds
DEBUG 01-15 16:10:44.099629.099629 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007575035095214844 seconds
DEBUG 01-15 16:10:44.100480.100480 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.101474.101474 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.101506.101506 cuda_h.py:19] end index_scatter cost 7.462501525878906e-05 seconds
DEBUG 01-15 16:10:44.101927.101927 cuda_h.py:19] end cpuoutputsdeal cost 0.0016255378723144531 seconds
DEBUG 01-15 16:10:44.101936.101936 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.101268.101268 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.101645.101645 cuda_h.py:19] end gpu_group_tensor cost 0.00014066696166992188 seconds
DEBUG 01-15 16:10:44.101785.101785 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.102658.102658 cuda_h.py:19] end gpu_group_tensor cost 0.0001227855682373047 seconds
DEBUG 01-15 16:10:44.102270.102270 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.103056.103056 cuda_h.py:19] end gpu_group_einsum cost 0.001055002212524414 seconds
DEBUG 01-15 16:10:44.103670.103670 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.104986.104986 cuda_h.py:19] end gpu_group_einsum cost 0.0004918575286865234 seconds
DEBUG 01-15 16:10:44.104832.104832 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.104041.104041 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.104527.104527 cuda_h.py:19] end all_expert_outputs_slices cost 0.00023937225341796875 seconds
DEBUG 01-15 16:10:44.104873.104873 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.104863.104863 cuda_h.py:19] end concat_expert_out cost 5.8650970458984375e-05 seconds
DEBUG 01-15 16:10:44.104322.104322 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.104683.104683 cuda_h.py:19] end index_scatter cost 5.6743621826171875e-05 seconds
DEBUG 01-15 16:10:44.105717.105717 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008790493011474609 seconds
DEBUG 01-15 16:10:44.105316.105316 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.105583.105583 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.105932.105932 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012040138244628906 seconds
DEBUG 01-15 16:10:44.105735.105735 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.105281.105281 cuda_h.py:19] end concat_expert_out cost 5.459785461425781e-05 seconds
DEBUG 01-15 16:10:44.105024.105024 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.105524.105524 cuda_h.py:19] end index_scatter cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:44.105571.105571 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004687309265136719 seconds
DEBUG 01-15 16:10:44.105335.105335 cuda_h.py:19] end gpu_experts_multi_device cost 0.042107343673706055 seconds
DEBUG 01-15 16:10:44.105960.105960 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.0513606071472168 seconds
DEBUG 01-15 16:10:44.106410.106410 cuda_h.py:19] end prefill_layer cost 0.05710887908935547 seconds
DEBUG 01-15 16:10:44.106306.106306 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:10:44.106486.106486 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.106142.106142 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:10:44.106276.106276 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:44.106316.106316 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:44.106074.106074 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 3.8623809814453125e-05 seconds
DEBUG 01-15 16:10:44.106353.106353 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 7.152557373046875e-05 seconds
DEBUG 01-15 16:10:44.106956.106956 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.106111.106111 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.106908.106908 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.106985.106985 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.106729.106729 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.107015.107015 cuda_h.py:19] end allocate_cuda_memory cost 0.0002090930938720703 seconds
DEBUG 01-15 16:10:44.107846.107846 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.107516.107516 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.107630.107630 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.107909.107909 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 740a89b7-2c1b-4998-af28-1dd3a6c0142f
DEBUG 01-15 16:10:44.107390.107390 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.107902.107902 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.108862.108862 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 740a89b7-2c1b-4998-af28-1dd3a6c0142f
DEBUG 01-15 16:10:44.108402.108402 cuda_h.py:19] end load_into_gpu_async cost 0.0015299320220947266 seconds
DEBUG 01-15 16:10:44.108158.108158 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.108036.108036 cuda_h.py:19] end restore_tensors2 cost 8.845329284667969e-05 seconds
DEBUG 01-15 16:10:44.108322.108322 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002110719680786133 seconds
INFO 01-15 16:10:44.108311.108311 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 740a89b7-2c1b-4998-af28-1dd3a6c0142f
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.110527.110527 cuda_h.py:19] end self_attn cost 0.0031583309173583984 seconds
DEBUG 01-15 16:10:44.111253.111253 cuda_h.py:19] end iln_self_attn_paln cost 0.0046787261962890625 seconds
DEBUG 01-15 16:10:44.111281.111281 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:10:44.111952.111952 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.112499.112499 cuda_h.py:19] end gate cost 0.0006799697875976562 seconds
DEBUG 01-15 16:10:44.112766.112766 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.112515.112515 lmp.py:1912] 
DEBUG 01-15 16:10:44.112515.112515 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.112291.112291 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.112808.112808 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.112789.112789 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.112386.112386 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.112314.112314 lmp.py:1917] 
DEBUG 01-15 16:10:44.112314.112314 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.112195.112195 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.112275.112275 lmp.py:1935]   Expert 32 |     31 | CPU
DEBUG 01-15 16:10:44.112918.112918 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:10:44.112084.112084 lmp.py:1935]   Expert  5 |     53 | CPU
DEBUG 01-15 16:10:44.112774.112774 lmp.py:1935]   Expert 46 |     74 | CPU
DEBUG 01-15 16:10:44.112463.112463 lmp.py:1935]   Expert  8 |     90 | CPU
DEBUG 01-15 16:10:44.112914.112914 lmp.py:1935]   Expert 40 |     90 | CPU
DEBUG 01-15 16:10:44.112603.112603 lmp.py:1935]   Expert 12 |    100 | CPU
DEBUG 01-15 16:10:44.112246.112246 lmp.py:1935]   Expert 17 |    107 | CPU
DEBUG 01-15 16:10:44.112128.112128 lmp.py:1935]   Expert 27 |    110 | CPU
DEBUG 01-15 16:10:44.112532.112532 lmp.py:1935]   Expert 60 |    110 | CPU
DEBUG 01-15 16:10:44.112937.112937 lmp.py:1935]   Expert  3 |    115 | CPU
DEBUG 01-15 16:10:44.112626.112626 lmp.py:1935]   Expert 58 |    116 | CPU
DEBUG 01-15 16:10:44.112315.112315 lmp.py:1935]   Expert 21 |    119 | CPU
DEBUG 01-15 16:10:44.112289.112289 lmp.py:1935]   Expert 28 |    119 | CPU
DEBUG 01-15 16:10:44.112740.112740 lmp.py:1935]   Expert 29 |    123 | CPU
DEBUG 01-15 16:10:44.112953.112953 lmp.py:1935]   Expert 41 |    124 | CPU
DEBUG 01-15 16:10:44.112927.112927 lmp.py:1935]   Expert 25 |    129 | CPU
DEBUG 01-15 16:10:44.112378.112378 lmp.py:1935]   Expert 35 |    131 | CPU
DEBUG 01-15 16:10:44.112590.112590 lmp.py:1935]   Expert 19 |    136 | CPU
DEBUG 01-15 16:10:44.112564.112564 lmp.py:1935]   Expert 52 |    143 | CPU
DEBUG 01-15 16:10:44.113730.113730 lmp.py:1935]   Expert  0 |    145 | CPU
DEBUG 01-15 16:10:44.113135.113135 lmp.py:1935]   Expert  6 |    146 | CPU
DEBUG 01-15 16:10:44.113301.113301 lmp.py:1935]   Expert 54 |    149 | CPU
DEBUG 01-15 16:10:44.113990.113990 lmp.py:1935]   Expert 56 |    150 | CPU
DEBUG 01-15 16:10:44.113441.113441 lmp.py:1935]   Expert 37 |    153 | CPU
DEBUG 01-15 16:10:44.113415.113415 lmp.py:1935]   Expert 53 |    156 | CPU
DEBUG 01-15 16:10:44.113628.113628 lmp.py:1935]   Expert 63 |    156 | CPU
DEBUG 01-15 16:10:44.113602.113602 lmp.py:1935]   Expert 36 |    161 | CPU
DEBUG 01-15 16:10:44.113576.113576 lmp.py:1935]   Expert 48 |    161 | CPU
DEBUG 01-15 16:10:44.113550.113550 lmp.py:1935]   Expert 59 |    168 | CPU
DEBUG 01-15 16:10:44.113285.113285 lmp.py:1935]   Expert  9 |    183 | CPU
DEBUG 01-15 16:10:44.113498.113498 lmp.py:1935]   Expert  1 |    188 | CPU
DEBUG 01-15 16:10:44.113002.113002 lmp.py:1935]   Expert 39 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113122.113122 lmp.py:1935]   Expert 20 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113765.113765 lmp.py:1935]   Expert 61 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113733.113733 lmp.py:1935]   Expert 43 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113091.113091 lmp.py:1935]   Expert 42 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113257.113257 lmp.py:1935]   Expert  7 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113185.113185 lmp.py:1935]   Expert 11 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113113.113113 lmp.py:1935]   Expert 34 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113564.113564 lmp.py:1935]   Expert 47 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113253.113253 lmp.py:1935]   Expert 55 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113181.113181 lmp.py:1935]   Expert 13 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113870.113870 lmp.py:1935]   Expert 16 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113559.113559 lmp.py:1935]   Expert 57 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113202.113202 lmp.py:1935]   Expert 18 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113845.113845 lmp.py:1935]   Expert 15 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113012.113012 lmp.py:1935]   Expert  4 |    239 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113654.113654 lmp.py:1935]   Expert 45 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113821.113821 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113748.113748 lmp.py:1935]   Expert 33 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113438.113438 lmp.py:1935]   Expert 22 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113889.113889 lmp.py:1935]   Expert 31 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113724.113724 lmp.py:1935]   Expert 51 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113651.113651 lmp.py:1935]   Expert 49 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113102.113102 lmp.py:1935]   Expert 38 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113792.113792 lmp.py:1935]   Expert 26 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113448.113448 lmp.py:1935]   Expert 10 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113376.113376 lmp.py:1935]   Expert 44 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113304.113304 lmp.py:1935]   Expert  2 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113470.113470 lmp.py:1935]   Expert 24 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113682.113682 lmp.py:1935]   Expert 14 |    309 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113895.113895 lmp.py:1935]   Expert 23 |    405 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.113345.113345 lmp.py:1935]   Expert 62 |    675 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.113843.113843 lmp.py:1937] 
DEBUG 01-15 16:10:44.113843.113843 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.113055.113055 lmp.py:1938]   CPU:   3988 tokens
DEBUG 01-15 16:10:44.113983.113983 lmp.py:1942]   cuda:1:   4246 tokens (16 experts)
DEBUG 01-15 16:10:44.113718.113718 lmp.py:1942]   cuda:2:   4054 tokens (16 experts)
DEBUG 01-15 16:10:44.113454.113454 lmp.py:1943]   Total GPU:   8300 tokens
DEBUG 01-15 16:10:44.113713.113713 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.113713.113713 lmp.py:1944] 
DEBUG 01-15 16:10:44.113886.113886 cuda_h.py:19] end experts_map_get cost 0.0017769336700439453 seconds
DEBUG 01-15 16:10:44.113259.113259 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.113584.113584 lmp.py:1953] 
DEBUG 01-15 16:10:44.113584.113584 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.114652.114652 cuda_h.py:19] end cpu_experts_submit cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:44.114130.114130 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.114244.114244 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.115187.115187 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.33.gate_proj.weight': 0, 'model.layers.18.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.43.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.43.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.43.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.20.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.20.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.20.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.18.mlp.experts.2.gate_proj.weight': 0, 'model.layers.18.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.57.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.57.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.57.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.61.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.61.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.61.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(22255501312, 5767168, 0, 0), (22261268480, 5767168, 5767168, 0), (22249734144, 5767168, 11534336, 0), (22272802816, 5767168, 17301504, 0), (22278569984, 5767168, 23068672, 0), (22267035648, 5767168, 28835840, 0), (21805662208, 5767168, 34603008, 0), (21811429376, 5767168, 40370176, 0), (21799895040, 5767168, 46137344, 0), (22359310336, 5767168, 51904512, 0), (22365077504, 5767168, 57671680, 0), (22353543168, 5767168, 63438848, 0), (22428516352, 5767168, 69206016, 0), (22434283520, 5767168, 74973184, 0), (22422749184, 5767168, 80740352, 0), (22445817856, 5767168, 86507520, 0), (22451585024, 5767168, 92274688, 0), (22440050688, 5767168, 98041856, 0), (21909471232, 5767168, 103809024, 0), (21915238400, 5767168, 109576192, 0), (21903704064, 5767168, 115343360, 0), (21944074240, 5767168, 121110528, 0), (21949841408, 5767168, 126877696, 0), (21938307072, 5767168, 132644864, 0), (21961375744, 5767168, 138412032, 0), (21967142912, 5767168, 144179200, 0), (21955608576, 5767168, 149946368, 0), (22532325376, 5767168, 155713536, 0), (22538092544, 5767168, 161480704, 0), (22526558208, 5767168, 167247872, 0), (22549626880, 5767168, 173015040, 0), (22555394048, 5767168, 178782208, 0), (22543859712, 5767168, 184549376, 0), (22030581760, 5767168, 190316544, 0), (22036348928, 5767168, 196083712, 0), (22024814592, 5767168, 201850880, 0), (22099787776, 5767168, 207618048, 0), (22105554944, 5767168, 213385216, 0), (22094020608, 5767168, 219152384, 0), (22134390784, 5767168, 224919552, 0), (22140157952, 5767168, 230686720, 0), (22128623616, 5767168, 236453888, 0), (22757244928, 5767168, 242221056, 0), (22763012096, 5767168, 247988224, 0), (22751477760, 5767168, 253755392, 0), (22220898304, 5767168, 259522560, 0), (22226665472, 5767168, 265289728, 0), (22215131136, 5767168, 271056896, 0)], 2: [(21719154688, 5767168, 0, 0), (21724921856, 5767168, 5767168, 0), (21713387520, 5767168, 11534336, 0), (21753757696, 5767168, 17301504, 0), (21759524864, 5767168, 23068672, 0), (21747990528, 5767168, 28835840, 0), (22342008832, 5767168, 34603008, 0), (22347776000, 5767168, 40370176, 0), (22336241664, 5767168, 46137344, 0), (21857566720, 5767168, 51904512, 0), (21863333888, 5767168, 57671680, 0), (21851799552, 5767168, 63438848, 0), (21874868224, 5767168, 69206016, 0), (21880635392, 5767168, 74973184, 0), (21869101056, 5767168, 80740352, 0), (22411214848, 5767168, 86507520, 0), (22416982016, 5767168, 92274688, 0), (22405447680, 5767168, 98041856, 0), (22463119360, 5767168, 103809024, 0), (22468886528, 5767168, 109576192, 0), (22457352192, 5767168, 115343360, 0), (21926772736, 5767168, 121110528, 0), (21932539904, 5767168, 126877696, 0), (21921005568, 5767168, 132644864, 0), (22497722368, 5767168, 138412032, 0), (22503489536, 5767168, 144179200, 0), (22491955200, 5767168, 149946368, 0), (21995978752, 5767168, 155713536, 0), (22001745920, 5767168, 161480704, 0), (21990211584, 5767168, 167247872, 0), (22566928384, 5767168, 173015040, 0), (22572695552, 5767168, 178782208, 0), (22561161216, 5767168, 184549376, 0), (22636134400, 5767168, 190316544, 0), (22641901568, 5767168, 196083712, 0), (22630367232, 5767168, 201850880, 0), (22065184768, 5767168, 207618048, 0), (22070951936, 5767168, 213385216, 0), (22059417600, 5767168, 219152384, 0), (22082486272, 5767168, 224919552, 0), (22088253440, 5767168, 230686720, 0), (22076719104, 5767168, 236453888, 0), (22670737408, 5767168, 242221056, 0), (22676504576, 5767168, 247988224, 0), (22664970240, 5767168, 253755392, 0), (22739943424, 5767168, 259522560, 0), (22745710592, 5767168, 265289728, 0), (22734176256, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e446e2f40>, 2: <capsule object NULL at 0x7a4e547ae700>}
DEBUG 01-15 16:10:44.116864.116864 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.116892.116892 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.116218.116218 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7b925ec5-bf5b-4cb9-acfb-080edaf9d839
DEBUG 01-15 16:10:44.116483.116483 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.116591.116591 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:44.116075.116075 client.py:127] Model loaded
DEBUG 01-15 16:10:44.117534.117534 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.117117.117117 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.117150.117150 cuda_h.py:19] end restore2model cost 0.0003437995910644531 seconds
DEBUG 01-15 16:10:44.117350.117350 cuda_h.py:19] end sllm_worker_task cost 0.010706424713134766 seconds
INFO 01-15 16:10:44.117202.117202 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7b925ec5-bf5b-4cb9-acfb-080edaf9d839
DEBUG 01-15 16:10:44.117643.117643 cuda_h.py:19] end move_flatidxs cost 0.0008254051208496094 seconds
DEBUG 01-15 16:10:44.117228.117228 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:44.118030.118030 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003849029541015625 seconds
DEBUG 01-15 16:10:44.118086.118086 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.120880.120880 cuda_h.py:19] end restore2model cost 0.0025186538696289062 seconds
DEBUG 01-15 16:10:44.120729.120729 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0065991878509521484 seconds
DEBUG 01-15 16:10:44.120240.120240 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.121833.121833 cuda_h.py:19] end gpu_sexperts cost 0.00026416778564453125 seconds
DEBUG 01-15 16:10:44.121232.121232 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.121962.121962 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.811981201171875e-05 seconds
DEBUG 01-15 16:10:44.121850.121850 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.121268.121268 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.121197.121197 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007944107055664062 seconds
DEBUG 01-15 16:10:44.122848.122848 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.122556.122556 cuda_h.py:19] end gpu_group_list cost 0.0001780986785888672 seconds
DEBUG 01-15 16:10:44.122740.122740 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.123562.123562 cuda_h.py:19] end group_tensors cost 0.0051958560943603516 seconds
DEBUG 01-15 16:10:44.123135.123135 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.123175.123175 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008590221405029297 seconds
DEBUG 01-15 16:10:44.123064.123064 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.124379.124379 cuda_h.py:19] end gpu_group_list cost 0.0004677772521972656 seconds
DEBUG 01-15 16:10:44.125956.125956 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.125792.125792 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7b925ec5-bf5b-4cb9-acfb-080edaf9d839
DEBUG 01-15 16:10:44.143115.143115 cuda_h.py:19] end group pad cost 0.01913928985595703 seconds
DEBUG 01-15 16:10:44.143634.143634 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.143789.143789 client.py:127] Model loaded
DEBUG 01-15 16:10:44.143871.143871 cuda_h.py:19] end wait_experts_multi_device cost 0.017965078353881836 seconds
DEBUG 01-15 16:10:44.143196.143196 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.168435.168435 cuda_h.py:19] end group_einsum cost 0.025014638900756836 seconds
DEBUG 01-15 16:10:44.168839.168839 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.172218.172218 cuda_h.py:19] end get_outputs_cpu1 cost 0.003916025161743164 seconds
DEBUG 01-15 16:10:44.173929.173929 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05629920959472656 seconds
DEBUG 01-15 16:10:44.173499.173499 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.030154943466186523 seconds
DEBUG 01-15 16:10:44.173781.173781 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.174709.174709 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.175727.175727 cuda_h.py:19] end index_scatter cost 7.62939453125e-05 seconds
DEBUG 01-15 16:10:44.175247.175247 cuda_h.py:19] end cpuoutputsdeal cost 0.0016384124755859375 seconds
DEBUG 01-15 16:10:44.175641.175641 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.175450.175450 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.175734.175734 cuda_h.py:19] end gpu_group_tensor cost 0.0001423358917236328 seconds
DEBUG 01-15 16:10:44.175735.175735 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.175244.175244 cuda_h.py:19] end gpu_group_tensor cost 0.0001266002655029297 seconds
DEBUG 01-15 16:10:44.175579.175579 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.176548.176548 cuda_h.py:19] end gpu_group_einsum cost 0.0006299018859863281 seconds
DEBUG 01-15 16:10:44.176123.176123 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.177112.177112 cuda_h.py:19] end gpu_group_einsum cost 0.00041484832763671875 seconds
DEBUG 01-15 16:10:44.177778.177778 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.177007.177007 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.177808.177808 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016641616821289062 seconds
DEBUG 01-15 16:10:44.177756.177756 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.177911.177911 cuda_h.py:19] end concat_expert_out cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:10:44.177277.177277 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.177300.177300 cuda_h.py:19] end index_scatter cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:44.178818.178818 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007565021514892578 seconds
DEBUG 01-15 16:10:44.178709.178709 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.178598.178598 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.178014.178014 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013446807861328125 seconds
DEBUG 01-15 16:10:44.178009.178009 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.178601.178601 cuda_h.py:19] end concat_expert_out cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:10:44.178365.178365 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.178381.178381 cuda_h.py:19] end index_scatter cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:44.178190.178190 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004935264587402344 seconds
DEBUG 01-15 16:10:44.178332.178332 cuda_h.py:19] end gpu_experts_multi_device cost 0.05767011642456055 seconds
DEBUG 01-15 16:10:44.178957.178957 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.06759810447692871 seconds
DEBUG 01-15 16:10:44.179479.179479 cuda_h.py:19] end prefill_layer cost 0.07296228408813477 seconds
DEBUG 01-15 16:10:44.179184.179184 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:10:44.179840.179840 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.179212.179212 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:10:44.179776.179776 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:44.179055.179055 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:44.179051.179051 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 3.886222839355469e-05 seconds
DEBUG 01-15 16:10:44.179138.179138 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:10:44.179503.179503 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.179671.179671 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.179071.179071 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.179816.179816 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.179229.179229 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.180252.180252 cuda_h.py:19] end allocate_cuda_memory cost 0.0002582073211669922 seconds
DEBUG 01-15 16:10:44.180983.180983 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.180845.180845 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.180915.180915 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.180724.180724 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6df2f9e0-760e-4cdd-9f64-0130875e619e
DEBUG 01-15 16:10:44.180111.180111 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.180272.180272 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.181538.181538 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6df2f9e0-760e-4cdd-9f64-0130875e619e
DEBUG 01-15 16:10:44.181096.181096 cuda_h.py:19] end load_into_gpu_async cost 0.0015535354614257812 seconds
DEBUG 01-15 16:10:44.181945.181945 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.182724.182724 cuda_h.py:19] end restore_tensors2 cost 8.630752563476562e-05 seconds
DEBUG 01-15 16:10:44.182963.182963 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021810531616210938 seconds
INFO 01-15 16:10:44.182906.182906 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6df2f9e0-760e-4cdd-9f64-0130875e619e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.184237.184237 cuda_h.py:19] end self_attn cost 0.003622770309448242 seconds
DEBUG 01-15 16:10:44.184818.184818 cuda_h.py:19] end iln_self_attn_paln cost 0.005219697952270508 seconds
DEBUG 01-15 16:10:44.184839.184839 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:10:44.184840.184840 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.185638.185638 cuda_h.py:19] end gate cost 0.0008296966552734375 seconds
DEBUG 01-15 16:10:44.185759.185759 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.186818.186818 lmp.py:1912] 
DEBUG 01-15 16:10:44.186818.186818 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.186959.186959 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.186324.186324 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.186351.186351 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.186232.186232 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.186637.186637 lmp.py:1917] 
DEBUG 01-15 16:10:44.186637.186637 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.186756.186756 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.186360.186360 lmp.py:1935]   Expert 44 |     39 | CPU
DEBUG 01-15 16:10:44.186764.186764 lmp.py:1935]   Expert  1 |     47 | CPU
DEBUG 01-15 16:10:44.186692.186692 lmp.py:1935]   Expert 60 |     61 | CPU
DEBUG 01-15 16:10:44.186143.186143 lmp.py:1935]   Expert 28 |     69 | CPU
DEBUG 01-15 16:10:44.186832.186832 lmp.py:1935]   Expert 48 |     77 | CPU
DEBUG 01-15 16:10:44.186045.186045 lmp.py:1935]   Expert 27 |     86 | CPU
DEBUG 01-15 16:10:44.186264.186264 lmp.py:1935]   Expert  0 |    102 | CPU
DEBUG 01-15 16:10:44.186099.186099 lmp.py:1935]   Expert 62 |    103 | CPU
DEBUG 01-15 16:10:44.186219.186219 lmp.py:1935]   Expert 42 |    112 | CPU
DEBUG 01-15 16:10:44.186100.186100 lmp.py:1935]   Expert 22 |    114 | CPU
DEBUG 01-15 16:10:44.186935.186935 lmp.py:1935]   Expert 30 |    114 | CPU
DEBUG 01-15 16:10:44.186578.186578 lmp.py:1935]   Expert 59 |    118 | CPU
DEBUG 01-15 16:10:44.186221.186221 lmp.py:1935]   Expert 58 |    121 | CPU
DEBUG 01-15 16:10:44.186626.186626 lmp.py:1935]   Expert 16 |    126 | CPU
DEBUG 01-15 16:10:44.186269.186269 lmp.py:1935]   Expert  8 |    128 | CPU
DEBUG 01-15 16:10:44.186673.186673 lmp.py:1935]   Expert 12 |    129 | CPU
DEBUG 01-15 16:10:44.186078.186078 lmp.py:1935]   Expert 50 |    135 | CPU
DEBUG 01-15 16:10:44.186483.186483 lmp.py:1935]   Expert 56 |    144 | CPU
DEBUG 01-15 16:10:44.186887.186887 lmp.py:1935]   Expert  5 |    145 | CPU
DEBUG 01-15 16:10:44.186245.186245 lmp.py:1935]   Expert 55 |    150 | CPU
DEBUG 01-15 16:10:44.186842.186842 lmp.py:1935]   Expert 15 |    151 | CPU
DEBUG 01-15 16:10:44.186723.186723 lmp.py:1935]   Expert 57 |    152 | CPU
DEBUG 01-15 16:10:44.186082.186082 lmp.py:1935]   Expert 26 |    153 | CPU
DEBUG 01-15 16:10:44.186725.186725 lmp.py:1935]   Expert 32 |    155 | CPU
DEBUG 01-15 16:10:44.186129.186129 lmp.py:1935]   Expert 47 |    159 | CPU
DEBUG 01-15 16:10:44.186057.186057 lmp.py:1935]   Expert 34 |    162 | CPU
DEBUG 01-15 16:10:44.186461.186461 lmp.py:1935]   Expert 24 |    164 | CPU
DEBUG 01-15 16:10:44.186628.186628 lmp.py:1935]   Expert  2 |    166 | CPU
DEBUG 01-15 16:10:44.186794.186794 lmp.py:1935]   Expert 52 |    167 | CPU
DEBUG 01-15 16:10:44.186960.186960 lmp.py:1935]   Expert 40 |    169 | CPU
DEBUG 01-15 16:10:44.186364.186364 lmp.py:1935]   Expert  6 |    172 | CPU
DEBUG 01-15 16:10:44.186246.186246 lmp.py:1935]   Expert 13 |    173 | CPU
DEBUG 01-15 16:10:44.186273.186273 lmp.py:1935]   Expert 41 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.186346.186346 lmp.py:1935]   Expert 18 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187897.187897 lmp.py:1935]   Expert  3 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187255.187255 lmp.py:1935]   Expert 54 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187852.187852 lmp.py:1935]   Expert 19 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187210.187210 lmp.py:1935]   Expert 37 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187330.187330 lmp.py:1935]   Expert 46 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187211.187211 lmp.py:1935]   Expert 20 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187092.187092 lmp.py:1935]   Expert 25 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187689.187689 lmp.py:1935]   Expert 51 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187432.187432 lmp.py:1935]   Expert 17 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187505.187505 lmp.py:1935]   Expert 43 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187221.187221 lmp.py:1935]   Expert 11 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187103.187103 lmp.py:1935]   Expert 31 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187507.187507 lmp.py:1935]   Expert 35 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187150.187150 lmp.py:1935]   Expert 23 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187032.187032 lmp.py:1935]   Expert 49 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187675.187675 lmp.py:1935]   Expert 39 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187748.187748 lmp.py:1935]   Expert 53 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187106.187106 lmp.py:1935]   Expert 10 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187465.187465 lmp.py:1935]   Expert 33 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187823.187823 lmp.py:1935]   Expert 36 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187466.187466 lmp.py:1935]   Expert 38 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187347.187347 lmp.py:1935]   Expert  4 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187229.187229 lmp.py:1935]   Expert 21 |    331 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187633.187633 lmp.py:1935]   Expert 14 |    349 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187515.187515 lmp.py:1935]   Expert 63 |    370 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187919.187919 lmp.py:1935]   Expert 45 |    371 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187562.187562 lmp.py:1935]   Expert 61 |    389 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187444.187444 lmp.py:1935]   Expert  9 |    390 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187040.187040 lmp.py:1935]   Expert 29 |    492 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.187160.187160 lmp.py:1935]   Expert  7 |    513 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.187565.187565 lmp.py:1937] 
DEBUG 01-15 16:10:44.187565.187565 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.187161.187161 lmp.py:1938]   CPU:   4063 tokens
DEBUG 01-15 16:10:44.187758.187758 lmp.py:1942]   cuda:1:   4113 tokens (16 experts)
DEBUG 01-15 16:10:44.187162.187162 lmp.py:1942]   cuda:2:   4112 tokens (16 experts)
DEBUG 01-15 16:10:44.187329.187329 lmp.py:1943]   Total GPU:   8225 tokens
DEBUG 01-15 16:10:44.187541.187541 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.187541.187541 lmp.py:1944] 
DEBUG 01-15 16:10:44.187191.187191 cuda_h.py:19] end experts_map_get cost 0.001796722412109375 seconds
DEBUG 01-15 16:10:44.187325.187325 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.187366.187366 lmp.py:1953] 
DEBUG 01-15 16:10:44.187366.187366 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.187719.187719 cuda_h.py:19] end cpu_experts_submit cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:10:44.187653.187653 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.187814.187814 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.188823.188823 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.35.gate_proj.weight': 0, 'model.layers.19.mlp.experts.35.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.35.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.19.mlp.experts.33.gate_proj.weight': 0, 'model.layers.19.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.46.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.46.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.46.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.49.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.49.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.49.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.17.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.17.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.17.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.51.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.51.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.51.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.53.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.53.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.53.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(23397400576, 5767168, 0, 0), (23403167744, 5767168, 5767168, 0), (23391633408, 5767168, 11534336, 0), (23414702080, 5767168, 17301504, 0), (23420469248, 5767168, 23068672, 0), (23408934912, 5767168, 28835840, 0), (23449305088, 5767168, 34603008, 0), (23455072256, 5767168, 40370176, 0), (23443537920, 5767168, 46137344, 0), (22912958464, 5767168, 51904512, 0), (22918725632, 5767168, 57671680, 0), (22907191296, 5767168, 63438848, 0), (23466606592, 5767168, 69206016, 0), (23472373760, 5767168, 74973184, 0), (23460839424, 5767168, 80740352, 0), (22964862976, 5767168, 86507520, 0), (22970630144, 5767168, 92274688, 0), (22959095808, 5767168, 98041856, 0), (23535812608, 5767168, 103809024, 0), (23541579776, 5767168, 109576192, 0), (23530045440, 5767168, 115343360, 0), (23103275008, 5767168, 121110528, 0), (23109042176, 5767168, 126877696, 0), (23097507840, 5767168, 132644864, 0), (23120576512, 5767168, 138412032, 0), (23126343680, 5767168, 144179200, 0), (23114809344, 5767168, 149946368, 0), (23137878016, 5767168, 155713536, 0), (23143645184, 5767168, 161480704, 0), (23132110848, 5767168, 167247872, 0), (23155179520, 5767168, 173015040, 0), (23160946688, 5767168, 178782208, 0), (23149412352, 5767168, 184549376, 0), (23726129152, 5767168, 190316544, 0), (23731896320, 5767168, 196083712, 0), (23720361984, 5767168, 201850880, 0), (23189782528, 5767168, 207618048, 0), (23195549696, 5767168, 213385216, 0), (23184015360, 5767168, 219152384, 0), (23224385536, 5767168, 224919552, 0), (23230152704, 5767168, 230686720, 0), (23218618368, 5767168, 236453888, 0), (23847239680, 5767168, 242221056, 0), (23853006848, 5767168, 247988224, 0), (23841472512, 5767168, 253755392, 0), (23881842688, 5767168, 259522560, 0), (23887609856, 5767168, 265289728, 0), (23876075520, 5767168, 271056896, 0)], 2: [(23362797568, 5767168, 0, 0), (23368564736, 5767168, 5767168, 0), (23357030400, 5767168, 11534336, 0), (22843752448, 5767168, 17301504, 0), (22849519616, 5767168, 23068672, 0), (22837985280, 5767168, 28835840, 0), (22861053952, 5767168, 34603008, 0), (22866821120, 5767168, 40370176, 0), (22855286784, 5767168, 46137344, 0), (23432003584, 5767168, 51904512, 0), (23437770752, 5767168, 57671680, 0), (23426236416, 5767168, 63438848, 0), (22947561472, 5767168, 69206016, 0), (22953328640, 5767168, 74973184, 0), (22941794304, 5767168, 80740352, 0), (23501209600, 5767168, 86507520, 0), (23506976768, 5767168, 92274688, 0), (23495442432, 5767168, 98041856, 0), (22982164480, 5767168, 103809024, 0), (22987931648, 5767168, 109576192, 0), (22976397312, 5767168, 115343360, 0), (23570415616, 5767168, 121110528, 0), (23576182784, 5767168, 126877696, 0), (23564648448, 5767168, 132644864, 0), (23034068992, 5767168, 138412032, 0), (23039836160, 5767168, 144179200, 0), (23028301824, 5767168, 149946368, 0), (23587717120, 5767168, 155713536, 0), (23593484288, 5767168, 161480704, 0), (23581949952, 5767168, 167247872, 0), (23639621632, 5767168, 173015040, 0), (23645388800, 5767168, 178782208, 0), (23633854464, 5767168, 184549376, 0), (23085973504, 5767168, 190316544, 0), (23091740672, 5767168, 196083712, 0), (23080206336, 5767168, 201850880, 0), (23674224640, 5767168, 207618048, 0), (23679991808, 5767168, 213385216, 0), (23668457472, 5767168, 219152384, 0), (23708827648, 5767168, 224919552, 0), (23714594816, 5767168, 230686720, 0), (23703060480, 5767168, 236453888, 0), (23293591552, 5767168, 242221056, 0), (23299358720, 5767168, 247988224, 0), (23287824384, 5767168, 253755392, 0), (23328194560, 5767168, 259522560, 0), (23333961728, 5767168, 265289728, 0), (23322427392, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec46930c0>, 2: <capsule object NULL at 0x7a4e547ae370>}
DEBUG 01-15 16:10:44.188854.188854 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.188843.188843 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.188453.188453 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c318430e-e889-4797-8180-811e1d42152f
DEBUG 01-15 16:10:44.189751.189751 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.189621.189621 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:44.189820.189820 client.py:127] Model loaded
DEBUG 01-15 16:10:44.189831.189831 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.189538.189538 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.189950.189950 cuda_h.py:19] end restore2model cost 0.00037097930908203125 seconds
DEBUG 01-15 16:10:44.189342.189342 cuda_h.py:19] end sllm_worker_task cost 0.009983062744140625 seconds
INFO 01-15 16:10:44.190909.190909 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c318430e-e889-4797-8180-811e1d42152f
DEBUG 01-15 16:10:44.190304.190304 cuda_h.py:19] end move_flatidxs cost 0.0008573532104492188 seconds
DEBUG 01-15 16:10:44.190749.190749 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:44.190524.190524 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002599954605102539 seconds
DEBUG 01-15 16:10:44.190004.190004 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.193441.193441 cuda_h.py:19] end restore2model cost 0.0025327205657958984 seconds
DEBUG 01-15 16:10:44.193184.193184 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005354881286621094 seconds
DEBUG 01-15 16:10:44.193364.193364 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.193626.193626 cuda_h.py:19] end gpu_sexperts cost 0.00026535987854003906 seconds
DEBUG 01-15 16:10:44.193786.193786 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.193324.193324 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:10:44.193212.193212 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.193915.193915 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.194042.194042 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008008480072021484 seconds
DEBUG 01-15 16:10:44.194932.194932 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.194164.194164 cuda_h.py:19] end gpu_group_list cost 0.00017833709716796875 seconds
DEBUG 01-15 16:10:44.195056.195056 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.196074.196074 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008666515350341797 seconds
DEBUG 01-15 16:10:44.196109.196109 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.196295.196295 cuda_h.py:19] end gpu_group_list cost 0.0001773834228515625 seconds
DEBUG 01-15 16:10:44.197460.197460 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.197481.197481 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c318430e-e889-4797-8180-811e1d42152f
DEBUG 01-15 16:10:44.197986.197986 cuda_h.py:19] end group_tensors cost 0.00662541389465332 seconds
DEBUG 01-15 16:10:44.198586.198586 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.203001.203001 cuda_h.py:19] end group pad cost 0.005235910415649414 seconds
DEBUG 01-15 16:10:44.203182.203182 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.216274.216274 client.py:127] Model loaded
DEBUG 01-15 16:10:44.216413.216413 cuda_h.py:19] end wait_experts_multi_device cost 0.019309282302856445 seconds
DEBUG 01-15 16:10:44.216972.216972 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.225544.225544 cuda_h.py:19] end group_einsum cost 0.021573543548583984 seconds
DEBUG 01-15 16:10:44.225595.225595 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.229338.229338 cuda_h.py:19] end get_outputs_cpu1 cost 0.004134178161621094 seconds
DEBUG 01-15 16:10:44.230705.230705 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04079890251159668 seconds
DEBUG 01-15 16:10:44.230467.230467 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013742685317993164 seconds
DEBUG 01-15 16:10:44.230325.230325 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.231234.231234 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.232504.232504 cuda_h.py:19] end index_scatter cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:10:44.232833.232833 cuda_h.py:19] end cpuoutputsdeal cost 0.001668691635131836 seconds
DEBUG 01-15 16:10:44.232174.232174 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.232414.232414 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.232790.232790 cuda_h.py:19] end gpu_group_tensor cost 0.00013899803161621094 seconds
DEBUG 01-15 16:10:44.232030.232030 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.232916.232916 cuda_h.py:19] end gpu_group_tensor cost 0.00013017654418945312 seconds
DEBUG 01-15 16:10:44.232959.232959 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.233096.233096 cuda_h.py:19] end gpu_group_einsum cost 0.00047469139099121094 seconds
DEBUG 01-15 16:10:44.233305.233305 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.234478.234478 cuda_h.py:19] end gpu_group_einsum cost 0.00035881996154785156 seconds
DEBUG 01-15 16:10:44.234144.234144 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.234564.234564 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.234095.234095 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001766681671142578 seconds
DEBUG 01-15 16:10:44.234950.234950 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.234589.234589 cuda_h.py:19] end concat_expert_out cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:10:44.234293.234293 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.234991.234991 cuda_h.py:19] end index_scatter cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:10:44.234979.234979 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007815361022949219 seconds
DEBUG 01-15 16:10:44.235585.235585 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.235157.235157 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.235395.235395 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013971328735351562 seconds
DEBUG 01-15 16:10:44.235296.235296 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.235034.235034 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:44.235831.235831 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.235662.235662 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:44.235140.235140 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005114078521728516 seconds
DEBUG 01-15 16:10:44.235096.235096 cuda_h.py:19] end gpu_experts_multi_device cost 0.04199099540710449 seconds
DEBUG 01-15 16:10:44.235913.235913 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.05082893371582031 seconds
DEBUG 01-15 16:10:44.236383.236383 cuda_h.py:19] end prefill_layer cost 0.056722402572631836 seconds
DEBUG 01-15 16:10:44.236517.236517 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:10:44.236174.236174 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.236784.236784 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:10:44.236587.236587 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:44.236104.236104 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:44.236597.236597 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 4.172325134277344e-05 seconds
DEBUG 01-15 16:10:44.236929.236929 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 9.036064147949219e-05 seconds
DEBUG 01-15 16:10:44.236678.236678 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.236741.236741 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.236141.236141 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.236210.236210 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.236954.236954 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.237388.237388 cuda_h.py:19] end allocate_cuda_memory cost 0.0002808570861816406 seconds
DEBUG 01-15 16:10:44.237265.237265 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.237604.237604 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.237341.237341 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.237197.237197 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 82540f99-b29b-45b1-aa78-e1598c88677f
DEBUG 01-15 16:10:44.237631.237631 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.237263.237263 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.238936.238936 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 82540f99-b29b-45b1-aa78-e1598c88677f
DEBUG 01-15 16:10:44.238971.238971 cuda_h.py:19] end load_into_gpu_async cost 0.0014641284942626953 seconds
DEBUG 01-15 16:10:44.238581.238581 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.238268.238268 cuda_h.py:19] end restore_tensors2 cost 8.845329284667969e-05 seconds
DEBUG 01-15 16:10:44.238554.238554 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021157264709472656 seconds
INFO 01-15 16:10:44.238417.238417 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 82540f99-b29b-45b1-aa78-e1598c88677f
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.241525.241525 cuda_h.py:19] end self_attn cost 0.003590822219848633 seconds
DEBUG 01-15 16:10:44.241948.241948 cuda_h.py:19] end iln_self_attn_paln cost 0.005253791809082031 seconds
DEBUG 01-15 16:10:44.241832.241832 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:10:44.241099.241099 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.242133.242133 cuda_h.py:19] end gate cost 0.0007829666137695312 seconds
DEBUG 01-15 16:10:44.242976.242976 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.243374.243374 lmp.py:1912] 
DEBUG 01-15 16:10:44.243374.243374 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.243276.243276 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.243641.243641 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.243668.243668 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.243026.243026 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.243146.243146 lmp.py:1917] 
DEBUG 01-15 16:10:44.243146.243146 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.243173.243173 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.243492.243492 lmp.py:1935]   Expert 54 |     21 | CPU
DEBUG 01-15 16:10:44.243088.243088 lmp.py:1935]   Expert  3 |     33 | CPU
DEBUG 01-15 16:10:44.243970.243970 lmp.py:1935]   Expert  8 |     41 | CPU
DEBUG 01-15 16:10:44.243374.243374 lmp.py:1935]   Expert 28 |     45 | CPU
DEBUG 01-15 16:10:44.243779.243779 lmp.py:1935]   Expert 43 |     54 | CPU
DEBUG 01-15 16:10:44.243945.243945 lmp.py:1935]   Expert 63 |     56 | CPU
DEBUG 01-15 16:10:44.243065.243065 lmp.py:1935]   Expert 36 |     77 | CPU
DEBUG 01-15 16:10:44.243423.243423 lmp.py:1935]   Expert  6 |     80 | CPU
DEBUG 01-15 16:10:44.243543.243543 lmp.py:1935]   Expert 38 |     80 | CPU
DEBUG 01-15 16:10:44.243663.243663 lmp.py:1935]   Expert 57 |     96 | CPU
DEBUG 01-15 16:10:44.243306.243306 lmp.py:1935]   Expert 39 |     97 | CPU
DEBUG 01-15 16:10:44.243710.243710 lmp.py:1935]   Expert 41 |    105 | CPU
DEBUG 01-15 16:10:44.243115.243115 lmp.py:1935]   Expert 12 |    106 | CPU
DEBUG 01-15 16:10:44.243281.243281 lmp.py:1935]   Expert 52 |    110 | CPU
DEBUG 01-15 16:10:44.243685.243685 lmp.py:1935]   Expert 19 |    120 | CPU
DEBUG 01-15 16:10:44.243282.243282 lmp.py:1935]   Expert 47 |    126 | CPU
DEBUG 01-15 16:10:44.243164.243164 lmp.py:1935]   Expert 13 |    135 | CPU
DEBUG 01-15 16:10:44.243045.243045 lmp.py:1935]   Expert 46 |    144 | CPU
DEBUG 01-15 16:10:44.243403.243403 lmp.py:1935]   Expert 22 |    145 | CPU
DEBUG 01-15 16:10:44.243046.243046 lmp.py:1935]   Expert 50 |    146 | CPU
DEBUG 01-15 16:10:44.243451.243451 lmp.py:1935]   Expert 20 |    161 | CPU
DEBUG 01-15 16:10:44.243855.243855 lmp.py:1935]   Expert 40 |    162 | CPU
DEBUG 01-15 16:10:44.243021.243021 lmp.py:1935]   Expert 24 |    163 | CPU
DEBUG 01-15 16:10:44.243426.243426 lmp.py:1935]   Expert 55 |    167 | CPU
DEBUG 01-15 16:10:44.243830.243830 lmp.py:1935]   Expert 23 |    171 | CPU
DEBUG 01-15 16:10:44.243758.243758 lmp.py:1935]   Expert 37 |    171 | CPU
DEBUG 01-15 16:10:44.243116.243116 lmp.py:1935]   Expert 53 |    173 | CPU
DEBUG 01-15 16:10:44.243998.243998 lmp.py:1935]   Expert  2 |    176 | CPU
DEBUG 01-15 16:10:44.243641.243641 lmp.py:1935]   Expert 42 |    176 | CPU
DEBUG 01-15 16:10:44.243284.243284 lmp.py:1935]   Expert 61 |    177 | CPU
DEBUG 01-15 16:10:44.243880.243880 lmp.py:1935]   Expert 21 |    178 | CPU
DEBUG 01-15 16:10:44.243523.243523 lmp.py:1935]   Expert 49 |    184 | CPU
DEBUG 01-15 16:10:44.243358.243358 lmp.py:1935]   Expert 18 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.243717.243717 lmp.py:1935]   Expert 33 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.243552.243552 lmp.py:1935]   Expert  0 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.243148.243148 lmp.py:1935]   Expert 32 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.243268.243268 lmp.py:1935]   Expert 30 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244865.244865 lmp.py:1935]   Expert  5 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244223.244223 lmp.py:1935]   Expert 14 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244535.244535 lmp.py:1935]   Expert 16 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244847.244847 lmp.py:1935]   Expert  7 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244682.244682 lmp.py:1935]   Expert 34 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244279.244279 lmp.py:1935]   Expert 31 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244637.244637 lmp.py:1935]   Expert 60 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244757.244757 lmp.py:1935]   Expert 62 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244876.244876 lmp.py:1935]   Expert  9 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244996.244996 lmp.py:1935]   Expert 59 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244116.244116 lmp.py:1935]   Expert 17 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244236.244236 lmp.py:1935]   Expert 10 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244833.244833 lmp.py:1935]   Expert 29 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244668.244668 lmp.py:1935]   Expert 15 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244218.244218 lmp.py:1935]   Expert  4 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244053.244053 lmp.py:1935]   Expert 58 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244922.244922 lmp.py:1935]   Expert 26 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244042.244042 lmp.py:1935]   Expert 51 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244923.244923 lmp.py:1935]   Expert 11 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244805.244805 lmp.py:1935]   Expert 44 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244130.244130 lmp.py:1935]   Expert 56 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244773.244773 lmp.py:1935]   Expert 27 |    292 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244131.244131 lmp.py:1935]   Expert  1 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244490.244490 lmp.py:1935]   Expert 45 |    368 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244848.244848 lmp.py:1935]   Expert 25 |    462 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244206.244206 lmp.py:1935]   Expert 35 |    512 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.244087.244087 lmp.py:1935]   Expert 48 |    641 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.244777.244777 lmp.py:1937] 
DEBUG 01-15 16:10:44.244777.244777 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.244420.244420 lmp.py:1938]   CPU:   3876 tokens
DEBUG 01-15 16:10:44.244301.244301 lmp.py:1942]   cuda:1:   4206 tokens (16 experts)
DEBUG 01-15 16:10:44.244706.244706 lmp.py:1942]   cuda:2:   4206 tokens (16 experts)
DEBUG 01-15 16:10:44.244633.244633 lmp.py:1943]   Total GPU:   8412 tokens
DEBUG 01-15 16:10:44.244515.244515 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.244515.244515 lmp.py:1944] 
DEBUG 01-15 16:10:44.244357.244357 cuda_h.py:19] end experts_map_get cost 0.001828908920288086 seconds
DEBUG 01-15 16:10:44.244014.244014 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.244055.244055 lmp.py:1953] 
DEBUG 01-15 16:10:44.244055.244055 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.244361.244361 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:44.244581.244581 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.244695.244695 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.245755.245755 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.20.mlp.experts.32.gate_proj.weight': 0, 'model.layers.20.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.33.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.33.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.33.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.35.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.35.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.35.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.30.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.30.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.30.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (24487395328, 5767168, 17301504, 0), (24493162496, 5767168, 23068672, 0), (24481628160, 5767168, 28835840, 0), (23968350208, 5767168, 34603008, 0), (23974117376, 5767168, 40370176, 0), (23962583040, 5767168, 46137344, 0), (23985651712, 5767168, 51904512, 0), (23991418880, 5767168, 57671680, 0), (23979884544, 5767168, 63438848, 0), (24072159232, 5767168, 69206016, 0), (24077926400, 5767168, 74973184, 0), (24066392064, 5767168, 80740352, 0), (24677711872, 5767168, 86507520, 0), (24683479040, 5767168, 92274688, 0), (24671944704, 5767168, 98041856, 0), (24141365248, 5767168, 103809024, 0), (24147132416, 5767168, 109576192, 0), (24135598080, 5767168, 115343360, 0), (24729616384, 5767168, 121110528, 0), (24735383552, 5767168, 126877696, 0), (24723849216, 5767168, 132644864, 0), (24193269760, 5767168, 138412032, 0), (24199036928, 5767168, 144179200, 0), (24187502592, 5767168, 149946368, 0), (24210571264, 5767168, 155713536, 0), (24216338432, 5767168, 161480704, 0), (24204804096, 5767168, 167247872, 0), (24781520896, 5767168, 173015040, 0), (24787288064, 5767168, 178782208, 0), (24775753728, 5767168, 184549376, 0), (24868028416, 5767168, 190316544, 0), (24873795584, 5767168, 196083712, 0), (24862261248, 5767168, 201850880, 0), (24902631424, 5767168, 207618048, 0), (24908398592, 5767168, 213385216, 0), (24896864256, 5767168, 219152384, 0), (24366284800, 5767168, 224919552, 0), (24372051968, 5767168, 230686720, 0), (24360517632, 5767168, 236453888, 0), (24937234432, 5767168, 242221056, 0), (24943001600, 5767168, 247988224, 0), (24931467264, 5767168, 253755392, 0), (24435490816, 5767168, 259522560, 0), (24441257984, 5767168, 265289728, 0), (24429723648, 5767168, 271056896, 0)], 2: [(24452792320, 5767168, 0, 0), (24458559488, 5767168, 5767168, 0), (24447025152, 5767168, 11534336, 0), (23916445696, 5767168, 17301504, 0), (23922212864, 5767168, 23068672, 0), (23910678528, 5767168, 28835840, 0), (24470093824, 5767168, 34603008, 0), (24475860992, 5767168, 40370176, 0), (24464326656, 5767168, 46137344, 0), (24504696832, 5767168, 51904512, 0), (24510464000, 5767168, 57671680, 0), (24498929664, 5767168, 63438848, 0), (24020254720, 5767168, 69206016, 0), (24026021888, 5767168, 74973184, 0), (24014487552, 5767168, 80740352, 0), (24054857728, 5767168, 86507520, 0), (24060624896, 5767168, 92274688, 0), (24049090560, 5767168, 98041856, 0), (24089460736, 5767168, 103809024, 0), (24095227904, 5767168, 109576192, 0), (24083693568, 5767168, 115343360, 0), (24660410368, 5767168, 121110528, 0), (24666177536, 5767168, 126877696, 0), (24654643200, 5767168, 132644864, 0), (24158666752, 5767168, 138412032, 0), (24164433920, 5767168, 144179200, 0), (24152899584, 5767168, 149946368, 0), (24175968256, 5767168, 155713536, 0), (24181735424, 5767168, 161480704, 0), (24170201088, 5767168, 167247872, 0), (24418189312, 5767168, 173015040, 0), (24423956480, 5767168, 178782208, 0), (24412422144, 5767168, 184549376, 0), (24331681792, 5767168, 190316544, 0), (24337448960, 5767168, 196083712, 0), (24325914624, 5767168, 201850880, 0), (24348983296, 5767168, 207618048, 0), (24354750464, 5767168, 213385216, 0), (24343216128, 5767168, 219152384, 0), (24919932928, 5767168, 224919552, 0), (24925700096, 5767168, 230686720, 0), (24914165760, 5767168, 236453888, 0), (24400887808, 5767168, 242221056, 0), (24406654976, 5767168, 247988224, 0), (24395120640, 5767168, 253755392, 0), (24971837440, 5767168, 259522560, 0), (24977604608, 5767168, 265289728, 0), (24966070272, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e342570c0>, 2: <capsule object NULL at 0x7a4e547ae430>}
DEBUG 01-15 16:10:44.245488.245488 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.245975.245975 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.245539.245539 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5b360044-c98d-46e1-9ec1-a3e4141d54c9
DEBUG 01-15 16:10:44.246976.246976 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.246101.246101 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:44.246040.246040 client.py:127] Model loaded
DEBUG 01-15 16:10:44.246214.246214 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.246488.246488 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.246002.246002 cuda_h.py:19] end restore2model cost 0.000335693359375 seconds
DEBUG 01-15 16:10:44.246486.246486 cuda_h.py:19] end sllm_worker_task cost 0.010043621063232422 seconds
INFO 01-15 16:10:44.246865.246865 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5b360044-c98d-46e1-9ec1-a3e4141d54c9
DEBUG 01-15 16:10:44.247498.247498 cuda_h.py:19] end move_flatidxs cost 0.0008242130279541016 seconds
DEBUG 01-15 16:10:44.247367.247367 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:44.247858.247858 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002477407455444336 seconds
DEBUG 01-15 16:10:44.247907.247907 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.249708.249708 cuda_h.py:19] end restore2model cost 0.002521991729736328 seconds
DEBUG 01-15 16:10:44.250081.250081 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00522613525390625 seconds
DEBUG 01-15 16:10:44.250566.250566 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.250450.250450 cuda_h.py:19] end gpu_sexperts cost 0.0002675056457519531 seconds
DEBUG 01-15 16:10:44.250657.250657 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.250480.250480 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:44.250891.250891 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.250594.250594 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.251867.251867 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008032321929931641 seconds
DEBUG 01-15 16:10:44.251286.251286 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.251187.251187 cuda_h.py:19] end gpu_group_list cost 0.00017833709716796875 seconds
DEBUG 01-15 16:10:44.252663.252663 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.253444.253444 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009007453918457031 seconds
DEBUG 01-15 16:10:44.253678.253678 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.253148.253148 cuda_h.py:19] end gpu_group_list cost 0.0001761913299560547 seconds
DEBUG 01-15 16:10:44.254221.254221 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.254050.254050 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5b360044-c98d-46e1-9ec1-a3e4141d54c9
DEBUG 01-15 16:10:44.256300.256300 cuda_h.py:19] end group_tensors cost 0.009254217147827148 seconds
DEBUG 01-15 16:10:44.257710.257710 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.261366.261366 cuda_h.py:19] end group pad cost 0.004116535186767578 seconds
DEBUG 01-15 16:10:44.261441.261441 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.273813.273813 client.py:127] Model loaded
DEBUG 01-15 16:10:44.273951.273951 cuda_h.py:19] end wait_experts_multi_device cost 0.01935863494873047 seconds
DEBUG 01-15 16:10:44.273079.273079 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.282429.282429 cuda_h.py:19] end group_einsum cost 0.020519018173217773 seconds
DEBUG 01-15 16:10:44.282659.282659 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.286979.286979 cuda_h.py:19] end get_outputs_cpu1 cost 0.0039215087890625 seconds
DEBUG 01-15 16:10:44.287976.287976 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.040886878967285156 seconds
DEBUG 01-15 16:10:44.287998.287998 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013738870620727539 seconds
DEBUG 01-15 16:10:44.287478.287478 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.288037.288037 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.289546.289546 cuda_h.py:19] end index_scatter cost 7.62939453125e-05 seconds
DEBUG 01-15 16:10:44.289867.289867 cuda_h.py:19] end cpuoutputsdeal cost 0.0016834735870361328 seconds
DEBUG 01-15 16:10:44.289261.289261 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.289739.289739 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.289460.289460 cuda_h.py:19] end gpu_group_tensor cost 0.00014662742614746094 seconds
DEBUG 01-15 16:10:44.289938.289938 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.289533.289533 cuda_h.py:19] end gpu_group_tensor cost 0.00012683868408203125 seconds
DEBUG 01-15 16:10:44.289073.289073 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.290273.290273 cuda_h.py:19] end gpu_group_einsum cost 0.0005860328674316406 seconds
DEBUG 01-15 16:10:44.290848.290848 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.291805.291805 cuda_h.py:19] end gpu_group_einsum cost 0.00043702125549316406 seconds
DEBUG 01-15 16:10:44.291855.291855 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.291196.291196 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.291269.291269 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015687942504882812 seconds
DEBUG 01-15 16:10:44.291740.291740 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.291226.291226 cuda_h.py:19] end concat_expert_out cost 4.6253204345703125e-05 seconds
DEBUG 01-15 16:10:44.291639.291639 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.291992.291992 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:44.292166.292166 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007441043853759766 seconds
DEBUG 01-15 16:10:44.292242.292242 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.292370.292370 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.292223.292223 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014138221740722656 seconds
DEBUG 01-15 16:10:44.292694.292694 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.292048.292048 cuda_h.py:19] end concat_expert_out cost 5.364418029785156e-05 seconds
DEBUG 01-15 16:10:44.292176.292176 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.292576.292576 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:44.292716.292716 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004901885986328125 seconds
DEBUG 01-15 16:10:44.292050.292050 cuda_h.py:19] end gpu_experts_multi_device cost 0.04229879379272461 seconds
DEBUG 01-15 16:10:44.292344.292344 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.051027536392211914 seconds
DEBUG 01-15 16:10:44.293767.293767 cuda_h.py:19] end prefill_layer cost 0.05704092979431152 seconds
DEBUG 01-15 16:10:44.293835.293835 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:10:44.293253.293253 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.293671.293671 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:10:44.293520.293520 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:44.293514.293514 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:44.293417.293417 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 4.00543212890625e-05 seconds
DEBUG 01-15 16:10:44.293650.293650 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 7.390975952148438e-05 seconds
DEBUG 01-15 16:10:44.293254.293254 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.293501.293501 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.293782.293782 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.293421.293421 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.293788.293788 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.294989.294989 cuda_h.py:19] end allocate_cuda_memory cost 0.0002524852752685547 seconds
DEBUG 01-15 16:10:44.294866.294866 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.294583.294583 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.294505.294505 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.294453.294453 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 83fdcac8-877f-4c0a-bca9-348f6c3b1e60
DEBUG 01-15 16:10:44.294695.294695 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.294487.294487 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.296431.296431 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 83fdcac8-877f-4c0a-bca9-348f6c3b1e60
DEBUG 01-15 16:10:44.296843.296843 cuda_h.py:19] end load_into_gpu_async cost 0.0018420219421386719 seconds
DEBUG 01-15 16:10:44.296930.296930 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.296808.296808 cuda_h.py:19] end restore_tensors2 cost 8.940696716308594e-05 seconds
DEBUG 01-15 16:10:44.296379.296379 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024628639221191406 seconds
INFO 01-15 16:10:44.296297.296297 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 83fdcac8-877f-4c0a-bca9-348f6c3b1e60
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.298072.298072 cuda_h.py:19] end self_attn cost 0.0031681060791015625 seconds
DEBUG 01-15 16:10:44.298990.298990 cuda_h.py:19] end iln_self_attn_paln cost 0.0047571659088134766 seconds
DEBUG 01-15 16:10:44.298489.298489 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:10:44.298205.298205 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.299607.299607 cuda_h.py:19] end gate cost 0.0006783008575439453 seconds
DEBUG 01-15 16:10:44.299397.299397 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.299086.299086 lmp.py:1912] 
DEBUG 01-15 16:10:44.299086.299086 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.299610.299610 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.299266.299266 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.299108.299108 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.299758.299758 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.299216.299216 lmp.py:1917] 
DEBUG 01-15 16:10:44.299216.299216 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.299773.299773 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.299814.299814 lmp.py:1935]   Expert 44 |     29 | CPU
DEBUG 01-15 16:10:44.299463.299463 lmp.py:1935]   Expert  9 |     33 | CPU
DEBUG 01-15 16:10:44.299398.299398 lmp.py:1935]   Expert 11 |     36 | CPU
DEBUG 01-15 16:10:44.299332.299332 lmp.py:1935]   Expert 56 |     56 | CPU
DEBUG 01-15 16:10:44.299552.299552 lmp.py:1935]   Expert 54 |     77 | CPU
DEBUG 01-15 16:10:44.299248.299248 lmp.py:1935]   Expert 62 |     90 | CPU
DEBUG 01-15 16:10:44.299136.299136 lmp.py:1935]   Expert  7 |     93 | CPU
DEBUG 01-15 16:10:44.299024.299024 lmp.py:1935]   Expert 47 |     94 | CPU
DEBUG 01-15 16:10:44.300151.300151 lmp.py:1935]   Expert 51 |    101 | CPU
DEBUG 01-15 16:10:44.300324.300324 lmp.py:1935]   Expert 52 |    108 | CPU
DEBUG 01-15 16:10:44.300497.300497 lmp.py:1935]   Expert 60 |    108 | CPU
DEBUG 01-15 16:10:44.300193.300193 lmp.py:1935]   Expert 22 |    110 | CPU
DEBUG 01-15 16:10:44.300650.300650 lmp.py:1935]   Expert 41 |    111 | CPU
DEBUG 01-15 16:10:44.300108.300108 lmp.py:1935]   Expert 53 |    113 | CPU
DEBUG 01-15 16:10:44.300089.300089 lmp.py:1935]   Expert  1 |    125 | CPU
DEBUG 01-15 16:10:44.300023.300023 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:10:44.300481.300481 lmp.py:1935]   Expert  8 |    127 | CPU
DEBUG 01-15 16:10:44.300608.300608 lmp.py:1935]   Expert 32 |    128 | CPU
DEBUG 01-15 16:10:44.300350.300350 lmp.py:1935]   Expert  2 |    129 | CPU
DEBUG 01-15 16:10:44.300470.300470 lmp.py:1935]   Expert 48 |    130 | CPU
DEBUG 01-15 16:10:44.300351.300351 lmp.py:1935]   Expert 59 |    140 | CPU
DEBUG 01-15 16:10:44.300471.300471 lmp.py:1935]   Expert 23 |    141 | CPU
DEBUG 01-15 16:10:44.300876.300876 lmp.py:1935]   Expert 27 |    141 | CPU
DEBUG 01-15 16:10:44.300280.300280 lmp.py:1935]   Expert 35 |    143 | CPU
DEBUG 01-15 16:10:44.300446.300446 lmp.py:1935]   Expert 39 |    143 | CPU
DEBUG 01-15 16:10:44.300851.300851 lmp.py:1935]   Expert 26 |    147 | CPU
DEBUG 01-15 16:10:44.300255.300255 lmp.py:1935]   Expert 50 |    151 | CPU
DEBUG 01-15 16:10:44.300243.300243 lmp.py:1935]   Expert 14 |    161 | CPU
DEBUG 01-15 16:10:44.300840.300840 lmp.py:1935]   Expert 24 |    166 | CPU
DEBUG 01-15 16:10:44.300529.300529 lmp.py:1935]   Expert 46 |    167 | CPU
DEBUG 01-15 16:10:44.300741.300741 lmp.py:1935]   Expert 38 |    169 | CPU
DEBUG 01-15 16:10:44.300669.300669 lmp.py:1935]   Expert  0 |    171 | CPU
DEBUG 01-15 16:10:44.300412.300412 lmp.py:1935]   Expert  4 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300439.300439 lmp.py:1935]   Expert 49 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300751.300751 lmp.py:1935]   Expert 34 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300063.300063 lmp.py:1935]   Expert  5 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300136.300136 lmp.py:1935]   Expert 40 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300494.300494 lmp.py:1935]   Expert 63 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300091.300091 lmp.py:1935]   Expert 19 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300688.300688 lmp.py:1935]   Expert 13 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300284.300284 lmp.py:1935]   Expert 43 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300404.300404 lmp.py:1935]   Expert 29 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300001.300001 lmp.py:1935]   Expert 57 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300597.300597 lmp.py:1935]   Expert 61 |    209 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300148.300148 lmp.py:1935]   Expert 33 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300221.300221 lmp.py:1935]   Expert 31 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300295.300295 lmp.py:1935]   Expert 16 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300891.300891 lmp.py:1935]   Expert 20 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300442.300442 lmp.py:1935]   Expert  3 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300800.300800 lmp.py:1935]   Expert 37 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300681.300681 lmp.py:1935]   Expert 15 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300947.300947 lmp.py:1935]   Expert 36 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300305.300305 lmp.py:1935]   Expert 18 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300186.300186 lmp.py:1935]   Expert 12 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300829.300829 lmp.py:1935]   Expert 17 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300188.300188 lmp.py:1935]   Expert 28 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300307.300307 lmp.py:1935]   Expert 55 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300904.300904 lmp.py:1935]   Expert 30 |    315 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.300739.300739 lmp.py:1935]   Expert 25 |    325 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.300574.300574 lmp.py:1935]   Expert 58 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.301932.301932 lmp.py:1935]   Expert 10 |    365 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.301483.301483 lmp.py:1935]   Expert 45 |    388 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.301079.301079 lmp.py:1935]   Expert 21 |    394 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.301438.301438 lmp.py:1935]   Expert 42 |    642 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.301604.301604 lmp.py:1937] 
DEBUG 01-15 16:10:44.301604.301604 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.301485.301485 lmp.py:1938]   CPU:   3765 tokens
DEBUG 01-15 16:10:44.301082.301082 lmp.py:1942]   cuda:1:   4340 tokens (16 experts)
DEBUG 01-15 16:10:44.301440.301440 lmp.py:1942]   cuda:2:   4183 tokens (16 experts)
DEBUG 01-15 16:10:44.301514.301514 lmp.py:1943]   Total GPU:   8523 tokens
DEBUG 01-15 16:10:44.301680.301680 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.301680.301680 lmp.py:1944] 
DEBUG 01-15 16:10:44.301760.301760 cuda_h.py:19] end experts_map_get cost 0.001920938491821289 seconds
DEBUG 01-15 16:10:44.301061.301061 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.301009.301009 lmp.py:1953] 
DEBUG 01-15 16:10:44.301009.301009 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.301222.301222 cuda_h.py:19] end cpu_experts_submit cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:44.301488.301488 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.301695.301695 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.303074.303074 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.33.gate_proj.weight': 0, 'model.layers.21.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.15.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.15.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.15.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.49.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.49.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.49.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.55.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.55.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.55.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.25.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.25.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.25.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.29.up_proj.weight': 271056896}, 2: {'model.layers.21.mlp.experts.36.gate_proj.weight': 0, 'model.layers.21.mlp.experts.36.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.36.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.63.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.63.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.63.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.57.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.57.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.57.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(25577390080, 5767168, 0, 0), (25583157248, 5767168, 5767168, 0), (25571622912, 5767168, 11534336, 0), (25594691584, 5767168, 17301504, 0), (25600458752, 5767168, 23068672, 0), (25588924416, 5767168, 28835840, 0), (25058344960, 5767168, 34603008, 0), (25064112128, 5767168, 40370176, 0), (25052577792, 5767168, 46137344, 0), (25092947968, 5767168, 51904512, 0), (25098715136, 5767168, 57671680, 0), (25087180800, 5767168, 63438848, 0), (25733103616, 5767168, 69206016, 0), (25738870784, 5767168, 74973184, 0), (25727336448, 5767168, 80740352, 0), (25179455488, 5767168, 86507520, 0), (25185222656, 5767168, 92274688, 0), (25173688320, 5767168, 98041856, 0), (25750405120, 5767168, 103809024, 0), (25756172288, 5767168, 109576192, 0), (25744637952, 5767168, 115343360, 0), (25265963008, 5767168, 121110528, 0), (25271730176, 5767168, 126877696, 0), (25260195840, 5767168, 132644864, 0), (25283264512, 5767168, 138412032, 0), (25289031680, 5767168, 144179200, 0), (25277497344, 5767168, 149946368, 0), (25300566016, 5767168, 155713536, 0), (25306333184, 5767168, 161480704, 0), (25294798848, 5767168, 167247872, 0), (25317867520, 5767168, 173015040, 0), (25323634688, 5767168, 178782208, 0), (25312100352, 5767168, 184549376, 0), (25335169024, 5767168, 190316544, 0), (25340936192, 5767168, 196083712, 0), (25329401856, 5767168, 201850880, 0), (25854214144, 5767168, 207618048, 0), (25859981312, 5767168, 213385216, 0), (25848446976, 5767168, 219152384, 0), (25958023168, 5767168, 224919552, 0), (25963790336, 5767168, 230686720, 0), (25952256000, 5767168, 236453888, 0), (25438978048, 5767168, 242221056, 0), (25444745216, 5767168, 247988224, 0), (25433210880, 5767168, 253755392, 0), (25508184064, 5767168, 259522560, 0), (25513951232, 5767168, 265289728, 0), (25502416896, 5767168, 271056896, 0)], 2: [(25629294592, 5767168, 0, 0), (25635061760, 5767168, 5767168, 0), (25623527424, 5767168, 11534336, 0), (25646596096, 5767168, 17301504, 0), (25652363264, 5767168, 23068672, 0), (25640828928, 5767168, 28835840, 0), (25075646464, 5767168, 34603008, 0), (25081413632, 5767168, 40370176, 0), (25069879296, 5767168, 46137344, 0), (25698500608, 5767168, 51904512, 0), (25704267776, 5767168, 57671680, 0), (25692733440, 5767168, 63438848, 0), (25214058496, 5767168, 69206016, 0), (25219825664, 5767168, 74973184, 0), (25208291328, 5767168, 80740352, 0), (25785008128, 5767168, 86507520, 0), (25790775296, 5767168, 92274688, 0), (25779240960, 5767168, 98041856, 0), (25231360000, 5767168, 103809024, 0), (25237127168, 5767168, 109576192, 0), (25225592832, 5767168, 115343360, 0), (26096435200, 5767168, 121110528, 0), (26102202368, 5767168, 126877696, 0), (26090668032, 5767168, 132644864, 0), (25352470528, 5767168, 138412032, 0), (25358237696, 5767168, 144179200, 0), (25346703360, 5767168, 149946368, 0), (25369772032, 5767168, 155713536, 0), (25375539200, 5767168, 161480704, 0), (25364004864, 5767168, 167247872, 0), (25992626176, 5767168, 173015040, 0), (25998393344, 5767168, 178782208, 0), (25986859008, 5767168, 184549376, 0), (26009927680, 5767168, 190316544, 0), (26015694848, 5767168, 196083712, 0), (26004160512, 5767168, 201850880, 0), (25490882560, 5767168, 207618048, 0), (25496649728, 5767168, 213385216, 0), (25485115392, 5767168, 219152384, 0), (26061832192, 5767168, 224919552, 0), (26067599360, 5767168, 230686720, 0), (26056065024, 5767168, 236453888, 0), (25525485568, 5767168, 242221056, 0), (25531252736, 5767168, 247988224, 0), (25519718400, 5767168, 253755392, 0), (25542787072, 5767168, 259522560, 0), (25548554240, 5767168, 265289728, 0), (25537019904, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e547ae4f0>, 2: <capsule object NULL at 0x7a4e547ae550>}
DEBUG 01-15 16:10:44.304365.304365 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.304625.304625 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.304235.304235 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 51c54b3c-df4f-49a0-8261-129f4401c09e
DEBUG 01-15 16:10:44.304315.304315 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.304087.304087 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:44.304303.304303 client.py:127] Model loaded
DEBUG 01-15 16:10:44.304086.304086 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.304362.304362 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.305397.305397 cuda_h.py:19] end restore2model cost 0.0003380775451660156 seconds
DEBUG 01-15 16:10:44.305643.305643 cuda_h.py:19] end sllm_worker_task cost 0.011373043060302734 seconds
DEBUG 01-15 16:10:44.305617.305617 cuda_h.py:19] end move_flatidxs cost 0.0008268356323242188 seconds
DEBUG 01-15 16:10:44.305917.305917 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:44.306940.306940 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 51c54b3c-df4f-49a0-8261-129f4401c09e
DEBUG 01-15 16:10:44.306992.306992 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005491971969604492 seconds
DEBUG 01-15 16:10:44.307948.307948 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.309518.309518 cuda_h.py:19] end restore2model cost 0.0025267601013183594 seconds
DEBUG 01-15 16:10:44.309043.309043 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00825190544128418 seconds
DEBUG 01-15 16:10:44.309362.309362 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.309478.309478 cuda_h.py:19] end gpu_sexperts cost 0.0002658367156982422 seconds
DEBUG 01-15 16:10:44.310731.310731 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.310315.310315 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.71661376953125e-05 seconds
DEBUG 01-15 16:10:44.310058.310058 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.310522.310522 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.310483.310483 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007853507995605469 seconds
DEBUG 01-15 16:10:44.310180.310180 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.311022.311022 cuda_h.py:19] end gpu_group_list cost 0.00017118453979492188 seconds
DEBUG 01-15 16:10:44.311615.311615 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.312362.312362 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008785724639892578 seconds
DEBUG 01-15 16:10:44.312635.312635 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.313536.313536 cuda_h.py:19] end gpu_group_list cost 0.0001785755157470703 seconds
DEBUG 01-15 16:10:44.313530.313530 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.313121.313121 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 51c54b3c-df4f-49a0-8261-129f4401c09e
DEBUG 01-15 16:10:44.315690.315690 cuda_h.py:19] end group_tensors cost 0.009207010269165039 seconds
DEBUG 01-15 16:10:44.315066.315066 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.320608.320608 cuda_h.py:19] end group pad cost 0.0040705204010009766 seconds
DEBUG 01-15 16:10:44.320398.320398 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.334762.334762 client.py:127] Model loaded
DEBUG 01-15 16:10:44.334122.334122 cuda_h.py:19] end wait_experts_multi_device cost 0.020906925201416016 seconds
DEBUG 01-15 16:10:44.334148.334148 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.339552.339552 cuda_h.py:19] end group_einsum cost 0.01972055435180664 seconds
DEBUG 01-15 16:10:44.340160.340160 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.343180.343180 cuda_h.py:19] end get_outputs_cpu1 cost 0.0038764476776123047 seconds
DEBUG 01-15 16:10:44.344360.344360 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03991222381591797 seconds
DEBUG 01-15 16:10:44.345799.345799 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010027170181274414 seconds
DEBUG 01-15 16:10:44.345041.345041 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.346367.346367 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.346968.346968 cuda_h.py:19] end index_scatter cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:10:44.346343.346343 cuda_h.py:19] end cpuoutputsdeal cost 0.0016548633575439453 seconds
DEBUG 01-15 16:10:44.346590.346590 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.346400.346400 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.347538.347538 cuda_h.py:19] end gpu_group_tensor cost 0.00014019012451171875 seconds
DEBUG 01-15 16:10:44.347347.347347 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.347326.347326 cuda_h.py:19] end gpu_group_tensor cost 0.0001285076141357422 seconds
DEBUG 01-15 16:10:44.347607.347607 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.348694.348694 cuda_h.py:19] end gpu_group_einsum cost 0.0005762577056884766 seconds
DEBUG 01-15 16:10:44.348355.348355 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.348717.348717 cuda_h.py:19] end gpu_group_einsum cost 0.0004894733428955078 seconds
DEBUG 01-15 16:10:44.348748.348748 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.349620.349620 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.349304.349304 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002117156982421875 seconds
DEBUG 01-15 16:10:44.349073.349073 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.349785.349785 cuda_h.py:19] end concat_expert_out cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:10:44.349198.349198 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.349843.349843 cuda_h.py:19] end index_scatter cost 5.5789947509765625e-05 seconds
DEBUG 01-15 16:10:44.349633.349633 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008442401885986328 seconds
DEBUG 01-15 16:10:44.349278.349278 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.349406.349406 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.350577.350577 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013017654418945312 seconds
DEBUG 01-15 16:10:44.350902.350902 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.350918.350918 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:44.350708.350708 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.350108.350108 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:44.350441.350441 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004706382751464844 seconds
DEBUG 01-15 16:10:44.350105.350105 cuda_h.py:19] end gpu_experts_multi_device cost 0.04035592079162598 seconds
DEBUG 01-15 16:10:44.350776.350776 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.052074432373046875 seconds
DEBUG 01-15 16:10:44.350591.350591 cuda_h.py:19] end prefill_layer cost 0.057520151138305664 seconds
DEBUG 01-15 16:10:44.350712.350712 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:10:44.351415.351415 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.351594.351594 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:10:44.351489.351489 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:44.351053.351053 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:44.351811.351811 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 3.790855407714844e-05 seconds
DEBUG 01-15 16:10:44.351235.351235 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:10:44.351554.351554 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.351656.351656 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.351102.351102 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.351986.351986 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.351823.351823 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.351131.351131 cuda_h.py:19] end allocate_cuda_memory cost 0.0002601146697998047 seconds
DEBUG 01-15 16:10:44.351014.351014 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.351877.351877 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.351037.351037 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.352747.352747 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b472748f-dfd2-44ad-9cf6-d811ea49c95f
DEBUG 01-15 16:10:44.352704.352704 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.352231.352231 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.353234.353234 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b472748f-dfd2-44ad-9cf6-d811ea49c95f
DEBUG 01-15 16:10:44.353462.353462 cuda_h.py:19] end load_into_gpu_async cost 0.0020456314086914062 seconds
DEBUG 01-15 16:10:44.354741.354741 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.354189.354189 cuda_h.py:19] end restore_tensors2 cost 8.726119995117188e-05 seconds
DEBUG 01-15 16:10:44.354713.354713 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002681255340576172 seconds
INFO 01-15 16:10:44.354371.354371 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b472748f-dfd2-44ad-9cf6-d811ea49c95f
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.355829.355829 cuda_h.py:19] end self_attn cost 0.0031414031982421875 seconds
DEBUG 01-15 16:10:44.355310.355310 cuda_h.py:19] end iln_self_attn_paln cost 0.004738330841064453 seconds
DEBUG 01-15 16:10:44.356093.356093 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:10:44.356254.356254 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.356326.356326 cuda_h.py:19] end gate cost 0.000751495361328125 seconds
DEBUG 01-15 16:10:44.356262.356262 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.357613.357613 lmp.py:1912] 
DEBUG 01-15 16:10:44.357613.357613 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.357661.357661 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.357841.357841 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.357444.357444 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.357617.357617 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.357313.357313 lmp.py:1917] 
DEBUG 01-15 16:10:44.357313.357313 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.357532.357532 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.357189.357189 lmp.py:1935]   Expert 25 |     14 | CPU
DEBUG 01-15 16:10:44.357408.357408 lmp.py:1935]   Expert 48 |     31 | CPU
DEBUG 01-15 16:10:44.357389.357389 lmp.py:1935]   Expert 45 |     36 | CPU
DEBUG 01-15 16:10:44.357754.357754 lmp.py:1935]   Expert  9 |     60 | CPU
DEBUG 01-15 16:10:44.357165.357165 lmp.py:1935]   Expert  0 |     85 | CPU
DEBUG 01-15 16:10:44.357577.357577 lmp.py:1935]   Expert 54 |     85 | CPU
DEBUG 01-15 16:10:44.357750.357750 lmp.py:1935]   Expert 20 |     87 | CPU
DEBUG 01-15 16:10:44.357399.357399 lmp.py:1935]   Expert 43 |     87 | CPU
DEBUG 01-15 16:10:44.357049.357049 lmp.py:1935]   Expert 47 |     88 | CPU
DEBUG 01-15 16:10:44.357414.357414 lmp.py:1935]   Expert  6 |     89 | CPU
DEBUG 01-15 16:10:44.357541.357541 lmp.py:1935]   Expert 57 |     90 | CPU
DEBUG 01-15 16:10:44.357760.357760 lmp.py:1935]   Expert 36 |     93 | CPU
DEBUG 01-15 16:10:44.357218.357218 lmp.py:1935]   Expert 13 |    104 | CPU
DEBUG 01-15 16:10:44.357675.357675 lmp.py:1935]   Expert 61 |    104 | CPU
DEBUG 01-15 16:10:44.357848.357848 lmp.py:1935]   Expert 50 |    105 | CPU
DEBUG 01-15 16:10:44.357306.357306 lmp.py:1935]   Expert 15 |    106 | CPU
DEBUG 01-15 16:10:44.357764.357764 lmp.py:1935]   Expert 62 |    106 | CPU
DEBUG 01-15 16:10:44.357890.357890 lmp.py:1935]   Expert  1 |    107 | CPU
DEBUG 01-15 16:10:44.357070.357070 lmp.py:1935]   Expert 38 |    110 | CPU
DEBUG 01-15 16:10:44.357667.357667 lmp.py:1935]   Expert 37 |    113 | CPU
DEBUG 01-15 16:10:44.357833.357833 lmp.py:1935]   Expert 14 |    119 | CPU
DEBUG 01-15 16:10:44.357522.357522 lmp.py:1935]   Expert 46 |    123 | CPU
DEBUG 01-15 16:10:44.357496.357496 lmp.py:1935]   Expert 21 |    135 | CPU
DEBUG 01-15 16:10:44.358947.358947 lmp.py:1935]   Expert  7 |    137 | CPU
DEBUG 01-15 16:10:44.358683.358683 lmp.py:1935]   Expert 28 |    138 | CPU
DEBUG 01-15 16:10:44.358657.358657 lmp.py:1935]   Expert 52 |    145 | CPU
DEBUG 01-15 16:10:44.358869.358869 lmp.py:1935]   Expert 44 |    146 | CPU
DEBUG 01-15 16:10:44.358082.358082 lmp.py:1935]   Expert 24 |    150 | CPU
DEBUG 01-15 16:10:44.358532.358532 lmp.py:1935]   Expert 10 |    151 | CPU
DEBUG 01-15 16:10:44.358745.358745 lmp.py:1935]   Expert 42 |    154 | CPU
DEBUG 01-15 16:10:44.358719.358719 lmp.py:1935]   Expert 11 |    156 | CPU
DEBUG 01-15 16:10:44.358170.358170 lmp.py:1935]   Expert  2 |    168 | CPU
DEBUG 01-15 16:10:44.358959.358959 lmp.py:1935]   Expert 35 |    170 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358794.358794 lmp.py:1935]   Expert 26 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358390.358390 lmp.py:1935]   Expert 31 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358748.358748 lmp.py:1935]   Expert 19 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358391.358391 lmp.py:1935]   Expert  3 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358034.358034 lmp.py:1935]   Expert 32 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358677.358677 lmp.py:1935]   Expert 12 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358082.358082 lmp.py:1935]   Expert 60 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358486.358486 lmp.py:1935]   Expert 56 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358891.358891 lmp.py:1935]   Expert 40 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358296.358296 lmp.py:1935]   Expert 41 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358700.358700 lmp.py:1935]   Expert  8 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358105.358105 lmp.py:1935]   Expert 23 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358509.358509 lmp.py:1935]   Expert 53 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358914.358914 lmp.py:1935]   Expert 16 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358318.358318 lmp.py:1935]   Expert 58 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358677.358677 lmp.py:1935]   Expert 51 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358558.358558 lmp.py:1935]   Expert 59 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358678.358678 lmp.py:1935]   Expert  4 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358559.358559 lmp.py:1935]   Expert 55 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358725.358725 lmp.py:1935]   Expert 49 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358130.358130 lmp.py:1935]   Expert 29 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358534.358534 lmp.py:1935]   Expert 34 |    284 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358939.358939 lmp.py:1935]   Expert 18 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358105.358105 lmp.py:1935]   Expert 63 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358271.358271 lmp.py:1935]   Expert 27 |    364 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358676.358676 lmp.py:1935]   Expert 39 |    379 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358080.358080 lmp.py:1935]   Expert 17 |    394 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358247.358247 lmp.py:1935]   Expert 22 |    429 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358366.358366 lmp.py:1935]   Expert 33 |    453 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358248.358248 lmp.py:1935]   Expert 30 |    460 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.358560.358560 lmp.py:1935]   Expert  5 |    710 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.358118.358118 lmp.py:1937] 
DEBUG 01-15 16:10:44.358118.358118 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.358523.358523 lmp.py:1938]   CPU:   3422 tokens
DEBUG 01-15 16:10:44.358941.358941 lmp.py:1942]   cuda:1:   4470 tokens (16 experts)
DEBUG 01-15 16:10:44.358392.358392 lmp.py:1942]   cuda:2:   4396 tokens (16 experts)
DEBUG 01-15 16:10:44.358366.358366 lmp.py:1943]   Total GPU:   8866 tokens
DEBUG 01-15 16:10:44.358386.358386 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.358386.358386 lmp.py:1944] 
DEBUG 01-15 16:10:44.358844.358844 cuda_h.py:19] end experts_map_get cost 0.0019063949584960938 seconds
DEBUG 01-15 16:10:44.358309.358309 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.358873.358873 lmp.py:1953] 
DEBUG 01-15 16:10:44.358873.358873 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.359610.359610 cuda_h.py:19] end cpu_experts_submit cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:44.359876.359876 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.359083.359083 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.360431.360431 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.34.gate_proj.weight': 0, 'model.layers.22.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.35.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.35.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.35.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.39.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.39.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.39.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.51.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.51.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.51.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.31.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.31.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.31.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.22.mlp.experts.32.gate_proj.weight': 0, 'model.layers.22.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.59.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.59.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.59.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.18.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.18.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.18.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.23.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.23.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.23.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(26701987840, 5767168, 0, 0), (26707755008, 5767168, 5767168, 0), (26696220672, 5767168, 11534336, 0), (26165641216, 5767168, 17301504, 0), (26171408384, 5767168, 23068672, 0), (26159874048, 5767168, 28835840, 0), (26182942720, 5767168, 34603008, 0), (26188709888, 5767168, 40370176, 0), (26177175552, 5767168, 46137344, 0), (26200244224, 5767168, 51904512, 0), (26206011392, 5767168, 57671680, 0), (26194477056, 5767168, 63438848, 0), (26719289344, 5767168, 69206016, 0), (26725056512, 5767168, 74973184, 0), (26713522176, 5767168, 80740352, 0), (26788495360, 5767168, 86507520, 0), (26794262528, 5767168, 92274688, 0), (26782728192, 5767168, 98041856, 0), (26823098368, 5767168, 103809024, 0), (26828865536, 5767168, 109576192, 0), (26817331200, 5767168, 115343360, 0), (26321354752, 5767168, 121110528, 0), (26327121920, 5767168, 126877696, 0), (26315587584, 5767168, 132644864, 0), (26961510400, 5767168, 138412032, 0), (26967277568, 5767168, 144179200, 0), (26955743232, 5767168, 149946368, 0), (26996113408, 5767168, 155713536, 0), (27001880576, 5767168, 161480704, 0), (26990346240, 5767168, 167247872, 0), (27030716416, 5767168, 173015040, 0), (27036483584, 5767168, 178782208, 0), (27024949248, 5767168, 184549376, 0), (26494369792, 5767168, 190316544, 0), (26500136960, 5767168, 196083712, 0), (26488602624, 5767168, 201850880, 0), (27082620928, 5767168, 207618048, 0), (27088388096, 5767168, 213385216, 0), (27076853760, 5767168, 219152384, 0), (27117223936, 5767168, 224919552, 0), (27122991104, 5767168, 230686720, 0), (27111456768, 5767168, 236453888, 0), (26650083328, 5767168, 242221056, 0), (26655850496, 5767168, 247988224, 0), (26644316160, 5767168, 253755392, 0), (27203731456, 5767168, 259522560, 0), (27209498624, 5767168, 265289728, 0), (27197964288, 5767168, 271056896, 0)], 2: [(26667384832, 5767168, 0, 0), (26673152000, 5767168, 5767168, 0), (26661617664, 5767168, 11534336, 0), (26684686336, 5767168, 17301504, 0), (26690453504, 5767168, 23068672, 0), (26678919168, 5767168, 28835840, 0), (26252148736, 5767168, 34603008, 0), (26257915904, 5767168, 40370176, 0), (26246381568, 5767168, 46137344, 0), (27134525440, 5767168, 51904512, 0), (27140292608, 5767168, 57671680, 0), (27128758272, 5767168, 63438848, 0), (26805796864, 5767168, 69206016, 0), (26811564032, 5767168, 74973184, 0), (26800029696, 5767168, 80740352, 0), (26390560768, 5767168, 86507520, 0), (26396327936, 5767168, 92274688, 0), (26384793600, 5767168, 98041856, 0), (26407862272, 5767168, 103809024, 0), (26413629440, 5767168, 109576192, 0), (26402095104, 5767168, 115343360, 0), (26425163776, 5767168, 121110528, 0), (26430930944, 5767168, 126877696, 0), (26419396608, 5767168, 132644864, 0), (26442465280, 5767168, 138412032, 0), (26448232448, 5767168, 144179200, 0), (26436698112, 5767168, 149946368, 0), (26511671296, 5767168, 155713536, 0), (26517438464, 5767168, 161480704, 0), (26505904128, 5767168, 167247872, 0), (27065319424, 5767168, 173015040, 0), (27071086592, 5767168, 178782208, 0), (27059552256, 5767168, 184549376, 0), (26563575808, 5767168, 190316544, 0), (26569342976, 5767168, 196083712, 0), (26557808640, 5767168, 201850880, 0), (26580877312, 5767168, 207618048, 0), (26586644480, 5767168, 213385216, 0), (26575110144, 5767168, 219152384, 0), (27151826944, 5767168, 224919552, 0), (27157594112, 5767168, 230686720, 0), (27146059776, 5767168, 236453888, 0), (26615480320, 5767168, 242221056, 0), (26621247488, 5767168, 247988224, 0), (26609713152, 5767168, 253755392, 0), (26632781824, 5767168, 259522560, 0), (26638548992, 5767168, 265289728, 0), (26627014656, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e447eb1e0>, 2: <capsule object NULL at 0x7a4e547ae880>}
DEBUG 01-15 16:10:44.361234.361234 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.361163.361163 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.361343.361343 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 87cc89ea-7ed9-414b-af98-b7bde30a5cda
DEBUG 01-15 16:10:44.361740.361740 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.361893.361893 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:44.361428.361428 client.py:127] Model loaded
DEBUG 01-15 16:10:44.361112.361112 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.361115.361115 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.362323.362323 cuda_h.py:19] end restore2model cost 0.00033593177795410156 seconds
DEBUG 01-15 16:10:44.362900.362900 cuda_h.py:19] end sllm_worker_task cost 0.010690450668334961 seconds
DEBUG 01-15 16:10:44.362225.362225 cuda_h.py:19] end move_flatidxs cost 0.0008289813995361328 seconds
DEBUG 01-15 16:10:44.362001.362001 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:44.363468.363468 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 87cc89ea-7ed9-414b-af98-b7bde30a5cda
DEBUG 01-15 16:10:44.364005.364005 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004957675933837891 seconds
DEBUG 01-15 16:10:44.364484.364484 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.366510.366510 cuda_h.py:19] end restore2model cost 0.0025129318237304688 seconds
DEBUG 01-15 16:10:44.366161.366161 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007701396942138672 seconds
DEBUG 01-15 16:10:44.366148.366148 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.367463.367463 cuda_h.py:19] end gpu_sexperts cost 0.00027060508728027344 seconds
DEBUG 01-15 16:10:44.367293.367293 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.367877.367877 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:10:44.367003.367003 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.367660.367660 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.368417.368417 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008068084716796875 seconds
DEBUG 01-15 16:10:44.368591.368591 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.368538.368538 cuda_h.py:19] end gpu_group_list cost 0.00017762184143066406 seconds
DEBUG 01-15 16:10:44.369172.369172 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.370641.370641 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008838176727294922 seconds
DEBUG 01-15 16:10:44.370590.370590 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.370584.370584 cuda_h.py:19] end gpu_group_list cost 0.0001761913299560547 seconds
DEBUG 01-15 16:10:44.370603.370603 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.370194.370194 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 87cc89ea-7ed9-414b-af98-b7bde30a5cda
DEBUG 01-15 16:10:44.371826.371826 cuda_h.py:19] end group_tensors cost 0.009173870086669922 seconds
DEBUG 01-15 16:10:44.372899.372899 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.376815.376815 cuda_h.py:19] end group pad cost 0.0039539337158203125 seconds
DEBUG 01-15 16:10:44.376651.376651 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.389110.389110 client.py:127] Model loaded
DEBUG 01-15 16:10:44.389419.389419 cuda_h.py:19] end wait_experts_multi_device cost 0.018970727920532227 seconds
DEBUG 01-15 16:10:44.389805.389805 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.396942.396942 cuda_h.py:19] end group_einsum cost 0.019449234008789062 seconds
DEBUG 01-15 16:10:44.396430.396430 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.400610.400610 cuda_h.py:19] end get_outputs_cpu1 cost 0.0035414695739746094 seconds
DEBUG 01-15 16:10:44.400097.400097 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0392305850982666 seconds
DEBUG 01-15 16:10:44.401839.401839 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01123809814453125 seconds
DEBUG 01-15 16:10:44.401220.401220 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.402167.402167 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.402317.402317 cuda_h.py:19] end index_scatter cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:10:44.402394.402394 cuda_h.py:19] end cpuoutputsdeal cost 0.0015988349914550781 seconds
DEBUG 01-15 16:10:44.403827.403827 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.403298.403298 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.403383.403383 cuda_h.py:19] end gpu_group_tensor cost 0.00013899803161621094 seconds
DEBUG 01-15 16:10:44.403762.403762 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.403396.403396 cuda_h.py:19] end gpu_group_tensor cost 0.0001227855682373047 seconds
DEBUG 01-15 16:10:44.403386.403386 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.404456.404456 cuda_h.py:19] end gpu_group_einsum cost 0.00046324729919433594 seconds
DEBUG 01-15 16:10:44.404274.404274 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.404888.404888 cuda_h.py:19] end gpu_group_einsum cost 0.0004703998565673828 seconds
DEBUG 01-15 16:10:44.404780.404780 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.404889.404889 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.405236.405236 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020813941955566406 seconds
DEBUG 01-15 16:10:44.405005.405005 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.405717.405717 cuda_h.py:19] end concat_expert_out cost 6.461143493652344e-05 seconds
DEBUG 01-15 16:10:44.405653.405653 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.405576.405576 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:44.405326.405326 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008409023284912109 seconds
DEBUG 01-15 16:10:44.405786.405786 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.405007.405007 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.406323.406323 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001323223114013672 seconds
DEBUG 01-15 16:10:44.406887.406887 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.406903.406903 cuda_h.py:19] end concat_expert_out cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:10:44.406316.406316 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.406478.406478 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:44.406095.406095 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004737377166748047 seconds
DEBUG 01-15 16:10:44.406713.406713 cuda_h.py:19] end gpu_experts_multi_device cost 0.039157867431640625 seconds
DEBUG 01-15 16:10:44.406146.406146 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.050391435623168945 seconds
DEBUG 01-15 16:10:44.406132.406132 cuda_h.py:19] end prefill_layer cost 0.055799007415771484 seconds
DEBUG 01-15 16:10:44.406167.406167 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:10:44.406870.406870 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.406050.406050 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:10:44.406945.406945 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:44.407032.407032 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:44.407643.407643 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 3.695487976074219e-05 seconds
DEBUG 01-15 16:10:44.407637.407637 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 7.009506225585938e-05 seconds
DEBUG 01-15 16:10:44.407718.407718 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.407965.407965 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.407923.407923 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.407131.407131 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.407166.407166 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.407752.407752 cuda_h.py:19] end allocate_cuda_memory cost 0.0002522468566894531 seconds
DEBUG 01-15 16:10:44.407576.407576 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.407054.407054 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.407168.407168 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.407639.407639 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f5d8e195-fb40-45c1-a606-45b98e9fdef0
DEBUG 01-15 16:10:44.408312.408312 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.408156.408156 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.410311.410311 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f5d8e195-fb40-45c1-a606-45b98e9fdef0
DEBUG 01-15 16:10:44.410161.410161 cuda_h.py:19] end load_into_gpu_async cost 0.002219676971435547 seconds
DEBUG 01-15 16:10:44.410248.410248 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.410934.410934 cuda_h.py:19] end restore_tensors2 cost 8.797645568847656e-05 seconds
DEBUG 01-15 16:10:44.410936.410936 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0028450489044189453 seconds
INFO 01-15 16:10:44.410163.410163 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f5d8e195-fb40-45c1-a606-45b98e9fdef0
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.411655.411655 cuda_h.py:19] end self_attn cost 0.0031280517578125 seconds
DEBUG 01-15 16:10:44.411612.411612 cuda_h.py:19] end iln_self_attn_paln cost 0.004735231399536133 seconds
DEBUG 01-15 16:10:44.411872.411872 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:10:44.411874.411874 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.412746.412746 cuda_h.py:19] end gate cost 0.0007100105285644531 seconds
DEBUG 01-15 16:10:44.412298.412298 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.413517.413517 lmp.py:1912] 
DEBUG 01-15 16:10:44.413517.413517 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.413326.413326 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.413983.413983 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.413063.413063 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.413713.413713 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.413409.413409 lmp.py:1917] 
DEBUG 01-15 16:10:44.413409.413409 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.413628.413628 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.413761.413761 lmp.py:1935]   Expert  5 |     13 | CPU
DEBUG 01-15 16:10:44.413981.413981 lmp.py:1935]   Expert 56 |     33 | CPU
DEBUG 01-15 16:10:44.413485.413485 lmp.py:1935]   Expert 16 |     86 | CPU
DEBUG 01-15 16:10:44.413227.413227 lmp.py:1935]   Expert 27 |     87 | CPU
DEBUG 01-15 16:10:44.413969.413969 lmp.py:1935]   Expert 17 |     90 | CPU
DEBUG 01-15 16:10:44.413427.413427 lmp.py:1935]   Expert 40 |     94 | CPU
DEBUG 01-15 16:10:44.413885.413885 lmp.py:1935]   Expert 63 |     97 | CPU
DEBUG 01-15 16:10:44.413819.413819 lmp.py:1935]   Expert 49 |    105 | CPU
DEBUG 01-15 16:10:44.413277.413277 lmp.py:1935]   Expert 51 |    105 | CPU
DEBUG 01-15 16:10:44.413019.413019 lmp.py:1935]   Expert 28 |    107 | CPU
DEBUG 01-15 16:10:44.413000.413000 lmp.py:1935]   Expert 53 |    107 | CPU
DEBUG 01-15 16:10:44.413743.413743 lmp.py:1935]   Expert  7 |    113 | CPU
DEBUG 01-15 16:10:44.413247.413247 lmp.py:1935]   Expert 47 |    119 | CPU
DEBUG 01-15 16:10:44.413512.413512 lmp.py:1935]   Expert 38 |    124 | CPU
DEBUG 01-15 16:10:44.413924.413924 lmp.py:1935]   Expert 37 |    125 | CPU
DEBUG 01-15 16:10:44.413381.413381 lmp.py:1935]   Expert 62 |    126 | CPU
DEBUG 01-15 16:10:44.413031.413031 lmp.py:1935]   Expert 11 |    130 | CPU
DEBUG 01-15 16:10:44.413965.413965 lmp.py:1935]   Expert 58 |    130 | CPU
DEBUG 01-15 16:10:44.413847.413847 lmp.py:1935]   Expert 57 |    136 | CPU
DEBUG 01-15 16:10:44.413967.413967 lmp.py:1935]   Expert 14 |    145 | CPU
DEBUG 01-15 16:10:44.413563.413563 lmp.py:1935]   Expert  1 |    146 | CPU
DEBUG 01-15 16:10:44.413968.413968 lmp.py:1935]   Expert 39 |    147 | CPU
DEBUG 01-15 16:10:44.413372.413372 lmp.py:1935]   Expert 52 |    153 | CPU
DEBUG 01-15 16:10:44.413539.413539 lmp.py:1935]   Expert 23 |    157 | CPU
DEBUG 01-15 16:10:44.413705.413705 lmp.py:1935]   Expert 25 |    158 | CPU
DEBUG 01-15 16:10:44.413109.413109 lmp.py:1935]   Expert 33 |    160 | CPU
DEBUG 01-15 16:10:44.413514.413514 lmp.py:1935]   Expert 21 |    164 | CPU
DEBUG 01-15 16:10:44.413203.413203 lmp.py:1935]   Expert 60 |    169 | CPU
DEBUG 01-15 16:10:44.413846.413846 lmp.py:1935]   Expert  6 |    170 | CPU
DEBUG 01-15 16:10:44.413774.413774 lmp.py:1935]   Expert 45 |    175 | CPU
DEBUG 01-15 16:10:44.413132.413132 lmp.py:1935]   Expert 19 |    180 | CPU
DEBUG 01-15 16:10:44.413490.413490 lmp.py:1935]   Expert  4 |    182 | CPU
DEBUG 01-15 16:10:44.414279.414279 lmp.py:1935]   Expert 12 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414591.414591 lmp.py:1935]   Expert 44 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414664.414664 lmp.py:1935]   Expert 30 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414784.414784 lmp.py:1935]   Expert 31 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414142.414142 lmp.py:1935]   Expert 36 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414739.414739 lmp.py:1935]   Expert  3 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414859.414859 lmp.py:1935]   Expert 55 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414979.414979 lmp.py:1935]   Expert  9 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414099.414099 lmp.py:1935]   Expert  0 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414887.414887 lmp.py:1935]   Expert 34 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414961.414961 lmp.py:1935]   Expert 22 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414557.414557 lmp.py:1935]   Expert 41 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414393.414393 lmp.py:1935]   Expert 54 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414751.414751 lmp.py:1935]   Expert 26 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414871.414871 lmp.py:1935]   Expert 43 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414752.414752 lmp.py:1935]   Expert 59 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414110.414110 lmp.py:1935]   Expert 13 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414230.414230 lmp.py:1935]   Expert 18 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414350.414350 lmp.py:1935]   Expert 20 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414470.414470 lmp.py:1935]   Expert 15 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414066.414066 lmp.py:1935]   Expert 50 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414663.414663 lmp.py:1935]   Expert 24 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414498.414498 lmp.py:1935]   Expert 42 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414333.414333 lmp.py:1935]   Expert 29 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414168.414168 lmp.py:1935]   Expert 61 |    274 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414526.414526 lmp.py:1935]   Expert 35 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414646.414646 lmp.py:1935]   Expert 32 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414528.414528 lmp.py:1935]   Expert  2 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414409.414409 lmp.py:1935]   Expert  8 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414052.414052 lmp.py:1935]   Expert 10 |    341 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414172.414172 lmp.py:1935]   Expert 46 |    423 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.414292.414292 lmp.py:1935]   Expert 48 |    447 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.414458.414458 lmp.py:1937] 
DEBUG 01-15 16:10:44.414458.414458 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.414531.414531 lmp.py:1938]   CPU:   4033 tokens
DEBUG 01-15 16:10:44.414843.414843 lmp.py:1942]   cuda:1:   4130 tokens (16 experts)
DEBUG 01-15 16:10:44.414944.414944 lmp.py:1942]   cuda:2:   4125 tokens (16 experts)
DEBUG 01-15 16:10:44.414348.414348 lmp.py:1943]   Total GPU:   8255 tokens
DEBUG 01-15 16:10:44.414005.414005 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.414005.414005 lmp.py:1944] 
DEBUG 01-15 16:10:44.414131.414131 cuda_h.py:19] end experts_map_get cost 0.0019176006317138672 seconds
DEBUG 01-15 16:10:44.414981.414981 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.414592.414592 lmp.py:1953] 
DEBUG 01-15 16:10:44.414592.414592 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.414620.414620 cuda_h.py:19] end cpu_experts_submit cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:44.414005.414005 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.414643.414643 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.416991.416991 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.32.gate_proj.weight': 0, 'model.layers.23.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.34.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.34.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.34.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.41.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.41.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.41.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.23.mlp.experts.0.gate_proj.weight': 0, 'model.layers.23.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.22.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.22.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.22.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.61.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.61.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.61.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(27774681088, 5767168, 0, 0), (27780448256, 5767168, 5767168, 0), (27768913920, 5767168, 11534336, 0), (27255635968, 5767168, 17301504, 0), (27261403136, 5767168, 23068672, 0), (27249868800, 5767168, 28835840, 0), (27826585600, 5767168, 34603008, 0), (27832352768, 5767168, 40370176, 0), (27820818432, 5767168, 46137344, 0), (27809284096, 5767168, 51904512, 0), (27815051264, 5767168, 57671680, 0), (27803516928, 5767168, 63438848, 0), (27930394624, 5767168, 69206016, 0), (27936161792, 5767168, 74973184, 0), (27924627456, 5767168, 80740352, 0), (27376746496, 5767168, 86507520, 0), (27382513664, 5767168, 92274688, 0), (27370979328, 5767168, 98041856, 0), (27964997632, 5767168, 103809024, 0), (27970764800, 5767168, 109576192, 0), (27959230464, 5767168, 115343360, 0), (27982299136, 5767168, 121110528, 0), (27988066304, 5767168, 126877696, 0), (27976531968, 5767168, 132644864, 0), (28051505152, 5767168, 138412032, 0), (28057272320, 5767168, 144179200, 0), (28045737984, 5767168, 149946368, 0), (27532460032, 5767168, 155713536, 0), (27538227200, 5767168, 161480704, 0), (27526692864, 5767168, 167247872, 0), (27567063040, 5767168, 173015040, 0), (27572830208, 5767168, 178782208, 0), (27561295872, 5767168, 184549376, 0), (27636269056, 5767168, 190316544, 0), (27642036224, 5767168, 196083712, 0), (27630501888, 5767168, 201850880, 0), (27670872064, 5767168, 207618048, 0), (27676639232, 5767168, 213385216, 0), (27665104896, 5767168, 219152384, 0), (27722776576, 5767168, 224919552, 0), (27728543744, 5767168, 230686720, 0), (27717009408, 5767168, 236453888, 0), (27740078080, 5767168, 242221056, 0), (27745845248, 5767168, 247988224, 0), (27734310912, 5767168, 253755392, 0), (27757379584, 5767168, 259522560, 0), (27763146752, 5767168, 265289728, 0), (27751612416, 5767168, 271056896, 0)], 2: [(27221032960, 5767168, 0, 0), (27226800128, 5767168, 5767168, 0), (27215265792, 5767168, 11534336, 0), (27272937472, 5767168, 17301504, 0), (27278704640, 5767168, 23068672, 0), (27267170304, 5767168, 28835840, 0), (27843887104, 5767168, 34603008, 0), (27849654272, 5767168, 40370176, 0), (27838119936, 5767168, 46137344, 0), (27359444992, 5767168, 51904512, 0), (27365212160, 5767168, 57671680, 0), (27353677824, 5767168, 63438848, 0), (27394048000, 5767168, 69206016, 0), (27399815168, 5767168, 74973184, 0), (27388280832, 5767168, 80740352, 0), (27947696128, 5767168, 86507520, 0), (27953463296, 5767168, 92274688, 0), (27941928960, 5767168, 98041856, 0), (27428651008, 5767168, 103809024, 0), (27434418176, 5767168, 109576192, 0), (27422883840, 5767168, 115343360, 0), (27445952512, 5767168, 121110528, 0), (27451719680, 5767168, 126877696, 0), (27440185344, 5767168, 132644864, 0), (28016902144, 5767168, 138412032, 0), (28022669312, 5767168, 144179200, 0), (28011134976, 5767168, 149946368, 0), (27480555520, 5767168, 155713536, 0), (27486322688, 5767168, 161480704, 0), (27474788352, 5767168, 167247872, 0), (28086108160, 5767168, 173015040, 0), (28091875328, 5767168, 178782208, 0), (28080340992, 5767168, 184549376, 0), (28172615680, 5767168, 190316544, 0), (28178382848, 5767168, 196083712, 0), (28166848512, 5767168, 201850880, 0), (28155314176, 5767168, 207618048, 0), (28161081344, 5767168, 213385216, 0), (28149547008, 5767168, 219152384, 0), (27601666048, 5767168, 224919552, 0), (27607433216, 5767168, 230686720, 0), (27595898880, 5767168, 236453888, 0), (28241821696, 5767168, 242221056, 0), (28247588864, 5767168, 247988224, 0), (28236054528, 5767168, 253755392, 0), (28276424704, 5767168, 259522560, 0), (28282191872, 5767168, 265289728, 0), (28270657536, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a5b00c9acd0>, 2: <capsule object NULL at 0x7a4e547ae850>}
DEBUG 01-15 16:10:44.417440.417440 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.417223.417223 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.417165.417165 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 84a75b39-d622-4a1a-a053-2e269fd3026a
DEBUG 01-15 16:10:44.417198.417198 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.417295.417295 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:44.417292.417292 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:44.418839.418839 client.py:127] Model loaded
DEBUG 01-15 16:10:44.418238.418238 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.418925.418925 cuda_h.py:19] end restore2model cost 0.00033545494079589844 seconds
DEBUG 01-15 16:10:44.418980.418980 cuda_h.py:19] end sllm_worker_task cost 0.011518478393554688 seconds
DEBUG 01-15 16:10:44.419718.419718 cuda_h.py:19] end move_flatidxs cost 0.001157522201538086 seconds
DEBUG 01-15 16:10:44.419287.419287 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:44.419387.419387 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 84a75b39-d622-4a1a-a053-2e269fd3026a
DEBUG 01-15 16:10:44.420798.420798 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004998922348022461 seconds
DEBUG 01-15 16:10:44.420277.420277 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.422568.422568 cuda_h.py:19] end restore2model cost 0.0025281906127929688 seconds
DEBUG 01-15 16:10:44.422358.422358 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0077512264251708984 seconds
DEBUG 01-15 16:10:44.422439.422439 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.423316.423316 cuda_h.py:19] end gpu_sexperts cost 0.0002644062042236328 seconds
DEBUG 01-15 16:10:44.423953.423953 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.423491.423491 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:10:44.423949.423949 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.423175.423175 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.423448.423448 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008013248443603516 seconds
DEBUG 01-15 16:10:44.424099.424099 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.424708.424708 cuda_h.py:19] end gpu_group_list cost 0.00017452239990234375 seconds
DEBUG 01-15 16:10:44.425860.425860 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.425475.425475 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008797645568847656 seconds
DEBUG 01-15 16:10:44.426239.426239 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.426464.426464 cuda_h.py:19] end gpu_group_list cost 0.00017213821411132812 seconds
DEBUG 01-15 16:10:44.426278.426278 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.426584.426584 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 84a75b39-d622-4a1a-a053-2e269fd3026a
DEBUG 01-15 16:10:44.437842.437842 cuda_h.py:19] end group_tensors cost 0.01765894889831543 seconds
DEBUG 01-15 16:10:44.438841.438841 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.442932.442932 cuda_h.py:19] end group pad cost 0.004418849945068359 seconds
DEBUG 01-15 16:10:44.442989.442989 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.445715.445715 client.py:127] Model loaded
DEBUG 01-15 16:10:44.446997.446997 cuda_h.py:19] end wait_experts_multi_device cost 0.019223928451538086 seconds
DEBUG 01-15 16:10:44.446266.446266 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.462298.462298 cuda_h.py:19] end group_einsum cost 0.019234895706176758 seconds
DEBUG 01-15 16:10:44.462476.462476 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.466945.466945 cuda_h.py:19] end get_outputs_cpu1 cost 0.004029273986816406 seconds
DEBUG 01-15 16:10:44.466139.466139 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04929161071777344 seconds
DEBUG 01-15 16:10:44.467601.467601 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.021320819854736328 seconds
DEBUG 01-15 16:10:44.467156.467156 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.471058.471058 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.471612.471612 cuda_h.py:19] end index_scatter cost 0.00016307830810546875 seconds
DEBUG 01-15 16:10:44.472312.472312 cuda_h.py:19] end cpuoutputsdeal cost 0.004158973693847656 seconds
DEBUG 01-15 16:10:44.472085.472085 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.472531.472531 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.472831.472831 cuda_h.py:19] end gpu_group_tensor cost 0.0003170967102050781 seconds
DEBUG 01-15 16:10:44.472661.472661 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.473330.473330 cuda_h.py:19] end gpu_group_tensor cost 0.000308990478515625 seconds
DEBUG 01-15 16:10:44.473106.473106 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.474042.474042 cuda_h.py:19] end gpu_group_einsum cost 0.0009706020355224609 seconds
DEBUG 01-15 16:10:44.474951.474951 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.475918.475918 cuda_h.py:19] end gpu_group_einsum cost 0.0008816719055175781 seconds
DEBUG 01-15 16:10:44.476529.476529 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.476087.476087 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.476917.476917 cuda_h.py:19] end all_expert_outputs_slices cost 0.0005848407745361328 seconds
DEBUG 01-15 16:10:44.477456.477456 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.477966.477966 cuda_h.py:19] end concat_expert_out cost 0.00012350082397460938 seconds
DEBUG 01-15 16:10:44.477939.477939 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.477280.477280 cuda_h.py:19] end index_scatter cost 6.961822509765625e-05 seconds
DEBUG 01-15 16:10:44.477434.477434 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0016117095947265625 seconds
DEBUG 01-15 16:10:44.477908.477908 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.477374.477374 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.478255.478255 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001952648162841797 seconds
DEBUG 01-15 16:10:44.478250.478250 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.478617.478617 cuda_h.py:19] end concat_expert_out cost 6.222724914550781e-05 seconds
DEBUG 01-15 16:10:44.478374.478374 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.478649.478649 cuda_h.py:19] end index_scatter cost 6.175041198730469e-05 seconds
DEBUG 01-15 16:10:44.478604.478604 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005805492401123047 seconds
DEBUG 01-15 16:10:44.478083.478083 cuda_h.py:19] end gpu_experts_multi_device cost 0.055413007736206055 seconds
DEBUG 01-15 16:10:44.478960.478960 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.06667518615722656 seconds
DEBUG 01-15 16:10:44.479350.479350 cuda_h.py:19] end prefill_layer cost 0.07205486297607422 seconds
DEBUG 01-15 16:10:44.479564.479564 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:10:44.479790.479790 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.479208.479208 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:10:44.479103.479103 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:44.479620.479620 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:44.479046.479046 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 3.8623809814453125e-05 seconds
DEBUG 01-15 16:10:44.479280.479280 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.479309.479309 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 0.00016188621520996094 seconds
DEBUG 01-15 16:10:44.479808.479808 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.479592.479592 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.479302.479302 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.479552.479552 cuda_h.py:19] end allocate_cuda_memory cost 0.00023603439331054688 seconds
DEBUG 01-15 16:10:44.479985.479985 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.480078.480078 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.480802.480802 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.480028.480028 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, dec0d1ba-bb62-4496-bc4e-4b0b5517dade
DEBUG 01-15 16:10:44.480011.480011 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.480314.480314 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.480791.480791 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.481614.481614 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, dec0d1ba-bb62-4496-bc4e-4b0b5517dade
DEBUG 01-15 16:10:44.481735.481735 cuda_h.py:19] end load_into_gpu_async cost 0.0018188953399658203 seconds
DEBUG 01-15 16:10:44.481862.481862 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.481759.481759 cuda_h.py:19] end restore_tensors2 cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:10:44.481416.481416 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002425670623779297 seconds
INFO 01-15 16:10:44.482636.482636 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, dec0d1ba-bb62-4496-bc4e-4b0b5517dade
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.484206.484206 cuda_h.py:19] end self_attn cost 0.003648519515991211 seconds
DEBUG 01-15 16:10:44.484750.484750 cuda_h.py:19] end iln_self_attn_paln cost 0.005189657211303711 seconds
DEBUG 01-15 16:10:44.484679.484679 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:10:44.484541.484541 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.485661.485661 cuda_h.py:19] end gate cost 0.0007848739624023438 seconds
DEBUG 01-15 16:10:44.485882.485882 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.486033.486033 lmp.py:1912] 
DEBUG 01-15 16:10:44.486033.486033 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.486511.486511 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.486506.486506 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.486016.486016 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.486196.486196 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.486892.486892 lmp.py:1917] 
DEBUG 01-15 16:10:44.486892.486892 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.486588.486588 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.486722.486722 lmp.py:1935]   Expert 36 |     20 | CPU
DEBUG 01-15 16:10:44.486656.486656 lmp.py:1935]   Expert 35 |     29 | CPU
DEBUG 01-15 16:10:44.486875.486875 lmp.py:1935]   Expert 25 |     46 | CPU
DEBUG 01-15 16:10:44.486856.486856 lmp.py:1935]   Expert 46 |     49 | CPU
DEBUG 01-15 16:10:44.486314.486314 lmp.py:1935]   Expert 51 |     51 | CPU
DEBUG 01-15 16:10:44.486533.486533 lmp.py:1935]   Expert 16 |     59 | CPU
DEBUG 01-15 16:10:44.486276.486276 lmp.py:1935]   Expert 30 |     62 | CPU
DEBUG 01-15 16:10:44.486495.486495 lmp.py:1935]   Expert  0 |     65 | CPU
DEBUG 01-15 16:10:44.486237.486237 lmp.py:1935]   Expert 43 |     68 | CPU
DEBUG 01-15 16:10:44.486980.486980 lmp.py:1935]   Expert 47 |     70 | CPU
DEBUG 01-15 16:10:44.486722.486722 lmp.py:1935]   Expert 39 |     74 | CPU
DEBUG 01-15 16:10:44.486464.486464 lmp.py:1935]   Expert 42 |     75 | CPU
DEBUG 01-15 16:10:44.486445.486445 lmp.py:1935]   Expert 44 |     75 | CPU
DEBUG 01-15 16:10:44.486188.486188 lmp.py:1935]   Expert 55 |     76 | CPU
DEBUG 01-15 16:10:44.486407.486407 lmp.py:1935]   Expert  2 |     81 | CPU
DEBUG 01-15 16:10:44.486388.486388 lmp.py:1935]   Expert  4 |    109 | CPU
DEBUG 01-15 16:10:44.486369.486369 lmp.py:1935]   Expert 33 |    118 | CPU
DEBUG 01-15 16:10:44.486349.486349 lmp.py:1935]   Expert 48 |    118 | CPU
DEBUG 01-15 16:10:44.486330.486330 lmp.py:1935]   Expert 24 |    122 | CPU
DEBUG 01-15 16:10:44.486311.486311 lmp.py:1935]   Expert  6 |    124 | CPU
DEBUG 01-15 16:10:44.486292.486292 lmp.py:1935]   Expert 13 |    125 | CPU
DEBUG 01-15 16:10:44.486511.486511 lmp.py:1935]   Expert 61 |    127 | CPU
DEBUG 01-15 16:10:44.487492.487492 lmp.py:1935]   Expert 56 |    131 | CPU
DEBUG 01-15 16:10:44.487234.487234 lmp.py:1935]   Expert 15 |    133 | CPU
DEBUG 01-15 16:10:44.487215.487215 lmp.py:1935]   Expert 29 |    137 | CPU
DEBUG 01-15 16:10:44.487196.487196 lmp.py:1935]   Expert 54 |    139 | CPU
DEBUG 01-15 16:10:44.487177.487177 lmp.py:1935]   Expert 38 |    141 | CPU
DEBUG 01-15 16:10:44.487158.487158 lmp.py:1935]   Expert  7 |    143 | CPU
DEBUG 01-15 16:10:44.487662.487662 lmp.py:1935]   Expert  9 |    144 | CPU
DEBUG 01-15 16:10:44.487404.487404 lmp.py:1935]   Expert 20 |    144 | CPU
DEBUG 01-15 16:10:44.487147.487147 lmp.py:1935]   Expert 59 |    148 | CPU
DEBUG 01-15 16:10:44.487127.487127 lmp.py:1935]   Expert 19 |    159 | CPU
DEBUG 01-15 16:10:44.487492.487492 lmp.py:1935]   Expert 45 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487381.487381 lmp.py:1935]   Expert 62 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487792.487792 lmp.py:1935]   Expert 34 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487488.487488 lmp.py:1935]   Expert 57 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487661.487661 lmp.py:1935]   Expert 50 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487596.487596 lmp.py:1935]   Expert 10 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487530.487530 lmp.py:1935]   Expert 31 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487226.487226 lmp.py:1935]   Expert 23 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487161.487161 lmp.py:1935]   Expert  8 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487095.487095 lmp.py:1935]   Expert 60 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487268.487268 lmp.py:1935]   Expert 18 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487964.487964 lmp.py:1935]   Expert 53 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487422.487422 lmp.py:1935]   Expert 22 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487880.487880 lmp.py:1935]   Expert 52 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487576.487576 lmp.py:1935]   Expert 37 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487225.487225 lmp.py:1935]   Expert  5 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487882.487882 lmp.py:1935]   Expert 17 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487055.487055 lmp.py:1935]   Expert 11 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487989.487989 lmp.py:1935]   Expert  1 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487685.487685 lmp.py:1935]   Expert 49 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487382.487382 lmp.py:1935]   Expert 41 |    281 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487839.487839 lmp.py:1935]   Expert 28 |    285 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487012.487012 lmp.py:1935]   Expert 26 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487947.487947 lmp.py:1935]   Expert 58 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487881.487881 lmp.py:1935]   Expert 32 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487816.487816 lmp.py:1935]   Expert 40 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487512.487512 lmp.py:1935]   Expert 14 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487208.487208 lmp.py:1935]   Expert 12 |    331 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487619.487619 lmp.py:1935]   Expert 63 |    336 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487315.487315 lmp.py:1935]   Expert 21 |    387 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487011.487011 lmp.py:1935]   Expert 27 |    670 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.487946.487946 lmp.py:1935]   Expert  3 |   1015 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.487165.487165 lmp.py:1937] 
DEBUG 01-15 16:10:44.487165.487165 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.487623.487623 lmp.py:1938]   CPU:   3162 tokens
DEBUG 01-15 16:10:44.487557.487557 lmp.py:1942]   cuda:1:   4497 tokens (15 experts)
DEBUG 01-15 16:10:44.488015.488015 lmp.py:1942]   cuda:2:   4629 tokens (17 experts)
DEBUG 01-15 16:10:44.488996.488996 lmp.py:1943]   Total GPU:   9126 tokens
DEBUG 01-15 16:10:44.488500.488500 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.488500.488500 lmp.py:1944] 
DEBUG 01-15 16:10:44.488395.488395 cuda_h.py:19] end experts_map_get cost 0.0022406578063964844 seconds
DEBUG 01-15 16:10:44.488073.488073 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.488359.488359 lmp.py:1953] 
DEBUG 01-15 16:10:44.488359.488359 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.488639.488639 cuda_h.py:19] end cpu_experts_submit cost 6.771087646484375e-05 seconds
DEBUG 01-15 16:10:44.488402.488402 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.488755.488755 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.489505.489505 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.32.gate_proj.weight': 0, 'model.layers.24.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.3.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.3.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.3.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.41.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.41.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.41.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.24.mlp.experts.5.gate_proj.weight': 0, 'model.layers.24.mlp.experts.5.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.5.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.40.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.40.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.40.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.60.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.60.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.60.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.62.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.31.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.31.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.31.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(28881977344, 5767168, 0, 0), (28887744512, 5767168, 5767168, 0), (28876210176, 5767168, 11534336, 0), (28345630720, 5767168, 17301504, 0), (28351397888, 5767168, 23068672, 0), (28339863552, 5767168, 28835840, 0), (28916580352, 5767168, 34603008, 0), (28922347520, 5767168, 40370176, 0), (28910813184, 5767168, 46137344, 0), (28380233728, 5767168, 51904512, 0), (28386000896, 5767168, 57671680, 0), (28374466560, 5767168, 63438848, 0), (29037690880, 5767168, 69206016, 0), (29043458048, 5767168, 74973184, 0), (29031923712, 5767168, 80740352, 0), (28501344256, 5767168, 86507520, 0), (28507111424, 5767168, 92274688, 0), (28495577088, 5767168, 98041856, 0), (28570550272, 5767168, 103809024, 0), (28576317440, 5767168, 109576192, 0), (28564783104, 5767168, 115343360, 0), (28622454784, 5767168, 121110528, 0), (28628221952, 5767168, 126877696, 0), (28616687616, 5767168, 132644864, 0), (28639756288, 5767168, 138412032, 0), (28645523456, 5767168, 144179200, 0), (28633989120, 5767168, 149946368, 0), (29193404416, 5767168, 155713536, 0), (29199171584, 5767168, 161480704, 0), (29187637248, 5767168, 167247872, 0), (29228007424, 5767168, 173015040, 0), (29233774592, 5767168, 178782208, 0), (29222240256, 5767168, 184549376, 0), (28708962304, 5767168, 190316544, 0), (28714729472, 5767168, 196083712, 0), (28703195136, 5767168, 201850880, 0), (28726263808, 5767168, 207618048, 0), (28732030976, 5767168, 213385216, 0), (28720496640, 5767168, 219152384, 0), (28778168320, 5767168, 224919552, 0), (28783935488, 5767168, 230686720, 0), (28772401152, 5767168, 236453888, 0), (29418323968, 5767168, 242221056, 0), (29424091136, 5767168, 247988224, 0), (29412556800, 5767168, 253755392, 0)], 2: [(28414836736, 5767168, 0, 0), (28420603904, 5767168, 5767168, 0), (28409069568, 5767168, 11534336, 0), (28968484864, 5767168, 17301504, 0), (28974252032, 5767168, 23068672, 0), (28962717696, 5767168, 28835840, 0), (29020389376, 5767168, 34603008, 0), (29026156544, 5767168, 40370176, 0), (29014622208, 5767168, 46137344, 0), (28466741248, 5767168, 51904512, 0), (28472508416, 5767168, 57671680, 0), (28460974080, 5767168, 63438848, 0), (28518645760, 5767168, 69206016, 0), (28524412928, 5767168, 74973184, 0), (28512878592, 5767168, 80740352, 0), (28535947264, 5767168, 86507520, 0), (28541714432, 5767168, 92274688, 0), (28530180096, 5767168, 98041856, 0), (29106896896, 5767168, 103809024, 0), (29112664064, 5767168, 109576192, 0), (29101129728, 5767168, 115343360, 0), (29366419456, 5767168, 121110528, 0), (29372186624, 5767168, 126877696, 0), (29360652288, 5767168, 132644864, 0), (29176102912, 5767168, 138412032, 0), (29181870080, 5767168, 144179200, 0), (29170335744, 5767168, 149946368, 0), (28691660800, 5767168, 155713536, 0), (28697427968, 5767168, 161480704, 0), (28685893632, 5767168, 167247872, 0), (29245308928, 5767168, 173015040, 0), (29251076096, 5767168, 178782208, 0), (29239541760, 5767168, 184549376, 0), (29314514944, 5767168, 190316544, 0), (29320282112, 5767168, 196083712, 0), (29308747776, 5767168, 201850880, 0), (29331816448, 5767168, 207618048, 0), (29337583616, 5767168, 213385216, 0), (29326049280, 5767168, 219152384, 0), (28795469824, 5767168, 224919552, 0), (28801236992, 5767168, 230686720, 0), (28789702656, 5767168, 236453888, 0), (28812771328, 5767168, 242221056, 0), (28818538496, 5767168, 247988224, 0), (28807004160, 5767168, 253755392, 0), (29401022464, 5767168, 259522560, 0), (29406789632, 5767168, 265289728, 0), (29395255296, 5767168, 271056896, 0), (28864675840, 5767168, 276824064, 0), (28870443008, 5767168, 282591232, 0), (28858908672, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e547ae520>, 2: <capsule object NULL at 0x7a4e547ae790>}
DEBUG 01-15 16:10:44.489988.489988 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.489871.489871 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.489766.489766 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b4246325-2fbe-4c89-915a-f6a01886ec08
DEBUG 01-15 16:10:44.490157.490157 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.490903.490903 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:44.490878.490878 client.py:127] Model loaded
DEBUG 01-15 16:10:44.490569.490569 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.490802.490802 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.490802.490802 cuda_h.py:19] end restore2model cost 0.0003829002380371094 seconds
DEBUG 01-15 16:10:44.490386.490386 cuda_h.py:19] end sllm_worker_task cost 0.01132345199584961 seconds
INFO 01-15 16:10:44.491448.491448 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b4246325-2fbe-4c89-915a-f6a01886ec08
DEBUG 01-15 16:10:44.491820.491820 cuda_h.py:19] end move_flatidxs cost 0.0008575916290283203 seconds
DEBUG 01-15 16:10:44.491034.491034 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:44.491755.491755 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0033364295959472656 seconds
DEBUG 01-15 16:10:44.491334.491334 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.494672.494672 cuda_h.py:19] end restore2model cost 0.002953767776489258 seconds
DEBUG 01-15 16:10:44.494839.494839 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006513833999633789 seconds
DEBUG 01-15 16:10:44.494205.494205 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.495361.495361 cuda_h.py:19] end gpu_sexperts cost 0.0002944469451904297 seconds
DEBUG 01-15 16:10:44.495091.495091 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.495960.495960 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:44.495941.495941 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.495644.495644 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.496306.496306 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009486675262451172 seconds
DEBUG 01-15 16:10:44.496295.496295 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.496209.496209 cuda_h.py:19] end gpu_group_list cost 0.00018978118896484375 seconds
DEBUG 01-15 16:10:44.497822.497822 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.498400.498400 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001169443130493164 seconds
DEBUG 01-15 16:10:44.498257.498257 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.498681.498681 cuda_h.py:19] end gpu_group_list cost 0.00017833709716796875 seconds
DEBUG 01-15 16:10:44.499488.499488 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.499702.499702 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b4246325-2fbe-4c89-915a-f6a01886ec08
DEBUG 01-15 16:10:44.501875.501875 cuda_h.py:19] end group_tensors cost 0.010059118270874023 seconds
DEBUG 01-15 16:10:44.502978.502978 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.506014.506014 cuda_h.py:19] end group pad cost 0.003970623016357422 seconds
DEBUG 01-15 16:10:44.506228.506228 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.522313.522313 client.py:127] Model loaded
DEBUG 01-15 16:10:44.523406.523406 cuda_h.py:19] end wait_experts_multi_device cost 0.023174047470092773 seconds
DEBUG 01-15 16:10:44.523964.523964 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.525216.525216 cuda_h.py:19] end group_einsum cost 0.018769264221191406 seconds
DEBUG 01-15 16:10:44.525367.525367 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.528025.528025 cuda_h.py:19] end get_outputs_cpu1 cost 0.003352642059326172 seconds
DEBUG 01-15 16:10:44.529669.529669 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03921341896057129 seconds
DEBUG 01-15 16:10:44.530454.530454 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007021665573120117 seconds
DEBUG 01-15 16:10:44.530028.530028 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.532623.532623 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.533870.533870 cuda_h.py:19] end index_scatter cost 0.00013136863708496094 seconds
DEBUG 01-15 16:10:44.533024.533024 cuda_h.py:19] end cpuoutputsdeal cost 0.003144502639770508 seconds
DEBUG 01-15 16:10:44.533001.533001 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.533566.533566 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.534570.534570 cuda_h.py:19] end gpu_group_tensor cost 0.0002560615539550781 seconds
DEBUG 01-15 16:10:44.534043.534043 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.534603.534603 cuda_h.py:19] end gpu_group_tensor cost 0.00023865699768066406 seconds
DEBUG 01-15 16:10:44.534324.534324 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.536313.536313 cuda_h.py:19] end gpu_group_einsum cost 0.0015516281127929688 seconds
DEBUG 01-15 16:10:44.536316.536316 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.537661.537661 cuda_h.py:19] end gpu_group_einsum cost 0.0008754730224609375 seconds
DEBUG 01-15 16:10:44.537747.537747 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.538183.538183 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.538320.538320 cuda_h.py:19] end all_expert_outputs_slices cost 0.0004534721374511719 seconds
DEBUG 01-15 16:10:44.538652.538652 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.538305.538305 cuda_h.py:19] end concat_expert_out cost 5.555152893066406e-05 seconds
DEBUG 01-15 16:10:44.538592.538592 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.538019.538019 cuda_h.py:19] end index_scatter cost 6.747245788574219e-05 seconds
DEBUG 01-15 16:10:44.539120.539120 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011775493621826172 seconds
DEBUG 01-15 16:10:44.539077.539077 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.539305.539305 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.539113.539113 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017642974853515625 seconds
DEBUG 01-15 16:10:44.539915.539915 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.539614.539614 cuda_h.py:19] end concat_expert_out cost 6.103515625e-05 seconds
DEBUG 01-15 16:10:44.539186.539186 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.539845.539845 cuda_h.py:19] end index_scatter cost 6.29425048828125e-05 seconds
DEBUG 01-15 16:10:44.539038.539038 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005636215209960938 seconds
DEBUG 01-15 16:10:44.539133.539133 cuda_h.py:19] end gpu_experts_multi_device cost 0.04467582702636719 seconds
DEBUG 01-15 16:10:44.540010.540010 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.0551607608795166 seconds
DEBUG 01-15 16:10:44.540909.540909 cuda_h.py:19] end prefill_layer cost 0.061290740966796875 seconds
DEBUG 01-15 16:10:44.540773.540773 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:10:44.540429.540429 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.540516.540516 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:10:44.540888.540888 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:44.540882.540882 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:44.540163.540163 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 3.695487976074219e-05 seconds
DEBUG 01-15 16:10:44.540733.540733 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 7.486343383789062e-05 seconds
DEBUG 01-15 16:10:44.540291.540291 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.540545.540545 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.540636.540636 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.540412.540412 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.541734.541734 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.541617.541617 cuda_h.py:19] end allocate_cuda_memory cost 0.0002243518829345703 seconds
DEBUG 01-15 16:10:44.541548.541548 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.541648.541648 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.541809.541809 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.541280.541280 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 93cca03f-7c9e-41ed-96f0-1263b79a06ec
DEBUG 01-15 16:10:44.541568.541568 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.542943.542943 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.543611.543611 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 93cca03f-7c9e-41ed-96f0-1263b79a06ec
DEBUG 01-15 16:10:44.543778.543778 cuda_h.py:19] end load_into_gpu_async cost 0.0017116069793701172 seconds
DEBUG 01-15 16:10:44.543289.543289 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.543763.543763 cuda_h.py:19] end restore_tensors2 cost 7.534027099609375e-05 seconds
DEBUG 01-15 16:10:44.543949.543949 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022885799407958984 seconds
INFO 01-15 16:10:44.543614.543614 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 93cca03f-7c9e-41ed-96f0-1263b79a06ec
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.546646.546646 cuda_h.py:19] end self_attn cost 0.004519462585449219 seconds
DEBUG 01-15 16:10:44.547958.547958 cuda_h.py:19] end iln_self_attn_paln cost 0.006249904632568359 seconds
DEBUG 01-15 16:10:44.547271.547271 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:10:44.547895.547895 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.547671.547671 cuda_h.py:19] end gate cost 0.0007770061492919922 seconds
DEBUG 01-15 16:10:44.547653.547653 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.548413.548413 lmp.py:1912] 
DEBUG 01-15 16:10:44.548413.548413 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.548560.548560 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.548932.548932 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.548251.548251 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.548185.548185 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.548166.548166 lmp.py:1917] 
DEBUG 01-15 16:10:44.548166.548166 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.548147.548147 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.548565.548565 lmp.py:1935]   Expert 13 |     29 | CPU
DEBUG 01-15 16:10:44.548784.548784 lmp.py:1935]   Expert 44 |     40 | CPU
DEBUG 01-15 16:10:44.548527.548527 lmp.py:1935]   Expert  9 |     42 | CPU
DEBUG 01-15 16:10:44.548792.548792 lmp.py:1935]   Expert 25 |     42 | CPU
DEBUG 01-15 16:10:44.548250.548250 lmp.py:1935]   Expert 16 |     46 | CPU
DEBUG 01-15 16:10:44.548423.548423 lmp.py:1935]   Expert 38 |     47 | CPU
DEBUG 01-15 16:10:44.548880.548880 lmp.py:1935]   Expert  2 |     52 | CPU
DEBUG 01-15 16:10:44.548259.548259 lmp.py:1935]   Expert 22 |     52 | CPU
DEBUG 01-15 16:10:44.548856.548856 lmp.py:1935]   Expert 33 |     59 | CPU
DEBUG 01-15 16:10:44.548022.548022 lmp.py:1935]   Expert 42 |     59 | CPU
DEBUG 01-15 16:10:44.548903.548903 lmp.py:1935]   Expert  5 |     66 | CPU
DEBUG 01-15 16:10:44.548215.548215 lmp.py:1935]   Expert 23 |     76 | CPU
DEBUG 01-15 16:10:44.548050.548050 lmp.py:1935]   Expert 24 |     80 | CPU
DEBUG 01-15 16:10:44.549647.549647 lmp.py:1935]   Expert 10 |     86 | CPU
DEBUG 01-15 16:10:44.549244.549244 lmp.py:1935]   Expert 59 |    102 | CPU
DEBUG 01-15 16:10:44.549363.549363 lmp.py:1935]   Expert 21 |    108 | CPU
DEBUG 01-15 16:10:44.549960.549960 lmp.py:1935]   Expert 46 |    113 | CPU
DEBUG 01-15 16:10:44.549795.549795 lmp.py:1935]   Expert 55 |    115 | CPU
DEBUG 01-15 16:10:44.549630.549630 lmp.py:1935]   Expert 45 |    117 | CPU
DEBUG 01-15 16:10:44.549419.549419 lmp.py:1935]   Expert 61 |    119 | CPU
DEBUG 01-15 16:10:44.549731.549731 lmp.py:1935]   Expert 31 |    128 | CPU
DEBUG 01-15 16:10:44.549520.549520 lmp.py:1935]   Expert 51 |    140 | CPU
DEBUG 01-15 16:10:44.549831.549831 lmp.py:1935]   Expert 36 |    141 | CPU
DEBUG 01-15 16:10:44.549190.549190 lmp.py:1935]   Expert  8 |    143 | CPU
DEBUG 01-15 16:10:44.549548.549548 lmp.py:1935]   Expert 43 |    143 | CPU
DEBUG 01-15 16:10:44.549383.549383 lmp.py:1935]   Expert  6 |    144 | CPU
DEBUG 01-15 16:10:44.549741.549741 lmp.py:1935]   Expert  3 |    152 | CPU
DEBUG 01-15 16:10:44.549099.549099 lmp.py:1935]   Expert 26 |    157 | CPU
DEBUG 01-15 16:10:44.549934.549934 lmp.py:1935]   Expert  0 |    159 | CPU
DEBUG 01-15 16:10:44.549246.549246 lmp.py:1935]   Expert 18 |    159 | CPU
DEBUG 01-15 16:10:44.549797.549797 lmp.py:1935]   Expert 48 |    161 | CPU
DEBUG 01-15 16:10:44.549585.549585 lmp.py:1935]   Expert 41 |    167 | CPU
DEBUG 01-15 16:10:44.549282.549282 lmp.py:1935]   Expert 12 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549786.549786 lmp.py:1935]   Expert  7 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549336.549336 lmp.py:1935]   Expert 20 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549125.549125 lmp.py:1935]   Expert 28 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549152.549152 lmp.py:1935]   Expert 56 |    189 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549464.549464 lmp.py:1935]   Expert 27 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549252.549252 lmp.py:1935]   Expert 34 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549803.549803 lmp.py:1935]   Expert  1 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549545.549545 lmp.py:1935]   Expert 47 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549764.549764 lmp.py:1935]   Expert 32 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549268.549268 lmp.py:1935]   Expert 11 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549978.549978 lmp.py:1935]   Expert 40 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549767.549767 lmp.py:1935]   Expert 49 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549840.549840 lmp.py:1935]   Expert 53 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549914.549914 lmp.py:1935]   Expert 63 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549749.549749 lmp.py:1935]   Expert 15 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549299.549299 lmp.py:1935]   Expert 50 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549134.549134 lmp.py:1935]   Expert 29 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549851.549851 lmp.py:1935]   Expert  4 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549832.549832 lmp.py:1935]   Expert 30 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549879.549879 lmp.py:1935]   Expert 35 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549814.549814 lmp.py:1935]   Expert 14 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549841.549841 lmp.py:1935]   Expert 37 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549914.549914 lmp.py:1935]   Expert 52 |    341 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549465.549465 lmp.py:1935]   Expert 17 |    357 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549253.549253 lmp.py:1935]   Expert 54 |    374 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549327.549327 lmp.py:1935]   Expert 39 |    387 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549877.549877 lmp.py:1935]   Expert 57 |    410 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.549189.549189 lmp.py:1935]   Expert 62 |    458 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.549501.549501 lmp.py:1935]   Expert 60 |    460 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.550197.550197 lmp.py:1935]   Expert 19 |    544 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.550238.550238 lmp.py:1935]   Expert 58 |    574 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.550802.550802 lmp.py:1937] 
DEBUG 01-15 16:10:44.550802.550802 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.550035.550035 lmp.py:1938]   CPU:   3244 tokens
DEBUG 01-15 16:10:44.550267.550267 lmp.py:1942]   cuda:1:   4522 tokens (16 experts)
DEBUG 01-15 16:10:44.550023.550023 lmp.py:1942]   cuda:2:   4522 tokens (16 experts)
DEBUG 01-15 16:10:44.550587.550587 lmp.py:1943]   Total GPU:   9044 tokens
DEBUG 01-15 16:10:44.550899.550899 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.550899.550899 lmp.py:1944] 
DEBUG 01-15 16:10:44.550794.550794 cuda_h.py:19] end experts_map_get cost 0.0022115707397460938 seconds
INFO 01-15 16:10:44.550878.550878 client.py:127] Model loaded
DEBUG 01-15 16:10:44.550496.550496 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.550646.550646 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.550012.550012 lmp.py:1953] 
DEBUG 01-15 16:10:44.550012.550012 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.550047.550047 cuda_h.py:19] end cpu_experts_submit cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:10:44.550081.550081 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.550249.550249 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.551933.551933 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.1.gate_proj.weight': 0, 'model.layers.25.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.7.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.7.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.7.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.20.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.20.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.20.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.25.mlp.experts.32.gate_proj.weight': 0, 'model.layers.25.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.40.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.40.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.40.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.28.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.28.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.28.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.54.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.54.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.54.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.56.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.56.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.56.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(29452926976, 5767168, 0, 0), (29458694144, 5767168, 5767168, 0), (29447159808, 5767168, 11534336, 0), (30023876608, 5767168, 17301504, 0), (30029643776, 5767168, 23068672, 0), (30018109440, 5767168, 28835840, 0), (30041178112, 5767168, 34603008, 0), (30046945280, 5767168, 40370176, 0), (30035410944, 5767168, 46137344, 0), (29504831488, 5767168, 51904512, 0), (29510598656, 5767168, 57671680, 0), (29499064320, 5767168, 63438848, 0), (30110384128, 5767168, 69206016, 0), (30116151296, 5767168, 74973184, 0), (30104616960, 5767168, 80740352, 0), (29556736000, 5767168, 86507520, 0), (29562503168, 5767168, 92274688, 0), (29550968832, 5767168, 98041856, 0), (29625942016, 5767168, 103809024, 0), (29631709184, 5767168, 109576192, 0), (29620174848, 5767168, 115343360, 0), (30248796160, 5767168, 121110528, 0), (30254563328, 5767168, 126877696, 0), (30243028992, 5767168, 132644864, 0), (29729751040, 5767168, 138412032, 0), (29735518208, 5767168, 144179200, 0), (29723983872, 5767168, 149946368, 0), (30300700672, 5767168, 155713536, 0), (30306467840, 5767168, 161480704, 0), (30294933504, 5767168, 167247872, 0), (30335303680, 5767168, 173015040, 0), (30341070848, 5767168, 178782208, 0), (30329536512, 5767168, 184549376, 0), (30352605184, 5767168, 190316544, 0), (30358372352, 5767168, 196083712, 0), (30346838016, 5767168, 201850880, 0), (29781655552, 5767168, 207618048, 0), (29787422720, 5767168, 213385216, 0), (29775888384, 5767168, 219152384, 0), (30439112704, 5767168, 224919552, 0), (30444879872, 5767168, 230686720, 0), (30433345536, 5767168, 236453888, 0), (30508318720, 5767168, 242221056, 0), (30514085888, 5767168, 247988224, 0), (30502551552, 5767168, 253755392, 0), (30525620224, 5767168, 259522560, 0), (30531387392, 5767168, 265289728, 0), (30519853056, 5767168, 271056896, 0)], 2: [(29989273600, 5767168, 0, 0), (29995040768, 5767168, 5767168, 0), (29983506432, 5767168, 11534336, 0), (30075781120, 5767168, 17301504, 0), (30081548288, 5767168, 23068672, 0), (30070013952, 5767168, 28835840, 0), (30127685632, 5767168, 34603008, 0), (30133452800, 5767168, 40370176, 0), (30121918464, 5767168, 46137344, 0), (29643243520, 5767168, 51904512, 0), (29649010688, 5767168, 57671680, 0), (29637476352, 5767168, 63438848, 0), (29920067584, 5767168, 69206016, 0), (29925834752, 5767168, 74973184, 0), (29914300416, 5767168, 80740352, 0), (29677846528, 5767168, 86507520, 0), (29683613696, 5767168, 92274688, 0), (29672079360, 5767168, 98041856, 0), (29695148032, 5767168, 103809024, 0), (29700915200, 5767168, 109576192, 0), (29689380864, 5767168, 115343360, 0), (30283399168, 5767168, 121110528, 0), (30289166336, 5767168, 126877696, 0), (30277632000, 5767168, 132644864, 0), (29764354048, 5767168, 138412032, 0), (29770121216, 5767168, 144179200, 0), (29758586880, 5767168, 149946368, 0), (30369906688, 5767168, 155713536, 0), (30375673856, 5767168, 161480704, 0), (30364139520, 5767168, 167247872, 0), (30404509696, 5767168, 173015040, 0), (30410276864, 5767168, 178782208, 0), (30398742528, 5767168, 184549376, 0), (30421811200, 5767168, 190316544, 0), (30427578368, 5767168, 196083712, 0), (30416044032, 5767168, 201850880, 0), (29902766080, 5767168, 207618048, 0), (29908533248, 5767168, 213385216, 0), (29896998912, 5767168, 219152384, 0), (30473715712, 5767168, 224919552, 0), (30479482880, 5767168, 230686720, 0), (30467948544, 5767168, 236453888, 0), (29937369088, 5767168, 242221056, 0), (29943136256, 5767168, 247988224, 0), (29931601920, 5767168, 253755392, 0), (29954670592, 5767168, 259522560, 0), (29960437760, 5767168, 265289728, 0), (29948903424, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b03d6760>, 2: <capsule object NULL at 0x7a4e547ae970>}
DEBUG 01-15 16:10:44.552237.552237 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.552769.552769 cuda_h.py:19] end restore2model cost 0.001767873764038086 seconds
DEBUG 01-15 16:10:44.552182.552182 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:44.552082.552082 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.552543.552543 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5bac5cb1-a138-4958-8524-3a8fd4806716
DEBUG 01-15 16:10:44.552100.552100 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.552086.552086 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.552932.552932 cuda_h.py:19] end sllm_worker_task cost 0.011381149291992188 seconds
DEBUG 01-15 16:10:44.553885.553885 cuda_h.py:19] end move_flatidxs cost 0.0008416175842285156 seconds
DEBUG 01-15 16:10:44.553045.553045 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:44.553725.553725 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5bac5cb1-a138-4958-8524-3a8fd4806716
DEBUG 01-15 16:10:44.554912.554912 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0031583309173583984 seconds
DEBUG 01-15 16:10:44.554106.554106 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.557885.557885 cuda_h.py:19] end restore2model cost 0.0030298233032226562 seconds
DEBUG 01-15 16:10:44.557675.557675 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006415843963623047 seconds
DEBUG 01-15 16:10:44.557854.557854 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.557052.557052 cuda_h.py:19] end gpu_sexperts cost 0.0003209114074707031 seconds
DEBUG 01-15 16:10:44.557550.557550 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.557187.557187 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.71661376953125e-05 seconds
DEBUG 01-15 16:10:44.557791.557791 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.557116.557116 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.558048.558048 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010750293731689453 seconds
DEBUG 01-15 16:10:44.559566.559566 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.559375.559375 cuda_h.py:19] end gpu_group_list cost 0.00017833709716796875 seconds
DEBUG 01-15 16:10:44.560202.560202 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.561693.561693 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001171112060546875 seconds
DEBUG 01-15 16:10:44.561888.561888 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.561643.561643 cuda_h.py:19] end gpu_group_list cost 0.00017595291137695312 seconds
DEBUG 01-15 16:10:44.562755.562755 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.562830.562830 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5bac5cb1-a138-4958-8524-3a8fd4806716
DEBUG 01-15 16:10:44.562143.562143 cuda_h.py:19] end group_tensors cost 0.0090179443359375 seconds
DEBUG 01-15 16:10:44.563745.563745 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.567699.567699 cuda_h.py:19] end group pad cost 0.003917694091796875 seconds
DEBUG 01-15 16:10:44.567774.567774 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.579909.579909 client.py:127] Model loaded
DEBUG 01-15 16:10:44.579676.579676 cuda_h.py:19] end wait_experts_multi_device cost 0.01714181900024414 seconds
DEBUG 01-15 16:10:44.579321.579321 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.586120.586120 cuda_h.py:19] end group_einsum cost 0.019173383712768555 seconds
DEBUG 01-15 16:10:44.586139.586139 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.590186.590186 cuda_h.py:19] end get_outputs_cpu1 cost 0.003515005111694336 seconds
DEBUG 01-15 16:10:44.591823.591823 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03867316246032715 seconds
DEBUG 01-15 16:10:44.591917.591917 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.011605024337768555 seconds
DEBUG 01-15 16:10:44.591728.591728 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.592115.592115 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.593670.593670 cuda_h.py:19] end index_scatter cost 7.2479248046875e-05 seconds
DEBUG 01-15 16:10:44.593721.593721 cuda_h.py:19] end cpuoutputsdeal cost 0.0017116069793701172 seconds
DEBUG 01-15 16:10:44.593823.593823 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.593201.593201 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.593286.593286 cuda_h.py:19] end gpu_group_tensor cost 0.00013589859008789062 seconds
DEBUG 01-15 16:10:44.593618.593618 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.593869.593869 cuda_h.py:19] end gpu_group_tensor cost 0.00012040138244628906 seconds
DEBUG 01-15 16:10:44.593812.593812 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.594834.594834 cuda_h.py:19] end gpu_group_einsum cost 0.0005967617034912109 seconds
DEBUG 01-15 16:10:44.594739.594739 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.595689.595689 cuda_h.py:19] end gpu_group_einsum cost 0.0004246234893798828 seconds
DEBUG 01-15 16:10:44.595494.595494 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.595577.595577 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.595934.595934 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001575946807861328 seconds
DEBUG 01-15 16:10:44.595975.595975 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.595362.595362 cuda_h.py:19] end concat_expert_out cost 4.410743713378906e-05 seconds
DEBUG 01-15 16:10:44.595821.595821 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.595790.595790 cuda_h.py:19] end index_scatter cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:44.596043.596043 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007121562957763672 seconds
DEBUG 01-15 16:10:44.596158.596158 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.596478.596478 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.596218.596218 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012731552124023438 seconds
DEBUG 01-15 16:10:44.596306.596306 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.596342.596342 cuda_h.py:19] end concat_expert_out cost 5.340576171875e-05 seconds
DEBUG 01-15 16:10:44.596615.596615 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.596824.596824 cuda_h.py:19] end index_scatter cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:44.596394.596394 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004863739013671875 seconds
DEBUG 01-15 16:10:44.596966.596966 cuda_h.py:19] end gpu_experts_multi_device cost 0.0388340950012207 seconds
DEBUG 01-15 16:10:44.596161.596161 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.04967379570007324 seconds
DEBUG 01-15 16:10:44.597008.597008 cuda_h.py:19] end prefill_layer cost 0.0566098690032959 seconds
DEBUG 01-15 16:10:44.597553.597553 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:10:44.597984.597984 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.597118.597118 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:10:44.597251.597251 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:44.597530.597530 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:44.597287.597287 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 3.8623809814453125e-05 seconds
DEBUG 01-15 16:10:44.597805.597805 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 7.176399230957031e-05 seconds
DEBUG 01-15 16:10:44.597693.597693 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.597848.597848 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.597056.597056 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:44.597556.597556 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:44.597631.597631 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:44.598072.598072 cuda_h.py:19] end allocate_cuda_memory cost 0.00028634071350097656 seconds
DEBUG 01-15 16:10:44.598849.598849 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:44.598043.598043 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.598349.598349 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.598820.598820 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e5f96f0e-7abc-4966-93c3-0f1933329675
DEBUG 01-15 16:10:44.598301.598301 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.598377.598377 cuda_h.py:10] start self_attn
INFO 01-15 16:10:44.599159.599159 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e5f96f0e-7abc-4966-93c3-0f1933329675
DEBUG 01-15 16:10:44.599287.599287 cuda_h.py:19] end load_into_gpu_async cost 0.0015494823455810547 seconds
DEBUG 01-15 16:10:44.599135.599135 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:44.599391.599391 cuda_h.py:19] end restore_tensors2 cost 8.702278137207031e-05 seconds
DEBUG 01-15 16:10:44.599154.599154 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002199411392211914 seconds
INFO 01-15 16:10:44.600249.600249 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e5f96f0e-7abc-4966-93c3-0f1933329675
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.602545.602545 cuda_h.py:19] end self_attn cost 0.0035583972930908203 seconds
DEBUG 01-15 16:10:44.602927.602927 cuda_h.py:19] end iln_self_attn_paln cost 0.005160331726074219 seconds
DEBUG 01-15 16:10:44.602471.602471 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:10:44.602280.602280 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.603962.603962 cuda_h.py:19] end gate cost 0.0007460117340087891 seconds
DEBUG 01-15 16:10:44.603229.603229 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.603488.603488 lmp.py:1912] 
DEBUG 01-15 16:10:44.603488.603488 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.604012.604012 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.604622.604622 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.604133.604133 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.604968.604968 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.604611.604611 lmp.py:1917] 
DEBUG 01-15 16:10:44.604611.604611 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.604731.604731 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.604526.604526 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:10:44.604169.604169 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:10:44.604859.604859 lmp.py:1935]   Expert 11 |     28 | CPU
DEBUG 01-15 16:10:44.604786.604786 lmp.py:1935]   Expert  7 |     39 | CPU
DEBUG 01-15 16:10:44.604714.604714 lmp.py:1935]   Expert 62 |     42 | CPU
DEBUG 01-15 16:10:44.604403.604403 lmp.py:1935]   Expert  3 |     45 | CPU
DEBUG 01-15 16:10:44.604093.604093 lmp.py:1935]   Expert 51 |     45 | CPU
DEBUG 01-15 16:10:44.604020.604020 lmp.py:1935]   Expert 30 |     51 | CPU
DEBUG 01-15 16:10:44.604425.604425 lmp.py:1935]   Expert 17 |     53 | CPU
DEBUG 01-15 16:10:44.604830.604830 lmp.py:1935]   Expert 29 |     54 | CPU
DEBUG 01-15 16:10:44.604711.604711 lmp.py:1935]   Expert  6 |     61 | CPU
DEBUG 01-15 16:10:44.604115.604115 lmp.py:1935]   Expert  9 |     67 | CPU
DEBUG 01-15 16:10:44.604805.604805 lmp.py:1935]   Expert 38 |     75 | CPU
DEBUG 01-15 16:10:44.604256.604256 lmp.py:1935]   Expert 63 |     76 | CPU
DEBUG 01-15 16:10:44.604905.604905 lmp.py:1935]   Expert 55 |     81 | CPU
DEBUG 01-15 16:10:44.604740.604740 lmp.py:1935]   Expert 59 |     87 | CPU
DEBUG 01-15 16:10:44.604383.604383 lmp.py:1935]   Expert 48 |     93 | CPU
DEBUG 01-15 16:10:44.604788.604788 lmp.py:1935]   Expert 19 |     94 | CPU
DEBUG 01-15 16:10:44.604193.604193 lmp.py:1935]   Expert  8 |     96 | CPU
DEBUG 01-15 16:10:44.604359.604359 lmp.py:1935]   Expert 22 |    102 | CPU
DEBUG 01-15 16:10:44.604763.604763 lmp.py:1935]   Expert 49 |    102 | CPU
DEBUG 01-15 16:10:44.604929.604929 lmp.py:1935]   Expert 24 |    109 | CPU
DEBUG 01-15 16:10:44.604857.604857 lmp.py:1935]   Expert 34 |    115 | CPU
DEBUG 01-15 16:10:44.604500.604500 lmp.py:1935]   Expert 36 |    116 | CPU
DEBUG 01-15 16:10:44.604620.604620 lmp.py:1935]   Expert 42 |    117 | CPU
DEBUG 01-15 16:10:44.604263.604263 lmp.py:1935]   Expert 50 |    120 | CPU
DEBUG 01-15 16:10:44.604144.604144 lmp.py:1935]   Expert 39 |    124 | CPU
DEBUG 01-15 16:10:44.604026.604026 lmp.py:1935]   Expert  4 |    129 | CPU
DEBUG 01-15 16:10:44.604430.604430 lmp.py:1935]   Expert 37 |    142 | CPU
DEBUG 01-15 16:10:44.604835.604835 lmp.py:1935]   Expert 41 |    145 | CPU
DEBUG 01-15 16:10:44.604001.604001 lmp.py:1935]   Expert 15 |    149 | CPU
DEBUG 01-15 16:10:44.604405.604405 lmp.py:1935]   Expert 23 |    158 | CPU
DEBUG 01-15 16:10:44.604479.604479 lmp.py:1935]   Expert 56 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.604552.604552 lmp.py:1935]   Expert 16 |    165 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.604149.604149 lmp.py:1935]   Expert 60 |    166 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.604746.604746 lmp.py:1935]   Expert 44 |    171 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.604866.604866 lmp.py:1935]   Expert  1 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.604701.604701 lmp.py:1935]   Expert 21 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.604774.604774 lmp.py:1935]   Expert 43 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.604609.604609 lmp.py:1935]   Expert 47 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.604444.604444 lmp.py:1935]   Expert 53 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.604802.604802 lmp.py:1935]   Expert 12 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.604684.604684 lmp.py:1935]   Expert 33 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.604804.604804 lmp.py:1935]   Expert 13 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.604400.604400 lmp.py:1935]   Expert 32 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.604759.604759 lmp.py:1935]   Expert 28 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.604117.604117 lmp.py:1935]   Expert  0 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.604998.604998 lmp.py:1935]   Expert 54 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.605641.605641 lmp.py:1935]   Expert 31 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.605920.605920 lmp.py:1935]   Expert 26 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.605994.605994 lmp.py:1935]   Expert 10 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.605352.605352 lmp.py:1935]   Expert 18 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.605233.605233 lmp.py:1935]   Expert 57 |    273 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.605115.605115 lmp.py:1935]   Expert  2 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.605519.605519 lmp.py:1935]   Expert 58 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.605162.605162 lmp.py:1935]   Expert 40 |    341 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.605567.605567 lmp.py:1935]   Expert 25 |    360 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.605210.605210 lmp.py:1935]   Expert 45 |    364 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.605091.605091 lmp.py:1935]   Expert  5 |    443 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.605734.605734 lmp.py:1935]   Expert 35 |    462 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.605377.605377 lmp.py:1935]   Expert 27 |    487 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.605212.605212 lmp.py:1935]   Expert 46 |    555 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.605571.605571 lmp.py:1935]   Expert 52 |    591 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.605167.605167 lmp.py:1935]   Expert 14 |    884 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.605095.605095 lmp.py:1937] 
DEBUG 01-15 16:10:44.605095.605095 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.605930.605930 lmp.py:1938]   CPU:   2737 tokens
DEBUG 01-15 16:10:44.605765.605765 lmp.py:1942]   cuda:1:   4827 tokens (16 experts)
DEBUG 01-15 16:10:44.605408.605408 lmp.py:1942]   cuda:2:   4724 tokens (16 experts)
DEBUG 01-15 16:10:44.605859.605859 lmp.py:1943]   Total GPU:   9551 tokens
DEBUG 01-15 16:10:44.605548.605548 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.605548.605548 lmp.py:1944] 
DEBUG 01-15 16:10:44.605913.605913 cuda_h.py:19] end experts_map_get cost 0.0018067359924316406 seconds
DEBUG 01-15 16:10:44.605194.605194 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.605996.605996 lmp.py:1953] 
DEBUG 01-15 16:10:44.605996.605996 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.605879.605879 cuda_h.py:19] end cpu_experts_submit cost 5.316734313964844e-05 seconds
DEBUG 01-15 16:10:44.605383.605383 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.605974.605974 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.606982.606982 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.32.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.32.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.32.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.33.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.33.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.33.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.1.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.1.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.1.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.5.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.5.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.5.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.60.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.60.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.60.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.56.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.56.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.56.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.31.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.31.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.31.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.57.up_proj.weight': 271056896}, 2: {'model.layers.26.mlp.experts.2.gate_proj.weight': 0, 'model.layers.26.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.40.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.40.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.40.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.12.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.12.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.12.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.44.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.44.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.44.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.54.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.54.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.54.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.26.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.26.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.26.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.28.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.28.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.28.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (31096569856, 5767168, 17301504, 0), (31102337024, 5767168, 23068672, 0), (31090802688, 5767168, 28835840, 0), (31113871360, 5767168, 34603008, 0), (31119638528, 5767168, 40370176, 0), (31108104192, 5767168, 46137344, 0), (30560223232, 5767168, 51904512, 0), (30565990400, 5767168, 57671680, 0), (30554456064, 5767168, 63438848, 0), (30629429248, 5767168, 69206016, 0), (30635196416, 5767168, 74973184, 0), (30623662080, 5767168, 80740352, 0), (30715936768, 5767168, 86507520, 0), (30721703936, 5767168, 92274688, 0), (30710169600, 5767168, 98041856, 0), (31286886400, 5767168, 103809024, 0), (31292653568, 5767168, 109576192, 0), (31281119232, 5767168, 115343360, 0), (30785142784, 5767168, 121110528, 0), (30790909952, 5767168, 126877696, 0), (30779375616, 5767168, 132644864, 0), (31581011968, 5767168, 138412032, 0), (31586779136, 5767168, 144179200, 0), (31575244800, 5767168, 149946368, 0), (31459901440, 5767168, 155713536, 0), (31465668608, 5767168, 161480704, 0), (31454134272, 5767168, 167247872, 0), (31511805952, 5767168, 173015040, 0), (31517573120, 5767168, 178782208, 0), (31506038784, 5767168, 184549376, 0), (30975459328, 5767168, 190316544, 0), (30981226496, 5767168, 196083712, 0), (30969692160, 5767168, 201850880, 0), (31546408960, 5767168, 207618048, 0), (31552176128, 5767168, 213385216, 0), (31540641792, 5767168, 219152384, 0), (31010062336, 5767168, 224919552, 0), (31015829504, 5767168, 230686720, 0), (31004295168, 5767168, 236453888, 0), (31079268352, 5767168, 242221056, 0), (31085035520, 5767168, 247988224, 0), (31073501184, 5767168, 253755392, 0), (31529107456, 5767168, 259522560, 0), (31534874624, 5767168, 265289728, 0), (31523340288, 5767168, 271056896, 0)], 2: [(30577524736, 5767168, 0, 0), (30583291904, 5767168, 5767168, 0), (30571757568, 5767168, 11534336, 0), (31148474368, 5767168, 17301504, 0), (31154241536, 5767168, 23068672, 0), (31142707200, 5767168, 28835840, 0), (31234981888, 5767168, 34603008, 0), (31240749056, 5767168, 40370176, 0), (31229214720, 5767168, 46137344, 0), (30750539776, 5767168, 51904512, 0), (30756306944, 5767168, 57671680, 0), (30744772608, 5767168, 63438848, 0), (31321489408, 5767168, 69206016, 0), (31327256576, 5767168, 74973184, 0), (31315722240, 5767168, 80740352, 0), (31338790912, 5767168, 86507520, 0), (31344558080, 5767168, 92274688, 0), (31333023744, 5767168, 98041856, 0), (30767841280, 5767168, 103809024, 0), (30773608448, 5767168, 109576192, 0), (30762074112, 5767168, 115343360, 0), (31356092416, 5767168, 121110528, 0), (31361859584, 5767168, 126877696, 0), (31350325248, 5767168, 132644864, 0), (31304187904, 5767168, 138412032, 0), (31309955072, 5767168, 144179200, 0), (31298420736, 5767168, 149946368, 0), (30854348800, 5767168, 155713536, 0), (30860115968, 5767168, 161480704, 0), (30848581632, 5767168, 167247872, 0), (30819745792, 5767168, 173015040, 0), (30825512960, 5767168, 178782208, 0), (30813978624, 5767168, 184549376, 0), (31442599936, 5767168, 190316544, 0), (31448367104, 5767168, 196083712, 0), (31436832768, 5767168, 201850880, 0), (30906253312, 5767168, 207618048, 0), (30912020480, 5767168, 213385216, 0), (30900486144, 5767168, 219152384, 0), (31477202944, 5767168, 224919552, 0), (31482970112, 5767168, 230686720, 0), (31471435776, 5767168, 236453888, 0), (30992760832, 5767168, 242221056, 0), (30998528000, 5767168, 247988224, 0), (30986993664, 5767168, 253755392, 0), (31027363840, 5767168, 259522560, 0), (31033131008, 5767168, 265289728, 0), (31021596672, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b858b8a0>, 2: <capsule object NULL at 0x7a4ec4743000>}
DEBUG 01-15 16:10:44.606895.606895 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.606016.606016 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.606295.606295 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0fcd5266-2e90-49af-b367-c8ad65408429
DEBUG 01-15 16:10:44.607745.607745 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.607595.607595 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:44.607870.607870 client.py:127] Model loaded
DEBUG 01-15 16:10:44.607752.607752 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.607671.607671 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.607016.607016 cuda_h.py:19] end restore2model cost 0.0003376007080078125 seconds
DEBUG 01-15 16:10:44.607024.607024 cuda_h.py:19] end sllm_worker_task cost 0.010045289993286133 seconds
INFO 01-15 16:10:44.608631.608631 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0fcd5266-2e90-49af-b367-c8ad65408429
DEBUG 01-15 16:10:44.608165.608165 cuda_h.py:19] end move_flatidxs cost 0.0008304119110107422 seconds
DEBUG 01-15 16:10:44.608246.608246 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:44.608783.608783 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0027785301208496094 seconds
DEBUG 01-15 16:10:44.608739.608739 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.611578.611578 cuda_h.py:19] end restore2model cost 0.002481698989868164 seconds
DEBUG 01-15 16:10:44.611799.611799 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005481243133544922 seconds
DEBUG 01-15 16:10:44.611879.611879 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.611611.611611 cuda_h.py:19] end gpu_sexperts cost 0.000263214111328125 seconds
DEBUG 01-15 16:10:44.611294.611294 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:44.611879.611879 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6927719116210938e-05 seconds
DEBUG 01-15 16:10:44.611813.611813 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.611085.611085 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.612736.612736 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007994174957275391 seconds
DEBUG 01-15 16:10:44.612731.612731 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.612010.612010 cuda_h.py:19] end gpu_group_list cost 0.0001773834228515625 seconds
DEBUG 01-15 16:10:44.613432.613432 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.614727.614727 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008633136749267578 seconds
DEBUG 01-15 16:10:44.614862.614862 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.614094.614094 cuda_h.py:19] end gpu_group_list cost 0.00017690658569335938 seconds
DEBUG 01-15 16:10:44.615829.615829 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.615704.615704 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0fcd5266-2e90-49af-b367-c8ad65408429
DEBUG 01-15 16:10:44.617571.617571 cuda_h.py:19] end group_tensors cost 0.009087800979614258 seconds
DEBUG 01-15 16:10:44.618935.618935 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.622866.622866 cuda_h.py:19] end group pad cost 0.0038340091705322266 seconds
DEBUG 01-15 16:10:44.622033.622033 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.634053.634053 client.py:127] Model loaded
DEBUG 01-15 16:10:44.634622.634622 cuda_h.py:19] end wait_experts_multi_device cost 0.019267797470092773 seconds
DEBUG 01-15 16:10:44.634775.634775 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.640934.640934 cuda_h.py:19] end group_einsum cost 0.01851487159729004 seconds
DEBUG 01-15 16:10:44.641800.641800 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.643798.643798 cuda_h.py:19] end get_outputs_cpu1 cost 0.002852916717529297 seconds
DEBUG 01-15 16:10:44.644831.644831 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03725409507751465 seconds
DEBUG 01-15 16:10:44.644118.644118 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010379314422607422 seconds
DEBUG 01-15 16:10:44.645923.645923 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.646279.646279 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.646436.646436 cuda_h.py:19] end index_scatter cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:10:44.646235.646235 cuda_h.py:19] end cpuoutputsdeal cost 0.0015597343444824219 seconds
DEBUG 01-15 16:10:44.646813.646813 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.646907.646907 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.646562.646562 cuda_h.py:19] end gpu_group_tensor cost 0.0001361370086669922 seconds
DEBUG 01-15 16:10:44.646179.646179 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.647667.647667 cuda_h.py:19] end gpu_group_tensor cost 0.00011992454528808594 seconds
DEBUG 01-15 16:10:44.647280.647280 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.648717.648717 cuda_h.py:19] end gpu_group_einsum cost 0.000728607177734375 seconds
DEBUG 01-15 16:10:44.648039.648039 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.648376.648376 cuda_h.py:19] end gpu_group_einsum cost 0.0005087852478027344 seconds
DEBUG 01-15 16:10:44.648890.648890 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.648431.648431 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.649201.649201 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020384788513183594 seconds
DEBUG 01-15 16:10:44.649685.649685 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.649298.649298 cuda_h.py:19] end concat_expert_out cost 6.270408630371094e-05 seconds
DEBUG 01-15 16:10:44.649949.649949 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.649064.649064 cuda_h.py:19] end index_scatter cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:44.649854.649854 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008308887481689453 seconds
DEBUG 01-15 16:10:44.649168.649168 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.649819.649819 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.650473.650473 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013494491577148438 seconds
DEBUG 01-15 16:10:44.650276.650276 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.650530.650530 cuda_h.py:19] end concat_expert_out cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:44.650227.650227 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.650628.650628 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:44.650722.650722 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000476837158203125 seconds
DEBUG 01-15 16:10:44.650486.650486 cuda_h.py:19] end gpu_experts_multi_device cost 0.03885197639465332 seconds
DEBUG 01-15 16:10:44.650965.650965 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.047734737396240234 seconds
DEBUG 01-15 16:10:44.650023.650023 cuda_h.py:19] end prefill_layer cost 0.0535430908203125 seconds
DEBUG 01-15 16:10:44.650065.650065 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:10:44.650722.650722 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:44.650901.650901 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:10:44.650558.650558 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:44.651898.651898 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:44.651299.651299 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:44.654728.654728 cuda_h.py:19] end self_attn cost 0.0026824474334716797 seconds
DEBUG 01-15 16:10:44.654103.654103 cuda_h.py:19] end iln_self_attn_paln cost 0.0033888816833496094 seconds
DEBUG 01-15 16:10:44.654125.654125 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:10:44.654887.654887 cuda_h.py:10] start gate
DEBUG 01-15 16:10:44.655989.655989 cuda_h.py:19] end gate cost 0.0006344318389892578 seconds
DEBUG 01-15 16:10:44.655064.655064 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:44.655707.655707 lmp.py:1912] 
DEBUG 01-15 16:10:44.655707.655707 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:44.655277.655277 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:44.655219.655219 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:44.655537.655537 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:44.655710.655710 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:44.655406.655406 lmp.py:1917] 
DEBUG 01-15 16:10:44.655406.655406 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:44.655579.655579 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:44.655474.655474 lmp.py:1935]   Expert 18 |     65 | CPU
DEBUG 01-15 16:10:44.655932.655932 lmp.py:1935]   Expert 47 |     70 | CPU
DEBUG 01-15 16:10:44.655390.655390 lmp.py:1935]   Expert 54 |     70 | CPU
DEBUG 01-15 16:10:44.655370.655370 lmp.py:1935]   Expert 23 |     76 | CPU
DEBUG 01-15 16:10:44.655113.655113 lmp.py:1935]   Expert 48 |     79 | CPU
DEBUG 01-15 16:10:44.655286.655286 lmp.py:1935]   Expert 45 |     82 | CPU
DEBUG 01-15 16:10:44.655697.655697 lmp.py:1935]   Expert 44 |     83 | CPU
DEBUG 01-15 16:10:44.655632.655632 lmp.py:1935]   Expert 20 |     91 | CPU
DEBUG 01-15 16:10:44.655805.655805 lmp.py:1935]   Expert 31 |     96 | CPU
DEBUG 01-15 16:10:44.655785.655785 lmp.py:1935]   Expert 36 |    103 | CPU
DEBUG 01-15 16:10:44.656051.656051 lmp.py:1935]   Expert 61 |    109 | CPU
DEBUG 01-15 16:10:44.656270.656270 lmp.py:1935]   Expert 33 |    119 | CPU
DEBUG 01-15 16:10:44.656536.656536 lmp.py:1935]   Expert 24 |    121 | CPU
DEBUG 01-15 16:10:44.656040.656040 lmp.py:1935]   Expert 42 |    121 | CPU
DEBUG 01-15 16:10:44.656305.656305 lmp.py:1935]   Expert 10 |    122 | CPU
DEBUG 01-15 16:10:44.656048.656048 lmp.py:1935]   Expert 43 |    123 | CPU
DEBUG 01-15 16:10:44.656982.656982 lmp.py:1935]   Expert 11 |    127 | CPU
DEBUG 01-15 16:10:44.656678.656678 lmp.py:1935]   Expert 56 |    130 | CPU
DEBUG 01-15 16:10:44.656374.656374 lmp.py:1935]   Expert 49 |    132 | CPU
DEBUG 01-15 16:10:44.656117.656117 lmp.py:1935]   Expert  6 |    136 | CPU
DEBUG 01-15 16:10:44.656621.656621 lmp.py:1935]   Expert 51 |    143 | CPU
DEBUG 01-15 16:10:44.656886.656886 lmp.py:1935]   Expert 17 |    149 | CPU
DEBUG 01-15 16:10:44.656390.656390 lmp.py:1935]   Expert  0 |    150 | CPU
DEBUG 01-15 16:10:44.656133.656133 lmp.py:1935]   Expert  5 |    155 | CPU
DEBUG 01-15 16:10:44.656398.656398 lmp.py:1935]   Expert 40 |    157 | CPU
DEBUG 01-15 16:10:44.656902.656902 lmp.py:1935]   Expert 12 |    160 | CPU
DEBUG 01-15 16:10:44.656837.656837 lmp.py:1935]   Expert 26 |    160 | CPU
DEBUG 01-15 16:10:44.656771.656771 lmp.py:1935]   Expert 55 |    160 | CPU
DEBUG 01-15 16:10:44.656720.656720 lmp.py:1935]   Expert 59 |    161 | CPU
DEBUG 01-15 16:10:44.656177.656177 lmp.py:1935]   Expert 57 |    162 | CPU
DEBUG 01-15 16:10:44.656397.656397 lmp.py:1935]   Expert 38 |    167 | CPU
DEBUG 01-15 16:10:44.656139.656139 lmp.py:1935]   Expert 46 |    167 | CPU
DEBUG 01-15 16:10:44.656027.656027 lmp.py:1935]   Expert 13 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656200.656200 lmp.py:1935]   Expert 35 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656373.656373 lmp.py:1935]   Expert 50 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656307.656307 lmp.py:1935]   Expert 58 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656672.656672 lmp.py:1935]   Expert 30 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656475.656475 lmp.py:1935]   Expert  7 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656840.656840 lmp.py:1935]   Expert 16 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656536.656536 lmp.py:1935]   Expert 15 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656232.656232 lmp.py:1935]   Expert 14 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656690.656690 lmp.py:1935]   Expert 32 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656909.656909 lmp.py:1935]   Expert  1 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656367.656367 lmp.py:1935]   Expert  3 |    220 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656824.656824 lmp.py:1935]   Expert  4 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656044.656044 lmp.py:1935]   Expert 34 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656932.656932 lmp.py:1935]   Expert 39 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656820.656820 lmp.py:1935]   Expert 28 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656993.656993 lmp.py:1935]   Expert 52 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656689.656689 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656147.656147 lmp.py:1935]   Expert 22 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656843.656843 lmp.py:1935]   Expert  2 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656824.656824 lmp.py:1935]   Expert 21 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.656043.656043 lmp.py:1935]   Expert 41 |    280 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656500.656500 lmp.py:1935]   Expert 60 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.656720.656720 lmp.py:1935]   Expert 63 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.657939.657939 lmp.py:1935]   Expert 29 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.657589.657589 lmp.py:1935]   Expert 62 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.657000.657000 lmp.py:1935]   Expert 27 |    302 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.657650.657650 lmp.py:1935]   Expert 37 |    330 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.657346.657346 lmp.py:1935]   Expert 53 |    335 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.657804.657804 lmp.py:1935]   Expert  8 |    337 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.657023.657023 lmp.py:1935]   Expert 19 |    440 | GPU2(cuda:2)
DEBUG 01-15 16:10:44.657481.657481 lmp.py:1935]   Expert  9 |    613 | GPU1(cuda:1)
DEBUG 01-15 16:10:44.657269.657269 lmp.py:1937] 
DEBUG 01-15 16:10:44.657269.657269 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:44.657250.657250 lmp.py:1938]   CPU:   3946 tokens
DEBUG 01-15 16:10:44.657469.657469 lmp.py:1942]   cuda:1:   4207 tokens (16 experts)
DEBUG 01-15 16:10:44.657642.657642 lmp.py:1942]   cuda:2:   4135 tokens (16 experts)
DEBUG 01-15 16:10:44.657100.657100 lmp.py:1943]   Total GPU:   8342 tokens
DEBUG 01-15 16:10:44.657081.657081 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:44.657081.657081 lmp.py:1944] 
DEBUG 01-15 16:10:44.657499.657499 cuda_h.py:19] end experts_map_get cost 0.0020782947540283203 seconds
DEBUG 01-15 16:10:44.657163.657163 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:44.657873.657873 lmp.py:1953] 
DEBUG 01-15 16:10:44.657873.657873 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:44.657769.657769 cuda_h.py:19] end cpu_experts_submit cost 6.270408630371094e-05 seconds
DEBUG 01-15 16:10:44.657757.657757 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:44.657554.657554 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:44.658244.658244 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.32.gate_proj.weight': 0, 'model.layers.27.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.28.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.28.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.28.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.27.mlp.experts.2.gate_proj.weight': 0, 'model.layers.27.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.35.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.35.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.35.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.7.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.7.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.7.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(32203866112, 5767168, 0, 0), (32209633280, 5767168, 5767168, 0), (32198098944, 5767168, 11534336, 0), (31667519488, 5767168, 17301504, 0), (31673286656, 5767168, 23068672, 0), (31661752320, 5767168, 28835840, 0), (31719424000, 5767168, 34603008, 0), (31725191168, 5767168, 40370176, 0), (31713656832, 5767168, 46137344, 0), (32324976640, 5767168, 51904512, 0), (32330743808, 5767168, 57671680, 0), (32319209472, 5767168, 63438848, 0), (31805931520, 5767168, 69206016, 0), (31811698688, 5767168, 74973184, 0), (31800164352, 5767168, 80740352, 0), (32359579648, 5767168, 86507520, 0), (32365346816, 5767168, 92274688, 0), (32353812480, 5767168, 98041856, 0), (32134660096, 5767168, 103809024, 0), (32140427264, 5767168, 109576192, 0), (32128892928, 5767168, 115343360, 0), (31875137536, 5767168, 121110528, 0), (31880904704, 5767168, 126877696, 0), (31869370368, 5767168, 132644864, 0), (31927042048, 5767168, 138412032, 0), (31932809216, 5767168, 144179200, 0), (31921274880, 5767168, 149946368, 0), (32515293184, 5767168, 155713536, 0), (32521060352, 5767168, 161480704, 0), (32509526016, 5767168, 167247872, 0), (32567197696, 5767168, 173015040, 0), (32572964864, 5767168, 178782208, 0), (32561430528, 5767168, 184549376, 0), (32030851072, 5767168, 190316544, 0), (32036618240, 5767168, 196083712, 0), (32025083904, 5767168, 201850880, 0), (32117358592, 5767168, 207618048, 0), (32123125760, 5767168, 213385216, 0), (32111591424, 5767168, 219152384, 0), (32688308224, 5767168, 224919552, 0), (32694075392, 5767168, 230686720, 0), (32682541056, 5767168, 236453888, 0), (32151961600, 5767168, 242221056, 0), (32157728768, 5767168, 247988224, 0), (32146194432, 5767168, 253755392, 0), (32169263104, 5767168, 259522560, 0), (32175030272, 5767168, 265289728, 0), (32163495936, 5767168, 271056896, 0)], 2: [(31684820992, 5767168, 0, 0), (31690588160, 5767168, 5767168, 0), (31679053824, 5767168, 11534336, 0), (32238469120, 5767168, 17301504, 0), (32244236288, 5767168, 23068672, 0), (32232701952, 5767168, 28835840, 0), (31702122496, 5767168, 34603008, 0), (31707889664, 5767168, 40370176, 0), (31696355328, 5767168, 46137344, 0), (32290373632, 5767168, 51904512, 0), (32296140800, 5767168, 57671680, 0), (32284606464, 5767168, 63438848, 0), (32255770624, 5767168, 69206016, 0), (32261537792, 5767168, 74973184, 0), (32250003456, 5767168, 80740352, 0), (31771328512, 5767168, 86507520, 0), (31777095680, 5767168, 92274688, 0), (31765561344, 5767168, 98041856, 0), (31788630016, 5767168, 103809024, 0), (31794397184, 5767168, 109576192, 0), (31782862848, 5767168, 115343360, 0), (31892439040, 5767168, 121110528, 0), (31898206208, 5767168, 126877696, 0), (31886671872, 5767168, 132644864, 0), (31909740544, 5767168, 138412032, 0), (31915507712, 5767168, 144179200, 0), (31903973376, 5767168, 149946368, 0), (31978946560, 5767168, 155713536, 0), (31984713728, 5767168, 161480704, 0), (31973179392, 5767168, 167247872, 0), (32549896192, 5767168, 173015040, 0), (32555663360, 5767168, 178782208, 0), (32544129024, 5767168, 184549376, 0), (32013549568, 5767168, 190316544, 0), (32019316736, 5767168, 196083712, 0), (32007782400, 5767168, 201850880, 0), (32082755584, 5767168, 207618048, 0), (32088522752, 5767168, 213385216, 0), (32076988416, 5767168, 219152384, 0), (32653705216, 5767168, 224919552, 0), (32659472384, 5767168, 230686720, 0), (32647938048, 5767168, 236453888, 0), (32722911232, 5767168, 242221056, 0), (32728678400, 5767168, 247988224, 0), (32717144064, 5767168, 253755392, 0), (32740212736, 5767168, 259522560, 0), (32745979904, 5767168, 265289728, 0), (32734445568, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec419a430>, 2: <capsule object NULL at 0x7a4e34648150>}
DEBUG 01-15 16:10:44.658680.658680 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:44.658775.658775 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:44.658022.658022 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 43b7bfb8-2418-4bd1-841d-d26247bdf891
DEBUG 01-15 16:10:44.658651.658651 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:44.658475.658475 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:44.659737.659737 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:44.660939.660939 cuda_h.py:19] end move_flatidxs cost 0.0008280277252197266 seconds
DEBUG 01-15 16:10:44.660430.660430 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:44.661465.661465 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 43b7bfb8-2418-4bd1-841d-d26247bdf891
DEBUG 01-15 16:10:44.662657.662657 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0044324398040771484 seconds
DEBUG 01-15 16:10:44.662321.662321 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:44.664380.664380 cuda_h.py:19] end restore2model cost 0.002500772476196289 seconds
DEBUG 01-15 16:10:44.664223.664223 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0071620941162109375 seconds
DEBUG 01-15 16:10:44.664734.664734 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:44.665618.665618 cuda_h.py:19] end gpu_sexperts cost 0.0002689361572265625 seconds
DEBUG 01-15 16:10:44.665302.665302 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:44.665442.665442 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.665920.665920 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008122920989990234 seconds
DEBUG 01-15 16:10:44.666240.666240 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.666982.666982 cuda_h.py:19] end gpu_group_list cost 0.00016760826110839844 seconds
DEBUG 01-15 16:10:44.666013.666013 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:44.666335.666335 cuda_h.py:19] end group_tensors cost 0.00620269775390625 seconds
DEBUG 01-15 16:10:44.667807.667807 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:44.668409.668409 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012798309326171875 seconds
DEBUG 01-15 16:10:44.668983.668983 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:44.668663.668663 cuda_h.py:19] end gpu_group_list cost 0.0002777576446533203 seconds
DEBUG 01-15 16:10:44.669964.669964 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:44.669728.669728 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 43b7bfb8-2418-4bd1-841d-d26247bdf891
DEBUG 01-15 16:10:44.671766.671766 cuda_h.py:19] end group pad cost 0.004053354263305664 seconds
DEBUG 01-15 16:10:44.671648.671648 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:44.689152.689152 client.py:127] Model loaded
DEBUG 01-15 16:10:44.689304.689304 cuda_h.py:19] end wait_experts_multi_device cost 0.01975536346435547 seconds
DEBUG 01-15 16:10:44.689995.689995 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:44.691310.691310 cuda_h.py:19] end group_einsum cost 0.019680500030517578 seconds
DEBUG 01-15 16:10:44.691785.691785 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:44.695191.695191 cuda_h.py:19] end get_outputs_cpu1 cost 0.00394749641418457 seconds
DEBUG 01-15 16:10:44.696590.696590 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.036995649337768555 seconds
DEBUG 01-15 16:10:44.696704.696704 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006813764572143555 seconds
DEBUG 01-15 16:10:44.696243.696243 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:44.697501.697501 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.697698.697698 cuda_h.py:19] end index_scatter cost 7.128715515136719e-05 seconds
DEBUG 01-15 16:10:44.698867.698867 cuda_h.py:19] end cpuoutputsdeal cost 0.0015795230865478516 seconds
DEBUG 01-15 16:10:44.698453.698453 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:44.698070.698070 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.698221.698221 cuda_h.py:19] end gpu_group_tensor cost 0.00014162063598632812 seconds
DEBUG 01-15 16:10:44.698554.698554 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:44.698810.698810 cuda_h.py:19] end gpu_group_tensor cost 0.00012493133544921875 seconds
DEBUG 01-15 16:10:44.698900.698900 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.699745.699745 cuda_h.py:19] end gpu_group_einsum cost 0.0004723072052001953 seconds
DEBUG 01-15 16:10:44.699378.699378 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:44.699074.699074 cuda_h.py:19] end gpu_group_einsum cost 0.00035881996154785156 seconds
DEBUG 01-15 16:10:44.699170.699170 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.700160.700160 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.700432.700432 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001633167266845703 seconds
DEBUG 01-15 16:10:44.700618.700618 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.700296.700296 cuda_h.py:19] end concat_expert_out cost 4.792213439941406e-05 seconds
DEBUG 01-15 16:10:44.700928.700928 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.700534.700534 cuda_h.py:19] end index_scatter cost 5.91278076171875e-05 seconds
DEBUG 01-15 16:10:44.700569.700569 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007750988006591797 seconds
DEBUG 01-15 16:10:44.700691.700691 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:44.700911.700911 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:44.701274.701274 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013017654418945312 seconds
DEBUG 01-15 16:10:44.701553.701553 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:44.701715.701715 cuda_h.py:19] end concat_expert_out cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:44.701889.701889 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:44.701674.701674 cuda_h.py:19] end index_scatter cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:10:44.701675.701675 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00047850608825683594 seconds
DEBUG 01-15 16:10:44.701293.701293 cuda_h.py:19] end gpu_experts_multi_device cost 0.03630828857421875 seconds
DEBUG 01-15 16:10:44.701110.701110 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.04700803756713867 seconds
DEBUG 01-15 16:10:44.701010.701010 cuda_h.py:19] end prefill_layer cost 0.050917863845825195 seconds
DEBUG 01-15 16:10:44.701509.701509 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:10:44.701457.701457 cuda_h.py:19] end prefill cost 1.5952389240264893 seconds
DEBUG 01-15 16:10:46.922428.922428 cuda_h.py:10] start generate_input_ids
generate input ids cost 0.08588242530822754 s
DEBUG 01-15 16:10:47.268467.268467 cuda_h.py:19] end generate_input_ids cost 0.3440518379211426 seconds
DEBUG 01-15 16:10:47.268945.268945 cuda_h.py:10] start init_cache
DEBUG 01-15 16:10:47.268478.268478 cuda_h.py:19] end init_cache cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:10:49.664367.664367 cuda_h.py:10] start init_meta_layer
DEBUG 01-15 16:10:49.665928.665928 cuda_h.py:19] end init_meta_layer cost 1.2636184692382812e-05 seconds
DEBUG 01-15 16:10:49.665955.665955 cuda_h.py:10] start init_weights
DEBUG 01-15 16:10:49.665341.665341 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:49.665097.665097 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:49.666497.666497 cuda_h.py:19] end allocate_cuda_memory cost 0.0006535053253173828 seconds
DEBUG 01-15 16:10:49.666347.666347 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:49.666103.666103 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:49.666634.666634 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:49.666953.666953 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, cb14fd5a-81c1-4979-97a8-7dfdf6ab259d
DEBUG 01-15 16:10:49.666783.666783 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:49.667239.667239 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, cb14fd5a-81c1-4979-97a8-7dfdf6ab259d
DEBUG 01-15 16:10:49.667783.667783 cuda_h.py:19] end load_into_gpu_async cost 0.0013439655303955078 seconds
DEBUG 01-15 16:10:49.668387.668387 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:49.668012.668012 cuda_h.py:19] end restore_tensors2 cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:49.668238.668238 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022554397583007812 seconds
DEBUG 01-15 16:10:49.668457.668457 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:49.668642.668642 cuda_h.py:19] end restore2model cost 0.000148773193359375 seconds
INFO 01-15 16:10:49.668828.668828 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, cb14fd5a-81c1-4979-97a8-7dfdf6ab259d
INFO 01-15 16:10:49.745311.745311 client.py:127] Model loaded
DEBUG 01-15 16:10:49.745594.745594 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 01-15 16:10:49.745134.745134 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:49.745469.745469 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:49.746987.746987 cuda_h.py:19] end allocate_cuda_memory cost 0.0003674030303955078 seconds
DEBUG 01-15 16:10:49.746886.746886 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:49.746994.746994 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:49.746547.746547 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:49.746735.746735 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4d1df626-3852-4ca8-bcc3-973decdead6b
DEBUG 01-15 16:10:49.746197.746197 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:49.747332.747332 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4d1df626-3852-4ca8-bcc3-973decdead6b
DEBUG 01-15 16:10:49.747681.747681 cuda_h.py:19] end load_into_gpu_async cost 0.0013082027435302734 seconds
DEBUG 01-15 16:10:49.747438.747438 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:49.748378.748378 cuda_h.py:19] end restore_tensors2 cost 0.00013136863708496094 seconds
DEBUG 01-15 16:10:49.748163.748163 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002367258071899414 seconds
INFO 01-15 16:10:49.748066.748066 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4d1df626-3852-4ca8-bcc3-973decdead6b
INFO 01-15 16:10:49.764052.764052 client.py:127] Model loaded
DEBUG 01-15 16:10:49.764300.764300 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:49.765893.765893 cuda_h.py:19] end restore2model cost 0.0008063316345214844 seconds
DEBUG 01-15 16:10:49.765633.765633 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.020061731338500977 seconds
DEBUG 01-15 16:10:49.765602.765602 cuda_h.py:19] end init_weights cost 0.09997940063476562 seconds
DEBUG 01-15 16:10:49.765306.765306 cuda_h.py:10] start copy_emodel
DEBUG 01-15 16:10:50.501034.501034 cuda_h.py:19] end copy_emodel cost 0.7350726127624512 seconds
DEBUG 01-15 16:10:50.501823.501823 cuda_h.py:10] start init_inputs_tokens
DEBUG 01-15 16:10:50.502145.502145 cuda_h.py:19] end init_inputs_tokens cost 0.0004730224609375 seconds
DEBUG 01-15 16:10:50.502643.502643 cuda_h.py:10] start prefill
DEBUG 01-15 16:10:50.502644.502644 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.502963.502963 lmp.py:1495] -------------------------------- start prefill layer 0 --------------------------------
DEBUG 01-15 16:10:50.502851.502851 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:50.502647.502647 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 01-15 16:10:50.502066.502066 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 3.647804260253906e-05 seconds
DEBUG 01-15 16:10:50.502100.502100 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 6.771087646484375e-05 seconds
DEBUG 01-15 16:10:50.502432.502432 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.502699.502699 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.502669.502669 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.502793.502793 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.502782.502782 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.503777.503777 cuda_h.py:19] end allocate_cuda_memory cost 0.00020074844360351562 seconds
DEBUG 01-15 16:10:50.503164.503164 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.503410.503410 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.503432.503432 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.503333.503333 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 780dd0f4-becd-401a-8067-5f1af35bab14
DEBUG 01-15 16:10:50.503337.503337 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.503089.503089 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.504950.504950 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 780dd0f4-becd-401a-8067-5f1af35bab14
DEBUG 01-15 16:10:50.504780.504780 cuda_h.py:19] end load_into_gpu_async cost 0.0015673637390136719 seconds
DEBUG 01-15 16:10:50.505313.505313 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.505324.505324 cuda_h.py:19] end restore_tensors2 cost 9.965896606445312e-05 seconds
DEBUG 01-15 16:10:50.505445.505445 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022280216217041016 seconds
INFO 01-15 16:10:50.505005.505005 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 780dd0f4-becd-401a-8067-5f1af35bab14
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.507361.507361 cuda_h.py:19] end self_attn cost 0.003890514373779297 seconds
DEBUG 01-15 16:10:50.508001.508001 cuda_h.py:19] end iln_self_attn_paln cost 0.006078481674194336 seconds
DEBUG 01-15 16:10:50.508354.508354 cuda_h.py:10] start dense_mlp
INFO 01-15 16:10:50.512385.512385 client.py:127] Model loaded
DEBUG 01-15 16:10:50.512049.512049 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.512792.512792 cuda_h.py:19] end restore2model cost 0.0005528926849365234 seconds
DEBUG 01-15 16:10:50.513595.513595 cuda_h.py:19] end sllm_worker_task cost 0.010112524032592773 seconds
DEBUG 01-15 16:10:50.513400.513400 cuda_h.py:19] end dense_mlp cost 0.0043125152587890625 seconds
DEBUG 01-15 16:10:50.513013.513013 cuda_h.py:19] end prefill_layer cost 0.01077723503112793 seconds
DEBUG 01-15 16:10:50.513319.513319 lmp.py:1553] -------------------------------- end prefill layer 0 --------------------------------
DEBUG 01-15 16:10:50.513684.513684 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.513811.513811 lmp.py:1495] -------------------------------- start prefill layer 1 --------------------------------
DEBUG 01-15 16:10:50.513983.513983 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:50.513348.513348 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 01-15 16:10:50.513979.513979 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 1.9788742065429688e-05 seconds
DEBUG 01-15 16:10:50.513490.513490 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 4.76837158203125e-05 seconds
DEBUG 01-15 16:10:50.513040.513040 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.513923.513923 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.513766.513766 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.513039.513039 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.513128.513128 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.513764.513764 cuda_h.py:19] end allocate_cuda_memory cost 0.0001742839813232422 seconds
DEBUG 01-15 16:10:50.514344.514344 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.514558.514558 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.514116.514116 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.514839.514839 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7e11f1f1-dcc1-4333-802d-fb045c3b825e
DEBUG 01-15 16:10:50.514220.514220 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.514533.514533 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.515174.515174 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7e11f1f1-dcc1-4333-802d-fb045c3b825e
DEBUG 01-15 16:10:50.515231.515231 cuda_h.py:19] end load_into_gpu_async cost 0.0012996196746826172 seconds
DEBUG 01-15 16:10:50.515769.515769 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.515938.515938 cuda_h.py:19] end restore_tensors2 cost 7.987022399902344e-05 seconds
DEBUG 01-15 16:10:50.515073.515073 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0019462108612060547 seconds
INFO 01-15 16:10:50.515188.515188 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7e11f1f1-dcc1-4333-802d-fb045c3b825e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.517108.517108 cuda_h.py:19] end self_attn cost 0.0032770633697509766 seconds
DEBUG 01-15 16:10:50.518528.518528 cuda_h.py:19] end iln_self_attn_paln cost 0.004648923873901367 seconds
DEBUG 01-15 16:10:50.518496.518496 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_2
DEBUG 01-15 16:10:50.518252.518252 cuda_h.py:10] start gate
DEBUG 01-15 16:10:50.518515.518515 cuda_h.py:19] end gate cost 0.0006871223449707031 seconds
DEBUG 01-15 16:10:50.519298.519298 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:50.519573.519573 lmp.py:1912] 
DEBUG 01-15 16:10:50.519573.519573 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:50.519952.519952 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:50.519390.519390 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:50.519179.519179 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:50.519107.519107 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:50.519843.519843 lmp.py:1917] 
DEBUG 01-15 16:10:50.519843.519843 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:50.519293.519293 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:50.519943.519943 lmp.py:1935]   Expert 25 |     64 | CPU
DEBUG 01-15 16:10:50.519632.519632 lmp.py:1935]   Expert 54 |     67 | CPU
DEBUG 01-15 16:10:50.519845.519845 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:50.519819.519819 lmp.py:1935]   Expert 31 |     72 | CPU
DEBUG 01-15 16:10:50.519555.519555 lmp.py:1935]   Expert 55 |     72 | CPU
DEBUG 01-15 16:10:50.519052.519052 lmp.py:1935]   Expert 62 |     87 | CPU
DEBUG 01-15 16:10:50.519264.519264 lmp.py:1935]   Expert 18 |     88 | CPU
DEBUG 01-15 16:10:50.519477.519477 lmp.py:1935]   Expert 52 |     98 | CPU
DEBUG 01-15 16:10:50.519451.519451 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:10:50.519902.519902 lmp.py:1935]   Expert 47 |    104 | CPU
DEBUG 01-15 16:10:50.519829.519829 lmp.py:1935]   Expert  0 |    113 | CPU
DEBUG 01-15 16:10:50.519803.519803 lmp.py:1935]   Expert 37 |    117 | CPU
DEBUG 01-15 16:10:50.519539.519539 lmp.py:1935]   Expert 27 |    121 | CPU
DEBUG 01-15 16:10:50.519036.519036 lmp.py:1935]   Expert 32 |    123 | CPU
DEBUG 01-15 16:10:50.519726.519726 lmp.py:1935]   Expert 41 |    130 | CPU
DEBUG 01-15 16:10:50.519223.519223 lmp.py:1935]   Expert 44 |    131 | CPU
DEBUG 01-15 16:10:50.519912.519912 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:10:50.519125.519125 lmp.py:1935]   Expert 13 |    138 | CPU
DEBUG 01-15 16:10:50.519575.519575 lmp.py:1935]   Expert 58 |    140 | CPU
DEBUG 01-15 16:10:50.519503.519503 lmp.py:1935]   Expert 60 |    144 | CPU
DEBUG 01-15 16:10:50.519192.519192 lmp.py:1935]   Expert 43 |    147 | CPU
DEBUG 01-15 16:10:50.519882.519882 lmp.py:1935]   Expert  1 |    150 | CPU
DEBUG 01-15 16:10:50.519571.519571 lmp.py:1935]   Expert 38 |    153 | CPU
DEBUG 01-15 16:10:50.519307.519307 lmp.py:1935]   Expert 49 |    154 | CPU
DEBUG 01-15 16:10:50.519996.519996 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:10:50.519732.519732 lmp.py:1935]   Expert 34 |    161 | CPU
DEBUG 01-15 16:10:50.519229.519229 lmp.py:1935]   Expert 35 |    164 | CPU
DEBUG 01-15 16:10:50.519726.519726 lmp.py:1935]   Expert 36 |    168 | CPU
DEBUG 01-15 16:10:50.519223.519223 lmp.py:1935]   Expert 11 |    170 | CPU
DEBUG 01-15 16:10:50.519482.519482 lmp.py:1935]   Expert 17 |    170 | CPU
DEBUG 01-15 16:10:50.519741.519741 lmp.py:1935]   Expert 59 |    174 | CPU
DEBUG 01-15 16:10:50.519238.519238 lmp.py:1935]   Expert 10 |    180 | CPU
DEBUG 01-15 16:10:50.519835.519835 lmp.py:1935]   Expert 20 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520193.520193 lmp.py:1935]   Expert  2 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520074.520074 lmp.py:1935]   Expert 39 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520240.520240 lmp.py:1935]   Expert 33 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520168.520168 lmp.py:1935]   Expert 12 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520619.520619 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520070.520070 lmp.py:1935]   Expert 48 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520759.520759 lmp.py:1935]   Expert 15 |    199 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520449.520449 lmp.py:1935]   Expert 53 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520138.520138 lmp.py:1935]   Expert 19 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520112.520112 lmp.py:1935]   Expert 26 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520563.520563 lmp.py:1935]   Expert 30 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520444.520444 lmp.py:1935]   Expert 45 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520087.520087 lmp.py:1935]   Expert  5 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520968.520968 lmp.py:1935]   Expert  4 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520896.520896 lmp.py:1935]   Expert 24 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520586.520586 lmp.py:1935]   Expert 42 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520036.520036 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520010.520010 lmp.py:1935]   Expert 29 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520700.520700 lmp.py:1935]   Expert 56 |    262 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520912.520912 lmp.py:1935]   Expert 61 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520363.520363 lmp.py:1935]   Expert  8 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520576.520576 lmp.py:1935]   Expert 63 |    285 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520656.520656 lmp.py:1935]   Expert 46 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520034.520034 lmp.py:1935]   Expert  9 |    300 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520201.520201 lmp.py:1935]   Expert  6 |    316 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520367.520367 lmp.py:1935]   Expert 16 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520294.520294 lmp.py:1935]   Expert 40 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520176.520176 lmp.py:1935]   Expert  7 |    322 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520296.520296 lmp.py:1935]   Expert 23 |    325 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520462.520462 lmp.py:1935]   Expert 14 |    413 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.520151.520151 lmp.py:1935]   Expert 57 |    464 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.520125.520125 lmp.py:1937] 
DEBUG 01-15 16:10:50.520125.520125 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:50.520576.520576 lmp.py:1938]   CPU:   4059 tokens
DEBUG 01-15 16:10:50.520027.520027 lmp.py:1942]   cuda:1:   4114 tokens (16 experts)
DEBUG 01-15 16:10:50.520239.520239 lmp.py:1942]   cuda:2:   4115 tokens (16 experts)
DEBUG 01-15 16:10:50.520213.520213 lmp.py:1943]   Total GPU:   8229 tokens
DEBUG 01-15 16:10:50.520472.520472 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:50.520472.520472 lmp.py:1944] 
DEBUG 01-15 16:10:50.520407.520407 cuda_h.py:19] end experts_map_get cost 0.0016314983367919922 seconds
DEBUG 01-15 16:10:50.520594.520594 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:50.520874.520874 lmp.py:1953] 
DEBUG 01-15 16:10:50.520874.520874 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:50.520326.520326 cuda_h.py:19] end cpu_experts_submit cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:10:50.520591.520591 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:50.520083.520083 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:50.522149.522149 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.1.mlp.experts.6.gate_proj.weight': 0, 'model.layers.1.mlp.experts.6.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.6.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.7.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.7.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.7.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.29.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.29.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.29.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.1.mlp.experts.33.gate_proj.weight': 0, 'model.layers.1.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.1.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.1.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.1.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.1.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.1.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.1.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.1.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.1.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.1.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.1.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.1.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.1.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.1.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.1.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.1.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.1.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.1.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.1.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.1.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.1.mlp.experts.45.gate_proj.weight': 121110528, 'model.layers.1.mlp.experts.45.down_proj.weight': 126877696, 'model.layers.1.mlp.experts.45.up_proj.weight': 132644864, 'model.layers.1.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.1.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.1.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.1.mlp.experts.14.gate_proj.weight': 155713536, 'model.layers.1.mlp.experts.14.down_proj.weight': 161480704, 'model.layers.1.mlp.experts.14.up_proj.weight': 167247872, 'model.layers.1.mlp.experts.16.gate_proj.weight': 173015040, 'model.layers.1.mlp.experts.16.down_proj.weight': 178782208, 'model.layers.1.mlp.experts.16.up_proj.weight': 184549376, 'model.layers.1.mlp.experts.15.gate_proj.weight': 190316544, 'model.layers.1.mlp.experts.15.down_proj.weight': 196083712, 'model.layers.1.mlp.experts.15.up_proj.weight': 201850880, 'model.layers.1.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.1.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.1.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.1.mlp.experts.21.gate_proj.weight': 224919552, 'model.layers.1.mlp.experts.21.down_proj.weight': 230686720, 'model.layers.1.mlp.experts.21.up_proj.weight': 236453888, 'model.layers.1.mlp.experts.23.gate_proj.weight': 242221056, 'model.layers.1.mlp.experts.23.down_proj.weight': 247988224, 'model.layers.1.mlp.experts.23.up_proj.weight': 253755392, 'model.layers.1.mlp.experts.56.gate_proj.weight': 259522560, 'model.layers.1.mlp.experts.56.down_proj.weight': 265289728, 'model.layers.1.mlp.experts.56.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(2964324352, 5767168, 0, 0), (2970091520, 5767168, 5767168, 0), (2958557184, 5767168, 11534336, 0), (2981625856, 5767168, 17301504, 0), (2987393024, 5767168, 23068672, 0), (2975858688, 5767168, 28835840, 0), (3535273984, 5767168, 34603008, 0), (3541041152, 5767168, 40370176, 0), (3529506816, 5767168, 46137344, 0), (3016228864, 5767168, 51904512, 0), (3021996032, 5767168, 57671680, 0), (3010461696, 5767168, 63438848, 0), (3068133376, 5767168, 69206016, 0), (3073900544, 5767168, 74973184, 0), (3062366208, 5767168, 80740352, 0), (3690987520, 5767168, 86507520, 0), (3696754688, 5767168, 92274688, 0), (3685220352, 5767168, 98041856, 0), (3362258944, 5767168, 103809024, 0), (3368026112, 5767168, 109576192, 0), (3356491776, 5767168, 115343360, 0), (3725590528, 5767168, 121110528, 0), (3731357696, 5767168, 126877696, 0), (3719823360, 5767168, 132644864, 0), (3189243904, 5767168, 138412032, 0), (3195011072, 5767168, 144179200, 0), (3183476736, 5767168, 149946368, 0), (3206545408, 5767168, 155713536, 0), (3212312576, 5767168, 161480704, 0), (3200778240, 5767168, 167247872, 0), (3275751424, 5767168, 173015040, 0), (3281518592, 5767168, 178782208, 0), (3269984256, 5767168, 184549376, 0), (3846701056, 5767168, 190316544, 0), (3852468224, 5767168, 196083712, 0), (3840933888, 5767168, 201850880, 0), (3310354432, 5767168, 207618048, 0), (3316121600, 5767168, 213385216, 0), (3304587264, 5767168, 219152384, 0), (3915907072, 5767168, 224919552, 0), (3921674240, 5767168, 230686720, 0), (3910139904, 5767168, 236453888, 0), (3379560448, 5767168, 242221056, 0), (3385327616, 5767168, 247988224, 0), (3373793280, 5767168, 253755392, 0), (3950510080, 5767168, 259522560, 0), (3956277248, 5767168, 265289728, 0), (3944742912, 5767168, 271056896, 0)], 2: [(3431464960, 5767168, 0, 0), (3437232128, 5767168, 5767168, 0), (3425697792, 5767168, 11534336, 0), (2895118336, 5767168, 17301504, 0), (2900885504, 5767168, 23068672, 0), (2889351168, 5767168, 28835840, 0), (2929721344, 5767168, 34603008, 0), (2935488512, 5767168, 40370176, 0), (2923954176, 5767168, 46137344, 0), (2947022848, 5767168, 51904512, 0), (2952790016, 5767168, 57671680, 0), (2941255680, 5767168, 63438848, 0), (3552575488, 5767168, 69206016, 0), (3558342656, 5767168, 74973184, 0), (3546808320, 5767168, 80740352, 0), (2998927360, 5767168, 86507520, 0), (3004694528, 5767168, 92274688, 0), (2993160192, 5767168, 98041856, 0), (3587178496, 5767168, 103809024, 0), (3592945664, 5767168, 109576192, 0), (3581411328, 5767168, 115343360, 0), (3639083008, 5767168, 121110528, 0), (3644850176, 5767168, 126877696, 0), (3633315840, 5767168, 132644864, 0), (3656384512, 5767168, 138412032, 0), (3662151680, 5767168, 144179200, 0), (3650617344, 5767168, 149946368, 0), (3102736384, 5767168, 155713536, 0), (3108503552, 5767168, 161480704, 0), (3096969216, 5767168, 167247872, 0), (3137339392, 5767168, 173015040, 0), (3143106560, 5767168, 178782208, 0), (3131572224, 5767168, 184549376, 0), (3120037888, 5767168, 190316544, 0), (3125805056, 5767168, 196083712, 0), (3114270720, 5767168, 201850880, 0), (3777495040, 5767168, 207618048, 0), (3783262208, 5767168, 213385216, 0), (3771727872, 5767168, 219152384, 0), (3223846912, 5767168, 224919552, 0), (3229614080, 5767168, 230686720, 0), (3218079744, 5767168, 236453888, 0), (3258449920, 5767168, 242221056, 0), (3264217088, 5767168, 247988224, 0), (3252682752, 5767168, 253755392, 0), (3829399552, 5767168, 259522560, 0), (3835166720, 5767168, 265289728, 0), (3823632384, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4681fe0>, 2: <capsule object NULL at 0x7a4ec4681770>}
DEBUG 01-15 16:10:50.522761.522761 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.522925.522925 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.522495.522495 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 803a1398-b8e4-48dc-a432-e5d77767b8fc
DEBUG 01-15 16:10:50.522873.522873 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:50.522939.522939 client.py:127] Model loaded
DEBUG 01-15 16:10:50.522266.522266 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.523616.523616 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:50.523771.523771 cuda_h.py:19] end restore2model cost 0.00040078163146972656 seconds
DEBUG 01-15 16:10:50.523123.523123 cuda_h.py:19] end sllm_worker_task cost 0.009827613830566406 seconds
DEBUG 01-15 16:10:50.523370.523370 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:50.524406.524406 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 803a1398-b8e4-48dc-a432-e5d77767b8fc
DEBUG 01-15 16:10:50.524600.524600 cuda_h.py:19] end move_flatidxs cost 0.0009911060333251953 seconds
DEBUG 01-15 16:10:50.524305.524305 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:50.525830.525830 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004101991653442383 seconds
DEBUG 01-15 16:10:50.525071.525071 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.527905.527905 cuda_h.py:19] end restore2model cost 0.0025129318237304688 seconds
DEBUG 01-15 16:10:50.527847.527847 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006838798522949219 seconds
DEBUG 01-15 16:10:50.527617.527617 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:50.528819.528819 cuda_h.py:19] end gpu_sexperts cost 0.00025844573974609375 seconds
DEBUG 01-15 16:10:50.528456.528456 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:50.528371.528371 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5020370483398438e-05 seconds
DEBUG 01-15 16:10:50.528068.528068 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:50.528055.528055 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.529494.529494 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008194446563720703 seconds
DEBUG 01-15 16:10:50.529960.529960 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.529000.529000 cuda_h.py:19] end gpu_group_list cost 0.0001766681671142578 seconds
DEBUG 01-15 16:10:50.530239.530239 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.532841.532841 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001504659652709961 seconds
DEBUG 01-15 16:10:50.532512.532512 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.532380.532380 cuda_h.py:19] end gpu_group_list cost 0.0001728534698486328 seconds
DEBUG 01-15 16:10:50.532838.532838 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:50.533575.533575 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 803a1398-b8e4-48dc-a432-e5d77767b8fc
DEBUG 01-15 16:10:50.536731.536731 cuda_h.py:19] end group_tensors cost 0.01125025749206543 seconds
DEBUG 01-15 16:10:50.537083.537083 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:50.541229.541229 cuda_h.py:19] end group pad cost 0.004506587982177734 seconds
DEBUG 01-15 16:10:50.541225.541225 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:50.551563.551563 client.py:127] Model loaded
DEBUG 01-15 16:10:50.551426.551426 cuda_h.py:19] end wait_experts_multi_device cost 0.01837921142578125 seconds
DEBUG 01-15 16:10:50.551213.551213 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:50.565370.565370 cuda_h.py:19] end group_einsum cost 0.023178577423095703 seconds
DEBUG 01-15 16:10:50.565754.565754 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:50.569491.569491 cuda_h.py:19] end get_outputs_cpu1 cost 0.0041141510009765625 seconds
DEBUG 01-15 16:10:50.570489.570489 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.046889543533325195 seconds
DEBUG 01-15 16:10:50.570912.570912 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01878809928894043 seconds
DEBUG 01-15 16:10:50.570479.570479 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:50.571882.571882 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.572530.572530 cuda_h.py:19] end index_scatter cost 8.296966552734375e-05 seconds
DEBUG 01-15 16:10:50.572752.572752 cuda_h.py:19] end cpuoutputsdeal cost 0.0015997886657714844 seconds
DEBUG 01-15 16:10:50.572023.572023 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:50.572262.572262 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.573761.573761 cuda_h.py:19] end gpu_group_tensor cost 0.0006093978881835938 seconds
DEBUG 01-15 16:10:50.573266.573266 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.573559.573559 cuda_h.py:19] end gpu_group_tensor cost 0.0005967617034912109 seconds
DEBUG 01-15 16:10:50.573735.573735 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.574760.574760 cuda_h.py:19] end gpu_group_einsum cost 0.0006682872772216797 seconds
DEBUG 01-15 16:10:50.574830.574830 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.575737.575737 cuda_h.py:19] end gpu_group_einsum cost 0.0009326934814453125 seconds
DEBUG 01-15 16:10:50.575608.575608 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.576444.576444 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.576988.576988 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001614093780517578 seconds
DEBUG 01-15 16:10:50.576658.576658 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.576799.576799 cuda_h.py:19] end concat_expert_out cost 0.0002090930938720703 seconds
DEBUG 01-15 16:10:50.576391.576391 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.576057.576057 cuda_h.py:19] end index_scatter cost 6.508827209472656e-05 seconds
DEBUG 01-15 16:10:50.577144.577144 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011212825775146484 seconds
DEBUG 01-15 16:10:50.577512.577512 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.577924.577924 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.577903.577903 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001289844512939453 seconds
DEBUG 01-15 16:10:50.577467.577467 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.577152.577152 cuda_h.py:19] end concat_expert_out cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:50.577564.577564 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.577932.577932 cuda_h.py:19] end index_scatter cost 6.0558319091796875e-05 seconds
DEBUG 01-15 16:10:50.577456.577456 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000484466552734375 seconds
DEBUG 01-15 16:10:50.577406.577406 cuda_h.py:19] end gpu_experts_multi_device cost 0.04962944984436035 seconds
DEBUG 01-15 16:10:50.577507.577507 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_2 cost 0.05963540077209473 seconds
DEBUG 01-15 16:10:50.578267.578267 cuda_h.py:19] end prefill_layer cost 0.06486892700195312 seconds
DEBUG 01-15 16:10:50.578634.578634 lmp.py:1553] -------------------------------- end prefill layer 1 --------------------------------
DEBUG 01-15 16:10:50.578290.578290 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.578516.578516 lmp.py:1495] -------------------------------- start prefill layer 2 --------------------------------
DEBUG 01-15 16:10:50.578219.578219 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:50.578068.578068 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 01-15 16:10:50.578540.578540 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 3.9577484130859375e-05 seconds
DEBUG 01-15 16:10:50.578337.578337 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.578312.578312 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 0.00015115737915039062 seconds
DEBUG 01-15 16:10:50.578898.578898 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.578111.578111 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.578690.578690 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.579448.579448 cuda_h.py:19] end allocate_cuda_memory cost 0.000186920166015625 seconds
DEBUG 01-15 16:10:50.579491.579491 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.579804.579804 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.579046.579046 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.579068.579068 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.579969.579969 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5440b77a-8536-4f48-8f6b-8107f646fd06
DEBUG 01-15 16:10:50.579735.579735 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.579938.579938 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.580013.580013 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5440b77a-8536-4f48-8f6b-8107f646fd06
DEBUG 01-15 16:10:50.580638.580638 cuda_h.py:19] end load_into_gpu_async cost 0.0016651153564453125 seconds
DEBUG 01-15 16:10:50.581871.581871 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.581074.581074 cuda_h.py:19] end restore_tensors2 cost 8.20159912109375e-05 seconds
DEBUG 01-15 16:10:50.581836.581836 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024139881134033203 seconds
INFO 01-15 16:10:50.581964.581964 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5440b77a-8536-4f48-8f6b-8107f646fd06
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.583419.583419 cuda_h.py:19] end self_attn cost 0.003236532211303711 seconds
DEBUG 01-15 16:10:50.583005.583005 cuda_h.py:19] end iln_self_attn_paln cost 0.004601001739501953 seconds
DEBUG 01-15 16:10:50.583311.583311 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_3
DEBUG 01-15 16:10:50.583928.583928 cuda_h.py:10] start gate
DEBUG 01-15 16:10:50.584541.584541 cuda_h.py:19] end gate cost 0.0006635189056396484 seconds
DEBUG 01-15 16:10:50.584039.584039 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:50.584349.584349 lmp.py:1912] 
DEBUG 01-15 16:10:50.584349.584349 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:50.584979.584979 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:50.584013.584013 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:50.584756.584756 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:50.584875.584875 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:50.584803.584803 lmp.py:1917] 
DEBUG 01-15 16:10:50.584803.584803 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:50.584969.584969 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:50.584096.584096 lmp.py:1935]   Expert 58 |     51 | CPU
DEBUG 01-15 16:10:50.584739.584739 lmp.py:1935]   Expert 27 |     56 | CPU
DEBUG 01-15 16:10:50.584428.584428 lmp.py:1935]   Expert  3 |     68 | CPU
DEBUG 01-15 16:10:50.584879.584879 lmp.py:1935]   Expert 17 |     83 | CPU
DEBUG 01-15 16:10:50.584092.584092 lmp.py:1935]   Expert  0 |     87 | CPU
DEBUG 01-15 16:10:50.584542.584542 lmp.py:1935]   Expert 24 |     87 | CPU
DEBUG 01-15 16:10:50.584993.584993 lmp.py:1935]   Expert 28 |    105 | CPU
DEBUG 01-15 16:10:50.584967.584967 lmp.py:1935]   Expert 34 |    115 | CPU
DEBUG 01-15 16:10:50.584180.584180 lmp.py:1935]   Expert 51 |    118 | CPU
DEBUG 01-15 16:10:50.584392.584392 lmp.py:1935]   Expert 32 |    120 | CPU
DEBUG 01-15 16:10:50.584605.584605 lmp.py:1935]   Expert  9 |    130 | CPU
DEBUG 01-15 16:10:50.584771.584771 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:10:50.584937.584937 lmp.py:1935]   Expert 15 |    135 | CPU
DEBUG 01-15 16:10:50.584865.584865 lmp.py:1935]   Expert 23 |    135 | CPU
DEBUG 01-15 16:10:50.584554.584554 lmp.py:1935]   Expert 26 |    138 | CPU
DEBUG 01-15 16:10:50.585766.585766 lmp.py:1935]   Expert 30 |    144 | CPU
DEBUG 01-15 16:10:50.585741.585741 lmp.py:1935]   Expert 45 |    146 | CPU
DEBUG 01-15 16:10:50.585715.585715 lmp.py:1935]   Expert 62 |    147 | CPU
DEBUG 01-15 16:10:50.585450.585450 lmp.py:1935]   Expert 57 |    151 | CPU
DEBUG 01-15 16:10:50.585901.585901 lmp.py:1935]   Expert  1 |    153 | CPU
DEBUG 01-15 16:10:50.585114.585114 lmp.py:1935]   Expert 36 |    155 | CPU
DEBUG 01-15 16:10:50.585849.585849 lmp.py:1935]   Expert  8 |    158 | CPU
DEBUG 01-15 16:10:50.585062.585062 lmp.py:1935]   Expert 29 |    160 | CPU
DEBUG 01-15 16:10:50.585274.585274 lmp.py:1935]   Expert 25 |    164 | CPU
DEBUG 01-15 16:10:50.585679.585679 lmp.py:1935]   Expert 54 |    167 | CPU
DEBUG 01-15 16:10:50.585606.585606 lmp.py:1935]   Expert  6 |    171 | CPU
DEBUG 01-15 16:10:50.585057.585057 lmp.py:1935]   Expert 49 |    171 | CPU
DEBUG 01-15 16:10:50.585747.585747 lmp.py:1935]   Expert 48 |    173 | CPU
DEBUG 01-15 16:10:50.585436.585436 lmp.py:1935]   Expert 37 |    175 | CPU
DEBUG 01-15 16:10:50.585410.585410 lmp.py:1935]   Expert 12 |    176 | CPU
DEBUG 01-15 16:10:50.585384.585384 lmp.py:1935]   Expert 35 |    176 | CPU
DEBUG 01-15 16:10:50.585358.585358 lmp.py:1935]   Expert 60 |    186 | CPU
DEBUG 01-15 16:10:50.585478.585478 lmp.py:1935]   Expert 13 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585074.585074 lmp.py:1935]   Expert 33 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585717.585717 lmp.py:1935]   Expert 53 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585122.585122 lmp.py:1935]   Expert 10 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585527.585527 lmp.py:1935]   Expert 16 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585123.585123 lmp.py:1935]   Expert 21 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585766.585766 lmp.py:1935]   Expert 40 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585409.585409 lmp.py:1935]   Expert 43 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585052.585052 lmp.py:1935]   Expert 38 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585695.585695 lmp.py:1935]   Expert  5 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585100.585100 lmp.py:1935]   Expert 44 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585789.585789 lmp.py:1935]   Expert 50 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585717.585717 lmp.py:1935]   Expert 52 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585406.585406 lmp.py:1935]   Expert 19 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585618.585618 lmp.py:1935]   Expert 41 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585308.585308 lmp.py:1935]   Expert  4 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585235.585235 lmp.py:1935]   Expert 59 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585925.585925 lmp.py:1935]   Expert 55 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585137.585137 lmp.py:1935]   Expert 56 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585065.585065 lmp.py:1935]   Expert 31 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585469.585469 lmp.py:1935]   Expert 39 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585874.585874 lmp.py:1935]   Expert 20 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585994.585994 lmp.py:1935]   Expert 22 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585637.585637 lmp.py:1935]   Expert  2 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585757.585757 lmp.py:1935]   Expert 47 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585638.585638 lmp.py:1935]   Expert 63 |    276 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585281.585281 lmp.py:1935]   Expert 42 |    303 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585686.585686 lmp.py:1935]   Expert 18 |    314 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585329.585329 lmp.py:1935]   Expert 14 |    317 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585210.585210 lmp.py:1935]   Expert 46 |    367 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585853.585853 lmp.py:1935]   Expert 11 |    389 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.585257.585257 lmp.py:1935]   Expert 61 |    462 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.585139.585139 lmp.py:1937] 
DEBUG 01-15 16:10:50.585139.585139 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:50.585736.585736 lmp.py:1938]   CPU:   4336 tokens
DEBUG 01-15 16:10:50.585094.585094 lmp.py:1942]   cuda:1:   3975 tokens (16 experts)
DEBUG 01-15 16:10:50.585167.585167 lmp.py:1942]   cuda:2:   3977 tokens (16 experts)
DEBUG 01-15 16:10:50.585333.585333 lmp.py:1943]   Total GPU:   7952 tokens
DEBUG 01-15 16:10:50.585784.585784 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:50.585784.585784 lmp.py:1944] 
DEBUG 01-15 16:10:50.585911.585911 cuda_h.py:19] end experts_map_get cost 0.0016665458679199219 seconds
DEBUG 01-15 16:10:50.586999.586999 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:50.586325.586325 lmp.py:1953] 
DEBUG 01-15 16:10:50.586325.586325 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:50.586730.586730 cuda_h.py:19] end cpu_experts_submit cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:50.586473.586473 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:50.586918.586918 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:50.587498.587498 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.2.mlp.experts.33.gate_proj.weight': 0, 'model.layers.2.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.50.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.50.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.50.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.2.mlp.experts.2.gate_proj.weight': 0, 'model.layers.2.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.2.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.2.mlp.experts.38.gate_proj.weight': 17301504, 'model.layers.2.mlp.experts.38.down_proj.weight': 23068672, 'model.layers.2.mlp.experts.38.up_proj.weight': 28835840, 'model.layers.2.mlp.experts.41.gate_proj.weight': 34603008, 'model.layers.2.mlp.experts.41.down_proj.weight': 40370176, 'model.layers.2.mlp.experts.41.up_proj.weight': 46137344, 'model.layers.2.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.2.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.2.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.2.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.2.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.2.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.2.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.2.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.2.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.2.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.2.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.2.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.2.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.2.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.2.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.2.mlp.experts.47.gate_proj.weight': 138412032, 'model.layers.2.mlp.experts.47.down_proj.weight': 144179200, 'model.layers.2.mlp.experts.47.up_proj.weight': 149946368, 'model.layers.2.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.2.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.2.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.2.mlp.experts.13.gate_proj.weight': 173015040, 'model.layers.2.mlp.experts.13.down_proj.weight': 178782208, 'model.layers.2.mlp.experts.13.up_proj.weight': 184549376, 'model.layers.2.mlp.experts.18.gate_proj.weight': 190316544, 'model.layers.2.mlp.experts.18.down_proj.weight': 196083712, 'model.layers.2.mlp.experts.18.up_proj.weight': 201850880, 'model.layers.2.mlp.experts.20.gate_proj.weight': 207618048, 'model.layers.2.mlp.experts.20.down_proj.weight': 213385216, 'model.layers.2.mlp.experts.20.up_proj.weight': 219152384, 'model.layers.2.mlp.experts.52.gate_proj.weight': 224919552, 'model.layers.2.mlp.experts.52.down_proj.weight': 230686720, 'model.layers.2.mlp.experts.52.up_proj.weight': 236453888, 'model.layers.2.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.2.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.2.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.2.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.2.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.2.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(4538761216, 5767168, 0, 0), (4544528384, 5767168, 5767168, 0), (4532994048, 5767168, 11534336, 0), (4037017600, 5767168, 17301504, 0), (4042784768, 5767168, 23068672, 0), (4031250432, 5767168, 28835840, 0), (4054319104, 5767168, 34603008, 0), (4060086272, 5767168, 40370176, 0), (4048551936, 5767168, 46137344, 0), (4642570240, 5767168, 51904512, 0), (4648337408, 5767168, 57671680, 0), (4636803072, 5767168, 63438848, 0), (4659871744, 5767168, 69206016, 0), (4665638912, 5767168, 74973184, 0), (4654104576, 5767168, 80740352, 0), (4694474752, 5767168, 86507520, 0), (4700241920, 5767168, 92274688, 0), (4688707584, 5767168, 98041856, 0), (4210032640, 5767168, 103809024, 0), (4215799808, 5767168, 109576192, 0), (4204265472, 5767168, 115343360, 0), (4832886784, 5767168, 121110528, 0), (4838653952, 5767168, 126877696, 0), (4827119616, 5767168, 132644864, 0), (4296540160, 5767168, 138412032, 0), (4302307328, 5767168, 144179200, 0), (4290772992, 5767168, 149946368, 0), (4331143168, 5767168, 155713536, 0), (4336910336, 5767168, 161480704, 0), (4325376000, 5767168, 167247872, 0), (4348444672, 5767168, 173015040, 0), (4354211840, 5767168, 178782208, 0), (4342677504, 5767168, 184549376, 0), (4884791296, 5767168, 190316544, 0), (4890558464, 5767168, 196083712, 0), (4879024128, 5767168, 201850880, 0), (4936695808, 5767168, 207618048, 0), (4942462976, 5767168, 213385216, 0), (4930928640, 5767168, 219152384, 0), (4988600320, 5767168, 224919552, 0), (4994367488, 5767168, 230686720, 0), (4982833152, 5767168, 236453888, 0), (5023203328, 5767168, 242221056, 0), (5028970496, 5767168, 247988224, 0), (5017436160, 5767168, 253755392, 0), (5057806336, 5767168, 259522560, 0), (5063573504, 5767168, 265289728, 0), (5052039168, 5767168, 271056896, 0)], 2: [(4002414592, 5767168, 0, 0), (4008181760, 5767168, 5767168, 0), (3996647424, 5767168, 11534336, 0), (4625268736, 5767168, 17301504, 0), (4631035904, 5767168, 23068672, 0), (4619501568, 5767168, 28835840, 0), (4677173248, 5767168, 34603008, 0), (4682940416, 5767168, 40370176, 0), (4671406080, 5767168, 46137344, 0), (4140826624, 5767168, 51904512, 0), (4146593792, 5767168, 57671680, 0), (4135059456, 5767168, 63438848, 0), (4158128128, 5767168, 69206016, 0), (4163895296, 5767168, 74973184, 0), (4152360960, 5767168, 80740352, 0), (4729077760, 5767168, 86507520, 0), (4734844928, 5767168, 92274688, 0), (4723310592, 5767168, 98041856, 0), (4711776256, 5767168, 103809024, 0), (4717543424, 5767168, 109576192, 0), (4706009088, 5767168, 115343360, 0), (4763680768, 5767168, 121110528, 0), (4769447936, 5767168, 126877696, 0), (4757913600, 5767168, 132644864, 0), (4780982272, 5767168, 138412032, 0), (4786749440, 5767168, 144179200, 0), (4775215104, 5767168, 149946368, 0), (4244635648, 5767168, 155713536, 0), (4250402816, 5767168, 161480704, 0), (4238868480, 5767168, 167247872, 0), (4192731136, 5767168, 173015040, 0), (4198498304, 5767168, 178782208, 0), (4186963968, 5767168, 184549376, 0), (4279238656, 5767168, 190316544, 0), (4285005824, 5767168, 196083712, 0), (4273471488, 5767168, 201850880, 0), (4313841664, 5767168, 207618048, 0), (4319608832, 5767168, 213385216, 0), (4308074496, 5767168, 219152384, 0), (4867489792, 5767168, 224919552, 0), (4873256960, 5767168, 230686720, 0), (4861722624, 5767168, 236453888, 0), (4919394304, 5767168, 242221056, 0), (4925161472, 5767168, 247988224, 0), (4913627136, 5767168, 253755392, 0), (4504158208, 5767168, 259522560, 0), (4509925376, 5767168, 265289728, 0), (4498391040, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec45c9d70>, 2: <capsule object NULL at 0x7a4ec4681c20>}
DEBUG 01-15 16:10:50.587224.587224 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.588384.588384 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.588041.588041 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0eb18e90-1f52-49e0-8348-6755251fdf49
DEBUG 01-15 16:10:50.588246.588246 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:50.588102.588102 client.py:127] Model loaded
DEBUG 01-15 16:10:50.588163.588163 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.589703.589703 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:50.589467.589467 cuda_h.py:19] end restore2model cost 0.0003330707550048828 seconds
DEBUG 01-15 16:10:50.589090.589090 cuda_h.py:19] end sllm_worker_task cost 0.010512351989746094 seconds
INFO 01-15 16:10:50.589794.589794 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0eb18e90-1f52-49e0-8348-6755251fdf49
DEBUG 01-15 16:10:50.589847.589847 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:50.589485.589485 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003652334213256836 seconds
DEBUG 01-15 16:10:50.589693.589693 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.590748.590748 cuda_h.py:19] end move_flatidxs cost 0.0009531974792480469 seconds
DEBUG 01-15 16:10:50.590240.590240 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:50.592495.592495 cuda_h.py:19] end restore2model cost 0.0025568008422851562 seconds
DEBUG 01-15 16:10:50.592246.592246 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0064465999603271484 seconds
DEBUG 01-15 16:10:50.592995.592995 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:50.592455.592455 cuda_h.py:19] end gpu_sexperts cost 0.00027298927307128906 seconds
DEBUG 01-15 16:10:50.592808.592808 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:50.593730.593730 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:10:50.593618.593618 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:50.593513.593513 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.593555.593555 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008087158203125 seconds
DEBUG 01-15 16:10:50.593252.593252 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.594146.594146 cuda_h.py:19] end gpu_group_list cost 0.00017499923706054688 seconds
DEBUG 01-15 16:10:50.594124.594124 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.595207.595207 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.00084686279296875 seconds
DEBUG 01-15 16:10:50.595680.595680 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.596673.596673 cuda_h.py:19] end gpu_group_list cost 0.00017547607421875 seconds
DEBUG 01-15 16:10:50.596541.596541 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:50.596040.596040 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0eb18e90-1f52-49e0-8348-6755251fdf49
DEBUG 01-15 16:10:50.596158.596158 cuda_h.py:19] end group_tensors cost 0.005626201629638672 seconds
DEBUG 01-15 16:10:50.596962.596962 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:50.601613.601613 cuda_h.py:19] end group pad cost 0.004150390625 seconds
DEBUG 01-15 16:10:50.601833.601833 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:50.616053.616053 client.py:127] Model loaded
DEBUG 01-15 16:10:50.617046.617046 cuda_h.py:19] end wait_experts_multi_device cost 0.020656585693359375 seconds
DEBUG 01-15 16:10:50.617357.617357 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:50.622779.622779 cuda_h.py:19] end group_einsum cost 0.021049022674560547 seconds
DEBUG 01-15 16:10:50.622268.622268 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:50.627999.627999 cuda_h.py:19] end get_outputs_cpu1 cost 0.0053253173828125 seconds
DEBUG 01-15 16:10:50.628803.628803 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039504289627075195 seconds
DEBUG 01-15 16:10:50.629863.629863 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01127934455871582 seconds
DEBUG 01-15 16:10:50.629668.629668 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:50.630423.630423 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.630157.630157 cuda_h.py:19] end index_scatter cost 7.319450378417969e-05 seconds
DEBUG 01-15 16:10:50.630042.630042 cuda_h.py:19] end cpuoutputsdeal cost 0.0016400814056396484 seconds
DEBUG 01-15 16:10:50.630767.630767 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:50.630814.630814 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.631422.631422 cuda_h.py:19] end gpu_group_tensor cost 0.0001380443572998047 seconds
DEBUG 01-15 16:10:50.631516.631516 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.631296.631296 cuda_h.py:19] end gpu_group_tensor cost 0.00012445449829101562 seconds
DEBUG 01-15 16:10:50.631624.631624 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.632986.632986 cuda_h.py:19] end gpu_group_einsum cost 0.0010607242584228516 seconds
DEBUG 01-15 16:10:50.632130.632130 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.633906.633906 cuda_h.py:19] end gpu_group_einsum cost 0.00037980079650878906 seconds
DEBUG 01-15 16:10:50.633916.633916 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.633714.633714 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.633833.633833 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015735626220703125 seconds
DEBUG 01-15 16:10:50.633351.633351 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.633360.633360 cuda_h.py:19] end concat_expert_out cost 4.6253204345703125e-05 seconds
DEBUG 01-15 16:10:50.633342.633342 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.633504.633504 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:50.633531.633531 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007250308990478516 seconds
DEBUG 01-15 16:10:50.634369.634369 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.634205.634205 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.634522.634522 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013113021850585938 seconds
DEBUG 01-15 16:10:50.634609.634609 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.634771.634771 cuda_h.py:19] end concat_expert_out cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:10:50.634037.634037 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.634292.634292 cuda_h.py:19] end index_scatter cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:50.634194.634194 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004677772521972656 seconds
DEBUG 01-15 16:10:50.634951.634951 cuda_h.py:19] end gpu_experts_multi_device cost 0.04154849052429199 seconds
DEBUG 01-15 16:10:50.634053.634053 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_3 cost 0.05118370056152344 seconds
DEBUG 01-15 16:10:50.635210.635210 cuda_h.py:19] end prefill_layer cost 0.05672287940979004 seconds
DEBUG 01-15 16:10:50.635246.635246 lmp.py:1553] -------------------------------- end prefill layer 2 --------------------------------
DEBUG 01-15 16:10:50.635141.635141 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.635320.635320 lmp.py:1495] -------------------------------- start prefill layer 3 --------------------------------
DEBUG 01-15 16:10:50.635454.635454 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:50.635448.635448 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 01-15 16:10:50.635643.635643 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 4.458427429199219e-05 seconds
DEBUG 01-15 16:10:50.635683.635683 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:10:50.635479.635479 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.635395.635395 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.635133.635133 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.635303.635303 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.635233.635233 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.636191.636191 cuda_h.py:19] end allocate_cuda_memory cost 0.0002646446228027344 seconds
DEBUG 01-15 16:10:50.636493.636493 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.636727.636727 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.636001.636001 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.636863.636863 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 33b9a565-78ea-46e9-944c-5a5106e605ba
DEBUG 01-15 16:10:50.636047.636047 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.636691.636691 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.638150.638150 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 33b9a565-78ea-46e9-944c-5a5106e605ba
DEBUG 01-15 16:10:50.638883.638883 cuda_h.py:19] end load_into_gpu_async cost 0.0020742416381835938 seconds
DEBUG 01-15 16:10:50.638918.638918 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.638329.638329 cuda_h.py:19] end restore_tensors2 cost 0.00012254714965820312 seconds
DEBUG 01-15 16:10:50.638682.638682 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002948760986328125 seconds
INFO 01-15 16:10:50.638746.638746 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 33b9a565-78ea-46e9-944c-5a5106e605ba
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.640459.640459 cuda_h.py:19] end self_attn cost 0.003481149673461914 seconds
DEBUG 01-15 16:10:50.640258.640258 cuda_h.py:19] end iln_self_attn_paln cost 0.005246162414550781 seconds
DEBUG 01-15 16:10:50.640995.640995 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_4
DEBUG 01-15 16:10:50.640235.640235 cuda_h.py:10] start gate
DEBUG 01-15 16:10:50.641861.641861 cuda_h.py:19] end gate cost 0.0006701946258544922 seconds
DEBUG 01-15 16:10:50.641598.641598 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:50.641602.641602 lmp.py:1912] 
DEBUG 01-15 16:10:50.641602.641602 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:50.641550.641550 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:50.641154.641154 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:50.641658.641658 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:50.641254.641254 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:50.641897.641897 lmp.py:1917] 
DEBUG 01-15 16:10:50.641897.641897 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:50.641494.641494 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:50.641859.641859 lmp.py:1935]   Expert  1 |     50 | CPU
DEBUG 01-15 16:10:50.641979.641979 lmp.py:1935]   Expert 27 |     62 | CPU
DEBUG 01-15 16:10:50.641145.641145 lmp.py:1935]   Expert  7 |     74 | CPU
DEBUG 01-15 16:10:50.642834.642834 lmp.py:1935]   Expert 48 |     81 | CPU
DEBUG 01-15 16:10:50.642000.642000 lmp.py:1935]   Expert 15 |     98 | CPU
DEBUG 01-15 16:10:50.642928.642928 lmp.py:1935]   Expert 30 |    111 | CPU
DEBUG 01-15 16:10:50.642379.642379 lmp.py:1935]   Expert 61 |    116 | CPU
DEBUG 01-15 16:10:50.642545.642545 lmp.py:1935]   Expert 18 |    118 | CPU
DEBUG 01-15 16:10:50.642473.642473 lmp.py:1935]   Expert 32 |    118 | CPU
DEBUG 01-15 16:10:50.642877.642877 lmp.py:1935]   Expert 45 |    119 | CPU
DEBUG 01-15 16:10:50.642520.642520 lmp.py:1935]   Expert 34 |    133 | CPU
DEBUG 01-15 16:10:50.642686.642686 lmp.py:1935]   Expert 39 |    133 | CPU
DEBUG 01-15 16:10:50.642376.642376 lmp.py:1935]   Expert 26 |    137 | CPU
DEBUG 01-15 16:10:50.642588.642588 lmp.py:1935]   Expert 36 |    138 | CPU
DEBUG 01-15 16:10:50.642039.642039 lmp.py:1935]   Expert 11 |    140 | CPU
DEBUG 01-15 16:10:50.642444.642444 lmp.py:1935]   Expert  6 |    142 | CPU
DEBUG 01-15 16:10:50.642644.642644 lmp.py:1935]   Expert  5 |    143 | CPU
DEBUG 01-15 16:10:50.642763.642763 lmp.py:1935]   Expert 59 |    143 | CPU
DEBUG 01-15 16:10:50.642466.642466 lmp.py:1935]   Expert 51 |    145 | CPU
DEBUG 01-15 16:10:50.642440.642440 lmp.py:1935]   Expert 49 |    155 | CPU
DEBUG 01-15 16:10:50.642176.642176 lmp.py:1935]   Expert 23 |    156 | CPU
DEBUG 01-15 16:10:50.642912.642912 lmp.py:1935]   Expert  2 |    157 | CPU
DEBUG 01-15 16:10:50.642409.642409 lmp.py:1935]   Expert  9 |    157 | CPU
DEBUG 01-15 16:10:50.642383.642383 lmp.py:1935]   Expert 50 |    165 | CPU
DEBUG 01-15 16:10:50.642357.642357 lmp.py:1935]   Expert 56 |    168 | CPU
DEBUG 01-15 16:10:50.642285.642285 lmp.py:1935]   Expert 40 |    169 | CPU
DEBUG 01-15 16:10:50.642212.642212 lmp.py:1935]   Expert 52 |    169 | CPU
DEBUG 01-15 16:10:50.642663.642663 lmp.py:1935]   Expert 35 |    172 | CPU
DEBUG 01-15 16:10:50.642876.642876 lmp.py:1935]   Expert 16 |    174 | CPU
DEBUG 01-15 16:10:50.642373.642373 lmp.py:1935]   Expert  4 |    187 | CPU
DEBUG 01-15 16:10:50.642109.642109 lmp.py:1935]   Expert 37 |    190 | CPU
DEBUG 01-15 16:10:50.642844.642844 lmp.py:1935]   Expert 42 |    190 | CPU
DEBUG 01-15 16:10:50.642487.642487 lmp.py:1935]   Expert 13 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642130.642130 lmp.py:1935]   Expert 17 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642296.642296 lmp.py:1935]   Expert 38 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642462.642462 lmp.py:1935]   Expert 62 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642152.642152 lmp.py:1935]   Expert 21 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642079.642079 lmp.py:1935]   Expert  3 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642583.642583 lmp.py:1935]   Expert 44 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642703.642703 lmp.py:1935]   Expert 28 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642346.642346 lmp.py:1935]   Expert 58 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642989.642989 lmp.py:1935]   Expert 60 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642155.642155 lmp.py:1935]   Expert 47 |    214 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642083.642083 lmp.py:1935]   Expert 10 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642249.642249 lmp.py:1935]   Expert 53 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642938.642938 lmp.py:1935]   Expert 55 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642866.642866 lmp.py:1935]   Expert 20 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642555.642555 lmp.py:1935]   Expert 57 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642006.642006 lmp.py:1935]   Expert 33 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642934.642934 lmp.py:1935]   Expert 31 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642623.642623 lmp.py:1935]   Expert 46 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642551.642551 lmp.py:1935]   Expert  8 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642717.642717 lmp.py:1935]   Expert 19 |    243 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642599.642599 lmp.py:1935]   Expert 24 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642003.642003 lmp.py:1935]   Expert 14 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642123.642123 lmp.py:1935]   Expert 63 |    267 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642528.642528 lmp.py:1935]   Expert 29 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.642455.642455 lmp.py:1935]   Expert 12 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642383.642383 lmp.py:1935]   Expert 22 |    278 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.642549.642549 lmp.py:1935]   Expert  0 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.643477.643477 lmp.py:1935]   Expert 43 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.643928.643928 lmp.py:1935]   Expert 54 |    339 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.643094.643094 lmp.py:1935]   Expert 41 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.643783.643783 lmp.py:1935]   Expert 25 |    410 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.643757.643757 lmp.py:1937] 
DEBUG 01-15 16:10:50.643757.643757 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:50.643685.643685 lmp.py:1938]   CPU:   4410 tokens
DEBUG 01-15 16:10:50.643613.643613 lmp.py:1942]   cuda:1:   3942 tokens (16 experts)
DEBUG 01-15 16:10:50.643302.643302 lmp.py:1942]   cuda:2:   3936 tokens (16 experts)
DEBUG 01-15 16:10:50.643276.643276 lmp.py:1943]   Total GPU:   7878 tokens
DEBUG 01-15 16:10:50.643012.643012 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:50.643012.643012 lmp.py:1944] 
DEBUG 01-15 16:10:50.643185.643185 cuda_h.py:19] end experts_map_get cost 0.0016684532165527344 seconds
DEBUG 01-15 16:10:50.643220.643220 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:50.643353.643353 lmp.py:1953] 
DEBUG 01-15 16:10:50.643353.643353 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:50.643560.643560 cuda_h.py:19] end cpu_experts_submit cost 4.649162292480469e-05 seconds
DEBUG 01-15 16:10:50.643369.643369 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:50.643291.643291 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:50.645611.645611 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.3.mlp.experts.0.gate_proj.weight': 0, 'model.layers.3.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.43.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.43.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.43.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.28.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.28.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.28.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.55.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.55.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.55.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.3.mlp.experts.3.gate_proj.weight': 0, 'model.layers.3.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.3.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.3.mlp.experts.8.gate_proj.weight': 17301504, 'model.layers.3.mlp.experts.8.down_proj.weight': 23068672, 'model.layers.3.mlp.experts.8.up_proj.weight': 28835840, 'model.layers.3.mlp.experts.41.gate_proj.weight': 34603008, 'model.layers.3.mlp.experts.41.down_proj.weight': 40370176, 'model.layers.3.mlp.experts.41.up_proj.weight': 46137344, 'model.layers.3.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.3.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.3.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.3.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.3.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.3.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.3.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.3.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.3.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.3.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.3.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.3.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.3.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.3.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.3.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.3.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.3.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.3.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.3.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.3.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.3.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.3.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.3.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.3.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.3.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.3.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.3.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.3.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.3.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.3.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.3.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.3.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.3.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.3.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.3.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.3.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.3.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.3.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.3.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(5075107840, 5767168, 0, 0), (5080875008, 5767168, 5767168, 0), (5069340672, 5767168, 11534336, 0), (5646057472, 5767168, 17301504, 0), (5651824640, 5767168, 23068672, 0), (5640290304, 5767168, 28835840, 0), (5732564992, 5767168, 34603008, 0), (5738332160, 5767168, 40370176, 0), (5726797824, 5767168, 46137344, 0), (5819072512, 5767168, 51904512, 0), (5824839680, 5767168, 57671680, 0), (5813305344, 5767168, 63438848, 0), (5559549952, 5767168, 69206016, 0), (5565317120, 5767168, 74973184, 0), (5553782784, 5767168, 80740352, 0), (5317328896, 5767168, 86507520, 0), (5323096064, 5767168, 92274688, 0), (5311561728, 5767168, 98041856, 0), (5888278528, 5767168, 103809024, 0), (5894045696, 5767168, 109576192, 0), (5882511360, 5767168, 115343360, 0), (5403836416, 5767168, 121110528, 0), (5409603584, 5767168, 126877696, 0), (5398069248, 5767168, 132644864, 0), (5438439424, 5767168, 138412032, 0), (5444206592, 5767168, 144179200, 0), (5432672256, 5767168, 149946368, 0), (6026690560, 5767168, 155713536, 0), (6032457728, 5767168, 161480704, 0), (6020923392, 5767168, 167247872, 0), (5507645440, 5767168, 173015040, 0), (5513412608, 5767168, 178782208, 0), (5501878272, 5767168, 184549376, 0), (6078595072, 5767168, 190316544, 0), (6084362240, 5767168, 196083712, 0), (6072827904, 5767168, 201850880, 0), (6061293568, 5767168, 207618048, 0), (6067060736, 5767168, 213385216, 0), (6055526400, 5767168, 219152384, 0), (5576851456, 5767168, 224919552, 0), (5582618624, 5767168, 230686720, 0), (5571084288, 5767168, 236453888, 0), (6147801088, 5767168, 242221056, 0), (6153568256, 5767168, 247988224, 0), (6142033920, 5767168, 253755392, 0), (5611454464, 5767168, 259522560, 0), (5617221632, 5767168, 265289728, 0), (5605687296, 5767168, 271056896, 0)], 2: [(5127012352, 5767168, 0, 0), (5132779520, 5767168, 5767168, 0), (5121245184, 5767168, 11534336, 0), (5213519872, 5767168, 17301504, 0), (5219287040, 5767168, 23068672, 0), (5207752704, 5767168, 28835840, 0), (5784469504, 5767168, 34603008, 0), (5790236672, 5767168, 40370176, 0), (5778702336, 5767168, 46137344, 0), (5248122880, 5767168, 51904512, 0), (5253890048, 5767168, 57671680, 0), (5242355712, 5767168, 63438848, 0), (5282725888, 5767168, 69206016, 0), (5288493056, 5767168, 74973184, 0), (5276958720, 5767168, 80740352, 0), (5836374016, 5767168, 86507520, 0), (5842141184, 5767168, 92274688, 0), (5830606848, 5767168, 98041856, 0), (5870977024, 5767168, 103809024, 0), (5876744192, 5767168, 109576192, 0), (5865209856, 5767168, 115343360, 0), (5300027392, 5767168, 121110528, 0), (5305794560, 5767168, 126877696, 0), (5294260224, 5767168, 132644864, 0), (5369233408, 5767168, 138412032, 0), (5375000576, 5767168, 144179200, 0), (5363466240, 5767168, 149946368, 0), (5421137920, 5767168, 155713536, 0), (5426905088, 5767168, 161480704, 0), (5415370752, 5767168, 167247872, 0), (5992087552, 5767168, 173015040, 0), (5997854720, 5767168, 178782208, 0), (5986320384, 5767168, 184549376, 0), (6009389056, 5767168, 190316544, 0), (6015156224, 5767168, 196083712, 0), (6003621888, 5767168, 201850880, 0), (5455740928, 5767168, 207618048, 0), (5461508096, 5767168, 213385216, 0), (5449973760, 5767168, 219152384, 0), (5490343936, 5767168, 224919552, 0), (5496111104, 5767168, 230686720, 0), (5484576768, 5767168, 236453888, 0), (6113198080, 5767168, 242221056, 0), (6118965248, 5767168, 247988224, 0), (6107430912, 5767168, 253755392, 0), (6165102592, 5767168, 259522560, 0), (6170869760, 5767168, 265289728, 0), (6159335424, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4311e90>, 2: <capsule object NULL at 0x7a4e547aeac0>}
DEBUG 01-15 16:10:50.645000.645000 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.645784.645784 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.645202.645202 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 63c762ff-2de4-49e8-9c12-5ca8872fa50e
DEBUG 01-15 16:10:50.645844.645844 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.646359.646359 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:50.646951.646951 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:50.646671.646671 client.py:127] Model loaded
DEBUG 01-15 16:10:50.646555.646555 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.647902.647902 cuda_h.py:19] end move_flatidxs cost 0.0008490085601806641 seconds
DEBUG 01-15 16:10:50.647977.647977 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:50.648342.648342 cuda_h.py:19] end restore2model cost 0.0010406970977783203 seconds
INFO 01-15 16:10:50.648631.648631 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 63c762ff-2de4-49e8-9c12-5ca8872fa50e
DEBUG 01-15 16:10:50.648820.648820 cuda_h.py:19] end sllm_worker_task cost 0.012659788131713867 seconds
DEBUG 01-15 16:10:50.648574.648574 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0054395198822021484 seconds
DEBUG 01-15 16:10:50.649421.649421 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.651268.651268 cuda_h.py:19] end restore2model cost 0.0025205612182617188 seconds
DEBUG 01-15 16:10:50.651872.651872 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008284568786621094 seconds
DEBUG 01-15 16:10:50.651165.651165 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:50.652341.652341 cuda_h.py:19] end gpu_sexperts cost 0.0002694129943847656 seconds
DEBUG 01-15 16:10:50.652932.652932 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:50.652708.652708 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7404556274414062e-05 seconds
DEBUG 01-15 16:10:50.652643.652643 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:50.652584.652584 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.652063.652063 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008139610290527344 seconds
DEBUG 01-15 16:10:50.653336.653336 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.653522.653522 cuda_h.py:19] end gpu_group_list cost 0.000179290771484375 seconds
DEBUG 01-15 16:10:50.653090.653090 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.653076.653076 cuda_h.py:19] end group_tensors cost 0.0061037540435791016 seconds
DEBUG 01-15 16:10:50.654983.654983 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:50.655586.655586 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0017082691192626953 seconds
DEBUG 01-15 16:10:50.655483.655483 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.656004.656004 cuda_h.py:19] end gpu_group_list cost 0.0002789497375488281 seconds
DEBUG 01-15 16:10:50.657583.657583 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:50.657858.657858 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 63c762ff-2de4-49e8-9c12-5ca8872fa50e
DEBUG 01-15 16:10:50.659247.659247 cuda_h.py:19] end group pad cost 0.005051136016845703 seconds
DEBUG 01-15 16:10:50.659752.659752 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:50.673786.673786 client.py:127] Model loaded
DEBUG 01-15 16:10:50.674513.674513 cuda_h.py:19] end wait_experts_multi_device cost 0.016986846923828125 seconds
DEBUG 01-15 16:10:50.674309.674309 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:50.683024.683024 cuda_h.py:19] end group_einsum cost 0.02345895767211914 seconds
DEBUG 01-15 16:10:50.683434.683434 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:50.687791.687791 cuda_h.py:19] end get_outputs_cpu1 cost 0.00446319580078125 seconds
DEBUG 01-15 16:10:50.688514.688514 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04235553741455078 seconds
DEBUG 01-15 16:10:50.688439.688439 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.014673233032226562 seconds
DEBUG 01-15 16:10:50.689674.689674 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:50.690396.690396 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.690157.690157 cuda_h.py:19] end index_scatter cost 0.00010395050048828125 seconds
DEBUG 01-15 16:10:50.690287.690287 cuda_h.py:19] end cpuoutputsdeal cost 0.0016405582427978516 seconds
DEBUG 01-15 16:10:50.690965.690965 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:50.690774.690774 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.691958.691958 cuda_h.py:19] end gpu_group_tensor cost 0.00013899803161621094 seconds
DEBUG 01-15 16:10:50.691529.691529 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.691237.691237 cuda_h.py:19] end gpu_group_tensor cost 0.00014138221740722656 seconds
DEBUG 01-15 16:10:50.691041.691041 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.691847.691847 cuda_h.py:19] end gpu_group_einsum cost 0.000476837158203125 seconds
DEBUG 01-15 16:10:50.691871.691871 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.692223.692223 cuda_h.py:19] end gpu_group_einsum cost 0.00036716461181640625 seconds
DEBUG 01-15 16:10:50.692803.692803 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.692648.692648 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.692151.692151 cuda_h.py:19] end all_expert_outputs_slices cost 0.00015878677368164062 seconds
DEBUG 01-15 16:10:50.692384.692384 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.692015.692015 cuda_h.py:19] end concat_expert_out cost 4.57763671875e-05 seconds
DEBUG 01-15 16:10:50.692382.692382 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.693543.693543 cuda_h.py:19] end index_scatter cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:50.693154.693154 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007388591766357422 seconds
DEBUG 01-15 16:10:50.693614.693614 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.693311.693311 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.693813.693813 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001289844512939453 seconds
DEBUG 01-15 16:10:50.693470.693470 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.693532.693532 cuda_h.py:19] end concat_expert_out cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:50.693945.693945 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.693915.693915 cuda_h.py:19] end index_scatter cost 4.982948303222656e-05 seconds
DEBUG 01-15 16:10:50.693008.693008 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004665851593017578 seconds
DEBUG 01-15 16:10:50.693819.693819 cuda_h.py:19] end gpu_experts_multi_device cost 0.04181957244873047 seconds
DEBUG 01-15 16:10:50.694252.694252 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_4 cost 0.053316354751586914 seconds
DEBUG 01-15 16:10:50.694814.694814 cuda_h.py:19] end prefill_layer cost 0.059243202209472656 seconds
DEBUG 01-15 16:10:50.694803.694803 lmp.py:1553] -------------------------------- end prefill layer 3 --------------------------------
DEBUG 01-15 16:10:50.694506.694506 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.694924.694924 lmp.py:1495] -------------------------------- start prefill layer 4 --------------------------------
DEBUG 01-15 16:10:50.694580.694580 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:50.694052.694052 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 01-15 16:10:50.694570.694570 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:10:50.694326.694326 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 7.104873657226562e-05 seconds
DEBUG 01-15 16:10:50.694930.694930 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.694926.694926 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.694592.694592 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.694051.694051 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.695917.695917 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.695299.695299 cuda_h.py:19] end allocate_cuda_memory cost 0.0002570152282714844 seconds
DEBUG 01-15 16:10:50.695733.695733 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.695775.695775 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.695333.695333 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.695242.695242 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ee14a4ac-23d2-4e95-98a6-7c301a56c378
DEBUG 01-15 16:10:50.695994.695994 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.696560.696560 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.697763.697763 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ee14a4ac-23d2-4e95-98a6-7c301a56c378
DEBUG 01-15 16:10:50.697276.697276 cuda_h.py:19] end load_into_gpu_async cost 0.0015306472778320312 seconds
DEBUG 01-15 16:10:50.697324.697324 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.697672.697672 cuda_h.py:19] end restore_tensors2 cost 8.273124694824219e-05 seconds
DEBUG 01-15 16:10:50.697388.697388 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002235889434814453 seconds
INFO 01-15 16:10:50.697053.697053 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ee14a4ac-23d2-4e95-98a6-7c301a56c378
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.699349.699349 cuda_h.py:19] end self_attn cost 0.0035371780395507812 seconds
DEBUG 01-15 16:10:50.700268.700268 cuda_h.py:19] end iln_self_attn_paln cost 0.00530695915222168 seconds
DEBUG 01-15 16:10:50.700051.700051 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_5
DEBUG 01-15 16:10:50.700337.700337 cuda_h.py:10] start gate
DEBUG 01-15 16:10:50.700053.700053 cuda_h.py:19] end gate cost 0.0007710456848144531 seconds
DEBUG 01-15 16:10:50.701081.701081 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:50.701543.701543 lmp.py:1912] 
DEBUG 01-15 16:10:50.701543.701543 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:50.701444.701444 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:50.701286.701286 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:50.701837.701837 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:50.701672.701672 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:50.701315.701315 lmp.py:1917] 
DEBUG 01-15 16:10:50.701315.701315 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:50.701911.701911 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:50.701276.701276 lmp.py:1935]   Expert 14 |     63 | CPU
DEBUG 01-15 16:10:50.701681.701681 lmp.py:1935]   Expert 57 |     72 | CPU
DEBUG 01-15 16:10:50.701370.701370 lmp.py:1935]   Expert 13 |     76 | CPU
DEBUG 01-15 16:10:50.701821.701821 lmp.py:1935]   Expert 26 |     81 | CPU
DEBUG 01-15 16:10:50.701510.701510 lmp.py:1935]   Expert 31 |     90 | CPU
DEBUG 01-15 16:10:50.701200.701200 lmp.py:1935]   Expert 54 |     92 | CPU
DEBUG 01-15 16:10:50.701412.701412 lmp.py:1935]   Expert 11 |     94 | CPU
DEBUG 01-15 16:10:50.701625.701625 lmp.py:1935]   Expert 45 |     94 | CPU
DEBUG 01-15 16:10:50.701268.701268 lmp.py:1935]   Expert 58 |    103 | CPU
DEBUG 01-15 16:10:50.701672.701672 lmp.py:1935]   Expert 30 |    106 | CPU
DEBUG 01-15 16:10:50.701077.701077 lmp.py:1935]   Expert 51 |    108 | CPU
DEBUG 01-15 16:10:50.701481.701481 lmp.py:1935]   Expert 36 |    111 | CPU
DEBUG 01-15 16:10:50.701409.701409 lmp.py:1935]   Expert 10 |    114 | CPU
DEBUG 01-15 16:10:50.701860.701860 lmp.py:1935]   Expert 32 |    115 | CPU
DEBUG 01-15 16:10:50.701072.701072 lmp.py:1935]   Expert 20 |    127 | CPU
DEBUG 01-15 16:10:50.701000.701000 lmp.py:1935]   Expert  8 |    137 | CPU
DEBUG 01-15 16:10:50.701689.701689 lmp.py:1935]   Expert  4 |    138 | CPU
DEBUG 01-15 16:10:50.701140.701140 lmp.py:1935]   Expert 63 |    138 | CPU
DEBUG 01-15 16:10:50.701260.701260 lmp.py:1935]   Expert 53 |    141 | CPU
DEBUG 01-15 16:10:50.701665.701665 lmp.py:1935]   Expert 34 |    144 | CPU
DEBUG 01-15 16:10:50.701831.701831 lmp.py:1935]   Expert 61 |    144 | CPU
DEBUG 01-15 16:10:50.701679.701679 lmp.py:1935]   Expert 16 |    147 | CPU
DEBUG 01-15 16:10:50.701038.701038 lmp.py:1935]   Expert 47 |    149 | CPU
DEBUG 01-15 16:10:50.701442.701442 lmp.py:1935]   Expert 28 |    159 | CPU
DEBUG 01-15 16:10:50.701608.701608 lmp.py:1935]   Expert 42 |    159 | CPU
DEBUG 01-15 16:10:50.701298.701298 lmp.py:1935]   Expert 60 |    159 | CPU
DEBUG 01-15 16:10:50.701510.701510 lmp.py:1935]   Expert 17 |    161 | CPU
DEBUG 01-15 16:10:50.701723.701723 lmp.py:1935]   Expert 29 |    171 | CPU
DEBUG 01-15 16:10:50.701935.701935 lmp.py:1935]   Expert 44 |    171 | CPU
DEBUG 01-15 16:10:50.702671.702671 lmp.py:1935]   Expert 27 |    175 | CPU
DEBUG 01-15 16:10:50.702645.702645 lmp.py:1935]   Expert  7 |    176 | CPU
DEBUG 01-15 16:10:50.702619.702619 lmp.py:1935]   Expert 41 |    179 | CPU
DEBUG 01-15 16:10:50.702738.702738 lmp.py:1935]   Expert 48 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702335.702335 lmp.py:1935]   Expert  9 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702932.702932 lmp.py:1935]   Expert 56 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702767.702767 lmp.py:1935]   Expert  3 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702887.702887 lmp.py:1935]   Expert  2 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702768.702768 lmp.py:1935]   Expert 15 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702173.702173 lmp.py:1935]   Expert 24 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702816.702816 lmp.py:1935]   Expert  0 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702982.702982 lmp.py:1935]   Expert 18 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702148.702148 lmp.py:1935]   Expert 55 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702314.702314 lmp.py:1935]   Expert 40 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702480.702480 lmp.py:1935]   Expert 38 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702408.702408 lmp.py:1935]   Expert 22 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702336.702336 lmp.py:1935]   Expert 23 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702853.702853 lmp.py:1935]   Expert 37 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702496.702496 lmp.py:1935]   Expert  6 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702901.702901 lmp.py:1935]   Expert 46 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702305.702305 lmp.py:1935]   Expert 19 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702187.702187 lmp.py:1935]   Expert 39 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702591.702591 lmp.py:1935]   Expert 25 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702281.702281 lmp.py:1935]   Expert 12 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702731.702731 lmp.py:1935]   Expert 50 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702421.702421 lmp.py:1935]   Expert 62 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702872.702872 lmp.py:1935]   Expert 21 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702322.702322 lmp.py:1935]   Expert 35 |    284 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702535.702535 lmp.py:1935]   Expert 49 |    289 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702463.702463 lmp.py:1935]   Expert 33 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702914.702914 lmp.py:1935]   Expert 52 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702841.702841 lmp.py:1935]   Expert  1 |    347 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702246.702246 lmp.py:1935]   Expert  5 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702650.702650 lmp.py:1935]   Expert 43 |    438 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.702816.702816 lmp.py:1935]   Expert 59 |    585 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.702744.702744 lmp.py:1937] 
DEBUG 01-15 16:10:50.702744.702744 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:50.702910.702910 lmp.py:1938]   CPU:   4094 tokens
DEBUG 01-15 16:10:50.702838.702838 lmp.py:1942]   cuda:1:   4130 tokens (16 experts)
DEBUG 01-15 16:10:50.702289.702289 lmp.py:1942]   cuda:2:   4064 tokens (16 experts)
DEBUG 01-15 16:10:50.702263.702263 lmp.py:1943]   Total GPU:   8194 tokens
DEBUG 01-15 16:10:50.702522.702522 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:50.702522.702522 lmp.py:1944] 
DEBUG 01-15 16:10:50.702979.702979 cuda_h.py:19] end experts_map_get cost 0.0016849040985107422 seconds
DEBUG 01-15 16:10:50.702160.702160 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:50.702009.702009 lmp.py:1953] 
DEBUG 01-15 16:10:50.702009.702009 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:50.702746.702746 cuda_h.py:19] end cpu_experts_submit cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:50.702680.702680 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:50.702901.702901 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:50.703940.703940 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.4.mlp.experts.1.gate_proj.weight': 0, 'model.layers.4.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.40.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.40.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.40.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.23.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.23.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.23.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.56.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.56.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.56.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.4.mlp.experts.0.gate_proj.weight': 0, 'model.layers.4.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.4.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.4.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.4.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.4.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.4.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.4.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.4.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.4.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.4.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.4.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.4.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.4.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.4.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.4.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.4.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.4.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.4.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.4.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.4.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.4.mlp.experts.43.gate_proj.weight': 121110528, 'model.layers.4.mlp.experts.43.down_proj.weight': 126877696, 'model.layers.4.mlp.experts.43.up_proj.weight': 132644864, 'model.layers.4.mlp.experts.12.gate_proj.weight': 138412032, 'model.layers.4.mlp.experts.12.down_proj.weight': 144179200, 'model.layers.4.mlp.experts.12.up_proj.weight': 149946368, 'model.layers.4.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.4.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.4.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.4.mlp.experts.49.gate_proj.weight': 173015040, 'model.layers.4.mlp.experts.49.down_proj.weight': 178782208, 'model.layers.4.mlp.experts.49.up_proj.weight': 184549376, 'model.layers.4.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.4.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.4.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.4.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.4.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.4.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.4.mlp.experts.22.gate_proj.weight': 224919552, 'model.layers.4.mlp.experts.22.down_proj.weight': 230686720, 'model.layers.4.mlp.experts.22.up_proj.weight': 236453888, 'model.layers.4.mlp.experts.55.gate_proj.weight': 242221056, 'model.layers.4.mlp.experts.55.down_proj.weight': 247988224, 'model.layers.4.mlp.experts.55.up_proj.weight': 253755392, 'model.layers.4.mlp.experts.25.gate_proj.weight': 259522560, 'model.layers.4.mlp.experts.25.down_proj.weight': 265289728, 'model.layers.4.mlp.experts.25.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(6199705600, 5767168, 0, 0), (6205472768, 5767168, 5767168, 0), (6193938432, 5767168, 11534336, 0), (6217007104, 5767168, 17301504, 0), (6222774272, 5767168, 23068672, 0), (6211239936, 5767168, 28835840, 0), (6787956736, 5767168, 34603008, 0), (6793723904, 5767168, 40370176, 0), (6782189568, 5767168, 46137344, 0), (6822559744, 5767168, 51904512, 0), (6828326912, 5767168, 57671680, 0), (6816792576, 5767168, 63438848, 0), (6857162752, 5767168, 69206016, 0), (6862929920, 5767168, 74973184, 0), (6851395584, 5767168, 80740352, 0), (6874464256, 5767168, 86507520, 0), (6880231424, 5767168, 92274688, 0), (6868697088, 5767168, 98041856, 0), (6978273280, 5767168, 103809024, 0), (6984040448, 5767168, 109576192, 0), (6972506112, 5767168, 115343360, 0), (7012876288, 5767168, 121110528, 0), (7018643456, 5767168, 126877696, 0), (7007109120, 5767168, 132644864, 0), (7047479296, 5767168, 138412032, 0), (7053246464, 5767168, 144179200, 0), (7041712128, 5767168, 149946368, 0), (6493831168, 5767168, 155713536, 0), (6499598336, 5767168, 161480704, 0), (6488064000, 5767168, 167247872, 0), (7082082304, 5767168, 173015040, 0), (7087849472, 5767168, 178782208, 0), (7076315136, 5767168, 184549376, 0), (6580338688, 5767168, 190316544, 0), (6586105856, 5767168, 196083712, 0), (6574571520, 5767168, 201850880, 0), (6597640192, 5767168, 207618048, 0), (6603407360, 5767168, 213385216, 0), (6591873024, 5767168, 219152384, 0), (7151288320, 5767168, 224919552, 0), (7157055488, 5767168, 230686720, 0), (7145521152, 5767168, 236453888, 0), (7203192832, 5767168, 242221056, 0), (7208960000, 5767168, 247988224, 0), (7197425664, 5767168, 253755392, 0), (7255097344, 5767168, 259522560, 0), (7260864512, 5767168, 265289728, 0), (7249330176, 5767168, 271056896, 0)], 2: [(6182404096, 5767168, 0, 0), (6188171264, 5767168, 5767168, 0), (6176636928, 5767168, 11534336, 0), (6753353728, 5767168, 17301504, 0), (6759120896, 5767168, 23068672, 0), (6747586560, 5767168, 28835840, 0), (6234308608, 5767168, 34603008, 0), (6240075776, 5767168, 40370176, 0), (6228541440, 5767168, 46137344, 0), (6268911616, 5767168, 51904512, 0), (6274678784, 5767168, 57671680, 0), (6263144448, 5767168, 63438848, 0), (6286213120, 5767168, 69206016, 0), (6291980288, 5767168, 74973184, 0), (6280445952, 5767168, 80740352, 0), (6839861248, 5767168, 86507520, 0), (6845628416, 5767168, 92274688, 0), (6834094080, 5767168, 98041856, 0), (6338117632, 5767168, 103809024, 0), (6343884800, 5767168, 109576192, 0), (6332350464, 5767168, 115343360, 0), (6926368768, 5767168, 121110528, 0), (6932135936, 5767168, 126877696, 0), (6920601600, 5767168, 132644864, 0), (6390022144, 5767168, 138412032, 0), (6395789312, 5767168, 144179200, 0), (6384254976, 5767168, 149946368, 0), (6441926656, 5767168, 155713536, 0), (6447693824, 5767168, 161480704, 0), (6436159488, 5767168, 167247872, 0), (7030177792, 5767168, 173015040, 0), (7035944960, 5767168, 178782208, 0), (7024410624, 5767168, 184549376, 0), (6511132672, 5767168, 190316544, 0), (6516899840, 5767168, 196083712, 0), (6505365504, 5767168, 201850880, 0), (6545735680, 5767168, 207618048, 0), (6551502848, 5767168, 213385216, 0), (6539968512, 5767168, 219152384, 0), (6563037184, 5767168, 224919552, 0), (6568804352, 5767168, 230686720, 0), (6557270016, 5767168, 236453888, 0), (7133986816, 5767168, 242221056, 0), (7139753984, 5767168, 247988224, 0), (7128219648, 5767168, 253755392, 0), (6614941696, 5767168, 259522560, 0), (6620708864, 5767168, 265289728, 0), (6609174528, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec424dcb0>, 2: <capsule object NULL at 0x7a4ec4681350>}
DEBUG 01-15 16:10:50.704573.704573 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.704734.704734 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.704390.704390 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 24b2b0ea-794c-4ebe-a50e-663ff0c7758e
DEBUG 01-15 16:10:50.704218.704218 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:50.704528.704528 client.py:127] Model loaded
DEBUG 01-15 16:10:50.704471.704471 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.704916.704916 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:50.705774.705774 cuda_h.py:19] end restore2model cost 0.00033092498779296875 seconds
DEBUG 01-15 16:10:50.705067.705067 cuda_h.py:19] end sllm_worker_task cost 0.010177135467529297 seconds
DEBUG 01-15 16:10:50.705535.705535 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:50.705626.705626 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 24b2b0ea-794c-4ebe-a50e-663ff0c7758e
DEBUG 01-15 16:10:50.706987.706987 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003121614456176758 seconds
DEBUG 01-15 16:10:50.706334.706334 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.706440.706440 cuda_h.py:19] end move_flatidxs cost 0.0008540153503417969 seconds
DEBUG 01-15 16:10:50.706739.706739 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:50.708975.708975 cuda_h.py:19] end restore2model cost 0.0025107860565185547 seconds
DEBUG 01-15 16:10:50.708779.708779 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005878448486328125 seconds
DEBUG 01-15 16:10:50.708336.708336 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:50.709167.709167 cuda_h.py:19] end gpu_sexperts cost 0.00026535987854003906 seconds
DEBUG 01-15 16:10:50.709612.709612 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:50.709958.709958 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5974044799804688e-05 seconds
DEBUG 01-15 16:10:50.709416.709416 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:50.709403.709403 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.710537.710537 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008060932159423828 seconds
DEBUG 01-15 16:10:50.710473.710473 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.710036.710036 cuda_h.py:19] end gpu_group_list cost 0.00017571449279785156 seconds
DEBUG 01-15 16:10:50.711544.711544 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.711680.711680 cuda_h.py:19] end group_tensors cost 0.005116939544677734 seconds
DEBUG 01-15 16:10:50.712290.712290 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008788108825683594 seconds
DEBUG 01-15 16:10:50.712239.712239 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:50.712823.712823 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.712326.712326 cuda_h.py:19] end gpu_group_list cost 0.00035262107849121094 seconds
DEBUG 01-15 16:10:50.713163.713163 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:50.713702.713702 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 24b2b0ea-794c-4ebe-a50e-663ff0c7758e
DEBUG 01-15 16:10:50.716568.716568 cuda_h.py:19] end group pad cost 0.004437923431396484 seconds
DEBUG 01-15 16:10:50.716119.716119 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:50.732039.732039 client.py:127] Model loaded
DEBUG 01-15 16:10:50.733557.733557 cuda_h.py:19] end wait_experts_multi_device cost 0.019354820251464844 seconds
DEBUG 01-15 16:10:50.733304.733304 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:50.737414.737414 cuda_h.py:19] end group_einsum cost 0.021164417266845703 seconds
DEBUG 01-15 16:10:50.737578.737578 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:50.742794.742794 cuda_h.py:19] end get_outputs_cpu1 cost 0.004388570785522461 seconds
DEBUG 01-15 16:10:50.743199.743199 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03803086280822754 seconds
DEBUG 01-15 16:10:50.743309.743309 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010334491729736328 seconds
DEBUG 01-15 16:10:50.743459.743459 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:50.745634.745634 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.745137.745137 cuda_h.py:19] end index_scatter cost 0.00010728836059570312 seconds
DEBUG 01-15 16:10:50.745798.745798 cuda_h.py:19] end cpuoutputsdeal cost 0.0021715164184570312 seconds
DEBUG 01-15 16:10:50.746165.746165 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:50.746312.746312 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.746206.746206 cuda_h.py:19] end gpu_group_tensor cost 0.00016880035400390625 seconds
DEBUG 01-15 16:10:50.746684.746684 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.746435.746435 cuda_h.py:19] end gpu_group_tensor cost 0.0004057884216308594 seconds
DEBUG 01-15 16:10:50.746433.746433 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.747233.747233 cuda_h.py:19] end gpu_group_einsum cost 0.0009226799011230469 seconds
DEBUG 01-15 16:10:50.748569.748569 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.748302.748302 cuda_h.py:19] end gpu_group_einsum cost 0.00045180320739746094 seconds
DEBUG 01-15 16:10:50.748895.748895 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.748806.748806 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.749901.749901 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002422332763671875 seconds
DEBUG 01-15 16:10:50.749326.749326 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.749833.749833 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:50.749967.749967 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.749196.749196 cuda_h.py:19] end index_scatter cost 6.246566772460938e-05 seconds
DEBUG 01-15 16:10:50.749894.749894 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008955001831054688 seconds
DEBUG 01-15 16:10:50.749228.749228 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.749317.749317 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.750927.750927 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020575523376464844 seconds
DEBUG 01-15 16:10:50.750683.750683 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.750382.750382 cuda_h.py:19] end concat_expert_out cost 6.0558319091796875e-05 seconds
DEBUG 01-15 16:10:50.750232.750232 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.750030.750030 cuda_h.py:19] end index_scatter cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:10:50.750269.750269 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005891323089599609 seconds
DEBUG 01-15 16:10:50.750616.750616 cuda_h.py:19] end gpu_experts_multi_device cost 0.041170597076416016 seconds
DEBUG 01-15 16:10:50.750778.750778 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_5 cost 0.05036330223083496 seconds
DEBUG 01-15 16:10:50.750873.750873 cuda_h.py:19] end prefill_layer cost 0.05641746520996094 seconds
DEBUG 01-15 16:10:50.751577.751577 lmp.py:1553] -------------------------------- end prefill layer 4 --------------------------------
DEBUG 01-15 16:10:50.751425.751425 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.751181.751181 lmp.py:1495] -------------------------------- start prefill layer 5 --------------------------------
DEBUG 01-15 16:10:50.751653.751653 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:50.751316.751316 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 01-15 16:10:50.751080.751080 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 4.124641418457031e-05 seconds
DEBUG 01-15 16:10:50.751697.751697 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 7.700920104980469e-05 seconds
DEBUG 01-15 16:10:50.751969.751969 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.751548.751548 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.751785.751785 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.751468.751468 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.751367.751367 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.751938.751938 cuda_h.py:19] end allocate_cuda_memory cost 0.00020694732666015625 seconds
DEBUG 01-15 16:10:50.752292.752292 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.752724.752724 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.752507.752507 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.752263.752263 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ff04ffc9-3655-48c0-85fb-f19edbe64165
DEBUG 01-15 16:10:50.752982.752982 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.752318.752318 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.753330.753330 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ff04ffc9-3655-48c0-85fb-f19edbe64165
DEBUG 01-15 16:10:50.753338.753338 cuda_h.py:19] end load_into_gpu_async cost 0.001539468765258789 seconds
DEBUG 01-15 16:10:50.753717.753717 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.753111.753111 cuda_h.py:19] end restore_tensors2 cost 8.487701416015625e-05 seconds
DEBUG 01-15 16:10:50.753682.753682 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021181106567382812 seconds
INFO 01-15 16:10:50.753777.753777 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ff04ffc9-3655-48c0-85fb-f19edbe64165
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.757897.757897 cuda_h.py:19] end self_attn cost 0.004357337951660156 seconds
DEBUG 01-15 16:10:50.757400.757400 cuda_h.py:19] end iln_self_attn_paln cost 0.0061113834381103516 seconds
DEBUG 01-15 16:10:50.757422.757422 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_6
DEBUG 01-15 16:10:50.757754.757754 cuda_h.py:10] start gate
DEBUG 01-15 16:10:50.758164.758164 cuda_h.py:19] end gate cost 0.0007228851318359375 seconds
DEBUG 01-15 16:10:50.758569.758569 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:50.758997.758997 lmp.py:1912] 
DEBUG 01-15 16:10:50.758997.758997 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:50.758183.758183 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:50.758263.758263 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:50.758529.758529 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:50.758126.758126 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:50.758007.758007 lmp.py:1917] 
DEBUG 01-15 16:10:50.758007.758007 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:50.758604.758604 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:50.758207.758207 lmp.py:1935]   Expert 34 |     24 | CPU
DEBUG 01-15 16:10:50.758042.758042 lmp.py:1935]   Expert 45 |     67 | CPU
DEBUG 01-15 16:10:50.759923.759923 lmp.py:1935]   Expert 22 |     73 | CPU
DEBUG 01-15 16:10:50.759328.759328 lmp.py:1935]   Expert 57 |     77 | CPU
DEBUG 01-15 16:10:50.759733.759733 lmp.py:1935]   Expert 17 |     95 | CPU
DEBUG 01-15 16:10:50.759899.759899 lmp.py:1935]   Expert 15 |    100 | CPU
DEBUG 01-15 16:10:50.759257.759257 lmp.py:1935]   Expert  4 |    102 | CPU
DEBUG 01-15 16:10:50.759138.759138 lmp.py:1935]   Expert 28 |    107 | CPU
DEBUG 01-15 16:10:50.759020.759020 lmp.py:1935]   Expert 32 |    112 | CPU
DEBUG 01-15 16:10:50.759378.759378 lmp.py:1935]   Expert 60 |    114 | CPU
DEBUG 01-15 16:10:50.759259.759259 lmp.py:1935]   Expert 36 |    124 | CPU
DEBUG 01-15 16:10:50.759425.759425 lmp.py:1935]   Expert 14 |    126 | CPU
DEBUG 01-15 16:10:50.759592.759592 lmp.py:1935]   Expert 12 |    127 | CPU
DEBUG 01-15 16:10:50.759519.759519 lmp.py:1935]   Expert 16 |    127 | CPU
DEBUG 01-15 16:10:50.759209.759209 lmp.py:1935]   Expert 52 |    130 | CPU
DEBUG 01-15 16:10:50.759375.759375 lmp.py:1935]   Expert 25 |    131 | CPU
DEBUG 01-15 16:10:50.759541.759541 lmp.py:1935]   Expert  8 |    134 | CPU
DEBUG 01-15 16:10:50.759469.759469 lmp.py:1935]   Expert  2 |    138 | CPU
DEBUG 01-15 16:10:50.759396.759396 lmp.py:1935]   Expert 35 |    144 | CPU
DEBUG 01-15 16:10:50.759324.759324 lmp.py:1935]   Expert  5 |    147 | CPU
DEBUG 01-15 16:10:50.759113.759113 lmp.py:1935]   Expert 30 |    153 | CPU
DEBUG 01-15 16:10:50.759709.759709 lmp.py:1935]   Expert 23 |    154 | CPU
DEBUG 01-15 16:10:50.759545.759545 lmp.py:1935]   Expert 61 |    157 | CPU
DEBUG 01-15 16:10:50.759856.759856 lmp.py:1935]   Expert  0 |    158 | CPU
DEBUG 01-15 16:10:50.759976.759976 lmp.py:1935]   Expert 39 |    158 | CPU
DEBUG 01-15 16:10:50.759096.759096 lmp.py:1935]   Expert  3 |    168 | CPU
DEBUG 01-15 16:10:50.759454.759454 lmp.py:1935]   Expert 13 |    169 | CPU
DEBUG 01-15 16:10:50.759097.759097 lmp.py:1935]   Expert 42 |    171 | CPU
DEBUG 01-15 16:10:50.759217.759217 lmp.py:1935]   Expert 31 |    173 | CPU
DEBUG 01-15 16:10:50.759098.759098 lmp.py:1935]   Expert 44 |    173 | CPU
DEBUG 01-15 16:10:50.759980.759980 lmp.py:1935]   Expert 41 |    175 | CPU
DEBUG 01-15 16:10:50.759338.759338 lmp.py:1935]   Expert 46 |    176 | CPU
DEBUG 01-15 16:10:50.759127.759127 lmp.py:1935]   Expert  9 |    178 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759677.759677 lmp.py:1935]   Expert 43 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759751.759751 lmp.py:1935]   Expert 26 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759063.759063 lmp.py:1935]   Expert 27 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759851.759851 lmp.py:1935]   Expert 18 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759878.759878 lmp.py:1935]   Expert 50 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759621.759621 lmp.py:1935]   Expert 62 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759410.759410 lmp.py:1935]   Expert 49 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759483.759483 lmp.py:1935]   Expert 51 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759080.759080 lmp.py:1935]   Expert 11 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759676.759676 lmp.py:1935]   Expert 47 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759511.759511 lmp.py:1935]   Expert 19 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759108.759108 lmp.py:1935]   Expert 20 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759466.759466 lmp.py:1935]   Expert 63 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759540.759540 lmp.py:1935]   Expert 55 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759136.759136 lmp.py:1935]   Expert 56 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759448.759448 lmp.py:1935]   Expert 38 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759522.759522 lmp.py:1935]   Expert 48 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759787.759787 lmp.py:1935]   Expert  1 |    235 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759530.759530 lmp.py:1935]   Expert 10 |    239 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.759080.759080 lmp.py:1935]   Expert 54 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759154.759154 lmp.py:1935]   Expert  7 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.759393.759393 lmp.py:1935]   Expert 21 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.760467.760467 lmp.py:1935]   Expert 33 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.760017.760017 lmp.py:1935]   Expert 29 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.760091.760091 lmp.py:1935]   Expert 40 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.760403.760403 lmp.py:1935]   Expert 24 |    270 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.760476.760476 lmp.py:1935]   Expert 59 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.760503.760503 lmp.py:1935]   Expert 37 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.760769.760769 lmp.py:1935]   Expert 58 |    365 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.760034.760034 lmp.py:1935]   Expert  6 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.760585.760585 lmp.py:1935]   Expert 53 |    854 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.760943.760943 lmp.py:1937] 
DEBUG 01-15 16:10:50.760943.760943 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:50.760778.760778 lmp.py:1938]   CPU:   4184 tokens
DEBUG 01-15 16:10:50.760328.760328 lmp.py:1942]   cuda:1:   3973 tokens (15 experts)
DEBUG 01-15 16:10:50.760163.760163 lmp.py:1942]   cuda:2:   4131 tokens (17 experts)
DEBUG 01-15 16:10:50.760045.760045 lmp.py:1943]   Total GPU:   8104 tokens
DEBUG 01-15 16:10:50.760403.760403 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:50.760403.760403 lmp.py:1944] 
DEBUG 01-15 16:10:50.760722.760722 cuda_h.py:19] end experts_map_get cost 0.001943826675415039 seconds
DEBUG 01-15 16:10:50.760916.760916 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:50.760984.760984 lmp.py:1953] 
DEBUG 01-15 16:10:50.760984.760984 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:50.760297.760297 cuda_h.py:19] end cpu_experts_submit cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:10:50.760947.760947 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:50.760445.760445 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:50.761631.761631 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.5.mlp.experts.33.gate_proj.weight': 0, 'model.layers.5.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.40.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.40.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.40.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.43.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.43.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.43.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.48.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.48.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.48.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.50.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.50.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.50.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.51.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.51.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.51.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.5.mlp.experts.1.gate_proj.weight': 0, 'model.layers.5.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.5.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.5.mlp.experts.56.gate_proj.weight': 17301504, 'model.layers.5.mlp.experts.56.down_proj.weight': 23068672, 'model.layers.5.mlp.experts.56.up_proj.weight': 28835840, 'model.layers.5.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.5.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.5.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.5.mlp.experts.6.gate_proj.weight': 51904512, 'model.layers.5.mlp.experts.6.down_proj.weight': 57671680, 'model.layers.5.mlp.experts.6.up_proj.weight': 63438848, 'model.layers.5.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.5.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.5.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.5.mlp.experts.38.gate_proj.weight': 86507520, 'model.layers.5.mlp.experts.38.down_proj.weight': 92274688, 'model.layers.5.mlp.experts.38.up_proj.weight': 98041856, 'model.layers.5.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.5.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.5.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.5.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.5.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.5.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.5.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.5.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.5.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.5.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.5.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.5.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.5.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.5.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.5.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.5.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.5.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.5.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.5.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.5.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.5.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.5.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.5.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.5.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.5.mlp.experts.27.gate_proj.weight': 242221056, 'model.layers.5.mlp.experts.27.down_proj.weight': 247988224, 'model.layers.5.mlp.experts.27.up_proj.weight': 253755392, 'model.layers.5.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.5.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.5.mlp.experts.29.up_proj.weight': 271056896, 'model.layers.5.mlp.experts.62.gate_proj.weight': 276824064, 'model.layers.5.mlp.experts.62.down_proj.weight': 282591232, 'model.layers.5.mlp.experts.62.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(7860649984, 5767168, 0, 0), (7866417152, 5767168, 5767168, 0), (7854882816, 5767168, 11534336, 0), (7981760512, 5767168, 17301504, 0), (7987527680, 5767168, 23068672, 0), (7975993344, 5767168, 28835840, 0), (7462715392, 5767168, 34603008, 0), (7468482560, 5767168, 40370176, 0), (7456948224, 5767168, 46137344, 0), (7480016896, 5767168, 51904512, 0), (7485784064, 5767168, 57671680, 0), (7474249728, 5767168, 63438848, 0), (8033665024, 5767168, 69206016, 0), (8039432192, 5767168, 74973184, 0), (8027897856, 5767168, 80740352, 0), (8120172544, 5767168, 86507520, 0), (8125939712, 5767168, 92274688, 0), (8114405376, 5767168, 98041856, 0), (8154775552, 5767168, 103809024, 0), (8160542720, 5767168, 109576192, 0), (8149008384, 5767168, 115343360, 0), (8172077056, 5767168, 121110528, 0), (8177844224, 5767168, 126877696, 0), (8166309888, 5767168, 132644864, 0), (7635730432, 5767168, 138412032, 0), (7641497600, 5767168, 144179200, 0), (7629963264, 5767168, 149946368, 0), (7653031936, 5767168, 155713536, 0), (7658799104, 5767168, 161480704, 0), (7647264768, 5767168, 167247872, 0), (8206680064, 5767168, 173015040, 0), (8212447232, 5767168, 178782208, 0), (8200912896, 5767168, 184549376, 0), (8241283072, 5767168, 190316544, 0), (8247050240, 5767168, 196083712, 0), (8235515904, 5767168, 201850880, 0), (7739539456, 5767168, 207618048, 0), (7745306624, 5767168, 213385216, 0), (7733772288, 5767168, 219152384, 0), (8310489088, 5767168, 224919552, 0), (8316256256, 5767168, 230686720, 0), (8304721920, 5767168, 236453888, 0), (8379695104, 5767168, 242221056, 0), (8385462272, 5767168, 247988224, 0), (8373927936, 5767168, 253755392, 0)], 2: [(7307001856, 5767168, 0, 0), (7312769024, 5767168, 5767168, 0), (7301234688, 5767168, 11534336, 0), (8258584576, 5767168, 17301504, 0), (8264351744, 5767168, 23068672, 0), (8252817408, 5767168, 28835840, 0), (7929856000, 5767168, 34603008, 0), (7935623168, 5767168, 40370176, 0), (7924088832, 5767168, 46137344, 0), (7393509376, 5767168, 51904512, 0), (7399276544, 5767168, 57671680, 0), (7387742208, 5767168, 63438848, 0), (7410810880, 5767168, 69206016, 0), (7416578048, 5767168, 74973184, 0), (7405043712, 5767168, 80740352, 0), (7947157504, 5767168, 86507520, 0), (7952924672, 5767168, 92274688, 0), (7941390336, 5767168, 98041856, 0), (7445413888, 5767168, 103809024, 0), (7451181056, 5767168, 109576192, 0), (7439646720, 5767168, 115343360, 0), (8102871040, 5767168, 121110528, 0), (8108638208, 5767168, 126877696, 0), (8097103872, 5767168, 132644864, 0), (8137474048, 5767168, 138412032, 0), (8143241216, 5767168, 144179200, 0), (8131706880, 5767168, 149946368, 0), (7601127424, 5767168, 155713536, 0), (7606894592, 5767168, 161480704, 0), (7595360256, 5767168, 167247872, 0), (7618428928, 5767168, 173015040, 0), (7624196096, 5767168, 178782208, 0), (7612661760, 5767168, 184549376, 0), (8223981568, 5767168, 190316544, 0), (8229748736, 5767168, 196083712, 0), (8218214400, 5767168, 201850880, 0), (7704936448, 5767168, 207618048, 0), (7710703616, 5767168, 213385216, 0), (7699169280, 5767168, 219152384, 0), (8293187584, 5767168, 224919552, 0), (8298954752, 5767168, 230686720, 0), (8287420416, 5767168, 236453888, 0), (7756840960, 5767168, 242221056, 0), (7762608128, 5767168, 247988224, 0), (7751073792, 5767168, 253755392, 0), (7791443968, 5767168, 259522560, 0), (7797211136, 5767168, 265289728, 0), (7785676800, 5767168, 271056896, 0), (8362393600, 5767168, 276824064, 0), (8368160768, 5767168, 282591232, 0), (8356626432, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec43dbe40>, 2: <capsule object NULL at 0x7a4f2c2c2580>}
INFO 01-15 16:10:50.761848.761848 client.py:127] Model loaded
DEBUG 01-15 16:10:50.761847.761847 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.761948.761948 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.761164.761164 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:50.761732.761732 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.762726.762726 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, df9d7edf-74a1-45ef-98e3-79dacfd9fb91
DEBUG 01-15 16:10:50.762637.762637 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:50.762945.762945 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.762698.762698 cuda_h.py:19] end restore2model cost 0.0007321834564208984 seconds
DEBUG 01-15 16:10:50.762581.762581 cuda_h.py:19] end sllm_worker_task cost 0.01108551025390625 seconds
DEBUG 01-15 16:10:50.763508.763508 cuda_h.py:19] end move_flatidxs cost 0.0008306503295898438 seconds
DEBUG 01-15 16:10:50.763569.763569 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:50.763693.763693 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, df9d7edf-74a1-45ef-98e3-79dacfd9fb91
DEBUG 01-15 16:10:50.763160.763160 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003301382064819336 seconds
DEBUG 01-15 16:10:50.763930.763930 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.767557.767557 cuda_h.py:19] end restore2model cost 0.0030570030212402344 seconds
DEBUG 01-15 16:10:50.767023.767023 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006596565246582031 seconds
DEBUG 01-15 16:10:50.767772.767772 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:50.767055.767055 cuda_h.py:19] end gpu_sexperts cost 0.0003159046173095703 seconds
DEBUG 01-15 16:10:50.767746.767746 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:50.767575.767575 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.811981201171875e-05 seconds
DEBUG 01-15 16:10:50.767656.767656 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:50.767981.767981 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.768348.768348 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010447502136230469 seconds
DEBUG 01-15 16:10:50.768828.768828 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.769868.769868 cuda_h.py:19] end gpu_group_list cost 0.00017452239990234375 seconds
DEBUG 01-15 16:10:50.769966.769966 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.771329.771329 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012748241424560547 seconds
DEBUG 01-15 16:10:50.771511.771511 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.771055.771055 cuda_h.py:19] end gpu_group_list cost 0.0001938343048095703 seconds
DEBUG 01-15 16:10:50.772337.772337 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:50.772538.772538 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, df9d7edf-74a1-45ef-98e3-79dacfd9fb91
DEBUG 01-15 16:10:50.772047.772047 cuda_h.py:19] end group_tensors cost 0.009093761444091797 seconds
DEBUG 01-15 16:10:50.772059.772059 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:50.777212.777212 cuda_h.py:19] end group pad cost 0.0045206546783447266 seconds
DEBUG 01-15 16:10:50.777731.777731 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:50.793306.793306 client.py:127] Model loaded
DEBUG 01-15 16:10:50.793864.793864 cuda_h.py:19] end wait_experts_multi_device cost 0.021106243133544922 seconds
DEBUG 01-15 16:10:50.793233.793233 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:50.798904.798904 cuda_h.py:19] end group_einsum cost 0.020505905151367188 seconds
DEBUG 01-15 16:10:50.798638.798638 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:50.802123.802123 cuda_h.py:19] end get_outputs_cpu1 cost 0.004309177398681641 seconds
DEBUG 01-15 16:10:50.803289.803289 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0414280891418457 seconds
DEBUG 01-15 16:10:50.803227.803227 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.009799957275390625 seconds
DEBUG 01-15 16:10:50.803899.803899 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:50.805400.805400 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.805751.805751 cuda_h.py:19] end index_scatter cost 8.845329284667969e-05 seconds
DEBUG 01-15 16:10:50.806682.806682 cuda_h.py:19] end cpuoutputsdeal cost 0.0021736621856689453 seconds
DEBUG 01-15 16:10:50.806890.806890 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:50.806276.806276 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.806715.806715 cuda_h.py:19] end gpu_group_tensor cost 0.0004208087921142578 seconds
DEBUG 01-15 16:10:50.806797.806797 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.807807.807807 cuda_h.py:19] end gpu_group_tensor cost 0.0006668567657470703 seconds
DEBUG 01-15 16:10:50.807004.807004 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.808687.808687 cuda_h.py:19] end gpu_group_einsum cost 0.0007648468017578125 seconds
DEBUG 01-15 16:10:50.808593.808593 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.809351.809351 cuda_h.py:19] end gpu_group_einsum cost 0.0004374980926513672 seconds
DEBUG 01-15 16:10:50.809368.809368 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.809564.809564 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.809249.809249 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002560615539550781 seconds
DEBUG 01-15 16:10:50.809435.809435 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.809650.809650 cuda_h.py:19] end concat_expert_out cost 5.3882598876953125e-05 seconds
DEBUG 01-15 16:10:50.809500.809500 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.809967.809967 cuda_h.py:19] end index_scatter cost 6.222724914550781e-05 seconds
DEBUG 01-15 16:10:50.810459.810459 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.000896453857421875 seconds
DEBUG 01-15 16:10:50.810787.810787 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.810206.810206 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.810049.810049 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019431114196777344 seconds
DEBUG 01-15 16:10:50.810758.810758 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.810026.810026 cuda_h.py:19] end concat_expert_out cost 5.841255187988281e-05 seconds
DEBUG 01-15 16:10:50.810876.810876 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.810058.810058 cuda_h.py:19] end index_scatter cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:10:50.810821.810821 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005838871002197266 seconds
DEBUG 01-15 16:10:50.811546.811546 cuda_h.py:19] end gpu_experts_multi_device cost 0.04336214065551758 seconds
DEBUG 01-15 16:10:50.811946.811946 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_6 cost 0.05358600616455078 seconds
DEBUG 01-15 16:10:50.811955.811955 cuda_h.py:19] end prefill_layer cost 0.06046795845031738 seconds
DEBUG 01-15 16:10:50.811950.811950 lmp.py:1553] -------------------------------- end prefill layer 5 --------------------------------
DEBUG 01-15 16:10:50.811799.811799 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.811316.811316 lmp.py:1495] -------------------------------- start prefill layer 6 --------------------------------
DEBUG 01-15 16:10:50.811311.811311 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:50.811974.811974 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 01-15 16:10:50.811546.811546 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 4.00543212890625e-05 seconds
DEBUG 01-15 16:10:50.811402.811402 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 7.557868957519531e-05 seconds
DEBUG 01-15 16:10:50.811582.811582 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.811227.811227 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.812726.812726 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.812075.812075 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.812759.812759 cuda_h.py:19] end allocate_cuda_memory cost 0.00021529197692871094 seconds
DEBUG 01-15 16:10:50.812715.812715 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.812670.812670 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.812214.812214 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.812970.812970 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0e7f7b9f-34aa-4244-af06-12e0e34e2669
DEBUG 01-15 16:10:50.812828.812828 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.812291.812291 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.813012.813012 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.813844.813844 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0e7f7b9f-34aa-4244-af06-12e0e34e2669
DEBUG 01-15 16:10:50.813164.813164 cuda_h.py:19] end load_into_gpu_async cost 0.0013113021850585938 seconds
DEBUG 01-15 16:10:50.813920.813920 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.813169.813169 cuda_h.py:19] end restore_tensors2 cost 8.320808410644531e-05 seconds
DEBUG 01-15 16:10:50.814216.814216 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0018858909606933594 seconds
INFO 01-15 16:10:50.814358.814358 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0e7f7b9f-34aa-4244-af06-12e0e34e2669
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.817677.817677 cuda_h.py:19] end self_attn cost 0.004353046417236328 seconds
DEBUG 01-15 16:10:50.818080.818080 cuda_h.py:19] end iln_self_attn_paln cost 0.005957841873168945 seconds
DEBUG 01-15 16:10:50.818956.818956 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_7
DEBUG 01-15 16:10:50.818335.818335 cuda_h.py:10] start gate
DEBUG 01-15 16:10:50.818578.818578 cuda_h.py:19] end gate cost 0.000705718994140625 seconds
DEBUG 01-15 16:10:50.818460.818460 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:50.819961.819961 lmp.py:1912] 
DEBUG 01-15 16:10:50.819961.819961 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:50.819624.819624 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:50.819658.819658 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:50.819262.819262 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:50.819004.819004 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:50.819747.819747 lmp.py:1917] 
DEBUG 01-15 16:10:50.819747.819747 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:50.819297.819297 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:50.819285.819285 lmp.py:1935]   Expert  1 |     46 | CPU
DEBUG 01-15 16:10:50.819312.819312 lmp.py:1935]   Expert  7 |     59 | CPU
DEBUG 01-15 16:10:50.819385.819385 lmp.py:1935]   Expert 37 |     71 | CPU
DEBUG 01-15 16:10:50.819697.819697 lmp.py:1935]   Expert 17 |     75 | CPU
DEBUG 01-15 16:10:50.819771.819771 lmp.py:1935]   Expert 54 |     77 | CPU
DEBUG 01-15 16:10:50.819082.819082 lmp.py:1935]   Expert 18 |     83 | CPU
DEBUG 01-15 16:10:50.819825.819825 lmp.py:1935]   Expert  9 |     92 | CPU
DEBUG 01-15 16:10:50.819329.819329 lmp.py:1935]   Expert 13 |     93 | CPU
DEBUG 01-15 16:10:50.819356.819356 lmp.py:1935]   Expert 22 |     98 | CPU
DEBUG 01-15 16:10:50.819860.819860 lmp.py:1935]   Expert 58 |    102 | CPU
DEBUG 01-15 16:10:50.819172.819172 lmp.py:1935]   Expert  0 |    109 | CPU
DEBUG 01-15 16:10:50.819007.819007 lmp.py:1935]   Expert 16 |    118 | CPU
DEBUG 01-15 16:10:50.819319.819319 lmp.py:1935]   Expert 26 |    118 | CPU
DEBUG 01-15 16:10:50.819154.819154 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:10:50.819227.819227 lmp.py:1935]   Expert 63 |    129 | CPU
DEBUG 01-15 16:10:50.819063.819063 lmp.py:1935]   Expert 59 |    134 | CPU
DEBUG 01-15 16:10:50.819898.819898 lmp.py:1935]   Expert 62 |    139 | CPU
DEBUG 01-15 16:10:50.819733.819733 lmp.py:1935]   Expert 43 |    142 | CPU
DEBUG 01-15 16:10:50.819806.819806 lmp.py:1935]   Expert 28 |    145 | CPU
DEBUG 01-15 16:10:50.819403.819403 lmp.py:1935]   Expert 33 |    147 | CPU
DEBUG 01-15 16:10:50.820430.820430 lmp.py:1935]   Expert 29 |    148 | CPU
DEBUG 01-15 16:10:50.820980.820980 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:10:50.820007.820007 lmp.py:1935]   Expert 55 |    163 | CPU
DEBUG 01-15 16:10:50.820273.820273 lmp.py:1935]   Expert 51 |    164 | CPU
DEBUG 01-15 16:10:50.820870.820870 lmp.py:1935]   Expert 11 |    167 | CPU
DEBUG 01-15 16:10:50.820943.820943 lmp.py:1935]   Expert  3 |    168 | CPU
DEBUG 01-15 16:10:50.820301.820301 lmp.py:1935]   Expert 23 |    168 | CPU
DEBUG 01-15 16:10:50.820898.820898 lmp.py:1935]   Expert 32 |    168 | CPU
DEBUG 01-15 16:10:50.820733.820733 lmp.py:1935]   Expert 45 |    168 | CPU
DEBUG 01-15 16:10:50.820330.820330 lmp.py:1935]   Expert 53 |    168 | CPU
DEBUG 01-15 16:10:50.820926.820926 lmp.py:1935]   Expert 40 |    169 | CPU
DEBUG 01-15 16:10:50.820285.820285 lmp.py:1935]   Expert 34 |    174 | CPU
DEBUG 01-15 16:10:50.820312.820312 lmp.py:1935]   Expert 14 |    176 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820531.820531 lmp.py:1935]   Expert 41 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820273.820273 lmp.py:1935]   Expert 52 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820016.820016 lmp.py:1935]   Expert 42 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820805.820805 lmp.py:1935]   Expert 21 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820832.820832 lmp.py:1935]   Expert 57 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820621.820621 lmp.py:1935]   Expert 30 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820092.820092 lmp.py:1935]   Expert 15 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820040.820040 lmp.py:1935]   Expert 35 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820352.820352 lmp.py:1935]   Expert  4 |    219 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820545.820545 lmp.py:1935]   Expert 12 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820049.820049 lmp.py:1935]   Expert 19 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820076.820076 lmp.py:1935]   Expert 46 |    230 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820103.820103 lmp.py:1935]   Expert 50 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820892.820892 lmp.py:1935]   Expert 24 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820204.820204 lmp.py:1935]   Expert 44 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820993.820993 lmp.py:1935]   Expert  8 |    234 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820782.820782 lmp.py:1935]   Expert 49 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820855.820855 lmp.py:1935]   Expert 38 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820882.820882 lmp.py:1935]   Expert  6 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820432.820432 lmp.py:1935]   Expert 47 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820652.820652 lmp.py:1935]   Expert 31 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820917.820917 lmp.py:1935]   Expert 61 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820660.820660 lmp.py:1935]   Expert 39 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820402.820402 lmp.py:1935]   Expert  5 |    304 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820429.820429 lmp.py:1935]   Expert 27 |    306 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820218.820218 lmp.py:1935]   Expert 36 |    306 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820530.820530 lmp.py:1935]   Expert 60 |    332 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820080.820080 lmp.py:1935]   Expert 20 |    340 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820392.820392 lmp.py:1935]   Expert 48 |    368 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820466.820466 lmp.py:1935]   Expert 25 |    397 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.820778.820778 lmp.py:1935]   Expert 56 |    558 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.820666.820666 lmp.py:1937] 
DEBUG 01-15 16:10:50.820666.820666 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:50.820455.820455 lmp.py:1938]   CPU:   4079 tokens
DEBUG 01-15 16:10:50.820958.820958 lmp.py:1942]   cuda:1:   4121 tokens (16 experts)
DEBUG 01-15 16:10:50.820224.820224 lmp.py:1942]   cuda:2:   4088 tokens (16 experts)
DEBUG 01-15 16:10:50.821821.821821 lmp.py:1943]   Total GPU:   8209 tokens
DEBUG 01-15 16:10:50.821941.821941 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:50.821941.821941 lmp.py:1944] 
DEBUG 01-15 16:10:50.821498.821498 cuda_h.py:19] end experts_map_get cost 0.002069234848022461 seconds
INFO 01-15 16:10:50.821454.821454 client.py:127] Model loaded
DEBUG 01-15 16:10:50.821336.821336 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.821464.821464 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:50.821724.821724 lmp.py:1953] 
DEBUG 01-15 16:10:50.821724.821724 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:50.821136.821136 cuda_h.py:19] end cpu_experts_submit cost 5.817413330078125e-05 seconds
DEBUG 01-15 16:10:50.821932.821932 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:50.821861.821861 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:50.822549.822549 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.6.mlp.experts.35.gate_proj.weight': 0, 'model.layers.6.mlp.experts.35.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.35.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.38.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.38.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.38.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.41.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.41.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.41.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.6.mlp.experts.36.gate_proj.weight': 0, 'model.layers.6.mlp.experts.36.down_proj.weight': 5767168, 'model.layers.6.mlp.experts.36.up_proj.weight': 11534336, 'model.layers.6.mlp.experts.6.gate_proj.weight': 17301504, 'model.layers.6.mlp.experts.6.down_proj.weight': 23068672, 'model.layers.6.mlp.experts.6.up_proj.weight': 28835840, 'model.layers.6.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.6.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.6.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.6.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.6.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.6.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.6.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.6.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.6.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.6.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.6.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.6.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.6.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.6.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.6.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.6.mlp.experts.57.gate_proj.weight': 121110528, 'model.layers.6.mlp.experts.57.down_proj.weight': 126877696, 'model.layers.6.mlp.experts.57.up_proj.weight': 132644864, 'model.layers.6.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.6.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.6.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.6.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.6.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.6.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.6.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.6.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.6.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.6.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.6.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.6.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.6.mlp.experts.52.gate_proj.weight': 207618048, 'model.layers.6.mlp.experts.52.down_proj.weight': 213385216, 'model.layers.6.mlp.experts.52.up_proj.weight': 219152384, 'model.layers.6.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.6.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.6.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.6.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.6.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.6.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.6.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.6.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.6.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(9002549248, 5767168, 0, 0), (9008316416, 5767168, 5767168, 0), (8996782080, 5767168, 11534336, 0), (8466202624, 5767168, 17301504, 0), (8471969792, 5767168, 23068672, 0), (8460435456, 5767168, 28835840, 0), (8483504128, 5767168, 34603008, 0), (8489271296, 5767168, 40370176, 0), (8477736960, 5767168, 46137344, 0), (9054453760, 5767168, 51904512, 0), (9060220928, 5767168, 57671680, 0), (9048686592, 5767168, 63438848, 0), (9106358272, 5767168, 69206016, 0), (9112125440, 5767168, 74973184, 0), (9100591104, 5767168, 80740352, 0), (9192865792, 5767168, 86507520, 0), (9198632960, 5767168, 92274688, 0), (9187098624, 5767168, 98041856, 0), (9210167296, 5767168, 103809024, 0), (9215934464, 5767168, 109576192, 0), (9204400128, 5767168, 115343360, 0), (8639217664, 5767168, 121110528, 0), (8644984832, 5767168, 126877696, 0), (8633450496, 5767168, 132644864, 0), (9244770304, 5767168, 138412032, 0), (9250537472, 5767168, 144179200, 0), (9239003136, 5767168, 149946368, 0), (8743026688, 5767168, 155713536, 0), (8748793856, 5767168, 161480704, 0), (8737259520, 5767168, 167247872, 0), (8760328192, 5767168, 173015040, 0), (8766095360, 5767168, 178782208, 0), (8754561024, 5767168, 184549376, 0), (9365880832, 5767168, 190316544, 0), (9371648000, 5767168, 196083712, 0), (9360113664, 5767168, 201850880, 0), (8812232704, 5767168, 207618048, 0), (8817999872, 5767168, 213385216, 0), (8806465536, 5767168, 219152384, 0), (8864137216, 5767168, 224919552, 0), (8869904384, 5767168, 230686720, 0), (8858370048, 5767168, 236453888, 0), (9452388352, 5767168, 242221056, 0), (9458155520, 5767168, 247988224, 0), (9446621184, 5767168, 253755392, 0), (8916041728, 5767168, 259522560, 0), (8921808896, 5767168, 265289728, 0), (8910274560, 5767168, 271056896, 0)], 2: [(9019850752, 5767168, 0, 0), (9025617920, 5767168, 5767168, 0), (9014083584, 5767168, 11534336, 0), (8500805632, 5767168, 17301504, 0), (8506572800, 5767168, 23068672, 0), (8495038464, 5767168, 28835840, 0), (9071755264, 5767168, 34603008, 0), (9077522432, 5767168, 40370176, 0), (9065988096, 5767168, 46137344, 0), (8535408640, 5767168, 51904512, 0), (8541175808, 5767168, 57671680, 0), (8529641472, 5767168, 63438848, 0), (9123659776, 5767168, 69206016, 0), (9129426944, 5767168, 74973184, 0), (9117892608, 5767168, 80740352, 0), (9158262784, 5767168, 86507520, 0), (9164029952, 5767168, 92274688, 0), (9152495616, 5767168, 98041856, 0), (8604614656, 5767168, 103809024, 0), (8610381824, 5767168, 109576192, 0), (8598847488, 5767168, 115343360, 0), (9383182336, 5767168, 121110528, 0), (9388949504, 5767168, 126877696, 0), (9377415168, 5767168, 132644864, 0), (8656519168, 5767168, 138412032, 0), (8662286336, 5767168, 144179200, 0), (8650752000, 5767168, 149946368, 0), (9227468800, 5767168, 155713536, 0), (9233235968, 5767168, 161480704, 0), (9221701632, 5767168, 167247872, 0), (9262071808, 5767168, 173015040, 0), (9267838976, 5767168, 178782208, 0), (9256304640, 5767168, 184549376, 0), (8725725184, 5767168, 190316544, 0), (8731492352, 5767168, 196083712, 0), (8719958016, 5767168, 201850880, 0), (9296674816, 5767168, 207618048, 0), (9302441984, 5767168, 213385216, 0), (9290907648, 5767168, 219152384, 0), (8829534208, 5767168, 224919552, 0), (8835301376, 5767168, 230686720, 0), (8823767040, 5767168, 236453888, 0), (9435086848, 5767168, 242221056, 0), (9440854016, 5767168, 247988224, 0), (9429319680, 5767168, 253755392, 0), (8933343232, 5767168, 259522560, 0), (8939110400, 5767168, 265289728, 0), (8927576064, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4195e90>, 2: <capsule object NULL at 0x7a51b06da130>}
DEBUG 01-15 16:10:50.822467.822467 cuda_h.py:19] end restore2model cost 0.0015006065368652344 seconds
DEBUG 01-15 16:10:50.822622.822622 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.822439.822439 cuda_h.py:19] end sllm_worker_task cost 0.010823726654052734 seconds
DEBUG 01-15 16:10:50.822773.822773 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.823657.823657 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 21b45016-ac7f-4bbc-a94f-10e7e0c7eefd
DEBUG 01-15 16:10:50.823044.823044 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:50.823299.823299 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.823438.823438 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:50.824178.824178 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 21b45016-ac7f-4bbc-a94f-10e7e0c7eefd
DEBUG 01-15 16:10:50.824746.824746 cuda_h.py:19] end move_flatidxs cost 0.0008366107940673828 seconds
DEBUG 01-15 16:10:50.824331.824331 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:50.824618.824618 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0028069019317626953 seconds
DEBUG 01-15 16:10:50.824012.824012 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.827650.827650 cuda_h.py:19] end restore2model cost 0.002997159957885742 seconds
DEBUG 01-15 16:10:50.827916.827916 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00603795051574707 seconds
DEBUG 01-15 16:10:50.827666.827666 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:50.828326.828326 cuda_h.py:19] end gpu_sexperts cost 0.0003132820129394531 seconds
DEBUG 01-15 16:10:50.828825.828825 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:50.828700.828700 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:10:50.828542.828542 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:50.828060.828060 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.829846.829846 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011072158813476562 seconds
DEBUG 01-15 16:10:50.829134.829134 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.829863.829863 cuda_h.py:19] end gpu_group_list cost 0.0001842975616455078 seconds
DEBUG 01-15 16:10:50.830492.830492 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.832694.832694 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011990070343017578 seconds
DEBUG 01-15 16:10:50.832279.832279 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.832571.832571 cuda_h.py:19] end gpu_group_list cost 0.00018310546875 seconds
DEBUG 01-15 16:10:50.833381.833381 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:50.833145.833145 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 21b45016-ac7f-4bbc-a94f-10e7e0c7eefd
DEBUG 01-15 16:10:50.833023.833023 cuda_h.py:19] end group_tensors cost 0.009310007095336914 seconds
DEBUG 01-15 16:10:50.834962.834962 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:50.838651.838651 cuda_h.py:19] end group pad cost 0.004108428955078125 seconds
DEBUG 01-15 16:10:50.838740.838740 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:50.850887.850887 client.py:127] Model loaded
DEBUG 01-15 16:10:50.851709.851709 cuda_h.py:19] end wait_experts_multi_device cost 0.017791032791137695 seconds
DEBUG 01-15 16:10:50.851174.851174 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:50.858090.858090 cuda_h.py:19] end group_einsum cost 0.020029067993164062 seconds
DEBUG 01-15 16:10:50.859347.859347 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:50.863623.863623 cuda_h.py:19] end get_outputs_cpu1 cost 0.004203319549560547 seconds
DEBUG 01-15 16:10:50.863955.863955 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04077911376953125 seconds
DEBUG 01-15 16:10:50.864450.864450 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.013292789459228516 seconds
DEBUG 01-15 16:10:50.864599.864599 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:50.866211.866211 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.866522.866522 cuda_h.py:19] end index_scatter cost 9.226799011230469e-05 seconds
DEBUG 01-15 16:10:50.866606.866606 cuda_h.py:19] end cpuoutputsdeal cost 0.0021369457244873047 seconds
DEBUG 01-15 16:10:50.866006.866006 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:50.866345.866345 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.867955.867955 cuda_h.py:19] end gpu_group_tensor cost 0.00016832351684570312 seconds
DEBUG 01-15 16:10:50.867863.867863 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.867651.867651 cuda_h.py:19] end gpu_group_tensor cost 0.0001609325408935547 seconds
DEBUG 01-15 16:10:50.867967.867967 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.868775.868775 cuda_h.py:19] end gpu_group_einsum cost 0.0005486011505126953 seconds
DEBUG 01-15 16:10:50.868766.868766 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.871819.871819 cuda_h.py:19] end gpu_group_einsum cost 0.002871274948120117 seconds
DEBUG 01-15 16:10:50.871011.871011 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.871532.871532 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.871213.871213 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003161430358886719 seconds
DEBUG 01-15 16:10:50.871069.871069 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.872012.872012 cuda_h.py:19] end concat_expert_out cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:10:50.872399.872399 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.872562.872562 cuda_h.py:19] end index_scatter cost 8.440017700195312e-05 seconds
DEBUG 01-15 16:10:50.872150.872150 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0011332035064697266 seconds
DEBUG 01-15 16:10:50.872327.872327 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.872568.872568 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.873027.873027 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002338886260986328 seconds
DEBUG 01-15 16:10:50.873975.873975 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.873727.873727 cuda_h.py:19] end concat_expert_out cost 6.4849853515625e-05 seconds
DEBUG 01-15 16:10:50.873252.873252 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.873395.873395 cuda_h.py:19] end index_scatter cost 6.818771362304688e-05 seconds
DEBUG 01-15 16:10:50.873350.873350 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006439685821533203 seconds
DEBUG 01-15 16:10:50.873254.873254 cuda_h.py:19] end gpu_experts_multi_device cost 0.04516148567199707 seconds
DEBUG 01-15 16:10:50.873145.873145 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_7 cost 0.05544614791870117 seconds
DEBUG 01-15 16:10:50.874083.874083 cuda_h.py:19] end prefill_layer cost 0.06284928321838379 seconds
DEBUG 01-15 16:10:50.874231.874231 lmp.py:1553] -------------------------------- end prefill layer 6 --------------------------------
DEBUG 01-15 16:10:50.874703.874703 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.874651.874651 lmp.py:1495] -------------------------------- start prefill layer 7 --------------------------------
DEBUG 01-15 16:10:50.874837.874837 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:50.874084.874084 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 01-15 16:10:50.874292.874292 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:10:50.874624.874624 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 8.893013000488281e-05 seconds
DEBUG 01-15 16:10:50.874565.874565 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.874237.874237 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.875935.875935 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.875785.875785 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.875581.875581 cuda_h.py:19] end allocate_cuda_memory cost 0.0004067420959472656 seconds
DEBUG 01-15 16:10:50.875107.875107 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.875446.875446 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.875801.875801 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.875983.875983 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.875257.875257 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6a26275f-386f-4e49-bdf4-7c1e8cfd6409
DEBUG 01-15 16:10:50.876188.876188 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.876819.876819 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.877102.877102 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6a26275f-386f-4e49-bdf4-7c1e8cfd6409
DEBUG 01-15 16:10:50.877223.877223 cuda_h.py:19] end load_into_gpu_async cost 0.0014531612396240234 seconds
DEBUG 01-15 16:10:50.877926.877926 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.877883.877883 cuda_h.py:19] end restore_tensors2 cost 8.177757263183594e-05 seconds
DEBUG 01-15 16:10:50.877355.877355 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0022084712982177734 seconds
INFO 01-15 16:10:50.877813.877813 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6a26275f-386f-4e49-bdf4-7c1e8cfd6409
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.881210.881210 cuda_h.py:19] end self_attn cost 0.0044939517974853516 seconds
DEBUG 01-15 16:10:50.881640.881640 cuda_h.py:19] end iln_self_attn_paln cost 0.0065479278564453125 seconds
DEBUG 01-15 16:10:50.881377.881377 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_8
DEBUG 01-15 16:10:50.881425.881425 cuda_h.py:10] start gate
DEBUG 01-15 16:10:50.882738.882738 cuda_h.py:19] end gate cost 0.0008268356323242188 seconds
DEBUG 01-15 16:10:50.882813.882813 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:50.882538.882538 lmp.py:1912] 
DEBUG 01-15 16:10:50.882538.882538 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:50.883632.883632 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:50.883381.883381 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:50.883839.883839 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:50.883151.883151 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:50.883748.883748 lmp.py:1917] 
DEBUG 01-15 16:10:50.883748.883748 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:50.883344.883344 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:50.883378.883378 lmp.py:1935]   Expert 50 |     44 | CPU
DEBUG 01-15 16:10:50.883928.883928 lmp.py:1935]   Expert  3 |     54 | CPU
DEBUG 01-15 16:10:50.883287.883287 lmp.py:1935]   Expert 46 |     55 | CPU
DEBUG 01-15 16:10:50.883122.883122 lmp.py:1935]   Expert  1 |     76 | CPU
DEBUG 01-15 16:10:50.883242.883242 lmp.py:1935]   Expert 29 |     87 | CPU
DEBUG 01-15 16:10:50.883600.883600 lmp.py:1935]   Expert  4 |     88 | CPU
DEBUG 01-15 16:10:50.883720.883720 lmp.py:1935]   Expert 15 |     96 | CPU
DEBUG 01-15 16:10:50.883078.883078 lmp.py:1935]   Expert 40 |     96 | CPU
DEBUG 01-15 16:10:50.883867.883867 lmp.py:1935]   Expert  8 |    110 | CPU
DEBUG 01-15 16:10:50.883940.883940 lmp.py:1935]   Expert 28 |    112 | CPU
DEBUG 01-15 16:10:50.883537.883537 lmp.py:1935]   Expert 41 |    114 | CPU
DEBUG 01-15 16:10:50.883610.883610 lmp.py:1935]   Expert 16 |    125 | CPU
DEBUG 01-15 16:10:50.883445.883445 lmp.py:1935]   Expert 27 |    128 | CPU
DEBUG 01-15 16:10:50.883804.883804 lmp.py:1935]   Expert 48 |    128 | CPU
DEBUG 01-15 16:10:50.883923.883923 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:10:50.883282.883282 lmp.py:1935]   Expert 13 |    131 | CPU
DEBUG 01-15 16:10:50.883163.883163 lmp.py:1935]   Expert 54 |    132 | CPU
DEBUG 01-15 16:10:50.883283.883283 lmp.py:1935]   Expert  7 |    135 | CPU
DEBUG 01-15 16:10:50.883164.883164 lmp.py:1935]   Expert 51 |    137 | CPU
DEBUG 01-15 16:10:50.883522.883522 lmp.py:1935]   Expert 39 |    139 | CPU
DEBUG 01-15 16:10:50.883404.883404 lmp.py:1935]   Expert 60 |    139 | CPU
DEBUG 01-15 16:10:50.883524.883524 lmp.py:1935]   Expert 18 |    140 | CPU
DEBUG 01-15 16:10:50.883882.883882 lmp.py:1935]   Expert 43 |    145 | CPU
DEBUG 01-15 16:10:50.883717.883717 lmp.py:1935]   Expert 14 |    146 | CPU
DEBUG 01-15 16:10:50.883075.883075 lmp.py:1935]   Expert 52 |    148 | CPU
DEBUG 01-15 16:10:50.883361.883361 lmp.py:1935]   Expert 56 |    148 | CPU
DEBUG 01-15 16:10:50.883104.883104 lmp.py:1935]   Expert 20 |    150 | CPU
DEBUG 01-15 16:10:50.883177.883177 lmp.py:1935]   Expert 55 |    151 | CPU
DEBUG 01-15 16:10:50.883297.883297 lmp.py:1935]   Expert 36 |    153 | CPU
DEBUG 01-15 16:10:50.883893.883893 lmp.py:1935]   Expert 10 |    156 | CPU
DEBUG 01-15 16:10:50.883252.883252 lmp.py:1935]   Expert 11 |    158 | CPU
DEBUG 01-15 16:10:50.883610.883610 lmp.py:1935]   Expert 45 |    160 | CPU
DEBUG 01-15 16:10:50.883875.883875 lmp.py:1935]   Expert  5 |    162 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.883856.883856 lmp.py:1935]   Expert 62 |    166 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.883122.883122 lmp.py:1935]   Expert 57 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.883864.883864 lmp.py:1935]   Expert 44 |    177 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.883891.883891 lmp.py:1935]   Expert 33 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.883634.883634 lmp.py:1935]   Expert 58 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.883946.883946 lmp.py:1935]   Expert 25 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.883258.883258 lmp.py:1935]   Expert 53 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.883570.883570 lmp.py:1935]   Expert  2 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.883120.883120 lmp.py:1935]   Expert 32 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.883432.883432 lmp.py:1935]   Expert 35 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.883982.883982 lmp.py:1935]   Expert 31 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.883294.883294 lmp.py:1935]   Expert 21 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884798.884798 lmp.py:1935]   Expert 63 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884878.884878 lmp.py:1935]   Expert 49 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884190.884190 lmp.py:1935]   Expert 17 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884264.884264 lmp.py:1935]   Expert 42 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884814.884814 lmp.py:1935]   Expert 34 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884888.884888 lmp.py:1935]   Expert 59 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884199.884199 lmp.py:1935]   Expert 37 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884988.884988 lmp.py:1935]   Expert  0 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884300.884300 lmp.py:1935]   Expert 22 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884612.884612 lmp.py:1935]   Expert 19 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884685.884685 lmp.py:1935]   Expert 24 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884997.884997 lmp.py:1935]   Expert 61 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884025.884025 lmp.py:1935]   Expert 30 |    301 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884052.884052 lmp.py:1935]   Expert 47 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884556.884556 lmp.py:1935]   Expert 38 |    364 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884106.884106 lmp.py:1935]   Expert 26 |    376 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884895.884895 lmp.py:1935]   Expert 12 |    428 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884207.884207 lmp.py:1935]   Expert  9 |    684 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.884757.884757 lmp.py:1935]   Expert 23 |    700 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.884745.884745 lmp.py:1937] 
DEBUG 01-15 16:10:50.884745.884745 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:50.884580.884580 lmp.py:1938]   CPU:   3910 tokens
DEBUG 01-15 16:10:50.884368.884368 lmp.py:1942]   cuda:1:   4190 tokens (16 experts)
DEBUG 01-15 16:10:50.884111.884111 lmp.py:1942]   cuda:2:   4188 tokens (16 experts)
DEBUG 01-15 16:10:50.884376.884376 lmp.py:1943]   Total GPU:   8378 tokens
DEBUG 01-15 16:10:50.884450.884450 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:50.884450.884450 lmp.py:1944] 
DEBUG 01-15 16:10:50.884484.884484 cuda_h.py:19] end experts_map_get cost 0.002009868621826172 seconds
INFO 01-15 16:10:50.884924.884924 client.py:127] Model loaded
DEBUG 01-15 16:10:50.884906.884906 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.884190.884190 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:50.884258.884258 lmp.py:1953] 
DEBUG 01-15 16:10:50.884258.884258 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:50.884154.884154 cuda_h.py:19] end cpu_experts_submit cost 6.365776062011719e-05 seconds
DEBUG 01-15 16:10:50.884234.884234 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:50.885732.885732 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:50.885578.885578 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.7.mlp.experts.32.gate_proj.weight': 0, 'model.layers.7.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.38.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.38.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.38.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.49.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.49.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.49.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.22.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.22.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.22.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.24.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.24.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.24.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.25.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.25.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.25.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.26.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.26.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.26.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.59.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.59.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.59.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.7.mlp.experts.0.gate_proj.weight': 0, 'model.layers.7.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.7.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.7.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.7.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.7.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.7.mlp.experts.2.gate_proj.weight': 34603008, 'model.layers.7.mlp.experts.2.down_proj.weight': 40370176, 'model.layers.7.mlp.experts.2.up_proj.weight': 46137344, 'model.layers.7.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.7.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.7.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.7.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.7.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.7.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.7.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.7.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.7.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.7.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.7.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.7.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.7.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.7.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.7.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.7.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.7.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.7.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.7.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.7.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.7.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.7.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.7.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.7.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.7.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.7.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.7.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.7.mlp.experts.62.gate_proj.weight': 207618048, 'model.layers.7.mlp.experts.62.down_proj.weight': 213385216, 'model.layers.7.mlp.experts.62.up_proj.weight': 219152384, 'model.layers.7.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.7.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.7.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.7.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.7.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.7.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.7.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.7.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.7.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(10057940992, 5767168, 0, 0), (10063708160, 5767168, 5767168, 0), (10052173824, 5767168, 11534336, 0), (10075242496, 5767168, 17301504, 0), (10081009664, 5767168, 23068672, 0), (10069475328, 5767168, 28835840, 0), (10109845504, 5767168, 34603008, 0), (10115612672, 5767168, 40370176, 0), (10104078336, 5767168, 46137344, 0), (9590800384, 5767168, 51904512, 0), (9596567552, 5767168, 57671680, 0), (9585033216, 5767168, 63438848, 0), (10161750016, 5767168, 69206016, 0), (10167517184, 5767168, 74973184, 0), (10155982848, 5767168, 80740352, 0), (10230956032, 5767168, 86507520, 0), (10236723200, 5767168, 92274688, 0), (10225188864, 5767168, 98041856, 0), (10352066560, 5767168, 103809024, 0), (10357833728, 5767168, 109576192, 0), (10346299392, 5767168, 115343360, 0), (9884925952, 5767168, 121110528, 0), (9890693120, 5767168, 126877696, 0), (9879158784, 5767168, 132644864, 0), (9902227456, 5767168, 138412032, 0), (9907994624, 5767168, 144179200, 0), (9896460288, 5767168, 149946368, 0), (9919528960, 5767168, 155713536, 0), (9925296128, 5767168, 161480704, 0), (9913761792, 5767168, 167247872, 0), (9936830464, 5767168, 173015040, 0), (9942597632, 5767168, 178782208, 0), (9931063296, 5767168, 184549376, 0), (9954131968, 5767168, 190316544, 0), (9959899136, 5767168, 196083712, 0), (9948364800, 5767168, 201850880, 0), (10525081600, 5767168, 207618048, 0), (10530848768, 5767168, 213385216, 0), (10519314432, 5767168, 219152384, 0), (10490478592, 5767168, 224919552, 0), (10496245760, 5767168, 230686720, 0), (10484711424, 5767168, 236453888, 0), (10559684608, 5767168, 242221056, 0), (10565451776, 5767168, 247988224, 0), (10553917440, 5767168, 253755392, 0), (10594287616, 5767168, 259522560, 0), (10600054784, 5767168, 265289728, 0), (10588520448, 5767168, 271056896, 0)], 2: [(9504292864, 5767168, 0, 0), (9510060032, 5767168, 5767168, 0), (9498525696, 5767168, 11534336, 0), (10092544000, 5767168, 17301504, 0), (10098311168, 5767168, 23068672, 0), (10086776832, 5767168, 28835840, 0), (9538895872, 5767168, 34603008, 0), (9544663040, 5767168, 40370176, 0), (9533128704, 5767168, 46137344, 0), (10144448512, 5767168, 51904512, 0), (10150215680, 5767168, 57671680, 0), (10138681344, 5767168, 63438848, 0), (9660006400, 5767168, 69206016, 0), (9665773568, 5767168, 74973184, 0), (9654239232, 5767168, 80740352, 0), (9711910912, 5767168, 86507520, 0), (9717678080, 5767168, 92274688, 0), (9706143744, 5767168, 98041856, 0), (10265559040, 5767168, 103809024, 0), (10271326208, 5767168, 109576192, 0), (10259791872, 5767168, 115343360, 0), (10317463552, 5767168, 121110528, 0), (10323230720, 5767168, 126877696, 0), (10311696384, 5767168, 132644864, 0), (9798418432, 5767168, 138412032, 0), (9804185600, 5767168, 144179200, 0), (9792651264, 5767168, 149946368, 0), (9833021440, 5767168, 155713536, 0), (9838788608, 5767168, 161480704, 0), (9827254272, 5767168, 167247872, 0), (9867624448, 5767168, 173015040, 0), (9873391616, 5767168, 178782208, 0), (9861857280, 5767168, 184549376, 0), (10421272576, 5767168, 190316544, 0), (10427039744, 5767168, 196083712, 0), (10415505408, 5767168, 201850880, 0), (10576986112, 5767168, 207618048, 0), (10582753280, 5767168, 213385216, 0), (10571218944, 5767168, 219152384, 0), (10507780096, 5767168, 224919552, 0), (10513547264, 5767168, 230686720, 0), (10502012928, 5767168, 236453888, 0), (10023337984, 5767168, 242221056, 0), (10029105152, 5767168, 247988224, 0), (10017570816, 5767168, 253755392, 0), (10040639488, 5767168, 259522560, 0), (10046406656, 5767168, 265289728, 0), (10034872320, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec43da640>, 2: <capsule object NULL at 0x7a51b06da7c0>}
DEBUG 01-15 16:10:50.886251.886251 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.886472.886472 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.886036.886036 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, db12c501-4a10-42ed-bfbc-0b5169d30972
DEBUG 01-15 16:10:50.886475.886475 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.886051.886051 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:50.886353.886353 cuda_h.py:19] end restore2model cost 0.002260923385620117 seconds
DEBUG 01-15 16:10:50.886129.886129 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:50.887183.887183 cuda_h.py:19] end sllm_worker_task cost 0.011955499649047852 seconds
INFO 01-15 16:10:50.887061.887061 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, db12c501-4a10-42ed-bfbc-0b5169d30972
DEBUG 01-15 16:10:50.887822.887822 cuda_h.py:19] end move_flatidxs cost 0.0008611679077148438 seconds
DEBUG 01-15 16:10:50.887612.887612 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:50.888755.888755 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0035991668701171875 seconds
DEBUG 01-15 16:10:50.888685.888685 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.892543.892543 cuda_h.py:19] end restore2model cost 0.003227710723876953 seconds
DEBUG 01-15 16:10:50.892598.892598 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007089853286743164 seconds
DEBUG 01-15 16:10:50.892752.892752 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:50.892745.892745 cuda_h.py:19] end gpu_sexperts cost 0.00034737586975097656 seconds
DEBUG 01-15 16:10:50.892435.892435 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:50.892695.892695 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.9788742065429688e-05 seconds
DEBUG 01-15 16:10:50.892729.892729 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:50.892008.892008 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.893417.893417 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0011107921600341797 seconds
DEBUG 01-15 16:10:50.893221.893221 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.894023.894023 cuda_h.py:19] end gpu_group_list cost 0.00017452239990234375 seconds
DEBUG 01-15 16:10:50.894646.894646 cuda_h.py:19] end group_tensors cost 0.006126880645751953 seconds
DEBUG 01-15 16:10:50.894095.894095 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:50.895324.895324 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.897392.897392 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.00193023681640625 seconds
DEBUG 01-15 16:10:50.897655.897655 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.897382.897382 cuda_h.py:19] end gpu_group_list cost 0.00031065940856933594 seconds
DEBUG 01-15 16:10:50.899649.899649 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:50.899401.899401 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, db12c501-4a10-42ed-bfbc-0b5169d30972
DEBUG 01-15 16:10:50.899315.899315 cuda_h.py:19] end group pad cost 0.004914999008178711 seconds
DEBUG 01-15 16:10:50.899204.899204 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:50.915302.915302 client.py:127] Model loaded
DEBUG 01-15 16:10:50.916301.916301 cuda_h.py:19] end wait_experts_multi_device cost 0.016942262649536133 seconds
DEBUG 01-15 16:10:50.916630.916630 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:50.919546.919546 cuda_h.py:19] end group_einsum cost 0.01940751075744629 seconds
DEBUG 01-15 16:10:50.919803.919803 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:50.923831.923831 cuda_h.py:19] end get_outputs_cpu1 cost 0.003952741622924805 seconds
DEBUG 01-15 16:10:50.924852.924852 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0374755859375 seconds
DEBUG 01-15 16:10:50.924321.924321 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.00808858871459961 seconds
DEBUG 01-15 16:10:50.924755.924755 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:50.926238.926238 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.926602.926602 cuda_h.py:19] end index_scatter cost 9.34600830078125e-05 seconds
DEBUG 01-15 16:10:50.927541.927541 cuda_h.py:19] end cpuoutputsdeal cost 0.002257108688354492 seconds
DEBUG 01-15 16:10:50.927888.927888 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:50.927558.927558 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.927075.927075 cuda_h.py:19] end gpu_group_tensor cost 0.00016951560974121094 seconds
DEBUG 01-15 16:10:50.927745.927745 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.927301.927301 cuda_h.py:19] end gpu_group_tensor cost 0.00016641616821289062 seconds
DEBUG 01-15 16:10:50.927987.927987 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.928358.928358 cuda_h.py:19] end gpu_group_einsum cost 0.0007448196411132812 seconds
DEBUG 01-15 16:10:50.928543.928543 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.929856.929856 cuda_h.py:19] end gpu_group_einsum cost 0.0005877017974853516 seconds
DEBUG 01-15 16:10:50.929821.929821 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.929097.929097 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.930898.930898 cuda_h.py:19] end all_expert_outputs_slices cost 0.00033164024353027344 seconds
DEBUG 01-15 16:10:50.930754.930754 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.930498.930498 cuda_h.py:19] end concat_expert_out cost 5.817413330078125e-05 seconds
DEBUG 01-15 16:10:50.930547.930547 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.930067.930067 cuda_h.py:19] end index_scatter cost 6.699562072753906e-05 seconds
DEBUG 01-15 16:10:50.930288.930288 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010213851928710938 seconds
DEBUG 01-15 16:10:50.930338.930338 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.930711.930711 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.931852.931852 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020956993103027344 seconds
DEBUG 01-15 16:10:50.931561.931561 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.931306.931306 cuda_h.py:19] end concat_expert_out cost 5.984306335449219e-05 seconds
DEBUG 01-15 16:10:50.931249.931249 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.931530.931530 cuda_h.py:19] end index_scatter cost 6.604194641113281e-05 seconds
DEBUG 01-15 16:10:50.931962.931962 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005970001220703125 seconds
DEBUG 01-15 16:10:50.931316.931316 cuda_h.py:19] end gpu_experts_multi_device cost 0.038854360580444336 seconds
DEBUG 01-15 16:10:50.931815.931815 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_8 cost 0.05004572868347168 seconds
DEBUG 01-15 16:10:50.932064.932064 cuda_h.py:19] end prefill_layer cost 0.05742835998535156 seconds
DEBUG 01-15 16:10:50.932921.932921 lmp.py:1553] -------------------------------- end prefill layer 7 --------------------------------
DEBUG 01-15 16:10:50.932484.932484 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.932479.932479 lmp.py:1495] -------------------------------- start prefill layer 8 --------------------------------
DEBUG 01-15 16:10:50.932473.932473 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:50.932375.932375 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 01-15 16:10:50.932377.932377 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 4.172325134277344e-05 seconds
DEBUG 01-15 16:10:50.932061.932061 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 9.131431579589844e-05 seconds
DEBUG 01-15 16:10:50.932002.932002 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.932581.932581 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.932632.932632 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.932031.932031 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.932599.932599 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.933549.933549 cuda_h.py:19] end allocate_cuda_memory cost 0.00023794174194335938 seconds
DEBUG 01-15 16:10:50.933849.933849 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.933711.933711 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.933018.933018 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.933774.933774 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d372ef11-abe0-4db6-90f5-0403f613562f
DEBUG 01-15 16:10:50.933301.933301 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.933193.933193 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.934424.934424 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d372ef11-abe0-4db6-90f5-0403f613562f
DEBUG 01-15 16:10:50.934996.934996 cuda_h.py:19] end load_into_gpu_async cost 0.0013670921325683594 seconds
DEBUG 01-15 16:10:50.934421.934421 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.934723.934723 cuda_h.py:19] end restore_tensors2 cost 8.487701416015625e-05 seconds
DEBUG 01-15 16:10:50.934770.934770 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.001977205276489258 seconds
INFO 01-15 16:10:50.935607.935607 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d372ef11-abe0-4db6-90f5-0403f613562f
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.938757.938757 cuda_h.py:19] end self_attn cost 0.004521846771240234 seconds
DEBUG 01-15 16:10:50.938028.938028 cuda_h.py:19] end iln_self_attn_paln cost 0.006267070770263672 seconds
DEBUG 01-15 16:10:50.938050.938050 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_9
DEBUG 01-15 16:10:50.938905.938905 cuda_h.py:10] start gate
DEBUG 01-15 16:10:50.939978.939978 cuda_h.py:19] end gate cost 0.0007560253143310547 seconds
DEBUG 01-15 16:10:50.939668.939668 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:50.940950.940950 lmp.py:1912] 
DEBUG 01-15 16:10:50.940950.940950 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:50.940851.940851 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:50.940693.940693 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:50.940197.940197 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:50.940556.940556 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:50.940675.940675 lmp.py:1917] 
DEBUG 01-15 16:10:50.940675.940675 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:50.940795.940795 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:50.940637.940637 lmp.py:1935]   Expert 38 |     13 | CPU
DEBUG 01-15 16:10:50.940757.940757 lmp.py:1935]   Expert 39 |     59 | CPU
DEBUG 01-15 16:10:50.940400.940400 lmp.py:1935]   Expert  7 |     72 | CPU
DEBUG 01-15 16:10:50.940043.940043 lmp.py:1935]   Expert 30 |     73 | CPU
DEBUG 01-15 16:10:50.940269.940269 lmp.py:1935]   Expert 24 |     92 | CPU
DEBUG 01-15 16:10:50.940078.940078 lmp.py:1935]   Expert 14 |     94 | CPU
DEBUG 01-15 16:10:50.940959.940959 lmp.py:1935]   Expert 27 |     94 | CPU
DEBUG 01-15 16:10:50.940841.940841 lmp.py:1935]   Expert 40 |     95 | CPU
DEBUG 01-15 16:10:50.940914.940914 lmp.py:1935]   Expert 17 |     98 | CPU
DEBUG 01-15 16:10:50.940511.940511 lmp.py:1935]   Expert 36 |     98 | CPU
DEBUG 01-15 16:10:50.940869.940869 lmp.py:1935]   Expert 16 |    105 | CPU
DEBUG 01-15 16:10:50.940227.940227 lmp.py:1935]   Expert 32 |    106 | CPU
DEBUG 01-15 16:10:50.940109.940109 lmp.py:1935]   Expert 18 |    111 | CPU
DEBUG 01-15 16:10:50.940513.940513 lmp.py:1935]   Expert 48 |    112 | CPU
DEBUG 01-15 16:10:50.940156.940156 lmp.py:1935]   Expert 12 |    116 | CPU
DEBUG 01-15 16:10:50.940799.940799 lmp.py:1935]   Expert  1 |    117 | CPU
DEBUG 01-15 16:10:50.940350.940350 lmp.py:1935]   Expert  6 |    129 | CPU
DEBUG 01-15 16:10:50.940185.940185 lmp.py:1935]   Expert 59 |    131 | CPU
DEBUG 01-15 16:10:50.940212.940212 lmp.py:1935]   Expert 42 |    137 | CPU
DEBUG 01-15 16:10:50.940524.940524 lmp.py:1935]   Expert  0 |    141 | CPU
DEBUG 01-15 16:10:50.940836.940836 lmp.py:1935]   Expert 22 |    146 | CPU
DEBUG 01-15 16:10:50.940148.940148 lmp.py:1935]   Expert 53 |    148 | CPU
DEBUG 01-15 16:10:50.940221.940221 lmp.py:1935]   Expert 51 |    149 | CPU
DEBUG 01-15 16:10:50.940579.940579 lmp.py:1935]   Expert  8 |    162 | CPU
DEBUG 01-15 16:10:50.940176.940176 lmp.py:1935]   Expert 44 |    165 | CPU
DEBUG 01-15 16:10:50.940011.940011 lmp.py:1935]   Expert 60 |    167 | CPU
DEBUG 01-15 16:10:50.940608.940608 lmp.py:1935]   Expert 15 |    170 | CPU
DEBUG 01-15 16:10:50.940966.940966 lmp.py:1935]   Expert 29 |    171 | CPU
DEBUG 01-15 16:10:50.940801.940801 lmp.py:1935]   Expert 54 |    172 | CPU
DEBUG 01-15 16:10:50.940113.940113 lmp.py:1935]   Expert 35 |    178 | CPU
DEBUG 01-15 16:10:50.940140.940140 lmp.py:1935]   Expert 33 |    181 | CPU
DEBUG 01-15 16:10:50.940452.940452 lmp.py:1935]   Expert 34 |    183 | CPU
DEBUG 01-15 16:10:50.940671.940671 lmp.py:1935]   Expert 19 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.940367.940367 lmp.py:1935]   Expert 47 |    190 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.940633.940633 lmp.py:1935]   Expert  9 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941660.941660 lmp.py:1935]   Expert 21 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941210.941210 lmp.py:1935]   Expert 46 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941237.941237 lmp.py:1935]   Expert 56 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941026.941026 lmp.py:1935]   Expert  3 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941577.941577 lmp.py:1935]   Expert 20 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941365.941365 lmp.py:1935]   Expert 45 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941392.941392 lmp.py:1935]   Expert 49 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941420.941420 lmp.py:1935]   Expert 28 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941208.941208 lmp.py:1935]   Expert 57 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941712.941712 lmp.py:1935]   Expert 13 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941263.941263 lmp.py:1935]   Expert  2 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941813.941813 lmp.py:1935]   Expert  4 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941886.941886 lmp.py:1935]   Expert 43 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941198.941198 lmp.py:1935]   Expert 10 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941272.941272 lmp.py:1935]   Expert 50 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941584.941584 lmp.py:1935]   Expert 41 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941896.941896 lmp.py:1935]   Expert 26 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941876.941876 lmp.py:1935]   Expert 63 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941619.941619 lmp.py:1935]   Expert 37 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941646.941646 lmp.py:1935]   Expert 31 |    271 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941673.941673 lmp.py:1935]   Expert 61 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941224.941224 lmp.py:1935]   Expert 52 |    305 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941535.941535 lmp.py:1935]   Expert 58 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941086.941086 lmp.py:1935]   Expert 62 |    324 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941636.941636 lmp.py:1935]   Expert 55 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941710.941710 lmp.py:1935]   Expert 11 |    380 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941021.941021 lmp.py:1935]   Expert 23 |    385 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941810.941810 lmp.py:1935]   Expert 25 |    409 | GPU2(cuda:2)
DEBUG 01-15 16:10:50.941645.941645 lmp.py:1935]   Expert  5 |    512 | GPU1(cuda:1)
DEBUG 01-15 16:10:50.941004.941004 lmp.py:1937] 
DEBUG 01-15 16:10:50.941004.941004 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:50.941746.941746 lmp.py:1938]   CPU:   3985 tokens
DEBUG 01-15 16:10:50.941250.941250 lmp.py:1942]   cuda:1:   4168 tokens (16 experts)
DEBUG 01-15 16:10:50.941800.941800 lmp.py:1942]   cuda:2:   4135 tokens (16 experts)
DEBUG 01-15 16:10:50.941635.941635 lmp.py:1943]   Total GPU:   8303 tokens
DEBUG 01-15 16:10:50.941755.941755 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:50.941755.941755 lmp.py:1944] 
DEBUG 01-15 16:10:50.941703.941703 cuda_h.py:19] end experts_map_get cost 0.0019974708557128906 seconds
INFO 01-15 16:10:50.941131.941131 client.py:127] Model loaded
DEBUG 01-15 16:10:50.941444.941444 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.941481.941481 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:50.942595.942595 lmp.py:1953] 
DEBUG 01-15 16:10:50.942595.942595 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:50.942816.942816 cuda_h.py:19] end cpu_experts_submit cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:10:50.942658.942658 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:50.942348.942348 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:50.942754.942754 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.8.mlp.experts.3.gate_proj.weight': 0, 'model.layers.8.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.45.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.45.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.45.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.46.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.46.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.46.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.47.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.47.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.47.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.63.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.63.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.63.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.8.mlp.experts.2.gate_proj.weight': 0, 'model.layers.8.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.8.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.8.mlp.experts.26.gate_proj.weight': 17301504, 'model.layers.8.mlp.experts.26.down_proj.weight': 23068672, 'model.layers.8.mlp.experts.26.up_proj.weight': 28835840, 'model.layers.8.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.8.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.8.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.8.mlp.experts.43.gate_proj.weight': 51904512, 'model.layers.8.mlp.experts.43.down_proj.weight': 57671680, 'model.layers.8.mlp.experts.43.up_proj.weight': 63438848, 'model.layers.8.mlp.experts.49.gate_proj.weight': 69206016, 'model.layers.8.mlp.experts.49.down_proj.weight': 74973184, 'model.layers.8.mlp.experts.49.up_proj.weight': 80740352, 'model.layers.8.mlp.experts.50.gate_proj.weight': 86507520, 'model.layers.8.mlp.experts.50.down_proj.weight': 92274688, 'model.layers.8.mlp.experts.50.up_proj.weight': 98041856, 'model.layers.8.mlp.experts.19.gate_proj.weight': 103809024, 'model.layers.8.mlp.experts.19.down_proj.weight': 109576192, 'model.layers.8.mlp.experts.19.up_proj.weight': 115343360, 'model.layers.8.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.8.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.8.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.8.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.8.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.8.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.8.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.8.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.8.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.8.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.8.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.8.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.8.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.8.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.8.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.8.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.8.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.8.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.8.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.8.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.8.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.8.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.8.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.8.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.8.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.8.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.8.mlp.experts.57.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(10663493632, 5767168, 0, 0), (10669260800, 5767168, 5767168, 0), (10657726464, 5767168, 11534336, 0), (10680795136, 5767168, 17301504, 0), (10686562304, 5767168, 23068672, 0), (10675027968, 5767168, 28835840, 0), (10698096640, 5767168, 34603008, 0), (10703863808, 5767168, 40370176, 0), (10692329472, 5767168, 46137344, 0), (11320950784, 5767168, 51904512, 0), (11326717952, 5767168, 57671680, 0), (11315183616, 5767168, 63438848, 0), (10784604160, 5767168, 69206016, 0), (10790371328, 5767168, 74973184, 0), (10778836992, 5767168, 80740352, 0), (10801905664, 5767168, 86507520, 0), (10807672832, 5767168, 92274688, 0), (10796138496, 5767168, 98041856, 0), (10767302656, 5767168, 103809024, 0), (10773069824, 5767168, 109576192, 0), (10761535488, 5767168, 115343360, 0), (10836508672, 5767168, 121110528, 0), (10842275840, 5767168, 126877696, 0), (10830741504, 5767168, 132644864, 0), (11390156800, 5767168, 138412032, 0), (11395923968, 5767168, 144179200, 0), (11384389632, 5767168, 149946368, 0), (11407458304, 5767168, 155713536, 0), (11413225472, 5767168, 161480704, 0), (11401691136, 5767168, 167247872, 0), (11424759808, 5767168, 173015040, 0), (11430526976, 5767168, 178782208, 0), (11418992640, 5767168, 184549376, 0), (11511267328, 5767168, 190316544, 0), (11517034496, 5767168, 196083712, 0), (11505500160, 5767168, 201850880, 0), (11096031232, 5767168, 207618048, 0), (11101798400, 5767168, 213385216, 0), (11090264064, 5767168, 219152384, 0), (11701583872, 5767168, 224919552, 0), (11707351040, 5767168, 230686720, 0), (11695816704, 5767168, 236453888, 0), (11684282368, 5767168, 242221056, 0), (11690049536, 5767168, 247988224, 0), (11678515200, 5767168, 253755392, 0), (11147935744, 5767168, 259522560, 0), (11153702912, 5767168, 265289728, 0), (11142168576, 5767168, 271056896, 0)], 2: [(10646192128, 5767168, 0, 0), (10651959296, 5767168, 5767168, 0), (10640424960, 5767168, 11534336, 0), (11061428224, 5767168, 17301504, 0), (11067195392, 5767168, 23068672, 0), (11055661056, 5767168, 28835840, 0), (11251744768, 5767168, 34603008, 0), (11257511936, 5767168, 40370176, 0), (11245977600, 5767168, 46137344, 0), (11355553792, 5767168, 51904512, 0), (11361320960, 5767168, 57671680, 0), (11349786624, 5767168, 63438848, 0), (11459362816, 5767168, 69206016, 0), (11465129984, 5767168, 74973184, 0), (11453595648, 5767168, 80740352, 0), (11476664320, 5767168, 86507520, 0), (11482431488, 5767168, 92274688, 0), (11470897152, 5767168, 98041856, 0), (10940317696, 5767168, 103809024, 0), (10946084864, 5767168, 109576192, 0), (10934550528, 5767168, 115343360, 0), (11009523712, 5767168, 121110528, 0), (11015290880, 5767168, 126877696, 0), (11003756544, 5767168, 132644864, 0), (10957619200, 5767168, 138412032, 0), (10963386368, 5767168, 144179200, 0), (10951852032, 5767168, 149946368, 0), (10974920704, 5767168, 155713536, 0), (10980687872, 5767168, 161480704, 0), (10969153536, 5767168, 167247872, 0), (11563171840, 5767168, 173015040, 0), (11568939008, 5767168, 178782208, 0), (11557404672, 5767168, 184549376, 0), (11580473344, 5767168, 190316544, 0), (11586240512, 5767168, 196083712, 0), (11574706176, 5767168, 201850880, 0), (11044126720, 5767168, 207618048, 0), (11049893888, 5767168, 213385216, 0), (11038359552, 5767168, 219152384, 0), (11615076352, 5767168, 224919552, 0), (11620843520, 5767168, 230686720, 0), (11609309184, 5767168, 236453888, 0), (11666980864, 5767168, 242221056, 0), (11672748032, 5767168, 247988224, 0), (11661213696, 5767168, 253755392, 0), (11597774848, 5767168, 259522560, 0), (11603542016, 5767168, 265289728, 0), (11592007680, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e547ae610>, 2: <capsule object NULL at 0x7a4ec4195b90>}
DEBUG 01-15 16:10:50.943947.943947 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.943883.943883 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.943162.943162 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 94cae5c5-4409-4bfd-8d54-8209f39a50f9
DEBUG 01-15 16:10:50.943713.943713 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.943211.943211 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:50.943811.943811 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:50.943268.943268 cuda_h.py:19] end restore2model cost 0.0020461082458496094 seconds
DEBUG 01-15 16:10:50.943952.943952 cuda_h.py:19] end sllm_worker_task cost 0.011296510696411133 seconds
DEBUG 01-15 16:10:50.944019.944019 cuda_h.py:19] end move_flatidxs cost 0.0008325576782226562 seconds
DEBUG 01-15 16:10:50.944557.944557 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:50.946968.946968 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 94cae5c5-4409-4bfd-8d54-8209f39a50f9
DEBUG 01-15 16:10:50.947063.947063 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00530242919921875 seconds
DEBUG 01-15 16:10:50.947086.947086 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:50.950801.950801 cuda_h.py:19] end restore2model cost 0.0031180381774902344 seconds
DEBUG 01-15 16:10:50.950890.950890 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00867772102355957 seconds
DEBUG 01-15 16:10:50.950090.950090 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:50.951188.951188 cuda_h.py:19] end gpu_sexperts cost 0.00031828880310058594 seconds
DEBUG 01-15 16:10:50.951117.951117 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:50.951231.951231 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:10:50.951357.951357 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:50.951252.951252 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.953503.953503 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001730203628540039 seconds
DEBUG 01-15 16:10:50.953406.953406 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.953837.953837 cuda_h.py:19] end gpu_group_list cost 0.00017976760864257812 seconds
DEBUG 01-15 16:10:50.954122.954122 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:50.953135.953135 cuda_h.py:19] end group_tensors cost 0.009090662002563477 seconds
DEBUG 01-15 16:10:50.954325.954325 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:50.956172.956172 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0018279552459716797 seconds
DEBUG 01-15 16:10:50.956482.956482 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:50.956560.956560 cuda_h.py:19] end gpu_group_list cost 0.00029015541076660156 seconds
DEBUG 01-15 16:10:50.958673.958673 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:50.958497.958497 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 94cae5c5-4409-4bfd-8d54-8209f39a50f9
DEBUG 01-15 16:10:50.958043.958043 cuda_h.py:19] end group pad cost 0.004199981689453125 seconds
DEBUG 01-15 16:10:50.959449.959449 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:50.974320.974320 client.py:127] Model loaded
DEBUG 01-15 16:10:50.975864.975864 cuda_h.py:19] end wait_experts_multi_device cost 0.017064571380615234 seconds
DEBUG 01-15 16:10:50.975370.975370 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:50.979592.979592 cuda_h.py:19] end group_einsum cost 0.020214319229125977 seconds
DEBUG 01-15 16:10:50.979736.979736 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:50.983583.983583 cuda_h.py:19] end get_outputs_cpu1 cost 0.00407099723815918 seconds
DEBUG 01-15 16:10:50.984722.984722 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04051518440246582 seconds
DEBUG 01-15 16:10:50.984006.984006 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.009404897689819336 seconds
DEBUG 01-15 16:10:50.984056.984056 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:50.986583.986583 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.986364.986364 cuda_h.py:19] end index_scatter cost 8.797645568847656e-05 seconds
DEBUG 01-15 16:10:50.987527.987527 cuda_h.py:19] end cpuoutputsdeal cost 0.0021653175354003906 seconds
DEBUG 01-15 16:10:50.987967.987967 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:50.987207.987207 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.987048.987048 cuda_h.py:19] end gpu_group_tensor cost 0.00016427040100097656 seconds
DEBUG 01-15 16:10:50.987049.987049 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:50.987499.987499 cuda_h.py:19] end gpu_group_tensor cost 0.00015807151794433594 seconds
DEBUG 01-15 16:10:50.987708.987708 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.988497.988497 cuda_h.py:19] end gpu_group_einsum cost 0.0005667209625244141 seconds
DEBUG 01-15 16:10:50.988351.988351 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:50.989908.989908 cuda_h.py:19] end gpu_group_einsum cost 0.0005640983581542969 seconds
DEBUG 01-15 16:10:50.989012.989012 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.989526.989526 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.989122.989122 cuda_h.py:19] end all_expert_outputs_slices cost 0.00032019615173339844 seconds
DEBUG 01-15 16:10:50.989023.989023 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.989146.989146 cuda_h.py:19] end concat_expert_out cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:10:50.989711.989711 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.990654.990654 cuda_h.py:19] end index_scatter cost 6.365776062011719e-05 seconds
DEBUG 01-15 16:10:50.990452.990452 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010018348693847656 seconds
DEBUG 01-15 16:10:50.990263.990263 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:50.990875.990875 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:50.990101.990101 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020313262939453125 seconds
DEBUG 01-15 16:10:50.990334.990334 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:50.990079.990079 cuda_h.py:19] end concat_expert_out cost 5.9604644775390625e-05 seconds
DEBUG 01-15 16:10:50.990167.990167 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:50.991687.991687 cuda_h.py:19] end index_scatter cost 6.628036499023438e-05 seconds
DEBUG 01-15 16:10:50.991688.991688 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005905628204345703 seconds
DEBUG 01-15 16:10:50.991896.991896 cuda_h.py:19] end gpu_experts_multi_device cost 0.039803504943847656 seconds
DEBUG 01-15 16:10:50.991727.991727 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_9 cost 0.05243515968322754 seconds
DEBUG 01-15 16:10:50.991828.991828 cuda_h.py:19] end prefill_layer cost 0.059488534927368164 seconds
DEBUG 01-15 16:10:50.991678.991678 lmp.py:1553] -------------------------------- end prefill layer 8 --------------------------------
DEBUG 01-15 16:10:50.991765.991765 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:50.991283.991283 lmp.py:1495] -------------------------------- start prefill layer 9 --------------------------------
DEBUG 01-15 16:10:50.991277.991277 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:50.991225.991225 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 01-15 16:10:50.991513.991513 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 4.076957702636719e-05 seconds
DEBUG 01-15 16:10:50.992414.992414 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 7.581710815429688e-05 seconds
DEBUG 01-15 16:10:50.992071.992071 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:50.992557.992557 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:50.992601.992601 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:50.992000.992000 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:50.992389.992389 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:50.992008.992008 cuda_h.py:19] end allocate_cuda_memory cost 0.00023937225341796875 seconds
DEBUG 01-15 16:10:50.992316.992316 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:50.992701.992701 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:50.992530.992530 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:50.992763.992763 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, df3cd3dd-050b-4992-ab7d-696098744d37
DEBUG 01-15 16:10:50.993674.993674 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:50.993839.993839 cuda_h.py:10] start self_attn
INFO 01-15 16:10:50.994910.994910 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, df3cd3dd-050b-4992-ab7d-696098744d37
DEBUG 01-15 16:10:50.994899.994899 cuda_h.py:19] end load_into_gpu_async cost 0.0015499591827392578 seconds
DEBUG 01-15 16:10:50.994462.994462 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:50.994241.994241 cuda_h.py:19] end restore_tensors2 cost 8.678436279296875e-05 seconds
DEBUG 01-15 16:10:50.994434.994434 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021648406982421875 seconds
INFO 01-15 16:10:50.994384.994384 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, df3cd3dd-050b-4992-ab7d-696098744d37
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:50.997297.997297 cuda_h.py:19] end self_attn cost 0.0043010711669921875 seconds
DEBUG 01-15 16:10:50.998793.998793 cuda_h.py:19] end iln_self_attn_paln cost 0.006112337112426758 seconds
DEBUG 01-15 16:10:50.998623.998623 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_10
DEBUG 01-15 16:10:50.998478.998478 cuda_h.py:10] start gate
DEBUG 01-15 16:10:50.999061.999061 cuda_h.py:19] end gate cost 0.000743865966796875 seconds
DEBUG 01-15 16:10:50.999043.999043 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:50.999364.999364 lmp.py:1912] 
DEBUG 01-15 16:10:50.999364.999364 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:50.999550.999550 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:50.999630.999630 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:50.999134.999134 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:50.999969.999969 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:50.999612.999612 lmp.py:1917] 
DEBUG 01-15 16:10:50.999612.999612 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:50.999732.999732 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:50.999813.999813 lmp.py:1935]   Expert 24 |     38 | CPU
DEBUG 01-15 16:10:50.999932.999932 lmp.py:1935]   Expert  2 |     48 | CPU
DEBUG 01-15 16:10:50.999337.999337 lmp.py:1935]   Expert 26 |     61 | CPU
DEBUG 01-15 16:10:50.999503.999503 lmp.py:1935]   Expert 32 |     65 | CPU
DEBUG 01-15 16:10:50.999908.999908 lmp.py:1935]   Expert 19 |     68 | CPU
DEBUG 01-15 16:10:50.999835.999835 lmp.py:1935]   Expert 50 |     70 | CPU
DEBUG 01-15 16:10:50.999763.999763 lmp.py:1935]   Expert 15 |     79 | CPU
DEBUG 01-15 16:10:50.999929.999929 lmp.py:1935]   Expert  4 |     80 | CPU
DEBUG 01-15 16:10:50.999572.999572 lmp.py:1935]   Expert 60 |     80 | CPU
DEBUG 01-15 16:10:50.999215.999215 lmp.py:1935]   Expert 28 |     81 | CPU
DEBUG 01-15 16:10:50.999097.999097 lmp.py:1935]   Expert  7 |     83 | CPU
DEBUG 01-15 16:10:50.999455.999455 lmp.py:1935]   Expert 59 |     90 | CPU
DEBUG 01-15 16:10:50.999859.999859 lmp.py:1935]   Expert 23 |     96 | CPU
DEBUG 01-15 16:10:50.999025.999025 lmp.py:1935]   Expert 49 |     99 | CPU
DEBUG 01-15 16:10:51.000953.000953 lmp.py:1935]   Expert  5 |    103 | CPU
DEBUG 01-15 16:10:51.000835.000835 lmp.py:1935]   Expert 12 |    104 | CPU
DEBUG 01-15 16:10:51.000001.000001 lmp.py:1935]   Expert 10 |    109 | CPU
DEBUG 01-15 16:10:51.000405.000405 lmp.py:1935]   Expert 27 |    112 | CPU
DEBUG 01-15 16:10:51.000810.000810 lmp.py:1935]   Expert 41 |    120 | CPU
DEBUG 01-15 16:10:51.000214.000214 lmp.py:1935]   Expert  3 |    125 | CPU
DEBUG 01-15 16:10:51.000381.000381 lmp.py:1935]   Expert 25 |    128 | CPU
DEBUG 01-15 16:10:51.000785.000785 lmp.py:1935]   Expert 20 |    129 | CPU
DEBUG 01-15 16:10:51.000428.000428 lmp.py:1935]   Expert 40 |    130 | CPU
DEBUG 01-15 16:10:51.000309.000309 lmp.py:1935]   Expert 13 |    132 | CPU
DEBUG 01-15 16:10:51.000621.000621 lmp.py:1935]   Expert 16 |    132 | CPU
DEBUG 01-15 16:10:51.000456.000456 lmp.py:1935]   Expert 37 |    145 | CPU
DEBUG 01-15 16:10:51.000815.000815 lmp.py:1935]   Expert 17 |    146 | CPU
DEBUG 01-15 16:10:51.000696.000696 lmp.py:1935]   Expert 35 |    146 | CPU
DEBUG 01-15 16:10:51.000816.000816 lmp.py:1935]   Expert 47 |    151 | CPU
DEBUG 01-15 16:10:51.000697.000697 lmp.py:1935]   Expert 22 |    159 | CPU
DEBUG 01-15 16:10:51.000579.000579 lmp.py:1935]   Expert 53 |    166 | CPU
DEBUG 01-15 16:10:51.000460.000460 lmp.py:1935]   Expert 39 |    171 | CPU
DEBUG 01-15 16:10:51.000249.000249 lmp.py:1935]   Expert 38 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000799.000799 lmp.py:1935]   Expert 44 |    179 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000111.000111 lmp.py:1935]   Expert 36 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000337.000337 lmp.py:1935]   Expert 52 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000577.000577 lmp.py:1935]   Expert 58 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000319.000319 lmp.py:1935]   Expert 18 |    188 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000108.000108 lmp.py:1935]   Expert 62 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000373.000373 lmp.py:1935]   Expert 11 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000162.000162 lmp.py:1935]   Expert 48 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000189.000189 lmp.py:1935]   Expert 14 |    215 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000263.000263 lmp.py:1935]   Expert 30 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000575.000575 lmp.py:1935]   Expert  1 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000125.000125 lmp.py:1935]   Expert 31 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000437.000437 lmp.py:1935]   Expert 45 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000510.000510 lmp.py:1935]   Expert 42 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000822.000822 lmp.py:1935]   Expert 51 |    242 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000896.000896 lmp.py:1935]   Expert  6 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000969.000969 lmp.py:1935]   Expert 29 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000043.000043 lmp.py:1935]   Expert 34 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000355.000355 lmp.py:1935]   Expert 33 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000905.000905 lmp.py:1935]   Expert 57 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000740.000740 lmp.py:1935]   Expert 61 |    304 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000006.000006 lmp.py:1935]   Expert 43 |    308 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000271.000271 lmp.py:1935]   Expert  0 |    320 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000822.000822 lmp.py:1935]   Expert 46 |    351 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000610.000610 lmp.py:1935]   Expert  8 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000161.000161 lmp.py:1935]   Expert  9 |    392 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000996.000996 lmp.py:1935]   Expert 54 |    392 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000308.000308 lmp.py:1935]   Expert 56 |    396 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000143.000143 lmp.py:1935]   Expert 63 |    409 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000693.000693 lmp.py:1935]   Expert 55 |    426 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.000766.000766 lmp.py:1935]   Expert 21 |    491 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.000886.000886 lmp.py:1937] 
DEBUG 01-15 16:10:51.000886.000886 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.000437.000437 lmp.py:1938]   CPU:   3446 tokens
DEBUG 01-15 16:10:51.001325.001325 lmp.py:1942]   cuda:1:   4421 tokens (16 experts)
DEBUG 01-15 16:10:51.001306.001306 lmp.py:1942]   cuda:2:   4421 tokens (16 experts)
DEBUG 01-15 16:10:51.001949.001949 lmp.py:1943]   Total GPU:   8842 tokens
DEBUG 01-15 16:10:51.001830.001830 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.001830.001830 lmp.py:1944] 
DEBUG 01-15 16:10:51.001931.001931 cuda_h.py:19] end experts_map_get cost 0.0019519329071044922 seconds
DEBUG 01-15 16:10:51.001940.001940 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.001365.001365 lmp.py:1953] 
DEBUG 01-15 16:10:51.001365.001365 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.001055.001055 cuda_h.py:19] end cpu_experts_submit cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:51.001751.001751 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.001727.001727 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.001980.001980 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.9.mlp.experts.34.gate_proj.weight': 0, 'model.layers.9.mlp.experts.34.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.34.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.38.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.38.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.38.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.42.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.42.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.42.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.43.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.43.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.43.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.52.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.52.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.52.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.54.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.54.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.54.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.9.mlp.experts.0.gate_proj.weight': 0, 'model.layers.9.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.9.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.9.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.9.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.9.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.9.mlp.experts.1.gate_proj.weight': 34603008, 'model.layers.9.mlp.experts.1.down_proj.weight': 40370176, 'model.layers.9.mlp.experts.1.up_proj.weight': 46137344, 'model.layers.9.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.9.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.9.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.9.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.9.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.9.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.9.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.9.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.9.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.9.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.9.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.9.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.9.mlp.experts.11.gate_proj.weight': 121110528, 'model.layers.9.mlp.experts.11.down_proj.weight': 126877696, 'model.layers.9.mlp.experts.11.up_proj.weight': 132644864, 'model.layers.9.mlp.experts.44.gate_proj.weight': 138412032, 'model.layers.9.mlp.experts.44.down_proj.weight': 144179200, 'model.layers.9.mlp.experts.44.up_proj.weight': 149946368, 'model.layers.9.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.9.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.9.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.9.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.9.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.9.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.9.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.9.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.9.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.9.mlp.experts.31.gate_proj.weight': 207618048, 'model.layers.9.mlp.experts.31.down_proj.weight': 213385216, 'model.layers.9.mlp.experts.31.up_proj.weight': 219152384, 'model.layers.9.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.9.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.9.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.9.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.9.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.9.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.9.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.9.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.9.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(12307136512, 5767168, 0, 0), (12312903680, 5767168, 5767168, 0), (12301369344, 5767168, 11534336, 0), (12376342528, 5767168, 17301504, 0), (12382109696, 5767168, 23068672, 0), (12370575360, 5767168, 28835840, 0), (12445548544, 5767168, 34603008, 0), (12451315712, 5767168, 40370176, 0), (12439781376, 5767168, 46137344, 0), (12462850048, 5767168, 51904512, 0), (12468617216, 5767168, 57671680, 0), (12457082880, 5767168, 63438848, 0), (12497453056, 5767168, 69206016, 0), (12503220224, 5767168, 74973184, 0), (12491685888, 5767168, 80740352, 0), (12514754560, 5767168, 86507520, 0), (12520521728, 5767168, 92274688, 0), (12508987392, 5767168, 98041856, 0), (11961106432, 5767168, 103809024, 0), (11966873600, 5767168, 109576192, 0), (11955339264, 5767168, 115343360, 0), (12549357568, 5767168, 121110528, 0), (12555124736, 5767168, 126877696, 0), (12543590400, 5767168, 132644864, 0), (12618563584, 5767168, 138412032, 0), (12624330752, 5767168, 144179200, 0), (12612796416, 5767168, 149946368, 0), (12082216960, 5767168, 155713536, 0), (12087984128, 5767168, 161480704, 0), (12076449792, 5767168, 167247872, 0), (12653166592, 5767168, 173015040, 0), (12658933760, 5767168, 178782208, 0), (12647399424, 5767168, 184549376, 0), (12687769600, 5767168, 190316544, 0), (12693536768, 5767168, 196083712, 0), (12682002432, 5767168, 201850880, 0), (12705071104, 5767168, 207618048, 0), (12710838272, 5767168, 213385216, 0), (12699303936, 5767168, 219152384, 0), (12722372608, 5767168, 224919552, 0), (12728139776, 5767168, 230686720, 0), (12716605440, 5767168, 236453888, 0), (12220628992, 5767168, 242221056, 0), (12226396160, 5767168, 247988224, 0), (12214861824, 5767168, 253755392, 0), (12237930496, 5767168, 259522560, 0), (12243697664, 5767168, 265289728, 0), (12232163328, 5767168, 271056896, 0)], 2: [(11718885376, 5767168, 0, 0), (11724652544, 5767168, 5767168, 0), (11713118208, 5767168, 11534336, 0), (12289835008, 5767168, 17301504, 0), (12295602176, 5767168, 23068672, 0), (12284067840, 5767168, 28835840, 0), (11736186880, 5767168, 34603008, 0), (11741954048, 5767168, 40370176, 0), (11730419712, 5767168, 46137344, 0), (12341739520, 5767168, 51904512, 0), (12347506688, 5767168, 57671680, 0), (12335972352, 5767168, 63438848, 0), (11822694400, 5767168, 69206016, 0), (11828461568, 5767168, 74973184, 0), (11816927232, 5767168, 80740352, 0), (11857297408, 5767168, 86507520, 0), (11863064576, 5767168, 92274688, 0), (11851530240, 5767168, 98041856, 0), (11874598912, 5767168, 103809024, 0), (11880366080, 5767168, 109576192, 0), (11868831744, 5767168, 115343360, 0), (11909201920, 5767168, 121110528, 0), (11914969088, 5767168, 126877696, 0), (11903434752, 5767168, 132644864, 0), (12480151552, 5767168, 138412032, 0), (12485918720, 5767168, 144179200, 0), (12474384384, 5767168, 149946368, 0), (12030312448, 5767168, 155713536, 0), (12036079616, 5767168, 161480704, 0), (12024545280, 5767168, 167247872, 0), (12601262080, 5767168, 173015040, 0), (12607029248, 5767168, 178782208, 0), (12595494912, 5767168, 184549376, 0), (12670468096, 5767168, 190316544, 0), (12676235264, 5767168, 196083712, 0), (12664700928, 5767168, 201850880, 0), (12255232000, 5767168, 207618048, 0), (12260999168, 5767168, 213385216, 0), (12249464832, 5767168, 219152384, 0), (12774277120, 5767168, 224919552, 0), (12780044288, 5767168, 230686720, 0), (12768509952, 5767168, 236453888, 0), (12791578624, 5767168, 242221056, 0), (12797345792, 5767168, 247988224, 0), (12785811456, 5767168, 253755392, 0), (12808880128, 5767168, 259522560, 0), (12814647296, 5767168, 265289728, 0), (12803112960, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e6c7dd980>, 2: <capsule object NULL at 0x7a4ec45c9dd0>}
INFO 01-15 16:10:51.002098.002098 client.py:127] Model loaded
DEBUG 01-15 16:10:51.002393.002393 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.002833.002833 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.002979.002979 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:51.002378.002378 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.002148.002148 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 431b5dfe-015f-4685-affe-12795eec7ba8
DEBUG 01-15 16:10:51.003505.003505 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.003612.003612 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.003635.003635 cuda_h.py:19] end restore2model cost 0.0006971359252929688 seconds
DEBUG 01-15 16:10:51.003941.003941 cuda_h.py:19] end sllm_worker_task cost 0.011099100112915039 seconds
DEBUG 01-15 16:10:51.003628.003628 cuda_h.py:19] end move_flatidxs cost 0.0008325576782226562 seconds
DEBUG 01-15 16:10:51.003689.003689 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:51.004045.004045 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 431b5dfe-015f-4685-affe-12795eec7ba8
DEBUG 01-15 16:10:51.005460.005460 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003665924072265625 seconds
DEBUG 01-15 16:10:51.005767.005767 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.008243.008243 cuda_h.py:19] end restore2model cost 0.0030863285064697266 seconds
DEBUG 01-15 16:10:51.008808.008808 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.007004737854003906 seconds
DEBUG 01-15 16:10:51.008770.008770 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.008159.008159 cuda_h.py:19] end gpu_sexperts cost 0.0003147125244140625 seconds
DEBUG 01-15 16:10:51.008657.008657 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.008341.008341 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:10:51.008229.008229 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.008078.008078 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.009393.009393 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010766983032226562 seconds
DEBUG 01-15 16:10:51.010813.010813 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.010230.010230 cuda_h.py:19] end gpu_group_list cost 0.0001728534698486328 seconds
DEBUG 01-15 16:10:51.010940.010940 cuda_h.py:19] end group_tensors cost 0.006254434585571289 seconds
DEBUG 01-15 16:10:51.011517.011517 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.011932.011932 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.013219.013219 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001996278762817383 seconds
DEBUG 01-15 16:10:51.013238.013238 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.013818.013818 cuda_h.py:19] end gpu_group_list cost 0.0002827644348144531 seconds
DEBUG 01-15 16:10:51.014197.014197 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.015657.015657 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 431b5dfe-015f-4685-affe-12795eec7ba8
DEBUG 01-15 16:10:51.015099.015099 cuda_h.py:19] end group pad cost 0.004093170166015625 seconds
DEBUG 01-15 16:10:51.015743.015743 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.030761.030761 client.py:127] Model loaded
DEBUG 01-15 16:10:51.030204.030204 cuda_h.py:19] end wait_experts_multi_device cost 0.015743494033813477 seconds
DEBUG 01-15 16:10:51.030517.030517 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.033438.033438 cuda_h.py:19] end group_einsum cost 0.017700672149658203 seconds
DEBUG 01-15 16:10:51.033172.033172 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.037563.037563 cuda_h.py:19] end get_outputs_cpu1 cost 0.003693819046020508 seconds
DEBUG 01-15 16:10:51.037748.037748 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03476667404174805 seconds
DEBUG 01-15 16:10:51.038201.038201 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007099151611328125 seconds
DEBUG 01-15 16:10:51.038178.038178 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.039167.039167 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.039471.039471 cuda_h.py:19] end index_scatter cost 8.988380432128906e-05 seconds
DEBUG 01-15 16:10:51.040455.040455 cuda_h.py:19] end cpuoutputsdeal cost 0.0020923614501953125 seconds
DEBUG 01-15 16:10:51.040371.040371 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.040684.040684 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.040440.040440 cuda_h.py:19] end gpu_group_tensor cost 0.0001709461212158203 seconds
DEBUG 01-15 16:10:51.040693.040693 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.040964.040964 cuda_h.py:19] end gpu_group_tensor cost 0.0001659393310546875 seconds
DEBUG 01-15 16:10:51.041511.041511 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.041367.041367 cuda_h.py:19] end gpu_group_einsum cost 0.0005800724029541016 seconds
DEBUG 01-15 16:10:51.041849.041849 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.042289.042289 cuda_h.py:19] end gpu_group_einsum cost 0.0004477500915527344 seconds
DEBUG 01-15 16:10:51.042558.042558 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.042284.042284 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.042831.042831 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002574920654296875 seconds
DEBUG 01-15 16:10:51.042686.042686 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.042292.042292 cuda_h.py:19] end concat_expert_out cost 5.9604644775390625e-05 seconds
DEBUG 01-15 16:10:51.043533.043533 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.043006.043006 cuda_h.py:19] end index_scatter cost 6.628036499023438e-05 seconds
DEBUG 01-15 16:10:51.043141.043141 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009400844573974609 seconds
DEBUG 01-15 16:10:51.043761.043761 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.043756.043756 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.043334.043334 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002148151397705078 seconds
DEBUG 01-15 16:10:51.043759.043759 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.043319.043319 cuda_h.py:19] end concat_expert_out cost 6.151199340820312e-05 seconds
DEBUG 01-15 16:10:51.044838.044838 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.044828.044828 cuda_h.py:19] end index_scatter cost 6.103515625e-05 seconds
DEBUG 01-15 16:10:51.044928.044928 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006098747253417969 seconds
DEBUG 01-15 16:10:51.044620.044620 cuda_h.py:19] end gpu_experts_multi_device cost 0.03537416458129883 seconds
DEBUG 01-15 16:10:51.044835.044835 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_10 cost 0.04605245590209961 seconds
DEBUG 01-15 16:10:51.044699.044699 cuda_h.py:19] end prefill_layer cost 0.05296969413757324 seconds
DEBUG 01-15 16:10:51.044484.044484 lmp.py:1553] -------------------------------- end prefill layer 9 --------------------------------
DEBUG 01-15 16:10:51.044240.044240 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.044949.044949 lmp.py:1495] -------------------------------- start prefill layer 10 --------------------------------
DEBUG 01-15 16:10:51.045897.045897 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:51.045236.045236 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 01-15 16:10:51.045292.045292 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 4.3392181396484375e-05 seconds
DEBUG 01-15 16:10:51.045624.045624 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 8.082389831542969e-05 seconds
DEBUG 01-15 16:10:51.045327.045327 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.045429.045429 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.045447.045447 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.045608.045608 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.045175.045175 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.045456.045456 cuda_h.py:19] end allocate_cuda_memory cost 0.00023794174194335938 seconds
DEBUG 01-15 16:10:51.045213.045213 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.045407.045407 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.046336.046336 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.046376.046376 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 90d16a3c-26f1-4469-a325-99f2d02bc41d
DEBUG 01-15 16:10:51.046327.046327 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.046220.046220 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.047754.047754 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 90d16a3c-26f1-4469-a325-99f2d02bc41d
DEBUG 01-15 16:10:51.047790.047790 cuda_h.py:19] end load_into_gpu_async cost 0.0015459060668945312 seconds
DEBUG 01-15 16:10:51.047592.047592 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.047510.047510 cuda_h.py:19] end restore_tensors2 cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:10:51.047557.047557 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021352767944335938 seconds
INFO 01-15 16:10:51.047560.047560 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 90d16a3c-26f1-4469-a325-99f2d02bc41d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.050912.050912 cuda_h.py:19] end self_attn cost 0.004369497299194336 seconds
DEBUG 01-15 16:10:51.051010.051010 cuda_h.py:19] end iln_self_attn_paln cost 0.006120204925537109 seconds
DEBUG 01-15 16:10:51.051985.051985 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_11
DEBUG 01-15 16:10:51.051225.051225 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.052126.052126 cuda_h.py:19] end gate cost 0.0007641315460205078 seconds
DEBUG 01-15 16:10:51.052532.052532 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.052377.052377 lmp.py:1912] 
DEBUG 01-15 16:10:51.052377.052377 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.052901.052901 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.052650.052650 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.052631.052631 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.052943.052943 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.052301.052301 lmp.py:1917] 
DEBUG 01-15 16:10:51.052301.052301 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.052660.052660 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.052978.052978 lmp.py:1935]   Expert 43 |     17 | CPU
DEBUG 01-15 16:10:51.052575.052575 lmp.py:1935]   Expert 27 |     32 | CPU
DEBUG 01-15 16:10:51.053456.053456 lmp.py:1935]   Expert 26 |     52 | CPU
DEBUG 01-15 16:10:51.053338.053338 lmp.py:1935]   Expert 56 |     52 | CPU
DEBUG 01-15 16:10:51.053173.053173 lmp.py:1935]   Expert 34 |     53 | CPU
DEBUG 01-15 16:10:51.053008.053008 lmp.py:1935]   Expert  3 |     58 | CPU
DEBUG 01-15 16:10:51.053320.053320 lmp.py:1935]   Expert  4 |     67 | CPU
DEBUG 01-15 16:10:51.053678.053678 lmp.py:1935]   Expert 61 |     80 | CPU
DEBUG 01-15 16:10:51.053321.053321 lmp.py:1935]   Expert 14 |     94 | CPU
DEBUG 01-15 16:10:51.053726.053726 lmp.py:1935]   Expert 38 |    101 | CPU
DEBUG 01-15 16:10:51.053130.053130 lmp.py:1935]   Expert  2 |    112 | CPU
DEBUG 01-15 16:10:51.053535.053535 lmp.py:1935]   Expert 17 |    120 | CPU
DEBUG 01-15 16:10:51.053701.053701 lmp.py:1935]   Expert 22 |    123 | CPU
DEBUG 01-15 16:10:51.053867.053867 lmp.py:1935]   Expert 47 |    127 | CPU
DEBUG 01-15 16:10:51.053033.053033 lmp.py:1935]   Expert 37 |    131 | CPU
DEBUG 01-15 16:10:51.053438.053438 lmp.py:1935]   Expert 55 |    131 | CPU
DEBUG 01-15 16:10:51.053988.053988 lmp.py:1935]   Expert 54 |    134 | CPU
DEBUG 01-15 16:10:51.053823.053823 lmp.py:1935]   Expert 28 |    136 | CPU
DEBUG 01-15 16:10:51.053897.053897 lmp.py:1935]   Expert  5 |    141 | CPU
DEBUG 01-15 16:10:51.053493.053493 lmp.py:1935]   Expert  7 |    142 | CPU
DEBUG 01-15 16:10:51.053851.053851 lmp.py:1935]   Expert 15 |    146 | CPU
DEBUG 01-15 16:10:51.053971.053971 lmp.py:1935]   Expert 48 |    147 | CPU
DEBUG 01-15 16:10:51.053091.053091 lmp.py:1935]   Expert 51 |    147 | CPU
DEBUG 01-15 16:10:51.053211.053211 lmp.py:1935]   Expert 45 |    148 | CPU
DEBUG 01-15 16:10:51.053569.053569 lmp.py:1935]   Expert 60 |    151 | CPU
DEBUG 01-15 16:10:51.053927.053927 lmp.py:1935]   Expert 12 |    154 | CPU
DEBUG 01-15 16:10:51.053286.053286 lmp.py:1935]   Expert 19 |    155 | CPU
DEBUG 01-15 16:10:51.053644.053644 lmp.py:1935]   Expert 63 |    156 | CPU
DEBUG 01-15 16:10:51.053717.053717 lmp.py:1935]   Expert 57 |    165 | CPU
DEBUG 01-15 16:10:51.053552.053552 lmp.py:1935]   Expert  6 |    167 | CPU
DEBUG 01-15 16:10:51.053387.053387 lmp.py:1935]   Expert 52 |    175 | CPU
DEBUG 01-15 16:10:51.053938.053938 lmp.py:1935]   Expert 50 |    178 | CPU
DEBUG 01-15 16:10:51.053680.053680 lmp.py:1935]   Expert 18 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053707.053707 lmp.py:1935]   Expert 44 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053735.053735 lmp.py:1935]   Expert 31 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053285.053285 lmp.py:1935]   Expert 13 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053074.053074 lmp.py:1935]   Expert 30 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053624.053624 lmp.py:1935]   Expert 23 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053174.053174 lmp.py:1935]   Expert 53 |    196 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053440.053440 lmp.py:1935]   Expert 39 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053467.053467 lmp.py:1935]   Expert 20 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053177.053177 lmp.py:1935]   Expert 21 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053965.053965 lmp.py:1935]   Expert 29 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053562.053562 lmp.py:1935]   Expert 59 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053636.053636 lmp.py:1935]   Expert 16 |    212 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053709.053709 lmp.py:1935]   Expert 36 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053544.053544 lmp.py:1935]   Expert 25 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053902.053902 lmp.py:1935]   Expert 41 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053261.053261 lmp.py:1935]   Expert 32 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053334.053334 lmp.py:1935]   Expert 49 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053646.053646 lmp.py:1935]   Expert 46 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053196.053196 lmp.py:1935]   Expert  8 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053747.053747 lmp.py:1935]   Expert 42 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.053297.053297 lmp.py:1935]   Expert 10 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.053609.053609 lmp.py:1935]   Expert 62 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.054205.054205 lmp.py:1935]   Expert 35 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.054041.054041 lmp.py:1935]   Expert 33 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.054114.054114 lmp.py:1935]   Expert 58 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.054949.054949 lmp.py:1935]   Expert  9 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.054023.054023 lmp.py:1935]   Expert 40 |    386 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.054024.054024 lmp.py:1935]   Expert 11 |    421 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.054574.054574 lmp.py:1935]   Expert  0 |    425 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.054555.054555 lmp.py:1935]   Expert 24 |    570 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.054582.054582 lmp.py:1935]   Expert  1 |    647 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.054179.054179 lmp.py:1937] 
DEBUG 01-15 16:10:51.054179.054179 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.054206.054206 lmp.py:1938]   CPU:   3792 tokens
DEBUG 01-15 16:10:51.054756.054756 lmp.py:1942]   cuda:1:   4249 tokens (16 experts)
DEBUG 01-15 16:10:51.054830.054830 lmp.py:1942]   cuda:2:   4247 tokens (16 experts)
DEBUG 01-15 16:10:51.054949.054949 lmp.py:1943]   Total GPU:   8496 tokens
DEBUG 01-15 16:10:51.054069.054069 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.054069.054069 lmp.py:1944] 
DEBUG 01-15 16:10:51.054103.054103 cuda_h.py:19] end experts_map_get cost 0.002004861831665039 seconds
DEBUG 01-15 16:10:51.054629.054629 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.054007.054007 lmp.py:1953] 
DEBUG 01-15 16:10:51.054007.054007 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.054698.054698 cuda_h.py:19] end cpu_experts_submit cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:51.054871.054871 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.054369.054369 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.055827.055827 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.10.mlp.experts.32.gate_proj.weight': 0, 'model.layers.10.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.42.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.42.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.42.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.11.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.11.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.11.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.41.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.41.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.41.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.13.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.13.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.13.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.44.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.44.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.44.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.20.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.20.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.20.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.59.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.59.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.59.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.10.mlp.experts.0.gate_proj.weight': 0, 'model.layers.10.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.10.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.10.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.10.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.10.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.10.mlp.experts.39.gate_proj.weight': 34603008, 'model.layers.10.mlp.experts.39.down_proj.weight': 40370176, 'model.layers.10.mlp.experts.39.up_proj.weight': 46137344, 'model.layers.10.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.10.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.10.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.10.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.10.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.10.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.10.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.10.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.10.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.10.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.10.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.10.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.10.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.10.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.10.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.10.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.10.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.10.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.10.mlp.experts.30.gate_proj.weight': 155713536, 'model.layers.10.mlp.experts.30.down_proj.weight': 161480704, 'model.layers.10.mlp.experts.30.up_proj.weight': 167247872, 'model.layers.10.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.10.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.10.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.10.mlp.experts.23.gate_proj.weight': 190316544, 'model.layers.10.mlp.experts.23.down_proj.weight': 196083712, 'model.layers.10.mlp.experts.23.up_proj.weight': 201850880, 'model.layers.10.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.10.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.10.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.10.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.10.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.10.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.10.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.10.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.10.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.10.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.10.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.10.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(13379829760, 5767168, 0, 0), (13385596928, 5767168, 5767168, 0), (13374062592, 5767168, 11534336, 0), (12843483136, 5767168, 17301504, 0), (12849250304, 5767168, 23068672, 0), (12837715968, 5767168, 28835840, 0), (13431734272, 5767168, 34603008, 0), (13437501440, 5767168, 40370176, 0), (13425967104, 5767168, 46137344, 0), (13449035776, 5767168, 51904512, 0), (13454802944, 5767168, 57671680, 0), (13443268608, 5767168, 63438848, 0), (12964593664, 5767168, 69206016, 0), (12970360832, 5767168, 74973184, 0), (12958826496, 5767168, 80740352, 0), (12981895168, 5767168, 86507520, 0), (12987662336, 5767168, 92274688, 0), (12976128000, 5767168, 98041856, 0), (13552844800, 5767168, 103809024, 0), (13558611968, 5767168, 109576192, 0), (13547077632, 5767168, 115343360, 0), (13016498176, 5767168, 121110528, 0), (13022265344, 5767168, 126877696, 0), (13010731008, 5767168, 132644864, 0), (13535543296, 5767168, 138412032, 0), (13541310464, 5767168, 144179200, 0), (13529776128, 5767168, 149946368, 0), (13051101184, 5767168, 155713536, 0), (13056868352, 5767168, 161480704, 0), (13045334016, 5767168, 167247872, 0), (13587447808, 5767168, 173015040, 0), (13593214976, 5767168, 178782208, 0), (13581680640, 5767168, 184549376, 0), (13172211712, 5767168, 190316544, 0), (13177978880, 5767168, 196083712, 0), (13166444544, 5767168, 201850880, 0), (13743161344, 5767168, 207618048, 0), (13748928512, 5767168, 213385216, 0), (13737394176, 5767168, 219152384, 0), (13829668864, 5767168, 224919552, 0), (13835436032, 5767168, 230686720, 0), (13823901696, 5767168, 236453888, 0), (13846970368, 5767168, 242221056, 0), (13852737536, 5767168, 247988224, 0), (13841203200, 5767168, 253755392, 0), (13362528256, 5767168, 259522560, 0), (13368295424, 5767168, 265289728, 0), (13356761088, 5767168, 271056896, 0)], 2: [(12826181632, 5767168, 0, 0), (12831948800, 5767168, 5767168, 0), (12820414464, 5767168, 11534336, 0), (13397131264, 5767168, 17301504, 0), (13402898432, 5767168, 23068672, 0), (13391364096, 5767168, 28835840, 0), (13500940288, 5767168, 34603008, 0), (13506707456, 5767168, 40370176, 0), (13495173120, 5767168, 46137344, 0), (13518241792, 5767168, 51904512, 0), (13524008960, 5767168, 57671680, 0), (13512474624, 5767168, 63438848, 0), (12999196672, 5767168, 69206016, 0), (13004963840, 5767168, 74973184, 0), (12993429504, 5767168, 80740352, 0), (13622050816, 5767168, 86507520, 0), (13627817984, 5767168, 92274688, 0), (13616283648, 5767168, 98041856, 0), (13103005696, 5767168, 103809024, 0), (13108772864, 5767168, 109576192, 0), (13097238528, 5767168, 115343360, 0), (13673955328, 5767168, 121110528, 0), (13679722496, 5767168, 126877696, 0), (13668188160, 5767168, 132644864, 0), (13137608704, 5767168, 138412032, 0), (13143375872, 5767168, 144179200, 0), (13131841536, 5767168, 149946368, 0), (13345226752, 5767168, 155713536, 0), (13350993920, 5767168, 161480704, 0), (13339459584, 5767168, 167247872, 0), (13189513216, 5767168, 173015040, 0), (13195280384, 5767168, 178782208, 0), (13183746048, 5767168, 184549376, 0), (13224116224, 5767168, 190316544, 0), (13229883392, 5767168, 196083712, 0), (13218349056, 5767168, 201850880, 0), (13241417728, 5767168, 207618048, 0), (13247184896, 5767168, 213385216, 0), (13235650560, 5767168, 219152384, 0), (13258719232, 5767168, 224919552, 0), (13264486400, 5767168, 230686720, 0), (13252952064, 5767168, 236453888, 0), (13327925248, 5767168, 242221056, 0), (13333692416, 5767168, 247988224, 0), (13322158080, 5767168, 253755392, 0), (13898874880, 5767168, 259522560, 0), (13904642048, 5767168, 265289728, 0), (13893107712, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec419a190>, 2: <capsule object NULL at 0x7a51b06da460>}
INFO 01-15 16:10:51.055884.055884 client.py:127] Model loaded
DEBUG 01-15 16:10:51.055438.055438 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.055540.055540 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.055568.055568 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.055893.055893 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d31855b6-c387-465a-b8a8-6f81e104f1a5
DEBUG 01-15 16:10:51.056285.056285 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:51.056781.056781 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.056687.056687 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.056004.056004 cuda_h.py:19] end restore2model cost 0.000690460205078125 seconds
DEBUG 01-15 16:10:51.056595.056595 cuda_h.py:19] end sllm_worker_task cost 0.011098146438598633 seconds
DEBUG 01-15 16:10:51.057202.057202 cuda_h.py:19] end move_flatidxs cost 0.0008590221405029297 seconds
DEBUG 01-15 16:10:51.057946.057946 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:51.057858.057858 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d31855b6-c387-465a-b8a8-6f81e104f1a5
DEBUG 01-15 16:10:51.058830.058830 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034286975860595703 seconds
DEBUG 01-15 16:10:51.058852.058852 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.061579.061579 cuda_h.py:19] end restore2model cost 0.0030622482299804688 seconds
DEBUG 01-15 16:10:51.061667.061667 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006742238998413086 seconds
DEBUG 01-15 16:10:51.061655.061655 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.061753.061753 cuda_h.py:19] end gpu_sexperts cost 0.0003192424774169922 seconds
DEBUG 01-15 16:10:51.061205.061205 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.061610.061610 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2172927856445312e-05 seconds
DEBUG 01-15 16:10:51.061214.061214 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.061016.061016 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.062981.062981 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010993480682373047 seconds
DEBUG 01-15 16:10:51.063215.063215 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.063348.063348 cuda_h.py:19] end gpu_group_list cost 0.0001723766326904297 seconds
DEBUG 01-15 16:10:51.064850.064850 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.065927.065927 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012214183807373047 seconds
DEBUG 01-15 16:10:51.065797.065797 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.065075.065075 cuda_h.py:19] end gpu_group_list cost 0.0001747608184814453 seconds
DEBUG 01-15 16:10:51.066698.066698 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.066250.066250 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d31855b6-c387-465a-b8a8-6f81e104f1a5
DEBUG 01-15 16:10:51.067127.067127 cuda_h.py:19] end group_tensors cost 0.01051783561706543 seconds
DEBUG 01-15 16:10:51.068106.068106 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.072762.072762 cuda_h.py:19] end group pad cost 0.004117250442504883 seconds
DEBUG 01-15 16:10:51.072837.072837 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.083637.083637 client.py:127] Model loaded
DEBUG 01-15 16:10:51.083703.083703 cuda_h.py:19] end wait_experts_multi_device cost 0.017269611358642578 seconds
DEBUG 01-15 16:10:51.083294.083294 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.091687.091687 cuda_h.py:19] end group_einsum cost 0.01898050308227539 seconds
DEBUG 01-15 16:10:51.092143.092143 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.096259.096259 cuda_h.py:19] end get_outputs_cpu1 cost 0.003985404968261719 seconds
DEBUG 01-15 16:10:51.096564.096564 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04068136215209961 seconds
DEBUG 01-15 16:10:51.097045.097045 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01326751708984375 seconds
DEBUG 01-15 16:10:51.097003.097003 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.098639.098639 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.099334.099334 cuda_h.py:19] end index_scatter cost 9.012222290039062e-05 seconds
DEBUG 01-15 16:10:51.099696.099696 cuda_h.py:19] end cpuoutputsdeal cost 0.0020835399627685547 seconds
DEBUG 01-15 16:10:51.099282.099282 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.099045.099045 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.099561.099561 cuda_h.py:19] end gpu_group_tensor cost 0.00017309188842773438 seconds
DEBUG 01-15 16:10:51.099847.099847 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.100768.100768 cuda_h.py:19] end gpu_group_tensor cost 0.00015354156494140625 seconds
DEBUG 01-15 16:10:51.100685.100685 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.100771.100771 cuda_h.py:19] end gpu_group_einsum cost 0.0007126331329345703 seconds
DEBUG 01-15 16:10:51.101048.101048 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.101577.101577 cuda_h.py:19] end gpu_group_einsum cost 0.0005040168762207031 seconds
DEBUG 01-15 16:10:51.101230.101230 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.101194.101194 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.102482.102482 cuda_h.py:19] end all_expert_outputs_slices cost 0.00024175643920898438 seconds
DEBUG 01-15 16:10:51.102668.102668 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.102314.102314 cuda_h.py:19] end concat_expert_out cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:10:51.102925.102925 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.102200.102200 cuda_h.py:19] end index_scatter cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:10:51.102844.102844 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008957386016845703 seconds
DEBUG 01-15 16:10:51.102133.102133 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.102652.102652 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.103700.103700 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020265579223632812 seconds
DEBUG 01-15 16:10:51.103124.103124 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.103161.103161 cuda_h.py:19] end concat_expert_out cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:10:51.103487.103487 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.103809.103809 cuda_h.py:19] end index_scatter cost 6.175041198730469e-05 seconds
DEBUG 01-15 16:10:51.103333.103333 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0006010532379150391 seconds
DEBUG 01-15 16:10:51.103111.103111 cuda_h.py:19] end gpu_experts_multi_device cost 0.04178810119628906 seconds
DEBUG 01-15 16:10:51.103557.103557 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_11 cost 0.05225014686584473 seconds
DEBUG 01-15 16:10:51.104718.104718 cuda_h.py:19] end prefill_layer cost 0.05916619300842285 seconds
DEBUG 01-15 16:10:51.104714.104714 lmp.py:1553] -------------------------------- end prefill layer 10 --------------------------------
DEBUG 01-15 16:10:51.104278.104278 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.104272.104272 lmp.py:1495] -------------------------------- start prefill layer 11 --------------------------------
DEBUG 01-15 16:10:51.104836.104836 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:51.104977.104977 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 01-15 16:10:51.104052.104052 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 5.9604644775390625e-05 seconds
DEBUG 01-15 16:10:51.104292.104292 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 9.918212890625e-05 seconds
DEBUG 01-15 16:10:51.104518.104518 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.104335.104335 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.104956.104956 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.104785.104785 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.104207.104207 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.105541.105541 cuda_h.py:19] end allocate_cuda_memory cost 0.00024008750915527344 seconds
DEBUG 01-15 16:10:51.105564.105564 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.105141.105141 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.105063.105063 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.105819.105819 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 7d9595b6-987d-4d80-9f9c-e100e9a5a8fe
DEBUG 01-15 16:10:51.105108.105108 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.105458.105458 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.106880.106880 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 7d9595b6-987d-4d80-9f9c-e100e9a5a8fe
DEBUG 01-15 16:10:51.106154.106154 cuda_h.py:19] end load_into_gpu_async cost 0.0015537738800048828 seconds
DEBUG 01-15 16:10:51.106002.106002 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.107066.107066 cuda_h.py:19] end restore_tensors2 cost 8.58306884765625e-05 seconds
DEBUG 01-15 16:10:51.107398.107398 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021660327911376953 seconds
INFO 01-15 16:10:51.107731.107731 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 7d9595b6-987d-4d80-9f9c-e100e9a5a8fe
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.110870.110870 cuda_h.py:19] end self_attn cost 0.004335880279541016 seconds
DEBUG 01-15 16:10:51.110216.110216 cuda_h.py:19] end iln_self_attn_paln cost 0.006166696548461914 seconds
DEBUG 01-15 16:10:51.110761.110761 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_12
DEBUG 01-15 16:10:51.110570.110570 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.111569.111569 cuda_h.py:19] end gate cost 0.0007348060607910156 seconds
DEBUG 01-15 16:10:51.111452.111452 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.112885.112885 lmp.py:1912] 
DEBUG 01-15 16:10:51.112885.112885 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.112834.112834 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.112152.112152 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.112133.112133 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.112253.112253 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.112896.112896 lmp.py:1917] 
DEBUG 01-15 16:10:51.112896.112896 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.112016.112016 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.112858.112858 lmp.py:1935]   Expert 13 |     16 | CPU
DEBUG 01-15 16:10:51.112693.112693 lmp.py:1935]   Expert 39 |     16 | CPU
DEBUG 01-15 16:10:51.112812.112812 lmp.py:1935]   Expert 49 |     38 | CPU
DEBUG 01-15 16:10:51.112217.112217 lmp.py:1935]   Expert 35 |     55 | CPU
DEBUG 01-15 16:10:51.112860.112860 lmp.py:1935]   Expert 19 |     64 | CPU
DEBUG 01-15 16:10:51.112933.112933 lmp.py:1935]   Expert  9 |     69 | CPU
DEBUG 01-15 16:10:51.112053.112053 lmp.py:1935]   Expert 32 |     72 | CPU
DEBUG 01-15 16:10:51.112710.112710 lmp.py:1935]   Expert 26 |     75 | CPU
DEBUG 01-15 16:10:51.112068.112068 lmp.py:1935]   Expert 41 |     79 | CPU
DEBUG 01-15 16:10:51.112711.112711 lmp.py:1935]   Expert 33 |     81 | CPU
DEBUG 01-15 16:10:51.112116.112116 lmp.py:1935]   Expert 23 |     87 | CPU
DEBUG 01-15 16:10:51.112520.112520 lmp.py:1935]   Expert 46 |     87 | CPU
DEBUG 01-15 16:10:51.112686.112686 lmp.py:1935]   Expert 18 |     90 | CPU
DEBUG 01-15 16:10:51.112852.112852 lmp.py:1935]   Expert 31 |     91 | CPU
DEBUG 01-15 16:10:51.112495.112495 lmp.py:1935]   Expert 38 |    100 | CPU
DEBUG 01-15 16:10:51.112377.112377 lmp.py:1935]   Expert 17 |    102 | CPU
DEBUG 01-15 16:10:51.112781.112781 lmp.py:1935]   Expert  3 |    105 | CPU
DEBUG 01-15 16:10:51.112332.112332 lmp.py:1935]   Expert  6 |    107 | CPU
DEBUG 01-15 16:10:51.112690.112690 lmp.py:1935]   Expert 20 |    117 | CPU
DEBUG 01-15 16:10:51.112287.112287 lmp.py:1935]   Expert 40 |    128 | CPU
DEBUG 01-15 16:10:51.112406.112406 lmp.py:1935]   Expert 61 |    130 | CPU
DEBUG 01-15 16:10:51.112526.112526 lmp.py:1935]   Expert 62 |    131 | CPU
DEBUG 01-15 16:10:51.112553.112553 lmp.py:1935]   Expert 44 |    132 | CPU
DEBUG 01-15 16:10:51.112150.112150 lmp.py:1935]   Expert 15 |    133 | CPU
DEBUG 01-15 16:10:51.112508.112508 lmp.py:1935]   Expert 43 |    136 | CPU
DEBUG 01-15 16:10:51.112866.112866 lmp.py:1935]   Expert 50 |    137 | CPU
DEBUG 01-15 16:10:51.112986.112986 lmp.py:1935]   Expert 16 |    140 | CPU
DEBUG 01-15 16:10:51.112344.112344 lmp.py:1935]   Expert 42 |    140 | CPU
DEBUG 01-15 16:10:51.112941.112941 lmp.py:1935]   Expert 63 |    140 | CPU
DEBUG 01-15 16:10:51.112061.112061 lmp.py:1935]   Expert 59 |    143 | CPU
DEBUG 01-15 16:10:51.112658.112658 lmp.py:1935]   Expert  2 |    146 | CPU
DEBUG 01-15 16:10:51.112493.112493 lmp.py:1935]   Expert 36 |    152 | CPU
DEBUG 01-15 16:10:51.112235.112235 lmp.py:1935]   Expert 10 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.112977.112977 lmp.py:1935]   Expert  5 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.112766.112766 lmp.py:1935]   Expert 34 |    185 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.112317.112317 lmp.py:1935]   Expert 27 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.112628.112628 lmp.py:1935]   Expert 52 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.112940.112940 lmp.py:1935]   Expert 45 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.112491.112491 lmp.py:1935]   Expert 60 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.112279.112279 lmp.py:1935]   Expert 48 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113591.113591 lmp.py:1935]   Expert 51 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113903.113903 lmp.py:1935]   Expert 56 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113215.113215 lmp.py:1935]   Expert 24 |    229 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113527.113527 lmp.py:1935]   Expert  7 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113600.113600 lmp.py:1935]   Expert 53 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113436.113436 lmp.py:1935]   Expert  8 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113509.113509 lmp.py:1935]   Expert 57 |    251 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113821.113821 lmp.py:1935]   Expert 47 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113371.113371 lmp.py:1935]   Expert 29 |    263 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113445.113445 lmp.py:1935]   Expert 21 |    264 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113518.113518 lmp.py:1935]   Expert  4 |    286 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113069.113069 lmp.py:1935]   Expert  0 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113619.113619 lmp.py:1935]   Expert 14 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113692.113692 lmp.py:1935]   Expert 22 |    314 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113243.113243 lmp.py:1935]   Expert 55 |    316 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113316.113316 lmp.py:1935]   Expert 37 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113390.113390 lmp.py:1935]   Expert 58 |    319 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113940.113940 lmp.py:1935]   Expert  1 |    320 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113490.113490 lmp.py:1935]   Expert 54 |    334 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113325.113325 lmp.py:1935]   Expert 28 |    359 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113637.113637 lmp.py:1935]   Expert 12 |    380 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113618.113618 lmp.py:1935]   Expert 25 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113976.113976 lmp.py:1935]   Expert 11 |    401 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.113335.113335 lmp.py:1935]   Expert 30 |    830 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.113315.113315 lmp.py:1937] 
DEBUG 01-15 16:10:51.113315.113315 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.113118.113118 lmp.py:1938]   CPU:   3239 tokens
DEBUG 01-15 16:10:51.113906.113906 lmp.py:1942]   cuda:1:   4446 tokens (15 experts)
DEBUG 01-15 16:10:51.113218.113218 lmp.py:1942]   cuda:2:   4603 tokens (17 experts)
DEBUG 01-15 16:10:51.113053.113053 lmp.py:1943]   Total GPU:   9049 tokens
DEBUG 01-15 16:10:51.113412.113412 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.113412.113412 lmp.py:1944] 
DEBUG 01-15 16:10:51.113207.113207 cuda_h.py:19] end experts_map_get cost 0.0019659996032714844 seconds
DEBUG 01-15 16:10:51.113448.113448 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.113635.113635 lmp.py:1953] 
DEBUG 01-15 16:10:51.113635.113635 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.113517.113517 cuda_h.py:19] end cpu_experts_submit cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:51.113167.113167 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.113811.113811 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.114820.114820 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.11.mlp.experts.0.gate_proj.weight': 0, 'model.layers.11.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.60.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.60.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.60.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.51.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.51.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.51.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.52.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.52.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.52.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.30.up_proj.weight': 253755392}, 2: {'model.layers.11.mlp.experts.37.gate_proj.weight': 0, 'model.layers.11.mlp.experts.37.down_proj.weight': 5767168, 'model.layers.11.mlp.experts.37.up_proj.weight': 11534336, 'model.layers.11.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.11.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.11.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.11.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.11.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.11.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.11.mlp.experts.10.gate_proj.weight': 51904512, 'model.layers.11.mlp.experts.10.down_proj.weight': 57671680, 'model.layers.11.mlp.experts.10.up_proj.weight': 63438848, 'model.layers.11.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.11.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.11.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.11.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.11.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.11.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.11.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.11.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.11.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.11.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.11.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.11.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.11.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.11.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.11.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.11.mlp.experts.21.gate_proj.weight': 155713536, 'model.layers.11.mlp.experts.21.down_proj.weight': 161480704, 'model.layers.11.mlp.experts.21.up_proj.weight': 167247872, 'model.layers.11.mlp.experts.54.gate_proj.weight': 173015040, 'model.layers.11.mlp.experts.54.down_proj.weight': 178782208, 'model.layers.11.mlp.experts.54.up_proj.weight': 184549376, 'model.layers.11.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.11.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.11.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.11.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.11.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.11.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.11.mlp.experts.25.gate_proj.weight': 224919552, 'model.layers.11.mlp.experts.25.down_proj.weight': 230686720, 'model.layers.11.mlp.experts.25.up_proj.weight': 236453888, 'model.layers.11.mlp.experts.27.gate_proj.weight': 242221056, 'model.layers.11.mlp.experts.27.down_proj.weight': 247988224, 'model.layers.11.mlp.experts.27.up_proj.weight': 253755392, 'model.layers.11.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.11.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.11.mlp.experts.29.up_proj.weight': 271056896, 'model.layers.11.mlp.experts.57.gate_proj.weight': 276824064, 'model.layers.11.mlp.experts.57.down_proj.weight': 282591232, 'model.layers.11.mlp.experts.57.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(13933477888, 5767168, 0, 0), (13939245056, 5767168, 5767168, 0), (13927710720, 5767168, 11534336, 0), (13950779392, 5767168, 17301504, 0), (13956546560, 5767168, 23068672, 0), (13945012224, 5767168, 28835840, 0), (14521729024, 5767168, 34603008, 0), (14527496192, 5767168, 40370176, 0), (14515961856, 5767168, 46137344, 0), (14002683904, 5767168, 51904512, 0), (14008451072, 5767168, 57671680, 0), (13996916736, 5767168, 63438848, 0), (14071889920, 5767168, 69206016, 0), (14077657088, 5767168, 74973184, 0), (14066122752, 5767168, 80740352, 0), (14971568128, 5767168, 86507520, 0), (14977335296, 5767168, 92274688, 0), (14965800960, 5767168, 98041856, 0), (14746648576, 5767168, 103809024, 0), (14752415744, 5767168, 109576192, 0), (14740881408, 5767168, 115343360, 0), (14815854592, 5767168, 121110528, 0), (14821621760, 5767168, 126877696, 0), (14810087424, 5767168, 132644864, 0), (14833156096, 5767168, 138412032, 0), (14838923264, 5767168, 144179200, 0), (14827388928, 5767168, 149946368, 0), (14850457600, 5767168, 155713536, 0), (14856224768, 5767168, 161480704, 0), (14844690432, 5767168, 167247872, 0), (14314110976, 5767168, 173015040, 0), (14319878144, 5767168, 178782208, 0), (14308343808, 5767168, 184549376, 0), (14902362112, 5767168, 190316544, 0), (14908129280, 5767168, 196083712, 0), (14896594944, 5767168, 201850880, 0), (14936965120, 5767168, 207618048, 0), (14942732288, 5767168, 213385216, 0), (14931197952, 5767168, 219152384, 0), (14417920000, 5767168, 224919552, 0), (14423687168, 5767168, 230686720, 0), (14412152832, 5767168, 236453888, 0), (14452523008, 5767168, 242221056, 0), (14458290176, 5767168, 247988224, 0), (14446755840, 5767168, 253755392, 0)], 2: [(14573633536, 5767168, 0, 0), (14579400704, 5767168, 5767168, 0), (14567866368, 5767168, 11534336, 0), (14019985408, 5767168, 17301504, 0), (14025752576, 5767168, 23068672, 0), (14014218240, 5767168, 28835840, 0), (14054588416, 5767168, 34603008, 0), (14060355584, 5767168, 40370176, 0), (14048821248, 5767168, 46137344, 0), (14106492928, 5767168, 51904512, 0), (14112260096, 5767168, 57671680, 0), (14100725760, 5767168, 63438848, 0), (14123794432, 5767168, 69206016, 0), (14129561600, 5767168, 74973184, 0), (14118027264, 5767168, 80740352, 0), (14141095936, 5767168, 86507520, 0), (14146863104, 5767168, 92274688, 0), (14135328768, 5767168, 98041856, 0), (14712045568, 5767168, 103809024, 0), (14717812736, 5767168, 109576192, 0), (14706278400, 5767168, 115343360, 0), (14175698944, 5767168, 121110528, 0), (14181466112, 5767168, 126877696, 0), (14169931776, 5767168, 132644864, 0), (14763950080, 5767168, 138412032, 0), (14769717248, 5767168, 144179200, 0), (14758182912, 5767168, 149946368, 0), (14296809472, 5767168, 155713536, 0), (14302576640, 5767168, 161480704, 0), (14291042304, 5767168, 167247872, 0), (14867759104, 5767168, 173015040, 0), (14873526272, 5767168, 178782208, 0), (14861991936, 5767168, 184549376, 0), (14885060608, 5767168, 190316544, 0), (14890827776, 5767168, 196083712, 0), (14879293440, 5767168, 201850880, 0), (14348713984, 5767168, 207618048, 0), (14354481152, 5767168, 213385216, 0), (14342946816, 5767168, 219152384, 0), (14366015488, 5767168, 224919552, 0), (14371782656, 5767168, 230686720, 0), (14360248320, 5767168, 236453888, 0), (14400618496, 5767168, 242221056, 0), (14406385664, 5767168, 247988224, 0), (14394851328, 5767168, 253755392, 0), (14435221504, 5767168, 259522560, 0), (14440988672, 5767168, 265289728, 0), (14429454336, 5767168, 271056896, 0), (14919663616, 5767168, 276824064, 0), (14925430784, 5767168, 282591232, 0), (14913896448, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4312160>, 2: <capsule object NULL at 0x7a51b06da430>}
INFO 01-15 16:10:51.114103.114103 client.py:127] Model loaded
DEBUG 01-15 16:10:51.114910.114910 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.115535.115535 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.115603.115603 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.115219.115219 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 970c0dd9-a7db-4567-8e4d-d6abded15098
DEBUG 01-15 16:10:51.115386.115386 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:51.115106.115106 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.115250.115250 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.115693.115693 cuda_h.py:19] end restore2model cost 0.0006811618804931641 seconds
DEBUG 01-15 16:10:51.115284.115284 cuda_h.py:19] end sllm_worker_task cost 0.011161327362060547 seconds
DEBUG 01-15 16:10:51.116373.116373 cuda_h.py:19] end move_flatidxs cost 0.0008378028869628906 seconds
DEBUG 01-15 16:10:51.116103.116103 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:51.116400.116400 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 970c0dd9-a7db-4567-8e4d-d6abded15098
DEBUG 01-15 16:10:51.117226.117226 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0034677982330322266 seconds
DEBUG 01-15 16:10:51.117620.117620 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.120646.120646 cuda_h.py:19] end restore2model cost 0.0031058788299560547 seconds
DEBUG 01-15 16:10:51.120132.120132 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006829977035522461 seconds
DEBUG 01-15 16:10:51.120550.120550 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.121833.121833 cuda_h.py:19] end gpu_sexperts cost 0.0003151893615722656 seconds
DEBUG 01-15 16:10:51.121093.121093 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.121837.121837 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8358230590820312e-05 seconds
DEBUG 01-15 16:10:51.121917.121917 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.121719.121719 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.122656.122656 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.001043558120727539 seconds
DEBUG 01-15 16:10:51.122566.122566 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.122837.122837 cuda_h.py:19] end gpu_group_list cost 0.00016927719116210938 seconds
DEBUG 01-15 16:10:51.123942.123942 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.124312.124312 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012841224670410156 seconds
DEBUG 01-15 16:10:51.124520.124520 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.125534.125534 cuda_h.py:19] end gpu_group_list cost 0.0001888275146484375 seconds
DEBUG 01-15 16:10:51.126941.126941 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.126983.126983 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 970c0dd9-a7db-4567-8e4d-d6abded15098
DEBUG 01-15 16:10:51.125179.125179 cuda_h.py:19] end group_tensors cost 0.009148597717285156 seconds
DEBUG 01-15 16:10:51.126885.126885 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.131931.131931 cuda_h.py:19] end group pad cost 0.00447845458984375 seconds
DEBUG 01-15 16:10:51.131813.131813 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.147086.147086 client.py:127] Model loaded
DEBUG 01-15 16:10:51.148831.148831 cuda_h.py:19] end wait_experts_multi_device cost 0.02193450927734375 seconds
DEBUG 01-15 16:10:51.148927.148927 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.150549.150549 cuda_h.py:19] end group_einsum cost 0.018927574157714844 seconds
DEBUG 01-15 16:10:51.150779.150779 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.153342.153342 cuda_h.py:19] end get_outputs_cpu1 cost 0.003509044647216797 seconds
DEBUG 01-15 16:10:51.154615.154615 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03900265693664551 seconds
DEBUG 01-15 16:10:51.154206.154206 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006671428680419922 seconds
DEBUG 01-15 16:10:51.154030.154030 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.156724.156724 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.156830.156830 cuda_h.py:19] end index_scatter cost 8.916854858398438e-05 seconds
DEBUG 01-15 16:10:51.157669.157669 cuda_h.py:19] end cpuoutputsdeal cost 0.002045154571533203 seconds
DEBUG 01-15 16:10:51.157447.157447 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.157594.157594 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.157627.157627 cuda_h.py:19] end gpu_group_tensor cost 0.00016641616821289062 seconds
DEBUG 01-15 16:10:51.157674.157674 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.157356.157356 cuda_h.py:19] end gpu_group_tensor cost 0.00015497207641601562 seconds
DEBUG 01-15 16:10:51.157174.157174 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.158724.158724 cuda_h.py:19] end gpu_group_einsum cost 0.000949859619140625 seconds
DEBUG 01-15 16:10:51.158611.158611 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.159202.159202 cuda_h.py:19] end gpu_group_einsum cost 0.0005857944488525391 seconds
DEBUG 01-15 16:10:51.159114.159114 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.159628.159628 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.160476.160476 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003299713134765625 seconds
DEBUG 01-15 16:10:51.160616.160616 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.160069.160069 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:51.160111.160111 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.160036.160036 cuda_h.py:19] end index_scatter cost 6.556510925292969e-05 seconds
DEBUG 01-15 16:10:51.160733.160733 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.001032114028930664 seconds
DEBUG 01-15 16:10:51.160161.160161 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.160633.160633 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.161223.161223 cuda_h.py:19] end all_expert_outputs_slices cost 0.00018930435180664062 seconds
DEBUG 01-15 16:10:51.161933.161933 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.161916.161916 cuda_h.py:19] end concat_expert_out cost 5.888938903808594e-05 seconds
DEBUG 01-15 16:10:51.161336.161336 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.161326.161326 cuda_h.py:19] end index_scatter cost 6.175041198730469e-05 seconds
DEBUG 01-15 16:10:51.161565.161565 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005757808685302734 seconds
DEBUG 01-15 16:10:51.161588.161588 cuda_h.py:19] end gpu_experts_multi_device cost 0.04039263725280762 seconds
DEBUG 01-15 16:10:51.161465.161465 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_12 cost 0.05087447166442871 seconds
DEBUG 01-15 16:10:51.162110.162110 cuda_h.py:19] end prefill_layer cost 0.05784749984741211 seconds
DEBUG 01-15 16:10:51.162914.162914 lmp.py:1553] -------------------------------- end prefill layer 11 --------------------------------
DEBUG 01-15 16:10:51.162239.162239 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.162234.162234 lmp.py:1495] -------------------------------- start prefill layer 12 --------------------------------
DEBUG 01-15 16:10:51.162797.162797 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:51.162461.162461 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 01-15 16:10:51.162954.162954 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:10:51.162524.162524 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 8.916854858398438e-05 seconds
DEBUG 01-15 16:10:51.162274.162274 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.162866.162866 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.162391.162391 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.162952.162952 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.162457.162457 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.163671.163671 cuda_h.py:19] end allocate_cuda_memory cost 0.00022339820861816406 seconds
DEBUG 01-15 16:10:51.163694.163694 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.163318.163318 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.163101.163101 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.163288.163288 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b930d9cf-78a0-44f7-a6b8-c453606d39de
DEBUG 01-15 16:10:51.163430.163430 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.163515.163515 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.164765.164765 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b930d9cf-78a0-44f7-a6b8-c453606d39de
DEBUG 01-15 16:10:51.164277.164277 cuda_h.py:19] end load_into_gpu_async cost 0.0015497207641601562 seconds
DEBUG 01-15 16:10:51.164080.164080 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.164289.164289 cuda_h.py:19] end restore_tensors2 cost 8.559226989746094e-05 seconds
DEBUG 01-15 16:10:51.164813.164813 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021467208862304688 seconds
INFO 01-15 16:10:51.164816.164816 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b930d9cf-78a0-44f7-a6b8-c453606d39de
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.168617.168617 cuda_h.py:19] end self_attn cost 0.004354000091552734 seconds
DEBUG 01-15 16:10:51.168506.168506 cuda_h.py:19] end iln_self_attn_paln cost 0.0061037540435791016 seconds
DEBUG 01-15 16:10:51.168050.168050 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_13
DEBUG 01-15 16:10:51.168098.168098 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.169058.169058 cuda_h.py:19] end gate cost 0.0007407665252685547 seconds
DEBUG 01-15 16:10:51.169417.169417 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.170328.170328 lmp.py:1912] 
DEBUG 01-15 16:10:51.170328.170328 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.170276.170276 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.170118.170118 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.170860.170860 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.170934.170934 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.170577.170577 lmp.py:1917] 
DEBUG 01-15 16:10:51.170577.170577 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.170173.170173 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.170015.170015 lmp.py:1935]   Expert 12 |     20 | CPU
DEBUG 01-15 16:10:51.170373.170373 lmp.py:1935]   Expert 47 |     22 | CPU
DEBUG 01-15 16:10:51.170016.170016 lmp.py:1935]   Expert 27 |     32 | CPU
DEBUG 01-15 16:10:51.170421.170421 lmp.py:1935]   Expert 38 |     32 | CPU
DEBUG 01-15 16:10:51.170825.170825 lmp.py:1935]   Expert 16 |     36 | CPU
DEBUG 01-15 16:10:51.170230.170230 lmp.py:1935]   Expert 52 |     40 | CPU
DEBUG 01-15 16:10:51.170635.170635 lmp.py:1935]   Expert 63 |     44 | CPU
DEBUG 01-15 16:10:51.170039.170039 lmp.py:1935]   Expert  4 |     59 | CPU
DEBUG 01-15 16:10:51.170921.170921 lmp.py:1935]   Expert 44 |     63 | CPU
DEBUG 01-15 16:10:51.170087.170087 lmp.py:1935]   Expert 61 |     64 | CPU
DEBUG 01-15 16:10:51.170445.170445 lmp.py:1935]   Expert 43 |     65 | CPU
DEBUG 01-15 16:10:51.170803.170803 lmp.py:1935]   Expert 34 |     76 | CPU
DEBUG 01-15 16:10:51.170684.170684 lmp.py:1935]   Expert 53 |     84 | CPU
DEBUG 01-15 16:10:51.170520.170520 lmp.py:1935]   Expert  0 |     88 | CPU
DEBUG 01-15 16:10:51.170163.170163 lmp.py:1935]   Expert 37 |     89 | CPU
DEBUG 01-15 16:10:51.170090.170090 lmp.py:1935]   Expert 32 |     91 | CPU
DEBUG 01-15 16:10:51.170495.170495 lmp.py:1935]   Expert 13 |    103 | CPU
DEBUG 01-15 16:10:51.170661.170661 lmp.py:1935]   Expert 39 |    114 | CPU
DEBUG 01-15 16:10:51.170973.170973 lmp.py:1935]   Expert 21 |    119 | CPU
DEBUG 01-15 16:10:51.170093.170093 lmp.py:1935]   Expert 11 |    122 | CPU
DEBUG 01-15 16:10:51.170212.170212 lmp.py:1935]   Expert 20 |    126 | CPU
DEBUG 01-15 16:10:51.170332.170332 lmp.py:1935]   Expert  8 |    128 | CPU
DEBUG 01-15 16:10:51.170452.170452 lmp.py:1935]   Expert 60 |    130 | CPU
DEBUG 01-15 16:10:51.170810.170810 lmp.py:1935]   Expert 57 |    139 | CPU
DEBUG 01-15 16:10:51.170407.170407 lmp.py:1935]   Expert 14 |    140 | CPU
DEBUG 01-15 16:10:51.170288.170288 lmp.py:1935]   Expert 22 |    140 | CPU
DEBUG 01-15 16:10:51.170408.170408 lmp.py:1935]   Expert 45 |    152 | CPU
DEBUG 01-15 16:10:51.170051.170051 lmp.py:1935]   Expert  2 |    156 | CPU
DEBUG 01-15 16:10:51.170933.170933 lmp.py:1935]   Expert 18 |    158 | CPU
DEBUG 01-15 16:10:51.170291.170291 lmp.py:1935]   Expert 17 |    159 | CPU
DEBUG 01-15 16:10:51.170172.170172 lmp.py:1935]   Expert 23 |    159 | CPU
DEBUG 01-15 16:10:51.170292.170292 lmp.py:1935]   Expert 58 |    162 | CPU
DEBUG 01-15 16:10:51.170511.170511 lmp.py:1935]   Expert  7 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.170538.170538 lmp.py:1935]   Expert 30 |    167 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.170612.170612 lmp.py:1935]   Expert 42 |    170 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.170924.170924 lmp.py:1935]   Expert 48 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.170759.170759 lmp.py:1935]   Expert 49 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.170071.170071 lmp.py:1935]   Expert 62 |    180 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.170383.170383 lmp.py:1935]   Expert 35 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.170456.170456 lmp.py:1935]   Expert 55 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.170245.170245 lmp.py:1935]   Expert 51 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.170318.170318 lmp.py:1935]   Expert  6 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.170392.170392 lmp.py:1935]   Expert 29 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.170989.170989 lmp.py:1935]   Expert 36 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.170585.170585 lmp.py:1935]   Expert 25 |    195 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.170659.170659 lmp.py:1935]   Expert  1 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171494.171494 lmp.py:1935]   Expert 31 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171329.171329 lmp.py:1935]   Expert 28 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.171402.171402 lmp.py:1935]   Expert  5 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171953.171953 lmp.py:1935]   Expert 41 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.171788.171788 lmp.py:1935]   Expert 54 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171861.171861 lmp.py:1935]   Expert  9 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.171696.171696 lmp.py:1935]   Expert 19 |    237 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.171531.171531 lmp.py:1935]   Expert 24 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171128.171128 lmp.py:1935]   Expert 50 |    287 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171725.171725 lmp.py:1935]   Expert 46 |    309 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.171560.171560 lmp.py:1935]   Expert 59 |    310 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171872.171872 lmp.py:1935]   Expert 56 |    371 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.171137.171137 lmp.py:1935]   Expert 26 |    402 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171449.171449 lmp.py:1935]   Expert 33 |    423 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.171807.171807 lmp.py:1935]   Expert  3 |    591 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171642.171642 lmp.py:1935]   Expert 10 |    637 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.171239.171239 lmp.py:1935]   Expert 15 |    651 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.171836.171836 lmp.py:1935]   Expert 40 |    791 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.171883.171883 lmp.py:1937] 
DEBUG 01-15 16:10:51.171883.171883 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.171957.171957 lmp.py:1938]   CPU:   3112 tokens
DEBUG 01-15 16:10:51.171030.171030 lmp.py:1942]   cuda:1:   4590 tokens (16 experts)
DEBUG 01-15 16:10:51.171580.171580 lmp.py:1942]   cuda:2:   4586 tokens (16 experts)
DEBUG 01-15 16:10:51.171177.171177 lmp.py:1943]   Total GPU:   9176 tokens
DEBUG 01-15 16:10:51.171297.171297 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.171297.171297 lmp.py:1944] 
DEBUG 01-15 16:10:51.171092.171092 cuda_h.py:19] end experts_map_get cost 0.0019414424896240234 seconds
DEBUG 01-15 16:10:51.171095.171095 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.171758.171758 lmp.py:1953] 
DEBUG 01-15 16:10:51.171758.171758 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.171926.171926 cuda_h.py:19] end cpu_experts_submit cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:51.171622.171622 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.171743.171743 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.172449.172449 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:51.172290.172290 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.12.mlp.experts.1.gate_proj.weight': 0, 'model.layers.12.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.36.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.36.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.36.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.6.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.6.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.6.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.35.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.35.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.35.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.40.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.40.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.40.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.7.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.7.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.7.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.54.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.54.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.54.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.24.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.24.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.24.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.12.mlp.experts.33.gate_proj.weight': 0, 'model.layers.12.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.12.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.12.mlp.experts.9.gate_proj.weight': 17301504, 'model.layers.12.mlp.experts.9.down_proj.weight': 23068672, 'model.layers.12.mlp.experts.9.up_proj.weight': 28835840, 'model.layers.12.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.12.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.12.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.12.mlp.experts.41.gate_proj.weight': 51904512, 'model.layers.12.mlp.experts.41.down_proj.weight': 57671680, 'model.layers.12.mlp.experts.41.up_proj.weight': 63438848, 'model.layers.12.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.12.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.12.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.12.mlp.experts.46.gate_proj.weight': 86507520, 'model.layers.12.mlp.experts.46.down_proj.weight': 92274688, 'model.layers.12.mlp.experts.46.up_proj.weight': 98041856, 'model.layers.12.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.12.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.12.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.12.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.12.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.12.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.12.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.12.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.12.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.12.mlp.experts.51.gate_proj.weight': 155713536, 'model.layers.12.mlp.experts.51.down_proj.weight': 161480704, 'model.layers.12.mlp.experts.51.up_proj.weight': 167247872, 'model.layers.12.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.12.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.12.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.12.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.12.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.12.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.12.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.12.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.12.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.12.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.12.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.12.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.12.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.12.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.12.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.12.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.12.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.12.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(15058075648, 5767168, 0, 0), (15063842816, 5767168, 5767168, 0), (15052308480, 5767168, 11534336, 0), (15092678656, 5767168, 17301504, 0), (15098445824, 5767168, 23068672, 0), (15086911488, 5767168, 28835840, 0), (15663628288, 5767168, 34603008, 0), (15669395456, 5767168, 40370176, 0), (15657861120, 5767168, 46137344, 0), (15127281664, 5767168, 51904512, 0), (15133048832, 5767168, 57671680, 0), (15121514496, 5767168, 63438848, 0), (15144583168, 5767168, 69206016, 0), (15150350336, 5767168, 74973184, 0), (15138816000, 5767168, 80740352, 0), (15646326784, 5767168, 86507520, 0), (15652093952, 5767168, 92274688, 0), (15640559616, 5767168, 98041856, 0), (15732834304, 5767168, 103809024, 0), (15738601472, 5767168, 109576192, 0), (15727067136, 5767168, 115343360, 0), (15161884672, 5767168, 121110528, 0), (15167651840, 5767168, 126877696, 0), (15156117504, 5767168, 132644864, 0), (15871246336, 5767168, 138412032, 0), (15877013504, 5767168, 144179200, 0), (15865479168, 5767168, 149946368, 0), (15905849344, 5767168, 155713536, 0), (15911616512, 5767168, 161480704, 0), (15900082176, 5767168, 167247872, 0), (15975055360, 5767168, 173015040, 0), (15980822528, 5767168, 178782208, 0), (15969288192, 5767168, 184549376, 0), (15456010240, 5767168, 190316544, 0), (15461777408, 5767168, 196083712, 0), (15450243072, 5767168, 201850880, 0), (15490613248, 5767168, 207618048, 0), (15496380416, 5767168, 213385216, 0), (15484846080, 5767168, 219152384, 0), (16061562880, 5767168, 224919552, 0), (16067330048, 5767168, 230686720, 0), (16055795712, 5767168, 236453888, 0), (16113467392, 5767168, 242221056, 0), (16119234560, 5767168, 247988224, 0), (16107700224, 5767168, 253755392, 0), (15577120768, 5767168, 259522560, 0), (15582887936, 5767168, 265289728, 0), (15571353600, 5767168, 271056896, 0)], 2: [(15611723776, 5767168, 0, 0), (15617490944, 5767168, 5767168, 0), (15605956608, 5767168, 11534336, 0), (15196487680, 5767168, 17301504, 0), (15202254848, 5767168, 23068672, 0), (15190720512, 5767168, 28835840, 0), (15213789184, 5767168, 34603008, 0), (15219556352, 5767168, 40370176, 0), (15208022016, 5767168, 46137344, 0), (15750135808, 5767168, 51904512, 0), (15755902976, 5767168, 57671680, 0), (15744368640, 5767168, 63438848, 0), (15767437312, 5767168, 69206016, 0), (15773204480, 5767168, 74973184, 0), (15761670144, 5767168, 80740352, 0), (15836643328, 5767168, 86507520, 0), (15842410496, 5767168, 92274688, 0), (15830876160, 5767168, 98041856, 0), (15300296704, 5767168, 103809024, 0), (15306063872, 5767168, 109576192, 0), (15294529536, 5767168, 115343360, 0), (15888547840, 5767168, 121110528, 0), (15894315008, 5767168, 126877696, 0), (15882780672, 5767168, 132644864, 0), (15369502720, 5767168, 138412032, 0), (15375269888, 5767168, 144179200, 0), (15363735552, 5767168, 149946368, 0), (15923150848, 5767168, 155713536, 0), (15928918016, 5767168, 161480704, 0), (15917383680, 5767168, 167247872, 0), (15992356864, 5767168, 173015040, 0), (15998124032, 5767168, 178782208, 0), (15986589696, 5767168, 184549376, 0), (16009658368, 5767168, 190316544, 0), (16015425536, 5767168, 196083712, 0), (16003891200, 5767168, 201850880, 0), (15473311744, 5767168, 207618048, 0), (15479078912, 5767168, 213385216, 0), (15467544576, 5767168, 219152384, 0), (15525216256, 5767168, 224919552, 0), (15530983424, 5767168, 230686720, 0), (15519449088, 5767168, 236453888, 0), (15542517760, 5767168, 242221056, 0), (15548284928, 5767168, 247988224, 0), (15536750592, 5767168, 253755392, 0), (15559819264, 5767168, 259522560, 0), (15565586432, 5767168, 265289728, 0), (15554052096, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec424dda0>, 2: <capsule object NULL at 0x7a51b06da3d0>}
DEBUG 01-15 16:10:51.172505.172505 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:51.172308.172308 client.py:127] Model loaded
DEBUG 01-15 16:10:51.172272.172272 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.172062.172062 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.172268.172268 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.173984.173984 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 496ac811-6c7f-4795-9b57-8318196dfe3e
DEBUG 01-15 16:10:51.173487.173487 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.173091.173091 cuda_h.py:19] end move_flatidxs cost 0.0008292198181152344 seconds
DEBUG 01-15 16:10:51.173152.173152 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.173095.173095 cuda_h.py:19] end restore2model cost 0.0006988048553466797 seconds
DEBUG 01-15 16:10:51.173494.173494 cuda_h.py:19] end sllm_worker_task cost 0.011048316955566406 seconds
INFO 01-15 16:10:51.174421.174421 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 496ac811-6c7f-4795-9b57-8318196dfe3e
DEBUG 01-15 16:10:51.175009.175009 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003109455108642578 seconds
DEBUG 01-15 16:10:51.175263.175263 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.178042.178042 cuda_h.py:19] end restore2model cost 0.0030660629272460938 seconds
DEBUG 01-15 16:10:51.178455.178455 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0066640377044677734 seconds
DEBUG 01-15 16:10:51.178132.178132 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.178846.178846 cuda_h.py:19] end gpu_sexperts cost 0.00031685829162597656 seconds
DEBUG 01-15 16:10:51.178344.178344 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.178981.178981 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:10:51.178777.178777 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.178864.178864 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.180219.180219 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010704994201660156 seconds
DEBUG 01-15 16:10:51.180022.180022 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.180639.180639 cuda_h.py:19] end gpu_group_list cost 0.00017714500427246094 seconds
DEBUG 01-15 16:10:51.181811.181811 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.182000.182000 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012204647064208984 seconds
DEBUG 01-15 16:10:51.182202.182202 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.182077.182077 cuda_h.py:19] end gpu_group_list cost 0.00017642974853515625 seconds
DEBUG 01-15 16:10:51.182893.182893 cuda_h.py:19] end group_tensors cost 0.009217500686645508 seconds
DEBUG 01-15 16:10:51.183097.183097 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.183992.183992 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.183795.183795 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 496ac811-6c7f-4795-9b57-8318196dfe3e
DEBUG 01-15 16:10:51.188202.188202 cuda_h.py:19] end group pad cost 0.0042765140533447266 seconds
DEBUG 01-15 16:10:51.188859.188859 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.201165.201165 client.py:127] Model loaded
DEBUG 01-15 16:10:51.201932.201932 cuda_h.py:19] end wait_experts_multi_device cost 0.017528057098388672 seconds
DEBUG 01-15 16:10:51.201192.201192 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.208789.208789 cuda_h.py:19] end group_einsum cost 0.020162105560302734 seconds
DEBUG 01-15 16:10:51.208834.208834 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.212126.212126 cuda_h.py:19] end get_outputs_cpu1 cost 0.0034890174865722656 seconds
DEBUG 01-15 16:10:51.212657.212657 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.040144920349121094 seconds
DEBUG 01-15 16:10:51.213607.213607 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.011719226837158203 seconds
DEBUG 01-15 16:10:51.213087.213087 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.214292.214292 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.215920.215920 cuda_h.py:19] end index_scatter cost 8.940696716308594e-05 seconds
DEBUG 01-15 16:10:51.215885.215885 cuda_h.py:19] end cpuoutputsdeal cost 0.002020120620727539 seconds
DEBUG 01-15 16:10:51.215709.215709 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.215949.215949 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.215405.215405 cuda_h.py:19] end gpu_group_tensor cost 0.0001633167266845703 seconds
DEBUG 01-15 16:10:51.215837.215837 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.215658.215658 cuda_h.py:19] end gpu_group_tensor cost 0.00015115737915039062 seconds
DEBUG 01-15 16:10:51.215198.215198 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.216225.216225 cuda_h.py:19] end gpu_group_einsum cost 0.0005679130554199219 seconds
DEBUG 01-15 16:10:51.216892.216892 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.217893.217893 cuda_h.py:19] end gpu_group_einsum cost 0.0005400180816650391 seconds
DEBUG 01-15 16:10:51.217666.217666 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.217180.217180 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.217007.217007 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003147125244140625 seconds
DEBUG 01-15 16:10:51.218909.218909 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.218316.218316 cuda_h.py:19] end concat_expert_out cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:10:51.218881.218881 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.218348.218348 cuda_h.py:19] end index_scatter cost 6.29425048828125e-05 seconds
DEBUG 01-15 16:10:51.218654.218654 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009865760803222656 seconds
DEBUG 01-15 16:10:51.218744.218744 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.218117.218117 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.218151.218151 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002009868621826172 seconds
DEBUG 01-15 16:10:51.218146.218146 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.219844.219844 cuda_h.py:19] end concat_expert_out cost 5.91278076171875e-05 seconds
DEBUG 01-15 16:10:51.219740.219740 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.219730.219730 cuda_h.py:19] end index_scatter cost 6.175041198730469e-05 seconds
DEBUG 01-15 16:10:51.219493.219493 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005826950073242188 seconds
DEBUG 01-15 16:10:51.219032.219032 cuda_h.py:19] end gpu_experts_multi_device cost 0.0404362678527832 seconds
DEBUG 01-15 16:10:51.219194.219194 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_13 cost 0.050740957260131836 seconds
DEBUG 01-15 16:10:51.219415.219415 cuda_h.py:19] end prefill_layer cost 0.057643890380859375 seconds
DEBUG 01-15 16:10:51.219842.219842 lmp.py:1553] -------------------------------- end prefill layer 12 --------------------------------
DEBUG 01-15 16:10:51.219406.219406 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.220400.220400 lmp.py:1495] -------------------------------- start prefill layer 13 --------------------------------
DEBUG 01-15 16:10:51.220964.220964 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:51.220435.220435 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 01-15 16:10:51.220722.220722 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 4.029273986816406e-05 seconds
DEBUG 01-15 16:10:51.220008.220008 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 7.82012939453125e-05 seconds
DEBUG 01-15 16:10:51.220234.220234 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.220634.220634 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.220322.220322 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.220524.220524 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.220745.220745 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.220900.220900 cuda_h.py:19] end allocate_cuda_memory cost 0.0002512931823730469 seconds
DEBUG 01-15 16:10:51.220757.220757 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.221665.221665 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.221018.221018 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.221681.221681 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a98371e0-81c9-4e82-a906-87cd6ac27ec2
DEBUG 01-15 16:10:51.221539.221539 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.221623.221623 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.222277.222277 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a98371e0-81c9-4e82-a906-87cd6ac27ec2
DEBUG 01-15 16:10:51.222597.222597 cuda_h.py:19] end load_into_gpu_async cost 0.001529693603515625 seconds
DEBUG 01-15 16:10:51.222161.222161 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.222271.222271 cuda_h.py:19] end restore_tensors2 cost 8.58306884765625e-05 seconds
DEBUG 01-15 16:10:51.222557.222557 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021309852600097656 seconds
INFO 01-15 16:10:51.222613.222613 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a98371e0-81c9-4e82-a906-87cd6ac27ec2
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.225175.225175 cuda_h.py:19] end self_attn cost 0.004332065582275391 seconds
DEBUG 01-15 16:10:51.226122.226122 cuda_h.py:19] end iln_self_attn_paln cost 0.006077289581298828 seconds
DEBUG 01-15 16:10:51.226951.226951 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_14
DEBUG 01-15 16:10:51.226568.226568 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.227012.227012 cuda_h.py:19] end gate cost 0.0007469654083251953 seconds
DEBUG 01-15 16:10:51.227325.227325 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.227507.227507 lmp.py:1912] 
DEBUG 01-15 16:10:51.227507.227507 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.227216.227216 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.227773.227773 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.227277.227277 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.227397.227397 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.227563.227563 lmp.py:1917] 
DEBUG 01-15 16:10:51.227563.227563 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.227206.227206 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.227287.227287 lmp.py:1935]   Expert 42 |     22 | CPU
DEBUG 01-15 16:10:51.227406.227406 lmp.py:1935]   Expert 19 |     24 | CPU
DEBUG 01-15 16:10:51.227241.227241 lmp.py:1935]   Expert 30 |     27 | CPU
DEBUG 01-15 16:10:51.227600.227600 lmp.py:1935]   Expert 32 |     50 | CPU
DEBUG 01-15 16:10:51.227004.227004 lmp.py:1935]   Expert  6 |     58 | CPU
DEBUG 01-15 16:10:51.228409.228409 lmp.py:1935]   Expert 53 |     73 | CPU
DEBUG 01-15 16:10:51.228575.228575 lmp.py:1935]   Expert  5 |     74 | CPU
DEBUG 01-15 16:10:51.228503.228503 lmp.py:1935]   Expert  1 |     82 | CPU
DEBUG 01-15 16:10:51.228430.228430 lmp.py:1935]   Expert 13 |    119 | CPU
DEBUG 01-15 16:10:51.228881.228881 lmp.py:1935]   Expert  9 |    122 | CPU
DEBUG 01-15 16:10:51.228809.228809 lmp.py:1935]   Expert 63 |    125 | CPU
DEBUG 01-15 16:10:51.228498.228498 lmp.py:1935]   Expert 34 |    128 | CPU
DEBUG 01-15 16:10:51.228856.228856 lmp.py:1935]   Expert 58 |    130 | CPU
DEBUG 01-15 16:10:51.228499.228499 lmp.py:1935]   Expert 50 |    131 | CPU
DEBUG 01-15 16:10:51.228427.228427 lmp.py:1935]   Expert 11 |    133 | CPU
DEBUG 01-15 16:10:51.228116.228116 lmp.py:1935]   Expert 18 |    137 | CPU
DEBUG 01-15 16:10:51.228044.228044 lmp.py:1935]   Expert 26 |    137 | CPU
DEBUG 01-15 16:10:51.228972.228972 lmp.py:1935]   Expert 31 |    138 | CPU
DEBUG 01-15 16:10:51.228900.228900 lmp.py:1935]   Expert 59 |    140 | CPU
DEBUG 01-15 16:10:51.228350.228350 lmp.py:1935]   Expert 12 |    147 | CPU
DEBUG 01-15 16:10:51.228424.228424 lmp.py:1935]   Expert 40 |    147 | CPU
DEBUG 01-15 16:10:51.228021.228021 lmp.py:1935]   Expert  4 |    150 | CPU
DEBUG 01-15 16:10:51.228333.228333 lmp.py:1935]   Expert 46 |    150 | CPU
DEBUG 01-15 16:10:51.228691.228691 lmp.py:1935]   Expert 20 |    152 | CPU
DEBUG 01-15 16:10:51.228811.228811 lmp.py:1935]   Expert  2 |    153 | CPU
DEBUG 01-15 16:10:51.228930.228930 lmp.py:1935]   Expert 48 |    153 | CPU
DEBUG 01-15 16:10:51.228573.228573 lmp.py:1935]   Expert 33 |    154 | CPU
DEBUG 01-15 16:10:51.228561.228561 lmp.py:1935]   Expert 56 |    154 | CPU
DEBUG 01-15 16:10:51.228370.228370 lmp.py:1935]   Expert 61 |    155 | CPU
DEBUG 01-15 16:10:51.228113.228113 lmp.py:1935]   Expert 35 |    165 | CPU
DEBUG 01-15 16:10:51.228994.228994 lmp.py:1935]   Expert 10 |    167 | CPU
DEBUG 01-15 16:10:51.228591.228591 lmp.py:1935]   Expert 55 |    170 | CPU
DEBUG 01-15 16:10:51.228095.228095 lmp.py:1935]   Expert 51 |    171 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228645.228645 lmp.py:1935]   Expert  8 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228718.228718 lmp.py:1935]   Expert 36 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228315.228315 lmp.py:1935]   Expert 52 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228673.228673 lmp.py:1935]   Expert 37 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228508.228508 lmp.py:1935]   Expert 57 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228867.228867 lmp.py:1935]   Expert  0 |    205 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228225.228225 lmp.py:1935]   Expert 39 |    221 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228060.228060 lmp.py:1935]   Expert 25 |    222 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228610.228610 lmp.py:1935]   Expert 62 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228445.228445 lmp.py:1935]   Expert 38 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228803.228803 lmp.py:1935]   Expert  7 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228923.228923 lmp.py:1935]   Expert 24 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228805.228805 lmp.py:1935]   Expert 27 |    251 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228163.228163 lmp.py:1935]   Expert  3 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228283.228283 lmp.py:1935]   Expert 28 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228403.228403 lmp.py:1935]   Expert 49 |    258 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228761.228761 lmp.py:1935]   Expert 60 |    258 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228357.228357 lmp.py:1935]   Expert 21 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228192.228192 lmp.py:1935]   Expert 16 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228266.228266 lmp.py:1935]   Expert 43 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228863.228863 lmp.py:1935]   Expert 23 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228982.228982 lmp.py:1935]   Expert 29 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228341.228341 lmp.py:1935]   Expert 15 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228460.228460 lmp.py:1935]   Expert 41 |    295 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.228580.228580 lmp.py:1935]   Expert 47 |    295 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.228462.228462 lmp.py:1935]   Expert 22 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.229343.229343 lmp.py:1935]   Expert 44 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.229893.229893 lmp.py:1935]   Expert 54 |    354 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.229728.229728 lmp.py:1935]   Expert 14 |    374 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.229325.229325 lmp.py:1935]   Expert 17 |    404 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.229445.229445 lmp.py:1935]   Expert 45 |    454 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.229373.229373 lmp.py:1937] 
DEBUG 01-15 16:10:51.229373.229373 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.229254.229254 lmp.py:1938]   CPU:   3867 tokens
DEBUG 01-15 16:10:51.229612.229612 lmp.py:1942]   cuda:1:   4207 tokens (16 experts)
DEBUG 01-15 16:10:51.229255.229255 lmp.py:1942]   cuda:2:   4214 tokens (16 experts)
DEBUG 01-15 16:10:51.229421.229421 lmp.py:1943]   Total GPU:   8421 tokens
DEBUG 01-15 16:10:51.229826.229826 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.229826.229826 lmp.py:1944] 
DEBUG 01-15 16:10:51.229098.229098 cuda_h.py:19] end experts_map_get cost 0.0019087791442871094 seconds
DEBUG 01-15 16:10:51.229339.229339 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.229049.229049 lmp.py:1953] 
DEBUG 01-15 16:10:51.229049.229049 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.229408.229408 cuda_h.py:19] end cpu_experts_submit cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:51.229627.229627 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.229887.229887 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.230075.230075 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.13.mlp.experts.0.gate_proj.weight': 0, 'model.layers.13.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.43.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.43.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.43.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.47.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.47.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.47.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.49.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.49.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.49.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.51.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.51.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.51.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.21.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.21.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.21.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.54.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.54.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.54.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.22.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.22.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.22.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.57.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.57.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.57.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.13.mlp.experts.3.gate_proj.weight': 0, 'model.layers.13.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.13.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.13.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.13.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.13.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.13.mlp.experts.7.gate_proj.weight': 34603008, 'model.layers.13.mlp.experts.7.down_proj.weight': 40370176, 'model.layers.13.mlp.experts.7.up_proj.weight': 46137344, 'model.layers.13.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.13.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.13.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.13.mlp.experts.41.gate_proj.weight': 69206016, 'model.layers.13.mlp.experts.41.down_proj.weight': 74973184, 'model.layers.13.mlp.experts.41.up_proj.weight': 80740352, 'model.layers.13.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.13.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.13.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.13.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.13.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.13.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.13.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.13.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.13.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.13.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.13.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.13.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.13.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.13.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.13.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.13.mlp.experts.17.gate_proj.weight': 173015040, 'model.layers.13.mlp.experts.17.down_proj.weight': 178782208, 'model.layers.13.mlp.experts.17.up_proj.weight': 184549376, 'model.layers.13.mlp.experts.52.gate_proj.weight': 190316544, 'model.layers.13.mlp.experts.52.down_proj.weight': 196083712, 'model.layers.13.mlp.experts.52.up_proj.weight': 201850880, 'model.layers.13.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.13.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.13.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.13.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.13.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.13.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.13.mlp.experts.25.gate_proj.weight': 242221056, 'model.layers.13.mlp.experts.25.down_proj.weight': 247988224, 'model.layers.13.mlp.experts.25.up_proj.weight': 253755392, 'model.layers.13.mlp.experts.60.gate_proj.weight': 259522560, 'model.layers.13.mlp.experts.60.down_proj.weight': 265289728, 'model.layers.13.mlp.experts.60.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(16148070400, 5767168, 0, 0), (16153837568, 5767168, 5767168, 0), (16142303232, 5767168, 11534336, 0), (16770924544, 5767168, 17301504, 0), (16776691712, 5767168, 23068672, 0), (16765157376, 5767168, 28835840, 0), (16805527552, 5767168, 34603008, 0), (16811294720, 5767168, 40370176, 0), (16799760384, 5767168, 46137344, 0), (16892035072, 5767168, 51904512, 0), (16897802240, 5767168, 57671680, 0), (16886267904, 5767168, 63438848, 0), (16926638080, 5767168, 69206016, 0), (16932405248, 5767168, 74973184, 0), (16920870912, 5767168, 80740352, 0), (16961241088, 5767168, 86507520, 0), (16967008256, 5767168, 92274688, 0), (16955473920, 5767168, 98041856, 0), (16995844096, 5767168, 103809024, 0), (17001611264, 5767168, 109576192, 0), (16990076928, 5767168, 115343360, 0), (17030447104, 5767168, 121110528, 0), (17036214272, 5767168, 126877696, 0), (17024679936, 5767168, 132644864, 0), (16511401984, 5767168, 138412032, 0), (16517169152, 5767168, 144179200, 0), (16505634816, 5767168, 149946368, 0), (17082351616, 5767168, 155713536, 0), (17088118784, 5767168, 161480704, 0), (17076584448, 5767168, 167247872, 0), (16528703488, 5767168, 173015040, 0), (16534470656, 5767168, 178782208, 0), (16522936320, 5767168, 184549376, 0), (17134256128, 5767168, 190316544, 0), (17140023296, 5767168, 196083712, 0), (17128488960, 5767168, 201850880, 0), (16615211008, 5767168, 207618048, 0), (16620978176, 5767168, 213385216, 0), (16609443840, 5767168, 219152384, 0), (16632512512, 5767168, 224919552, 0), (16638279680, 5767168, 230686720, 0), (16626745344, 5767168, 236453888, 0), (16649814016, 5767168, 242221056, 0), (16655581184, 5767168, 247988224, 0), (16644046848, 5767168, 253755392, 0), (17220763648, 5767168, 259522560, 0), (17226530816, 5767168, 265289728, 0), (17214996480, 5767168, 271056896, 0)], 2: [(16199974912, 5767168, 0, 0), (16205742080, 5767168, 5767168, 0), (16194207744, 5767168, 11534336, 0), (16788226048, 5767168, 17301504, 0), (16793993216, 5767168, 23068672, 0), (16782458880, 5767168, 28835840, 0), (16269180928, 5767168, 34603008, 0), (16274948096, 5767168, 40370176, 0), (16263413760, 5767168, 46137344, 0), (16822829056, 5767168, 51904512, 0), (16828596224, 5767168, 57671680, 0), (16817061888, 5767168, 63438848, 0), (16857432064, 5767168, 69206016, 0), (16863199232, 5767168, 74973184, 0), (16851664896, 5767168, 80740352, 0), (16286482432, 5767168, 86507520, 0), (16292249600, 5767168, 92274688, 0), (16280715264, 5767168, 98041856, 0), (16909336576, 5767168, 103809024, 0), (16915103744, 5767168, 109576192, 0), (16903569408, 5767168, 115343360, 0), (16390291456, 5767168, 121110528, 0), (16396058624, 5767168, 126877696, 0), (16384524288, 5767168, 132644864, 0), (16407592960, 5767168, 138412032, 0), (16413360128, 5767168, 144179200, 0), (16401825792, 5767168, 149946368, 0), (16424894464, 5767168, 155713536, 0), (16430661632, 5767168, 161480704, 0), (16419127296, 5767168, 167247872, 0), (16442195968, 5767168, 173015040, 0), (16447963136, 5767168, 178782208, 0), (16436428800, 5767168, 184549376, 0), (17047748608, 5767168, 190316544, 0), (17053515776, 5767168, 196083712, 0), (17041981440, 5767168, 201850880, 0), (16546004992, 5767168, 207618048, 0), (16551772160, 5767168, 213385216, 0), (16540237824, 5767168, 219152384, 0), (16563306496, 5767168, 224919552, 0), (16569073664, 5767168, 230686720, 0), (16557539328, 5767168, 236453888, 0), (16580608000, 5767168, 242221056, 0), (16586375168, 5767168, 247988224, 0), (16574840832, 5767168, 253755392, 0), (17186160640, 5767168, 259522560, 0), (17191927808, 5767168, 265289728, 0), (17180393472, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4ec4681380>, 2: <capsule object NULL at 0x7a51b06da9a0>}
DEBUG 01-15 16:10:51.230200.230200 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.230467.230467 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.230223.230223 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b6db4313-e87c-417b-b07c-ba4fc71da6b5
DEBUG 01-15 16:10:51.230865.230865 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.230639.230639 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:51.230089.230089 client.py:127] Model loaded
DEBUG 01-15 16:10:51.230634.230634 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.231979.231979 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.231282.231282 cuda_h.py:19] end restore2model cost 0.00034236907958984375 seconds
DEBUG 01-15 16:10:51.231575.231575 cuda_h.py:19] end sllm_worker_task cost 0.01082754135131836 seconds
INFO 01-15 16:10:51.231764.231764 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b6db4313-e87c-417b-b07c-ba4fc71da6b5
DEBUG 01-15 16:10:51.232420.232420 cuda_h.py:19] end move_flatidxs cost 0.0008285045623779297 seconds
DEBUG 01-15 16:10:51.232196.232196 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.232233.232233 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0029511451721191406 seconds
DEBUG 01-15 16:10:51.232593.232593 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.235287.235287 cuda_h.py:19] end restore2model cost 0.0030722618103027344 seconds
DEBUG 01-15 16:10:51.235560.235560 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006281375885009766 seconds
DEBUG 01-15 16:10:51.235071.235071 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.236394.236394 cuda_h.py:19] end gpu_sexperts cost 0.0003104209899902344 seconds
DEBUG 01-15 16:10:51.236654.236654 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.236861.236861 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.71661376953125e-05 seconds
DEBUG 01-15 16:10:51.236180.236180 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.236790.236790 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.237827.237827 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010821819305419922 seconds
DEBUG 01-15 16:10:51.237247.237247 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.237386.237386 cuda_h.py:19] end gpu_group_list cost 0.00017690658569335938 seconds
DEBUG 01-15 16:10:51.238028.238028 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.239467.239467 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0012083053588867188 seconds
DEBUG 01-15 16:10:51.239053.239053 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.240431.240431 cuda_h.py:19] end gpu_group_list cost 0.0001773834228515625 seconds
DEBUG 01-15 16:10:51.240762.240762 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.240552.240552 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b6db4313-e87c-417b-b07c-ba4fc71da6b5
DEBUG 01-15 16:10:51.241722.241722 cuda_h.py:19] end group_tensors cost 0.009340763092041016 seconds
DEBUG 01-15 16:10:51.242284.242284 cuda_h.py:10] start group pad
INFO 01-15 16:10:51.256364.256364 client.py:127] Model loaded
DEBUG 01-15 16:10:51.257082.257082 cuda_h.py:19] end wait_experts_multi_device cost 0.01601433753967285 seconds
DEBUG 01-15 16:10:51.257044.257044 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.257614.257614 cuda_h.py:19] end group pad cost 0.015356063842773438 seconds
DEBUG 01-15 16:10:51.257312.257312 cuda_h.py:10] start group_einsum
DEBUG 01-15 16:10:51.280415.280415 cuda_h.py:19] end group_einsum cost 0.02217245101928711 seconds
DEBUG 01-15 16:10:51.280427.280427 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.287071.287071 cuda_h.py:19] end get_outputs_cpu1 cost 0.007494211196899414 seconds
DEBUG 01-15 16:10:51.288158.288158 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.05736994743347168 seconds
DEBUG 01-15 16:10:51.288758.288758 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.031748056411743164 seconds
DEBUG 01-15 16:10:51.288569.288569 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.290575.290575 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.290859.290859 cuda_h.py:19] end index_scatter cost 8.845329284667969e-05 seconds
DEBUG 01-15 16:10:51.291062.291062 cuda_h.py:19] end cpuoutputsdeal cost 0.0020062923431396484 seconds
DEBUG 01-15 16:10:51.291978.291978 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.291887.291887 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.291026.291026 cuda_h.py:19] end gpu_group_tensor cost 0.00016546249389648438 seconds
DEBUG 01-15 16:10:51.291458.291458 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.291663.291663 cuda_h.py:19] end gpu_group_tensor cost 0.00015282630920410156 seconds
DEBUG 01-15 16:10:51.291773.291773 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.292879.292879 cuda_h.py:19] end gpu_group_einsum cost 0.0005571842193603516 seconds
DEBUG 01-15 16:10:51.292712.292712 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.293614.293614 cuda_h.py:19] end gpu_group_einsum cost 0.0005729198455810547 seconds
DEBUG 01-15 16:10:51.293341.293341 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.293471.293471 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.293675.293675 cuda_h.py:19] end all_expert_outputs_slices cost 0.0003132820129394531 seconds
DEBUG 01-15 16:10:51.293862.293862 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.293315.293315 cuda_h.py:19] end concat_expert_out cost 5.4836273193359375e-05 seconds
DEBUG 01-15 16:10:51.293880.293880 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.294917.294917 cuda_h.py:19] end index_scatter cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:10:51.294616.294616 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0010199546813964844 seconds
DEBUG 01-15 16:10:51.294480.294480 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.294714.294714 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.294749.294749 cuda_h.py:19] end all_expert_outputs_slices cost 0.000202178955078125 seconds
DEBUG 01-15 16:10:51.294697.294697 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.294349.294349 cuda_h.py:19] end concat_expert_out cost 5.9604644775390625e-05 seconds
DEBUG 01-15 16:10:51.294722.294722 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.295613.295613 cuda_h.py:19] end index_scatter cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:10:51.295137.295137 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005853176116943359 seconds
DEBUG 01-15 16:10:51.295484.295484 cuda_h.py:19] end gpu_experts_multi_device cost 0.05890798568725586 seconds
DEBUG 01-15 16:10:51.295646.295646 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_14 cost 0.06877565383911133 seconds
DEBUG 01-15 16:10:51.295404.295404 cuda_h.py:19] end prefill_layer cost 0.07566618919372559 seconds
DEBUG 01-15 16:10:51.295671.295671 lmp.py:1553] -------------------------------- end prefill layer 13 --------------------------------
DEBUG 01-15 16:10:51.295996.295996 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.295990.295990 lmp.py:1495] -------------------------------- start prefill layer 14 --------------------------------
DEBUG 01-15 16:10:51.295554.295554 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:51.295456.295456 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 01-15 16:10:51.295028.295028 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 4.029273986816406e-05 seconds
DEBUG 01-15 16:10:51.295745.295745 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 7.915496826171875e-05 seconds
DEBUG 01-15 16:10:51.296732.296732 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.296410.296410 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.296170.296170 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.296615.296615 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.296243.296243 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.296439.296439 cuda_h.py:19] end allocate_cuda_memory cost 0.00026297569274902344 seconds
DEBUG 01-15 16:10:51.296799.296799 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.296900.296900 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.296253.296253 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.296916.296916 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a552a080-d311-4d21-8aa6-71be2e06f958
DEBUG 01-15 16:10:51.297489.297489 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.297704.297704 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.298624.298624 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a552a080-d311-4d21-8aa6-71be2e06f958
DEBUG 01-15 16:10:51.298705.298705 cuda_h.py:19] end load_into_gpu_async cost 0.0015065670013427734 seconds
DEBUG 01-15 16:10:51.298269.298269 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.298141.298141 cuda_h.py:19] end restore_tensors2 cost 8.463859558105469e-05 seconds
DEBUG 01-15 16:10:51.298427.298427 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021619796752929688 seconds
INFO 01-15 16:10:51.298005.298005 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a552a080-d311-4d21-8aa6-71be2e06f958
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.301934.301934 cuda_h.py:19] end self_attn cost 0.004202365875244141 seconds
DEBUG 01-15 16:10:51.301992.301992 cuda_h.py:19] end iln_self_attn_paln cost 0.0059490203857421875 seconds
DEBUG 01-15 16:10:51.302206.302206 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_15
DEBUG 01-15 16:10:51.302128.302128 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.302822.302822 cuda_h.py:19] end gate cost 0.000720977783203125 seconds
DEBUG 01-15 16:10:51.302135.302135 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.303417.303417 lmp.py:1912] 
DEBUG 01-15 16:10:51.303417.303417 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.303179.303179 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.303405.303405 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.303340.303340 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.303890.303890 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.303725.303725 lmp.py:1917] 
DEBUG 01-15 16:10:51.303725.303725 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.303037.303037 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.303310.303310 lmp.py:1935]   Expert 34 |     28 | CPU
DEBUG 01-15 16:10:51.303575.303575 lmp.py:1935]   Expert  7 |     33 | CPU
DEBUG 01-15 16:10:51.303887.303887 lmp.py:1935]   Expert 13 |     41 | CPU
DEBUG 01-15 16:10:51.303199.303199 lmp.py:1935]   Expert 54 |     77 | CPU
DEBUG 01-15 16:10:51.303034.303034 lmp.py:1935]   Expert 18 |     85 | CPU
DEBUG 01-15 16:10:51.303346.303346 lmp.py:1935]   Expert 49 |     86 | CPU
DEBUG 01-15 16:10:51.303850.303850 lmp.py:1935]   Expert 39 |     89 | CPU
DEBUG 01-15 16:10:51.303923.303923 lmp.py:1935]   Expert 59 |    105 | CPU
DEBUG 01-15 16:10:51.303520.303520 lmp.py:1935]   Expert 21 |    107 | CPU
DEBUG 01-15 16:10:51.303117.303117 lmp.py:1935]   Expert 16 |    108 | CPU
DEBUG 01-15 16:10:51.303952.303952 lmp.py:1935]   Expert  0 |    110 | CPU
DEBUG 01-15 16:10:51.303787.303787 lmp.py:1935]   Expert 41 |    117 | CPU
DEBUG 01-15 16:10:51.303384.303384 lmp.py:1935]   Expert 15 |    120 | CPU
DEBUG 01-15 16:10:51.303503.303503 lmp.py:1935]   Expert 22 |    120 | CPU
DEBUG 01-15 16:10:51.303338.303338 lmp.py:1935]   Expert 45 |    121 | CPU
DEBUG 01-15 16:10:51.303935.303935 lmp.py:1935]   Expert 17 |    125 | CPU
DEBUG 01-15 16:10:51.303532.303532 lmp.py:1935]   Expert 61 |    134 | CPU
DEBUG 01-15 16:10:51.303890.303890 lmp.py:1935]   Expert 35 |    136 | CPU
DEBUG 01-15 16:10:51.303248.303248 lmp.py:1935]   Expert 52 |    136 | CPU
DEBUG 01-15 16:10:51.303037.303037 lmp.py:1935]   Expert  8 |    138 | CPU
DEBUG 01-15 16:10:51.303349.303349 lmp.py:1935]   Expert 38 |    140 | CPU
DEBUG 01-15 16:10:51.303184.303184 lmp.py:1935]   Expert 12 |    143 | CPU
DEBUG 01-15 16:10:51.303781.303781 lmp.py:1935]   Expert 31 |    147 | CPU
DEBUG 01-15 16:10:51.304139.304139 lmp.py:1935]   Expert 48 |    149 | CPU
DEBUG 01-15 16:10:51.304735.304735 lmp.py:1935]   Expert 53 |    153 | CPU
DEBUG 01-15 16:10:51.304094.304094 lmp.py:1935]   Expert 36 |    156 | CPU
DEBUG 01-15 16:10:51.304690.304690 lmp.py:1935]   Expert 50 |    160 | CPU
DEBUG 01-15 16:10:51.304525.304525 lmp.py:1935]   Expert 60 |    160 | CPU
DEBUG 01-15 16:10:51.304884.304884 lmp.py:1935]   Expert 40 |    161 | CPU
DEBUG 01-15 16:10:51.304957.304957 lmp.py:1935]   Expert 27 |    174 | CPU
DEBUG 01-15 16:10:51.304554.304554 lmp.py:1935]   Expert 19 |    194 | CPU
DEBUG 01-15 16:10:51.304912.304912 lmp.py:1935]   Expert  4 |    197 | CPU
DEBUG 01-15 16:10:51.304416.304416 lmp.py:1935]   Expert 29 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304443.304443 lmp.py:1935]   Expert 30 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304993.304993 lmp.py:1935]   Expert 11 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304544.304544 lmp.py:1935]   Expert 26 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304094.304094 lmp.py:1935]   Expert 20 |    222 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304360.304360 lmp.py:1935]   Expert 57 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304148.304148 lmp.py:1935]   Expert  6 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304699.304699 lmp.py:1935]   Expert 46 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304249.304249 lmp.py:1935]   Expert 43 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304799.304799 lmp.py:1935]   Expert 23 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304350.304350 lmp.py:1935]   Expert  2 |    240 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304768.304768 lmp.py:1935]   Expert 42 |    244 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304795.304795 lmp.py:1935]   Expert 33 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304584.304584 lmp.py:1935]   Expert 55 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304326.304326 lmp.py:1935]   Expert 32 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304353.304353 lmp.py:1935]   Expert 56 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304142.304142 lmp.py:1935]   Expert  3 |    259 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304216.304216 lmp.py:1935]   Expert  9 |    261 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304243.304243 lmp.py:1935]   Expert 14 |    262 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304031.304031 lmp.py:1935]   Expert 28 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304343.304343 lmp.py:1935]   Expert 44 |    274 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304132.304132 lmp.py:1935]   Expert 51 |    276 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304874.304874 lmp.py:1935]   Expert  1 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304378.304378 lmp.py:1935]   Expert 58 |    280 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304167.304167 lmp.py:1935]   Expert 63 |    289 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304718.304718 lmp.py:1935]   Expert 37 |    290 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304506.304506 lmp.py:1935]   Expert 47 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304818.304818 lmp.py:1935]   Expert 24 |    307 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304607.304607 lmp.py:1935]   Expert 10 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304396.304396 lmp.py:1935]   Expert 62 |    313 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304184.304184 lmp.py:1935]   Expert 25 |    317 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.304735.304735 lmp.py:1935]   Expert  5 |    364 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.304285.304285 lmp.py:1937] 
DEBUG 01-15 16:10:51.304285.304285 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.304597.304597 lmp.py:1938]   CPU:   3950 tokens
DEBUG 01-15 16:10:51.304147.304147 lmp.py:1942]   cuda:1:   4167 tokens (16 experts)
DEBUG 01-15 16:10:51.304698.304698 lmp.py:1942]   cuda:2:   4171 tokens (16 experts)
DEBUG 01-15 16:10:51.304817.304817 lmp.py:1943]   Total GPU:   8338 tokens
DEBUG 01-15 16:10:51.304699.304699 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.304699.304699 lmp.py:1944] 
DEBUG 01-15 16:10:51.304256.304256 cuda_h.py:19] end experts_map_get cost 0.001989126205444336 seconds
DEBUG 01-15 16:10:51.304589.304589 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.305491.305491 lmp.py:1953] 
DEBUG 01-15 16:10:51.305491.305491 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.305943.305943 cuda_h.py:19] end cpu_experts_submit cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:51.305878.305878 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.305568.305568 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.305918.305918 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:51.306566.306566 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.14.mlp.experts.32.gate_proj.weight': 0, 'model.layers.14.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.33.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.33.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.33.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.3.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.3.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.3.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.2.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.2.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.2.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.5.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.5.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.5.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.10.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.10.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.10.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.43.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.43.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.43.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.44.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.44.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.44.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.14.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.14.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.14.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.47.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.47.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.47.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.46.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.46.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.46.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.20.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.20.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.20.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.14.mlp.experts.56.gate_proj.weight': 0, 'model.layers.14.mlp.experts.56.down_proj.weight': 5767168, 'model.layers.14.mlp.experts.56.up_proj.weight': 11534336, 'model.layers.14.mlp.experts.37.gate_proj.weight': 17301504, 'model.layers.14.mlp.experts.37.down_proj.weight': 23068672, 'model.layers.14.mlp.experts.37.up_proj.weight': 28835840, 'model.layers.14.mlp.experts.6.gate_proj.weight': 34603008, 'model.layers.14.mlp.experts.6.down_proj.weight': 40370176, 'model.layers.14.mlp.experts.6.up_proj.weight': 46137344, 'model.layers.14.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.14.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.14.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.14.mlp.experts.42.gate_proj.weight': 69206016, 'model.layers.14.mlp.experts.42.down_proj.weight': 74973184, 'model.layers.14.mlp.experts.42.up_proj.weight': 80740352, 'model.layers.14.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.14.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.14.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.14.mlp.experts.51.gate_proj.weight': 103809024, 'model.layers.14.mlp.experts.51.down_proj.weight': 109576192, 'model.layers.14.mlp.experts.51.up_proj.weight': 115343360, 'model.layers.14.mlp.experts.23.gate_proj.weight': 121110528, 'model.layers.14.mlp.experts.23.down_proj.weight': 126877696, 'model.layers.14.mlp.experts.23.up_proj.weight': 132644864, 'model.layers.14.mlp.experts.30.gate_proj.weight': 138412032, 'model.layers.14.mlp.experts.30.down_proj.weight': 144179200, 'model.layers.14.mlp.experts.30.up_proj.weight': 149946368, 'model.layers.14.mlp.experts.55.gate_proj.weight': 155713536, 'model.layers.14.mlp.experts.55.down_proj.weight': 161480704, 'model.layers.14.mlp.experts.55.up_proj.weight': 167247872, 'model.layers.14.mlp.experts.24.gate_proj.weight': 173015040, 'model.layers.14.mlp.experts.24.down_proj.weight': 178782208, 'model.layers.14.mlp.experts.24.up_proj.weight': 184549376, 'model.layers.14.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.14.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.14.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.14.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.14.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.14.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.14.mlp.experts.28.gate_proj.weight': 224919552, 'model.layers.14.mlp.experts.28.down_proj.weight': 230686720, 'model.layers.14.mlp.experts.28.up_proj.weight': 236453888, 'model.layers.14.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.14.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.14.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.14.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.14.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.14.mlp.experts.57.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(17809014784, 5767168, 0, 0), (17814781952, 5767168, 5767168, 0), (17803247616, 5767168, 11534336, 0), (17272668160, 5767168, 17301504, 0), (17278435328, 5767168, 23068672, 0), (17266900992, 5767168, 28835840, 0), (17826316288, 5767168, 34603008, 0), (17832083456, 5767168, 40370176, 0), (17820549120, 5767168, 46137344, 0), (17307271168, 5767168, 51904512, 0), (17313038336, 5767168, 57671680, 0), (17301504000, 5767168, 63438848, 0), (17289969664, 5767168, 69206016, 0), (17295736832, 5767168, 74973184, 0), (17284202496, 5767168, 80740352, 0), (17341874176, 5767168, 86507520, 0), (17347641344, 5767168, 92274688, 0), (17336107008, 5767168, 98041856, 0), (17428381696, 5767168, 103809024, 0), (17434148864, 5767168, 109576192, 0), (17422614528, 5767168, 115343360, 0), (17999331328, 5767168, 121110528, 0), (18005098496, 5767168, 126877696, 0), (17993564160, 5767168, 132644864, 0), (18016632832, 5767168, 138412032, 0), (18022400000, 5767168, 144179200, 0), (18010865664, 5767168, 149946368, 0), (17497587712, 5767168, 155713536, 0), (17503354880, 5767168, 161480704, 0), (17491820544, 5767168, 167247872, 0), (18068537344, 5767168, 173015040, 0), (18074304512, 5767168, 178782208, 0), (18062770176, 5767168, 184549376, 0), (18051235840, 5767168, 190316544, 0), (18057003008, 5767168, 196083712, 0), (18045468672, 5767168, 201850880, 0), (17601396736, 5767168, 207618048, 0), (17607163904, 5767168, 213385216, 0), (17595629568, 5767168, 219152384, 0), (17705205760, 5767168, 224919552, 0), (17710972928, 5767168, 230686720, 0), (17699438592, 5767168, 236453888, 0), (17757110272, 5767168, 242221056, 0), (17762877440, 5767168, 247988224, 0), (17751343104, 5767168, 253755392, 0), (18345361408, 5767168, 259522560, 0), (18351128576, 5767168, 265289728, 0), (18339594240, 5767168, 271056896, 0)], 2: [(18224250880, 5767168, 0, 0), (18230018048, 5767168, 5767168, 0), (18218483712, 5767168, 11534336, 0), (17895522304, 5767168, 17301504, 0), (17901289472, 5767168, 23068672, 0), (17889755136, 5767168, 28835840, 0), (17359175680, 5767168, 34603008, 0), (17364942848, 5767168, 40370176, 0), (17353408512, 5767168, 46137344, 0), (17411080192, 5767168, 51904512, 0), (17416847360, 5767168, 57671680, 0), (17405313024, 5767168, 63438848, 0), (17982029824, 5767168, 69206016, 0), (17987796992, 5767168, 74973184, 0), (17976262656, 5767168, 80740352, 0), (17445683200, 5767168, 86507520, 0), (17451450368, 5767168, 92274688, 0), (17439916032, 5767168, 98041856, 0), (18137743360, 5767168, 103809024, 0), (18143510528, 5767168, 109576192, 0), (18131976192, 5767168, 115343360, 0), (17653301248, 5767168, 121110528, 0), (17659068416, 5767168, 126877696, 0), (17647534080, 5767168, 132644864, 0), (17774411776, 5767168, 138412032, 0), (17780178944, 5767168, 144179200, 0), (17768644608, 5767168, 149946368, 0), (18206949376, 5767168, 155713536, 0), (18212716544, 5767168, 161480704, 0), (18201182208, 5767168, 167247872, 0), (17670602752, 5767168, 173015040, 0), (17676369920, 5767168, 178782208, 0), (17664835584, 5767168, 184549376, 0), (17687904256, 5767168, 190316544, 0), (17693671424, 5767168, 196083712, 0), (17682137088, 5767168, 201850880, 0), (18258853888, 5767168, 207618048, 0), (18264621056, 5767168, 213385216, 0), (18253086720, 5767168, 219152384, 0), (17739808768, 5767168, 224919552, 0), (17745575936, 5767168, 230686720, 0), (17734041600, 5767168, 236453888, 0), (18328059904, 5767168, 242221056, 0), (18333827072, 5767168, 247988224, 0), (18322292736, 5767168, 253755392, 0), (18241552384, 5767168, 259522560, 0), (18247319552, 5767168, 265289728, 0), (18235785216, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e6c525ce0>, 2: <capsule object NULL at 0x7a4e54679ce0>}
DEBUG 01-15 16:10:51.306411.306411 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:51.306152.306152 client.py:127] Model loaded
DEBUG 01-15 16:10:51.306779.306779 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.306814.306814 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.306928.306928 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.306837.306837 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0f3c9da8-2832-4976-8ed3-1cf91bee1b7c
DEBUG 01-15 16:10:51.306764.306764 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.307661.307661 cuda_h.py:19] end move_flatidxs cost 0.0008637905120849609 seconds
DEBUG 01-15 16:10:51.307734.307734 cuda_h.py:19] end restore2model cost 0.0006837844848632812 seconds
DEBUG 01-15 16:10:51.307006.307006 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.307087.307087 cuda_h.py:19] end sllm_worker_task cost 0.010928153991699219 seconds
INFO 01-15 16:10:51.307141.307141 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0f3c9da8-2832-4976-8ed3-1cf91bee1b7c
DEBUG 01-15 16:10:51.308053.308053 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0029010772705078125 seconds
DEBUG 01-15 16:10:51.308798.308798 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.311313.311313 cuda_h.py:19] end restore2model cost 0.0030803680419921875 seconds
DEBUG 01-15 16:10:51.311156.311156 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0065004825592041016 seconds
DEBUG 01-15 16:10:51.311143.311143 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.312904.312904 cuda_h.py:19] end gpu_sexperts cost 0.0003161430358886719 seconds
DEBUG 01-15 16:10:51.312402.312402 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.312801.312801 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:10:51.312120.312120 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.312684.312684 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.313171.313171 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010633468627929688 seconds
DEBUG 01-15 16:10:51.313828.313828 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.313498.313498 cuda_h.py:19] end gpu_group_list cost 0.00018286705017089844 seconds
DEBUG 01-15 16:10:51.314709.314709 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.313774.313774 cuda_h.py:19] end group_tensors cost 0.006663084030151367 seconds
DEBUG 01-15 16:10:51.314447.314447 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.317068.317068 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0025496482849121094 seconds
DEBUG 01-15 16:10:51.317296.317296 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.317246.317246 cuda_h.py:19] end gpu_group_list cost 0.00024056434631347656 seconds
DEBUG 01-15 16:10:51.318414.318414 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.318490.318490 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0f3c9da8-2832-4976-8ed3-1cf91bee1b7c
DEBUG 01-15 16:10:51.321761.321761 cuda_h.py:19] end group pad cost 0.0067386627197265625 seconds
DEBUG 01-15 16:10:51.321028.321028 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.334798.334798 client.py:127] Model loaded
DEBUG 01-15 16:10:51.334904.334904 cuda_h.py:19] end wait_experts_multi_device cost 0.016290903091430664 seconds
DEBUG 01-15 16:10:51.335622.335622 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.348877.348877 cuda_h.py:19] end group_einsum cost 0.02704143524169922 seconds
DEBUG 01-15 16:10:51.348696.348696 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.353774.353774 cuda_h.py:19] end get_outputs_cpu1 cost 0.004030466079711914 seconds
DEBUG 01-15 16:10:51.353768.353768 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04768538475036621 seconds
DEBUG 01-15 16:10:51.354693.354693 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01904916763305664 seconds
DEBUG 01-15 16:10:51.354365.354365 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.355583.355583 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.356788.356788 cuda_h.py:19] end index_scatter cost 8.845329284667969e-05 seconds
DEBUG 01-15 16:10:51.357677.357677 cuda_h.py:19] end cpuoutputsdeal cost 0.0027365684509277344 seconds
DEBUG 01-15 16:10:51.357839.357839 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.357271.357271 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.357827.357827 cuda_h.py:19] end gpu_group_tensor cost 0.00016641616821289062 seconds
DEBUG 01-15 16:10:51.357636.357636 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.357934.357934 cuda_h.py:19] end gpu_group_tensor cost 0.000152587890625 seconds
DEBUG 01-15 16:10:51.357712.357712 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.358520.358520 cuda_h.py:19] end gpu_group_einsum cost 0.0005488395690917969 seconds
DEBUG 01-15 16:10:51.358889.358889 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.358263.358263 cuda_h.py:19] end gpu_group_einsum cost 0.0004353523254394531 seconds
DEBUG 01-15 16:10:51.359472.359472 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.359284.359284 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.359287.359287 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002434253692626953 seconds
DEBUG 01-15 16:10:51.359235.359235 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.359642.359642 cuda_h.py:19] end concat_expert_out cost 5.459785461425781e-05 seconds
DEBUG 01-15 16:10:51.359492.359492 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.359720.359720 cuda_h.py:19] end index_scatter cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:10:51.360948.360948 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008914470672607422 seconds
DEBUG 01-15 16:10:51.360521.360521 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.360417.360417 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.360088.360088 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021457672119140625 seconds
DEBUG 01-15 16:10:51.360228.360228 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.360549.360549 cuda_h.py:19] end concat_expert_out cost 6.198883056640625e-05 seconds
DEBUG 01-15 16:10:51.360922.360922 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.360289.360289 cuda_h.py:19] end index_scatter cost 6.031990051269531e-05 seconds
DEBUG 01-15 16:10:51.360529.360529 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005998611450195312 seconds
DEBUG 01-15 16:10:51.360452.360452 cuda_h.py:19] end gpu_experts_multi_device cost 0.0486757755279541 seconds
DEBUG 01-15 16:10:51.360091.360091 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_15 cost 0.05882525444030762 seconds
DEBUG 01-15 16:10:51.361551.361551 cuda_h.py:19] end prefill_layer cost 0.06557774543762207 seconds
DEBUG 01-15 16:10:51.361672.361672 lmp.py:1553] -------------------------------- end prefill layer 14 --------------------------------
DEBUG 01-15 16:10:51.361236.361236 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.361992.361992 lmp.py:1495] -------------------------------- start prefill layer 15 --------------------------------
DEBUG 01-15 16:10:51.361079.361079 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:51.361504.361504 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 01-15 16:10:51.361744.361744 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 4.124641418457031e-05 seconds
DEBUG 01-15 16:10:51.361838.361838 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 7.796287536621094e-05 seconds
DEBUG 01-15 16:10:51.361064.361064 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.361418.361418 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.361679.361679 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.361709.361709 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.362406.362406 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.362078.362078 cuda_h.py:19] end allocate_cuda_memory cost 0.0002460479736328125 seconds
DEBUG 01-15 16:10:51.362247.362247 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.362155.362155 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.362508.362508 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.362264.362264 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0298f134-cb54-4c42-b67e-18e777c81faf
DEBUG 01-15 16:10:51.362698.362698 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.362259.362259 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.364528.364528 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0298f134-cb54-4c42-b67e-18e777c81faf
DEBUG 01-15 16:10:51.364809.364809 cuda_h.py:19] end load_into_gpu_async cost 0.0023059844970703125 seconds
DEBUG 01-15 16:10:51.364611.364611 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.364959.364959 cuda_h.py:19] end restore_tensors2 cost 8.559226989746094e-05 seconds
DEBUG 01-15 16:10:51.364960.364960 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0029256343841552734 seconds
INFO 01-15 16:10:51.364850.364850 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0298f134-cb54-4c42-b67e-18e777c81faf
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.366957.366957 cuda_h.py:19] end self_attn cost 0.003951072692871094 seconds
DEBUG 01-15 16:10:51.367809.367809 cuda_h.py:19] end iln_self_attn_paln cost 0.005604743957519531 seconds
DEBUG 01-15 16:10:51.367354.367354 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_16
DEBUG 01-15 16:10:51.367879.367879 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.368281.368281 cuda_h.py:19] end gate cost 0.0007157325744628906 seconds
DEBUG 01-15 16:10:51.368833.368833 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.368618.368618 lmp.py:1912] 
DEBUG 01-15 16:10:51.368618.368618 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.368302.368302 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.368766.368766 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.368939.368939 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.368251.368251 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.368086.368086 lmp.py:1917] 
DEBUG 01-15 16:10:51.368086.368086 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.368160.368160 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.368670.368670 lmp.py:1935]   Expert 15 |     64 | CPU
DEBUG 01-15 16:10:51.368744.368744 lmp.py:1935]   Expert 41 |     72 | CPU
DEBUG 01-15 16:10:51.368579.368579 lmp.py:1935]   Expert  0 |     76 | CPU
DEBUG 01-15 16:10:51.368414.368414 lmp.py:1935]   Expert 63 |     76 | CPU
DEBUG 01-15 16:10:51.369772.369772 lmp.py:1935]   Expert 20 |     83 | CPU
DEBUG 01-15 16:10:51.369130.369130 lmp.py:1935]   Expert 45 |     92 | CPU
DEBUG 01-15 16:10:51.369158.369158 lmp.py:1935]   Expert  7 |     93 | CPU
DEBUG 01-15 16:10:51.369708.369708 lmp.py:1935]   Expert 28 |     96 | CPU
DEBUG 01-15 16:10:51.369543.369543 lmp.py:1935]   Expert 54 |    106 | CPU
DEBUG 01-15 16:10:51.369901.369901 lmp.py:1935]   Expert 12 |    108 | CPU
DEBUG 01-15 16:10:51.369498.369498 lmp.py:1935]   Expert 40 |    121 | CPU
DEBUG 01-15 16:10:51.369379.369379 lmp.py:1935]   Expert 52 |    122 | CPU
DEBUG 01-15 16:10:51.369738.369738 lmp.py:1935]   Expert 59 |    122 | CPU
DEBUG 01-15 16:10:51.369857.369857 lmp.py:1935]   Expert  5 |    123 | CPU
DEBUG 01-15 16:10:51.369216.369216 lmp.py:1935]   Expert  4 |    131 | CPU
DEBUG 01-15 16:10:51.369574.369574 lmp.py:1935]   Expert 34 |    132 | CPU
DEBUG 01-15 16:10:51.369455.369455 lmp.py:1935]   Expert 62 |    135 | CPU
DEBUG 01-15 16:10:51.369337.369337 lmp.py:1935]   Expert 13 |    137 | CPU
DEBUG 01-15 16:10:51.369218.369218 lmp.py:1935]   Expert 61 |    137 | CPU
DEBUG 01-15 16:10:51.369530.369530 lmp.py:1935]   Expert 21 |    139 | CPU
DEBUG 01-15 16:10:51.369127.369127 lmp.py:1935]   Expert 42 |    139 | CPU
DEBUG 01-15 16:10:51.369723.369723 lmp.py:1935]   Expert 55 |    141 | CPU
DEBUG 01-15 16:10:51.369843.369843 lmp.py:1935]   Expert 14 |    145 | CPU
DEBUG 01-15 16:10:51.369486.369486 lmp.py:1935]   Expert 10 |    149 | CPU
DEBUG 01-15 16:10:51.369367.369367 lmp.py:1935]   Expert 22 |    149 | CPU
DEBUG 01-15 16:10:51.369249.369249 lmp.py:1935]   Expert 51 |    155 | CPU
DEBUG 01-15 16:10:51.369607.369607 lmp.py:1935]   Expert 32 |    159 | CPU
DEBUG 01-15 16:10:51.369488.369488 lmp.py:1935]   Expert 25 |    166 | CPU
DEBUG 01-15 16:10:51.369370.369370 lmp.py:1935]   Expert 53 |    175 | CPU
DEBUG 01-15 16:10:51.369251.369251 lmp.py:1935]   Expert  1 |    176 | CPU
DEBUG 01-15 16:10:51.369609.369609 lmp.py:1935]   Expert 47 |    176 | CPU
DEBUG 01-15 16:10:51.369729.369729 lmp.py:1935]   Expert 19 |    178 | CPU
DEBUG 01-15 16:10:51.369756.369756 lmp.py:1935]   Expert 50 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369545.369545 lmp.py:1935]   Expert 26 |    180 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369095.369095 lmp.py:1935]   Expert  6 |    181 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369123.369123 lmp.py:1935]   Expert  2 |    183 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369911.369911 lmp.py:1935]   Expert 35 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369462.369462 lmp.py:1935]   Expert 11 |    184 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369535.369535 lmp.py:1935]   Expert 30 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369847.369847 lmp.py:1935]   Expert 57 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369921.369921 lmp.py:1935]   Expert 56 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369994.369994 lmp.py:1935]   Expert 48 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369068.369068 lmp.py:1935]   Expert 24 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369379.369379 lmp.py:1935]   Expert 44 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369453.369453 lmp.py:1935]   Expert 16 |    213 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369765.369765 lmp.py:1935]   Expert 46 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369600.369600 lmp.py:1935]   Expert 39 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369673.369673 lmp.py:1935]   Expert 18 |    227 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369508.369508 lmp.py:1935]   Expert 29 |    234 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369774.369774 lmp.py:1935]   Expert 37 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369993.369993 lmp.py:1935]   Expert 31 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369497.369497 lmp.py:1935]   Expert  3 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369286.369286 lmp.py:1935]   Expert 36 |    257 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369313.369313 lmp.py:1935]   Expert 60 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369625.369625 lmp.py:1935]   Expert 38 |    263 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.369652.369652 lmp.py:1935]   Expert 17 |    265 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.369441.369441 lmp.py:1935]   Expert  9 |    266 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.370468.370468 lmp.py:1935]   Expert 23 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.370734.370734 lmp.py:1935]   Expert 27 |    344 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.370761.370761 lmp.py:1935]   Expert 43 |    360 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.370311.370311 lmp.py:1935]   Expert  8 |    398 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.370100.370100 lmp.py:1935]   Expert 33 |    399 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.370889.370889 lmp.py:1935]   Expert 58 |    444 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.370916.370916 lmp.py:1935]   Expert 49 |    540 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.370274.370274 lmp.py:1937] 
DEBUG 01-15 16:10:51.370274.370274 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.370063.370063 lmp.py:1938]   CPU:   4073 tokens
DEBUG 01-15 16:10:51.370328.370328 lmp.py:1942]   cuda:1:   4121 tokens (16 experts)
DEBUG 01-15 16:10:51.370832.370832 lmp.py:1942]   cuda:2:   4094 tokens (16 experts)
DEBUG 01-15 16:10:51.370144.370144 lmp.py:1943]   Total GPU:   8215 tokens
DEBUG 01-15 16:10:51.370456.370456 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.370456.370456 lmp.py:1944] 
DEBUG 01-15 16:10:51.370205.370205 cuda_h.py:19] end experts_map_get cost 0.0019989013671875 seconds
DEBUG 01-15 16:10:51.370638.370638 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.370302.370302 lmp.py:1953] 
DEBUG 01-15 16:10:51.370302.370302 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.370946.370946 cuda_h.py:19] end cpu_experts_submit cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:51.370311.370311 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.370809.370809 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.371951.371951 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.15.mlp.experts.2.gate_proj.weight': 0, 'model.layers.15.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.38.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.38.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.38.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.9.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.9.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.9.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.6.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.6.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.6.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.15.mlp.experts.33.gate_proj.weight': 0, 'model.layers.15.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.15.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.15.mlp.experts.3.gate_proj.weight': 17301504, 'model.layers.15.mlp.experts.3.down_proj.weight': 23068672, 'model.layers.15.mlp.experts.3.up_proj.weight': 28835840, 'model.layers.15.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.15.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.15.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.15.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.15.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.15.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.15.mlp.experts.26.gate_proj.weight': 69206016, 'model.layers.15.mlp.experts.26.down_proj.weight': 74973184, 'model.layers.15.mlp.experts.26.up_proj.weight': 80740352, 'model.layers.15.mlp.experts.43.gate_proj.weight': 86507520, 'model.layers.15.mlp.experts.43.down_proj.weight': 92274688, 'model.layers.15.mlp.experts.43.up_proj.weight': 98041856, 'model.layers.15.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.15.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.15.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.15.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.15.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.15.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.15.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.15.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.15.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.15.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.15.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.15.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.15.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.15.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.15.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.15.mlp.experts.23.gate_proj.weight': 190316544, 'model.layers.15.mlp.experts.23.down_proj.weight': 196083712, 'model.layers.15.mlp.experts.23.up_proj.weight': 201850880, 'model.layers.15.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.15.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.15.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.15.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.15.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.15.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.15.mlp.experts.58.gate_proj.weight': 242221056, 'model.layers.15.mlp.experts.58.down_proj.weight': 247988224, 'model.layers.15.mlp.experts.58.up_proj.weight': 253755392, 'model.layers.15.mlp.experts.60.gate_proj.weight': 259522560, 'model.layers.15.mlp.experts.60.down_proj.weight': 265289728, 'model.layers.15.mlp.experts.60.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(18397265920, 5767168, 0, 0), (18403033088, 5767168, 5767168, 0), (18391498752, 5767168, 11534336, 0), (18985517056, 5767168, 17301504, 0), (18991284224, 5767168, 23068672, 0), (18979749888, 5767168, 28835840, 0), (19020120064, 5767168, 34603008, 0), (19025887232, 5767168, 40370176, 0), (19014352896, 5767168, 46137344, 0), (19037421568, 5767168, 51904512, 0), (19043188736, 5767168, 57671680, 0), (19031654400, 5767168, 63438848, 0), (18501074944, 5767168, 69206016, 0), (18506842112, 5767168, 74973184, 0), (18495307776, 5767168, 80740352, 0), (18518376448, 5767168, 86507520, 0), (18524143616, 5767168, 92274688, 0), (18512609280, 5767168, 98041856, 0), (18466471936, 5767168, 103809024, 0), (18472239104, 5767168, 109576192, 0), (18460704768, 5767168, 115343360, 0), (19123929088, 5767168, 121110528, 0), (19129696256, 5767168, 126877696, 0), (19118161920, 5767168, 132644864, 0), (18639486976, 5767168, 138412032, 0), (18645254144, 5767168, 144179200, 0), (18633719808, 5767168, 149946368, 0), (19210436608, 5767168, 155713536, 0), (19216203776, 5767168, 161480704, 0), (19204669440, 5767168, 167247872, 0), (19227738112, 5767168, 173015040, 0), (19233505280, 5767168, 178782208, 0), (19221970944, 5767168, 184549376, 0), (19331547136, 5767168, 190316544, 0), (19337314304, 5767168, 196083712, 0), (19325779968, 5767168, 201850880, 0), (18829803520, 5767168, 207618048, 0), (18835570688, 5767168, 213385216, 0), (18824036352, 5767168, 219152384, 0), (18864406528, 5767168, 224919552, 0), (18870173696, 5767168, 230686720, 0), (18858639360, 5767168, 236453888, 0), (18881708032, 5767168, 242221056, 0), (18887475200, 5767168, 247988224, 0), (18875940864, 5767168, 253755392, 0), (18899009536, 5767168, 259522560, 0), (18904776704, 5767168, 265289728, 0), (18893242368, 5767168, 271056896, 0)], 2: [(18933612544, 5767168, 0, 0), (18939379712, 5767168, 5767168, 0), (18927845376, 5767168, 11534336, 0), (18414567424, 5767168, 17301504, 0), (18420334592, 5767168, 23068672, 0), (18408800256, 5767168, 28835840, 0), (18968215552, 5767168, 34603008, 0), (18973982720, 5767168, 40370176, 0), (18962448384, 5767168, 46137344, 0), (19002818560, 5767168, 51904512, 0), (19008585728, 5767168, 57671680, 0), (18997051392, 5767168, 63438848, 0), (18812502016, 5767168, 69206016, 0), (18818269184, 5767168, 74973184, 0), (18806734848, 5767168, 80740352, 0), (19106627584, 5767168, 86507520, 0), (19112394752, 5767168, 92274688, 0), (19100860416, 5767168, 98041856, 0), (18552979456, 5767168, 103809024, 0), (18558746624, 5767168, 109576192, 0), (18547212288, 5767168, 115343360, 0), (19158532096, 5767168, 121110528, 0), (19164299264, 5767168, 126877696, 0), (19152764928, 5767168, 132644864, 0), (19193135104, 5767168, 138412032, 0), (19198902272, 5767168, 144179200, 0), (19187367936, 5767168, 149946368, 0), (18656788480, 5767168, 155713536, 0), (18662555648, 5767168, 161480704, 0), (18651021312, 5767168, 167247872, 0), (18674089984, 5767168, 173015040, 0), (18679857152, 5767168, 178782208, 0), (18668322816, 5767168, 184549376, 0), (18760597504, 5767168, 190316544, 0), (18766364672, 5767168, 196083712, 0), (18754830336, 5767168, 201850880, 0), (18777899008, 5767168, 207618048, 0), (18783666176, 5767168, 213385216, 0), (18772131840, 5767168, 219152384, 0), (19348848640, 5767168, 224919552, 0), (19354615808, 5767168, 230686720, 0), (19343081472, 5767168, 236453888, 0), (19366150144, 5767168, 242221056, 0), (19371917312, 5767168, 247988224, 0), (19360382976, 5767168, 253755392, 0), (19400753152, 5767168, 259522560, 0), (19406520320, 5767168, 265289728, 0), (19394985984, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e6c469a40>, 2: <capsule object NULL at 0x7a51b06da970>}
DEBUG 01-15 16:10:51.371261.371261 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.371621.371621 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.371661.371661 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 790ccc26-4495-44f5-850b-30dc8376e6fe
DEBUG 01-15 16:10:51.371966.371966 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.371283.371283 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:51.372508.372508 client.py:127] Model loaded
DEBUG 01-15 16:10:51.372053.372053 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.372564.372564 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.372099.372099 cuda_h.py:19] end restore2model cost 0.0003523826599121094 seconds
DEBUG 01-15 16:10:51.372491.372491 cuda_h.py:19] end sllm_worker_task cost 0.010522842407226562 seconds
INFO 01-15 16:10:51.372362.372362 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 790ccc26-4495-44f5-850b-30dc8376e6fe
DEBUG 01-15 16:10:51.372627.372627 cuda_h.py:19] end move_flatidxs cost 0.0008304119110107422 seconds
DEBUG 01-15 16:10:51.373642.373642 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.373670.373670 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0027420520782470703 seconds
DEBUG 01-15 16:10:51.373256.373256 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.376924.376924 cuda_h.py:19] end restore2model cost 0.0030870437622070312 seconds
DEBUG 01-15 16:10:51.376151.376151 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006071805953979492 seconds
DEBUG 01-15 16:10:51.376092.376092 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.376898.376898 cuda_h.py:19] end gpu_sexperts cost 0.00031495094299316406 seconds
DEBUG 01-15 16:10:51.376351.376351 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.377465.377465 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.811981201171875e-05 seconds
DEBUG 01-15 16:10:51.377307.377307 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.377109.377109 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.378596.378596 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0010633468627929688 seconds
DEBUG 01-15 16:10:51.378638.378638 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.378453.378453 cuda_h.py:19] end gpu_group_list cost 0.0001785755157470703 seconds
DEBUG 01-15 16:10:51.378203.378203 cuda_h.py:19] end group_tensors cost 0.005259513854980469 seconds
DEBUG 01-15 16:10:51.379036.379036 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.379776.379776 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.381279.381279 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0018663406372070312 seconds
DEBUG 01-15 16:10:51.381629.381629 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.382646.382646 cuda_h.py:19] end gpu_group_list cost 0.0002911090850830078 seconds
DEBUG 01-15 16:10:51.383396.383396 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.383743.383743 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 790ccc26-4495-44f5-850b-30dc8376e6fe
DEBUG 01-15 16:10:51.383003.383003 cuda_h.py:19] end group pad cost 0.0044841766357421875 seconds
DEBUG 01-15 16:10:51.383939.383939 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.400152.400152 client.py:127] Model loaded
DEBUG 01-15 16:10:51.401413.401413 cuda_h.py:19] end wait_experts_multi_device cost 0.01743030548095703 seconds
DEBUG 01-15 16:10:51.401800.401800 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.406402.406402 cuda_h.py:19] end group_einsum cost 0.022647380828857422 seconds
DEBUG 01-15 16:10:51.406480.406480 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.410289.410289 cuda_h.py:19] end get_outputs_cpu1 cost 0.004105806350708008 seconds
DEBUG 01-15 16:10:51.411025.411025 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03949689865112305 seconds
DEBUG 01-15 16:10:51.411633.411633 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01074361801147461 seconds
DEBUG 01-15 16:10:51.412113.412113 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.413952.413952 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.413342.413342 cuda_h.py:19] end index_scatter cost 9.083747863769531e-05 seconds
DEBUG 01-15 16:10:51.414135.414135 cuda_h.py:19] end cpuoutputsdeal cost 0.0021843910217285156 seconds
DEBUG 01-15 16:10:51.414674.414674 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.414821.414821 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.414842.414842 cuda_h.py:19] end gpu_group_tensor cost 0.0001895427703857422 seconds
DEBUG 01-15 16:10:51.414135.414135 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.414790.414790 cuda_h.py:19] end gpu_group_tensor cost 0.0001666545867919922 seconds
DEBUG 01-15 16:10:51.414430.414430 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.415070.415070 cuda_h.py:19] end gpu_group_einsum cost 0.0006639957427978516 seconds
DEBUG 01-15 16:10:51.415519.415519 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.416512.416512 cuda_h.py:19] end gpu_group_einsum cost 0.000530242919921875 seconds
DEBUG 01-15 16:10:51.416205.416205 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.416408.416408 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.416047.416047 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002543926239013672 seconds
DEBUG 01-15 16:10:51.416664.416664 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.417455.417455 cuda_h.py:19] end concat_expert_out cost 5.650520324707031e-05 seconds
DEBUG 01-15 16:10:51.417259.417259 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.417540.417540 cuda_h.py:19] end index_scatter cost 6.651878356933594e-05 seconds
DEBUG 01-15 16:10:51.417390.417390 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0009300708770751953 seconds
DEBUG 01-15 16:10:51.417526.417526 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.417614.417614 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.417549.417549 cuda_h.py:19] end all_expert_outputs_slices cost 0.00019931793212890625 seconds
DEBUG 01-15 16:10:51.417305.417305 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.418812.418812 cuda_h.py:19] end concat_expert_out cost 5.841255187988281e-05 seconds
DEBUG 01-15 16:10:51.418284.418284 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.418705.418705 cuda_h.py:19] end index_scatter cost 6.270408630371094e-05 seconds
DEBUG 01-15 16:10:51.418991.418991 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0005860328674316406 seconds
DEBUG 01-15 16:10:51.418007.418007 cuda_h.py:19] end gpu_experts_multi_device cost 0.041255950927734375 seconds
DEBUG 01-15 16:10:51.418930.418930 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_16 cost 0.050989627838134766 seconds
DEBUG 01-15 16:10:51.418179.418179 cuda_h.py:19] end prefill_layer cost 0.05740189552307129 seconds
DEBUG 01-15 16:10:51.418730.418730 lmp.py:1553] -------------------------------- end prefill layer 15 --------------------------------
DEBUG 01-15 16:10:51.419579.419579 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.419335.419335 lmp.py:1495] -------------------------------- start prefill layer 16 --------------------------------
DEBUG 01-15 16:10:51.419422.419422 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:51.419324.419324 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 01-15 16:10:51.419896.419896 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 3.981590270996094e-05 seconds
DEBUG 01-15 16:10:51.419705.419705 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 7.748603820800781e-05 seconds
DEBUG 01-15 16:10:51.419977.419977 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.419900.419900 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.419446.419446 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.419721.419721 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.419988.419988 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.419952.419952 cuda_h.py:19] end allocate_cuda_memory cost 0.00027823448181152344 seconds
DEBUG 01-15 16:10:51.419690.419690 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.420268.420268 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.420097.420097 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.420522.420522 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a68bd13b-b391-4499-a37b-fbbdc4ff149d
DEBUG 01-15 16:10:51.420864.420864 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.420611.420611 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.421691.421691 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a68bd13b-b391-4499-a37b-fbbdc4ff149d
DEBUG 01-15 16:10:51.422587.422587 cuda_h.py:19] end load_into_gpu_async cost 0.002026081085205078 seconds
DEBUG 01-15 16:10:51.422867.422867 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.422738.422738 cuda_h.py:19] end restore_tensors2 cost 8.487701416015625e-05 seconds
DEBUG 01-15 16:10:51.422786.422786 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0026836395263671875 seconds
INFO 01-15 16:10:51.422344.422344 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a68bd13b-b391-4499-a37b-fbbdc4ff149d
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.424830.424830 cuda_h.py:19] end self_attn cost 0.0038690567016601562 seconds
DEBUG 01-15 16:10:51.424120.424120 cuda_h.py:19] end iln_self_attn_paln cost 0.005616903305053711 seconds
DEBUG 01-15 16:10:51.424665.424665 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_17
DEBUG 01-15 16:10:51.424904.424904 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.425912.425912 cuda_h.py:19] end gate cost 0.0007755756378173828 seconds
DEBUG 01-15 16:10:51.425841.425841 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.426791.426791 lmp.py:1912] 
DEBUG 01-15 16:10:51.426791.426791 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.426216.426216 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.426011.426011 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.426992.426992 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.426589.426589 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.426232.426232 lmp.py:1917] 
DEBUG 01-15 16:10:51.426232.426232 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.426067.426067 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.426670.426670 lmp.py:1935]   Expert 58 |     35 | CPU
DEBUG 01-15 16:10:51.426505.426505 lmp.py:1935]   Expert 47 |     59 | CPU
DEBUG 01-15 16:10:51.426817.426817 lmp.py:1935]   Expert 31 |     60 | CPU
DEBUG 01-15 16:10:51.426175.426175 lmp.py:1935]   Expert 49 |     62 | CPU
DEBUG 01-15 16:10:51.426057.426057 lmp.py:1935]   Expert  4 |     64 | CPU
DEBUG 01-15 16:10:51.426700.426700 lmp.py:1935]   Expert 38 |     69 | CPU
DEBUG 01-15 16:10:51.426581.426581 lmp.py:1935]   Expert 45 |     74 | CPU
DEBUG 01-15 16:10:51.426939.426939 lmp.py:1935]   Expert 41 |     82 | CPU
DEBUG 01-15 16:10:51.426821.426821 lmp.py:1935]   Expert 43 |     82 | CPU
DEBUG 01-15 16:10:51.426225.426225 lmp.py:1935]   Expert 33 |     94 | CPU
DEBUG 01-15 16:10:51.426630.426630 lmp.py:1935]   Expert 50 |     98 | CPU
DEBUG 01-15 16:10:51.426511.426511 lmp.py:1935]   Expert 57 |    102 | CPU
DEBUG 01-15 16:10:51.426108.426108 lmp.py:1935]   Expert 11 |    107 | CPU
DEBUG 01-15 16:10:51.426420.426420 lmp.py:1935]   Expert  2 |    113 | CPU
DEBUG 01-15 16:10:51.426447.426447 lmp.py:1935]   Expert 51 |    116 | CPU
DEBUG 01-15 16:10:51.426282.426282 lmp.py:1935]   Expert  0 |    123 | CPU
DEBUG 01-15 16:10:51.426879.426879 lmp.py:1935]   Expert 14 |    124 | CPU
DEBUG 01-15 16:10:51.426714.426714 lmp.py:1935]   Expert 54 |    128 | CPU
DEBUG 01-15 16:10:51.426072.426072 lmp.py:1935]   Expert 26 |    141 | CPU
DEBUG 01-15 16:10:51.426430.426430 lmp.py:1935]   Expert 34 |    142 | CPU
DEBUG 01-15 16:10:51.426265.426265 lmp.py:1935]   Expert 56 |    142 | CPU
DEBUG 01-15 16:10:51.426862.426862 lmp.py:1935]   Expert 27 |    154 | CPU
DEBUG 01-15 16:10:51.426651.426651 lmp.py:1935]   Expert 28 |    158 | CPU
DEBUG 01-15 16:10:51.426486.426486 lmp.py:1935]   Expert 55 |    159 | CPU
DEBUG 01-15 16:10:51.426082.426082 lmp.py:1935]   Expert 10 |    163 | CPU
DEBUG 01-15 16:10:51.426679.426679 lmp.py:1935]   Expert 25 |    163 | CPU
DEBUG 01-15 16:10:51.426037.426037 lmp.py:1935]   Expert  9 |    177 | CPU
DEBUG 01-15 16:10:51.426872.426872 lmp.py:1935]   Expert 13 |    179 | CPU
DEBUG 01-15 16:10:51.426469.426469 lmp.py:1935]   Expert 61 |    186 | CPU
DEBUG 01-15 16:10:51.427827.427827 lmp.py:1935]   Expert  6 |    193 | CPU
DEBUG 01-15 16:10:51.427662.427662 lmp.py:1935]   Expert  7 |    193 | CPU
DEBUG 01-15 16:10:51.427213.427213 lmp.py:1935]   Expert 48 |    193 | CPU
DEBUG 01-15 16:10:51.427193.427193 lmp.py:1935]   Expert 24 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427221.427221 lmp.py:1935]   Expert 42 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427248.427248 lmp.py:1935]   Expert 46 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427798.427798 lmp.py:1935]   Expert 18 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427587.427587 lmp.py:1935]   Expert 40 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427376.427376 lmp.py:1935]   Expert 12 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427926.427926 lmp.py:1935]   Expert 63 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427476.427476 lmp.py:1935]   Expert 22 |    218 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427695.427695 lmp.py:1935]   Expert 29 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427961.427961 lmp.py:1935]   Expert 59 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427750.427750 lmp.py:1935]   Expert 21 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427300.427300 lmp.py:1935]   Expert 32 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427850.427850 lmp.py:1935]   Expert 19 |    229 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427639.427639 lmp.py:1935]   Expert 36 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427951.427951 lmp.py:1935]   Expert  3 |    242 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427740.427740 lmp.py:1935]   Expert 37 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427290.427290 lmp.py:1935]   Expert  1 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427602.427602 lmp.py:1935]   Expert 16 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427775.427775 lmp.py:1935]   Expert 20 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427041.427041 lmp.py:1935]   Expert  5 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427591.427591 lmp.py:1935]   Expert  8 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427903.427903 lmp.py:1935]   Expert 30 |    269 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427215.427215 lmp.py:1935]   Expert 62 |    272 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427765.427765 lmp.py:1935]   Expert 15 |    273 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427315.427315 lmp.py:1935]   Expert 35 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427058.427058 lmp.py:1935]   Expert 39 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427608.427608 lmp.py:1935]   Expert 17 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427920.427920 lmp.py:1935]   Expert 60 |    319 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427993.427993 lmp.py:1935]   Expert 52 |    355 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427067.427067 lmp.py:1935]   Expert 23 |    362 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427379.427379 lmp.py:1935]   Expert 44 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.427691.427691 lmp.py:1935]   Expert 53 |    436 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.427287.427287 lmp.py:1937] 
DEBUG 01-15 16:10:51.427287.427287 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.427791.427791 lmp.py:1938]   CPU:   3935 tokens
DEBUG 01-15 16:10:51.427580.427580 lmp.py:1942]   cuda:1:   4177 tokens (16 experts)
DEBUG 01-15 16:10:51.427415.427415 lmp.py:1942]   cuda:2:   4176 tokens (16 experts)
DEBUG 01-15 16:10:51.427773.427773 lmp.py:1943]   Total GPU:   8353 tokens
DEBUG 01-15 16:10:51.427655.427655 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.427655.427655 lmp.py:1944] 
DEBUG 01-15 16:10:51.427450.427450 cuda_h.py:19] end experts_map_get cost 0.0019638538360595703 seconds
DEBUG 01-15 16:10:51.427307.427307 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.427255.427255 lmp.py:1953] 
DEBUG 01-15 16:10:51.427255.427255 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.427091.427091 cuda_h.py:19] end cpu_experts_submit cost 5.412101745605469e-05 seconds
DEBUG 01-15 16:10:51.428072.428072 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.428332.428332 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.429436.429436 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.16.mlp.experts.1.gate_proj.weight': 0, 'model.layers.16.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.35.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.35.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.35.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.8.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.8.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.8.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.52.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.52.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.52.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.16.mlp.experts.32.gate_proj.weight': 0, 'model.layers.16.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.16.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.16.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.16.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.16.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.16.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.16.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.16.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.16.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.16.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.16.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.16.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.16.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.16.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.16.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.16.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.16.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.16.mlp.experts.12.gate_proj.weight': 103809024, 'model.layers.16.mlp.experts.12.down_proj.weight': 109576192, 'model.layers.16.mlp.experts.12.up_proj.weight': 115343360, 'model.layers.16.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.16.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.16.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.16.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.16.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.16.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.16.mlp.experts.16.gate_proj.weight': 155713536, 'model.layers.16.mlp.experts.16.down_proj.weight': 161480704, 'model.layers.16.mlp.experts.16.up_proj.weight': 167247872, 'model.layers.16.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.16.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.16.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.16.mlp.experts.23.gate_proj.weight': 190316544, 'model.layers.16.mlp.experts.23.down_proj.weight': 196083712, 'model.layers.16.mlp.experts.23.up_proj.weight': 201850880, 'model.layers.16.mlp.experts.59.gate_proj.weight': 207618048, 'model.layers.16.mlp.experts.59.down_proj.weight': 213385216, 'model.layers.16.mlp.experts.59.up_proj.weight': 219152384, 'model.layers.16.mlp.experts.60.gate_proj.weight': 224919552, 'model.layers.16.mlp.experts.60.down_proj.weight': 230686720, 'model.layers.16.mlp.experts.60.up_proj.weight': 236453888, 'model.layers.16.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.16.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.16.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.16.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.16.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.16.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(19487260672, 5767168, 0, 0), (19493027840, 5767168, 5767168, 0), (19481493504, 5767168, 11534336, 0), (20075511808, 5767168, 17301504, 0), (20081278976, 5767168, 23068672, 0), (20069744640, 5767168, 28835840, 0), (19521863680, 5767168, 34603008, 0), (19527630848, 5767168, 40370176, 0), (19516096512, 5767168, 46137344, 0), (19608371200, 5767168, 51904512, 0), (19614138368, 5767168, 57671680, 0), (19602604032, 5767168, 63438848, 0), (20162019328, 5767168, 69206016, 0), (20167786496, 5767168, 74973184, 0), (20156252160, 5767168, 80740352, 0), (20196622336, 5767168, 86507520, 0), (20202389504, 5767168, 92274688, 0), (20190855168, 5767168, 98041856, 0), (19764084736, 5767168, 103809024, 0), (19769851904, 5767168, 109576192, 0), (19758317568, 5767168, 115343360, 0), (19798687744, 5767168, 121110528, 0), (19804454912, 5767168, 126877696, 0), (19792920576, 5767168, 132644864, 0), (20369637376, 5767168, 138412032, 0), (20375404544, 5767168, 144179200, 0), (20363870208, 5767168, 149946368, 0), (20386938880, 5767168, 155713536, 0), (20392706048, 5767168, 161480704, 0), (20381171712, 5767168, 167247872, 0), (19815989248, 5767168, 173015040, 0), (19821756416, 5767168, 178782208, 0), (19810222080, 5767168, 184549376, 0), (19833290752, 5767168, 190316544, 0), (19839057920, 5767168, 196083712, 0), (19827523584, 5767168, 201850880, 0), (19850592256, 5767168, 207618048, 0), (19856359424, 5767168, 213385216, 0), (19844825088, 5767168, 219152384, 0), (19885195264, 5767168, 224919552, 0), (19890962432, 5767168, 230686720, 0), (19879428096, 5767168, 236453888, 0), (20542652416, 5767168, 242221056, 0), (20548419584, 5767168, 247988224, 0), (20536885248, 5767168, 253755392, 0), (20559953920, 5767168, 259522560, 0), (20565721088, 5767168, 265289728, 0), (20554186752, 5767168, 271056896, 0)], 2: [(20023607296, 5767168, 0, 0), (20029374464, 5767168, 5767168, 0), (20017840128, 5767168, 11534336, 0), (20092813312, 5767168, 17301504, 0), (20098580480, 5767168, 23068672, 0), (20087046144, 5767168, 28835840, 0), (19556466688, 5767168, 34603008, 0), (19562233856, 5767168, 40370176, 0), (19550699520, 5767168, 46137344, 0), (20110114816, 5767168, 51904512, 0), (20115881984, 5767168, 57671680, 0), (20104347648, 5767168, 63438848, 0), (20144717824, 5767168, 69206016, 0), (20150484992, 5767168, 74973184, 0), (20138950656, 5767168, 80740352, 0), (20231225344, 5767168, 86507520, 0), (20236992512, 5767168, 92274688, 0), (20225458176, 5767168, 98041856, 0), (19677577216, 5767168, 103809024, 0), (19683344384, 5767168, 109576192, 0), (19671810048, 5767168, 115343360, 0), (20265828352, 5767168, 121110528, 0), (20271595520, 5767168, 126877696, 0), (20260061184, 5767168, 132644864, 0), (19729481728, 5767168, 138412032, 0), (19735248896, 5767168, 144179200, 0), (19723714560, 5767168, 149946368, 0), (19746783232, 5767168, 155713536, 0), (19752550400, 5767168, 161480704, 0), (19741016064, 5767168, 167247872, 0), (19781386240, 5767168, 173015040, 0), (19787153408, 5767168, 178782208, 0), (19775619072, 5767168, 184549376, 0), (19867893760, 5767168, 190316544, 0), (19873660928, 5767168, 196083712, 0), (19862126592, 5767168, 201850880, 0), (20490747904, 5767168, 207618048, 0), (20496515072, 5767168, 213385216, 0), (20484980736, 5767168, 219152384, 0), (20508049408, 5767168, 224919552, 0), (20513816576, 5767168, 230686720, 0), (20502282240, 5767168, 236453888, 0), (19971702784, 5767168, 242221056, 0), (19977469952, 5767168, 247988224, 0), (19965935616, 5767168, 253755392, 0), (19989004288, 5767168, 259522560, 0), (19994771456, 5767168, 265289728, 0), (19983237120, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e6c3ad860>, 2: <capsule object NULL at 0x7a51b06da850>}
DEBUG 01-15 16:10:51.429301.429301 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.429462.429462 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.429357.429357 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 940f0a5e-127a-41f1-ae96-581d93f62079
DEBUG 01-15 16:10:51.430423.430423 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:51.430008.430008 client.py:127] Model loaded
DEBUG 01-15 16:10:51.430791.430791 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.430070.430070 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:51.431101.431101 cuda_h.py:19] end restore2model cost 0.0003361701965332031 seconds
DEBUG 01-15 16:10:51.431063.431063 cuda_h.py:19] end sllm_worker_task cost 0.01160430908203125 seconds
DEBUG 01-15 16:10:51.431478.431478 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:51.431093.431093 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 940f0a5e-127a-41f1-ae96-581d93f62079
DEBUG 01-15 16:10:51.432276.432276 cuda_h.py:19] end move_flatidxs cost 0.0008447170257568359 seconds
DEBUG 01-15 16:10:51.432066.432066 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.432953.432953 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004250764846801758 seconds
DEBUG 01-15 16:10:51.432598.432598 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.435329.435329 cuda_h.py:19] end restore2model cost 0.002610445022583008 seconds
DEBUG 01-15 16:10:51.435324.435324 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0071141719818115234 seconds
DEBUG 01-15 16:10:51.435551.435551 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.435124.435124 cuda_h.py:19] end gpu_sexperts cost 0.00028514862060546875 seconds
DEBUG 01-15 16:10:51.435623.435623 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.435160.435160 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:10:51.435764.435764 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.435705.435705 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.436231.436231 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008478164672851562 seconds
DEBUG 01-15 16:10:51.436412.436412 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.436227.436227 cuda_h.py:19] end gpu_group_list cost 0.0001850128173828125 seconds
DEBUG 01-15 16:10:51.437213.437213 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.438947.438947 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008978843688964844 seconds
DEBUG 01-15 16:10:51.438473.438473 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.438434.438434 cuda_h.py:19] end gpu_group_list cost 0.00018548965454101562 seconds
DEBUG 01-15 16:10:51.439057.439057 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.439370.439370 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 940f0a5e-127a-41f1-ae96-581d93f62079
DEBUG 01-15 16:10:51.442991.442991 cuda_h.py:19] end group_tensors cost 0.010750770568847656 seconds
DEBUG 01-15 16:10:51.443613.443613 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.447874.447874 cuda_h.py:19] end group pad cost 0.004005908966064453 seconds
DEBUG 01-15 16:10:51.447764.447764 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.458708.458708 client.py:127] Model loaded
DEBUG 01-15 16:10:51.459009.459009 cuda_h.py:19] end wait_experts_multi_device cost 0.01963973045349121 seconds
DEBUG 01-15 16:10:51.459390.459390 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.470168.470168 cuda_h.py:19] end group_einsum cost 0.022253990173339844 seconds
DEBUG 01-15 16:10:51.470656.470656 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.474853.474853 cuda_h.py:19] end get_outputs_cpu1 cost 0.004026174545288086 seconds
DEBUG 01-15 16:10:51.475781.475781 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04410219192504883 seconds
DEBUG 01-15 16:10:51.475056.475056 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.016225814819335938 seconds
DEBUG 01-15 16:10:51.475152.475152 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.476591.476591 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.477093.477093 cuda_h.py:19] end index_scatter cost 7.462501525878906e-05 seconds
DEBUG 01-15 16:10:51.477666.477666 cuda_h.py:19] end cpuoutputsdeal cost 0.001672506332397461 seconds
DEBUG 01-15 16:10:51.477867.477867 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.477061.477061 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.477743.477743 cuda_h.py:19] end gpu_group_tensor cost 0.00014281272888183594 seconds
DEBUG 01-15 16:10:51.477982.477982 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.477477.477477 cuda_h.py:19] end gpu_group_tensor cost 0.00012445449829101562 seconds
DEBUG 01-15 16:10:51.477905.477905 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.478618.478618 cuda_h.py:19] end gpu_group_einsum cost 0.0004661083221435547 seconds
DEBUG 01-15 16:10:51.478165.478165 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.479070.479070 cuda_h.py:19] end gpu_group_einsum cost 0.0004730224609375 seconds
DEBUG 01-15 16:10:51.479723.479723 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.479641.479641 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.479511.479511 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002071857452392578 seconds
DEBUG 01-15 16:10:51.479850.479850 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.479939.479939 cuda_h.py:19] end concat_expert_out cost 6.413459777832031e-05 seconds
DEBUG 01-15 16:10:51.479113.479113 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.479560.479560 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:51.480203.480203 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008275508880615234 seconds
DEBUG 01-15 16:10:51.480663.480663 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.480453.480453 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.480029.480029 cuda_h.py:19] end all_expert_outputs_slices cost 0.00014638900756835938 seconds
DEBUG 01-15 16:10:51.480454.480454 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.480807.480807 cuda_h.py:19] end concat_expert_out cost 5.340576171875e-05 seconds
DEBUG 01-15 16:10:51.480551.480551 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.480090.480090 cuda_h.py:19] end index_scatter cost 4.8160552978515625e-05 seconds
DEBUG 01-15 16:10:51.480422.480422 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004892349243164062 seconds
DEBUG 01-15 16:10:51.480564.480564 cuda_h.py:19] end gpu_experts_multi_device cost 0.045229434967041016 seconds
DEBUG 01-15 16:10:51.480997.480997 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_17 cost 0.055971622467041016 seconds
DEBUG 01-15 16:10:51.481712.481712 cuda_h.py:19] end prefill_layer cost 0.06228375434875488 seconds
DEBUG 01-15 16:10:51.481939.481939 lmp.py:1553] -------------------------------- end prefill layer 16 --------------------------------
DEBUG 01-15 16:10:51.481119.481119 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.481537.481537 lmp.py:1495] -------------------------------- start prefill layer 17 --------------------------------
DEBUG 01-15 16:10:51.481193.481193 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:51.481234.481234 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 01-15 16:10:51.481892.481892 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 3.62396240234375e-05 seconds
DEBUG 01-15 16:10:51.481370.481370 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 7.510185241699219e-05 seconds
DEBUG 01-15 16:10:51.481973.481973 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.481744.481744 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.481383.481383 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.481863.481863 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.481468.481468 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.482398.482398 cuda_h.py:19] end allocate_cuda_memory cost 0.0002608299255371094 seconds
DEBUG 01-15 16:10:51.482329.482329 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.482714.482714 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.482351.482351 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.482630.482630 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2caf795a-eac5-4622-a90a-ed13466718eb
DEBUG 01-15 16:10:51.482734.482734 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.482001.482001 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.484523.484523 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2caf795a-eac5-4622-a90a-ed13466718eb
DEBUG 01-15 16:10:51.484989.484989 cuda_h.py:19] end load_into_gpu_async cost 0.0018870830535888672 seconds
DEBUG 01-15 16:10:51.484030.484030 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.484160.484160 cuda_h.py:19] end restore_tensors2 cost 0.00010251998901367188 seconds
DEBUG 01-15 16:10:51.484638.484638 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002537965774536133 seconds
INFO 01-15 16:10:51.484607.484607 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2caf795a-eac5-4622-a90a-ed13466718eb
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.486553.486553 cuda_h.py:19] end self_attn cost 0.003353118896484375 seconds
DEBUG 01-15 16:10:51.486756.486756 cuda_h.py:19] end iln_self_attn_paln cost 0.00493311882019043 seconds
DEBUG 01-15 16:10:51.486777.486777 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_18
DEBUG 01-15 16:10:51.486494.486494 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.487531.487531 cuda_h.py:19] end gate cost 0.0006911754608154297 seconds
DEBUG 01-15 16:10:51.487705.487705 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.487348.487348 lmp.py:1912] 
DEBUG 01-15 16:10:51.487348.487348 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.487926.487926 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.487867.487867 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.487470.487470 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.487690.487690 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.487194.487194 lmp.py:1917] 
DEBUG 01-15 16:10:51.487194.487194 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.488413.488413 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.488546.488546 lmp.py:1935]   Expert  4 |     10 | CPU
DEBUG 01-15 16:10:51.488242.488242 lmp.py:1935]   Expert 28 |     28 | CPU
DEBUG 01-15 16:10:51.488462.488462 lmp.py:1935]   Expert  7 |     47 | CPU
DEBUG 01-15 16:10:51.488442.488442 lmp.py:1935]   Expert 53 |     57 | CPU
DEBUG 01-15 16:10:51.488615.488615 lmp.py:1935]   Expert 52 |     68 | CPU
DEBUG 01-15 16:10:51.488596.488596 lmp.py:1935]   Expert 43 |     72 | CPU
DEBUG 01-15 16:10:51.488339.488339 lmp.py:1935]   Expert 49 |     83 | CPU
DEBUG 01-15 16:10:51.488081.488081 lmp.py:1935]   Expert 12 |     88 | CPU
DEBUG 01-15 16:10:51.488062.488062 lmp.py:1935]   Expert 47 |    103 | CPU
DEBUG 01-15 16:10:51.488043.488043 lmp.py:1935]   Expert 24 |    104 | CPU
DEBUG 01-15 16:10:51.488785.488785 lmp.py:1935]   Expert 33 |    107 | CPU
DEBUG 01-15 16:10:51.488527.488527 lmp.py:1935]   Expert 50 |    109 | CPU
DEBUG 01-15 16:10:51.488892.488892 lmp.py:1935]   Expert  2 |    110 | CPU
DEBUG 01-15 16:10:51.488059.488059 lmp.py:1935]   Expert 15 |    112 | CPU
DEBUG 01-15 16:10:51.488748.488748 lmp.py:1935]   Expert 60 |    113 | CPU
DEBUG 01-15 16:10:51.488199.488199 lmp.py:1935]   Expert 39 |    115 | CPU
DEBUG 01-15 16:10:51.488888.488888 lmp.py:1935]   Expert 36 |    118 | CPU
DEBUG 01-15 16:10:51.488922.488922 lmp.py:1935]   Expert 25 |    123 | CPU
DEBUG 01-15 16:10:51.488565.488565 lmp.py:1935]   Expert  6 |    124 | CPU
DEBUG 01-15 16:10:51.488446.488446 lmp.py:1935]   Expert 61 |    130 | CPU
DEBUG 01-15 16:10:51.488851.488851 lmp.py:1935]   Expert 59 |    134 | CPU
DEBUG 01-15 16:10:51.488540.488540 lmp.py:1935]   Expert  3 |    141 | CPU
DEBUG 01-15 16:10:51.488753.488753 lmp.py:1935]   Expert 27 |    145 | CPU
DEBUG 01-15 16:10:51.488965.488965 lmp.py:1935]   Expert 58 |    146 | CPU
DEBUG 01-15 16:10:51.488416.488416 lmp.py:1935]   Expert  8 |    149 | CPU
DEBUG 01-15 16:10:51.488867.488867 lmp.py:1935]   Expert 30 |    151 | CPU
DEBUG 01-15 16:10:51.488318.488318 lmp.py:1935]   Expert 31 |    151 | CPU
DEBUG 01-15 16:10:51.488246.488246 lmp.py:1935]   Expert 10 |    156 | CPU
DEBUG 01-15 16:10:51.488696.488696 lmp.py:1935]   Expert 38 |    158 | CPU
DEBUG 01-15 16:10:51.488147.488147 lmp.py:1935]   Expert 40 |    160 | CPU
DEBUG 01-15 16:10:51.488360.488360 lmp.py:1935]   Expert 14 |    161 | CPU
DEBUG 01-15 16:10:51.488334.488334 lmp.py:1935]   Expert 41 |    161 | CPU
DEBUG 01-15 16:10:51.488454.488454 lmp.py:1935]   Expert 57 |    161 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488812.488812 lmp.py:1935]   Expert 54 |    163 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488693.488693 lmp.py:1935]   Expert 32 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488336.488336 lmp.py:1935]   Expert 37 |    165 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488979.488979 lmp.py:1935]   Expert 46 |    168 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488337.488337 lmp.py:1935]   Expert 19 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488219.488219 lmp.py:1935]   Expert 42 |    173 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488862.488862 lmp.py:1935]   Expert 11 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488266.488266 lmp.py:1935]   Expert 34 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488194.488194 lmp.py:1935]   Expert 26 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488075.488075 lmp.py:1935]   Expert 22 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488718.488718 lmp.py:1935]   Expert 18 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488123.488123 lmp.py:1935]   Expert  0 |    197 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488766.488766 lmp.py:1935]   Expert 56 |    199 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488124.488124 lmp.py:1935]   Expert 44 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488767.488767 lmp.py:1935]   Expert  1 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488933.488933 lmp.py:1935]   Expert 51 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488099.488099 lmp.py:1935]   Expert 20 |    225 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488027.488027 lmp.py:1935]   Expert 29 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.488385.488385 lmp.py:1935]   Expert 48 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.488267.488267 lmp.py:1935]   Expert 45 |    240 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.489387.489387 lmp.py:1935]   Expert 21 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.489506.489506 lmp.py:1935]   Expert 16 |    252 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.489342.489342 lmp.py:1935]   Expert 55 |    252 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.489461.489461 lmp.py:1935]   Expert 35 |    253 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.489581.489581 lmp.py:1935]   Expert  5 |    294 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.489701.489701 lmp.py:1935]   Expert 23 |    370 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.489582.489582 lmp.py:1935]   Expert 13 |    382 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.489702.489702 lmp.py:1935]   Expert 17 |    435 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.489584.489584 lmp.py:1935]   Expert  9 |    457 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.489227.489227 lmp.py:1935]   Expert 63 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.489062.489062 lmp.py:1935]   Expert 62 |   1185 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.489420.489420 lmp.py:1937] 
DEBUG 01-15 16:10:51.489420.489420 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.489732.489732 lmp.py:1938]   CPU:   3634 tokens
DEBUG 01-15 16:10:51.489328.489328 lmp.py:1942]   cuda:1:   4290 tokens (15 experts)
DEBUG 01-15 16:10:51.489939.489939 lmp.py:1942]   cuda:2:   4364 tokens (17 experts)
DEBUG 01-15 16:10:51.489582.489582 lmp.py:1943]   Total GPU:   8654 tokens
DEBUG 01-15 16:10:51.489986.489986 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.489986.489986 lmp.py:1944] 
DEBUG 01-15 16:10:51.489828.489828 cuda_h.py:19] end experts_map_get cost 0.0018482208251953125 seconds
DEBUG 01-15 16:10:51.489347.489347 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.489957.489957 lmp.py:1953] 
DEBUG 01-15 16:10:51.489957.489957 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.489025.489025 cuda_h.py:19] end cpu_experts_submit cost 5.0067901611328125e-05 seconds
DEBUG 01-15 16:10:51.489959.489959 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.489988.489988 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.490306.490306 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.17.mlp.experts.1.gate_proj.weight': 0, 'model.layers.17.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.5.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.5.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.5.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.42.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.42.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.42.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.16.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.16.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.16.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.48.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.48.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.48.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.18.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.18.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.18.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.62.up_proj.weight': 253755392}, 2: {'model.layers.17.mlp.experts.0.gate_proj.weight': 0, 'model.layers.17.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.17.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.17.mlp.experts.32.gate_proj.weight': 17301504, 'model.layers.17.mlp.experts.32.down_proj.weight': 23068672, 'model.layers.17.mlp.experts.32.up_proj.weight': 28835840, 'model.layers.17.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.17.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.17.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.17.mlp.experts.35.gate_proj.weight': 51904512, 'model.layers.17.mlp.experts.35.down_proj.weight': 57671680, 'model.layers.17.mlp.experts.35.up_proj.weight': 63438848, 'model.layers.17.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.17.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.17.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.17.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.17.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.17.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.17.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.17.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.17.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.17.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.17.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.17.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.17.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.17.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.17.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.17.mlp.experts.51.gate_proj.weight': 155713536, 'model.layers.17.mlp.experts.51.down_proj.weight': 161480704, 'model.layers.17.mlp.experts.51.up_proj.weight': 167247872, 'model.layers.17.mlp.experts.19.gate_proj.weight': 173015040, 'model.layers.17.mlp.experts.19.down_proj.weight': 178782208, 'model.layers.17.mlp.experts.19.up_proj.weight': 184549376, 'model.layers.17.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.17.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.17.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.17.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.17.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.17.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.17.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.17.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.17.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.17.mlp.experts.57.gate_proj.weight': 242221056, 'model.layers.17.mlp.experts.57.down_proj.weight': 247988224, 'model.layers.17.mlp.experts.57.up_proj.weight': 253755392, 'model.layers.17.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.17.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.17.mlp.experts.29.up_proj.weight': 271056896, 'model.layers.17.mlp.experts.63.gate_proj.weight': 276824064, 'model.layers.17.mlp.experts.63.down_proj.weight': 282591232, 'model.layers.17.mlp.experts.63.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(20594556928, 5767168, 0, 0), (20600324096, 5767168, 5767168, 0), (20588789760, 5767168, 11534336, 0), (20663762944, 5767168, 17301504, 0), (20669530112, 5767168, 23068672, 0), (20657995776, 5767168, 28835840, 0), (21217411072, 5767168, 34603008, 0), (21223178240, 5767168, 40370176, 0), (21211643904, 5767168, 46137344, 0), (21303918592, 5767168, 51904512, 0), (21309685760, 5767168, 57671680, 0), (21298151424, 5767168, 63438848, 0), (20767571968, 5767168, 69206016, 0), (20773339136, 5767168, 74973184, 0), (20761804800, 5767168, 80740352, 0), (20802174976, 5767168, 86507520, 0), (20807942144, 5767168, 92274688, 0), (20796407808, 5767168, 98041856, 0), (20854079488, 5767168, 103809024, 0), (20859846656, 5767168, 109576192, 0), (20848312320, 5767168, 115343360, 0), (21407727616, 5767168, 121110528, 0), (21413494784, 5767168, 126877696, 0), (21401960448, 5767168, 132644864, 0), (20888682496, 5767168, 138412032, 0), (20894449664, 5767168, 144179200, 0), (20882915328, 5767168, 149946368, 0), (20923285504, 5767168, 155713536, 0), (20929052672, 5767168, 161480704, 0), (20917518336, 5767168, 167247872, 0), (20940587008, 5767168, 173015040, 0), (20946354176, 5767168, 178782208, 0), (20934819840, 5767168, 184549376, 0), (21511536640, 5767168, 190316544, 0), (21517303808, 5767168, 196083712, 0), (21505769472, 5767168, 201850880, 0), (21546139648, 5767168, 207618048, 0), (21551906816, 5767168, 213385216, 0), (21540372480, 5767168, 219152384, 0), (21027094528, 5767168, 224919552, 0), (21032861696, 5767168, 230686720, 0), (21021327360, 5767168, 236453888, 0), (21649948672, 5767168, 242221056, 0), (21655715840, 5767168, 247988224, 0), (21644181504, 5767168, 253755392, 0)], 2: [(20577255424, 5767168, 0, 0), (20583022592, 5767168, 5767168, 0), (20571488256, 5767168, 11534336, 0), (21130903552, 5767168, 17301504, 0), (21136670720, 5767168, 23068672, 0), (21125136384, 5767168, 28835840, 0), (21165506560, 5767168, 34603008, 0), (21171273728, 5767168, 40370176, 0), (21159739392, 5767168, 46137344, 0), (21182808064, 5767168, 51904512, 0), (21188575232, 5767168, 57671680, 0), (21177040896, 5767168, 63438848, 0), (20732968960, 5767168, 69206016, 0), (20738736128, 5767168, 74973184, 0), (20727201792, 5767168, 80740352, 0), (21338521600, 5767168, 86507520, 0), (21344288768, 5767168, 92274688, 0), (21332754432, 5767168, 98041856, 0), (21355823104, 5767168, 103809024, 0), (21361590272, 5767168, 109576192, 0), (21350055936, 5767168, 115343360, 0), (21373124608, 5767168, 121110528, 0), (21378891776, 5767168, 126877696, 0), (21367357440, 5767168, 132644864, 0), (20871380992, 5767168, 138412032, 0), (20877148160, 5767168, 144179200, 0), (20865613824, 5767168, 149946368, 0), (21459632128, 5767168, 155713536, 0), (21465399296, 5767168, 161480704, 0), (21453864960, 5767168, 167247872, 0), (20905984000, 5767168, 173015040, 0), (20911751168, 5767168, 178782208, 0), (20900216832, 5767168, 184549376, 0), (21528838144, 5767168, 190316544, 0), (21534605312, 5767168, 196083712, 0), (21523070976, 5767168, 201850880, 0), (20957888512, 5767168, 207618048, 0), (20963655680, 5767168, 213385216, 0), (20952121344, 5767168, 219152384, 0), (20975190016, 5767168, 224919552, 0), (20980957184, 5767168, 230686720, 0), (20969422848, 5767168, 236453888, 0), (21563441152, 5767168, 242221056, 0), (21569208320, 5767168, 247988224, 0), (21557673984, 5767168, 253755392, 0), (21078999040, 5767168, 259522560, 0), (21084766208, 5767168, 265289728, 0), (21073231872, 5767168, 271056896, 0), (21667250176, 5767168, 276824064, 0), (21673017344, 5767168, 282591232, 0), (21661483008, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e545c1a40>, 2: <capsule object NULL at 0x7a51b06da5e0>}
DEBUG 01-15 16:10:51.491815.491815 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.491267.491267 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.491831.491831 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, dcdfc806-c8af-47b8-ba8b-fb7e8b021f06
DEBUG 01-15 16:10:51.491857.491857 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.491370.491370 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:51.491324.491324 client.py:127] Model loaded
DEBUG 01-15 16:10:51.491452.491452 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.491936.491936 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.492432.492432 cuda_h.py:19] end restore2model cost 0.0003361701965332031 seconds
DEBUG 01-15 16:10:51.492393.492393 cuda_h.py:19] end sllm_worker_task cost 0.010302543640136719 seconds
INFO 01-15 16:10:51.492086.492086 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, dcdfc806-c8af-47b8-ba8b-fb7e8b021f06
DEBUG 01-15 16:10:51.492741.492741 cuda_h.py:19] end move_flatidxs cost 0.0008518695831298828 seconds
DEBUG 01-15 16:10:51.492816.492816 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.492997.492997 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0032045841217041016 seconds
DEBUG 01-15 16:10:51.492052.492052 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.495627.495627 cuda_h.py:19] end restore2model cost 0.002497434616088867 seconds
DEBUG 01-15 16:10:51.495179.495179 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00593256950378418 seconds
DEBUG 01-15 16:10:51.495736.495736 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.495481.495481 cuda_h.py:19] end gpu_sexperts cost 0.00027298927307128906 seconds
DEBUG 01-15 16:10:51.495264.495264 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.495802.495802 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:10:51.495498.495498 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.495347.495347 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.496116.496116 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007808208465576172 seconds
DEBUG 01-15 16:10:51.496720.496720 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.497231.497231 cuda_h.py:19] end gpu_group_list cost 0.00017118453979492188 seconds
DEBUG 01-15 16:10:51.497208.497208 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.498241.498241 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009121894836425781 seconds
DEBUG 01-15 16:10:51.498382.498382 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.498919.498919 cuda_h.py:19] end gpu_group_list cost 0.00019025802612304688 seconds
DEBUG 01-15 16:10:51.499410.499410 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.499816.499816 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, dcdfc806-c8af-47b8-ba8b-fb7e8b021f06
DEBUG 01-15 16:10:51.503108.503108 cuda_h.py:19] end group_tensors cost 0.010670900344848633 seconds
DEBUG 01-15 16:10:51.504142.504142 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.508242.508242 cuda_h.py:19] end group pad cost 0.003921031951904297 seconds
DEBUG 01-15 16:10:51.508217.508217 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.521269.521269 client.py:127] Model loaded
DEBUG 01-15 16:10:51.521580.521580 cuda_h.py:19] end wait_experts_multi_device cost 0.022211074829101562 seconds
DEBUG 01-15 16:10:51.521058.521058 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.527853.527853 cuda_h.py:19] end group_einsum cost 0.01871466636657715 seconds
DEBUG 01-15 16:10:51.527772.527772 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.531178.531178 cuda_h.py:19] end get_outputs_cpu1 cost 0.003743410110473633 seconds
DEBUG 01-15 16:10:51.531409.531409 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.040067195892333984 seconds
DEBUG 01-15 16:10:51.532246.532246 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010124683380126953 seconds
DEBUG 01-15 16:10:51.532336.532336 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.533895.533895 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.533112.533112 cuda_h.py:19] end index_scatter cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:10:51.533486.533486 cuda_h.py:19] end cpuoutputsdeal cost 0.0016863346099853516 seconds
DEBUG 01-15 16:10:51.534211.534211 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.534397.534397 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.534290.534290 cuda_h.py:19] end gpu_group_tensor cost 0.00013637542724609375 seconds
DEBUG 01-15 16:10:51.534100.534100 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.534926.534926 cuda_h.py:19] end gpu_group_tensor cost 0.00012350082397460938 seconds
DEBUG 01-15 16:10:51.534161.534161 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.535687.535687 cuda_h.py:19] end gpu_group_einsum cost 0.0012149810791015625 seconds
DEBUG 01-15 16:10:51.535632.535632 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.536885.536885 cuda_h.py:19] end gpu_group_einsum cost 0.0003790855407714844 seconds
DEBUG 01-15 16:10:51.536684.536684 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.536442.536442 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.536350.536350 cuda_h.py:19] end all_expert_outputs_slices cost 0.00017499923706054688 seconds
DEBUG 01-15 16:10:51.536537.536537 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.536930.536930 cuda_h.py:19] end concat_expert_out cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:51.536104.536104 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.537312.537312 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:51.537479.537479 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007483959197998047 seconds
DEBUG 01-15 16:10:51.537555.537555 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.537636.537636 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.537509.537509 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012159347534179688 seconds
DEBUG 01-15 16:10:51.537119.537119 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.537612.537612 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:51.537879.537879 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.537179.537179 cuda_h.py:19] end index_scatter cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:51.537604.537604 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004572868347167969 seconds
DEBUG 01-15 16:10:51.537084.537084 cuda_h.py:19] end gpu_experts_multi_device cost 0.04198026657104492 seconds
DEBUG 01-15 16:10:51.537470.537470 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_18 cost 0.051320791244506836 seconds
DEBUG 01-15 16:10:51.538026.538026 cuda_h.py:19] end prefill_layer cost 0.05692291259765625 seconds
DEBUG 01-15 16:10:51.538114.538114 lmp.py:1553] -------------------------------- end prefill layer 17 --------------------------------
DEBUG 01-15 16:10:51.538055.538055 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.538235.538235 lmp.py:1495] -------------------------------- start prefill layer 18 --------------------------------
DEBUG 01-15 16:10:51.538938.538938 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:51.538787.538787 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 01-15 16:10:51.538160.538160 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 3.695487976074219e-05 seconds
DEBUG 01-15 16:10:51.538723.538723 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:10:51.538519.538519 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.538528.538528 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.538882.538882 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.538031.538031 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.538252.538252 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.539883.539883 cuda_h.py:19] end allocate_cuda_memory cost 0.00021505355834960938 seconds
DEBUG 01-15 16:10:51.539336.539336 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.539960.539960 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.539790.539790 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.539546.539546 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 020e5754-07e6-4e13-a1d2-a7ffd459906e
DEBUG 01-15 16:10:51.539933.539933 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.539617.539617 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.540681.540681 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 020e5754-07e6-4e13-a1d2-a7ffd459906e
DEBUG 01-15 16:10:51.540478.540478 cuda_h.py:19] end load_into_gpu_async cost 0.0016095638275146484 seconds
DEBUG 01-15 16:10:51.541280.541280 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.541728.541728 cuda_h.py:19] end restore_tensors2 cost 8.821487426757812e-05 seconds
DEBUG 01-15 16:10:51.541345.541345 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021975040435791016 seconds
INFO 01-15 16:10:51.541949.541949 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 020e5754-07e6-4e13-a1d2-a7ffd459906e
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.543479.543479 cuda_h.py:19] end self_attn cost 0.003514528274536133 seconds
DEBUG 01-15 16:10:51.543967.543967 cuda_h.py:19] end iln_self_attn_paln cost 0.005037069320678711 seconds
DEBUG 01-15 16:10:51.543942.543942 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_19
DEBUG 01-15 16:10:51.543135.543135 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.544921.544921 cuda_h.py:19] end gate cost 0.0006814002990722656 seconds
DEBUG 01-15 16:10:51.544850.544850 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.544662.544662 lmp.py:1912] 
DEBUG 01-15 16:10:51.544662.544662 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.544087.544087 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.544452.544452 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.544718.544718 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.545791.545791 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.545196.545196 lmp.py:1917] 
DEBUG 01-15 16:10:51.545196.545196 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.545077.545077 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.545965.545965 lmp.py:1935]   Expert 32 |     34 | CPU
DEBUG 01-15 16:10:51.545323.545323 lmp.py:1935]   Expert  5 |     51 | CPU
DEBUG 01-15 16:10:51.545490.545490 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:10:51.545179.545179 lmp.py:1935]   Expert 46 |     73 | CPU
DEBUG 01-15 16:10:51.545345.545345 lmp.py:1935]   Expert  8 |     89 | CPU
DEBUG 01-15 16:10:51.545273.545273 lmp.py:1935]   Expert 40 |     89 | CPU
DEBUG 01-15 16:10:51.545962.545962 lmp.py:1935]   Expert 12 |    100 | CPU
DEBUG 01-15 16:10:51.545367.545367 lmp.py:1935]   Expert 17 |    108 | CPU
DEBUG 01-15 16:10:51.545771.545771 lmp.py:1935]   Expert 60 |    111 | CPU
DEBUG 01-15 16:10:51.545176.545176 lmp.py:1935]   Expert 27 |    115 | CPU
DEBUG 01-15 16:10:51.545819.545819 lmp.py:1935]   Expert 58 |    117 | CPU
DEBUG 01-15 16:10:51.545700.545700 lmp.py:1935]   Expert  3 |    118 | CPU
DEBUG 01-15 16:10:51.545628.545628 lmp.py:1935]   Expert 21 |    118 | CPU
DEBUG 01-15 16:10:51.545317.545317 lmp.py:1935]   Expert 28 |    120 | CPU
DEBUG 01-15 16:10:51.545530.545530 lmp.py:1935]   Expert 29 |    121 | CPU
DEBUG 01-15 16:10:51.545742.545742 lmp.py:1935]   Expert 25 |    127 | CPU
DEBUG 01-15 16:10:51.545193.545193 lmp.py:1935]   Expert 41 |    129 | CPU
DEBUG 01-15 16:10:51.545644.545644 lmp.py:1935]   Expert 35 |    133 | CPU
DEBUG 01-15 16:10:51.545333.545333 lmp.py:1935]   Expert 19 |    136 | CPU
DEBUG 01-15 16:10:51.545546.545546 lmp.py:1935]   Expert 52 |    140 | CPU
DEBUG 01-15 16:10:51.545950.545950 lmp.py:1935]   Expert  0 |    143 | CPU
DEBUG 01-15 16:10:51.545308.545308 lmp.py:1935]   Expert  6 |    145 | CPU
DEBUG 01-15 16:10:51.545998.545998 lmp.py:1935]   Expert 56 |    148 | CPU
DEBUG 01-15 16:10:51.545925.545925 lmp.py:1935]   Expert 54 |    149 | CPU
DEBUG 01-15 16:10:51.545376.545376 lmp.py:1935]   Expert 37 |    153 | CPU
DEBUG 01-15 16:10:51.545318.545318 lmp.py:1935]   Expert 53 |    155 | CPU
DEBUG 01-15 16:10:51.545292.545292 lmp.py:1935]   Expert 48 |    156 | CPU
DEBUG 01-15 16:10:51.545504.545504 lmp.py:1935]   Expert 63 |    157 | CPU
DEBUG 01-15 16:10:51.545240.545240 lmp.py:1935]   Expert 36 |    163 | CPU
DEBUG 01-15 16:10:51.545704.545704 lmp.py:1935]   Expert 59 |    171 | CPU
DEBUG 01-15 16:10:51.545440.545440 lmp.py:1935]   Expert  9 |    180 | CPU
DEBUG 01-15 16:10:51.545937.545937 lmp.py:1935]   Expert  1 |    187 | CPU
DEBUG 01-15 16:10:51.545103.545103 lmp.py:1935]   Expert 39 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545984.545984 lmp.py:1935]   Expert 20 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545820.545820 lmp.py:1935]   Expert 61 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545701.545701 lmp.py:1935]   Expert  7 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545344.545344 lmp.py:1935]   Expert 42 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545748.545748 lmp.py:1935]   Expert 11 |    206 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545915.545915 lmp.py:1935]   Expert 43 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545273.545273 lmp.py:1935]   Expert 47 |    207 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545154.545154 lmp.py:1935]   Expert 34 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545274.545274 lmp.py:1935]   Expert 55 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545963.545963 lmp.py:1935]   Expert 13 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545129.545129 lmp.py:1935]   Expert 16 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545772.545772 lmp.py:1935]   Expert 57 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545939.545939 lmp.py:1935]   Expert 18 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545105.545105 lmp.py:1935]   Expert 15 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545509.545509 lmp.py:1935]   Expert  4 |    238 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545152.545152 lmp.py:1935]   Expert 50 |    244 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545749.545749 lmp.py:1935]   Expert 22 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545392.545392 lmp.py:1935]   Expert 33 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.545796.545796 lmp.py:1935]   Expert 45 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545439.545439 lmp.py:1935]   Expert 31 |    253 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.545606.545606 lmp.py:1935]   Expert 51 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.546010.546010 lmp.py:1935]   Expert 49 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.546653.546653 lmp.py:1935]   Expert 38 |    277 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.546058.546058 lmp.py:1935]   Expert 26 |    283 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.546701.546701 lmp.py:1935]   Expert 10 |    286 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.546820.546820 lmp.py:1935]   Expert 44 |    293 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.546225.546225 lmp.py:1935]   Expert  2 |    300 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.546391.546391 lmp.py:1935]   Expert 24 |    307 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.546557.546557 lmp.py:1935]   Expert 14 |    311 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.546962.546962 lmp.py:1935]   Expert 23 |    406 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.546890.546890 lmp.py:1935]   Expert 62 |    675 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.546340.546340 lmp.py:1937] 
DEBUG 01-15 16:10:51.546340.546340 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.546791.546791 lmp.py:1938]   CPU:   3988 tokens
DEBUG 01-15 16:10:51.546957.546957 lmp.py:1942]   cuda:1:   4247 tokens (16 experts)
DEBUG 01-15 16:10:51.546600.546600 lmp.py:1942]   cuda:2:   4053 tokens (16 experts)
DEBUG 01-15 16:10:51.546767.546767 lmp.py:1943]   Total GPU:   8300 tokens
DEBUG 01-15 16:10:51.546741.546741 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.546741.546741 lmp.py:1944] 
DEBUG 01-15 16:10:51.546914.546914 cuda_h.py:19] end experts_map_get cost 0.0016679763793945312 seconds
DEBUG 01-15 16:10:51.546048.546048 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.546705.546705 lmp.py:1953] 
DEBUG 01-15 16:10:51.546705.546705 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.546773.546773 cuda_h.py:19] end cpu_experts_submit cost 4.935264587402344e-05 seconds
DEBUG 01-15 16:10:51.546443.546443 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.546703.546703 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.547820.547820 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.18.mlp.experts.7.gate_proj.weight': 0, 'model.layers.18.mlp.experts.7.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.7.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.39.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.39.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.39.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.43.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.43.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.43.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.44.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.44.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.44.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.13.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.13.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.13.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.47.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.47.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.47.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.49.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.49.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.49.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.50.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.50.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.50.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.20.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.20.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.20.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.24.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.24.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.24.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.18.mlp.experts.33.gate_proj.weight': 0, 'model.layers.18.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.18.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.18.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.18.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.18.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.18.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.18.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.18.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.18.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.18.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.18.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.18.mlp.experts.38.gate_proj.weight': 69206016, 'model.layers.18.mlp.experts.38.down_proj.weight': 74973184, 'model.layers.18.mlp.experts.38.up_proj.weight': 80740352, 'model.layers.18.mlp.experts.10.gate_proj.weight': 86507520, 'model.layers.18.mlp.experts.10.down_proj.weight': 92274688, 'model.layers.18.mlp.experts.10.up_proj.weight': 98041856, 'model.layers.18.mlp.experts.11.gate_proj.weight': 103809024, 'model.layers.18.mlp.experts.11.down_proj.weight': 109576192, 'model.layers.18.mlp.experts.11.up_proj.weight': 115343360, 'model.layers.18.mlp.experts.42.gate_proj.weight': 121110528, 'model.layers.18.mlp.experts.42.down_proj.weight': 126877696, 'model.layers.18.mlp.experts.42.up_proj.weight': 132644864, 'model.layers.18.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.18.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.18.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.18.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.18.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.18.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.18.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.18.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.18.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.18.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.18.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.18.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.18.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.18.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.18.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.18.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.18.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.18.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.18.mlp.experts.57.gate_proj.weight': 242221056, 'model.layers.18.mlp.experts.57.down_proj.weight': 247988224, 'model.layers.18.mlp.experts.57.up_proj.weight': 253755392, 'model.layers.18.mlp.experts.61.gate_proj.weight': 259522560, 'model.layers.18.mlp.experts.61.down_proj.weight': 265289728, 'model.layers.18.mlp.experts.61.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(21805662208, 5767168, 0, 0), (21811429376, 5767168, 5767168, 0), (21799895040, 5767168, 11534336, 0), (22359310336, 5767168, 17301504, 0), (22365077504, 5767168, 23068672, 0), (22353543168, 5767168, 28835840, 0), (22428516352, 5767168, 34603008, 0), (22434283520, 5767168, 40370176, 0), (22422749184, 5767168, 46137344, 0), (22445817856, 5767168, 51904512, 0), (22451585024, 5767168, 57671680, 0), (22440050688, 5767168, 63438848, 0), (22463119360, 5767168, 69206016, 0), (22468886528, 5767168, 74973184, 0), (22457352192, 5767168, 80740352, 0), (21909471232, 5767168, 86507520, 0), (21915238400, 5767168, 92274688, 0), (21903704064, 5767168, 98041856, 0), (22497722368, 5767168, 103809024, 0), (22503489536, 5767168, 109576192, 0), (22491955200, 5767168, 115343360, 0), (21961375744, 5767168, 121110528, 0), (21967142912, 5767168, 126877696, 0), (21955608576, 5767168, 132644864, 0), (22532325376, 5767168, 138412032, 0), (22538092544, 5767168, 144179200, 0), (22526558208, 5767168, 149946368, 0), (22549626880, 5767168, 155713536, 0), (22555394048, 5767168, 161480704, 0), (22543859712, 5767168, 167247872, 0), (21995978752, 5767168, 173015040, 0), (22001745920, 5767168, 178782208, 0), (21990211584, 5767168, 184549376, 0), (22030581760, 5767168, 190316544, 0), (22036348928, 5767168, 196083712, 0), (22024814592, 5767168, 201850880, 0), (22099787776, 5767168, 207618048, 0), (22105554944, 5767168, 213385216, 0), (22094020608, 5767168, 219152384, 0), (22134390784, 5767168, 224919552, 0), (22140157952, 5767168, 230686720, 0), (22128623616, 5767168, 236453888, 0), (22757244928, 5767168, 242221056, 0), (22763012096, 5767168, 247988224, 0), (22751477760, 5767168, 253755392, 0), (22220898304, 5767168, 259522560, 0), (22226665472, 5767168, 265289728, 0), (22215131136, 5767168, 271056896, 0)], 2: [(22255501312, 5767168, 0, 0), (22261268480, 5767168, 5767168, 0), (22249734144, 5767168, 11534336, 0), (21719154688, 5767168, 17301504, 0), (21724921856, 5767168, 23068672, 0), (21713387520, 5767168, 28835840, 0), (22272802816, 5767168, 34603008, 0), (22278569984, 5767168, 40370176, 0), (22267035648, 5767168, 46137344, 0), (21753757696, 5767168, 51904512, 0), (21759524864, 5767168, 57671680, 0), (21747990528, 5767168, 63438848, 0), (22342008832, 5767168, 69206016, 0), (22347776000, 5767168, 74973184, 0), (22336241664, 5767168, 80740352, 0), (21857566720, 5767168, 86507520, 0), (21863333888, 5767168, 92274688, 0), (21851799552, 5767168, 98041856, 0), (21874868224, 5767168, 103809024, 0), (21880635392, 5767168, 109576192, 0), (21869101056, 5767168, 115343360, 0), (22411214848, 5767168, 121110528, 0), (22416982016, 5767168, 126877696, 0), (22405447680, 5767168, 132644864, 0), (21926772736, 5767168, 138412032, 0), (21932539904, 5767168, 144179200, 0), (21921005568, 5767168, 149946368, 0), (21944074240, 5767168, 155713536, 0), (21949841408, 5767168, 161480704, 0), (21938307072, 5767168, 167247872, 0), (22566928384, 5767168, 173015040, 0), (22572695552, 5767168, 178782208, 0), (22561161216, 5767168, 184549376, 0), (22636134400, 5767168, 190316544, 0), (22641901568, 5767168, 196083712, 0), (22630367232, 5767168, 201850880, 0), (22065184768, 5767168, 207618048, 0), (22070951936, 5767168, 213385216, 0), (22059417600, 5767168, 219152384, 0), (22082486272, 5767168, 224919552, 0), (22088253440, 5767168, 230686720, 0), (22076719104, 5767168, 236453888, 0), (22670737408, 5767168, 242221056, 0), (22676504576, 5767168, 247988224, 0), (22664970240, 5767168, 253755392, 0), (22739943424, 5767168, 259522560, 0), (22745710592, 5767168, 265289728, 0), (22734176256, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b06da790>, 2: <capsule object NULL at 0x7a51b06da4c0>}
DEBUG 01-15 16:10:51.548772.548772 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.548701.548701 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.548881.548881 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a7b970ca-badd-4767-8aaf-7d82f5044a59
DEBUG 01-15 16:10:51.548702.548702 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.548949.548949 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:51.548109.548109 client.py:127] Model loaded
DEBUG 01-15 16:10:51.548279.548279 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.548813.548813 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.548676.548676 cuda_h.py:19] end restore2model cost 0.0003647804260253906 seconds
DEBUG 01-15 16:10:51.548307.548307 cuda_h.py:19] end sllm_worker_task cost 0.010010004043579102 seconds
INFO 01-15 16:10:51.549484.549484 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a7b970ca-badd-4767-8aaf-7d82f5044a59
DEBUG 01-15 16:10:51.549207.549207 cuda_h.py:19] end move_flatidxs cost 0.0008294582366943359 seconds
DEBUG 01-15 16:10:51.549553.549553 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.549068.549068 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0031528472900390625 seconds
DEBUG 01-15 16:10:51.549931.549931 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.552368.552368 cuda_h.py:19] end restore2model cost 0.002536296844482422 seconds
DEBUG 01-15 16:10:51.552139.552139 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.00593256950378418 seconds
DEBUG 01-15 16:10:51.552173.552173 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.552918.552918 cuda_h.py:19] end gpu_sexperts cost 0.00027251243591308594 seconds
DEBUG 01-15 16:10:51.552748.552748 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.552093.552093 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6450881958007812e-05 seconds
DEBUG 01-15 16:10:51.552505.552505 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.552446.552446 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.553256.553256 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008118152618408203 seconds
DEBUG 01-15 16:10:51.553152.553152 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.554099.554099 cuda_h.py:19] end gpu_group_list cost 0.00017833709716796875 seconds
DEBUG 01-15 16:10:51.554588.554588 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.554381.554381 cuda_h.py:19] end group_tensors cost 0.005314826965332031 seconds
DEBUG 01-15 16:10:51.555034.555034 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.555149.555149 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008776187896728516 seconds
DEBUG 01-15 16:10:51.555582.555582 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.556946.556946 cuda_h.py:19] end gpu_group_list cost 0.0007407665252685547 seconds
DEBUG 01-15 16:10:51.557227.557227 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.557501.557501 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a7b970ca-badd-4767-8aaf-7d82f5044a59
DEBUG 01-15 16:10:51.562957.562957 cuda_h.py:19] end group pad cost 0.006563901901245117 seconds
DEBUG 01-15 16:10:51.562747.562747 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.575681.575681 client.py:127] Model loaded
DEBUG 01-15 16:10:51.575999.575999 cuda_h.py:19] end wait_experts_multi_device cost 0.017615318298339844 seconds
DEBUG 01-15 16:10:51.575219.575219 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.586270.586270 cuda_h.py:19] end group_einsum cost 0.024240732192993164 seconds
DEBUG 01-15 16:10:51.586745.586745 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.590736.590736 cuda_h.py:19] end get_outputs_cpu1 cost 0.003999948501586914 seconds
DEBUG 01-15 16:10:51.591850.591850 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.04304933547973633 seconds
DEBUG 01-15 16:10:51.591553.591553 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.016225337982177734 seconds
DEBUG 01-15 16:10:51.591788.591788 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.593175.593175 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.593260.593260 cuda_h.py:19] end index_scatter cost 7.534027099609375e-05 seconds
DEBUG 01-15 16:10:51.593303.593303 cuda_h.py:19] end cpuoutputsdeal cost 0.0017082691192626953 seconds
DEBUG 01-15 16:10:51.593458.593458 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.593983.593983 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.594883.594883 cuda_h.py:19] end gpu_group_tensor cost 0.00014019012451171875 seconds
DEBUG 01-15 16:10:51.594645.594645 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.594730.594730 cuda_h.py:19] end gpu_group_tensor cost 0.0001251697540283203 seconds
DEBUG 01-15 16:10:51.594681.594681 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.594679.594679 cuda_h.py:19] end gpu_group_einsum cost 0.0004775524139404297 seconds
DEBUG 01-15 16:10:51.594418.594418 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.595879.595879 cuda_h.py:19] end gpu_group_einsum cost 0.00046443939208984375 seconds
DEBUG 01-15 16:10:51.595009.595009 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.595073.595073 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.595174.595174 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020170211791992188 seconds
DEBUG 01-15 16:10:51.596943.596943 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.596364.596364 cuda_h.py:19] end concat_expert_out cost 6.318092346191406e-05 seconds
DEBUG 01-15 16:10:51.596869.596869 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.596792.596792 cuda_h.py:19] end index_scatter cost 4.9591064453125e-05 seconds
DEBUG 01-15 16:10:51.596423.596423 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008447170257568359 seconds
DEBUG 01-15 16:10:51.596652.596652 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.596064.596064 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.596752.596752 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012564659118652344 seconds
DEBUG 01-15 16:10:51.596839.596839 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.596855.596855 cuda_h.py:19] end concat_expert_out cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:51.596453.596453 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.597422.597422 cuda_h.py:19] end index_scatter cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:51.597801.597801 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046062469482421875 seconds
DEBUG 01-15 16:10:51.597465.597465 cuda_h.py:19] end gpu_experts_multi_device cost 0.04429483413696289 seconds
DEBUG 01-15 16:10:51.597375.597375 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_19 cost 0.05344653129577637 seconds
DEBUG 01-15 16:10:51.597169.597169 cuda_h.py:19] end prefill_layer cost 0.05914616584777832 seconds
DEBUG 01-15 16:10:51.597158.597158 lmp.py:1553] -------------------------------- end prefill layer 18 --------------------------------
DEBUG 01-15 16:10:51.597576.597576 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.597577.597577 lmp.py:1495] -------------------------------- start prefill layer 19 --------------------------------
DEBUG 01-15 16:10:51.597949.597949 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:51.597036.597036 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 01-15 16:10:51.597317.597317 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:10:51.597544.597544 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.598049.598049 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 0.00015664100646972656 seconds
DEBUG 01-15 16:10:51.598694.598694 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.598385.598385 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.598255.598255 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.598166.598166 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.598985.598985 cuda_h.py:19] end allocate_cuda_memory cost 0.0002605915069580078 seconds
DEBUG 01-15 16:10:51.598036.598036 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.598852.598852 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.598489.598489 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.598437.598437 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 883e0211-fa92-4b51-8242-062caf57743b
DEBUG 01-15 16:10:51.599971.599971 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.599148.599148 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.600144.600144 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 883e0211-fa92-4b51-8242-062caf57743b
DEBUG 01-15 16:10:51.600226.600226 cuda_h.py:19] end load_into_gpu_async cost 0.0014681816101074219 seconds
DEBUG 01-15 16:10:51.600074.600074 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.600251.600251 cuda_h.py:19] end restore_tensors2 cost 9.942054748535156e-05 seconds
DEBUG 01-15 16:10:51.600398.600398 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002309560775756836 seconds
INFO 01-15 16:10:51.600453.600453 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 883e0211-fa92-4b51-8242-062caf57743b
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.603012.603012 cuda_h.py:19] end self_attn cost 0.0035843849182128906 seconds
DEBUG 01-15 16:10:51.603023.603023 cuda_h.py:19] end iln_self_attn_paln cost 0.005071878433227539 seconds
DEBUG 01-15 16:10:51.603998.603998 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_20
DEBUG 01-15 16:10:51.603715.603715 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.604437.604437 cuda_h.py:19] end gate cost 0.0007746219635009766 seconds
DEBUG 01-15 16:10:51.604466.604466 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.604265.604265 lmp.py:1912] 
DEBUG 01-15 16:10:51.604265.604265 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.604836.604836 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.604916.604916 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.604659.604659 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.604540.604540 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.604468.604468 lmp.py:1917] 
DEBUG 01-15 16:10:51.604468.604468 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.604872.604872 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.604761.604761 lmp.py:1935]   Expert 44 |     40 | CPU
DEBUG 01-15 16:10:51.604642.604642 lmp.py:1935]   Expert  1 |     45 | CPU
DEBUG 01-15 16:10:51.604000.604000 lmp.py:1935]   Expert 60 |     63 | CPU
DEBUG 01-15 16:10:51.604643.604643 lmp.py:1935]   Expert 28 |     72 | CPU
DEBUG 01-15 16:10:51.604571.604571 lmp.py:1935]   Expert 48 |     79 | CPU
DEBUG 01-15 16:10:51.604260.604260 lmp.py:1935]   Expert 27 |     88 | CPU
DEBUG 01-15 16:10:51.604949.604949 lmp.py:1935]   Expert  0 |     97 | CPU
DEBUG 01-15 16:10:51.604639.604639 lmp.py:1935]   Expert 62 |    106 | CPU
DEBUG 01-15 16:10:51.604805.604805 lmp.py:1935]   Expert 22 |    113 | CPU
DEBUG 01-15 16:10:51.604640.604640 lmp.py:1935]   Expert 30 |    113 | CPU
DEBUG 01-15 16:10:51.604521.604521 lmp.py:1935]   Expert 42 |    115 | CPU
DEBUG 01-15 16:10:51.605164.605164 lmp.py:1935]   Expert 59 |    116 | CPU
DEBUG 01-15 16:10:51.605807.605807 lmp.py:1935]   Expert 58 |    120 | CPU
DEBUG 01-15 16:10:51.605212.605212 lmp.py:1935]   Expert 16 |    124 | CPU
DEBUG 01-15 16:10:51.605616.605616 lmp.py:1935]   Expert 12 |    127 | CPU
DEBUG 01-15 16:10:51.605783.605783 lmp.py:1935]   Expert  8 |    131 | CPU
DEBUG 01-15 16:10:51.605426.605426 lmp.py:1935]   Expert 50 |    133 | CPU
DEBUG 01-15 16:10:51.605830.605830 lmp.py:1935]   Expert 56 |    142 | CPU
DEBUG 01-15 16:10:51.605950.605950 lmp.py:1935]   Expert  5 |    143 | CPU
DEBUG 01-15 16:10:51.605308.605308 lmp.py:1935]   Expert 55 |    152 | CPU
DEBUG 01-15 16:10:51.605951.605951 lmp.py:1935]   Expert 15 |    153 | CPU
DEBUG 01-15 16:10:51.605356.605356 lmp.py:1935]   Expert 26 |    153 | CPU
DEBUG 01-15 16:10:51.605760.605760 lmp.py:1935]   Expert 57 |    153 | CPU
DEBUG 01-15 16:10:51.605165.605165 lmp.py:1935]   Expert 32 |    157 | CPU
DEBUG 01-15 16:10:51.605569.605569 lmp.py:1935]   Expert 47 |    158 | CPU
DEBUG 01-15 16:10:51.605974.605974 lmp.py:1935]   Expert 34 |    163 | CPU
DEBUG 01-15 16:10:51.605617.605617 lmp.py:1935]   Expert 24 |    164 | CPU
DEBUG 01-15 16:10:51.605260.605260 lmp.py:1935]   Expert  2 |    166 | CPU
DEBUG 01-15 16:10:51.605664.605664 lmp.py:1935]   Expert 52 |    169 | CPU
DEBUG 01-15 16:10:51.605261.605261 lmp.py:1935]   Expert 18 |    171 | CPU
DEBUG 01-15 16:10:51.605142.605142 lmp.py:1935]   Expert 40 |    171 | CPU
DEBUG 01-15 16:10:51.605501.605501 lmp.py:1935]   Expert 41 |    171 | CPU
DEBUG 01-15 16:10:51.605005.605005 lmp.py:1935]   Expert  6 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605045.605045 lmp.py:1935]   Expert 54 |    172 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605642.605642 lmp.py:1935]   Expert  3 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605000.605000 lmp.py:1935]   Expert 13 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605597.605597 lmp.py:1935]   Expert 19 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605670.605670 lmp.py:1935]   Expert 20 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605505.605505 lmp.py:1935]   Expert 46 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605625.605625 lmp.py:1935]   Expert 37 |    185 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605745.605745 lmp.py:1935]   Expert 25 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605627.605627 lmp.py:1935]   Expert 51 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605746.605746 lmp.py:1935]   Expert 17 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605866.605866 lmp.py:1935]   Expert 43 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605748.605748 lmp.py:1935]   Expert 35 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605629.605629 lmp.py:1935]   Expert 11 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605749.605749 lmp.py:1935]   Expert 31 |    204 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605392.605392 lmp.py:1935]   Expert 23 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605512.605512 lmp.py:1935]   Expert 49 |    219 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605108.605108 lmp.py:1935]   Expert 39 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605228.605228 lmp.py:1935]   Expert 53 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605109.605109 lmp.py:1935]   Expert 10 |    232 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605991.605991 lmp.py:1935]   Expert 33 |    249 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605634.605634 lmp.py:1935]   Expert 38 |    268 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605754.605754 lmp.py:1935]   Expert 36 |    269 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605635.605635 lmp.py:1935]   Expert  4 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605278.605278 lmp.py:1935]   Expert 21 |    336 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605921.605921 lmp.py:1935]   Expert 14 |    348 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605564.605564 lmp.py:1935]   Expert 63 |    366 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605922.605922 lmp.py:1935]   Expert 45 |    372 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605519.605519 lmp.py:1935]   Expert  9 |    389 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605162.605162 lmp.py:1935]   Expert 61 |    391 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605282.605282 lmp.py:1935]   Expert 29 |    483 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.605163.605163 lmp.py:1935]   Expert  7 |    512 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.605091.605091 lmp.py:1937] 
DEBUG 01-15 16:10:51.605091.605091 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.605734.605734 lmp.py:1938]   CPU:   4068 tokens
DEBUG 01-15 16:10:51.606615.606615 lmp.py:1942]   cuda:1:   4109 tokens (16 experts)
DEBUG 01-15 16:10:51.606496.606496 lmp.py:1942]   cuda:2:   4111 tokens (16 experts)
DEBUG 01-15 16:10:51.606616.606616 lmp.py:1943]   Total GPU:   8220 tokens
DEBUG 01-15 16:10:51.606021.606021 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.606021.606021 lmp.py:1944] 
DEBUG 01-15 16:10:51.606909.606909 cuda_h.py:19] end experts_map_get cost 0.0017344951629638672 seconds
DEBUG 01-15 16:10:51.606805.606805 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.606177.606177 lmp.py:1953] 
DEBUG 01-15 16:10:51.606177.606177 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.606199.606199 cuda_h.py:19] end cpu_experts_submit cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:51.606418.606418 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.606578.606578 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.607544.607544 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.19.mlp.experts.35.gate_proj.weight': 0, 'model.layers.19.mlp.experts.35.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.35.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.38.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.38.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.38.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.7.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.7.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.7.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.39.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.39.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.39.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.9.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.9.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.9.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.10.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.10.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.10.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.43.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.43.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.43.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.6.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.6.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.6.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.46.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.46.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.46.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.19.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.19.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.19.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.21.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.21.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.21.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.23.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.23.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.23.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.25.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.25.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.25.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.19.mlp.experts.33.gate_proj.weight': 0, 'model.layers.19.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.19.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.19.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.19.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.19.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.19.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.19.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.19.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.19.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.19.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.19.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.19.mlp.experts.45.gate_proj.weight': 69206016, 'model.layers.19.mlp.experts.45.down_proj.weight': 74973184, 'model.layers.19.mlp.experts.45.up_proj.weight': 80740352, 'model.layers.19.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.19.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.19.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.19.mlp.experts.13.gate_proj.weight': 103809024, 'model.layers.19.mlp.experts.13.down_proj.weight': 109576192, 'model.layers.19.mlp.experts.13.up_proj.weight': 115343360, 'model.layers.19.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.19.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.19.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.19.mlp.experts.29.gate_proj.weight': 138412032, 'model.layers.19.mlp.experts.29.down_proj.weight': 144179200, 'model.layers.19.mlp.experts.29.up_proj.weight': 149946368, 'model.layers.19.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.19.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.19.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.19.mlp.experts.51.gate_proj.weight': 173015040, 'model.layers.19.mlp.experts.51.down_proj.weight': 178782208, 'model.layers.19.mlp.experts.51.up_proj.weight': 184549376, 'model.layers.19.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.19.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.19.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.19.mlp.experts.20.gate_proj.weight': 207618048, 'model.layers.19.mlp.experts.20.down_proj.weight': 213385216, 'model.layers.19.mlp.experts.20.up_proj.weight': 219152384, 'model.layers.19.mlp.experts.54.gate_proj.weight': 224919552, 'model.layers.19.mlp.experts.54.down_proj.weight': 230686720, 'model.layers.19.mlp.experts.54.up_proj.weight': 236453888, 'model.layers.19.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.19.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.19.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.19.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.19.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.19.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(23397400576, 5767168, 0, 0), (23403167744, 5767168, 5767168, 0), (23391633408, 5767168, 11534336, 0), (23414702080, 5767168, 17301504, 0), (23420469248, 5767168, 23068672, 0), (23408934912, 5767168, 28835840, 0), (22843752448, 5767168, 34603008, 0), (22849519616, 5767168, 40370176, 0), (22837985280, 5767168, 46137344, 0), (23449305088, 5767168, 51904512, 0), (23455072256, 5767168, 57671680, 0), (23443537920, 5767168, 63438848, 0), (22912958464, 5767168, 69206016, 0), (22918725632, 5767168, 74973184, 0), (22907191296, 5767168, 80740352, 0), (23466606592, 5767168, 86507520, 0), (23472373760, 5767168, 92274688, 0), (23460839424, 5767168, 98041856, 0), (22947561472, 5767168, 103809024, 0), (22953328640, 5767168, 109576192, 0), (22941794304, 5767168, 115343360, 0), (22964862976, 5767168, 121110528, 0), (22970630144, 5767168, 126877696, 0), (22959095808, 5767168, 132644864, 0), (23535812608, 5767168, 138412032, 0), (23541579776, 5767168, 144179200, 0), (23530045440, 5767168, 149946368, 0), (22895656960, 5767168, 155713536, 0), (22901424128, 5767168, 161480704, 0), (22889889792, 5767168, 167247872, 0), (23587717120, 5767168, 173015040, 0), (23593484288, 5767168, 178782208, 0), (23581949952, 5767168, 184549376, 0), (23120576512, 5767168, 190316544, 0), (23126343680, 5767168, 196083712, 0), (23114809344, 5767168, 201850880, 0), (23155179520, 5767168, 207618048, 0), (23160946688, 5767168, 213385216, 0), (23149412352, 5767168, 219152384, 0), (23189782528, 5767168, 224919552, 0), (23195549696, 5767168, 230686720, 0), (23184015360, 5767168, 236453888, 0), (23224385536, 5767168, 242221056, 0), (23230152704, 5767168, 247988224, 0), (23218618368, 5767168, 253755392, 0), (23881842688, 5767168, 259522560, 0), (23887609856, 5767168, 265289728, 0), (23876075520, 5767168, 271056896, 0)], 2: [(23362797568, 5767168, 0, 0), (23368564736, 5767168, 5767168, 0), (23357030400, 5767168, 11534336, 0), (22861053952, 5767168, 17301504, 0), (22866821120, 5767168, 23068672, 0), (22855286784, 5767168, 28835840, 0), (23432003584, 5767168, 34603008, 0), (23437770752, 5767168, 40370176, 0), (23426236416, 5767168, 46137344, 0), (22982164480, 5767168, 51904512, 0), (22987931648, 5767168, 57671680, 0), (22976397312, 5767168, 63438848, 0), (23570415616, 5767168, 69206016, 0), (23576182784, 5767168, 74973184, 0), (23564648448, 5767168, 80740352, 0), (23034068992, 5767168, 86507520, 0), (23039836160, 5767168, 92274688, 0), (23028301824, 5767168, 98041856, 0), (23016767488, 5767168, 103809024, 0), (23022534656, 5767168, 109576192, 0), (23011000320, 5767168, 115343360, 0), (23639621632, 5767168, 121110528, 0), (23645388800, 5767168, 126877696, 0), (23633854464, 5767168, 132644864, 0), (23293591552, 5767168, 138412032, 0), (23299358720, 5767168, 144179200, 0), (23287824384, 5767168, 149946368, 0), (23085973504, 5767168, 155713536, 0), (23091740672, 5767168, 161480704, 0), (23080206336, 5767168, 167247872, 0), (23674224640, 5767168, 173015040, 0), (23679991808, 5767168, 178782208, 0), (23668457472, 5767168, 184549376, 0), (23708827648, 5767168, 190316544, 0), (23714594816, 5767168, 196083712, 0), (23703060480, 5767168, 201850880, 0), (23137878016, 5767168, 207618048, 0), (23143645184, 5767168, 213385216, 0), (23132110848, 5767168, 219152384, 0), (23726129152, 5767168, 224919552, 0), (23731896320, 5767168, 230686720, 0), (23720361984, 5767168, 236453888, 0), (23847239680, 5767168, 242221056, 0), (23853006848, 5767168, 247988224, 0), (23841472512, 5767168, 253755392, 0), (23328194560, 5767168, 259522560, 0), (23333961728, 5767168, 265289728, 0), (23322427392, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e54731980>, 2: <capsule object NULL at 0x7a4e4421ff30>}
DEBUG 01-15 16:10:51.607749.607749 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.607155.607155 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.607765.607765 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9a520098-e5df-4729-ba0f-018917cc432b
DEBUG 01-15 16:10:51.607069.607069 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.607109.607109 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:51.607115.607115 client.py:127] Model loaded
DEBUG 01-15 16:10:51.607613.607613 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.607820.607820 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.608977.608977 cuda_h.py:19] end restore2model cost 0.0003414154052734375 seconds
DEBUG 01-15 16:10:51.608031.608031 cuda_h.py:19] end sllm_worker_task cost 0.0102691650390625 seconds
INFO 01-15 16:10:51.608686.608686 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9a520098-e5df-4729-ba0f-018917cc432b
DEBUG 01-15 16:10:51.608506.608506 cuda_h.py:19] end move_flatidxs cost 0.0008347034454345703 seconds
DEBUG 01-15 16:10:51.608613.608613 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.609077.609077 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002794981002807617 seconds
DEBUG 01-15 16:10:51.609563.609563 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.611589.611589 cuda_h.py:19] end restore2model cost 0.0025153160095214844 seconds
DEBUG 01-15 16:10:51.611385.611385 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005536079406738281 seconds
DEBUG 01-15 16:10:51.611942.611942 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.612919.612919 cuda_h.py:19] end gpu_sexperts cost 0.0002684593200683594 seconds
DEBUG 01-15 16:10:51.612034.612034 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.612764.612764 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7404556274414062e-05 seconds
DEBUG 01-15 16:10:51.612937.612937 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.612355.612355 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.613356.613356 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008137226104736328 seconds
DEBUG 01-15 16:10:51.613061.613061 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.613008.613008 cuda_h.py:19] end gpu_group_list cost 0.00017690658569335938 seconds
DEBUG 01-15 16:10:51.614152.614152 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.614070.614070 cuda_h.py:19] end group_tensors cost 0.005287647247314453 seconds
DEBUG 01-15 16:10:51.614450.614450 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.615070.615070 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008618831634521484 seconds
DEBUG 01-15 16:10:51.615992.615992 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.615680.615680 cuda_h.py:19] end gpu_group_list cost 0.0003304481506347656 seconds
DEBUG 01-15 16:10:51.616525.616525 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.616171.616171 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9a520098-e5df-4729-ba0f-018917cc432b
DEBUG 01-15 16:10:51.618951.618951 cuda_h.py:19] end group pad cost 0.0040416717529296875 seconds
DEBUG 01-15 16:10:51.619118.619118 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.636005.636005 client.py:127] Model loaded
DEBUG 01-15 16:10:51.636622.636622 cuda_h.py:19] end wait_experts_multi_device cost 0.019573450088500977 seconds
DEBUG 01-15 16:10:51.636936.636936 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.638737.638737 cuda_h.py:19] end group_einsum cost 0.01978302001953125 seconds
DEBUG 01-15 16:10:51.639173.639173 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.643137.643137 cuda_h.py:19] end get_outputs_cpu1 cost 0.004217863082885742 seconds
DEBUG 01-15 16:10:51.643861.643861 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03623652458190918 seconds
DEBUG 01-15 16:10:51.644208.644208 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007898330688476562 seconds
DEBUG 01-15 16:10:51.644674.644674 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.645042.645042 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.646280.646280 cuda_h.py:19] end index_scatter cost 7.963180541992188e-05 seconds
DEBUG 01-15 16:10:51.646131.646131 cuda_h.py:19] end cpuoutputsdeal cost 0.0017359256744384766 seconds
DEBUG 01-15 16:10:51.646094.646094 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.646049.646049 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.646048.646048 cuda_h.py:19] end gpu_group_tensor cost 0.00014281272888183594 seconds
DEBUG 01-15 16:10:51.646811.646811 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.646790.646790 cuda_h.py:19] end gpu_group_tensor cost 0.00012874603271484375 seconds
DEBUG 01-15 16:10:51.646363.646363 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.647534.647534 cuda_h.py:19] end gpu_group_einsum cost 0.0004992485046386719 seconds
DEBUG 01-15 16:10:51.647770.647770 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.648833.648833 cuda_h.py:19] end gpu_group_einsum cost 0.00045013427734375 seconds
DEBUG 01-15 16:10:51.648864.648864 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.648974.648974 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.648719.648719 cuda_h.py:19] end all_expert_outputs_slices cost 0.0002205371856689453 seconds
DEBUG 01-15 16:10:51.648966.648966 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.648903.648903 cuda_h.py:19] end concat_expert_out cost 5.745887756347656e-05 seconds
DEBUG 01-15 16:10:51.648315.648315 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.648146.648146 cuda_h.py:19] end index_scatter cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:51.649379.649379 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008883476257324219 seconds
DEBUG 01-15 16:10:51.649362.649362 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.649629.649629 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.649045.649045 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013446807861328125 seconds
DEBUG 01-15 16:10:51.649040.649040 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.649725.649725 cuda_h.py:19] end concat_expert_out cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:51.649230.649230 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.649299.649299 cuda_h.py:19] end index_scatter cost 5.2928924560546875e-05 seconds
DEBUG 01-15 16:10:51.649346.649346 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00047898292541503906 seconds
DEBUG 01-15 16:10:51.649395.649395 cuda_h.py:19] end gpu_experts_multi_device cost 0.037604331970214844 seconds
DEBUG 01-15 16:10:51.649874.649874 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_20 cost 0.04650449752807617 seconds
DEBUG 01-15 16:10:51.650775.650775 cuda_h.py:19] end prefill_layer cost 0.05257248878479004 seconds
DEBUG 01-15 16:10:51.650988.650988 lmp.py:1553] -------------------------------- end prefill layer 19 --------------------------------
DEBUG 01-15 16:10:51.650930.650930 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.650394.650394 lmp.py:1495] -------------------------------- start prefill layer 20 --------------------------------
DEBUG 01-15 16:10:51.650097.650097 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:51.650661.650661 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 01-15 16:10:51.650749.650749 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 3.814697265625e-05 seconds
DEBUG 01-15 16:10:51.650028.650028 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:10:51.650870.650870 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.650972.650972 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.650829.650829 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.650628.650628 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.650040.650040 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.651912.651912 cuda_h.py:19] end allocate_cuda_memory cost 0.0002868175506591797 seconds
DEBUG 01-15 16:10:51.651703.651703 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.651566.651566 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.651157.651157 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.651105.651105 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9893e634-c408-41df-8c75-b714a3f1b574
DEBUG 01-15 16:10:51.651731.651731 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.651053.651053 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.653253.653253 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9893e634-c408-41df-8c75-b714a3f1b574
DEBUG 01-15 16:10:51.653196.653196 cuda_h.py:19] end load_into_gpu_async cost 0.0020096302032470703 seconds
DEBUG 01-15 16:10:51.653237.653237 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.653492.653492 cuda_h.py:19] end restore_tensors2 cost 8.630752563476562e-05 seconds
DEBUG 01-15 16:10:51.653824.653824 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002672910690307617 seconds
INFO 01-15 16:10:51.653913.653913 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9893e634-c408-41df-8c75-b714a3f1b574
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.655717.655717 cuda_h.py:19] end self_attn cost 0.0031566619873046875 seconds
DEBUG 01-15 16:10:51.655304.655304 cuda_h.py:19] end iln_self_attn_paln cost 0.004816770553588867 seconds
DEBUG 01-15 16:10:51.655087.655087 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_21
DEBUG 01-15 16:10:51.655042.655042 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.656358.656358 cuda_h.py:19] end gate cost 0.0006852149963378906 seconds
DEBUG 01-15 16:10:51.656148.656148 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.656639.656639 lmp.py:1912] 
DEBUG 01-15 16:10:51.656639.656639 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.656071.656071 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.656204.656204 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.656046.656046 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.656742.656742 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.656246.656246 lmp.py:1917] 
DEBUG 01-15 16:10:51.656246.656246 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.656465.656465 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.656552.656552 lmp.py:1935]   Expert 54 |     22 | CPU
DEBUG 01-15 16:10:51.656487.656487 lmp.py:1935]   Expert  3 |     33 | CPU
DEBUG 01-15 16:10:51.656852.656852 lmp.py:1935]   Expert  8 |     40 | CPU
DEBUG 01-15 16:10:51.657217.657217 lmp.py:1935]   Expert 28 |     43 | CPU
DEBUG 01-15 16:10:51.657344.657344 lmp.py:1935]   Expert 43 |     54 | CPU
DEBUG 01-15 16:10:51.657993.657993 lmp.py:1935]   Expert 63 |     55 | CPU
DEBUG 01-15 16:10:51.657928.657928 lmp.py:1935]   Expert 38 |     75 | CPU
DEBUG 01-15 16:10:51.657624.657624 lmp.py:1935]   Expert 36 |     77 | CPU
DEBUG 01-15 16:10:51.657558.657558 lmp.py:1935]   Expert  6 |     85 | CPU
DEBUG 01-15 16:10:51.657254.657254 lmp.py:1935]   Expert 39 |     95 | CPU
DEBUG 01-15 16:10:51.657427.657427 lmp.py:1935]   Expert 57 |     98 | CPU
DEBUG 01-15 16:10:51.657170.657170 lmp.py:1935]   Expert 41 |    104 | CPU
DEBUG 01-15 16:10:51.657813.657813 lmp.py:1935]   Expert 12 |    108 | CPU
DEBUG 01-15 16:10:51.657456.657456 lmp.py:1935]   Expert 52 |    111 | CPU
DEBUG 01-15 16:10:51.657860.657860 lmp.py:1935]   Expert 19 |    119 | CPU
DEBUG 01-15 16:10:51.657265.657265 lmp.py:1935]   Expert 47 |    127 | CPU
DEBUG 01-15 16:10:51.657431.657431 lmp.py:1935]   Expert 13 |    134 | CPU
DEBUG 01-15 16:10:51.657074.657074 lmp.py:1935]   Expert 22 |    143 | CPU
DEBUG 01-15 16:10:51.657717.657717 lmp.py:1935]   Expert 46 |    147 | CPU
DEBUG 01-15 16:10:51.657122.657122 lmp.py:1935]   Expert 50 |    153 | CPU
DEBUG 01-15 16:10:51.657764.657764 lmp.py:1935]   Expert 24 |    162 | CPU
DEBUG 01-15 16:10:51.657169.657169 lmp.py:1935]   Expert 20 |    164 | CPU
DEBUG 01-15 16:10:51.657812.657812 lmp.py:1935]   Expert 40 |    164 | CPU
DEBUG 01-15 16:10:51.657978.657978 lmp.py:1935]   Expert 55 |    168 | CPU
DEBUG 01-15 16:10:51.657144.657144 lmp.py:1935]   Expert 23 |    169 | CPU
DEBUG 01-15 16:10:51.657549.657549 lmp.py:1935]   Expert 53 |    171 | CPU
DEBUG 01-15 16:10:51.657238.657238 lmp.py:1935]   Expert 21 |    172 | CPU
DEBUG 01-15 16:10:51.657643.657643 lmp.py:1935]   Expert 37 |    172 | CPU
DEBUG 01-15 16:10:51.657809.657809 lmp.py:1935]   Expert 49 |    173 | CPU
DEBUG 01-15 16:10:51.657213.657213 lmp.py:1935]   Expert 42 |    176 | CPU
DEBUG 01-15 16:10:51.657380.657380 lmp.py:1935]   Expert 61 |    178 | CPU
DEBUG 01-15 16:10:51.657784.657784 lmp.py:1935]   Expert  2 |    179 | CPU
DEBUG 01-15 16:10:51.657096.657096 lmp.py:1935]   Expert 18 |    191 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657169.657169 lmp.py:1935]   Expert 33 |    191 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657528.657528 lmp.py:1935]   Expert 32 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657363.657363 lmp.py:1935]   Expert  0 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657721.657721 lmp.py:1935]   Expert  5 |    202 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657318.657318 lmp.py:1935]   Expert 30 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657437.657437 lmp.py:1935]   Expert 16 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657557.657557 lmp.py:1935]   Expert 14 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657392.657392 lmp.py:1935]   Expert  7 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657751.657751 lmp.py:1935]   Expert 34 |    210 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657870.657870 lmp.py:1935]   Expert 31 |    211 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657229.657229 lmp.py:1935]   Expert 62 |    216 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657587.657587 lmp.py:1935]   Expert 60 |    217 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657707.657707 lmp.py:1935]   Expert 59 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657826.657826 lmp.py:1935]   Expert  9 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657946.657946 lmp.py:1935]   Expert 17 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657828.657828 lmp.py:1935]   Expert 29 |    227 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657948.657948 lmp.py:1935]   Expert 10 |    228 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657829.657829 lmp.py:1935]   Expert  4 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657710.657710 lmp.py:1935]   Expert 58 |    236 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657592.657592 lmp.py:1935]   Expert 15 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657473.657473 lmp.py:1935]   Expert 26 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657593.657593 lmp.py:1935]   Expert 51 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.657474.657474 lmp.py:1935]   Expert 11 |    264 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.657594.657594 lmp.py:1935]   Expert 44 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.658429.658429 lmp.py:1935]   Expert 56 |    288 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.658741.658741 lmp.py:1935]   Expert 27 |    291 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.658291.658291 lmp.py:1935]   Expert  1 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.658603.658603 lmp.py:1935]   Expert 45 |    365 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.658915.658915 lmp.py:1935]   Expert 25 |    461 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.658434.658434 lmp.py:1935]   Expert 35 |    520 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.658746.658746 lmp.py:1935]   Expert 48 |    639 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.658104.658104 lmp.py:1937] 
DEBUG 01-15 16:10:51.658104.658104 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.658476.658476 lmp.py:1938]   CPU:   3871 tokens
DEBUG 01-15 16:10:51.658119.658119 lmp.py:1942]   cuda:1:   4209 tokens (16 experts)
DEBUG 01-15 16:10:51.658285.658285 lmp.py:1942]   cuda:2:   4208 tokens (16 experts)
DEBUG 01-15 16:10:51.658498.658498 lmp.py:1943]   Total GPU:   8417 tokens
DEBUG 01-15 16:10:51.658233.658233 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.658233.658233 lmp.py:1944] 
DEBUG 01-15 16:10:51.658121.658121 cuda_h.py:19] end experts_map_get cost 0.0019497871398925781 seconds
DEBUG 01-15 16:10:51.658879.658879 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.658966.658966 lmp.py:1953] 
DEBUG 01-15 16:10:51.658966.658966 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.658603.658603 cuda_h.py:19] end cpu_experts_submit cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:51.658345.658345 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.658221.658221 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.660694.660694 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.20.mlp.experts.32.gate_proj.weight': 0, 'model.layers.20.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.59.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.59.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.59.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.11.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.11.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.11.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.48.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.48.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.48.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.56.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.56.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.56.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.58.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.58.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.58.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.27.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.27.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.27.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.31.up_proj.weight': 271056896}, 2: {'model.layers.20.mlp.experts.0.gate_proj.weight': 0, 'model.layers.20.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.20.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.20.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.20.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.20.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.20.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.20.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.20.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.20.mlp.experts.4.gate_proj.weight': 51904512, 'model.layers.20.mlp.experts.4.down_proj.weight': 57671680, 'model.layers.20.mlp.experts.4.up_proj.weight': 63438848, 'model.layers.20.mlp.experts.5.gate_proj.weight': 69206016, 'model.layers.20.mlp.experts.5.down_proj.weight': 74973184, 'model.layers.20.mlp.experts.5.up_proj.weight': 80740352, 'model.layers.20.mlp.experts.7.gate_proj.weight': 86507520, 'model.layers.20.mlp.experts.7.down_proj.weight': 92274688, 'model.layers.20.mlp.experts.7.up_proj.weight': 98041856, 'model.layers.20.mlp.experts.10.gate_proj.weight': 103809024, 'model.layers.20.mlp.experts.10.down_proj.weight': 109576192, 'model.layers.20.mlp.experts.10.up_proj.weight': 115343360, 'model.layers.20.mlp.experts.44.gate_proj.weight': 121110528, 'model.layers.20.mlp.experts.44.down_proj.weight': 126877696, 'model.layers.20.mlp.experts.44.up_proj.weight': 132644864, 'model.layers.20.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.20.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.20.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.20.mlp.experts.17.gate_proj.weight': 155713536, 'model.layers.20.mlp.experts.17.down_proj.weight': 161480704, 'model.layers.20.mlp.experts.17.up_proj.weight': 167247872, 'model.layers.20.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.20.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.20.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.20.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.20.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.20.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.20.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.20.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.20.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.20.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.20.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.20.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.20.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.20.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.20.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.20.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.20.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.20.mlp.experts.62.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(24452792320, 5767168, 0, 0), (24458559488, 5767168, 5767168, 0), (24447025152, 5767168, 11534336, 0), (24470093824, 5767168, 17301504, 0), (24475860992, 5767168, 23068672, 0), (24464326656, 5767168, 28835840, 0), (24487395328, 5767168, 34603008, 0), (24493162496, 5767168, 40370176, 0), (24481628160, 5767168, 46137344, 0), (24054857728, 5767168, 51904512, 0), (24060624896, 5767168, 57671680, 0), (24049090560, 5767168, 63438848, 0), (24919932928, 5767168, 69206016, 0), (24925700096, 5767168, 74973184, 0), (24914165760, 5767168, 80740352, 0), (24089460736, 5767168, 86507520, 0), (24095227904, 5767168, 92274688, 0), (24083693568, 5767168, 98041856, 0), (24677711872, 5767168, 103809024, 0), (24683479040, 5767168, 109576192, 0), (24671944704, 5767168, 115343360, 0), (24141365248, 5767168, 121110528, 0), (24147132416, 5767168, 126877696, 0), (24135598080, 5767168, 132644864, 0), (24158666752, 5767168, 138412032, 0), (24164433920, 5767168, 144179200, 0), (24152899584, 5767168, 149946368, 0), (24729616384, 5767168, 155713536, 0), (24735383552, 5767168, 161480704, 0), (24723849216, 5767168, 167247872, 0), (24868028416, 5767168, 173015040, 0), (24873795584, 5767168, 178782208, 0), (24862261248, 5767168, 184549376, 0), (24902631424, 5767168, 190316544, 0), (24908398592, 5767168, 196083712, 0), (24896864256, 5767168, 201850880, 0), (24366284800, 5767168, 207618048, 0), (24372051968, 5767168, 213385216, 0), (24360517632, 5767168, 219152384, 0), (24400887808, 5767168, 224919552, 0), (24406654976, 5767168, 230686720, 0), (24395120640, 5767168, 236453888, 0), (24418189312, 5767168, 242221056, 0), (24423956480, 5767168, 247988224, 0), (24412422144, 5767168, 253755392, 0), (24435490816, 5767168, 259522560, 0), (24441257984, 5767168, 265289728, 0), (24429723648, 5767168, 271056896, 0)], 2: [(23899144192, 5767168, 0, 0), (23904911360, 5767168, 5767168, 0), (23893377024, 5767168, 11534336, 0), (23916445696, 5767168, 17301504, 0), (23922212864, 5767168, 23068672, 0), (23910678528, 5767168, 28835840, 0), (24504696832, 5767168, 34603008, 0), (24510464000, 5767168, 40370176, 0), (24498929664, 5767168, 46137344, 0), (23968350208, 5767168, 51904512, 0), (23974117376, 5767168, 57671680, 0), (23962583040, 5767168, 63438848, 0), (23985651712, 5767168, 69206016, 0), (23991418880, 5767168, 74973184, 0), (23979884544, 5767168, 80740352, 0), (24020254720, 5767168, 86507520, 0), (24026021888, 5767168, 92274688, 0), (24014487552, 5767168, 98041856, 0), (24072159232, 5767168, 103809024, 0), (24077926400, 5767168, 109576192, 0), (24066392064, 5767168, 115343360, 0), (24660410368, 5767168, 121110528, 0), (24666177536, 5767168, 126877696, 0), (24654643200, 5767168, 132644864, 0), (24175968256, 5767168, 138412032, 0), (24181735424, 5767168, 144179200, 0), (24170201088, 5767168, 149946368, 0), (24193269760, 5767168, 155713536, 0), (24199036928, 5767168, 161480704, 0), (24187502592, 5767168, 167247872, 0), (24210571264, 5767168, 173015040, 0), (24216338432, 5767168, 178782208, 0), (24204804096, 5767168, 184549376, 0), (24781520896, 5767168, 190316544, 0), (24787288064, 5767168, 196083712, 0), (24775753728, 5767168, 201850880, 0), (24331681792, 5767168, 207618048, 0), (24337448960, 5767168, 213385216, 0), (24325914624, 5767168, 219152384, 0), (24348983296, 5767168, 224919552, 0), (24354750464, 5767168, 230686720, 0), (24343216128, 5767168, 236453888, 0), (24937234432, 5767168, 242221056, 0), (24943001600, 5767168, 247988224, 0), (24931467264, 5767168, 253755392, 0), (24971837440, 5767168, 259522560, 0), (24977604608, 5767168, 265289728, 0), (24966070272, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c2751a0>, 2: <capsule object NULL at 0x7a51b06da9d0>}
DEBUG 01-15 16:10:51.661077.661077 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.661290.661290 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.661947.661947 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d5f2cea5-9ef9-47d6-b806-ee3c4009da77
DEBUG 01-15 16:10:51.661027.661027 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.661631.661631 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:51.661542.661542 client.py:127] Model loaded
DEBUG 01-15 16:10:51.661418.661418 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.661397.661397 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.662397.662397 cuda_h.py:19] end restore2model cost 0.0003409385681152344 seconds
DEBUG 01-15 16:10:51.662690.662690 cuda_h.py:19] end sllm_worker_task cost 0.011362791061401367 seconds
DEBUG 01-15 16:10:51.662837.662837 cuda_h.py:19] end move_flatidxs cost 0.0008292198181152344 seconds
DEBUG 01-15 16:10:51.662991.662991 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:51.663302.663302 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d5f2cea5-9ef9-47d6-b806-ee3c4009da77
DEBUG 01-15 16:10:51.664024.664024 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005547523498535156 seconds
DEBUG 01-15 16:10:51.664702.664702 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.666814.666814 cuda_h.py:19] end restore2model cost 0.0025055408477783203 seconds
DEBUG 01-15 16:10:51.666127.666127 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008278608322143555 seconds
DEBUG 01-15 16:10:51.666399.666399 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.667330.667330 cuda_h.py:19] end gpu_sexperts cost 0.0002644062042236328 seconds
DEBUG 01-15 16:10:51.667252.667252 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.667405.667405 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5735626220703125e-05 seconds
DEBUG 01-15 16:10:51.667247.667247 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.667997.667997 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.668680.668680 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000789642333984375 seconds
DEBUG 01-15 16:10:51.668569.668569 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.668371.668371 cuda_h.py:19] end gpu_group_list cost 0.0001766681671142578 seconds
DEBUG 01-15 16:10:51.669787.669787 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.670646.670646 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008878707885742188 seconds
DEBUG 01-15 16:10:51.670172.670172 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.670497.670497 cuda_h.py:19] end gpu_group_list cost 0.00017452239990234375 seconds
DEBUG 01-15 16:10:51.670847.670847 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.670869.670869 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d5f2cea5-9ef9-47d6-b806-ee3c4009da77
DEBUG 01-15 16:10:51.672168.672168 cuda_h.py:19] end group_tensors cost 0.009224414825439453 seconds
DEBUG 01-15 16:10:51.672387.672387 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.677152.677152 cuda_h.py:19] end group pad cost 0.00419926643371582 seconds
DEBUG 01-15 16:10:51.677035.677035 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.690721.690721 client.py:127] Model loaded
DEBUG 01-15 16:10:51.690382.690382 cuda_h.py:19] end wait_experts_multi_device cost 0.01967310905456543 seconds
DEBUG 01-15 16:10:51.690351.690351 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.697424.697424 cuda_h.py:19] end group_einsum cost 0.01986980438232422 seconds
DEBUG 01-15 16:10:51.697151.697151 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.701632.701632 cuda_h.py:19] end get_outputs_cpu1 cost 0.0040013790130615234 seconds
DEBUG 01-15 16:10:51.702338.702338 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.040409088134765625 seconds
DEBUG 01-15 16:10:51.702788.702788 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01181173324584961 seconds
DEBUG 01-15 16:10:51.702308.702308 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.703893.703893 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.704249.704249 cuda_h.py:19] end index_scatter cost 7.200241088867188e-05 seconds
DEBUG 01-15 16:10:51.704459.704459 cuda_h.py:19] end cpuoutputsdeal cost 0.0016858577728271484 seconds
DEBUG 01-15 16:10:51.704634.704634 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.704635.704635 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.704396.704396 cuda_h.py:19] end gpu_group_tensor cost 0.00014352798461914062 seconds
DEBUG 01-15 16:10:51.704967.704967 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.704469.704469 cuda_h.py:19] end gpu_group_tensor cost 0.0001285076141357422 seconds
DEBUG 01-15 16:10:51.704803.704803 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.705938.705938 cuda_h.py:19] end gpu_group_einsum cost 0.0006110668182373047 seconds
DEBUG 01-15 16:10:51.705427.705427 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.706517.706517 cuda_h.py:19] end gpu_group_einsum cost 0.000461578369140625 seconds
DEBUG 01-15 16:10:51.706270.706270 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.706572.706572 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.706434.706434 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020194053649902344 seconds
DEBUG 01-15 16:10:51.706250.706250 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.706108.706108 cuda_h.py:19] end concat_expert_out cost 5.7697296142578125e-05 seconds
DEBUG 01-15 16:10:51.707666.707666 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.707928.707928 cuda_h.py:19] end index_scatter cost 5.1975250244140625e-05 seconds
DEBUG 01-15 16:10:51.707730.707730 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008466243743896484 seconds
DEBUG 01-15 16:10:51.707244.707244 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.707517.707517 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.707933.707933 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013375282287597656 seconds
DEBUG 01-15 16:10:51.707928.707928 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.707851.707851 cuda_h.py:19] end concat_expert_out cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:51.707933.707933 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.707763.707763 cuda_h.py:19] end index_scatter cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:51.707334.707334 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004858970642089844 seconds
DEBUG 01-15 16:10:51.708767.708767 cuda_h.py:19] end gpu_experts_multi_device cost 0.04080963134765625 seconds
DEBUG 01-15 16:10:51.708637.708637 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_21 cost 0.052583932876586914 seconds
DEBUG 01-15 16:10:51.708201.708201 cuda_h.py:19] end prefill_layer cost 0.05810213088989258 seconds
DEBUG 01-15 16:10:51.708613.708613 lmp.py:1553] -------------------------------- end prefill layer 20 --------------------------------
DEBUG 01-15 16:10:51.708462.708462 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.708787.708787 lmp.py:1495] -------------------------------- start prefill layer 21 --------------------------------
DEBUG 01-15 16:10:51.708682.708682 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:51.708630.708630 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 01-15 16:10:51.708964.708964 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 4.0531158447265625e-05 seconds
DEBUG 01-15 16:10:51.708251.708251 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.708995.708995 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 0.00016832351684570312 seconds
DEBUG 01-15 16:10:51.709786.709786 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.709145.709145 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.709684.709684 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.709456.709456 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.709534.709534 cuda_h.py:19] end allocate_cuda_memory cost 0.0002598762512207031 seconds
DEBUG 01-15 16:10:51.709969.709969 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.709070.709070 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.709092.709092 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.709901.709901 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 25144dcb-c7f3-4876-bd70-6f68a0aa6d5c
DEBUG 01-15 16:10:51.709004.709004 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.710380.710380 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.711754.711754 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 25144dcb-c7f3-4876-bd70-6f68a0aa6d5c
DEBUG 01-15 16:10:51.711405.711405 cuda_h.py:19] end load_into_gpu_async cost 0.0021080970764160156 seconds
DEBUG 01-15 16:10:51.711208.711208 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.712463.712463 cuda_h.py:19] end restore_tensors2 cost 8.630752563476562e-05 seconds
DEBUG 01-15 16:10:51.712749.712749 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0029544830322265625 seconds
INFO 01-15 16:10:51.712453.712453 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 25144dcb-c7f3-4876-bd70-6f68a0aa6d5c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.713859.713859 cuda_h.py:19] end self_attn cost 0.003161907196044922 seconds
DEBUG 01-15 16:10:51.713823.713823 cuda_h.py:19] end iln_self_attn_paln cost 0.004677295684814453 seconds
DEBUG 01-15 16:10:51.713322.713322 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_22
DEBUG 01-15 16:10:51.713323.713323 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.714393.714393 cuda_h.py:19] end gate cost 0.0006794929504394531 seconds
DEBUG 01-15 16:10:51.714898.714898 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.715111.715111 lmp.py:1912] 
DEBUG 01-15 16:10:51.715111.715111 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.715112.715112 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.715530.715530 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.715802.715802 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.715260.715260 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.715002.715002 lmp.py:1917] 
DEBUG 01-15 16:10:51.715002.715002 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.715937.715937 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.715024.715024 lmp.py:1935]   Expert 44 |     30 | CPU
DEBUG 01-15 16:10:51.715958.715958 lmp.py:1935]   Expert  9 |     33 | CPU
DEBUG 01-15 16:10:51.715655.715655 lmp.py:1935]   Expert 11 |     36 | CPU
DEBUG 01-15 16:10:51.715020.715020 lmp.py:1935]   Expert 56 |     59 | CPU
DEBUG 01-15 16:10:51.715954.715954 lmp.py:1935]   Expert 54 |     80 | CPU
DEBUG 01-15 16:10:51.715697.715697 lmp.py:1935]   Expert 62 |     91 | CPU
DEBUG 01-15 16:10:51.715346.715346 lmp.py:1935]   Expert  7 |     93 | CPU
DEBUG 01-15 16:10:51.715903.715903 lmp.py:1935]   Expert 47 |     93 | CPU
DEBUG 01-15 16:10:51.715454.715454 lmp.py:1935]   Expert 51 |    101 | CPU
DEBUG 01-15 16:10:51.715097.715097 lmp.py:1935]   Expert 60 |    105 | CPU
DEBUG 01-15 16:10:51.715978.715978 lmp.py:1935]   Expert 22 |    110 | CPU
DEBUG 01-15 16:10:51.715575.715575 lmp.py:1935]   Expert 52 |    110 | CPU
DEBUG 01-15 16:10:51.715410.715410 lmp.py:1935]   Expert 41 |    111 | CPU
DEBUG 01-15 16:10:51.715053.715053 lmp.py:1935]   Expert 53 |    113 | CPU
DEBUG 01-15 16:10:51.715696.715696 lmp.py:1935]   Expert  6 |    127 | CPU
DEBUG 01-15 16:10:51.715100.715100 lmp.py:1935]   Expert 32 |    127 | CPU
DEBUG 01-15 16:10:51.715266.715266 lmp.py:1935]   Expert  1 |    128 | CPU
DEBUG 01-15 16:10:51.715433.715433 lmp.py:1935]   Expert 48 |    128 | CPU
DEBUG 01-15 16:10:51.715076.715076 lmp.py:1935]   Expert  2 |    129 | CPU
DEBUG 01-15 16:10:51.715480.715480 lmp.py:1935]   Expert  8 |    129 | CPU
DEBUG 01-15 16:10:51.715838.715838 lmp.py:1935]   Expert 59 |    140 | CPU
DEBUG 01-15 16:10:51.715435.715435 lmp.py:1935]   Expert 23 |    141 | CPU
DEBUG 01-15 16:10:51.715316.715316 lmp.py:1935]   Expert 27 |    141 | CPU
DEBUG 01-15 16:10:51.715483.715483 lmp.py:1935]   Expert 35 |    142 | CPU
DEBUG 01-15 16:10:51.715887.715887 lmp.py:1935]   Expert 39 |    145 | CPU
DEBUG 01-15 16:10:51.715815.715815 lmp.py:1935]   Expert 50 |    146 | CPU
DEBUG 01-15 16:10:51.715981.715981 lmp.py:1935]   Expert 26 |    147 | CPU
DEBUG 01-15 16:10:51.715385.715385 lmp.py:1935]   Expert 14 |    156 | CPU
DEBUG 01-15 16:10:51.715790.715790 lmp.py:1935]   Expert 46 |    166 | CPU
DEBUG 01-15 16:10:51.715195.715195 lmp.py:1935]   Expert 24 |    170 | CPU
DEBUG 01-15 16:10:51.715599.715599 lmp.py:1935]   Expert  0 |    171 | CPU
DEBUG 01-15 16:10:51.715673.715673 lmp.py:1935]   Expert 34 |    172 | CPU
DEBUG 01-15 16:10:51.715700.715700 lmp.py:1935]   Expert 38 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.715773.715773 lmp.py:1935]   Expert  4 |    174 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.715085.715085 lmp.py:1935]   Expert 49 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.715159.715159 lmp.py:1935]   Expert 40 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716232.716232 lmp.py:1935]   Expert  5 |    187 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716829.716829 lmp.py:1935]   Expert 63 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716949.716949 lmp.py:1935]   Expert 19 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716022.716022 lmp.py:1935]   Expert 13 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716380.716380 lmp.py:1935]   Expert 29 |    201 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716454.716454 lmp.py:1935]   Expert 43 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716812.716812 lmp.py:1935]   Expert 61 |    209 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716409.716409 lmp.py:1935]   Expert 57 |    210 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716005.716005 lmp.py:1935]   Expert 31 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716602.716602 lmp.py:1935]   Expert 33 |    223 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716483.716483 lmp.py:1935]   Expert 16 |    248 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716318.716318 lmp.py:1935]   Expert 37 |    250 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716677.716677 lmp.py:1935]   Expert 20 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716035.716035 lmp.py:1935]   Expert  3 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716870.716870 lmp.py:1935]   Expert 15 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716467.716467 lmp.py:1935]   Expert 36 |    272 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716063.716063 lmp.py:1935]   Expert 18 |    278 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716090.716090 lmp.py:1935]   Expert 12 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716402.716402 lmp.py:1935]   Expert 17 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716999.716999 lmp.py:1935]   Expert 28 |    303 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716326.716326 lmp.py:1935]   Expert 55 |    311 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716922.716922 lmp.py:1935]   Expert 30 |    318 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716234.716234 lmp.py:1935]   Expert 25 |    325 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716831.716831 lmp.py:1935]   Expert 58 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716189.716189 lmp.py:1935]   Expert 10 |    362 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716832.716832 lmp.py:1935]   Expert 45 |    384 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716190.716190 lmp.py:1935]   Expert 21 |    393 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.716833.716833 lmp.py:1935]   Expert 42 |    644 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.716761.716761 lmp.py:1937] 
DEBUG 01-15 16:10:51.716761.716761 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.716404.716404 lmp.py:1938]   CPU:   3770 tokens
DEBUG 01-15 16:10:51.716047.716047 lmp.py:1942]   cuda:1:   4174 tokens (15 experts)
DEBUG 01-15 16:10:51.716405.716405 lmp.py:1942]   cuda:2:   4344 tokens (17 experts)
DEBUG 01-15 16:10:51.716048.716048 lmp.py:1943]   Total GPU:   8518 tokens
DEBUG 01-15 16:10:51.716976.716976 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.716976.716976 lmp.py:1944] 
DEBUG 01-15 16:10:51.716579.716579 cuda_h.py:19] end experts_map_get cost 0.001909494400024414 seconds
DEBUG 01-15 16:10:51.716906.716906 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.716516.716516 lmp.py:1953] 
DEBUG 01-15 16:10:51.716516.716516 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.716730.716730 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:51.716519.716519 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.716117.716117 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.718365.718365 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.21.mlp.experts.33.gate_proj.weight': 0, 'model.layers.21.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.42.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.42.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.42.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.10.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.10.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.10.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.15.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.15.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.15.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.16.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.16.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.16.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.17.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.17.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.17.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.18.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.18.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.18.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.29.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.29.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.29.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.20.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.20.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.20.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.49.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.49.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.49.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.55.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.55.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.55.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.25.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.25.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.25.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.61.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.61.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.61.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.21.mlp.experts.3.gate_proj.weight': 0, 'model.layers.21.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.21.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.21.mlp.experts.36.gate_proj.weight': 17301504, 'model.layers.21.mlp.experts.36.down_proj.weight': 23068672, 'model.layers.21.mlp.experts.36.up_proj.weight': 28835840, 'model.layers.21.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.21.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.21.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.21.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.21.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.21.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.21.mlp.experts.4.gate_proj.weight': 69206016, 'model.layers.21.mlp.experts.4.down_proj.weight': 74973184, 'model.layers.21.mlp.experts.4.up_proj.weight': 80740352, 'model.layers.21.mlp.experts.40.gate_proj.weight': 86507520, 'model.layers.21.mlp.experts.40.down_proj.weight': 92274688, 'model.layers.21.mlp.experts.40.up_proj.weight': 98041856, 'model.layers.21.mlp.experts.38.gate_proj.weight': 103809024, 'model.layers.21.mlp.experts.38.down_proj.weight': 109576192, 'model.layers.21.mlp.experts.38.up_proj.weight': 115343360, 'model.layers.21.mlp.experts.43.gate_proj.weight': 121110528, 'model.layers.21.mlp.experts.43.down_proj.weight': 126877696, 'model.layers.21.mlp.experts.43.up_proj.weight': 132644864, 'model.layers.21.mlp.experts.12.gate_proj.weight': 138412032, 'model.layers.21.mlp.experts.12.down_proj.weight': 144179200, 'model.layers.21.mlp.experts.12.up_proj.weight': 149946368, 'model.layers.21.mlp.experts.45.gate_proj.weight': 155713536, 'model.layers.21.mlp.experts.45.down_proj.weight': 161480704, 'model.layers.21.mlp.experts.45.up_proj.weight': 167247872, 'model.layers.21.mlp.experts.13.gate_proj.weight': 173015040, 'model.layers.21.mlp.experts.13.down_proj.weight': 178782208, 'model.layers.21.mlp.experts.13.up_proj.weight': 184549376, 'model.layers.21.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.21.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.21.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.21.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.21.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.21.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.21.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.21.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.21.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.21.mlp.experts.28.gate_proj.weight': 242221056, 'model.layers.21.mlp.experts.28.down_proj.weight': 247988224, 'model.layers.21.mlp.experts.28.up_proj.weight': 253755392, 'model.layers.21.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.21.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.21.mlp.experts.30.up_proj.weight': 271056896, 'model.layers.21.mlp.experts.31.gate_proj.weight': 276824064, 'model.layers.21.mlp.experts.31.down_proj.weight': 282591232, 'model.layers.21.mlp.experts.31.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(25577390080, 5767168, 0, 0), (25583157248, 5767168, 5767168, 0), (25571622912, 5767168, 11534336, 0), (25733103616, 5767168, 17301504, 0), (25738870784, 5767168, 23068672, 0), (25727336448, 5767168, 28835840, 0), (25179455488, 5767168, 34603008, 0), (25185222656, 5767168, 40370176, 0), (25173688320, 5767168, 46137344, 0), (25265963008, 5767168, 51904512, 0), (25271730176, 5767168, 57671680, 0), (25260195840, 5767168, 63438848, 0), (25283264512, 5767168, 69206016, 0), (25289031680, 5767168, 74973184, 0), (25277497344, 5767168, 80740352, 0), (25300566016, 5767168, 86507520, 0), (25306333184, 5767168, 92274688, 0), (25294798848, 5767168, 98041856, 0), (25317867520, 5767168, 103809024, 0), (25323634688, 5767168, 109576192, 0), (25312100352, 5767168, 115343360, 0), (25508184064, 5767168, 121110528, 0), (25513951232, 5767168, 126877696, 0), (25502416896, 5767168, 132644864, 0), (25352470528, 5767168, 138412032, 0), (25358237696, 5767168, 144179200, 0), (25346703360, 5767168, 149946368, 0), (25335169024, 5767168, 155713536, 0), (25340936192, 5767168, 161480704, 0), (25329401856, 5767168, 167247872, 0), (25854214144, 5767168, 173015040, 0), (25859981312, 5767168, 178782208, 0), (25848446976, 5767168, 184549376, 0), (25958023168, 5767168, 190316544, 0), (25963790336, 5767168, 196083712, 0), (25952256000, 5767168, 201850880, 0), (25438978048, 5767168, 207618048, 0), (25444745216, 5767168, 213385216, 0), (25433210880, 5767168, 219152384, 0), (26061832192, 5767168, 224919552, 0), (26067599360, 5767168, 230686720, 0), (26056065024, 5767168, 236453888, 0), (26096435200, 5767168, 242221056, 0), (26102202368, 5767168, 247988224, 0), (26090668032, 5767168, 253755392, 0)], 2: [(25058344960, 5767168, 0, 0), (25064112128, 5767168, 5767168, 0), (25052577792, 5767168, 11534336, 0), (25629294592, 5767168, 17301504, 0), (25635061760, 5767168, 23068672, 0), (25623527424, 5767168, 28835840, 0), (25646596096, 5767168, 34603008, 0), (25652363264, 5767168, 40370176, 0), (25640828928, 5767168, 46137344, 0), (25092947968, 5767168, 51904512, 0), (25098715136, 5767168, 57671680, 0), (25087180800, 5767168, 63438848, 0), (25075646464, 5767168, 69206016, 0), (25081413632, 5767168, 74973184, 0), (25069879296, 5767168, 80740352, 0), (25698500608, 5767168, 86507520, 0), (25704267776, 5767168, 92274688, 0), (25692733440, 5767168, 98041856, 0), (25663897600, 5767168, 103809024, 0), (25669664768, 5767168, 109576192, 0), (25658130432, 5767168, 115343360, 0), (25750405120, 5767168, 121110528, 0), (25756172288, 5767168, 126877696, 0), (25744637952, 5767168, 132644864, 0), (25214058496, 5767168, 138412032, 0), (25219825664, 5767168, 144179200, 0), (25208291328, 5767168, 149946368, 0), (25785008128, 5767168, 155713536, 0), (25790775296, 5767168, 161480704, 0), (25779240960, 5767168, 167247872, 0), (25231360000, 5767168, 173015040, 0), (25237127168, 5767168, 178782208, 0), (25225592832, 5767168, 184549376, 0), (25369772032, 5767168, 190316544, 0), (25375539200, 5767168, 196083712, 0), (25364004864, 5767168, 201850880, 0), (25992626176, 5767168, 207618048, 0), (25998393344, 5767168, 213385216, 0), (25986859008, 5767168, 219152384, 0), (26009927680, 5767168, 224919552, 0), (26015694848, 5767168, 230686720, 0), (26004160512, 5767168, 236453888, 0), (25490882560, 5767168, 242221056, 0), (25496649728, 5767168, 247988224, 0), (25485115392, 5767168, 253755392, 0), (25525485568, 5767168, 259522560, 0), (25531252736, 5767168, 265289728, 0), (25519718400, 5767168, 271056896, 0), (25542787072, 5767168, 276824064, 0), (25548554240, 5767168, 282591232, 0), (25537019904, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e54305b30>, 2: <capsule object NULL at 0x7a51b06da340>}
DEBUG 01-15 16:10:51.718455.718455 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.718543.718543 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.718769.718769 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b1a75be3-d53a-451f-8a06-6be8c6a4bca5
DEBUG 01-15 16:10:51.718689.718689 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.719788.719788 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:51.720315.720315 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:51.720953.720953 client.py:127] Model loaded
DEBUG 01-15 16:10:51.720112.720112 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.720972.720972 cuda_h.py:19] end restore2model cost 0.00034165382385253906 seconds
DEBUG 01-15 16:10:51.720226.720226 cuda_h.py:19] end sllm_worker_task cost 0.011960029602050781 seconds
DEBUG 01-15 16:10:51.720392.720392 cuda_h.py:19] end move_flatidxs cost 0.0008399486541748047 seconds
DEBUG 01-15 16:10:51.721168.721168 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:51.722521.722521 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b1a75be3-d53a-451f-8a06-6be8c6a4bca5
DEBUG 01-15 16:10:51.722865.722865 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.00571751594543457 seconds
DEBUG 01-15 16:10:51.722629.722629 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.725184.725184 cuda_h.py:19] end restore2model cost 0.0024819374084472656 seconds
DEBUG 01-15 16:10:51.725788.725788 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008428573608398438 seconds
DEBUG 01-15 16:10:51.725630.725630 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.725741.725741 cuda_h.py:19] end gpu_sexperts cost 0.0002951622009277344 seconds
DEBUG 01-15 16:10:51.725570.725570 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.725108.725108 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.71661376953125e-05 seconds
DEBUG 01-15 16:10:51.725804.725804 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.725792.725792 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.726348.726348 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.000766754150390625 seconds
DEBUG 01-15 16:10:51.726714.726714 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.726218.726218 cuda_h.py:19] end gpu_group_list cost 0.0001671314239501953 seconds
DEBUG 01-15 16:10:51.727056.727056 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.728387.728387 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009202957153320312 seconds
DEBUG 01-15 16:10:51.728144.728144 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.728675.728675 cuda_h.py:19] end gpu_group_list cost 0.00018548965454101562 seconds
DEBUG 01-15 16:10:51.729411.729411 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.729671.729671 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b1a75be3-d53a-451f-8a06-6be8c6a4bca5
DEBUG 01-15 16:10:51.730443.730443 cuda_h.py:19] end group_tensors cost 0.009366273880004883 seconds
DEBUG 01-15 16:10:51.731576.731576 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.735806.735806 cuda_h.py:19] end group pad cost 0.003842592239379883 seconds
DEBUG 01-15 16:10:51.735211.735211 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.752872.752872 client.py:127] Model loaded
DEBUG 01-15 16:10:51.752067.752067 cuda_h.py:19] end wait_experts_multi_device cost 0.023090362548828125 seconds
DEBUG 01-15 16:10:51.752010.752010 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.754274.754274 cuda_h.py:19] end group_einsum cost 0.019377470016479492 seconds
DEBUG 01-15 16:10:51.754980.754980 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.758716.758716 cuda_h.py:19] end get_outputs_cpu1 cost 0.003913402557373047 seconds
DEBUG 01-15 16:10:51.759677.759677 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039525747299194336 seconds
DEBUG 01-15 16:10:51.759043.759043 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.007060050964355469 seconds
DEBUG 01-15 16:10:51.759873.759873 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.761961.761961 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.761662.761662 cuda_h.py:19] end index_scatter cost 7.724761962890625e-05 seconds
DEBUG 01-15 16:10:51.761003.761003 cuda_h.py:19] end cpuoutputsdeal cost 0.0016720294952392578 seconds
DEBUG 01-15 16:10:51.761920.761920 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.761729.761729 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.761245.761245 cuda_h.py:19] end gpu_group_tensor cost 0.00013899803161621094 seconds
DEBUG 01-15 16:10:51.761339.761339 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.762549.762549 cuda_h.py:19] end gpu_group_tensor cost 0.00012540817260742188 seconds
DEBUG 01-15 16:10:51.762261.762261 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.762336.762336 cuda_h.py:19] end gpu_group_einsum cost 0.0006024837493896484 seconds
DEBUG 01-15 16:10:51.762725.762725 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.763492.763492 cuda_h.py:19] end gpu_group_einsum cost 0.0005040168762207031 seconds
DEBUG 01-15 16:10:51.763815.763815 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.763640.763640 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.764536.764536 cuda_h.py:19] end all_expert_outputs_slices cost 0.00021910667419433594 seconds
DEBUG 01-15 16:10:51.764067.764067 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.764488.764488 cuda_h.py:19] end concat_expert_out cost 6.103515625e-05 seconds
DEBUG 01-15 16:10:51.764377.764377 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.764546.764546 cuda_h.py:19] end index_scatter cost 5.507469177246094e-05 seconds
DEBUG 01-15 16:10:51.764846.764846 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008809566497802734 seconds
DEBUG 01-15 16:10:51.764121.764121 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.764063.764063 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.764473.764473 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012874603271484375 seconds
DEBUG 01-15 16:10:51.764705.764705 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.765914.765914 cuda_h.py:19] end concat_expert_out cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:51.765803.765803 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.765488.765488 cuda_h.py:19] end index_scatter cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:51.765059.765059 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004780292510986328 seconds
DEBUG 01-15 16:10:51.765299.765299 cuda_h.py:19] end gpu_experts_multi_device cost 0.03949332237243652 seconds
DEBUG 01-15 16:10:51.765832.765832 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_22 cost 0.051396846771240234 seconds
DEBUG 01-15 16:10:51.765190.765190 cuda_h.py:19] end prefill_layer cost 0.057103633880615234 seconds
DEBUG 01-15 16:10:51.765185.765185 lmp.py:1553] -------------------------------- end prefill layer 21 --------------------------------
DEBUG 01-15 16:10:51.765795.765795 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.765883.765883 lmp.py:1495] -------------------------------- start prefill layer 22 --------------------------------
DEBUG 01-15 16:10:51.765016.765016 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:51.765010.765010 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 01-15 16:10:51.766721.766721 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 3.7670135498046875e-05 seconds
DEBUG 01-15 16:10:51.766385.766385 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 7.43865966796875e-05 seconds
DEBUG 01-15 16:10:51.766180.766180 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.766666.766666 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.766682.766682 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.766798.766798 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.766733.766733 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.766101.766101 cuda_h.py:19] end allocate_cuda_memory cost 0.0002675056457519531 seconds
DEBUG 01-15 16:10:51.766747.766747 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.766609.766609 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.766485.766485 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.766717.766717 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3a4cede9-5067-4fbc-8a7f-9c774d9b0553
DEBUG 01-15 16:10:51.767436.767436 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.767989.767989 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.768882.768882 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3a4cede9-5067-4fbc-8a7f-9c774d9b0553
DEBUG 01-15 16:10:51.768017.768017 cuda_h.py:19] end load_into_gpu_async cost 0.002090930938720703 seconds
DEBUG 01-15 16:10:51.768965.768965 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.769267.769267 cuda_h.py:19] end restore_tensors2 cost 8.654594421386719e-05 seconds
DEBUG 01-15 16:10:51.769983.769983 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0027370452880859375 seconds
INFO 01-15 16:10:51.769356.769356 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3a4cede9-5067-4fbc-8a7f-9c774d9b0553
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.770906.770906 cuda_h.py:19] end self_attn cost 0.003368377685546875 seconds
DEBUG 01-15 16:10:51.771401.771401 cuda_h.py:19] end iln_self_attn_paln cost 0.004975080490112305 seconds
DEBUG 01-15 16:10:51.771469.771469 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_23
DEBUG 01-15 16:10:51.771185.771185 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.771216.771216 cuda_h.py:19] end gate cost 0.0006856918334960938 seconds
DEBUG 01-15 16:10:51.771483.771483 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.772794.772794 lmp.py:1912] 
DEBUG 01-15 16:10:51.772794.772794 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.772286.772286 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.772419.772419 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.772500.772500 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.772103.772103 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.772799.772799 lmp.py:1917] 
DEBUG 01-15 16:10:51.772799.772799 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.772972.772972 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.772821.772821 lmp.py:1935]   Expert 25 |     14 | CPU
DEBUG 01-15 16:10:51.772471.772471 lmp.py:1935]   Expert 48 |     32 | CPU
DEBUG 01-15 16:10:51.772405.772405 lmp.py:1935]   Expert 45 |     36 | CPU
DEBUG 01-15 16:10:51.772816.772816 lmp.py:1935]   Expert  9 |     64 | CPU
DEBUG 01-15 16:10:51.772705.772705 lmp.py:1935]   Expert  0 |     82 | CPU
DEBUG 01-15 16:10:51.772116.772116 lmp.py:1935]   Expert 43 |     87 | CPU
DEBUG 01-15 16:10:51.772958.772958 lmp.py:1935]   Expert 54 |     87 | CPU
DEBUG 01-15 16:10:51.772839.772839 lmp.py:1935]   Expert 47 |     89 | CPU
DEBUG 01-15 16:10:51.772244.772244 lmp.py:1935]   Expert 20 |     90 | CPU
DEBUG 01-15 16:10:51.772887.772887 lmp.py:1935]   Expert 57 |     91 | CPU
DEBUG 01-15 16:10:51.772768.772768 lmp.py:1935]   Expert  6 |     92 | CPU
DEBUG 01-15 16:10:51.772650.772650 lmp.py:1935]   Expert 36 |     97 | CPU
DEBUG 01-15 16:10:51.772054.772054 lmp.py:1935]   Expert 15 |    105 | CPU
DEBUG 01-15 16:10:51.772697.772697 lmp.py:1935]   Expert 62 |    105 | CPU
DEBUG 01-15 16:10:51.772102.772102 lmp.py:1935]   Expert 13 |    107 | CPU
DEBUG 01-15 16:10:51.772745.772745 lmp.py:1935]   Expert 61 |    108 | CPU
DEBUG 01-15 16:10:51.772149.772149 lmp.py:1935]   Expert 50 |    109 | CPU
DEBUG 01-15 16:10:51.772554.772554 lmp.py:1935]   Expert  1 |    111 | CPU
DEBUG 01-15 16:10:51.772720.772720 lmp.py:1935]   Expert 38 |    111 | CPU
DEBUG 01-15 16:10:51.772555.772555 lmp.py:1935]   Expert 37 |    115 | CPU
DEBUG 01-15 16:10:51.772675.772675 lmp.py:1935]   Expert 14 |    120 | CPU
DEBUG 01-15 16:10:51.772079.772079 lmp.py:1935]   Expert 46 |    122 | CPU
DEBUG 01-15 16:10:51.772484.772484 lmp.py:1935]   Expert 28 |    135 | CPU
DEBUG 01-15 16:10:51.772888.772888 lmp.py:1935]   Expert  7 |    136 | CPU
DEBUG 01-15 16:10:51.773055.773055 lmp.py:1935]   Expert 21 |    138 | CPU
DEBUG 01-15 16:10:51.773697.773697 lmp.py:1935]   Expert 52 |    142 | CPU
DEBUG 01-15 16:10:51.773340.773340 lmp.py:1935]   Expert 44 |    144 | CPU
DEBUG 01-15 16:10:51.773745.773745 lmp.py:1935]   Expert 42 |    148 | CPU
DEBUG 01-15 16:10:51.773388.773388 lmp.py:1935]   Expert 10 |    152 | CPU
DEBUG 01-15 16:10:51.773508.773508 lmp.py:1935]   Expert 24 |    152 | CPU
DEBUG 01-15 16:10:51.773389.773389 lmp.py:1935]   Expert  2 |    163 | CPU
DEBUG 01-15 16:10:51.773794.773794 lmp.py:1935]   Expert 11 |    164 | CPU
DEBUG 01-15 16:10:51.773344.773344 lmp.py:1935]   Expert 35 |    172 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773894.773894 lmp.py:1935]   Expert 26 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773491.773491 lmp.py:1935]   Expert 31 |    177 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773326.773326 lmp.py:1935]   Expert 19 |    183 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773446.773446 lmp.py:1935]   Expert  3 |    184 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773566.773566 lmp.py:1935]   Expert 32 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773447.773447 lmp.py:1935]   Expert 12 |    192 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773329.773329 lmp.py:1935]   Expert 56 |    207 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773164.773164 lmp.py:1935]   Expert 60 |    208 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773999.773999 lmp.py:1935]   Expert 40 |    213 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773834.773834 lmp.py:1935]   Expert 41 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773192.773192 lmp.py:1935]   Expert 53 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773312.773312 lmp.py:1935]   Expert  8 |    228 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773147.773147 lmp.py:1935]   Expert 23 |    232 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773267.773267 lmp.py:1935]   Expert 16 |    233 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773102.773102 lmp.py:1935]   Expert 51 |    233 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773937.773937 lmp.py:1935]   Expert 58 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773010.773010 lmp.py:1935]   Expert 59 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773322.773322 lmp.py:1935]   Expert  4 |    249 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773648.773648 lmp.py:1935]   Expert 55 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773006.773006 lmp.py:1935]   Expert 49 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773841.773841 lmp.py:1935]   Expert 29 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773722.773722 lmp.py:1935]   Expert 18 |    282 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773604.773604 lmp.py:1935]   Expert 34 |    283 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773499.773499 lmp.py:1935]   Expert 63 |    297 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773619.773619 lmp.py:1935]   Expert 27 |    357 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773785.773785 lmp.py:1935]   Expert 39 |    379 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773428.773428 lmp.py:1935]   Expert 17 |    392 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773594.773594 lmp.py:1935]   Expert 22 |    430 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773998.773998 lmp.py:1935]   Expert 33 |    454 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773688.773688 lmp.py:1935]   Expert 30 |    458 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.773854.773854 lmp.py:1935]   Expert  5 |    705 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.773066.773066 lmp.py:1937] 
DEBUG 01-15 16:10:51.773066.773066 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.773709.773709 lmp.py:1938]   CPU:   3448 tokens
DEBUG 01-15 16:10:51.773067.773067 lmp.py:1942]   cuda:1:   4457 tokens (16 experts)
DEBUG 01-15 16:10:51.773710.773710 lmp.py:1942]   cuda:2:   4383 tokens (16 experts)
DEBUG 01-15 16:10:51.773400.773400 lmp.py:1943]   Total GPU:   8840 tokens
DEBUG 01-15 16:10:51.773089.773089 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.773089.773089 lmp.py:1944] 
DEBUG 01-15 16:10:51.773977.773977 cuda_h.py:19] end experts_map_get cost 0.0018775463104248047 seconds
DEBUG 01-15 16:10:51.773258.773258 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.773821.773821 lmp.py:1953] 
DEBUG 01-15 16:10:51.773821.773821 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.774320.774320 cuda_h.py:19] end cpu_experts_submit cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:51.774321.774321 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.774435.774435 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.775254.775254 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.22.mlp.experts.3.gate_proj.weight': 0, 'model.layers.22.mlp.experts.3.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.3.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.5.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.5.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.5.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.35.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.35.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.35.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.39.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.39.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.39.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.8.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.8.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.8.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.31.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.31.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.31.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.18.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.18.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.18.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.51.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.51.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.51.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.22.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.22.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.22.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.63.up_proj.weight': 271056896}, 2: {'model.layers.22.mlp.experts.32.gate_proj.weight': 0, 'model.layers.22.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.22.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.22.mlp.experts.33.gate_proj.weight': 17301504, 'model.layers.22.mlp.experts.33.down_proj.weight': 23068672, 'model.layers.22.mlp.experts.33.up_proj.weight': 28835840, 'model.layers.22.mlp.experts.34.gate_proj.weight': 34603008, 'model.layers.22.mlp.experts.34.down_proj.weight': 40370176, 'model.layers.22.mlp.experts.34.up_proj.weight': 46137344, 'model.layers.22.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.22.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.22.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.22.mlp.experts.59.gate_proj.weight': 69206016, 'model.layers.22.mlp.experts.59.down_proj.weight': 74973184, 'model.layers.22.mlp.experts.59.up_proj.weight': 80740352, 'model.layers.22.mlp.experts.16.gate_proj.weight': 86507520, 'model.layers.22.mlp.experts.16.down_proj.weight': 92274688, 'model.layers.22.mlp.experts.16.up_proj.weight': 98041856, 'model.layers.22.mlp.experts.17.gate_proj.weight': 103809024, 'model.layers.22.mlp.experts.17.down_proj.weight': 109576192, 'model.layers.22.mlp.experts.17.up_proj.weight': 115343360, 'model.layers.22.mlp.experts.19.gate_proj.weight': 121110528, 'model.layers.22.mlp.experts.19.down_proj.weight': 126877696, 'model.layers.22.mlp.experts.19.up_proj.weight': 132644864, 'model.layers.22.mlp.experts.23.gate_proj.weight': 138412032, 'model.layers.22.mlp.experts.23.down_proj.weight': 144179200, 'model.layers.22.mlp.experts.23.up_proj.weight': 149946368, 'model.layers.22.mlp.experts.53.gate_proj.weight': 155713536, 'model.layers.22.mlp.experts.53.down_proj.weight': 161480704, 'model.layers.22.mlp.experts.53.up_proj.weight': 167247872, 'model.layers.22.mlp.experts.55.gate_proj.weight': 173015040, 'model.layers.22.mlp.experts.55.down_proj.weight': 178782208, 'model.layers.22.mlp.experts.55.up_proj.weight': 184549376, 'model.layers.22.mlp.experts.56.gate_proj.weight': 190316544, 'model.layers.22.mlp.experts.56.down_proj.weight': 196083712, 'model.layers.22.mlp.experts.56.up_proj.weight': 201850880, 'model.layers.22.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.22.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.22.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.22.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.22.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.22.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.22.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.22.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.22.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.22.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.22.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.22.mlp.experts.30.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(26165641216, 5767168, 0, 0), (26171408384, 5767168, 5767168, 0), (26159874048, 5767168, 11534336, 0), (26182942720, 5767168, 17301504, 0), (26188709888, 5767168, 23068672, 0), (26177175552, 5767168, 28835840, 0), (26200244224, 5767168, 34603008, 0), (26206011392, 5767168, 40370176, 0), (26194477056, 5767168, 46137344, 0), (26719289344, 5767168, 51904512, 0), (26725056512, 5767168, 57671680, 0), (26713522176, 5767168, 63438848, 0), (26788495360, 5767168, 69206016, 0), (26794262528, 5767168, 74973184, 0), (26782728192, 5767168, 80740352, 0), (26252148736, 5767168, 86507520, 0), (26257915904, 5767168, 92274688, 0), (26246381568, 5767168, 98041856, 0), (26823098368, 5767168, 103809024, 0), (26828865536, 5767168, 109576192, 0), (26817331200, 5767168, 115343360, 0), (26321354752, 5767168, 121110528, 0), (26327121920, 5767168, 126877696, 0), (26315587584, 5767168, 132644864, 0), (26650083328, 5767168, 138412032, 0), (26655850496, 5767168, 144179200, 0), (26644316160, 5767168, 149946368, 0), (26961510400, 5767168, 155713536, 0), (26967277568, 5767168, 161480704, 0), (26955743232, 5767168, 167247872, 0), (26425163776, 5767168, 173015040, 0), (26430930944, 5767168, 178782208, 0), (26419396608, 5767168, 184549376, 0), (26996113408, 5767168, 190316544, 0), (27001880576, 5767168, 196083712, 0), (26990346240, 5767168, 201850880, 0), (26494369792, 5767168, 207618048, 0), (26500136960, 5767168, 213385216, 0), (26488602624, 5767168, 219152384, 0), (27117223936, 5767168, 224919552, 0), (27122991104, 5767168, 230686720, 0), (27111456768, 5767168, 236453888, 0), (27151826944, 5767168, 242221056, 0), (27157594112, 5767168, 247988224, 0), (27146059776, 5767168, 253755392, 0), (27203731456, 5767168, 259522560, 0), (27209498624, 5767168, 265289728, 0), (27197964288, 5767168, 271056896, 0)], 2: [(26667384832, 5767168, 0, 0), (26673152000, 5767168, 5767168, 0), (26661617664, 5767168, 11534336, 0), (26684686336, 5767168, 17301504, 0), (26690453504, 5767168, 23068672, 0), (26678919168, 5767168, 28835840, 0), (26701987840, 5767168, 34603008, 0), (26707755008, 5767168, 40370176, 0), (26696220672, 5767168, 46137344, 0), (26805796864, 5767168, 51904512, 0), (26811564032, 5767168, 57671680, 0), (26800029696, 5767168, 63438848, 0), (27134525440, 5767168, 69206016, 0), (27140292608, 5767168, 74973184, 0), (27128758272, 5767168, 80740352, 0), (26390560768, 5767168, 86507520, 0), (26396327936, 5767168, 92274688, 0), (26384793600, 5767168, 98041856, 0), (26407862272, 5767168, 103809024, 0), (26413629440, 5767168, 109576192, 0), (26402095104, 5767168, 115343360, 0), (26442465280, 5767168, 121110528, 0), (26448232448, 5767168, 126877696, 0), (26436698112, 5767168, 132644864, 0), (26511671296, 5767168, 138412032, 0), (26517438464, 5767168, 144179200, 0), (26505904128, 5767168, 149946368, 0), (27030716416, 5767168, 155713536, 0), (27036483584, 5767168, 161480704, 0), (27024949248, 5767168, 167247872, 0), (27065319424, 5767168, 173015040, 0), (27071086592, 5767168, 178782208, 0), (27059552256, 5767168, 184549376, 0), (27082620928, 5767168, 190316544, 0), (27088388096, 5767168, 196083712, 0), (27076853760, 5767168, 201850880, 0), (26563575808, 5767168, 207618048, 0), (26569342976, 5767168, 213385216, 0), (26557808640, 5767168, 219152384, 0), (26580877312, 5767168, 224919552, 0), (26586644480, 5767168, 230686720, 0), (26575110144, 5767168, 236453888, 0), (26615480320, 5767168, 242221056, 0), (26621247488, 5767168, 247988224, 0), (26609713152, 5767168, 253755392, 0), (26632781824, 5767168, 259522560, 0), (26638548992, 5767168, 265289728, 0), (26627014656, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e344d7660>, 2: <capsule object NULL at 0x7a51b06daa00>}
DEBUG 01-15 16:10:51.775293.775293 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.775129.775129 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.775978.775978 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ca6a20e5-f11f-4082-8ab3-1e4718febd91
DEBUG 01-15 16:10:51.776713.776713 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.776657.776657 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:51.776083.776083 client.py:127] Model loaded
DEBUG 01-15 16:10:51.776873.776873 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.776462.776462 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.776673.776673 cuda_h.py:19] end restore2model cost 0.0003452301025390625 seconds
DEBUG 01-15 16:10:51.776827.776827 cuda_h.py:19] end sllm_worker_task cost 0.010387659072875977 seconds
INFO 01-15 16:10:51.777241.777241 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ca6a20e5-f11f-4082-8ab3-1e4718febd91
DEBUG 01-15 16:10:51.777651.777651 cuda_h.py:19] end move_flatidxs cost 0.0008490085601806641 seconds
DEBUG 01-15 16:10:51.777918.777918 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.777856.777856 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.003440380096435547 seconds
DEBUG 01-15 16:10:51.777481.777481 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.780726.780726 cuda_h.py:19] end restore2model cost 0.0025353431701660156 seconds
DEBUG 01-15 16:10:51.780854.780854 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0061991214752197266 seconds
DEBUG 01-15 16:10:51.780603.780603 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.780362.780362 cuda_h.py:19] end gpu_sexperts cost 0.0002818107604980469 seconds
DEBUG 01-15 16:10:51.780668.780668 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.780120.780120 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.288818359375e-05 seconds
DEBUG 01-15 16:10:51.780485.780485 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.780950.780950 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.781567.781567 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008106231689453125 seconds
DEBUG 01-15 16:10:51.781126.781126 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.781212.781212 cuda_h.py:19] end gpu_group_list cost 0.0001761913299560547 seconds
DEBUG 01-15 16:10:51.782562.782562 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.783269.783269 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008845329284667969 seconds
DEBUG 01-15 16:10:51.783887.783887 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.783596.783596 cuda_h.py:19] end gpu_group_list cost 0.00017642974853515625 seconds
DEBUG 01-15 16:10:51.784165.784165 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.784094.784094 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ca6a20e5-f11f-4082-8ab3-1e4718febd91
DEBUG 01-15 16:10:51.788233.788233 cuda_h.py:19] end group_tensors cost 0.010753870010375977 seconds
DEBUG 01-15 16:10:51.789299.789299 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.792196.792196 cuda_h.py:19] end group pad cost 0.003771066665649414 seconds
DEBUG 01-15 16:10:51.792363.792363 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.803306.803306 client.py:127] Model loaded
DEBUG 01-15 16:10:51.803472.803472 cuda_h.py:19] end wait_experts_multi_device cost 0.019357681274414062 seconds
DEBUG 01-15 16:10:51.803023.803023 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.814706.814706 cuda_h.py:19] end group_einsum cost 0.021452903747558594 seconds
DEBUG 01-15 16:10:51.814181.814181 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.818483.818483 cuda_h.py:19] end get_outputs_cpu1 cost 0.003597736358642578 seconds
DEBUG 01-15 16:10:51.818152.818152 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.042606353759765625 seconds
DEBUG 01-15 16:10:51.819001.819001 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01529550552368164 seconds
DEBUG 01-15 16:10:51.819051.819051 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.820510.820510 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.820390.820390 cuda_h.py:19] end index_scatter cost 7.224082946777344e-05 seconds
DEBUG 01-15 16:10:51.821221.821221 cuda_h.py:19] end cpuoutputsdeal cost 0.0016639232635498047 seconds
DEBUG 01-15 16:10:51.821469.821469 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.821152.821152 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.821767.821767 cuda_h.py:19] end gpu_group_tensor cost 0.00014138221740722656 seconds
DEBUG 01-15 16:10:51.821338.821338 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.821687.821687 cuda_h.py:19] end gpu_group_tensor cost 0.00012254714965820312 seconds
DEBUG 01-15 16:10:51.821492.821492 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.822157.822157 cuda_h.py:19] end gpu_group_einsum cost 0.0006039142608642578 seconds
DEBUG 01-15 16:10:51.822546.822546 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.822589.822589 cuda_h.py:19] end gpu_group_einsum cost 0.0004317760467529297 seconds
DEBUG 01-15 16:10:51.823301.823301 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.823099.823099 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.823900.823900 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016760826110839844 seconds
DEBUG 01-15 16:10:51.823848.823848 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.823527.823527 cuda_h.py:19] end concat_expert_out cost 4.744529724121094e-05 seconds
DEBUG 01-15 16:10:51.823224.823224 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.823485.823485 cuda_h.py:19] end index_scatter cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:51.823897.823897 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007457733154296875 seconds
DEBUG 01-15 16:10:51.823595.823595 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.824438.824438 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.824033.824033 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001270771026611328 seconds
DEBUG 01-15 16:10:51.824120.824120 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.824375.824375 cuda_h.py:19] end concat_expert_out cost 5.078315734863281e-05 seconds
DEBUG 01-15 16:10:51.824403.824403 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.824400.824400 cuda_h.py:19] end index_scatter cost 6.890296936035156e-05 seconds
DEBUG 01-15 16:10:51.824639.824639 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00048613548278808594 seconds
DEBUG 01-15 16:10:51.824781.824781 cuda_h.py:19] end gpu_experts_multi_device cost 0.04380464553833008 seconds
DEBUG 01-15 16:10:51.824883.824883 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_23 cost 0.0534670352935791 seconds
DEBUG 01-15 16:10:51.825591.825591 cuda_h.py:19] end prefill_layer cost 0.059120893478393555 seconds
DEBUG 01-15 16:10:51.825467.825467 lmp.py:1553] -------------------------------- end prefill layer 22 --------------------------------
DEBUG 01-15 16:10:51.825693.825693 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.825111.825111 lmp.py:1495] -------------------------------- start prefill layer 23 --------------------------------
DEBUG 01-15 16:10:51.825052.825052 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:51.825616.825616 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 01-15 16:10:51.825181.825181 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 3.7670135498046875e-05 seconds
DEBUG 01-15 16:10:51.825983.825983 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 6.985664367675781e-05 seconds
DEBUG 01-15 16:10:51.825587.825587 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.825689.825689 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.825566.825566 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.825787.825787 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.825485.825485 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.825170.825170 cuda_h.py:19] end allocate_cuda_memory cost 0.0002562999725341797 seconds
DEBUG 01-15 16:10:51.825596.825596 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.826597.826597 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.826473.826473 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.826183.826183 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c6358cb8-8166-43cb-ad63-c3a4cded2c82
DEBUG 01-15 16:10:51.826756.826756 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.826403.826403 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.827316.827316 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c6358cb8-8166-43cb-ad63-c3a4cded2c82
DEBUG 01-15 16:10:51.827212.827212 cuda_h.py:19] end load_into_gpu_async cost 0.0015342235565185547 seconds
DEBUG 01-15 16:10:51.827729.827729 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.827886.827886 cuda_h.py:19] end restore_tensors2 cost 8.368492126464844e-05 seconds
DEBUG 01-15 16:10:51.827410.827410 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002146005630493164 seconds
INFO 01-15 16:10:51.827068.827068 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c6358cb8-8166-43cb-ad63-c3a4cded2c82
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.830647.830647 cuda_h.py:19] end self_attn cost 0.0036535263061523438 seconds
DEBUG 01-15 16:10:51.830797.830797 cuda_h.py:19] end iln_self_attn_paln cost 0.005248069763183594 seconds
DEBUG 01-15 16:10:51.830818.830818 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_24
DEBUG 01-15 16:10:51.830773.830773 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.831620.831620 cuda_h.py:19] end gate cost 0.0007262229919433594 seconds
DEBUG 01-15 16:10:51.831887.831887 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.831337.831337 lmp.py:1912] 
DEBUG 01-15 16:10:51.831337.831337 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.831623.831623 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.831995.831995 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.831075.831075 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.831202.831202 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.832660.832660 lmp.py:1917] 
DEBUG 01-15 16:10:51.832660.832660 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.832978.832978 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.832535.832535 lmp.py:1935]   Expert  5 |     13 | CPU
DEBUG 01-15 16:10:51.832178.832178 lmp.py:1935]   Expert 56 |     32 | CPU
DEBUG 01-15 16:10:51.832106.832106 lmp.py:1935]   Expert 16 |     86 | CPU
DEBUG 01-15 16:10:51.832795.832795 lmp.py:1935]   Expert 27 |     86 | CPU
DEBUG 01-15 16:10:51.832485.832485 lmp.py:1935]   Expert 17 |     89 | CPU
DEBUG 01-15 16:10:51.832936.832936 lmp.py:1935]   Expert 40 |     91 | CPU
DEBUG 01-15 16:10:51.832294.832294 lmp.py:1935]   Expert 51 |    103 | CPU
DEBUG 01-15 16:10:51.832460.832460 lmp.py:1935]   Expert 63 |    105 | CPU
DEBUG 01-15 16:10:51.832865.832865 lmp.py:1935]   Expert 28 |    106 | CPU
DEBUG 01-15 16:10:51.832269.832269 lmp.py:1935]   Expert 49 |    108 | CPU
DEBUG 01-15 16:10:51.832674.832674 lmp.py:1935]   Expert 53 |    109 | CPU
DEBUG 01-15 16:10:51.832555.832555 lmp.py:1935]   Expert  7 |    111 | CPU
DEBUG 01-15 16:10:51.832960.832960 lmp.py:1935]   Expert 47 |    120 | CPU
DEBUG 01-15 16:10:51.832603.832603 lmp.py:1935]   Expert 38 |    121 | CPU
DEBUG 01-15 16:10:51.832961.832961 lmp.py:1935]   Expert 37 |    122 | CPU
DEBUG 01-15 16:10:51.832081.832081 lmp.py:1935]   Expert 62 |    125 | CPU
DEBUG 01-15 16:10:51.832962.832962 lmp.py:1935]   Expert 58 |    126 | CPU
DEBUG 01-15 16:10:51.832367.832367 lmp.py:1935]   Expert 11 |    132 | CPU
DEBUG 01-15 16:10:51.832771.832771 lmp.py:1935]   Expert 57 |    137 | CPU
DEBUG 01-15 16:10:51.832375.832375 lmp.py:1935]   Expert  1 |    143 | CPU
DEBUG 01-15 16:10:51.832256.832256 lmp.py:1935]   Expert 14 |    146 | CPU
DEBUG 01-15 16:10:51.832661.832661 lmp.py:1935]   Expert 39 |    146 | CPU
DEBUG 01-15 16:10:51.832588.832588 lmp.py:1935]   Expert 52 |    155 | CPU
DEBUG 01-15 16:10:51.832993.832993 lmp.py:1935]   Expert 23 |    157 | CPU
DEBUG 01-15 16:10:51.832874.832874 lmp.py:1935]   Expert 25 |    158 | CPU
DEBUG 01-15 16:10:51.832563.832563 lmp.py:1935]   Expert 33 |    161 | CPU
DEBUG 01-15 16:10:51.832968.832968 lmp.py:1935]   Expert 21 |    167 | CPU
DEBUG 01-15 16:10:51.832896.832896 lmp.py:1935]   Expert  6 |    170 | CPU
DEBUG 01-15 16:10:51.832300.832300 lmp.py:1935]   Expert 60 |    170 | CPU
DEBUG 01-15 16:10:51.832420.832420 lmp.py:1935]   Expert 45 |    174 | CPU
DEBUG 01-15 16:10:51.832063.832063 lmp.py:1935]   Expert 19 |    178 | CPU
DEBUG 01-15 16:10:51.832229.832229 lmp.py:1935]   Expert  4 |    183 | CPU
DEBUG 01-15 16:10:51.832780.832780 lmp.py:1935]   Expert 44 |    186 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.832330.832330 lmp.py:1935]   Expert 12 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832403.832403 lmp.py:1935]   Expert  3 |    193 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.832000.832000 lmp.py:1935]   Expert 30 |    194 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832835.832835 lmp.py:1935]   Expert 31 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.832955.832955 lmp.py:1935]   Expert 55 |    198 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832028.832028 lmp.py:1935]   Expert 36 |    204 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832625.832625 lmp.py:1935]   Expert  9 |    211 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.832222.832222 lmp.py:1935]   Expert  0 |    221 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832103.832103 lmp.py:1935]   Expert 34 |    223 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.832700.832700 lmp.py:1935]   Expert 22 |    225 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.832581.832581 lmp.py:1935]   Expert 41 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832701.832701 lmp.py:1935]   Expert 26 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.832298.832298 lmp.py:1935]   Expert 54 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832894.832894 lmp.py:1935]   Expert 43 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832445.832445 lmp.py:1935]   Expert 59 |    247 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.832518.832518 lmp.py:1935]   Expert 13 |    254 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.832592.832592 lmp.py:1935]   Expert 18 |    254 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832427.832427 lmp.py:1935]   Expert 20 |    256 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.832785.832785 lmp.py:1935]   Expert 50 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.833143.833143 lmp.py:1935]   Expert 15 |    259 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.833501.833501 lmp.py:1935]   Expert 24 |    265 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.833112.833112 lmp.py:1935]   Expert 42 |    266 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.833993.833993 lmp.py:1935]   Expert 29 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.833643.833643 lmp.py:1935]   Expert 61 |    271 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.833524.833524 lmp.py:1935]   Expert 35 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.833452.833452 lmp.py:1935]   Expert 32 |    305 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.833380.833380 lmp.py:1935]   Expert 10 |    336 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.833069.833069 lmp.py:1935]   Expert  8 |    339 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.833235.833235 lmp.py:1935]   Expert  2 |    343 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.833924.833924 lmp.py:1935]   Expert 46 |    427 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.833614.833614 lmp.py:1935]   Expert 48 |    446 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.833349.833349 lmp.py:1937] 
DEBUG 01-15 16:10:51.833349.833349 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.833038.833038 lmp.py:1938]   CPU:   4030 tokens
DEBUG 01-15 16:10:51.833966.833966 lmp.py:1942]   cuda:1:   4128 tokens (16 experts)
DEBUG 01-15 16:10:51.833894.833894 lmp.py:1942]   cuda:2:   4130 tokens (16 experts)
DEBUG 01-15 16:10:51.833868.833868 lmp.py:1943]   Total GPU:   8258 tokens
DEBUG 01-15 16:10:51.833604.833604 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.833604.833604 lmp.py:1944] 
DEBUG 01-15 16:10:51.833777.833777 cuda_h.py:19] end experts_map_get cost 0.0018177032470703125 seconds
DEBUG 01-15 16:10:51.833911.833911 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.833045.833045 lmp.py:1953] 
DEBUG 01-15 16:10:51.833045.833045 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.833020.833020 cuda_h.py:19] end cpu_experts_submit cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:51.833954.833954 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.833784.833784 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.834740.834740 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.23.mlp.experts.32.gate_proj.weight': 0, 'model.layers.23.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.0.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.0.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.0.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.36.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.36.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.36.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.8.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.8.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.8.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.43.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.43.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.43.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.12.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.12.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.12.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.48.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.48.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.48.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.20.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.20.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.20.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.54.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.54.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.54.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.55.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.55.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.55.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.24.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.24.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.24.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.29.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.29.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.29.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.30.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.30.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.30.up_proj.weight': 271056896}, 2: {'model.layers.23.mlp.experts.2.gate_proj.weight': 0, 'model.layers.23.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.23.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.23.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.23.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.23.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.23.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.23.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.23.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.23.mlp.experts.9.gate_proj.weight': 51904512, 'model.layers.23.mlp.experts.9.down_proj.weight': 57671680, 'model.layers.23.mlp.experts.9.up_proj.weight': 63438848, 'model.layers.23.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.23.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.23.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.23.mlp.experts.42.gate_proj.weight': 86507520, 'model.layers.23.mlp.experts.42.down_proj.weight': 92274688, 'model.layers.23.mlp.experts.42.up_proj.weight': 98041856, 'model.layers.23.mlp.experts.44.gate_proj.weight': 103809024, 'model.layers.23.mlp.experts.44.down_proj.weight': 109576192, 'model.layers.23.mlp.experts.44.up_proj.weight': 115343360, 'model.layers.23.mlp.experts.13.gate_proj.weight': 121110528, 'model.layers.23.mlp.experts.13.down_proj.weight': 126877696, 'model.layers.23.mlp.experts.13.up_proj.weight': 132644864, 'model.layers.23.mlp.experts.46.gate_proj.weight': 138412032, 'model.layers.23.mlp.experts.46.down_proj.weight': 144179200, 'model.layers.23.mlp.experts.46.up_proj.weight': 149946368, 'model.layers.23.mlp.experts.15.gate_proj.weight': 155713536, 'model.layers.23.mlp.experts.15.down_proj.weight': 161480704, 'model.layers.23.mlp.experts.15.up_proj.weight': 167247872, 'model.layers.23.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.23.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.23.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.23.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.23.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.23.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.23.mlp.experts.26.gate_proj.weight': 207618048, 'model.layers.23.mlp.experts.26.down_proj.weight': 213385216, 'model.layers.23.mlp.experts.26.up_proj.weight': 219152384, 'model.layers.23.mlp.experts.59.gate_proj.weight': 224919552, 'model.layers.23.mlp.experts.59.down_proj.weight': 230686720, 'model.layers.23.mlp.experts.59.up_proj.weight': 236453888, 'model.layers.23.mlp.experts.61.gate_proj.weight': 242221056, 'model.layers.23.mlp.experts.61.down_proj.weight': 247988224, 'model.layers.23.mlp.experts.61.up_proj.weight': 253755392, 'model.layers.23.mlp.experts.31.gate_proj.weight': 259522560, 'model.layers.23.mlp.experts.31.down_proj.weight': 265289728, 'model.layers.23.mlp.experts.31.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(27774681088, 5767168, 0, 0), (27780448256, 5767168, 5767168, 0), (27768913920, 5767168, 11534336, 0), (27221032960, 5767168, 17301504, 0), (27226800128, 5767168, 23068672, 0), (27215265792, 5767168, 28835840, 0), (27826585600, 5767168, 34603008, 0), (27832352768, 5767168, 40370176, 0), (27820818432, 5767168, 46137344, 0), (27843887104, 5767168, 51904512, 0), (27849654272, 5767168, 57671680, 0), (27838119936, 5767168, 63438848, 0), (27359444992, 5767168, 69206016, 0), (27365212160, 5767168, 74973184, 0), (27353677824, 5767168, 80740352, 0), (27930394624, 5767168, 86507520, 0), (27936161792, 5767168, 92274688, 0), (27924627456, 5767168, 98041856, 0), (27964997632, 5767168, 103809024, 0), (27970764800, 5767168, 109576192, 0), (27959230464, 5767168, 115343360, 0), (27428651008, 5767168, 121110528, 0), (27434418176, 5767168, 126877696, 0), (27422883840, 5767168, 132644864, 0), (28051505152, 5767168, 138412032, 0), (28057272320, 5767168, 144179200, 0), (28045737984, 5767168, 149946368, 0), (27532460032, 5767168, 155713536, 0), (27538227200, 5767168, 161480704, 0), (27526692864, 5767168, 167247872, 0), (27567063040, 5767168, 173015040, 0), (27572830208, 5767168, 178782208, 0), (27561295872, 5767168, 184549376, 0), (28155314176, 5767168, 190316544, 0), (28161081344, 5767168, 196083712, 0), (28149547008, 5767168, 201850880, 0), (28172615680, 5767168, 207618048, 0), (28178382848, 5767168, 213385216, 0), (28166848512, 5767168, 219152384, 0), (27636269056, 5767168, 224919552, 0), (27642036224, 5767168, 230686720, 0), (27630501888, 5767168, 236453888, 0), (27722776576, 5767168, 242221056, 0), (27728543744, 5767168, 247988224, 0), (27717009408, 5767168, 253755392, 0), (27740078080, 5767168, 259522560, 0), (27745845248, 5767168, 265289728, 0), (27734310912, 5767168, 271056896, 0)], 2: [(27255635968, 5767168, 0, 0), (27261403136, 5767168, 5767168, 0), (27249868800, 5767168, 11534336, 0), (27809284096, 5767168, 17301504, 0), (27815051264, 5767168, 23068672, 0), (27803516928, 5767168, 28835840, 0), (27272937472, 5767168, 34603008, 0), (27278704640, 5767168, 40370176, 0), (27267170304, 5767168, 46137344, 0), (27376746496, 5767168, 51904512, 0), (27382513664, 5767168, 57671680, 0), (27370979328, 5767168, 63438848, 0), (27394048000, 5767168, 69206016, 0), (27399815168, 5767168, 74973184, 0), (27388280832, 5767168, 80740352, 0), (27947696128, 5767168, 86507520, 0), (27953463296, 5767168, 92274688, 0), (27941928960, 5767168, 98041856, 0), (27982299136, 5767168, 103809024, 0), (27988066304, 5767168, 109576192, 0), (27976531968, 5767168, 115343360, 0), (27445952512, 5767168, 121110528, 0), (27451719680, 5767168, 126877696, 0), (27440185344, 5767168, 132644864, 0), (28016902144, 5767168, 138412032, 0), (28022669312, 5767168, 144179200, 0), (28011134976, 5767168, 149946368, 0), (27480555520, 5767168, 155713536, 0), (27486322688, 5767168, 161480704, 0), (27474788352, 5767168, 167247872, 0), (28086108160, 5767168, 173015040, 0), (28091875328, 5767168, 178782208, 0), (28080340992, 5767168, 184549376, 0), (27601666048, 5767168, 190316544, 0), (27607433216, 5767168, 196083712, 0), (27595898880, 5767168, 201850880, 0), (27670872064, 5767168, 207618048, 0), (27676639232, 5767168, 213385216, 0), (27665104896, 5767168, 219152384, 0), (28241821696, 5767168, 224919552, 0), (28247588864, 5767168, 230686720, 0), (28236054528, 5767168, 236453888, 0), (28276424704, 5767168, 242221056, 0), (28282191872, 5767168, 247988224, 0), (28270657536, 5767168, 253755392, 0), (27757379584, 5767168, 259522560, 0), (27763146752, 5767168, 265289728, 0), (27751612416, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b06da490>, 2: <capsule object NULL at 0x7a51b06da760>}
DEBUG 01-15 16:10:51.834176.834176 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.834820.834820 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.834192.834192 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 40dd856b-8e09-43ca-a235-2c959749ac13
DEBUG 01-15 16:10:51.834033.834033 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.834001.834001 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:51.835157.835157 client.py:127] Model loaded
DEBUG 01-15 16:10:51.835033.835033 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.835620.835620 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.835913.835913 cuda_h.py:19] end restore2model cost 0.0003361701965332031 seconds
DEBUG 01-15 16:10:51.835683.835683 cuda_h.py:19] end sllm_worker_task cost 0.009939193725585938 seconds
INFO 01-15 16:10:51.835028.835028 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 40dd856b-8e09-43ca-a235-2c959749ac13
DEBUG 01-15 16:10:51.836206.836206 cuda_h.py:19] end move_flatidxs cost 0.0008318424224853516 seconds
DEBUG 01-15 16:10:51.836075.836075 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.836903.836903 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.002725362777709961 seconds
DEBUG 01-15 16:10:51.836574.836574 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.838506.838506 cuda_h.py:19] end restore2model cost 0.002462148666381836 seconds
DEBUG 01-15 16:10:51.838441.838441 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0054280757904052734 seconds
DEBUG 01-15 16:10:51.838475.838475 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.839598.839598 cuda_h.py:19] end gpu_sexperts cost 0.0002696514129638672 seconds
DEBUG 01-15 16:10:51.839428.839428 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.839350.839350 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7881393432617188e-05 seconds
DEBUG 01-15 16:10:51.839999.839999 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.839941.839941 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.840505.840505 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008080005645751953 seconds
DEBUG 01-15 16:10:51.840633.840633 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.840885.840885 cuda_h.py:19] end gpu_group_list cost 0.00019168853759765625 seconds
DEBUG 01-15 16:10:51.841751.841751 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.842378.842378 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008587837219238281 seconds
DEBUG 01-15 16:10:51.842314.842314 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.842705.842705 cuda_h.py:19] end gpu_group_list cost 0.0001838207244873047 seconds
DEBUG 01-15 16:10:51.843301.843301 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.843561.843561 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 40dd856b-8e09-43ca-a235-2c959749ac13
DEBUG 01-15 16:10:51.845331.845331 cuda_h.py:19] end group_tensors cost 0.009772777557373047 seconds
DEBUG 01-15 16:10:51.846596.846596 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.850249.850249 cuda_h.py:19] end group pad cost 0.004221439361572266 seconds
DEBUG 01-15 16:10:51.851608.851608 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.862853.862853 client.py:127] Model loaded
DEBUG 01-15 16:10:51.862257.862257 cuda_h.py:19] end wait_experts_multi_device cost 0.019380807876586914 seconds
DEBUG 01-15 16:10:51.862186.862186 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.871650.871650 cuda_h.py:19] end group_einsum cost 0.01996612548828125 seconds
DEBUG 01-15 16:10:51.871397.871397 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.875830.875830 cuda_h.py:19] end get_outputs_cpu1 cost 0.004145145416259766 seconds
DEBUG 01-15 16:10:51.876707.876707 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.041188716888427734 seconds
DEBUG 01-15 16:10:51.876292.876292 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.01398468017578125 seconds
DEBUG 01-15 16:10:51.876627.876627 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.877502.877502 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.878083.878083 cuda_h.py:19] end index_scatter cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:10:51.878934.878934 cuda_h.py:19] end cpuoutputsdeal cost 0.0016257762908935547 seconds
DEBUG 01-15 16:10:51.878566.878566 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.878614.878614 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.878659.878659 cuda_h.py:19] end gpu_group_tensor cost 0.00014281272888183594 seconds
DEBUG 01-15 16:10:51.878184.878184 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.878778.878778 cuda_h.py:19] end gpu_group_tensor cost 0.00012731552124023438 seconds
DEBUG 01-15 16:10:51.878490.878490 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.879452.879452 cuda_h.py:19] end gpu_group_einsum cost 0.0005872249603271484 seconds
DEBUG 01-15 16:10:51.879695.879695 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.880873.880873 cuda_h.py:19] end gpu_group_einsum cost 0.000492095947265625 seconds
DEBUG 01-15 16:10:51.880613.880613 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.880823.880823 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.880692.880692 cuda_h.py:19] end all_expert_outputs_slices cost 0.00020623207092285156 seconds
DEBUG 01-15 16:10:51.880508.880508 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.880836.880836 cuda_h.py:19] end concat_expert_out cost 6.29425048828125e-05 seconds
DEBUG 01-15 16:10:51.881156.881156 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.881609.881609 cuda_h.py:19] end index_scatter cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:51.881789.881789 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008473396301269531 seconds
DEBUG 01-15 16:10:51.881918.881918 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.881284.881284 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.881224.881224 cuda_h.py:19] end all_expert_outputs_slices cost 0.000133514404296875 seconds
DEBUG 01-15 16:10:51.881172.881172 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.881618.881618 cuda_h.py:19] end concat_expert_out cost 5.221366882324219e-05 seconds
DEBUG 01-15 16:10:51.881700.881700 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.881484.881484 cuda_h.py:19] end index_scatter cost 5.14984130859375e-05 seconds
DEBUG 01-15 16:10:51.881009.881009 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004858970642089844 seconds
DEBUG 01-15 16:10:51.882395.882395 cuda_h.py:19] end gpu_experts_multi_device cost 0.042649030685424805 seconds
DEBUG 01-15 16:10:51.882689.882689 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_24 cost 0.05148625373840332 seconds
DEBUG 01-15 16:10:51.882835.882835 cuda_h.py:19] end prefill_layer cost 0.05741143226623535 seconds
DEBUG 01-15 16:10:51.882923.882923 lmp.py:1553] -------------------------------- end prefill layer 23 --------------------------------
DEBUG 01-15 16:10:51.882056.882056 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.882143.882143 lmp.py:1495] -------------------------------- start prefill layer 24 --------------------------------
DEBUG 01-15 16:10:51.882469.882469 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:51.882463.882463 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 01-15 16:10:51.882651.882651 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 3.838539123535156e-05 seconds
DEBUG 01-15 16:10:51.882553.882553 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 7.367134094238281e-05 seconds
DEBUG 01-15 16:10:51.882063.882063 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.882172.882172 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.883102.883102 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.883801.883801 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.883213.883213 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.883780.883780 cuda_h.py:19] end allocate_cuda_memory cost 0.0002722740173339844 seconds
DEBUG 01-15 16:10:51.883611.883611 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.883427.883427 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.883402.883402 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.883588.883588 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5703d41b-a3b1-4d64-9876-811354f19b8c
DEBUG 01-15 16:10:51.883499.883499 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.884317.884317 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.885850.885850 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5703d41b-a3b1-4d64-9876-811354f19b8c
DEBUG 01-15 16:10:51.885462.885462 cuda_h.py:19] end load_into_gpu_async cost 0.0016400814056396484 seconds
DEBUG 01-15 16:10:51.885694.885694 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.885672.885672 cuda_h.py:19] end restore_tensors2 cost 9.059906005859375e-05 seconds
DEBUG 01-15 16:10:51.885780.885780 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.00229644775390625 seconds
INFO 01-15 16:10:51.885298.885298 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5703d41b-a3b1-4d64-9876-811354f19b8c
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.887102.887102 cuda_h.py:19] end self_attn cost 0.003591299057006836 seconds
DEBUG 01-15 16:10:51.888305.888305 cuda_h.py:19] end iln_self_attn_paln cost 0.005199432373046875 seconds
DEBUG 01-15 16:10:51.888757.888757 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_25
DEBUG 01-15 16:10:51.888235.888235 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.888302.888302 cuda_h.py:19] end gate cost 0.0007836818695068359 seconds
DEBUG 01-15 16:10:51.889953.889953 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.889828.889828 lmp.py:1912] 
DEBUG 01-15 16:10:51.889828.889828 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.889306.889306 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.889201.889201 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.889519.889519 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.889407.889407 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.889673.889673 lmp.py:1917] 
DEBUG 01-15 16:10:51.889673.889673 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.889554.889554 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.889919.889919 lmp.py:1935]   Expert 36 |     20 | CPU
DEBUG 01-15 16:10:51.889801.889801 lmp.py:1935]   Expert 35 |     31 | CPU
DEBUG 01-15 16:10:51.889205.889205 lmp.py:1935]   Expert 46 |     46 | CPU
DEBUG 01-15 16:10:51.889325.889325 lmp.py:1935]   Expert 25 |     47 | CPU
DEBUG 01-15 16:10:51.889445.889445 lmp.py:1935]   Expert 51 |     51 | CPU
DEBUG 01-15 16:10:51.889611.889611 lmp.py:1935]   Expert 16 |     58 | CPU
DEBUG 01-15 16:10:51.889300.889300 lmp.py:1935]   Expert  0 |     64 | CPU
DEBUG 01-15 16:10:51.889751.889751 lmp.py:1935]   Expert 30 |     64 | CPU
DEBUG 01-15 16:10:51.889441.889441 lmp.py:1935]   Expert 43 |     70 | CPU
DEBUG 01-15 16:10:51.889130.889130 lmp.py:1935]   Expert 47 |     70 | CPU
DEBUG 01-15 16:10:51.889342.889342 lmp.py:1935]   Expert 42 |     73 | CPU
DEBUG 01-15 16:10:51.889793.889793 lmp.py:1935]   Expert 39 |     74 | CPU
DEBUG 01-15 16:10:51.889483.889483 lmp.py:1935]   Expert 44 |     74 | CPU
DEBUG 01-15 16:10:51.889126.889126 lmp.py:1935]   Expert 55 |     75 | CPU
DEBUG 01-15 16:10:51.889769.889769 lmp.py:1935]   Expert  2 |     84 | CPU
DEBUG 01-15 16:10:51.889458.889458 lmp.py:1935]   Expert  4 |    106 | CPU
DEBUG 01-15 16:10:51.889909.889909 lmp.py:1935]   Expert 13 |    120 | CPU
DEBUG 01-15 16:10:51.889121.889121 lmp.py:1935]   Expert 33 |    120 | CPU
DEBUG 01-15 16:10:51.889334.889334 lmp.py:1935]   Expert 48 |    122 | CPU
DEBUG 01-15 16:10:51.889546.889546 lmp.py:1935]   Expert  6 |    125 | CPU
DEBUG 01-15 16:10:51.889520.889520 lmp.py:1935]   Expert 24 |    126 | CPU
DEBUG 01-15 16:10:51.889494.889494 lmp.py:1935]   Expert 61 |    127 | CPU
DEBUG 01-15 16:10:51.889468.889468 lmp.py:1935]   Expert 29 |    130 | CPU
DEBUG 01-15 16:10:51.890681.890681 lmp.py:1935]   Expert 56 |    130 | CPU
DEBUG 01-15 16:10:51.890562.890562 lmp.py:1935]   Expert 15 |    132 | CPU
DEBUG 01-15 16:10:51.890205.890205 lmp.py:1935]   Expert 38 |    142 | CPU
DEBUG 01-15 16:10:51.890325.890325 lmp.py:1935]   Expert  9 |    143 | CPU
DEBUG 01-15 16:10:51.890253.890253 lmp.py:1935]   Expert 54 |    143 | CPU
DEBUG 01-15 16:10:51.890657.890657 lmp.py:1935]   Expert 20 |    145 | CPU
DEBUG 01-15 16:10:51.890823.890823 lmp.py:1935]   Expert  7 |    147 | CPU
DEBUG 01-15 16:10:51.890228.890228 lmp.py:1935]   Expert 59 |    148 | CPU
DEBUG 01-15 16:10:51.890632.890632 lmp.py:1935]   Expert 62 |    154 | CPU
DEBUG 01-15 16:10:51.890183.890183 lmp.py:1935]   Expert 45 |    159 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890779.890779 lmp.py:1935]   Expert 19 |    160 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890853.890853 lmp.py:1935]   Expert 34 |    188 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890926.890926 lmp.py:1935]   Expert 57 |    192 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890761.890761 lmp.py:1935]   Expert 50 |    197 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890120.890120 lmp.py:1935]   Expert 10 |    203 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890478.890478 lmp.py:1935]   Expert 31 |    203 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890075.890075 lmp.py:1935]   Expert 23 |    206 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890433.890433 lmp.py:1935]   Expert  8 |    214 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890745.890745 lmp.py:1935]   Expert 60 |    215 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890057.890057 lmp.py:1935]   Expert 18 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890845.890845 lmp.py:1935]   Expert 22 |    220 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890157.890157 lmp.py:1935]   Expert 53 |    224 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890754.890754 lmp.py:1935]   Expert 52 |    226 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890126.890126 lmp.py:1935]   Expert 37 |    231 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890259.890259 lmp.py:1935]   Expert  5 |    241 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890664.890664 lmp.py:1935]   Expert 17 |    243 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890783.890783 lmp.py:1935]   Expert 11 |    255 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890188.890188 lmp.py:1935]   Expert  1 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890354.890354 lmp.py:1935]   Expert 49 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890520.890520 lmp.py:1935]   Expert 41 |    277 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890686.890686 lmp.py:1935]   Expert 28 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890091.890091 lmp.py:1935]   Expert 26 |    290 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890496.890496 lmp.py:1935]   Expert 32 |    294 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890900.890900 lmp.py:1935]   Expert 58 |    299 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890305.890305 lmp.py:1935]   Expert 40 |    302 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890186.890186 lmp.py:1935]   Expert 14 |    309 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890591.890591 lmp.py:1935]   Expert 12 |    329 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890757.890757 lmp.py:1935]   Expert 63 |    334 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890684.890684 lmp.py:1935]   Expert 21 |    383 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890327.890327 lmp.py:1935]   Expert 27 |    668 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.890970.890970 lmp.py:1935]   Expert  3 |   1019 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.890421.890421 lmp.py:1937] 
DEBUG 01-15 16:10:51.890421.890421 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.890826.890826 lmp.py:1938]   CPU:   3161 tokens
DEBUG 01-15 16:10:51.890230.890230 lmp.py:1942]   cuda:1:   4497 tokens (15 experts)
DEBUG 01-15 16:10:51.890635.890635 lmp.py:1942]   cuda:2:   4630 tokens (17 experts)
DEBUG 01-15 16:10:51.890039.890039 lmp.py:1943]   Total GPU:   9127 tokens
DEBUG 01-15 16:10:51.890729.890729 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.890729.890729 lmp.py:1944] 
DEBUG 01-15 16:10:51.890855.890855 cuda_h.py:19] end experts_map_get cost 0.0017919540405273438 seconds
DEBUG 01-15 16:10:51.890156.890156 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.890674.890674 lmp.py:1953] 
DEBUG 01-15 16:10:51.890674.890674 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.891172.891172 cuda_h.py:19] end cpu_experts_submit cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:51.891676.891676 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.891221.891221 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.891885.891885 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.24.mlp.experts.1.gate_proj.weight': 0, 'model.layers.24.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.58.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.58.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.58.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.5.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.5.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.5.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.37.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.37.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.37.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.41.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.41.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.41.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.10.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.10.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.10.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.14.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.14.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.14.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.50.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.50.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.50.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.23.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.23.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.23.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.26.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.26.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.26.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.63.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.63.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.63.up_proj.weight': 253755392}, 2: {'model.layers.24.mlp.experts.32.gate_proj.weight': 0, 'model.layers.24.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.24.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.24.mlp.experts.40.gate_proj.weight': 17301504, 'model.layers.24.mlp.experts.40.down_proj.weight': 23068672, 'model.layers.24.mlp.experts.40.up_proj.weight': 28835840, 'model.layers.24.mlp.experts.8.gate_proj.weight': 34603008, 'model.layers.24.mlp.experts.8.down_proj.weight': 40370176, 'model.layers.24.mlp.experts.8.up_proj.weight': 46137344, 'model.layers.24.mlp.experts.11.gate_proj.weight': 51904512, 'model.layers.24.mlp.experts.11.down_proj.weight': 57671680, 'model.layers.24.mlp.experts.11.up_proj.weight': 63438848, 'model.layers.24.mlp.experts.12.gate_proj.weight': 69206016, 'model.layers.24.mlp.experts.12.down_proj.weight': 74973184, 'model.layers.24.mlp.experts.12.up_proj.weight': 80740352, 'model.layers.24.mlp.experts.45.gate_proj.weight': 86507520, 'model.layers.24.mlp.experts.45.down_proj.weight': 92274688, 'model.layers.24.mlp.experts.45.up_proj.weight': 98041856, 'model.layers.24.mlp.experts.60.gate_proj.weight': 103809024, 'model.layers.24.mlp.experts.60.down_proj.weight': 109576192, 'model.layers.24.mlp.experts.60.up_proj.weight': 115343360, 'model.layers.24.mlp.experts.49.gate_proj.weight': 121110528, 'model.layers.24.mlp.experts.49.down_proj.weight': 126877696, 'model.layers.24.mlp.experts.49.up_proj.weight': 132644864, 'model.layers.24.mlp.experts.17.gate_proj.weight': 138412032, 'model.layers.24.mlp.experts.17.down_proj.weight': 144179200, 'model.layers.24.mlp.experts.17.up_proj.weight': 149946368, 'model.layers.24.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.24.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.24.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.24.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.24.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.24.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.24.mlp.experts.21.gate_proj.weight': 190316544, 'model.layers.24.mlp.experts.21.down_proj.weight': 196083712, 'model.layers.24.mlp.experts.21.up_proj.weight': 201850880, 'model.layers.24.mlp.experts.53.gate_proj.weight': 207618048, 'model.layers.24.mlp.experts.53.down_proj.weight': 213385216, 'model.layers.24.mlp.experts.53.up_proj.weight': 219152384, 'model.layers.24.mlp.experts.57.gate_proj.weight': 224919552, 'model.layers.24.mlp.experts.57.down_proj.weight': 230686720, 'model.layers.24.mlp.experts.57.up_proj.weight': 236453888, 'model.layers.24.mlp.experts.27.gate_proj.weight': 242221056, 'model.layers.24.mlp.experts.27.down_proj.weight': 247988224, 'model.layers.24.mlp.experts.27.up_proj.weight': 253755392, 'model.layers.24.mlp.experts.28.gate_proj.weight': 259522560, 'model.layers.24.mlp.experts.28.down_proj.weight': 265289728, 'model.layers.24.mlp.experts.28.up_proj.weight': 271056896, 'model.layers.24.mlp.experts.31.gate_proj.weight': 276824064, 'model.layers.24.mlp.experts.31.down_proj.weight': 282591232, 'model.layers.24.mlp.experts.31.up_proj.weight': 288358400}}tensor_copy_chunks_device_map {1: [(28345630720, 5767168, 0, 0), (28351397888, 5767168, 5767168, 0), (28339863552, 5767168, 11534336, 0), (28916580352, 5767168, 17301504, 0), (28922347520, 5767168, 23068672, 0), (28910813184, 5767168, 28835840, 0), (28380233728, 5767168, 34603008, 0), (28386000896, 5767168, 40370176, 0), (28374466560, 5767168, 46137344, 0), (29331816448, 5767168, 51904512, 0), (29337583616, 5767168, 57671680, 0), (29326049280, 5767168, 63438848, 0), (28414836736, 5767168, 69206016, 0), (28420603904, 5767168, 74973184, 0), (28409069568, 5767168, 80740352, 0), (28968484864, 5767168, 86507520, 0), (28974252032, 5767168, 92274688, 0), (28962717696, 5767168, 98041856, 0), (29037690880, 5767168, 103809024, 0), (29043458048, 5767168, 109576192, 0), (29031923712, 5767168, 115343360, 0), (28501344256, 5767168, 121110528, 0), (28507111424, 5767168, 126877696, 0), (28495577088, 5767168, 132644864, 0), (28570550272, 5767168, 138412032, 0), (28576317440, 5767168, 144179200, 0), (28564783104, 5767168, 149946368, 0), (28639756288, 5767168, 155713536, 0), (28645523456, 5767168, 161480704, 0), (28633989120, 5767168, 167247872, 0), (29193404416, 5767168, 173015040, 0), (29199171584, 5767168, 178782208, 0), (29187637248, 5767168, 184549376, 0), (28708962304, 5767168, 190316544, 0), (28714729472, 5767168, 196083712, 0), (28703195136, 5767168, 201850880, 0), (28726263808, 5767168, 207618048, 0), (28732030976, 5767168, 213385216, 0), (28720496640, 5767168, 219152384, 0), (28778168320, 5767168, 224919552, 0), (28783935488, 5767168, 230686720, 0), (28772401152, 5767168, 236453888, 0), (29418323968, 5767168, 242221056, 0), (29424091136, 5767168, 247988224, 0), (29412556800, 5767168, 253755392, 0)], 2: [(28881977344, 5767168, 0, 0), (28887744512, 5767168, 5767168, 0), (28876210176, 5767168, 11534336, 0), (29020389376, 5767168, 17301504, 0), (29026156544, 5767168, 23068672, 0), (29014622208, 5767168, 28835840, 0), (28466741248, 5767168, 34603008, 0), (28472508416, 5767168, 40370176, 0), (28460974080, 5767168, 46137344, 0), (28518645760, 5767168, 51904512, 0), (28524412928, 5767168, 57671680, 0), (28512878592, 5767168, 63438848, 0), (28535947264, 5767168, 69206016, 0), (28541714432, 5767168, 74973184, 0), (28530180096, 5767168, 80740352, 0), (29106896896, 5767168, 86507520, 0), (29112664064, 5767168, 92274688, 0), (29101129728, 5767168, 98041856, 0), (29366419456, 5767168, 103809024, 0), (29372186624, 5767168, 109576192, 0), (29360652288, 5767168, 115343360, 0), (29176102912, 5767168, 121110528, 0), (29181870080, 5767168, 126877696, 0), (29170335744, 5767168, 132644864, 0), (28622454784, 5767168, 138412032, 0), (28628221952, 5767168, 144179200, 0), (28616687616, 5767168, 149946368, 0), (28657057792, 5767168, 155713536, 0), (28662824960, 5767168, 161480704, 0), (28651290624, 5767168, 167247872, 0), (29228007424, 5767168, 173015040, 0), (29233774592, 5767168, 178782208, 0), (29222240256, 5767168, 184549376, 0), (28691660800, 5767168, 190316544, 0), (28697427968, 5767168, 196083712, 0), (28685893632, 5767168, 201850880, 0), (29245308928, 5767168, 207618048, 0), (29251076096, 5767168, 213385216, 0), (29239541760, 5767168, 219152384, 0), (29314514944, 5767168, 224919552, 0), (29320282112, 5767168, 230686720, 0), (29308747776, 5767168, 236453888, 0), (28795469824, 5767168, 242221056, 0), (28801236992, 5767168, 247988224, 0), (28789702656, 5767168, 253755392, 0), (28812771328, 5767168, 259522560, 0), (28818538496, 5767168, 265289728, 0), (28807004160, 5767168, 271056896, 0), (28864675840, 5767168, 276824064, 0), (28870443008, 5767168, 282591232, 0), (28858908672, 5767168, 288358400, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4e6c2f1b30>, 2: <capsule object NULL at 0x7a51b06da8b0>}
DEBUG 01-15 16:10:51.892036.892036 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.892349.892349 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.892257.892257 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 111b161e-9ef7-47e9-85db-4bb888036900
DEBUG 01-15 16:10:51.892138.892138 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.892680.892680 cuda_h.py:10] start experts_func_gpu_einsum_mp
INFO 01-15 16:10:51.892476.892476 client.py:127] Model loaded
DEBUG 01-15 16:10:51.892412.892412 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.893518.893518 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:51.893776.893776 cuda_h.py:19] end restore2model cost 0.000339508056640625 seconds
DEBUG 01-15 16:10:51.893499.893499 cuda_h.py:19] end sllm_worker_task cost 0.01018071174621582 seconds
INFO 01-15 16:10:51.893630.893630 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 111b161e-9ef7-47e9-85db-4bb888036900
DEBUG 01-15 16:10:51.893872.893872 cuda_h.py:19] end move_flatidxs cost 0.000835418701171875 seconds
DEBUG 01-15 16:10:51.893979.893979 cuda_h.py:10] start group_tensors
DEBUG 01-15 16:10:51.894629.894629 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.0028693675994873047 seconds
DEBUG 01-15 16:10:51.894055.894055 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.896664.896664 cuda_h.py:19] end restore2model cost 0.002522706985473633 seconds
DEBUG 01-15 16:10:51.896692.896692 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.005608081817626953 seconds
DEBUG 01-15 16:10:51.896203.896203 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.897902.897902 cuda_h.py:19] end gpu_sexperts cost 0.00027370452880859375 seconds
DEBUG 01-15 16:10:51.897493.897493 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.897362.897362 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.621246337890625e-05 seconds
DEBUG 01-15 16:10:51.897297.897297 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.897284.897284 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.897119.897119 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007605552673339844 seconds
DEBUG 01-15 16:10:51.898293.898293 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.898081.898081 cuda_h.py:19] end gpu_group_list cost 0.00016617774963378906 seconds
DEBUG 01-15 16:10:51.898429.898429 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.899939.899939 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0009136199951171875 seconds
DEBUG 01-15 16:10:51.899173.899173 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.900796.900796 cuda_h.py:19] end gpu_group_list cost 0.00018405914306640625 seconds
DEBUG 01-15 16:10:51.900724.900724 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.900176.900176 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 111b161e-9ef7-47e9-85db-4bb888036900
DEBUG 01-15 16:10:51.903671.903671 cuda_h.py:19] end group_tensors cost 0.009356975555419922 seconds
DEBUG 01-15 16:10:51.904089.904089 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.908716.908716 cuda_h.py:19] end group pad cost 0.0038535594940185547 seconds
DEBUG 01-15 16:10:51.908884.908884 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.923523.923523 client.py:127] Model loaded
DEBUG 01-15 16:10:51.924552.924552 cuda_h.py:19] end wait_experts_multi_device cost 0.023411273956298828 seconds
DEBUG 01-15 16:10:51.924782.924782 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.928963.928963 cuda_h.py:19] end group_einsum cost 0.0200955867767334 seconds
DEBUG 01-15 16:10:51.928856.928856 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.931990.931990 cuda_h.py:19] end get_outputs_cpu1 cost 0.0033409595489501953 seconds
DEBUG 01-15 16:10:51.932992.932992 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03972768783569336 seconds
DEBUG 01-15 16:10:51.932754.932754 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008347272872924805 seconds
DEBUG 01-15 16:10:51.933890.933890 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.934890.934890 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.934869.934869 cuda_h.py:19] end index_scatter cost 7.295608520507812e-05 seconds
DEBUG 01-15 16:10:51.934905.934905 cuda_h.py:19] end cpuoutputsdeal cost 0.0016231536865234375 seconds
DEBUG 01-15 16:10:51.934246.934246 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.934876.934876 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.934822.934822 cuda_h.py:19] end gpu_group_tensor cost 0.00013947486877441406 seconds
DEBUG 01-15 16:10:51.935439.935439 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.935557.935557 cuda_h.py:19] end gpu_group_tensor cost 0.0001285076141357422 seconds
DEBUG 01-15 16:10:51.935177.935177 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.936300.936300 cuda_h.py:19] end gpu_group_einsum cost 0.0008435249328613281 seconds
DEBUG 01-15 16:10:51.936464.936464 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.936855.936855 cuda_h.py:19] end gpu_group_einsum cost 0.0005440711975097656 seconds
DEBUG 01-15 16:10:51.937124.937124 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.937764.937764 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.937846.937846 cuda_h.py:19] end all_expert_outputs_slices cost 0.00022268295288085938 seconds
DEBUG 01-15 16:10:51.937138.937138 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.937890.937890 cuda_h.py:19] end concat_expert_out cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:10:51.937541.937541 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.937610.937610 cuda_h.py:19] end index_scatter cost 5.173683166503906e-05 seconds
DEBUG 01-15 16:10:51.937200.937200 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0008463859558105469 seconds
DEBUG 01-15 16:10:51.938077.938077 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.938629.938629 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.938740.938740 cuda_h.py:19] end all_expert_outputs_slices cost 0.00012111663818359375 seconds
DEBUG 01-15 16:10:51.938165.938165 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.938611.938611 cuda_h.py:19] end concat_expert_out cost 5.269050598144531e-05 seconds
DEBUG 01-15 16:10:51.938455.938455 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.938901.938901 cuda_h.py:19] end index_scatter cost 5.030632019042969e-05 seconds
DEBUG 01-15 16:10:51.938518.938518 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00046563148498535156 seconds
DEBUG 01-15 16:10:51.938898.938898 cuda_h.py:19] end gpu_experts_multi_device cost 0.04146742820739746 seconds
DEBUG 01-15 16:10:51.938046.938046 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_25 cost 0.05053210258483887 seconds
DEBUG 01-15 16:10:51.939953.939953 cuda_h.py:19] end prefill_layer cost 0.05642223358154297 seconds
DEBUG 01-15 16:10:51.939167.939167 lmp.py:1553] -------------------------------- end prefill layer 24 --------------------------------
DEBUG 01-15 16:10:51.939108.939108 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.939765.939765 lmp.py:1495] -------------------------------- start prefill layer 25 --------------------------------
DEBUG 01-15 16:10:51.939183.939183 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:51.939747.939747 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 01-15 16:10:51.939458.939458 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 3.9577484130859375e-05 seconds
DEBUG 01-15 16:10:51.939690.939690 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 7.343292236328125e-05 seconds
DEBUG 01-15 16:10:51.939532.939532 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.939919.939919 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.939412.939412 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.939985.939985 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.939636.939636 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.939565.939565 cuda_h.py:19] end allocate_cuda_memory cost 0.00022411346435546875 seconds
DEBUG 01-15 16:10:51.940945.940945 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.940708.940708 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.940060.940060 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.940532.940532 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5f70ff7e-4fe8-4283-8416-258fbcfabae3
DEBUG 01-15 16:10:51.940635.940635 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.940407.940407 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.942601.942601 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5f70ff7e-4fe8-4283-8416-258fbcfabae3
DEBUG 01-15 16:10:51.942967.942967 cuda_h.py:19] end load_into_gpu_async cost 0.0020155906677246094 seconds
DEBUG 01-15 16:10:51.942770.942770 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.942787.942787 cuda_h.py:19] end restore_tensors2 cost 8.606910705566406e-05 seconds
DEBUG 01-15 16:10:51.942358.942358 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002592325210571289 seconds
INFO 01-15 16:10:51.942393.942393 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5f70ff7e-4fe8-4283-8416-258fbcfabae3
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:51.943871.943871 cuda_h.py:19] end self_attn cost 0.0031130313873291016 seconds
DEBUG 01-15 16:10:51.944021.944021 cuda_h.py:19] end iln_self_attn_paln cost 0.004682064056396484 seconds
DEBUG 01-15 16:10:51.944235.944235 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_26
DEBUG 01-15 16:10:51.944667.944667 cuda_h.py:10] start gate
DEBUG 01-15 16:10:51.944857.944857 cuda_h.py:19] end gate cost 0.0006978511810302734 seconds
DEBUG 01-15 16:10:51.944077.944077 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:51.945243.945243 lmp.py:1912] 
DEBUG 01-15 16:10:51.945243.945243 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:51.945529.945529 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:51.945139.945139 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:51.945696.945696 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:51.945154.945154 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:51.945658.945658 lmp.py:1917] 
DEBUG 01-15 16:10:51.945658.945658 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:51.945354.945354 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:51.945726.945726 lmp.py:1935]   Expert 13 |     29 | CPU
DEBUG 01-15 16:10:51.945707.945707 lmp.py:1935]   Expert 44 |     41 | CPU
DEBUG 01-15 16:10:51.945880.945880 lmp.py:1935]   Expert 25 |     42 | CPU
DEBUG 01-15 16:10:51.945576.945576 lmp.py:1935]   Expert  9 |     43 | CPU
DEBUG 01-15 16:10:51.945557.945557 lmp.py:1935]   Expert 16 |     45 | CPU
DEBUG 01-15 16:10:51.945968.945968 lmp.py:1935]   Expert 38 |     47 | CPU
DEBUG 01-15 16:10:51.945903.945903 lmp.py:1935]   Expert  2 |     55 | CPU
DEBUG 01-15 16:10:51.945599.945599 lmp.py:1935]   Expert 22 |     55 | CPU
DEBUG 01-15 16:10:51.945533.945533 lmp.py:1935]   Expert 33 |     58 | CPU
DEBUG 01-15 16:10:51.945660.945660 lmp.py:1935]   Expert 42 |     61 | CPU
DEBUG 01-15 16:10:51.945925.945925 lmp.py:1935]   Expert  5 |     64 | CPU
DEBUG 01-15 16:10:51.945091.945091 lmp.py:1935]   Expert 23 |     77 | CPU
DEBUG 01-15 16:10:51.945973.945973 lmp.py:1935]   Expert 24 |     80 | CPU
DEBUG 01-15 16:10:51.945139.945139 lmp.py:1935]   Expert 10 |     82 | CPU
DEBUG 01-15 16:10:51.945544.945544 lmp.py:1935]   Expert 59 |    102 | CPU
DEBUG 01-15 16:10:51.945710.945710 lmp.py:1935]   Expert 21 |    106 | CPU
DEBUG 01-15 16:10:51.945876.945876 lmp.py:1935]   Expert 55 |    113 | CPU
DEBUG 01-15 16:10:51.945519.945519 lmp.py:1935]   Expert 45 |    115 | CPU
DEBUG 01-15 16:10:51.945162.945162 lmp.py:1935]   Expert 46 |    117 | CPU
DEBUG 01-15 16:10:51.945805.945805 lmp.py:1935]   Expert 61 |    120 | CPU
DEBUG 01-15 16:10:51.945209.945209 lmp.py:1935]   Expert 31 |    130 | CPU
DEBUG 01-15 16:10:51.945852.945852 lmp.py:1935]   Expert  6 |    141 | CPU
DEBUG 01-15 16:10:51.945018.945018 lmp.py:1935]   Expert  8 |    141 | CPU
DEBUG 01-15 16:10:51.945661.945661 lmp.py:1935]   Expert 51 |    143 | CPU
DEBUG 01-15 16:10:51.946066.946066 lmp.py:1935]   Expert 36 |    144 | CPU
DEBUG 01-15 16:10:51.946709.946709 lmp.py:1935]   Expert 43 |    146 | CPU
DEBUG 01-15 16:10:51.946113.946113 lmp.py:1935]   Expert  0 |    153 | CPU
DEBUG 01-15 16:10:51.946664.946664 lmp.py:1935]   Expert  3 |    153 | CPU
DEBUG 01-15 16:10:51.946068.946068 lmp.py:1935]   Expert 26 |    156 | CPU
DEBUG 01-15 16:10:51.946711.946711 lmp.py:1935]   Expert 18 |    160 | CPU
DEBUG 01-15 16:10:51.946877.946877 lmp.py:1935]   Expert 48 |    161 | CPU
DEBUG 01-15 16:10:51.946282.946282 lmp.py:1935]   Expert 41 |    167 | CPU
DEBUG 01-15 16:10:51.946038.946038 lmp.py:1935]   Expert  7 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946873.946873 lmp.py:1935]   Expert 12 |    175 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946947.946947 lmp.py:1935]   Expert 20 |    182 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946543.946543 lmp.py:1935]   Expert 56 |    187 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946378.946378 lmp.py:1935]   Expert 28 |    189 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946975.946975 lmp.py:1935]   Expert 27 |    194 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946856.946856 lmp.py:1935]   Expert 34 |    195 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946976.946976 lmp.py:1935]   Expert  1 |    196 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946096.946096 lmp.py:1935]   Expert 47 |    201 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946216.946216 lmp.py:1935]   Expert 11 |    212 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946097.946097 lmp.py:1935]   Expert 32 |    216 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946694.946694 lmp.py:1935]   Expert 40 |    230 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946006.946006 lmp.py:1935]   Expert 49 |    235 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946794.946794 lmp.py:1935]   Expert 63 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946868.946868 lmp.py:1935]   Expert 53 |    237 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946465.946465 lmp.py:1935]   Expert 15 |    241 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946061.946061 lmp.py:1935]   Expert  4 |    245 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946419.946419 lmp.py:1935]   Expert 50 |    245 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946301.946301 lmp.py:1935]   Expert 29 |    246 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946911.946911 lmp.py:1935]   Expert 30 |    248 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946792.946792 lmp.py:1935]   Expert 35 |    270 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946435.946435 lmp.py:1935]   Expert 14 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946840.946840 lmp.py:1935]   Expert 37 |    305 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946245.946245 lmp.py:1935]   Expert 52 |    340 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946603.946603 lmp.py:1935]   Expert 17 |    359 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946438.946438 lmp.py:1935]   Expert 54 |    379 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946842.946842 lmp.py:1935]   Expert 39 |    389 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946247.946247 lmp.py:1935]   Expert 57 |    409 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946652.946652 lmp.py:1935]   Expert 60 |    456 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946533.946533 lmp.py:1935]   Expert 62 |    459 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946937.946937 lmp.py:1935]   Expert 19 |    544 | GPU2(cuda:2)
DEBUG 01-15 16:10:51.946104.946104 lmp.py:1935]   Expert 58 |    571 | GPU1(cuda:1)
DEBUG 01-15 16:10:51.946078.946078 lmp.py:1937] 
DEBUG 01-15 16:10:51.946078.946078 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:51.946482.946482 lmp.py:1938]   CPU:   3247 tokens
DEBUG 01-15 16:10:51.946887.946887 lmp.py:1942]   cuda:1:   4522 tokens (16 experts)
DEBUG 01-15 16:10:51.946053.946053 lmp.py:1942]   cuda:2:   4519 tokens (16 experts)
DEBUG 01-15 16:10:51.946504.946504 lmp.py:1943]   Total GPU:   9041 tokens
DEBUG 01-15 16:10:51.946432.946432 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:51.946432.946432 lmp.py:1944] 
DEBUG 01-15 16:10:51.946558.946558 cuda_h.py:19] end experts_map_get cost 0.0018672943115234375 seconds
DEBUG 01-15 16:10:51.946408.946408 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:51.946733.946733 lmp.py:1953] 
DEBUG 01-15 16:10:51.946733.946733 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:51.947325.947325 cuda_h.py:19] end cpu_experts_submit cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:51.947067.947067 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:51.947658.947658 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:51.948865.948865 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.25.mlp.experts.32.gate_proj.weight': 0, 'model.layers.25.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.11.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.11.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.11.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.28.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.28.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.28.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.17.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.17.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.17.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.49.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.49.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.49.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.56.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.56.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.56.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.58.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.58.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.58.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.29.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.29.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.29.up_proj.weight': 271056896}, 2: {'model.layers.25.mlp.experts.1.gate_proj.weight': 0, 'model.layers.25.mlp.experts.1.down_proj.weight': 5767168, 'model.layers.25.mlp.experts.1.up_proj.weight': 11534336, 'model.layers.25.mlp.experts.4.gate_proj.weight': 17301504, 'model.layers.25.mlp.experts.4.down_proj.weight': 23068672, 'model.layers.25.mlp.experts.4.up_proj.weight': 28835840, 'model.layers.25.mlp.experts.37.gate_proj.weight': 34603008, 'model.layers.25.mlp.experts.37.down_proj.weight': 40370176, 'model.layers.25.mlp.experts.37.up_proj.weight': 46137344, 'model.layers.25.mlp.experts.7.gate_proj.weight': 51904512, 'model.layers.25.mlp.experts.7.down_proj.weight': 57671680, 'model.layers.25.mlp.experts.7.up_proj.weight': 63438848, 'model.layers.25.mlp.experts.40.gate_proj.weight': 69206016, 'model.layers.25.mlp.experts.40.down_proj.weight': 74973184, 'model.layers.25.mlp.experts.40.up_proj.weight': 80740352, 'model.layers.25.mlp.experts.14.gate_proj.weight': 86507520, 'model.layers.25.mlp.experts.14.down_proj.weight': 92274688, 'model.layers.25.mlp.experts.14.up_proj.weight': 98041856, 'model.layers.25.mlp.experts.15.gate_proj.weight': 103809024, 'model.layers.25.mlp.experts.15.down_proj.weight': 109576192, 'model.layers.25.mlp.experts.15.up_proj.weight': 115343360, 'model.layers.25.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.25.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.25.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.25.mlp.experts.19.gate_proj.weight': 138412032, 'model.layers.25.mlp.experts.19.down_proj.weight': 144179200, 'model.layers.25.mlp.experts.19.up_proj.weight': 149946368, 'model.layers.25.mlp.experts.20.gate_proj.weight': 155713536, 'model.layers.25.mlp.experts.20.down_proj.weight': 161480704, 'model.layers.25.mlp.experts.20.up_proj.weight': 167247872, 'model.layers.25.mlp.experts.54.gate_proj.weight': 173015040, 'model.layers.25.mlp.experts.54.down_proj.weight': 178782208, 'model.layers.25.mlp.experts.54.up_proj.weight': 184549376, 'model.layers.25.mlp.experts.30.gate_proj.weight': 190316544, 'model.layers.25.mlp.experts.30.down_proj.weight': 196083712, 'model.layers.25.mlp.experts.30.up_proj.weight': 201850880, 'model.layers.25.mlp.experts.57.gate_proj.weight': 207618048, 'model.layers.25.mlp.experts.57.down_proj.weight': 213385216, 'model.layers.25.mlp.experts.57.up_proj.weight': 219152384, 'model.layers.25.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.25.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.25.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.25.mlp.experts.62.gate_proj.weight': 242221056, 'model.layers.25.mlp.experts.62.down_proj.weight': 247988224, 'model.layers.25.mlp.experts.62.up_proj.weight': 253755392, 'model.layers.25.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.25.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.25.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(29989273600, 5767168, 0, 0), (29995040768, 5767168, 5767168, 0), (29983506432, 5767168, 11534336, 0), (30023876608, 5767168, 17301504, 0), (30029643776, 5767168, 23068672, 0), (30018109440, 5767168, 28835840, 0), (30041178112, 5767168, 34603008, 0), (30046945280, 5767168, 40370176, 0), (30035410944, 5767168, 46137344, 0), (30110384128, 5767168, 51904512, 0), (30116151296, 5767168, 57671680, 0), (30104616960, 5767168, 63438848, 0), (29625942016, 5767168, 69206016, 0), (29631709184, 5767168, 74973184, 0), (29620174848, 5767168, 80740352, 0), (29643243520, 5767168, 86507520, 0), (29649010688, 5767168, 92274688, 0), (29637476352, 5767168, 98041856, 0), (29920067584, 5767168, 103809024, 0), (29925834752, 5767168, 109576192, 0), (29914300416, 5767168, 115343360, 0), (29729751040, 5767168, 121110528, 0), (29735518208, 5767168, 126877696, 0), (29723983872, 5767168, 132644864, 0), (30300700672, 5767168, 138412032, 0), (30306467840, 5767168, 144179200, 0), (30294933504, 5767168, 149946368, 0), (30283399168, 5767168, 155713536, 0), (30289166336, 5767168, 161480704, 0), (30277632000, 5767168, 167247872, 0), (30335303680, 5767168, 173015040, 0), (30341070848, 5767168, 178782208, 0), (30329536512, 5767168, 184549376, 0), (30352605184, 5767168, 190316544, 0), (30358372352, 5767168, 196083712, 0), (30346838016, 5767168, 201850880, 0), (30404509696, 5767168, 207618048, 0), (30410276864, 5767168, 213385216, 0), (30398742528, 5767168, 219152384, 0), (30439112704, 5767168, 224919552, 0), (30444879872, 5767168, 230686720, 0), (30433345536, 5767168, 236453888, 0), (30473715712, 5767168, 242221056, 0), (30479482880, 5767168, 247988224, 0), (30467948544, 5767168, 253755392, 0), (29937369088, 5767168, 259522560, 0), (29943136256, 5767168, 265289728, 0), (29931601920, 5767168, 271056896, 0)], 2: [(29452926976, 5767168, 0, 0), (29458694144, 5767168, 5767168, 0), (29447159808, 5767168, 11534336, 0), (29504831488, 5767168, 17301504, 0), (29510598656, 5767168, 23068672, 0), (29499064320, 5767168, 28835840, 0), (30075781120, 5767168, 34603008, 0), (30081548288, 5767168, 40370176, 0), (30070013952, 5767168, 46137344, 0), (29556736000, 5767168, 51904512, 0), (29562503168, 5767168, 57671680, 0), (29550968832, 5767168, 63438848, 0), (30127685632, 5767168, 69206016, 0), (30133452800, 5767168, 74973184, 0), (30121918464, 5767168, 80740352, 0), (29677846528, 5767168, 86507520, 0), (29683613696, 5767168, 92274688, 0), (29672079360, 5767168, 98041856, 0), (29695148032, 5767168, 103809024, 0), (29700915200, 5767168, 109576192, 0), (29689380864, 5767168, 115343360, 0), (30248796160, 5767168, 121110528, 0), (30254563328, 5767168, 126877696, 0), (30243028992, 5767168, 132644864, 0), (29764354048, 5767168, 138412032, 0), (29770121216, 5767168, 144179200, 0), (29758586880, 5767168, 149946368, 0), (29781655552, 5767168, 155713536, 0), (29787422720, 5767168, 161480704, 0), (29775888384, 5767168, 167247872, 0), (30369906688, 5767168, 173015040, 0), (30375673856, 5767168, 178782208, 0), (30364139520, 5767168, 184549376, 0), (29954670592, 5767168, 190316544, 0), (29960437760, 5767168, 196083712, 0), (29948903424, 5767168, 201850880, 0), (30421811200, 5767168, 207618048, 0), (30427578368, 5767168, 213385216, 0), (30416044032, 5767168, 219152384, 0), (29902766080, 5767168, 224919552, 0), (29908533248, 5767168, 230686720, 0), (29896998912, 5767168, 236453888, 0), (30508318720, 5767168, 242221056, 0), (30514085888, 5767168, 247988224, 0), (30502551552, 5767168, 253755392, 0), (30525620224, 5767168, 259522560, 0), (30531387392, 5767168, 265289728, 0), (30519853056, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a4f2c2758c0>, 2: <capsule object NULL at 0x7a51b06da730>}
DEBUG 01-15 16:10:51.949155.949155 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.949845.949845 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.949502.949502 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c1b5644f-7b34-4f79-bfd5-a04985bed36d
DEBUG 01-15 16:10:51.949912.949912 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.949599.949599 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:51.950507.950507 cuda_h.py:10] start move_flatidxs
INFO 01-15 16:10:51.950083.950083 client.py:127] Model loaded
DEBUG 01-15 16:10:51.950244.950244 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.950408.950408 cuda_h.py:19] end restore2model cost 0.0003361701965332031 seconds
DEBUG 01-15 16:10:51.950224.950224 cuda_h.py:19] end sllm_worker_task cost 0.011316776275634766 seconds
DEBUG 01-15 16:10:51.951479.951479 cuda_h.py:19] end move_flatidxs cost 0.0008463859558105469 seconds
DEBUG 01-15 16:10:51.951792.951792 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:51.952822.952822 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c1b5644f-7b34-4f79-bfd5-a04985bed36d
DEBUG 01-15 16:10:51.952923.952923 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005501985549926758 seconds
DEBUG 01-15 16:10:51.952925.952925 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:51.955439.955439 cuda_h.py:19] end restore2model cost 0.0024492740631103516 seconds
DEBUG 01-15 16:10:51.955944.955944 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.008174657821655273 seconds
DEBUG 01-15 16:10:51.955547.955547 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:51.955802.955802 cuda_h.py:19] end gpu_sexperts cost 0.0002627372741699219 seconds
DEBUG 01-15 16:10:51.955201.955201 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:51.955355.955355 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.5020370483398438e-05 seconds
DEBUG 01-15 16:10:51.955051.955051 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:51.955277.955277 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.956286.956286 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008182525634765625 seconds
DEBUG 01-15 16:10:51.956367.956367 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.956129.956129 cuda_h.py:19] end gpu_group_list cost 0.00018167495727539062 seconds
DEBUG 01-15 16:10:51.957724.957724 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:51.958973.958973 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008521080017089844 seconds
DEBUG 01-15 16:10:51.958631.958631 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:51.958340.958340 cuda_h.py:19] end gpu_group_list cost 0.0001780986785888672 seconds
DEBUG 01-15 16:10:51.959776.959776 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:51.959367.959367 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c1b5644f-7b34-4f79-bfd5-a04985bed36d
DEBUG 01-15 16:10:51.961754.961754 cuda_h.py:19] end group_tensors cost 0.010710716247558594 seconds
DEBUG 01-15 16:10:51.962973.962973 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:51.966835.966835 cuda_h.py:19] end group pad cost 0.0037469863891601562 seconds
DEBUG 01-15 16:10:51.966718.966718 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:51.978338.978338 client.py:127] Model loaded
DEBUG 01-15 16:10:51.979032.979032 cuda_h.py:19] end wait_experts_multi_device cost 0.019645214080810547 seconds
DEBUG 01-15 16:10:51.979034.979034 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:51.985317.985317 cuda_h.py:19] end group_einsum cost 0.018779516220092773 seconds
DEBUG 01-15 16:10:51.985706.985706 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:51.989222.989222 cuda_h.py:19] end get_outputs_cpu1 cost 0.0034732818603515625 seconds
DEBUG 01-15 16:10:51.989713.989713 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.039937496185302734 seconds
DEBUG 01-15 16:10:51.990906.990906 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.010982275009155273 seconds
DEBUG 01-15 16:10:51.990665.990665 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:51.991180.991180 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.991146.991146 cuda_h.py:19] end index_scatter cost 7.2479248046875e-05 seconds
DEBUG 01-15 16:10:51.991798.991798 cuda_h.py:19] end cpuoutputsdeal cost 0.0015692710876464844 seconds
DEBUG 01-15 16:10:51.991476.991476 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:51.991570.991570 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.992086.992086 cuda_h.py:19] end gpu_group_tensor cost 0.00014019012451171875 seconds
DEBUG 01-15 16:10:51.992180.992180 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:51.992575.992575 cuda_h.py:19] end gpu_group_tensor cost 0.00012302398681640625 seconds
DEBUG 01-15 16:10:51.992188.992188 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.992047.992047 cuda_h.py:19] end gpu_group_einsum cost 0.0004658699035644531 seconds
DEBUG 01-15 16:10:51.993163.993163 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:51.993508.993508 cuda_h.py:19] end gpu_group_einsum cost 0.0003445148468017578 seconds
DEBUG 01-15 16:10:51.993703.993703 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.993601.993601 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.993303.993303 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001633167266845703 seconds
DEBUG 01-15 16:10:51.993536.993536 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.993260.993260 cuda_h.py:19] end concat_expert_out cost 4.601478576660156e-05 seconds
DEBUG 01-15 16:10:51.993004.993004 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.994450.994450 cuda_h.py:19] end index_scatter cost 5.054473876953125e-05 seconds
DEBUG 01-15 16:10:51.994776.994776 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007472038269042969 seconds
DEBUG 01-15 16:10:51.994322.994322 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:51.994735.994735 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:51.994052.994052 cuda_h.py:19] end all_expert_outputs_slices cost 0.0001316070556640625 seconds
DEBUG 01-15 16:10:51.994139.994139 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:51.994108.994108 cuda_h.py:19] end concat_expert_out cost 5.245208740234375e-05 seconds
DEBUG 01-15 16:10:51.994613.994613 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:51.994544.994544 cuda_h.py:19] end index_scatter cost 5.53131103515625e-05 seconds
DEBUG 01-15 16:10:51.994161.994161 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004773139953613281 seconds
DEBUG 01-15 16:10:51.994448.994448 cuda_h.py:19] end gpu_experts_multi_device cost 0.03931474685668945 seconds
DEBUG 01-15 16:10:51.995596.995596 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_26 cost 0.050899505615234375 seconds
DEBUG 01-15 16:10:51.995542.995542 cuda_h.py:19] end prefill_layer cost 0.05625796318054199 seconds
DEBUG 01-15 16:10:51.995849.995849 lmp.py:1553] -------------------------------- end prefill layer 25 --------------------------------
DEBUG 01-15 16:10:51.995029.995029 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:51.995447.995447 lmp.py:1495] -------------------------------- start prefill layer 26 --------------------------------
DEBUG 01-15 16:10:51.995626.995626 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:51.995713.995713 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 01-15 16:10:51.995802.995802 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 3.743171691894531e-05 seconds
DEBUG 01-15 16:10:51.995558.995558 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 7.033348083496094e-05 seconds
DEBUG 01-15 16:10:51.995353.995353 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:51.995362.995362 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:51.995670.995670 cuda_h.py:10] start sllm_worker_task
DEBUG 01-15 16:10:51.995892.995892 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 01-15 16:10:51.996874.996874 cuda_h.py:10] start allocate_cuda_memory
DEBUG 01-15 16:10:51.996619.996619 cuda_h.py:19] end allocate_cuda_memory cost 0.0002651214599609375 seconds
DEBUG 01-15 16:10:51.996854.996854 cuda_h.py:10] start load_into_gpu_async
DEBUG 01-15 16:10:51.996385.996385 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:51.996169.996169 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:51.996163.996163 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c198faa8-5b05-4622-8d25-538c30cbebc5
DEBUG 01-15 16:10:51.996690.996690 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:51.996826.996826 cuda_h.py:10] start self_attn
INFO 01-15 16:10:51.998057.998057 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c198faa8-5b05-4622-8d25-538c30cbebc5
DEBUG 01-15 16:10:51.998615.998615 cuda_h.py:19] end load_into_gpu_async cost 0.0019249916076660156 seconds
DEBUG 01-15 16:10:51.998179.998179 cuda_h.py:10] start restore_tensors2
DEBUG 01-15 16:10:51.998673.998673 cuda_h.py:19] end restore_tensors2 cost 8.702278137207031e-05 seconds
DEBUG 01-15 16:10:51.998721.998721 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0025794506072998047 seconds
INFO 01-15 16:10:51.998518.998518 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c198faa8-5b05-4622-8d25-538c30cbebc5
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:52.000781.000781 cuda_h.py:19] end self_attn cost 0.0031232833862304688 seconds
DEBUG 01-15 16:10:52.000998.000998 cuda_h.py:19] end iln_self_attn_paln cost 0.004967451095581055 seconds
DEBUG 01-15 16:10:52.000649.000649 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_27
DEBUG 01-15 16:10:52.000842.000842 cuda_h.py:10] start gate
DEBUG 01-15 16:10:52.001351.001351 cuda_h.py:19] end gate cost 0.0007226467132568359 seconds
DEBUG 01-15 16:10:52.001048.001048 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:52.002068.002068 lmp.py:1912] 
DEBUG 01-15 16:10:52.002068.002068 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:52.002977.002977 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:52.002825.002825 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:52.002859.002859 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:52.002509.002509 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:52.002728.002728 lmp.py:1917] 
DEBUG 01-15 16:10:52.002728.002728 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:52.002901.002901 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:52.002988.002988 lmp.py:1935]   Expert 20 |     11 | CPU
DEBUG 01-15 16:10:52.002400.002400 lmp.py:1935]   Expert 61 |     11 | CPU
DEBUG 01-15 16:10:52.002526.002526 lmp.py:1935]   Expert 11 |     29 | CPU
DEBUG 01-15 16:10:52.002891.002891 lmp.py:1935]   Expert  7 |     37 | CPU
DEBUG 01-15 16:10:52.002826.002826 lmp.py:1935]   Expert 62 |     45 | CPU
DEBUG 01-15 16:10:52.002237.002237 lmp.py:1935]   Expert 51 |     46 | CPU
DEBUG 01-15 16:10:52.002933.002933 lmp.py:1935]   Expert  3 |     47 | CPU
DEBUG 01-15 16:10:52.002629.002629 lmp.py:1935]   Expert 30 |     52 | CPU
DEBUG 01-15 16:10:52.002849.002849 lmp.py:1935]   Expert 17 |     54 | CPU
DEBUG 01-15 16:10:52.002406.002406 lmp.py:1935]   Expert 29 |     56 | CPU
DEBUG 01-15 16:10:52.002479.002479 lmp.py:1935]   Expert  6 |     61 | CPU
DEBUG 01-15 16:10:52.002122.002122 lmp.py:1935]   Expert  9 |     64 | CPU
DEBUG 01-15 16:10:52.002765.002765 lmp.py:1935]   Expert 38 |     76 | CPU
DEBUG 01-15 16:10:52.002408.002408 lmp.py:1935]   Expert 63 |     78 | CPU
DEBUG 01-15 16:10:52.002813.002813 lmp.py:1935]   Expert 55 |     84 | CPU
DEBUG 01-15 16:10:52.002456.002456 lmp.py:1935]   Expert 59 |     88 | CPU
DEBUG 01-15 16:10:52.002860.002860 lmp.py:1935]   Expert 48 |     91 | CPU
DEBUG 01-15 16:10:52.002503.002503 lmp.py:1935]   Expert  8 |     97 | CPU
DEBUG 01-15 16:10:52.002908.002908 lmp.py:1935]   Expert 19 |     97 | CPU
DEBUG 01-15 16:10:52.002028.002028 lmp.py:1935]   Expert 22 |    100 | CPU
DEBUG 01-15 16:10:52.002386.002386 lmp.py:1935]   Expert 49 |    103 | CPU
DEBUG 01-15 16:10:52.002029.002029 lmp.py:1935]   Expert 24 |    113 | CPU
DEBUG 01-15 16:10:52.002433.002433 lmp.py:1935]   Expert 34 |    113 | CPU
DEBUG 01-15 16:10:52.002599.002599 lmp.py:1935]   Expert 36 |    114 | CPU
DEBUG 01-15 16:10:52.002766.002766 lmp.py:1935]   Expert 42 |    118 | CPU
DEBUG 01-15 16:10:52.002693.002693 lmp.py:1935]   Expert 50 |    120 | CPU
DEBUG 01-15 16:10:52.002098.002098 lmp.py:1935]   Expert 39 |    123 | CPU
DEBUG 01-15 16:10:52.002741.002741 lmp.py:1935]   Expert  4 |    133 | CPU
DEBUG 01-15 16:10:52.002145.002145 lmp.py:1935]   Expert 41 |    144 | CPU
DEBUG 01-15 16:10:52.002027.002027 lmp.py:1935]   Expert 37 |    145 | CPU
DEBUG 01-15 16:10:52.002385.002385 lmp.py:1935]   Expert 15 |    148 | CPU
DEBUG 01-15 16:10:52.002028.002028 lmp.py:1935]   Expert 23 |    155 | CPU
DEBUG 01-15 16:10:52.002340.002340 lmp.py:1935]   Expert 56 |    161 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.002937.002937 lmp.py:1935]   Expert 16 |    164 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.002772.002772 lmp.py:1935]   Expert 60 |    164 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.002368.002368 lmp.py:1935]   Expert 44 |    169 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.002727.002727 lmp.py:1935]   Expert  1 |    179 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.002323.002323 lmp.py:1935]   Expert 21 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.002397.002397 lmp.py:1935]   Expert 43 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.002470.002470 lmp.py:1935]   Expert 53 |    190 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.002590.002590 lmp.py:1935]   Expert 47 |    193 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003948.003948 lmp.py:1935]   Expert 33 |    198 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003830.003830 lmp.py:1935]   Expert 12 |    200 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003949.003949 lmp.py:1935]   Expert 13 |    208 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003308.003308 lmp.py:1935]   Expert 32 |    226 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003666.003666 lmp.py:1935]   Expert 28 |    231 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003786.003786 lmp.py:1935]   Expert  0 |    255 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003336.003336 lmp.py:1935]   Expert 54 |    256 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003171.003171 lmp.py:1935]   Expert 26 |    260 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003768.003768 lmp.py:1935]   Expert 31 |    260 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003888.003888 lmp.py:1935]   Expert 18 |    267 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003007.003007 lmp.py:1935]   Expert 10 |    268 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003889.003889 lmp.py:1935]   Expert 57 |    273 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003009.003009 lmp.py:1935]   Expert  2 |    282 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003049.003049 lmp.py:1935]   Expert 58 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003692.003692 lmp.py:1935]   Expert 40 |    340 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003097.003097 lmp.py:1935]   Expert 25 |    361 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003740.003740 lmp.py:1935]   Expert 45 |    363 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003098.003098 lmp.py:1935]   Expert  5 |    442 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003979.003979 lmp.py:1935]   Expert 35 |    456 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003622.003622 lmp.py:1935]   Expert 27 |    485 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003265.003265 lmp.py:1935]   Expert 46 |    554 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003670.003670 lmp.py:1935]   Expert 52 |    591 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.003313.003313 lmp.py:1935]   Expert 14 |    880 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.003764.003764 lmp.py:1937] 
DEBUG 01-15 16:10:52.003764.003764 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:52.003407.003407 lmp.py:1938]   CPU:   2753 tokens
DEBUG 01-15 16:10:52.003050.003050 lmp.py:1942]   cuda:1:   4824 tokens (16 experts)
DEBUG 01-15 16:10:52.003408.003408 lmp.py:1942]   cuda:2:   4711 tokens (16 experts)
DEBUG 01-15 16:10:52.003859.003859 lmp.py:1943]   Total GPU:   9535 tokens
DEBUG 01-15 16:10:52.003071.003071 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:52.003071.003071 lmp.py:1944] 
DEBUG 01-15 16:10:52.003582.003582 cuda_h.py:19] end experts_map_get cost 0.0018684864044189453 seconds
DEBUG 01-15 16:10:52.003638.003638 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:52.003930.003930 lmp.py:1953] 
DEBUG 01-15 16:10:52.003930.003930 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:52.003906.003906 cuda_h.py:19] end cpu_experts_submit cost 5.125999450683594e-05 seconds
DEBUG 01-15 16:10:52.003933.003933 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:52.003478.003478 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:52.005449.005449 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.26.mlp.experts.0.gate_proj.weight': 0, 'model.layers.26.mlp.experts.0.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.0.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.32.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.32.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.32.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.1.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.1.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.1.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.5.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.5.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.5.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.43.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.43.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.43.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.12.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.12.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.12.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.14.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.14.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.14.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.47.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.47.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.47.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.16.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.16.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.16.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.18.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.18.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.18.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.56.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.56.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.56.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.31.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.31.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.31.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.57.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.57.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.57.up_proj.weight': 271056896}, 2: {'model.layers.26.mlp.experts.33.gate_proj.weight': 0, 'model.layers.26.mlp.experts.33.down_proj.weight': 5767168, 'model.layers.26.mlp.experts.33.up_proj.weight': 11534336, 'model.layers.26.mlp.experts.2.gate_proj.weight': 17301504, 'model.layers.26.mlp.experts.2.down_proj.weight': 23068672, 'model.layers.26.mlp.experts.2.up_proj.weight': 28835840, 'model.layers.26.mlp.experts.35.gate_proj.weight': 34603008, 'model.layers.26.mlp.experts.35.down_proj.weight': 40370176, 'model.layers.26.mlp.experts.35.up_proj.weight': 46137344, 'model.layers.26.mlp.experts.40.gate_proj.weight': 51904512, 'model.layers.26.mlp.experts.40.down_proj.weight': 57671680, 'model.layers.26.mlp.experts.40.up_proj.weight': 63438848, 'model.layers.26.mlp.experts.10.gate_proj.weight': 69206016, 'model.layers.26.mlp.experts.10.down_proj.weight': 74973184, 'model.layers.26.mlp.experts.10.up_proj.weight': 80740352, 'model.layers.26.mlp.experts.44.gate_proj.weight': 86507520, 'model.layers.26.mlp.experts.44.down_proj.weight': 92274688, 'model.layers.26.mlp.experts.44.up_proj.weight': 98041856, 'model.layers.26.mlp.experts.45.gate_proj.weight': 103809024, 'model.layers.26.mlp.experts.45.down_proj.weight': 109576192, 'model.layers.26.mlp.experts.45.up_proj.weight': 115343360, 'model.layers.26.mlp.experts.46.gate_proj.weight': 121110528, 'model.layers.26.mlp.experts.46.down_proj.weight': 126877696, 'model.layers.26.mlp.experts.46.up_proj.weight': 132644864, 'model.layers.26.mlp.experts.13.gate_proj.weight': 138412032, 'model.layers.26.mlp.experts.13.down_proj.weight': 144179200, 'model.layers.26.mlp.experts.13.up_proj.weight': 149946368, 'model.layers.26.mlp.experts.60.gate_proj.weight': 155713536, 'model.layers.26.mlp.experts.60.down_proj.weight': 161480704, 'model.layers.26.mlp.experts.60.up_proj.weight': 167247872, 'model.layers.26.mlp.experts.52.gate_proj.weight': 173015040, 'model.layers.26.mlp.experts.52.down_proj.weight': 178782208, 'model.layers.26.mlp.experts.52.up_proj.weight': 184549376, 'model.layers.26.mlp.experts.53.gate_proj.weight': 190316544, 'model.layers.26.mlp.experts.53.down_proj.weight': 196083712, 'model.layers.26.mlp.experts.53.up_proj.weight': 201850880, 'model.layers.26.mlp.experts.54.gate_proj.weight': 207618048, 'model.layers.26.mlp.experts.54.down_proj.weight': 213385216, 'model.layers.26.mlp.experts.54.up_proj.weight': 219152384, 'model.layers.26.mlp.experts.21.gate_proj.weight': 224919552, 'model.layers.26.mlp.experts.21.down_proj.weight': 230686720, 'model.layers.26.mlp.experts.21.up_proj.weight': 236453888, 'model.layers.26.mlp.experts.26.gate_proj.weight': 242221056, 'model.layers.26.mlp.experts.26.down_proj.weight': 247988224, 'model.layers.26.mlp.experts.26.up_proj.weight': 253755392, 'model.layers.26.mlp.experts.28.gate_proj.weight': 259522560, 'model.layers.26.mlp.experts.28.down_proj.weight': 265289728, 'model.layers.26.mlp.experts.28.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(30542921728, 5767168, 0, 0), (30548688896, 5767168, 5767168, 0), (30537154560, 5767168, 11534336, 0), (31096569856, 5767168, 17301504, 0), (31102337024, 5767168, 23068672, 0), (31090802688, 5767168, 28835840, 0), (30560223232, 5767168, 34603008, 0), (30565990400, 5767168, 40370176, 0), (30554456064, 5767168, 46137344, 0), (30629429248, 5767168, 51904512, 0), (30635196416, 5767168, 57671680, 0), (30623662080, 5767168, 63438848, 0), (31286886400, 5767168, 69206016, 0), (31292653568, 5767168, 74973184, 0), (31281119232, 5767168, 80740352, 0), (30750539776, 5767168, 86507520, 0), (30756306944, 5767168, 92274688, 0), (30744772608, 5767168, 98041856, 0), (30785142784, 5767168, 103809024, 0), (30790909952, 5767168, 109576192, 0), (30779375616, 5767168, 115343360, 0), (31356092416, 5767168, 121110528, 0), (31361859584, 5767168, 126877696, 0), (31350325248, 5767168, 132644864, 0), (30819745792, 5767168, 138412032, 0), (30825512960, 5767168, 144179200, 0), (30813978624, 5767168, 149946368, 0), (30854348800, 5767168, 155713536, 0), (30860115968, 5767168, 161480704, 0), (30848581632, 5767168, 167247872, 0), (31511805952, 5767168, 173015040, 0), (31517573120, 5767168, 178782208, 0), (31506038784, 5767168, 184549376, 0), (30975459328, 5767168, 190316544, 0), (30981226496, 5767168, 196083712, 0), (30969692160, 5767168, 201850880, 0), (31546408960, 5767168, 207618048, 0), (31552176128, 5767168, 213385216, 0), (31540641792, 5767168, 219152384, 0), (31010062336, 5767168, 224919552, 0), (31015829504, 5767168, 230686720, 0), (31004295168, 5767168, 236453888, 0), (31079268352, 5767168, 242221056, 0), (31085035520, 5767168, 247988224, 0), (31073501184, 5767168, 253755392, 0), (31529107456, 5767168, 259522560, 0), (31534874624, 5767168, 265289728, 0), (31523340288, 5767168, 271056896, 0)], 2: [(31113871360, 5767168, 0, 0), (31119638528, 5767168, 5767168, 0), (31108104192, 5767168, 11534336, 0), (30577524736, 5767168, 17301504, 0), (30583291904, 5767168, 23068672, 0), (30571757568, 5767168, 28835840, 0), (31148474368, 5767168, 34603008, 0), (31154241536, 5767168, 40370176, 0), (31142707200, 5767168, 46137344, 0), (31234981888, 5767168, 51904512, 0), (31240749056, 5767168, 57671680, 0), (31229214720, 5767168, 63438848, 0), (30715936768, 5767168, 69206016, 0), (30721703936, 5767168, 74973184, 0), (30710169600, 5767168, 80740352, 0), (31304187904, 5767168, 86507520, 0), (31309955072, 5767168, 92274688, 0), (31298420736, 5767168, 98041856, 0), (31321489408, 5767168, 103809024, 0), (31327256576, 5767168, 109576192, 0), (31315722240, 5767168, 115343360, 0), (31338790912, 5767168, 121110528, 0), (31344558080, 5767168, 126877696, 0), (31333023744, 5767168, 132644864, 0), (30767841280, 5767168, 138412032, 0), (30773608448, 5767168, 144179200, 0), (30762074112, 5767168, 149946368, 0), (31581011968, 5767168, 155713536, 0), (31586779136, 5767168, 161480704, 0), (31575244800, 5767168, 167247872, 0), (31442599936, 5767168, 173015040, 0), (31448367104, 5767168, 178782208, 0), (31436832768, 5767168, 184549376, 0), (31459901440, 5767168, 190316544, 0), (31465668608, 5767168, 196083712, 0), (31454134272, 5767168, 201850880, 0), (31477202944, 5767168, 207618048, 0), (31482970112, 5767168, 213385216, 0), (31471435776, 5767168, 219152384, 0), (30906253312, 5767168, 224919552, 0), (30912020480, 5767168, 230686720, 0), (30900486144, 5767168, 236453888, 0), (30992760832, 5767168, 242221056, 0), (30998528000, 5767168, 247988224, 0), (30986993664, 5767168, 253755392, 0), (31027363840, 5767168, 259522560, 0), (31033131008, 5767168, 265289728, 0), (31021596672, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a5b00d71140>, 2: <capsule object NULL at 0x7a51b06da520>}
DEBUG 01-15 16:10:52.006851.006851 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:52.006819.006819 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:52.006522.006522 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4967a690-a5c7-4160-ab9c-7ab0550d217e
DEBUG 01-15 16:10:52.006442.006442 client.py:106] call stub.LoadModelAsync
INFO 01-15 16:10:52.006041.006041 client.py:127] Model loaded
DEBUG 01-15 16:10:52.006970.006970 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:52.007634.007634 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:52.007664.007664 cuda_h.py:19] end restore2model cost 0.00033926963806152344 seconds
DEBUG 01-15 16:10:52.007672.007672 cuda_h.py:19] end sllm_worker_task cost 0.011259078979492188 seconds
DEBUG 01-15 16:10:52.007233.007233 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:52.008726.008726 cuda_h.py:19] end move_flatidxs cost 0.0008318424224853516 seconds
DEBUG 01-15 16:10:52.008264.008264 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:52.008004.008004 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4967a690-a5c7-4160-ab9c-7ab0550d217e
DEBUG 01-15 16:10:52.009017.009017 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.005213737487792969 seconds
DEBUG 01-15 16:10:52.009159.009159 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:52.011974.011974 cuda_h.py:19] end restore2model cost 0.0023593902587890625 seconds
DEBUG 01-15 16:10:52.011711.011711 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.0077855587005615234 seconds
DEBUG 01-15 16:10:52.011096.011096 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:52.011140.011140 cuda_h.py:19] end gpu_sexperts cost 0.00028252601623535156 seconds
DEBUG 01-15 16:10:52.011777.011777 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 01-15 16:10:52.011169.011169 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.52587890625e-05 seconds
DEBUG 01-15 16:10:52.012865.012865 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:52.012807.012807 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:52.012901.012901 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008118152618408203 seconds
DEBUG 01-15 16:10:52.012082.012082 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:52.013712.013712 cuda_h.py:19] end gpu_group_list cost 0.00018930435180664062 seconds
DEBUG 01-15 16:10:52.013909.013909 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:52.014151.014151 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008547306060791016 seconds
DEBUG 01-15 16:10:52.014856.014856 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:52.015757.015757 cuda_h.py:19] end gpu_group_list cost 0.00017905235290527344 seconds
DEBUG 01-15 16:10:52.015630.015630 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:52.015652.015652 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4967a690-a5c7-4160-ab9c-7ab0550d217e
DEBUG 01-15 16:10:52.017366.017366 cuda_h.py:19] end group_tensors cost 0.009133338928222656 seconds
DEBUG 01-15 16:10:52.018735.018735 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:52.022890.022890 cuda_h.py:19] end group pad cost 0.0037872791290283203 seconds
DEBUG 01-15 16:10:52.022057.022057 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:52.035466.035466 client.py:127] Model loaded
DEBUG 01-15 16:10:52.035907.035907 cuda_h.py:19] end wait_experts_multi_device cost 0.020205974578857422 seconds
DEBUG 01-15 16:10:52.036715.036715 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:52.040821.040821 cuda_h.py:19] end group_einsum cost 0.018773317337036133 seconds
DEBUG 01-15 16:10:52.041309.041309 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:52.044961.044961 cuda_h.py:19] end get_outputs_cpu1 cost 0.002975940704345703 seconds
DEBUG 01-15 16:10:52.044604.044604 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.03762102127075195 seconds
DEBUG 01-15 16:10:52.045214.045214 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.008855819702148438 seconds
DEBUG 01-15 16:10:52.045688.045688 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:52.046256.046256 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:52.046275.046275 cuda_h.py:19] end index_scatter cost 7.271766662597656e-05 seconds
DEBUG 01-15 16:10:52.046172.046172 cuda_h.py:19] end cpuoutputsdeal cost 0.0015819072723388672 seconds
DEBUG 01-15 16:10:52.046851.046851 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:52.047229.047229 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:52.047599.047599 cuda_h.py:19] end gpu_group_tensor cost 0.00013709068298339844 seconds
DEBUG 01-15 16:10:52.047693.047693 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:52.047989.047989 cuda_h.py:19] end gpu_group_tensor cost 0.00011920928955078125 seconds
DEBUG 01-15 16:10:52.047079.047079 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:52.048069.048069 cuda_h.py:19] end gpu_group_einsum cost 0.0008530616760253906 seconds
DEBUG 01-15 16:10:52.048380.048380 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:52.049715.049715 cuda_h.py:19] end gpu_group_einsum cost 0.00043201446533203125 seconds
DEBUG 01-15 16:10:52.049957.049957 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:52.049993.049993 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:52.049411.049411 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016498565673828125 seconds
DEBUG 01-15 16:10:52.049882.049882 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:52.049368.049368 cuda_h.py:19] end concat_expert_out cost 4.649162292480469e-05 seconds
DEBUG 01-15 16:10:52.049496.049496 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:52.049042.049042 cuda_h.py:19] end index_scatter cost 5.340576171875e-05 seconds
DEBUG 01-15 16:10:52.049923.049923 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007381439208984375 seconds
DEBUG 01-15 16:10:52.050847.050847 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:52.050014.050014 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:52.050046.050046 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013065338134765625 seconds
DEBUG 01-15 16:10:52.050041.050041 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:52.050295.050295 cuda_h.py:19] end concat_expert_out cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:52.050515.050515 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:52.050008.050008 cuda_h.py:19] end index_scatter cost 4.863739013671875e-05 seconds
DEBUG 01-15 16:10:52.050294.050294 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0004711151123046875 seconds
DEBUG 01-15 16:10:52.050250.050250 cuda_h.py:19] end gpu_experts_multi_device cost 0.03856825828552246 seconds
DEBUG 01-15 16:10:52.050498.050498 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_27 cost 0.049855947494506836 seconds
DEBUG 01-15 16:10:52.051808.051808 cuda_h.py:19] end prefill_layer cost 0.05549478530883789 seconds
DEBUG 01-15 16:10:52.051565.051565 lmp.py:1553] -------------------------------- end prefill layer 26 --------------------------------
DEBUG 01-15 16:10:52.051222.051222 cuda_h.py:10] start prefill_layer
DEBUG 01-15 16:10:52.051879.051879 lmp.py:1495] -------------------------------- start prefill layer 27 --------------------------------
DEBUG 01-15 16:10:52.051820.051820 cuda_h.py:10] start iln_self_attn_paln
DEBUG 01-15 16:10:52.051206.051206 mlpmodule.py:393] cuda:1 cuda:1
DEBUG 01-15 16:10:52.051514.051514 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 01-15 16:10:52.054681.054681 cuda_h.py:19] end self_attn cost 0.0027348995208740234 seconds
DEBUG 01-15 16:10:52.054486.054486 cuda_h.py:19] end iln_self_attn_paln cost 0.003443479537963867 seconds
DEBUG 01-15 16:10:52.054938.054938 cuda_h.py:10] start layer_moe_generate_mp_multi_device_l_28
DEBUG 01-15 16:10:52.054238.054238 cuda_h.py:10] start gate
DEBUG 01-15 16:10:52.055585.055585 cuda_h.py:19] end gate cost 0.0006375312805175781 seconds
DEBUG 01-15 16:10:52.055328.055328 cuda_h.py:10] start experts_map_get
DEBUG 01-15 16:10:52.055679.055679 lmp.py:1912] 
DEBUG 01-15 16:10:52.055679.055679 lmp.py:1912] Expert Token Distribution & Multi-Device Allocation (MP):
DEBUG 01-15 16:10:52.055250.055250 lmp.py:1913]   Total experts: 64
DEBUG 01-15 16:10:52.055668.055668 lmp.py:1914]   CPU experts: 32 (50%)
DEBUG 01-15 16:10:52.055987.055987 lmp.py:1915]   GPU experts: 32 (50%)
DEBUG 01-15 16:10:52.056445.056445 lmp.py:1916]   Number of GPU devices: 2
DEBUG 01-15 16:10:52.056949.056949 lmp.py:1917] 
DEBUG 01-15 16:10:52.056949.056949 lmp.py:1917]   Expert ID | Tokens | Device
DEBUG 01-15 16:10:52.056883.056883 lmp.py:1918]   -----------------------------------
DEBUG 01-15 16:10:52.056732.056732 lmp.py:1935]   Expert 18 |     64 | CPU
DEBUG 01-15 16:10:52.056905.056905 lmp.py:1935]   Expert 47 |     69 | CPU
DEBUG 01-15 16:10:52.056886.056886 lmp.py:1935]   Expert 54 |     71 | CPU
DEBUG 01-15 16:10:52.056105.056105 lmp.py:1935]   Expert 23 |     72 | CPU
DEBUG 01-15 16:10:52.056278.056278 lmp.py:1935]   Expert 48 |     82 | CPU
DEBUG 01-15 16:10:52.056497.056497 lmp.py:1935]   Expert 44 |     85 | CPU
DEBUG 01-15 16:10:52.056239.056239 lmp.py:1935]   Expert 45 |     86 | CPU
DEBUG 01-15 16:10:52.056982.056982 lmp.py:1935]   Expert 20 |     92 | CPU
DEBUG 01-15 16:10:52.056486.056486 lmp.py:1935]   Expert 31 |     97 | CPU
DEBUG 01-15 16:10:52.056990.056990 lmp.py:1935]   Expert 36 |    108 | CPU
DEBUG 01-15 16:10:52.056732.056732 lmp.py:1935]   Expert 61 |    110 | CPU
DEBUG 01-15 16:10:52.056236.056236 lmp.py:1935]   Expert 33 |    119 | CPU
DEBUG 01-15 16:10:52.056740.056740 lmp.py:1935]   Expert 42 |    119 | CPU
DEBUG 01-15 16:10:52.056006.056006 lmp.py:1935]   Expert 10 |    121 | CPU
DEBUG 01-15 16:10:52.056748.056748 lmp.py:1935]   Expert 24 |    125 | CPU
DEBUG 01-15 16:10:52.056617.056617 lmp.py:1935]   Expert 43 |    125 | CPU
DEBUG 01-15 16:10:52.056029.056029 lmp.py:1935]   Expert 49 |    127 | CPU
DEBUG 01-15 16:10:52.056009.056009 lmp.py:1935]   Expert 11 |    129 | CPU
DEBUG 01-15 16:10:52.056752.056752 lmp.py:1935]   Expert 56 |    131 | CPU
DEBUG 01-15 16:10:52.056494.056494 lmp.py:1935]   Expert  6 |    136 | CPU
DEBUG 01-15 16:10:52.056998.056998 lmp.py:1935]   Expert 51 |    143 | CPU
DEBUG 01-15 16:10:52.056741.056741 lmp.py:1935]   Expert  0 |    147 | CPU
DEBUG 01-15 16:10:52.056768.056768 lmp.py:1935]   Expert 17 |    148 | CPU
DEBUG 01-15 16:10:52.056510.056510 lmp.py:1935]   Expert  5 |    149 | CPU
DEBUG 01-15 16:10:52.056445.056445 lmp.py:1935]   Expert 12 |    156 | CPU
DEBUG 01-15 16:10:52.056949.056949 lmp.py:1935]   Expert 40 |    160 | CPU
DEBUG 01-15 16:10:52.056453.056453 lmp.py:1935]   Expert 55 |    161 | CPU
DEBUG 01-15 16:10:52.056242.056242 lmp.py:1935]   Expert 57 |    162 | CPU
DEBUG 01-15 16:10:52.056746.056746 lmp.py:1935]   Expert 59 |    162 | CPU
DEBUG 01-15 16:10:52.056011.056011 lmp.py:1935]   Expert 26 |    165 | CPU
DEBUG 01-15 16:10:52.056515.056515 lmp.py:1935]   Expert 13 |    167 | CPU
DEBUG 01-15 16:10:52.056019.056019 lmp.py:1935]   Expert 38 |    168 | CPU
DEBUG 01-15 16:10:52.056914.056914 lmp.py:1935]   Expert 46 |    168 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.056279.056279 lmp.py:1935]   Expert 35 |    173 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.056214.056214 lmp.py:1935]   Expert 58 |    174 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.056387.056387 lmp.py:1935]   Expert 30 |    175 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.056559.056559 lmp.py:1935]   Expert 50 |    178 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.056256.056256 lmp.py:1935]   Expert  7 |    181 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.056952.056952 lmp.py:1935]   Expert 16 |    182 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.056648.056648 lmp.py:1935]   Expert 15 |    200 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.056013.056013 lmp.py:1935]   Expert 32 |    202 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.056139.056139 lmp.py:1935]   Expert 14 |    205 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057551.057551 lmp.py:1935]   Expert  1 |    217 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057008.057008 lmp.py:1935]   Expert  3 |    218 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057181.057181 lmp.py:1935]   Expert  4 |    224 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057639.057639 lmp.py:1935]   Expert 34 |    236 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057335.057335 lmp.py:1935]   Expert 39 |    238 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057508.057508 lmp.py:1935]   Expert 28 |    246 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057158.057158 lmp.py:1935]   Expert 52 |    247 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057854.057854 lmp.py:1935]   Expert 25 |    257 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057550.057550 lmp.py:1935]   Expert 22 |    261 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057246.057246 lmp.py:1935]   Expert  2 |    275 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057704.057704 lmp.py:1935]   Expert 21 |    279 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057161.057161 lmp.py:1935]   Expert 41 |    279 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057619.057619 lmp.py:1935]   Expert 60 |    283 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057077.057077 lmp.py:1935]   Expert 63 |    288 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057773.057773 lmp.py:1935]   Expert 29 |    296 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057423.057423 lmp.py:1935]   Expert 62 |    296 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057357.057357 lmp.py:1935]   Expert 27 |    301 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057815.057815 lmp.py:1935]   Expert 37 |    330 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057272.057272 lmp.py:1935]   Expert 53 |    334 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057969.057969 lmp.py:1935]   Expert  8 |    335 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057665.057665 lmp.py:1935]   Expert 19 |    443 | GPU2(cuda:2)
DEBUG 01-15 16:10:52.057122.057122 lmp.py:1935]   Expert  9 |    611 | GPU1(cuda:1)
DEBUG 01-15 16:10:52.057626.057626 lmp.py:1937] 
DEBUG 01-15 16:10:52.057626.057626 lmp.py:1937]   Device Token Distribution:
DEBUG 01-15 16:10:52.057607.057607 lmp.py:1938]   CPU:   3956 tokens
DEBUG 01-15 16:10:52.057826.057826 lmp.py:1942]   cuda:1:   4195 tokens (16 experts)
DEBUG 01-15 16:10:52.057046.057046 lmp.py:1942]   cuda:2:   4137 tokens (16 experts)
DEBUG 01-15 16:10:52.057503.057503 lmp.py:1943]   Total GPU:   8332 tokens
DEBUG 01-15 16:10:52.057769.057769 lmp.py:1944] ============================================================
DEBUG 01-15 16:10:52.057769.057769 lmp.py:1944] 
DEBUG 01-15 16:10:52.057472.057472 cuda_h.py:19] end experts_map_get cost 0.002074718475341797 seconds
DEBUG 01-15 16:10:52.057183.057183 cuda_h.py:10] start cpu_experts_submit
DEBUG 01-15 16:10:52.057892.057892 lmp.py:1953] 
DEBUG 01-15 16:10:52.057892.057892 lmp.py:1953]   Computing 32 experts on CPU MP...
DEBUG 01-15 16:10:52.057881.057881 cuda_h.py:19] end cpu_experts_submit cost 6.079673767089844e-05 seconds
DEBUG 01-15 16:10:52.057869.057869 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model_multi_device
DEBUG 01-15 16:10:52.057334.057334 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu_multi_device
DEBUG 01-15 16:10:52.058052.058052 cuda_memory_view.py:520] tensor_device_offsets_device_map {1: {'model.layers.27.mlp.experts.32.gate_proj.weight': 0, 'model.layers.27.mlp.experts.32.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.32.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.1.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.1.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.1.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.4.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.4.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.4.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.39.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.39.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.39.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.9.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.9.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.9.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.41.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.41.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.41.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.46.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.46.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.46.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.16.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.16.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.16.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.50.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.50.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.50.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.52.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.52.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.52.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.53.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.53.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.53.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.22.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.22.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.22.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.58.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.58.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.58.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.27.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.27.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.27.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.60.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.60.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.60.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.62.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.62.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.62.up_proj.weight': 271056896}, 2: {'model.layers.27.mlp.experts.2.gate_proj.weight': 0, 'model.layers.27.mlp.experts.2.down_proj.weight': 5767168, 'model.layers.27.mlp.experts.2.up_proj.weight': 11534336, 'model.layers.27.mlp.experts.34.gate_proj.weight': 17301504, 'model.layers.27.mlp.experts.34.down_proj.weight': 23068672, 'model.layers.27.mlp.experts.34.up_proj.weight': 28835840, 'model.layers.27.mlp.experts.3.gate_proj.weight': 34603008, 'model.layers.27.mlp.experts.3.down_proj.weight': 40370176, 'model.layers.27.mlp.experts.3.up_proj.weight': 46137344, 'model.layers.27.mlp.experts.37.gate_proj.weight': 51904512, 'model.layers.27.mlp.experts.37.down_proj.weight': 57671680, 'model.layers.27.mlp.experts.37.up_proj.weight': 63438848, 'model.layers.27.mlp.experts.35.gate_proj.weight': 69206016, 'model.layers.27.mlp.experts.35.down_proj.weight': 74973184, 'model.layers.27.mlp.experts.35.up_proj.weight': 80740352, 'model.layers.27.mlp.experts.7.gate_proj.weight': 86507520, 'model.layers.27.mlp.experts.7.down_proj.weight': 92274688, 'model.layers.27.mlp.experts.7.up_proj.weight': 98041856, 'model.layers.27.mlp.experts.8.gate_proj.weight': 103809024, 'model.layers.27.mlp.experts.8.down_proj.weight': 109576192, 'model.layers.27.mlp.experts.8.up_proj.weight': 115343360, 'model.layers.27.mlp.experts.14.gate_proj.weight': 121110528, 'model.layers.27.mlp.experts.14.down_proj.weight': 126877696, 'model.layers.27.mlp.experts.14.up_proj.weight': 132644864, 'model.layers.27.mlp.experts.15.gate_proj.weight': 138412032, 'model.layers.27.mlp.experts.15.down_proj.weight': 144179200, 'model.layers.27.mlp.experts.15.up_proj.weight': 149946368, 'model.layers.27.mlp.experts.19.gate_proj.weight': 155713536, 'model.layers.27.mlp.experts.19.down_proj.weight': 161480704, 'model.layers.27.mlp.experts.19.up_proj.weight': 167247872, 'model.layers.27.mlp.experts.21.gate_proj.weight': 173015040, 'model.layers.27.mlp.experts.21.down_proj.weight': 178782208, 'model.layers.27.mlp.experts.21.up_proj.weight': 184549376, 'model.layers.27.mlp.experts.25.gate_proj.weight': 190316544, 'model.layers.27.mlp.experts.25.down_proj.weight': 196083712, 'model.layers.27.mlp.experts.25.up_proj.weight': 201850880, 'model.layers.27.mlp.experts.28.gate_proj.weight': 207618048, 'model.layers.27.mlp.experts.28.down_proj.weight': 213385216, 'model.layers.27.mlp.experts.28.up_proj.weight': 219152384, 'model.layers.27.mlp.experts.29.gate_proj.weight': 224919552, 'model.layers.27.mlp.experts.29.down_proj.weight': 230686720, 'model.layers.27.mlp.experts.29.up_proj.weight': 236453888, 'model.layers.27.mlp.experts.30.gate_proj.weight': 242221056, 'model.layers.27.mlp.experts.30.down_proj.weight': 247988224, 'model.layers.27.mlp.experts.30.up_proj.weight': 253755392, 'model.layers.27.mlp.experts.63.gate_proj.weight': 259522560, 'model.layers.27.mlp.experts.63.down_proj.weight': 265289728, 'model.layers.27.mlp.experts.63.up_proj.weight': 271056896}}tensor_copy_chunks_device_map {1: [(32203866112, 5767168, 0, 0), (32209633280, 5767168, 5767168, 0), (32198098944, 5767168, 11534336, 0), (31667519488, 5767168, 17301504, 0), (31673286656, 5767168, 23068672, 0), (31661752320, 5767168, 28835840, 0), (31719424000, 5767168, 34603008, 0), (31725191168, 5767168, 40370176, 0), (31713656832, 5767168, 46137344, 0), (32324976640, 5767168, 51904512, 0), (32330743808, 5767168, 57671680, 0), (32319209472, 5767168, 63438848, 0), (31805931520, 5767168, 69206016, 0), (31811698688, 5767168, 74973184, 0), (31800164352, 5767168, 80740352, 0), (32359579648, 5767168, 86507520, 0), (32365346816, 5767168, 92274688, 0), (32353812480, 5767168, 98041856, 0), (32446087168, 5767168, 103809024, 0), (32451854336, 5767168, 109576192, 0), (32440320000, 5767168, 115343360, 0), (31927042048, 5767168, 121110528, 0), (31932809216, 5767168, 126877696, 0), (31921274880, 5767168, 132644864, 0), (32515293184, 5767168, 138412032, 0), (32521060352, 5767168, 144179200, 0), (32509526016, 5767168, 149946368, 0), (32549896192, 5767168, 155713536, 0), (32555663360, 5767168, 161480704, 0), (32544129024, 5767168, 167247872, 0), (32567197696, 5767168, 173015040, 0), (32572964864, 5767168, 178782208, 0), (32561430528, 5767168, 184549376, 0), (32030851072, 5767168, 190316544, 0), (32036618240, 5767168, 196083712, 0), (32025083904, 5767168, 201850880, 0), (32653705216, 5767168, 207618048, 0), (32659472384, 5767168, 213385216, 0), (32647938048, 5767168, 219152384, 0), (32117358592, 5767168, 224919552, 0), (32123125760, 5767168, 230686720, 0), (32111591424, 5767168, 236453888, 0), (32688308224, 5767168, 242221056, 0), (32694075392, 5767168, 247988224, 0), (32682541056, 5767168, 253755392, 0), (32722911232, 5767168, 259522560, 0), (32728678400, 5767168, 265289728, 0), (32717144064, 5767168, 271056896, 0)], 2: [(31684820992, 5767168, 0, 0), (31690588160, 5767168, 5767168, 0), (31679053824, 5767168, 11534336, 0), (32238469120, 5767168, 17301504, 0), (32244236288, 5767168, 23068672, 0), (32232701952, 5767168, 28835840, 0), (31702122496, 5767168, 34603008, 0), (31707889664, 5767168, 40370176, 0), (31696355328, 5767168, 46137344, 0), (32290373632, 5767168, 51904512, 0), (32296140800, 5767168, 57671680, 0), (32284606464, 5767168, 63438848, 0), (32255770624, 5767168, 69206016, 0), (32261537792, 5767168, 74973184, 0), (32250003456, 5767168, 80740352, 0), (31771328512, 5767168, 86507520, 0), (31777095680, 5767168, 92274688, 0), (31765561344, 5767168, 98041856, 0), (31788630016, 5767168, 103809024, 0), (31794397184, 5767168, 109576192, 0), (31782862848, 5767168, 115343360, 0), (31892439040, 5767168, 121110528, 0), (31898206208, 5767168, 126877696, 0), (31886671872, 5767168, 132644864, 0), (31909740544, 5767168, 138412032, 0), (31915507712, 5767168, 144179200, 0), (31903973376, 5767168, 149946368, 0), (31978946560, 5767168, 155713536, 0), (31984713728, 5767168, 161480704, 0), (31973179392, 5767168, 167247872, 0), (32013549568, 5767168, 173015040, 0), (32019316736, 5767168, 178782208, 0), (32007782400, 5767168, 184549376, 0), (32082755584, 5767168, 190316544, 0), (32088522752, 5767168, 196083712, 0), (32076988416, 5767168, 201850880, 0), (32134660096, 5767168, 207618048, 0), (32140427264, 5767168, 213385216, 0), (32128892928, 5767168, 219152384, 0), (32151961600, 5767168, 224919552, 0), (32157728768, 5767168, 230686720, 0), (32146194432, 5767168, 236453888, 0), (32169263104, 5767168, 242221056, 0), (32175030272, 5767168, 247988224, 0), (32163495936, 5767168, 253755392, 0), (32740212736, 5767168, 259522560, 0), (32745979904, 5767168, 265289728, 0), (32734445568, 5767168, 271056896, 0)]}cuda_memory_handles_device_map {1: <capsule object NULL at 0x7a51b842e5e0>, 2: <capsule object NULL at 0x7a5afdc8cf00>}
DEBUG 01-15 16:10:52.058143.058143 sllm_store_c.py:27] get device uuid map
DEBUG 01-15 16:10:52.058960.058960 sllm_store_c.py:29] call client load into gpu
DEBUG 01-15 16:10:52.059637.059637 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1cb712ad-c94a-49f0-befe-7eebd9ac52d1
DEBUG 01-15 16:10:52.059194.059194 client.py:106] call stub.LoadModelAsync
DEBUG 01-15 16:10:52.059257.059257 cuda_h.py:10] start experts_func_gpu_einsum_mp
DEBUG 01-15 16:10:52.059836.059836 cuda_h.py:10] start move_flatidxs
DEBUG 01-15 16:10:52.060608.060608 cuda_h.py:19] end move_flatidxs cost 0.0008280277252197266 seconds
DEBUG 01-15 16:10:52.060238.060238 cuda_h.py:10] start group_tensors
INFO 01-15 16:10:52.061667.061667 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1cb712ad-c94a-49f0-befe-7eebd9ac52d1
DEBUG 01-15 16:10:52.062342.062342 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu_multi_device cost 0.004149913787841797 seconds
DEBUG 01-15 16:10:52.062344.062344 cuda_h.py:10] start restore2model
DEBUG 01-15 16:10:52.064824.064824 cuda_h.py:19] end restore2model cost 0.0024280548095703125 seconds
DEBUG 01-15 16:10:52.064190.064190 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model_multi_device cost 0.006810665130615234 seconds
DEBUG 01-15 16:10:52.064032.064032 cuda_h.py:10] start gpu_sexperts
DEBUG 01-15 16:10:52.064194.064194 cuda_h.py:19] end gpu_sexperts cost 0.0002644062042236328 seconds
DEBUG 01-15 16:10:52.065070.065070 cuda_h.py:10] start gpu_experts_multi_device
DEBUG 01-15 16:10:52.065449.065449 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:52.065602.065602 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0007834434509277344 seconds
DEBUG 01-15 16:10:52.065445.065445 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:52.066723.066723 cuda_h.py:19] end gpu_group_list cost 0.0001766681671142578 seconds
DEBUG 01-15 16:10:52.066754.066754 cuda_h.py:10] start experts_func_mgpu_group_pad
DEBUG 01-15 16:10:52.066770.066770 cuda_h.py:19] end group_tensors cost 0.006146669387817383 seconds
DEBUG 01-15 16:10:52.067003.067003 cuda_h.py:10] start group pad
DEBUG 01-15 16:10:52.067494.067494 cuda_h.py:19] end experts_func_mgpu_group_pad cost 0.0008575916290283203 seconds
DEBUG 01-15 16:10:52.068523.068523 cuda_h.py:10] start gpu_group_list
DEBUG 01-15 16:10:52.068510.068510 cuda_h.py:19] end gpu_group_list cost 0.00036215782165527344 seconds
DEBUG 01-15 16:10:52.069825.069825 cuda_h.py:10] start wait_experts_multi_device
INFO 01-15 16:10:52.069503.069503 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1cb712ad-c94a-49f0-befe-7eebd9ac52d1
DEBUG 01-15 16:10:52.071357.071357 cuda_h.py:19] end group pad cost 0.004169940948486328 seconds
DEBUG 01-15 16:10:52.071392.071392 cuda_h.py:10] start group_einsum
INFO 01-15 16:10:52.089001.089001 client.py:127] Model loaded
DEBUG 01-15 16:10:52.089326.089326 cuda_h.py:19] end wait_experts_multi_device cost 0.020392417907714844 seconds
DEBUG 01-15 16:10:52.089486.089486 cuda_h.py:10] start cpu_thread_manager_mp_wait
DEBUG 01-15 16:10:52.091630.091630 cuda_h.py:19] end group_einsum cost 0.01947760581970215 seconds
DEBUG 01-15 16:10:52.091283.091283 cuda_h.py:10] start get_outputs_cpu1
DEBUG 01-15 16:10:52.095780.095780 cuda_h.py:19] end get_outputs_cpu1 cost 0.004078865051269531 seconds
DEBUG 01-15 16:10:52.096504.096504 cuda_h.py:19] end experts_func_gpu_einsum_mp cost 0.0369868278503418 seconds
DEBUG 01-15 16:10:52.096033.096033 cuda_h.py:19] end cpu_thread_manager_mp_wait cost 0.006777286529541016 seconds
DEBUG 01-15 16:10:52.096565.096565 cuda_h.py:10] start cpuoutputsdeal
DEBUG 01-15 16:10:52.097643.097643 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:52.098542.098542 cuda_h.py:19] end index_scatter cost 7.081031799316406e-05 seconds
DEBUG 01-15 16:10:52.098949.098949 cuda_h.py:19] end cpuoutputsdeal cost 0.0015442371368408203 seconds
DEBUG 01-15 16:10:52.098959.098959 cuda_h.py:10] start gpu_group_einsum_mp_multi_list
DEBUG 01-15 16:10:52.098860.098860 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:52.098561.098561 cuda_h.py:19] end gpu_group_tensor cost 0.0001354217529296875 seconds
DEBUG 01-15 16:10:52.098655.098655 cuda_h.py:10] start gpu_group_tensor
DEBUG 01-15 16:10:52.098097.098097 cuda_h.py:19] end gpu_group_tensor cost 0.00012111663818359375 seconds
DEBUG 01-15 16:10:52.098948.098948 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:52.099281.099281 cuda_h.py:19] end gpu_group_einsum cost 0.0005826950073242188 seconds
DEBUG 01-15 16:10:52.099815.099815 cuda_h.py:10] start gpu_group_einsum
DEBUG 01-15 16:10:52.100819.100819 cuda_h.py:19] end gpu_group_einsum cost 0.00043654441833496094 seconds
DEBUG 01-15 16:10:52.100054.100054 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:52.100409.100409 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:52.100310.100310 cuda_h.py:19] end all_expert_outputs_slices cost 0.00016641616821289062 seconds
DEBUG 01-15 16:10:52.100020.100020 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:52.100890.100890 cuda_h.py:19] end concat_expert_out cost 4.887580871582031e-05 seconds
DEBUG 01-15 16:10:52.100256.100256 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:52.100610.100610 cuda_h.py:19] end index_scatter cost 5.1021575927734375e-05 seconds
DEBUG 01-15 16:10:52.101790.101790 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.0007789134979248047 seconds
DEBUG 01-15 16:10:52.101104.101104 cuda_h.py:10] start gpu_final_hidden_states_scatter
DEBUG 01-15 16:10:52.101709.101709 cuda_h.py:10] start all_expert_outputs_slices
DEBUG 01-15 16:10:52.101118.101118 cuda_h.py:19] end all_expert_outputs_slices cost 0.00013113021850585938 seconds
DEBUG 01-15 16:10:52.101921.101921 cuda_h.py:10] start concat_expert_out
DEBUG 01-15 16:10:52.101559.101559 cuda_h.py:19] end concat_expert_out cost 5.435943603515625e-05 seconds
DEBUG 01-15 16:10:52.101303.101303 cuda_h.py:10] start index_scatter
DEBUG 01-15 16:10:52.101081.101081 cuda_h.py:19] end index_scatter cost 4.9114227294921875e-05 seconds
DEBUG 01-15 16:10:52.101890.101890 cuda_h.py:19] end gpu_final_hidden_states_scatter cost 0.00047397613525390625 seconds
DEBUG 01-15 16:10:52.101985.101985 cuda_h.py:19] end gpu_experts_multi_device cost 0.03677821159362793 seconds
DEBUG 01-15 16:10:52.101709.101709 cuda_h.py:19] end layer_moe_generate_mp_multi_device_l_28 cost 0.047129154205322266 seconds
DEBUG 01-15 16:10:52.102292.102292 cuda_h.py:19] end prefill_layer cost 0.05110764503479004 seconds
DEBUG 01-15 16:10:52.102790.102790 lmp.py:1553] -------------------------------- end prefill layer 27 --------------------------------
DEBUG 01-15 16:10:52.102354.102354 cuda_h.py:19] end prefill cost 1.5999374389648438 seconds
Collecting data...
Generating '/tmp/nsys-report-d33a.qdstrm'

[1/1] [0%                          ] report1.nsys-rep
[1/1] [0%                          ] report1.nsys-rep
[1/1] [0%                          ] report1.nsys-rep
[1/1] [5%                          ] report1.nsys-rep
[1/1] [8%                          ] report1.nsys-rep
[1/1] [11%                         ] report1.nsys-rep
[1/1] [14%                         ] report1.nsys-rep
[1/1] [=17%                        ] report1.nsys-rep
[1/1] [==20%                       ] report1.nsys-rep
[1/1] [===23%                      ] report1.nsys-rep
[1/1] [====26%                     ] report1.nsys-rep
[1/1] [=====30%                    ] report1.nsys-rep
[1/1] [======33%                   ] report1.nsys-rep
[1/1] [=======37%                  ] report1.nsys-rep
[1/1] [========40%                 ] report1.nsys-rep
[1/1] [=========44%                ] report1.nsys-rep
[1/1] [==========47%               ] report1.nsys-rep
[1/1] [===========50%              ] report1.nsys-rep
[1/1] [========================100%] report1.nsys-rep
[1/1] [========================100%] report1.nsys-rep
Generated:
	/mnt/zhengcf3/lmp/examples/report1.nsys-rep
