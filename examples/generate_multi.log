here pin
INFO 12-24 11:28:03.295088.295088 pinpool.py:28] Initializing PinnedMemoryPool with 2GB total, allocating in 1024MB chunks...
DEBUG 12-24 11:28:04.125529.125529 pinpool.py:40] Allocated chunk 1: 536870912 elements (1024.0 MB)
DEBUG 12-24 11:28:04.561485.561485 pinpool.py:40] Allocated chunk 2: 536870912 elements (1024.0 MB)
INFO 12-24 11:28:04.561346.561346 pinpool.py:52] Successfully allocated 2 chunks, total 1073741824 elements (2048.0 MB) in 1.266s
DEBUG 12-24 11:28:05.384668.384668 cuda_memory_view.py:162] 
DEBUG 12-24 11:28:05.384668.384668 cuda_memory_view.py:162] restore_tensors_from_shared_memory_names time: 0.013347148895263672
DEBUG 12-24 11:28:07.915838.915838 mlpmodule.py:140] restore_hm_state_dict2model loaded 5265 expert tensors (including shared_experts) for Deepseek model
DEBUG 12-24 11:28:07.928124.928124 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:07.928555.928555 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:08.059092.059092 cuda_h.py:19] end allocate_cuda_memory cost 0.13066959381103516 seconds
DEBUG 12-24 11:28:08.059788.059788 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:08.059823.059823 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:08.059175.059175 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:08.059885.059885 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 52c8ae4d-72ba-40dd-aba7-987b1d84dd04
DEBUG 12-24 11:28:08.059498.059498 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:08.061945.061945 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 52c8ae4d-72ba-40dd-aba7-987b1d84dd04
DEBUG 12-24 11:28:08.061285.061285 cuda_h.py:19] end load_into_gpu_async cost 0.002143383026123047 seconds
DEBUG 12-24 11:28:08.061749.061749 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:08.061763.061763 cuda_h.py:19] end restore_tensors2 cost 0.00019550323486328125 seconds
DEBUG 12-24 11:28:08.062572.062572 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.1335587501525879 seconds
INFO 12-24 11:28:10.636653.636653 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 52c8ae4d-72ba-40dd-aba7-987b1d84dd04
INFO 12-24 11:28:10.637093.637093 client.py:127] Model loaded
DEBUG 12-24 11:28:10.640267.640267 cuda_h.py:10] start generate_input_ids
generate input ids cost 0.11079001426696777 s
DEBUG 12-24 11:28:11.046048.046048 cuda_h.py:19] end generate_input_ids cost 0.40592503547668457 seconds
DEBUG 12-24 11:28:11.046887.046887 cuda_h.py:10] start init_cache
DEBUG 12-24 11:28:11.049691.049691 cuda_h.py:19] end init_cache cost 0.0026171207427978516 seconds
DEBUG 12-24 11:28:11.049615.049615 cuda_h.py:10] start load_qkvogns_weight_l_0
DEBUG 12-24 11:28:11.049828.049828 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:11.049697.049697 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:11.049819.049819 cuda_h.py:19] end allocate_cuda_memory cost 0.00021958351135253906 seconds
DEBUG 12-24 11:28:11.049848.049848 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:11.049419.049419 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:11.049295.049295 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:11.049759.049759 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, fefa464a-1d85-40de-af68-daa152433448
DEBUG 12-24 11:28:11.049518.049518 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:11.051165.051165 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, fefa464a-1d85-40de-af68-daa152433448
DEBUG 12-24 11:28:11.051505.051505 cuda_h.py:19] end load_into_gpu_async cost 0.0017845630645751953 seconds
DEBUG 12-24 11:28:11.051228.051228 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:11.051304.051304 cuda_h.py:19] end restore_tensors2 cost 6.556510925292969e-05 seconds
DEBUG 12-24 11:28:11.051153.051153 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002356290817260742 seconds
INFO 12-24 11:28:11.051240.051240 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, fefa464a-1d85-40de-af68-daa152433448
INFO 12-24 11:28:11.056837.056837 client.py:127] Model loaded
DEBUG 12-24 11:28:11.056319.056319 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.0073664188385009766 seconds
DEBUG 12-24 11:28:11.056142.056142 cuda_h.py:10] start warm_up
DEBUG 12-24 11:28:11.056442.056442 cuda_h.py:10] start warm_up_gate
DEBUG 12-24 11:28:11.163485.163485 cuda_h.py:19] end warm_up_gate cost 0.10646224021911621 seconds
DEBUG 12-24 11:28:11.163417.163417 cuda_h.py:10] start warm_up_cpu
DEBUG 12-24 11:28:11.177850.177850 mlpmodule.py:630] group tensors cost 0.013178348541259766 s
DEBUG 12-24 11:28:11.199986.199986 mlpmodule.py:668] pad cost 0.021450042724609375 s
DEBUG 12-24 11:28:11.199507.199507 mlpmodule.py:674] create cpu tensor cost 6.985664367675781e-05 s
DEBUG 12-24 11:28:11.199914.199914 mlpmodule.py:679] move to cpu cost 3.790855407714844e-05 s
DEBUG 12-24 11:28:11.248087.248087 mlpmodule.py:694] group_w3: shape=torch.Size([6, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=17301504
DEBUG 12-24 11:28:11.248262.248262 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:11.259052.259052 mlpmodule.py:700] group_w3 first element: -0.0107421875
WARNING 12-24 11:28:11.260819.260819 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:11.272900.272900 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:11.275874.275874 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:11.288343.288343 mlpmodule.py:723] group einsum cost 0.08868074417114258 s
DEBUG 12-24 11:28:11.290418.290418 mlpmodule.py:731] cpy2cputensor cost 0.0015077590942382812 s
DEBUG 12-24 11:28:11.312110.312110 cuda_h.py:19] end warm_up_cpu cost 0.14855051040649414 seconds
DEBUG 12-24 11:28:11.312041.312041 cuda_h.py:10] start warm_up_gpu
DEBUG 12-24 11:28:11.325967.325967 mlpmodule.py:588]  experts func einsum cost 0.16194391250610352 s
DEBUG 12-24 11:28:11.505702.505702 cuda_h.py:19] end warm_up_gpu cost 0.19351410865783691 seconds
DEBUG 12-24 11:28:11.576812.576812 cuda_h.py:10] start warm_up_gate
DEBUG 12-24 11:28:11.577048.577048 cuda_h.py:19] end warm_up_gate cost 0.0007522106170654297 seconds
DEBUG 12-24 11:28:11.577322.577322 cuda_h.py:10] start warm_up_cpu
DEBUG 12-24 11:28:11.589200.589200 mlpmodule.py:630] group tensors cost 0.01212763786315918 s
DEBUG 12-24 11:28:11.591746.591746 mlpmodule.py:668] pad cost 0.0014960765838623047 s
DEBUG 12-24 11:28:11.592782.592782 mlpmodule.py:674] create cpu tensor cost 4.00543212890625e-05 s
DEBUG 12-24 11:28:11.592771.592771 mlpmodule.py:679] move to cpu cost 2.8371810913085938e-05 s
DEBUG 12-24 11:28:11.658545.658545 mlpmodule.py:694] group_w3: shape=torch.Size([19, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=54788096
DEBUG 12-24 11:28:11.658076.658076 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:11.658260.658260 mlpmodule.py:700] group_w3 first element: -0.0107421875
WARNING 12-24 11:28:11.658907.658907 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:11.705820.705820 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:11.711088.711088 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:11.743662.743662 mlpmodule.py:723] group einsum cost 0.15092968940734863 s
DEBUG 12-24 11:28:11.749338.749338 mlpmodule.py:731] cpy2cputensor cost 0.006177186965942383 s
DEBUG 12-24 11:28:11.753439.753439 cuda_h.py:19] end warm_up_cpu cost 0.1756749153137207 seconds
DEBUG 12-24 11:28:11.753873.753873 cuda_h.py:10] start warm_up_gpu
DEBUG 12-24 11:28:11.753361.753361 cuda_h.py:19] end warm_up_gpu cost 0.00048613548278808594 seconds
DEBUG 12-24 11:28:11.754506.754506 cuda_h.py:10] start warm_up_gate
DEBUG 12-24 11:28:11.759244.759244 cuda_h.py:19] end warm_up_gate cost 0.004734516143798828 seconds
DEBUG 12-24 11:28:11.759596.759596 cuda_h.py:10] start warm_up_cpu
DEBUG 12-24 11:28:11.790419.790419 mlpmodule.py:588]  experts func einsum cost 0.21292448043823242 s
DEBUG 12-24 11:28:11.794856.794856 mlpmodule.py:630] group tensors cost 0.0038068294525146484 s
DEBUG 12-24 11:28:11.795597.795597 mlpmodule.py:668] pad cost 0.0004374980926513672 s
DEBUG 12-24 11:28:11.795932.795932 mlpmodule.py:674] create cpu tensor cost 4.1484832763671875e-05 s
DEBUG 12-24 11:28:11.795497.795497 mlpmodule.py:679] move to cpu cost 3.147125244140625e-05 s
DEBUG 12-24 11:28:11.815617.815617 mlpmodule.py:694] group_w3: shape=torch.Size([6, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=17301504
DEBUG 12-24 11:28:11.816894.816894 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:11.816182.816182 mlpmodule.py:700] group_w3 first element: -0.0107421875
WARNING 12-24 11:28:11.816875.816875 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:11.826258.826258 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:11.828003.828003 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:11.839193.839193 mlpmodule.py:723] group einsum cost 0.04404401779174805 s
DEBUG 12-24 11:28:11.841581.841581 mlpmodule.py:731] cpy2cputensor cost 0.0014491081237792969 s
DEBUG 12-24 11:28:11.842388.842388 cuda_h.py:19] end warm_up_cpu cost 0.08328771591186523 seconds
DEBUG 12-24 11:28:11.843571.843571 cuda_h.py:10] start warm_up_gpu
DEBUG 12-24 11:28:11.843418.843418 cuda_h.py:19] end warm_up_gpu cost 0.0005202293395996094 seconds
DEBUG 12-24 11:28:11.844861.844861 cuda_h.py:19] end warm_up cost 0.7876384258270264 seconds
DEBUG 12-24 11:28:11.844488.844488 cuda_h.py:10] start multi_layer
DEBUG 12-24 11:28:11.844219.844219 lmp.py:418] -------------------------------- start layer 0 --------------------------------
DEBUG 12-24 11:28:11.844949.844949 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:11.844231.844231 cuda_h.py:10] start self_attn
DEBUG 12-24 11:28:11.858418.858418 mlpmodule.py:588]  experts func einsum cost 0.06807541847229004 s
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:11.884108.884108 cuda_h.py:19] end self_attn cost 0.03954911231994629 seconds
DEBUG 12-24 11:28:11.884127.884127 cuda_h.py:19] end iln_self_attn_paln cost 0.04025435447692871 seconds
DEBUG 12-24 11:28:11.885705.885705 cuda_h.py:10] start dense_mlp
DEBUG 12-24 11:28:11.885720.885720 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1
DEBUG 12-24 11:28:11.885424.885424 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 3.266334533691406e-05 seconds
DEBUG 12-24 11:28:11.885110.885110 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:11.885419.885419 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:11.885852.885852 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:11.885189.885189 cuda_h.py:19] end allocate_cuda_memory cost 0.00034356117248535156 seconds
DEBUG 12-24 11:28:11.885126.885126 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:11.885380.885380 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:11.886885.886885 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:11.886979.886979 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 3f43e1b0-4542-46be-865a-d4a507306294
DEBUG 12-24 11:28:11.886260.886260 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:11.887691.887691 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 3f43e1b0-4542-46be-865a-d4a507306294
DEBUG 12-24 11:28:11.887210.887210 cuda_h.py:19] end load_into_gpu_async cost 0.0014545917510986328 seconds
DEBUG 12-24 11:28:11.887443.887443 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:11.887261.887261 cuda_h.py:19] end restore_tensors2 cost 7.939338684082031e-05 seconds
DEBUG 12-24 11:28:11.887500.887500 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0021910667419433594 seconds
INFO 12-24 11:28:11.888828.888828 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 3f43e1b0-4542-46be-865a-d4a507306294
INFO 12-24 11:28:11.894397.894397 client.py:127] Model loaded
DEBUG 12-24 11:28:11.895724.895724 cuda_h.py:19] end sllm_worker_task cost 0.009713411331176758 seconds
DEBUG 12-24 11:28:11.895608.895608 cuda_h.py:19] end dense_mlp cost 0.01009821891784668 seconds
DEBUG 12-24 11:28:11.895625.895625 lmp.py:445] -------------------------------- end layer 0 --------------------------------
DEBUG 12-24 11:28:11.895388.895388 lmp.py:418] -------------------------------- start layer 1 --------------------------------
DEBUG 12-24 11:28:11.895799.895799 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:11.895306.895306 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:11.898776.898776 cuda_h.py:19] end self_attn cost 0.0024907588958740234 seconds
DEBUG 12-24 11:28:11.898416.898416 cuda_h.py:19] end iln_self_attn_paln cost 0.003144502639770508 seconds
DEBUG 12-24 11:28:11.898233.898233 cuda_h.py:10] start layer_moe_generate_1
DEBUG 12-24 11:28:11.898394.898394 cuda_h.py:10] start gate
DEBUG 12-24 11:28:11.911353.911353 cuda_h.py:19] end gate cost 0.012776613235473633 seconds
DEBUG 12-24 11:28:11.911951.911951 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:11.911086.911086 lmp.py:519] 
DEBUG 12-24 11:28:11.911086.911086 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:11.911511.911511 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:11.911684.911684 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:11.911803.911803 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:11.911254.911254 lmp.py:523] 
DEBUG 12-24 11:28:11.911254.911254 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:11.911441.911441 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:11.911567.911567 lmp.py:530]   Expert 62 |     84 | CPU
DEBUG 12-24 11:28:11.911210.911210 lmp.py:530]   Expert 22 |     96 | CPU
DEBUG 12-24 11:28:11.911900.911900 lmp.py:530]   Expert 32 |    106 | CPU
DEBUG 12-24 11:28:11.911351.911351 lmp.py:530]   Expert 17 |    117 | CPU
DEBUG 12-24 11:28:11.912801.912801 lmp.py:530]   Expert 38 |    117 | CPU
DEBUG 12-24 11:28:11.912252.912252 lmp.py:530]   Expert 52 |    120 | CPU
DEBUG 12-24 11:28:11.912465.912465 lmp.py:530]   Expert 18 |    127 | CPU
DEBUG 12-24 11:28:11.912916.912916 lmp.py:530]   Expert  3 |    135 | CPU
DEBUG 12-24 11:28:11.912367.912367 lmp.py:530]   Expert 41 |    135 | CPU
DEBUG 12-24 11:28:11.912030.912030 lmp.py:530]   Expert 54 |    135 | CPU
DEBUG 12-24 11:28:11.912958.912958 lmp.py:530]   Expert 27 |    136 | CPU
DEBUG 12-24 11:28:11.912647.912647 lmp.py:530]   Expert 28 |    140 | CPU
DEBUG 12-24 11:28:11.912813.912813 lmp.py:530]   Expert 47 |    141 | CPU
DEBUG 12-24 11:28:11.912933.912933 lmp.py:530]   Expert 25 |    142 | CPU
DEBUG 12-24 11:28:11.912814.912814 lmp.py:530]   Expert 13 |    144 | CPU
DEBUG 12-24 11:28:11.912934.912934 lmp.py:530]   Expert 21 |    147 | CPU
DEBUG 12-24 11:28:11.912815.912815 lmp.py:530]   Expert 37 |    147 | CPU
DEBUG 12-24 11:28:11.912982.912982 lmp.py:530]   Expert 29 |    149 | CPU
DEBUG 12-24 11:28:11.912194.912194 lmp.py:530]   Expert 39 |    150 | CPU
DEBUG 12-24 11:28:11.912883.912883 lmp.py:530]   Expert 58 |    150 | CPU
DEBUG 12-24 11:28:11.912573.912573 lmp.py:530]   Expert 33 |    161 | CPU
DEBUG 12-24 11:28:11.912262.912262 lmp.py:530]   Expert 49 |    161 | CPU
DEBUG 12-24 11:28:11.912951.912951 lmp.py:530]   Expert 11 |    165 | CPU
DEBUG 12-24 11:28:11.912402.912402 lmp.py:530]   Expert 24 |    168 | CPU
DEBUG 12-24 11:28:11.912091.912091 lmp.py:530]   Expert 30 |    168 | CPU
DEBUG 12-24 11:28:11.912258.912258 lmp.py:530]   Expert 31 |    171 | CPU
DEBUG 12-24 11:28:11.912291.912291 lmp.py:530]   Expert 34 |    172 | CPU
DEBUG 12-24 11:28:11.912219.912219 lmp.py:530]   Expert 45 |    175 | CPU
DEBUG 12-24 11:28:11.912147.912147 lmp.py:530]   Expert 50 |    175 | CPU
DEBUG 12-24 11:28:11.912313.912313 lmp.py:530]   Expert 48 |    176 | CPU
DEBUG 12-24 11:28:11.912241.912241 lmp.py:530]   Expert 56 |    178 | CPU
DEBUG 12-24 11:28:11.912553.912553 lmp.py:530]   Expert 53 |    179 | CPU
DEBUG 12-24 11:28:11.912911.912911 lmp.py:530]   Expert  6 |    183 | GPU
DEBUG 12-24 11:28:11.912031.912031 lmp.py:530]   Expert 36 |    183 | GPU
DEBUG 12-24 11:28:11.912720.912720 lmp.py:530]   Expert  4 |    184 | GPU
DEBUG 12-24 11:28:11.912648.912648 lmp.py:530]   Expert 19 |    185 | GPU
DEBUG 12-24 11:28:11.912337.912337 lmp.py:530]   Expert  0 |    188 | GPU
DEBUG 12-24 11:28:11.912265.912265 lmp.py:530]   Expert 10 |    192 | GPU
DEBUG 12-24 11:28:11.912716.912716 lmp.py:530]   Expert 57 |    192 | GPU
DEBUG 12-24 11:28:11.912405.912405 lmp.py:530]   Expert 35 |    193 | GPU
DEBUG 12-24 11:28:11.912094.912094 lmp.py:530]   Expert 55 |    196 | GPU
DEBUG 12-24 11:28:11.912545.912545 lmp.py:530]   Expert  2 |    197 | GPU
DEBUG 12-24 11:28:11.912996.912996 lmp.py:530]   Expert 12 |    203 | GPU
DEBUG 12-24 11:28:11.912685.912685 lmp.py:530]   Expert 61 |    205 | GPU
DEBUG 12-24 11:28:11.912136.912136 lmp.py:530]   Expert 15 |    206 | GPU
DEBUG 12-24 11:28:11.912587.912587 lmp.py:530]   Expert  5 |    213 | GPU
DEBUG 12-24 11:28:11.912276.912276 lmp.py:530]   Expert 43 |    215 | GPU
DEBUG 12-24 11:28:11.912966.912966 lmp.py:530]   Expert 44 |    219 | GPU
DEBUG 12-24 11:28:11.912655.912655 lmp.py:530]   Expert 60 |    221 | GPU
DEBUG 12-24 11:28:11.912298.912298 lmp.py:530]   Expert 63 |    222 | GPU
DEBUG 12-24 11:28:11.912418.912418 lmp.py:530]   Expert 23 |    223 | GPU
DEBUG 12-24 11:28:11.912345.912345 lmp.py:530]   Expert 51 |    235 | GPU
DEBUG 12-24 11:28:11.912988.912988 lmp.py:530]   Expert  9 |    242 | GPU
DEBUG 12-24 11:28:11.912678.912678 lmp.py:530]   Expert 20 |    249 | GPU
DEBUG 12-24 11:28:11.912367.912367 lmp.py:530]   Expert  1 |    267 | GPU
DEBUG 12-24 11:28:11.912580.912580 lmp.py:530]   Expert 40 |    277 | GPU
DEBUG 12-24 11:28:11.912269.912269 lmp.py:530]   Expert  8 |    284 | GPU
DEBUG 12-24 11:28:11.912720.912720 lmp.py:530]   Expert 26 |    286 | GPU
DEBUG 12-24 11:28:11.912409.912409 lmp.py:530]   Expert 59 |    291 | GPU
DEBUG 12-24 11:28:11.912621.912621 lmp.py:530]   Expert  7 |    312 | GPU
DEBUG 12-24 11:28:11.912072.912072 lmp.py:530]   Expert 42 |    321 | GPU
DEBUG 12-24 11:28:11.912000.912000 lmp.py:530]   Expert 14 |    330 | GPU
DEBUG 12-24 11:28:11.913643.913643 lmp.py:530]   Expert 16 |    335 | GPU
DEBUG 12-24 11:28:11.913670.913670 lmp.py:530]   Expert 46 |    372 | GPU
DEBUG 12-24 11:28:11.913744.913744 lmp.py:531] 
DEBUG 12-24 11:28:11.913744.913744 lmp.py:531]   CPU total tokens: 4667 (38.0%)
DEBUG 12-24 11:28:11.913817.913817 lmp.py:532]   GPU total tokens: 7621 (62.0%)
DEBUG 12-24 11:28:11.913467.913467 cuda_h.py:19] end experts_map_get cost 0.0015459060668945312 seconds
DEBUG 12-24 11:28:11.913110.913110 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:11.913536.913536 lmp.py:541] 
DEBUG 12-24 11:28:11.913536.913536 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:11.913372.913372 cuda_h.py:19] end cpu_experts_submit cost 0.0001010894775390625 seconds
DEBUG 12-24 11:28:11.913068.913068 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:11.913183.913183 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:11.913796.913796 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:11.913163.913163 cuda_h.py:19] end allocate_cuda_memory cost 0.00023436546325683594 seconds
DEBUG 12-24 11:28:11.913536.913536 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:11.913246.913246 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:11.913400.913400 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:11.913864.913864 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 83ddcec6-1c66-48e2-8530-42e5a8517d43
DEBUG 12-24 11:28:11.914453.914453 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:11.933744.933744 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 83ddcec6-1c66-48e2-8530-42e5a8517d43
DEBUG 12-24 11:28:11.933211.933211 mlpmodule.py:630] group tensors cost 0.018847942352294922 s
DEBUG 12-24 11:28:11.933311.933311 cuda_h.py:19] end load_into_gpu_async cost 0.019950389862060547 seconds
DEBUG 12-24 11:28:11.933885.933885 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:11.934447.934447 cuda_h.py:19] end restore_tensors2 cost 0.00034046173095703125 seconds
DEBUG 12-24 11:28:11.934283.934283 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.020968914031982422 seconds
DEBUG 12-24 11:28:11.939090.939090 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.026310205459594727 seconds
DEBUG 12-24 11:28:11.939591.939591 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 12-24 11:28:11.939720.939720 cuda_h.py:10] start start_load_qkvogn_s_weight_l_2
DEBUG 12-24 11:28:11.939955.939955 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 3.4809112548828125e-05 seconds
DEBUG 12-24 11:28:11.940773.940773 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:11.940796.940796 cuda_h.py:19] end start_load_qkvogn_s_weight_l_2 cost 0.00023484230041503906 seconds
DEBUG 12-24 11:28:11.940617.940617 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:11.940044.940044 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:11.940381.940381 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:11.941014.941014 cuda_h.py:19] end allocate_cuda_memory cost 0.0004138946533203125 seconds
DEBUG 12-24 11:28:11.941359.941359 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:11.941608.941608 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:11.941031.941031 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:11.941776.941776 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 14d6cf15-2d67-4a1b-9aa2-061b1dd05192
DEBUG 12-24 11:28:11.942982.942982 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:11.942552.942552 cuda_h.py:19] end gpu_sexperts cost 0.001817464828491211 seconds
DEBUG 12-24 11:28:11.942681.942681 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:11.942764.942764 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 83ddcec6-1c66-48e2-8530-42e5a8517d43
INFO 12-24 11:28:11.943733.943733 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 14d6cf15-2d67-4a1b-9aa2-061b1dd05192
DEBUG 12-24 11:28:11.943599.943599 cuda_h.py:19] end load_into_gpu_async cost 0.0017094612121582031 seconds
DEBUG 12-24 11:28:11.943279.943279 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:11.943147.943147 cuda_h.py:19] end restore_tensors2 cost 0.00012731552124023438 seconds
DEBUG 12-24 11:28:11.943661.943661 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003266572952270508 seconds
INFO 12-24 11:28:11.944065.944065 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 14d6cf15-2d67-4a1b-9aa2-061b1dd05192
DEBUG 12-24 11:28:11.945178.945178 mlpmodule.py:668] pad cost 0.011580705642700195 s
DEBUG 12-24 11:28:11.945149.945149 mlpmodule.py:674] create cpu tensor cost 5.7220458984375e-05 s
DEBUG 12-24 11:28:11.945575.945575 mlpmodule.py:679] move to cpu cost 2.9802322387695312e-05 s
DEBUG 12-24 11:28:11.957892.957892 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:11.958716.958716 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:11.958826.958826 mlpmodule.py:700] group_w3 first element: 0.01348876953125
WARNING 12-24 11:28:11.958598.958598 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:11.969408.969408 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:11.970321.970321 mlpmodule.py:719] start einsum3
INFO 12-24 11:28:11.977791.977791 client.py:127] Model loaded
DEBUG 12-24 11:28:11.978352.978352 cuda_h.py:19] end wait_experts cost 0.03548312187194824 seconds
DEBUG 12-24 11:28:11.978701.978701 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:11.978533.978533 lmp.py:585]   Computing 32 experts on GPU...
INFO 12-24 11:28:11.980886.980886 client.py:127] Model loaded
DEBUG 12-24 11:28:11.980924.980924 cuda_h.py:19] end sllm_worker_task cost 0.04005241394042969 seconds
DEBUG 12-24 11:28:11.980003.980003 mlpmodule.py:723] group einsum cost 0.03485107421875 s
DEBUG 12-24 11:28:11.981060.981060 mlpmodule.py:731] cpy2cputensor cost 0.0006852149963378906 s
DEBUG 12-24 11:28:11.982324.982324 mlpmodule.py:457] gpu group tensors cost 0.003523588180541992 s
DEBUG 12-24 11:28:11.984232.984232 mlpmodule.py:490] gpu pad cost 0.0022916793823242188 s
DEBUG 12-24 11:28:11.985974.985974 mlpmodule.py:508] gpu group einsum cost 0.00070953369140625 s
DEBUG 12-24 11:28:11.989789.989789 mlpmodule.py:537] gpu experts func einsum cost 0.010895967483520508 s
DEBUG 12-24 11:28:11.989138.989138 cuda_h.py:19] end gpu_experts cost 0.011370658874511719 seconds
DEBUG 12-24 11:28:11.989768.989768 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:11.989042.989042 cuda_h.py:19] end wait_cetm_experts cost 2.09808349609375e-05 seconds
DEBUG 12-24 11:28:11.990897.990897 lmp.py:615] gpu end - einsum end = 0.8ms
DEBUG 12-24 11:28:11.990913.990913 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:11.990717.990717 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.7179718017578125e-05 seconds
DEBUG 12-24 11:28:11.990864.990864 cuda_h.py:19] end layer_moe_generate_1 cost 0.09157776832580566 seconds
DEBUG 12-24 11:28:11.990544.990544 lmp.py:445] -------------------------------- end layer 1 --------------------------------
DEBUG 12-24 11:28:11.990599.990599 lmp.py:418] -------------------------------- start layer 2 --------------------------------
DEBUG 12-24 11:28:11.990162.990162 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:11.990405.990405 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:11.993566.993566 cuda_h.py:19] end self_attn cost 0.0023775100708007812 seconds
DEBUG 12-24 11:28:11.993721.993721 cuda_h.py:19] end iln_self_attn_paln cost 0.003012418746948242 seconds
DEBUG 12-24 11:28:11.993101.993101 cuda_h.py:10] start layer_moe_generate_2
DEBUG 12-24 11:28:11.993878.993878 cuda_h.py:10] start gate
DEBUG 12-24 11:28:11.994734.994734 cuda_h.py:19] end gate cost 0.0006296634674072266 seconds
DEBUG 12-24 11:28:11.994279.994279 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:11.994978.994978 lmp.py:519] 
DEBUG 12-24 11:28:11.994978.994978 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:11.994363.994363 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:11.994159.994159 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:11.994855.994855 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:11.994929.994929 lmp.py:523] 
DEBUG 12-24 11:28:11.994929.994929 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:11.994240.994240 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:11.994513.994513 lmp.py:530]   Expert  8 |     71 | CPU
DEBUG 12-24 11:28:11.994732.994732 lmp.py:530]   Expert 34 |     75 | CPU
DEBUG 12-24 11:28:11.994613.994613 lmp.py:530]   Expert 26 |     89 | CPU
DEBUG 12-24 11:28:11.994495.994495 lmp.py:530]   Expert 36 |     98 | CPU
DEBUG 12-24 11:28:11.994138.994138 lmp.py:530]   Expert 28 |    101 | CPU
DEBUG 12-24 11:28:11.994734.994734 lmp.py:530]   Expert 52 |    105 | CPU
DEBUG 12-24 11:28:11.994007.994007 lmp.py:530]   Expert 58 |    113 | CPU
DEBUG 12-24 11:28:11.994180.994180 lmp.py:530]   Expert  7 |    114 | CPU
DEBUG 12-24 11:28:11.994776.994776 lmp.py:530]   Expert  9 |    120 | CPU
DEBUG 12-24 11:28:11.994896.994896 lmp.py:530]   Expert 21 |    120 | CPU
DEBUG 12-24 11:28:11.994016.994016 lmp.py:530]   Expert 27 |    122 | CPU
DEBUG 12-24 11:28:11.994374.994374 lmp.py:530]   Expert 19 |    125 | CPU
DEBUG 12-24 11:28:11.994733.994733 lmp.py:530]   Expert  3 |    129 | CPU
DEBUG 12-24 11:28:11.995806.995806 lmp.py:530]   Expert  0 |    131 | CPU
DEBUG 12-24 11:28:11.995211.995211 lmp.py:530]   Expert 29 |    134 | CPU
DEBUG 12-24 11:28:11.995615.995615 lmp.py:530]   Expert  6 |    138 | CPU
DEBUG 12-24 11:28:11.995781.995781 lmp.py:530]   Expert 38 |    138 | CPU
DEBUG 12-24 11:28:11.995947.995947 lmp.py:530]   Expert 40 |    138 | CPU
DEBUG 12-24 11:28:11.995021.995021 lmp.py:530]   Expert 49 |    143 | CPU
DEBUG 12-24 11:28:11.995002.995002 lmp.py:530]   Expert  5 |    145 | CPU
DEBUG 12-24 11:28:11.995413.995413 lmp.py:530]   Expert 13 |    147 | CPU
DEBUG 12-24 11:28:11.995533.995533 lmp.py:530]   Expert  1 |    157 | CPU
DEBUG 12-24 11:28:11.995176.995176 lmp.py:530]   Expert 33 |    157 | CPU
DEBUG 12-24 11:28:11.995296.995296 lmp.py:530]   Expert 10 |    159 | CPU
DEBUG 12-24 11:28:11.995939.995939 lmp.py:530]   Expert 24 |    161 | CPU
DEBUG 12-24 11:28:11.995105.995105 lmp.py:530]   Expert 16 |    163 | CPU
DEBUG 12-24 11:28:11.995655.995655 lmp.py:530]   Expert 25 |    168 | CPU
DEBUG 12-24 11:28:11.995060.995060 lmp.py:530]   Expert 54 |    169 | CPU
DEBUG 12-24 11:28:11.995226.995226 lmp.py:530]   Expert 62 |    169 | CPU
DEBUG 12-24 11:28:11.995630.995630 lmp.py:530]   Expert 50 |    175 | CPU
DEBUG 12-24 11:28:11.995558.995558 lmp.py:530]   Expert 59 |    180 | CPU
DEBUG 12-24 11:28:11.995691.995691 lmp.py:530]   Expert 35 |    182 | CPU
DEBUG 12-24 11:28:11.995957.995957 lmp.py:530]   Expert 30 |    185 | GPU
DEBUG 12-24 11:28:11.995077.995077 lmp.py:530]   Expert 39 |    185 | GPU
DEBUG 12-24 11:28:11.995958.995958 lmp.py:530]   Expert 48 |    186 | GPU
DEBUG 12-24 11:28:11.995601.995601 lmp.py:530]   Expert 44 |    188 | GPU
DEBUG 12-24 11:28:11.995244.995244 lmp.py:530]   Expert 63 |    188 | GPU
DEBUG 12-24 11:28:11.995318.995318 lmp.py:530]   Expert 17 |    192 | GPU
DEBUG 12-24 11:28:11.995961.995961 lmp.py:530]   Expert 56 |    199 | GPU
DEBUG 12-24 11:28:11.995888.995888 lmp.py:530]   Expert 51 |    203 | GPU
DEBUG 12-24 11:28:11.995054.995054 lmp.py:530]   Expert  2 |    210 | GPU
DEBUG 12-24 11:28:11.995221.995221 lmp.py:530]   Expert 60 |    215 | GPU
DEBUG 12-24 11:28:11.995102.995102 lmp.py:530]   Expert 55 |    216 | GPU
DEBUG 12-24 11:28:11.995182.995182 lmp.py:530]   Expert 57 |    218 | GPU
DEBUG 12-24 11:28:11.995541.995541 lmp.py:530]   Expert 45 |    220 | GPU
DEBUG 12-24 11:28:11.995184.995184 lmp.py:530]   Expert 18 |    223 | GPU
DEBUG 12-24 11:28:11.995826.995826 lmp.py:530]   Expert 22 |    224 | GPU
DEBUG 12-24 11:28:11.995946.995946 lmp.py:530]   Expert  4 |    230 | GPU
DEBUG 12-24 11:28:11.995112.995112 lmp.py:530]   Expert 41 |    231 | GPU
DEBUG 12-24 11:28:11.995186.995186 lmp.py:530]   Expert 11 |    242 | GPU
DEBUG 12-24 11:28:11.995352.995352 lmp.py:530]   Expert 12 |    243 | GPU
DEBUG 12-24 11:28:11.995518.995518 lmp.py:530]   Expert 31 |    247 | GPU
DEBUG 12-24 11:28:11.995684.995684 lmp.py:530]   Expert 53 |    259 | GPU
DEBUG 12-24 11:28:11.995612.995612 lmp.py:530]   Expert 37 |    264 | GPU
DEBUG 12-24 11:28:11.995209.995209 lmp.py:530]   Expert 15 |    272 | GPU
DEBUG 12-24 11:28:11.995812.995812 lmp.py:530]   Expert 23 |    275 | GPU
DEBUG 12-24 11:28:11.995932.995932 lmp.py:530]   Expert 14 |    284 | GPU
DEBUG 12-24 11:28:11.995337.995337 lmp.py:530]   Expert 47 |    288 | GPU
DEBUG 12-24 11:28:11.995741.995741 lmp.py:530]   Expert 20 |    297 | GPU
DEBUG 12-24 11:28:11.995146.995146 lmp.py:530]   Expert 42 |    310 | GPU
DEBUG 12-24 11:28:11.995312.995312 lmp.py:530]   Expert 32 |    322 | GPU
DEBUG 12-24 11:28:11.995432.995432 lmp.py:530]   Expert 46 |    336 | GPU
DEBUG 12-24 11:28:11.995836.995836 lmp.py:530]   Expert 43 |    363 | GPU
DEBUG 12-24 11:28:11.995764.995764 lmp.py:530]   Expert 61 |    437 | GPU
DEBUG 12-24 11:28:11.995360.995360 lmp.py:531] 
DEBUG 12-24 11:28:11.995360.995360 lmp.py:531]   CPU total tokens: 4336 (35.3%)
DEBUG 12-24 11:28:11.995719.995719 lmp.py:532]   GPU total tokens: 7952 (64.7%)
DEBUG 12-24 11:28:11.995230.995230 cuda_h.py:19] end experts_map_get cost 0.0016508102416992188 seconds
DEBUG 12-24 11:28:11.996025.996025 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:11.996173.996173 lmp.py:541] 
DEBUG 12-24 11:28:11.996173.996173 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:11.996593.996593 cuda_h.py:19] end cpu_experts_submit cost 0.00011324882507324219 seconds
DEBUG 12-24 11:28:11.996719.996719 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:11.996085.996085 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:11.996607.996607 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:11.997003.997003 cuda_h.py:19] end allocate_cuda_memory cost 0.0015060901641845703 seconds
DEBUG 12-24 11:28:11.998172.998172 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:11.998789.998789 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:11.998896.998896 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:11.998268.998268 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, d96c2e88-8693-4d2c-83db-94fbf96b6b07
DEBUG 12-24 11:28:11.998440.998440 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:11.999031.999031 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, d96c2e88-8693-4d2c-83db-94fbf96b6b07
DEBUG 12-24 11:28:11.999451.999451 cuda_h.py:19] end load_into_gpu_async cost 0.0015745162963867188 seconds
DEBUG 12-24 11:28:11.999823.999823 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:11.999230.999230 cuda_h.py:19] end restore_tensors2 cost 0.0002694129943847656 seconds
DEBUG 12-24 11:28:12.000397.000397 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0037474632263183594 seconds
DEBUG 12-24 11:28:12.002078.002078 mlpmodule.py:588]  experts func einsum cost 0.08809256553649902 s
DEBUG 12-24 11:28:12.002827.002827 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.006748676300048828 seconds
DEBUG 12-24 11:28:12.013775.013775 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 12-24 11:28:12.013592.013592 mlpmodule.py:630] group tensors cost 0.010679483413696289 s
DEBUG 12-24 11:28:12.013852.013852 cuda_h.py:10] start start_load_qkvogn_s_weight_l_3
DEBUG 12-24 11:28:12.014173.014173 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 3.266334533691406e-05 seconds
DEBUG 12-24 11:28:12.014221.014221 cuda_h.py:19] end start_load_qkvogn_s_weight_l_3 cost 7.271766662597656e-05 seconds
DEBUG 12-24 11:28:12.014632.014632 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.014027.014027 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.014219.014219 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.015456.015456 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.015812.015812 cuda_h.py:19] end allocate_cuda_memory cost 0.0006957054138183594 seconds
DEBUG 12-24 11:28:12.015041.015041 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.015585.015585 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.015912.015912 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.016344.016344 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 690a7db3-457b-49b8-ad71-887b7930c16a
DEBUG 12-24 11:28:12.016891.016891 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.016115.016115 cuda_h.py:19] end gpu_sexperts cost 0.001924753189086914 seconds
DEBUG 12-24 11:28:12.016913.016913 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.016074.016074 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, d96c2e88-8693-4d2c-83db-94fbf96b6b07
INFO 12-24 11:28:12.017200.017200 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 690a7db3-457b-49b8-ad71-887b7930c16a
DEBUG 12-24 11:28:12.017409.017409 cuda_h.py:19] end load_into_gpu_async cost 0.0014491081237792969 seconds
DEBUG 12-24 11:28:12.017702.017702 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.017593.017593 cuda_h.py:19] end restore_tensors2 cost 8.940696716308594e-05 seconds
DEBUG 12-24 11:28:12.017317.017317 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002599477767944336 seconds
INFO 12-24 11:28:12.018143.018143 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 690a7db3-457b-49b8-ad71-887b7930c16a
DEBUG 12-24 11:28:12.019170.019170 mlpmodule.py:668] pad cost 0.004933357238769531 s
DEBUG 12-24 11:28:12.019996.019996 mlpmodule.py:674] create cpu tensor cost 5.8650970458984375e-05 s
DEBUG 12-24 11:28:12.019045.019045 mlpmodule.py:679] move to cpu cost 2.956390380859375e-05 s
DEBUG 12-24 11:28:12.031770.031770 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:12.031259.031259 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.031409.031409 mlpmodule.py:700] group_w3 first element: -0.0380859375
WARNING 12-24 11:28:12.031672.031672 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.040003.040003 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.041750.041750 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.052805.052805 mlpmodule.py:723] group einsum cost 0.03329586982727051 s
DEBUG 12-24 11:28:12.054457.054457 mlpmodule.py:731] cpy2cputensor cost 0.0008032321929931641 s
INFO 12-24 11:28:12.062886.062886 client.py:127] Model loaded
DEBUG 12-24 11:28:12.062733.062733 cuda_h.py:19] end wait_experts cost 0.04576706886291504 seconds
DEBUG 12-24 11:28:12.062595.062595 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.062457.062457 lmp.py:585]   Computing 32 experts on GPU...
INFO 12-24 11:28:12.062078.062078 client.py:127] Model loaded
DEBUG 12-24 11:28:12.063737.063737 cuda_h.py:19] end sllm_worker_task cost 0.048497915267944336 seconds
DEBUG 12-24 11:28:12.063289.063289 mlpmodule.py:457] gpu group tensors cost 0.0007557868957519531 s
DEBUG 12-24 11:28:12.065889.065889 mlpmodule.py:490] gpu pad cost 0.0018074512481689453 s
DEBUG 12-24 11:28:12.077487.077487 mlpmodule.py:588]  experts func einsum cost 0.0745687484741211 s
DEBUG 12-24 11:28:12.077851.077851 mlpmodule.py:508] gpu group einsum cost 0.012136697769165039 s
DEBUG 12-24 11:28:12.086287.086287 mlpmodule.py:537] gpu experts func einsum cost 0.02343463897705078 s
DEBUG 12-24 11:28:12.086155.086155 cuda_h.py:19] end gpu_experts cost 0.023997068405151367 seconds
DEBUG 12-24 11:28:12.086145.086145 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.086713.086713 cuda_h.py:19] end wait_cetm_experts cost 4.220008850097656e-05 seconds
DEBUG 12-24 11:28:12.086298.086298 lmp.py:615] gpu end - einsum end = 27.3ms
DEBUG 12-24 11:28:12.087019.087019 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.087587.087587 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 4.124641418457031e-05 seconds
DEBUG 12-24 11:28:12.087172.087172 cuda_h.py:19] end layer_moe_generate_2 cost 0.09365153312683105 seconds
DEBUG 12-24 11:28:12.087515.087515 lmp.py:445] -------------------------------- end layer 2 --------------------------------
DEBUG 12-24 11:28:12.087313.087313 lmp.py:418] -------------------------------- start layer 3 --------------------------------
DEBUG 12-24 11:28:12.087302.087302 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.088904.088904 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.090204.090204 cuda_h.py:19] end self_attn cost 0.002740144729614258 seconds
DEBUG 12-24 11:28:12.091336.091336 cuda_h.py:19] end iln_self_attn_paln cost 0.003527402877807617 seconds
DEBUG 12-24 11:28:12.091868.091868 cuda_h.py:10] start layer_moe_generate_3
DEBUG 12-24 11:28:12.091605.091605 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.092877.092877 cuda_h.py:19] end gate cost 0.0007491111755371094 seconds
DEBUG 12-24 11:28:12.092250.092250 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.092071.092071 lmp.py:519] 
DEBUG 12-24 11:28:12.092071.092071 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.092993.092993 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:12.092133.092133 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:12.092935.092935 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:12.092638.092638 lmp.py:523] 
DEBUG 12-24 11:28:12.092638.092638 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.092818.092818 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.092673.092673 lmp.py:530]   Expert 61 |     25 | CPU
DEBUG 12-24 11:28:12.092899.092899 lmp.py:530]   Expert  7 |     40 | CPU
DEBUG 12-24 11:28:12.092172.092172 lmp.py:530]   Expert 24 |     42 | CPU
DEBUG 12-24 11:28:12.092206.092206 lmp.py:530]   Expert 16 |     43 | CPU
DEBUG 12-24 11:28:12.093525.093525 lmp.py:530]   Expert 59 |     45 | CPU
DEBUG 12-24 11:28:12.093274.093274 lmp.py:530]   Expert 15 |     46 | CPU
DEBUG 12-24 11:28:12.093261.093261 lmp.py:530]   Expert 37 |     46 | CPU
DEBUG 12-24 11:28:12.093772.093772 lmp.py:530]   Expert 56 |     47 | CPU
DEBUG 12-24 11:28:12.093760.093760 lmp.py:530]   Expert  6 |     51 | CPU
DEBUG 12-24 11:28:12.093317.093317 lmp.py:530]   Expert 23 |     53 | CPU
DEBUG 12-24 11:28:12.093636.093636 lmp.py:530]   Expert 11 |     54 | CPU
DEBUG 12-24 11:28:12.093193.093193 lmp.py:530]   Expert 30 |     56 | CPU
DEBUG 12-24 11:28:12.093227.093227 lmp.py:530]   Expert 49 |     57 | CPU
DEBUG 12-24 11:28:12.093784.093784 lmp.py:530]   Expert 57 |     61 | CPU
DEBUG 12-24 11:28:12.093579.093579 lmp.py:530]   Expert 10 |     63 | CPU
DEBUG 12-24 11:28:12.093328.093328 lmp.py:530]   Expert 58 |     63 | CPU
DEBUG 12-24 11:28:12.093601.093601 lmp.py:530]   Expert 54 |     64 | CPU
DEBUG 12-24 11:28:12.093350.093350 lmp.py:530]   Expert 19 |     65 | CPU
DEBUG 12-24 11:28:12.093384.093384 lmp.py:530]   Expert 38 |     66 | CPU
DEBUG 12-24 11:28:12.093226.093226 lmp.py:530]   Expert 44 |     66 | CPU
DEBUG 12-24 11:28:12.093545.093545 lmp.py:530]   Expert 36 |     67 | CPU
DEBUG 12-24 11:28:12.093625.093625 lmp.py:530]   Expert 28 |     68 | CPU
DEBUG 12-24 11:28:12.093182.093182 lmp.py:530]   Expert  8 |     70 | CPU
DEBUG 12-24 11:28:12.093977.093977 lmp.py:530]   Expert 12 |     72 | CPU
DEBUG 12-24 11:28:12.093296.093296 lmp.py:530]   Expert 35 |     72 | CPU
DEBUG 12-24 11:28:12.093569.093569 lmp.py:530]   Expert 18 |     75 | CPU
DEBUG 12-24 11:28:12.093079.093079 lmp.py:530]   Expert 47 |     75 | CPU
DEBUG 12-24 11:28:12.093590.093590 lmp.py:530]   Expert 50 |     76 | CPU
DEBUG 12-24 11:28:12.093055.093055 lmp.py:530]   Expert 32 |     78 | CPU
DEBUG 12-24 11:28:12.093373.093373 lmp.py:530]   Expert 42 |     78 | CPU
DEBUG 12-24 11:28:12.093692.093692 lmp.py:530]   Expert 48 |     78 | CPU
DEBUG 12-24 11:28:12.093534.093534 lmp.py:530]   Expert 33 |     79 | CPU
DEBUG 12-24 11:28:12.093091.093091 lmp.py:530]   Expert 52 |     82 | GPU
DEBUG 12-24 11:28:12.093171.093171 lmp.py:530]   Expert 20 |     85 | GPU
DEBUG 12-24 11:28:12.093728.093728 lmp.py:530]   Expert 29 |     85 | GPU
DEBUG 12-24 11:28:12.093478.093478 lmp.py:530]   Expert 51 |     85 | GPU
DEBUG 12-24 11:28:12.093750.093750 lmp.py:530]   Expert 31 |     86 | GPU
DEBUG 12-24 11:28:12.093022.093022 lmp.py:530]   Expert 43 |     86 | GPU
DEBUG 12-24 11:28:12.093579.093579 lmp.py:530]   Expert 53 |     86 | GPU
DEBUG 12-24 11:28:12.093137.093137 lmp.py:530]   Expert 55 |     86 | GPU
DEBUG 12-24 11:28:12.093217.093217 lmp.py:530]   Expert 63 |     86 | GPU
DEBUG 12-24 11:28:12.093297.093297 lmp.py:530]   Expert 39 |     87 | GPU
DEBUG 12-24 11:28:12.093377.093377 lmp.py:530]   Expert 13 |     88 | GPU
DEBUG 12-24 11:28:12.093696.093696 lmp.py:530]   Expert 46 |     88 | GPU
DEBUG 12-24 11:28:12.094015.094015 lmp.py:530]   Expert 21 |     92 | GPU
DEBUG 12-24 11:28:12.094333.094333 lmp.py:530]   Expert 26 |     92 | GPU
DEBUG 12-24 11:28:12.094891.094891 lmp.py:530]   Expert 17 |     95 | GPU
DEBUG 12-24 11:28:12.094163.094163 lmp.py:530]   Expert 27 |     96 | GPU
DEBUG 12-24 11:28:12.094674.094674 lmp.py:530]   Expert 62 |     98 | GPU
DEBUG 12-24 11:28:12.094184.094184 lmp.py:530]   Expert 40 |    106 | GPU
DEBUG 12-24 11:28:12.094742.094742 lmp.py:530]   Expert 45 |    107 | GPU
DEBUG 12-24 11:28:12.094822.094822 lmp.py:530]   Expert 60 |    107 | GPU
DEBUG 12-24 11:28:12.094902.094902 lmp.py:530]   Expert 22 |    113 | GPU
DEBUG 12-24 11:28:12.094221.094221 lmp.py:530]   Expert 34 |    115 | GPU
DEBUG 12-24 11:28:12.094540.094540 lmp.py:530]   Expert 14 |    120 | GPU
DEBUG 12-24 11:28:12.094858.094858 lmp.py:530]   Expert 25 |    126 | GPU
DEBUG 12-24 11:28:12.094177.094177 lmp.py:530]   Expert  9 |    131 | GPU
DEBUG 12-24 11:28:12.094449.094449 lmp.py:530]   Expert 41 |    163 | GPU
DEBUG 12-24 11:28:12.094199.094199 lmp.py:530]   Expert  4 |   1250 | GPU
DEBUG 12-24 11:28:12.094709.094709 lmp.py:530]   Expert  1 |   1270 | GPU
DEBUG 12-24 11:28:12.094651.094651 lmp.py:530]   Expert  3 |   1304 | GPU
DEBUG 12-24 11:28:12.094685.094685 lmp.py:530]   Expert  2 |   1305 | GPU
DEBUG 12-24 11:28:12.094480.094480 lmp.py:530]   Expert  0 |   1312 | GPU
DEBUG 12-24 11:28:12.094560.094560 lmp.py:530]   Expert  5 |   1315 | GPU
DEBUG 12-24 11:28:12.094310.094310 lmp.py:531] 
DEBUG 12-24 11:28:12.094310.094310 lmp.py:531]   CPU total tokens: 1941 (15.8%)
DEBUG 12-24 11:28:12.094059.094059 lmp.py:532]   GPU total tokens: 10347 (84.2%)
DEBUG 12-24 11:28:12.094768.094768 cuda_h.py:19] end experts_map_get cost 0.0022041797637939453 seconds
DEBUG 12-24 11:28:12.094756.094756 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.094256.094256 lmp.py:541] 
DEBUG 12-24 11:28:12.094256.094256 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:12.094649.094649 cuda_h.py:19] end cpu_experts_submit cost 0.0001316070556640625 seconds
DEBUG 12-24 11:28:12.094882.094882 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.094070.094070 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.095724.095724 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.095403.095403 cuda_h.py:19] end allocate_cuda_memory cost 0.0002429485321044922 seconds
DEBUG 12-24 11:28:12.095041.095041 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.095964.095964 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.095529.095529 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.095391.095391 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 0682a1c0-73d0-4d6e-af94-faac2d3ed6ab
DEBUG 12-24 11:28:12.095300.095300 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.111678.111678 mlpmodule.py:630] group tensors cost 0.014807462692260742 s
INFO 12-24 11:28:12.112378.112378 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 0682a1c0-73d0-4d6e-af94-faac2d3ed6ab
DEBUG 12-24 11:28:12.112342.112342 cuda_h.py:19] end load_into_gpu_async cost 0.016849756240844727 seconds
DEBUG 12-24 11:28:12.112066.112066 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.112285.112285 cuda_h.py:19] end restore_tensors2 cost 0.0003688335418701172 seconds
DEBUG 12-24 11:28:12.112982.112982 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.01791238784790039 seconds
DEBUG 12-24 11:28:12.118452.118452 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.023311138153076172 seconds
DEBUG 12-24 11:28:12.118945.118945 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 12-24 11:28:12.118290.118290 cuda_h.py:10] start start_load_qkvogn_s_weight_l_4
DEBUG 12-24 11:28:12.118995.118995 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 2.9325485229492188e-05 seconds
DEBUG 12-24 11:28:12.118943.118943 cuda_h.py:19] end start_load_qkvogn_s_weight_l_4 cost 6.413459777832031e-05 seconds
DEBUG 12-24 11:28:12.118738.118738 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.118180.118180 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.118307.118307 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.118432.118432 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.120300.120300 cuda_h.py:19] end allocate_cuda_memory cost 0.0010752677917480469 seconds
DEBUG 12-24 11:28:12.120220.120220 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.120617.120617 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.120053.120053 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.120646.120646 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c18cbe0b-46ae-402a-9223-ab2653864d68
DEBUG 12-24 11:28:12.121270.121270 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.121553.121553 cuda_h.py:19] end gpu_sexperts cost 0.0029408931732177734 seconds
DEBUG 12-24 11:28:12.121504.121504 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.121466.121466 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 0682a1c0-73d0-4d6e-af94-faac2d3ed6ab
DEBUG 12-24 11:28:12.122504.122504 mlpmodule.py:668] pad cost 0.01003718376159668 s
DEBUG 12-24 11:28:12.122157.122157 mlpmodule.py:674] create cpu tensor cost 4.744529724121094e-05 s
DEBUG 12-24 11:28:12.122875.122875 mlpmodule.py:679] move to cpu cost 3.361701965332031e-05 s
INFO 12-24 11:28:12.122886.122886 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c18cbe0b-46ae-402a-9223-ab2653864d68
DEBUG 12-24 11:28:12.122838.122838 cuda_h.py:19] end load_into_gpu_async cost 0.002234935760498047 seconds
DEBUG 12-24 11:28:12.122615.122615 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.123745.123745 cuda_h.py:19] end restore_tensors2 cost 0.0002052783966064453 seconds
DEBUG 12-24 11:28:12.123088.123088 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0045795440673828125 seconds
INFO 12-24 11:28:12.129324.129324 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c18cbe0b-46ae-402a-9223-ab2653864d68
DEBUG 12-24 11:28:12.133472.133472 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:12.133656.133656 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.133289.133289 mlpmodule.py:700] group_w3 first element: 0.0206298828125
WARNING 12-24 11:28:12.133896.133896 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.142969.142969 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.143306.143306 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.153218.153218 mlpmodule.py:723] group einsum cost 0.03149724006652832 s
DEBUG 12-24 11:28:12.154804.154804 mlpmodule.py:731] cpy2cputensor cost 0.00044345855712890625 s
INFO 12-24 11:28:12.162930.162930 client.py:127] Model loaded
DEBUG 12-24 11:28:12.162134.162134 cuda_h.py:19] end wait_experts cost 0.04111218452453613 seconds
DEBUG 12-24 11:28:12.162241.162241 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.162580.162580 lmp.py:585]   Computing 32 experts on GPU...
INFO 12-24 11:28:12.163537.163537 client.py:127] Model loaded
DEBUG 12-24 11:28:12.163751.163751 cuda_h.py:19] end sllm_worker_task cost 0.044732093811035156 seconds
DEBUG 12-24 11:28:12.163269.163269 mlpmodule.py:457] gpu group tensors cost 0.0010499954223632812 s
DEBUG 12-24 11:28:12.181786.181786 mlpmodule.py:490] gpu pad cost 0.017152070999145508 s
DEBUG 12-24 11:28:12.183542.183542 mlpmodule.py:588]  experts func einsum cost 0.08750033378601074 s
DEBUG 12-24 11:28:12.184834.184834 mlpmodule.py:508] gpu group einsum cost 0.0036628246307373047 s
DEBUG 12-24 11:28:12.190748.190748 mlpmodule.py:537] gpu experts func einsum cost 0.02741837501525879 s
DEBUG 12-24 11:28:12.190190.190190 cuda_h.py:19] end gpu_experts cost 0.027695417404174805 seconds
DEBUG 12-24 11:28:12.190702.190702 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.190586.190586 cuda_h.py:19] end wait_cetm_experts cost 2.8848648071289062e-05 seconds
DEBUG 12-24 11:28:12.190607.190607 lmp.py:615] gpu end - einsum end = 26.7ms
DEBUG 12-24 11:28:12.190916.190916 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.190177.190177 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.8371810913085938e-05 seconds
DEBUG 12-24 11:28:12.190007.190007 cuda_h.py:19] end layer_moe_generate_3 cost 0.0994255542755127 seconds
DEBUG 12-24 11:28:12.191486.191486 lmp.py:445] -------------------------------- end layer 3 --------------------------------
DEBUG 12-24 11:28:12.191535.191535 lmp.py:418] -------------------------------- start layer 4 --------------------------------
DEBUG 12-24 11:28:12.191066.191066 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.191678.191678 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.197107.197107 cuda_h.py:19] end self_attn cost 0.005338191986083984 seconds
DEBUG 12-24 11:28:12.198951.198951 cuda_h.py:19] end iln_self_attn_paln cost 0.006760835647583008 seconds
DEBUG 12-24 11:28:12.198552.198552 cuda_h.py:10] start layer_moe_generate_4
DEBUG 12-24 11:28:12.198470.198470 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.199500.199500 cuda_h.py:19] end gate cost 0.001192331314086914 seconds
DEBUG 12-24 11:28:12.199728.199728 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.200318.200318 lmp.py:519] 
DEBUG 12-24 11:28:12.200318.200318 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.200003.200003 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:12.200839.200839 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:12.200523.200523 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:12.200246.200246 lmp.py:523] 
DEBUG 12-24 11:28:12.200246.200246 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.200493.200493 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.200229.200229 lmp.py:530]   Expert 13 |     15 | CPU
DEBUG 12-24 11:28:12.200191.200191 lmp.py:530]   Expert 26 |     33 | CPU
DEBUG 12-24 11:28:12.200961.200961 lmp.py:530]   Expert 11 |     35 | CPU
DEBUG 12-24 11:28:12.200538.200538 lmp.py:530]   Expert 45 |     40 | CPU
DEBUG 12-24 11:28:12.200639.200639 lmp.py:530]   Expert 51 |     41 | CPU
DEBUG 12-24 11:28:12.200216.200216 lmp.py:530]   Expert 60 |     43 | CPU
DEBUG 12-24 11:28:12.200747.200747 lmp.py:530]   Expert 36 |     44 | CPU
DEBUG 12-24 11:28:12.200279.200279 lmp.py:530]   Expert 56 |     45 | CPU
DEBUG 12-24 11:28:12.201379.201379 lmp.py:530]   Expert 48 |     48 | CPU
DEBUG 12-24 11:28:12.201003.201003 lmp.py:530]   Expert 16 |     49 | CPU
DEBUG 12-24 11:28:12.201819.201819 lmp.py:530]   Expert 41 |     50 | CPU
DEBUG 12-24 11:28:12.201635.201635 lmp.py:530]   Expert 33 |     51 | CPU
DEBUG 12-24 11:28:12.201166.201166 lmp.py:530]   Expert 58 |     51 | CPU
DEBUG 12-24 11:28:12.201174.201174 lmp.py:530]   Expert  6 |     53 | CPU
DEBUG 12-24 11:28:12.201990.201990 lmp.py:530]   Expert  7 |     54 | CPU
DEBUG 12-24 11:28:12.201614.201614 lmp.py:530]   Expert 25 |     55 | CPU
DEBUG 12-24 11:28:12.201476.201476 lmp.py:530]   Expert 47 |     55 | CPU
DEBUG 12-24 11:28:12.201338.201338 lmp.py:530]   Expert 10 |     56 | CPU
DEBUG 12-24 11:28:12.201916.201916 lmp.py:530]   Expert 19 |     59 | CPU
DEBUG 12-24 11:28:12.201970.201970 lmp.py:530]   Expert 23 |     61 | CPU
DEBUG 12-24 11:28:12.201547.201547 lmp.py:530]   Expert 24 |     61 | CPU
DEBUG 12-24 11:28:12.201171.201171 lmp.py:530]   Expert 28 |     62 | CPU
DEBUG 12-24 11:28:12.201272.201272 lmp.py:530]   Expert  9 |     64 | CPU
DEBUG 12-24 11:28:12.201896.201896 lmp.py:530]   Expert 35 |     65 | CPU
DEBUG 12-24 11:28:12.201712.201712 lmp.py:530]   Expert 14 |     67 | CPU
DEBUG 12-24 11:28:12.201528.201528 lmp.py:530]   Expert 34 |     67 | CPU
DEBUG 12-24 11:28:12.201390.201390 lmp.py:530]   Expert 38 |     67 | CPU
DEBUG 12-24 11:28:12.201537.201537 lmp.py:530]   Expert 46 |     69 | CPU
DEBUG 12-24 11:28:12.201922.201922 lmp.py:530]   Expert 55 |     70 | CPU
DEBUG 12-24 11:28:12.201500.201500 lmp.py:530]   Expert 50 |     75 | CPU
DEBUG 12-24 11:28:12.201316.201316 lmp.py:530]   Expert 29 |     76 | CPU
DEBUG 12-24 11:28:12.201655.201655 lmp.py:530]   Expert 40 |     76 | CPU
DEBUG 12-24 11:28:12.201802.201802 lmp.py:530]   Expert 20 |     82 | GPU
DEBUG 12-24 11:28:12.201425.201425 lmp.py:530]   Expert 15 |     83 | GPU
DEBUG 12-24 11:28:12.202003.202003 lmp.py:530]   Expert 31 |     83 | GPU
DEBUG 12-24 11:28:12.202772.202772 lmp.py:530]   Expert 12 |     85 | GPU
DEBUG 12-24 11:28:12.202873.202873 lmp.py:530]   Expert 17 |     86 | GPU
DEBUG 12-24 11:28:12.202020.202020 lmp.py:530]   Expert 18 |     87 | GPU
DEBUG 12-24 11:28:12.202882.202882 lmp.py:530]   Expert 44 |     87 | GPU
DEBUG 12-24 11:28:12.202937.202937 lmp.py:530]   Expert 42 |     91 | GPU
DEBUG 12-24 11:28:12.202753.202753 lmp.py:530]   Expert 63 |     91 | GPU
DEBUG 12-24 11:28:12.202899.202899 lmp.py:530]   Expert 39 |     92 | GPU
DEBUG 12-24 11:28:12.202285.202285 lmp.py:530]   Expert 21 |     94 | GPU
DEBUG 12-24 11:28:12.202193.202193 lmp.py:530]   Expert 22 |     96 | GPU
DEBUG 12-24 11:28:12.202102.202102 lmp.py:530]   Expert 54 |     96 | GPU
DEBUG 12-24 11:28:12.202441.202441 lmp.py:530]   Expert 57 |     97 | GPU
DEBUG 12-24 11:28:12.202257.202257 lmp.py:530]   Expert 30 |     99 | GPU
DEBUG 12-24 11:28:12.202166.202166 lmp.py:530]   Expert 53 |     99 | GPU
DEBUG 12-24 11:28:12.202074.202074 lmp.py:530]   Expert  8 |    100 | GPU
DEBUG 12-24 11:28:12.202744.202744 lmp.py:530]   Expert 32 |    100 | GPU
DEBUG 12-24 11:28:12.202414.202414 lmp.py:530]   Expert 52 |    103 | GPU
DEBUG 12-24 11:28:12.202084.202084 lmp.py:530]   Expert 37 |    108 | GPU
DEBUG 12-24 11:28:12.202199.202199 lmp.py:530]   Expert 27 |    113 | GPU
DEBUG 12-24 11:28:12.202730.202730 lmp.py:530]   Expert 49 |    121 | GPU
DEBUG 12-24 11:28:12.202307.202307 lmp.py:530]   Expert 43 |    130 | GPU
DEBUG 12-24 11:28:12.202216.202216 lmp.py:530]   Expert 61 |    138 | GPU
DEBUG 12-24 11:28:12.202363.202363 lmp.py:530]   Expert 62 |    149 | GPU
DEBUG 12-24 11:28:12.202464.202464 lmp.py:530]   Expert 59 |    165 | GPU
DEBUG 12-24 11:28:12.202279.202279 lmp.py:530]   Expert  3 |   1264 | GPU
DEBUG 12-24 11:28:12.203665.203665 lmp.py:530]   Expert  2 |   1275 | GPU
DEBUG 12-24 11:28:12.203812.203812 lmp.py:530]   Expert  4 |   1286 | GPU
DEBUG 12-24 11:28:12.203912.203912 lmp.py:530]   Expert  0 |   1339 | GPU
DEBUG 12-24 11:28:12.203059.203059 lmp.py:530]   Expert  1 |   1342 | GPU
DEBUG 12-24 11:28:12.203637.203637 lmp.py:530]   Expert  5 |   1350 | GPU
DEBUG 12-24 11:28:12.203883.203883 lmp.py:531] 
DEBUG 12-24 11:28:12.203883.203883 lmp.py:531]   CPU total tokens: 1757 (14.3%)
DEBUG 12-24 11:28:12.203368.203368 lmp.py:532]   GPU total tokens: 10531 (85.7%)
DEBUG 12-24 11:28:12.203244.203244 cuda_h.py:19] end experts_map_get cost 0.0034770965576171875 seconds
DEBUG 12-24 11:28:12.203490.203490 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.203521.203521 lmp.py:541] 
DEBUG 12-24 11:28:12.203521.203521 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:12.203346.203346 cuda_h.py:19] end cpu_experts_submit cost 0.00019431114196777344 seconds
DEBUG 12-24 11:28:12.203695.203695 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.203964.203964 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.204403.204403 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.204294.204294 cuda_h.py:19] end allocate_cuda_memory cost 0.0004444122314453125 seconds
DEBUG 12-24 11:28:12.204364.204364 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.204883.204883 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.204237.204237 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.204981.204981 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 777c4a8b-b8bd-43bd-9140-f6a4bbc40185
DEBUG 12-24 11:28:12.205576.205576 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.218113.218113 mlpmodule.py:630] group tensors cost 0.013089418411254883 s
INFO 12-24 11:28:12.219511.219511 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 777c4a8b-b8bd-43bd-9140-f6a4bbc40185
DEBUG 12-24 11:28:12.219449.219449 cuda_h.py:19] end load_into_gpu_async cost 0.014914989471435547 seconds
DEBUG 12-24 11:28:12.219093.219093 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.220158.220158 cuda_h.py:19] end restore_tensors2 cost 0.0004892349243164062 seconds
DEBUG 12-24 11:28:12.220175.220175 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.01649165153503418 seconds
DEBUG 12-24 11:28:12.226354.226354 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.022396326065063477 seconds
DEBUG 12-24 11:28:12.226059.226059 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 12-24 11:28:12.226956.226956 cuda_h.py:10] start start_load_qkvogn_s_weight_l_5
DEBUG 12-24 11:28:12.226609.226609 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 4.291534423828125e-05 seconds
DEBUG 12-24 11:28:12.226299.226299 cuda_h.py:19] end start_load_qkvogn_s_weight_l_5 cost 0.00010037422180175781 seconds
DEBUG 12-24 11:28:12.226493.226493 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.226816.226816 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.226381.226381 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.227918.227918 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.228668.228668 cuda_h.py:19] end allocate_cuda_memory cost 0.0007236003875732422 seconds
DEBUG 12-24 11:28:12.228364.228364 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.228838.228838 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.228882.228882 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.228183.228183 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 6ad58b89-e1cb-4731-84dc-5ad4b194b147
DEBUG 12-24 11:28:12.228501.228501 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.229861.229861 cuda_h.py:19] end gpu_sexperts cost 0.0026307106018066406 seconds
DEBUG 12-24 11:28:12.229137.229137 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.229265.229265 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 777c4a8b-b8bd-43bd-9140-f6a4bbc40185
INFO 12-24 11:28:12.230963.230963 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 6ad58b89-e1cb-4731-84dc-5ad4b194b147
DEBUG 12-24 11:28:12.230180.230180 cuda_h.py:19] end load_into_gpu_async cost 0.0021104812622070312 seconds
DEBUG 12-24 11:28:12.230535.230535 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.230166.230166 cuda_h.py:19] end restore_tensors2 cost 0.0001537799835205078 seconds
DEBUG 12-24 11:28:12.230925.230925 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0038127899169921875 seconds
INFO 12-24 11:28:12.232814.232814 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 6ad58b89-e1cb-4731-84dc-5ad4b194b147
DEBUG 12-24 11:28:12.233451.233451 mlpmodule.py:668] pad cost 0.013585805892944336 s
DEBUG 12-24 11:28:12.233953.233953 mlpmodule.py:674] create cpu tensor cost 4.673004150390625e-05 s
DEBUG 12-24 11:28:12.233525.233525 mlpmodule.py:679] move to cpu cost 2.9802322387695312e-05 s
DEBUG 12-24 11:28:12.244293.244293 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:12.244237.244237 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.244756.244756 mlpmodule.py:700] group_w3 first element: -0.0015106201171875
WARNING 12-24 11:28:12.244307.244307 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.253108.253108 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.253478.253478 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.263079.263079 mlpmodule.py:723] group einsum cost 0.029952526092529297 s
DEBUG 12-24 11:28:12.263950.263950 mlpmodule.py:731] cpy2cputensor cost 0.00039076805114746094 s
INFO 12-24 11:28:12.265531.265531 client.py:127] Model loaded
DEBUG 12-24 11:28:12.265993.265993 cuda_h.py:19] end wait_experts cost 0.0363771915435791 seconds
DEBUG 12-24 11:28:12.265716.265716 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.265194.265194 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:12.266107.266107 mlpmodule.py:457] gpu group tensors cost 0.0006477832794189453 s
DEBUG 12-24 11:28:12.269311.269311 mlpmodule.py:490] gpu pad cost 0.003245830535888672 s
DEBUG 12-24 11:28:12.270888.270888 mlpmodule.py:508] gpu group einsum cost 0.0009105205535888672 s
INFO 12-24 11:28:12.271690.271690 client.py:127] Model loaded
DEBUG 12-24 11:28:12.271132.271132 cuda_h.py:19] end sllm_worker_task cost 0.04458928108215332 seconds
DEBUG 12-24 11:28:12.274676.274676 mlpmodule.py:537] gpu experts func einsum cost 0.008753299713134766 s
DEBUG 12-24 11:28:12.274270.274270 cuda_h.py:19] end gpu_experts cost 0.008977890014648438 seconds
DEBUG 12-24 11:28:12.274887.274887 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.274624.274624 cuda_h.py:19] end wait_cetm_experts cost 2.1219253540039062e-05 seconds
DEBUG 12-24 11:28:12.274850.274850 lmp.py:615] gpu end - einsum end = 2.7ms
DEBUG 12-24 11:28:12.275713.275713 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.275265.275265 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2649765014648438e-05 seconds
DEBUG 12-24 11:28:12.275113.275113 cuda_h.py:19] end layer_moe_generate_4 cost 0.07680583000183105 seconds
DEBUG 12-24 11:28:12.275138.275138 lmp.py:445] -------------------------------- end layer 4 --------------------------------
DEBUG 12-24 11:28:12.275339.275339 lmp.py:418] -------------------------------- start layer 5 --------------------------------
DEBUG 12-24 11:28:12.275511.275511 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.275078.275078 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.279760.279760 cuda_h.py:19] end self_attn cost 0.0036716461181640625 seconds
DEBUG 12-24 11:28:12.279961.279961 cuda_h.py:19] end iln_self_attn_paln cost 0.004303932189941406 seconds
DEBUG 12-24 11:28:12.279480.279480 cuda_h.py:10] start layer_moe_generate_5
DEBUG 12-24 11:28:12.279528.279528 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.280001.280001 cuda_h.py:19] end gate cost 0.0006666183471679688 seconds
DEBUG 12-24 11:28:12.280308.280308 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.280410.280410 lmp.py:519] 
DEBUG 12-24 11:28:12.280410.280410 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.280504.280504 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:12.280630.280630 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:12.280704.280704 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:12.280108.280108 lmp.py:523] 
DEBUG 12-24 11:28:12.280108.280108 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.280990.280990 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.280832.280832 lmp.py:530]   Expert 34 |     15 | CPU
DEBUG 12-24 11:28:12.281097.281097 lmp.py:530]   Expert 15 |     19 | CPU
DEBUG 12-24 11:28:12.281979.281979 lmp.py:530]   Expert 47 |     30 | CPU
DEBUG 12-24 11:28:12.281622.281622 lmp.py:530]   Expert 39 |     31 | CPU
DEBUG 12-24 11:28:12.281788.281788 lmp.py:530]   Expert 46 |     36 | CPU
DEBUG 12-24 11:28:12.281716.281716 lmp.py:530]   Expert 57 |     37 | CPU
DEBUG 12-24 11:28:12.281266.281266 lmp.py:530]   Expert 45 |     38 | CPU
DEBUG 12-24 11:28:12.281955.281955 lmp.py:530]   Expert 14 |     39 | CPU
DEBUG 12-24 11:28:12.281883.281883 lmp.py:530]   Expert 23 |     49 | CPU
DEBUG 12-24 11:28:12.281387.281387 lmp.py:530]   Expert 30 |     51 | CPU
DEBUG 12-24 11:28:12.281553.281553 lmp.py:530]   Expert 18 |     53 | CPU
DEBUG 12-24 11:28:12.281481.281481 lmp.py:530]   Expert 60 |     53 | CPU
DEBUG 12-24 11:28:12.281647.281647 lmp.py:530]   Expert  8 |     61 | CPU
DEBUG 12-24 11:28:12.281813.281813 lmp.py:530]   Expert 35 |     61 | CPU
DEBUG 12-24 11:28:12.281741.281741 lmp.py:530]   Expert 27 |     62 | CPU
DEBUG 12-24 11:28:12.281668.281668 lmp.py:530]   Expert 17 |     63 | CPU
DEBUG 12-24 11:28:12.281596.281596 lmp.py:530]   Expert 22 |     63 | CPU
DEBUG 12-24 11:28:12.281285.281285 lmp.py:530]   Expert 21 |     64 | CPU
DEBUG 12-24 11:28:12.281213.281213 lmp.py:530]   Expert 49 |     64 | CPU
DEBUG 12-24 11:28:12.281618.281618 lmp.py:530]   Expert  7 |     66 | CPU
DEBUG 12-24 11:28:12.281545.281545 lmp.py:530]   Expert 36 |     68 | CPU
DEBUG 12-24 11:28:12.281473.281473 lmp.py:530]   Expert 10 |     69 | CPU
DEBUG 12-24 11:28:12.281401.281401 lmp.py:530]   Expert 54 |     69 | CPU
DEBUG 12-24 11:28:12.281329.281329 lmp.py:530]   Expert 62 |     69 | CPU
DEBUG 12-24 11:28:12.281256.281256 lmp.py:530]   Expert 41 |     71 | CPU
DEBUG 12-24 11:28:12.281422.281422 lmp.py:530]   Expert  9 |     72 | CPU
DEBUG 12-24 11:28:12.281873.281873 lmp.py:530]   Expert 32 |     73 | CPU
DEBUG 12-24 11:28:12.281563.281563 lmp.py:530]   Expert 59 |     73 | CPU
DEBUG 12-24 11:28:12.281490.281490 lmp.py:530]   Expert 63 |     73 | CPU
DEBUG 12-24 11:28:12.281425.281425 lmp.py:530]   Expert 50 |     76 | CPU
DEBUG 12-24 11:28:12.281353.281353 lmp.py:530]   Expert 26 |     77 | CPU
DEBUG 12-24 11:28:12.281042.281042 lmp.py:530]   Expert 48 |     77 | CPU
DEBUG 12-24 11:28:12.281208.281208 lmp.py:530]   Expert 11 |     78 | GPU
DEBUG 12-24 11:28:12.281136.281136 lmp.py:530]   Expert 38 |     80 | GPU
DEBUG 12-24 11:28:12.281825.281825 lmp.py:530]   Expert 24 |     83 | GPU
DEBUG 12-24 11:28:12.281514.281514 lmp.py:530]   Expert 20 |     84 | GPU
DEBUG 12-24 11:28:12.281204.281204 lmp.py:530]   Expert 56 |     84 | GPU
DEBUG 12-24 11:28:12.281860.281860 lmp.py:530]   Expert 12 |     85 | GPU
DEBUG 12-24 11:28:12.281311.281311 lmp.py:530]   Expert 31 |     86 | GPU
DEBUG 12-24 11:28:12.281762.281762 lmp.py:530]   Expert 42 |     86 | GPU
DEBUG 12-24 11:28:12.281736.281736 lmp.py:530]   Expert 52 |     87 | GPU
DEBUG 12-24 11:28:12.281187.281187 lmp.py:530]   Expert 19 |     88 | GPU
DEBUG 12-24 11:28:12.281552.281552 lmp.py:530]   Expert 13 |     91 | GPU
DEBUG 12-24 11:28:12.281764.281764 lmp.py:530]   Expert 40 |     94 | GPU
DEBUG 12-24 11:28:12.281977.281977 lmp.py:530]   Expert 29 |     96 | GPU
DEBUG 12-24 11:28:12.281666.281666 lmp.py:530]   Expert 55 |     98 | GPU
DEBUG 12-24 11:28:12.281832.281832 lmp.py:530]   Expert 16 |    100 | GPU
DEBUG 12-24 11:28:12.281283.281283 lmp.py:530]   Expert 43 |    101 | GPU
DEBUG 12-24 11:28:12.281257.281257 lmp.py:530]   Expert 28 |    103 | GPU
DEBUG 12-24 11:28:12.281708.281708 lmp.py:530]   Expert 58 |    104 | GPU
DEBUG 12-24 11:28:12.281974.281974 lmp.py:530]   Expert 33 |    105 | GPU
DEBUG 12-24 11:28:12.281948.281948 lmp.py:530]   Expert 51 |    108 | GPU
DEBUG 12-24 11:28:12.281160.281160 lmp.py:530]   Expert 25 |    116 | GPU
DEBUG 12-24 11:28:12.281849.281849 lmp.py:530]   Expert 44 |    124 | GPU
DEBUG 12-24 11:28:12.281824.281824 lmp.py:530]   Expert  6 |    125 | GPU
DEBUG 12-24 11:28:12.281274.281274 lmp.py:530]   Expert 37 |    134 | GPU
DEBUG 12-24 11:28:12.281487.281487 lmp.py:530]   Expert 61 |    134 | GPU
DEBUG 12-24 11:28:12.281461.281461 lmp.py:530]   Expert 53 |    187 | GPU
DEBUG 12-24 11:28:12.281673.281673 lmp.py:530]   Expert  3 |   1262 | GPU
DEBUG 12-24 11:28:12.282363.282363 lmp.py:530]   Expert  2 |   1271 | GPU
DEBUG 12-24 11:28:12.282575.282575 lmp.py:530]   Expert  1 |   1300 | GPU
DEBUG 12-24 11:28:12.282788.282788 lmp.py:530]   Expert  0 |   1315 | GPU
DEBUG 12-24 11:28:12.282000.282000 lmp.py:530]   Expert  4 |   1321 | GPU
DEBUG 12-24 11:28:12.282451.282451 lmp.py:530]   Expert  5 |   1336 | GPU
DEBUG 12-24 11:28:12.282617.282617 lmp.py:531] 
DEBUG 12-24 11:28:12.282617.282617 lmp.py:531]   CPU total tokens: 1822 (14.8%)
DEBUG 12-24 11:28:12.282783.282783 lmp.py:532]   GPU total tokens: 10466 (85.2%)
DEBUG 12-24 11:28:12.282718.282718 cuda_h.py:19] end experts_map_get cost 0.0015370845794677734 seconds
DEBUG 12-24 11:28:12.282361.282361 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.282078.282078 lmp.py:541] 
DEBUG 12-24 11:28:12.282078.282078 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:12.282915.282915 cuda_h.py:19] end cpu_experts_submit cost 0.00010609626770019531 seconds
DEBUG 12-24 11:28:12.282055.282055 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.282395.282395 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.282804.282804 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.282793.282793 cuda_h.py:19] end allocate_cuda_memory cost 0.00019693374633789062 seconds
DEBUG 12-24 11:28:12.282093.282093 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.282240.282240 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.282382.282382 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.283880.283880 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bc5031f4-c862-45a5-b322-1ddefdd5952d
DEBUG 12-24 11:28:12.283251.283251 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:12.284850.284850 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bc5031f4-c862-45a5-b322-1ddefdd5952d
DEBUG 12-24 11:28:12.284508.284508 cuda_h.py:19] end load_into_gpu_async cost 0.0014584064483642578 seconds
DEBUG 12-24 11:28:12.284337.284337 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.284528.284528 cuda_h.py:19] end restore_tensors2 cost 0.0003101825714111328 seconds
DEBUG 12-24 11:28:12.284848.284848 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.002405881881713867 seconds
DEBUG 12-24 11:28:12.287000.287000 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.005047798156738281 seconds
DEBUG 12-24 11:28:12.287797.287797 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 12-24 11:28:12.287196.287196 cuda_h.py:10] start start_load_qkvogn_s_weight_l_6
DEBUG 12-24 11:28:12.287761.287761 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 2.9802322387695312e-05 seconds
DEBUG 12-24 11:28:12.287702.287702 cuda_h.py:19] end start_load_qkvogn_s_weight_l_6 cost 6.008148193359375e-05 seconds
DEBUG 12-24 11:28:12.287444.287444 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.287084.287084 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.287337.287337 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.288554.288554 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.288139.288139 cuda_h.py:19] end allocate_cuda_memory cost 0.0007145404815673828 seconds
DEBUG 12-24 11:28:12.289954.289954 mlpmodule.py:588]  experts func einsum cost 0.0835561752319336 s
DEBUG 12-24 11:28:12.289110.289110 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.289889.289889 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.289992.289992 cuda_h.py:19] end gpu_sexperts cost 0.002076387405395508 seconds
DEBUG 12-24 11:28:12.289887.289887 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.289564.289564 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bc5031f4-c862-45a5-b322-1ddefdd5952d
DEBUG 12-24 11:28:12.290483.290483 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.290780.290780 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e3e04625-89f5-4466-81ae-994098691d8f
DEBUG 12-24 11:28:12.290880.290880 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.297634.297634 mlpmodule.py:630] group tensors cost 0.007061481475830078 s
DEBUG 12-24 11:28:12.299570.299570 mlpmodule.py:668] pad cost 0.00150299072265625 s
DEBUG 12-24 11:28:12.299011.299011 mlpmodule.py:674] create cpu tensor cost 4.124641418457031e-05 s
DEBUG 12-24 11:28:12.300052.300052 mlpmodule.py:679] move to cpu cost 2.8371810913085938e-05 s
DEBUG 12-24 11:28:12.310450.310450 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:12.310088.310088 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.310369.310369 mlpmodule.py:700] group_w3 first element: 0.0537109375
WARNING 12-24 11:28:12.310782.310782 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.319125.319125 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.319098.319098 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.329361.329361 mlpmodule.py:723] group einsum cost 0.029509782791137695 s
DEBUG 12-24 11:28:12.330696.330696 mlpmodule.py:731] cpy2cputensor cost 0.0003933906555175781 s
INFO 12-24 11:28:12.341742.341742 client.py:127] Model loaded
DEBUG 12-24 11:28:12.341736.341736 cuda_h.py:19] end wait_experts cost 0.05179309844970703 seconds
DEBUG 12-24 11:28:12.341110.341110 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.341795.341795 lmp.py:585]   Computing 32 experts on GPU...
INFO 12-24 11:28:12.343638.343638 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e3e04625-89f5-4466-81ae-994098691d8f
DEBUG 12-24 11:28:12.343638.343638 cuda_h.py:19] end load_into_gpu_async cost 0.053987741470336914 seconds
DEBUG 12-24 11:28:12.343760.343760 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.343027.343027 cuda_h.py:19] end restore_tensors2 cost 0.00018262863159179688 seconds
DEBUG 12-24 11:28:12.344355.344355 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.056014060974121094 seconds
INFO 12-24 11:28:12.345220.345220 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e3e04625-89f5-4466-81ae-994098691d8f
DEBUG 12-24 11:28:12.345628.345628 mlpmodule.py:457] gpu group tensors cost 0.003535032272338867 s
INFO 12-24 11:28:12.349652.349652 client.py:127] Model loaded
DEBUG 12-24 11:28:12.350988.350988 cuda_h.py:19] end sllm_worker_task cost 0.06218075752258301 seconds
DEBUG 12-24 11:28:12.350488.350488 mlpmodule.py:490] gpu pad cost 0.004554271697998047 s
DEBUG 12-24 11:28:12.350424.350424 mlpmodule.py:588]  experts func einsum cost 0.059943437576293945 s
DEBUG 12-24 11:28:12.351517.351517 mlpmodule.py:508] gpu group einsum cost 0.0010943412780761719 s
DEBUG 12-24 11:28:12.358481.358481 mlpmodule.py:537] gpu experts func einsum cost 0.016150474548339844 s
DEBUG 12-24 11:28:12.358769.358769 cuda_h.py:19] end gpu_experts cost 0.016465187072753906 seconds
DEBUG 12-24 11:28:12.358632.358632 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.358229.358229 cuda_h.py:19] end wait_cetm_experts cost 1.9073486328125e-05 seconds
DEBUG 12-24 11:28:12.358177.358177 lmp.py:615] gpu end - einsum end = 23.9ms
DEBUG 12-24 11:28:12.358054.358054 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.358712.358712 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.384185791015625e-05 seconds
DEBUG 12-24 11:28:12.358852.358852 cuda_h.py:19] end layer_moe_generate_5 cost 0.07893204689025879 seconds
DEBUG 12-24 11:28:12.358496.358496 lmp.py:445] -------------------------------- end layer 5 --------------------------------
DEBUG 12-24 11:28:12.359073.359073 lmp.py:418] -------------------------------- start layer 6 --------------------------------
DEBUG 12-24 11:28:12.359823.359823 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.359589.359589 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.364865.364865 cuda_h.py:19] end self_attn cost 0.005150318145751953 seconds
DEBUG 12-24 11:28:12.365880.365880 cuda_h.py:19] end iln_self_attn_paln cost 0.006412029266357422 seconds
DEBUG 12-24 11:28:12.365574.365574 cuda_h.py:10] start layer_moe_generate_6
DEBUG 12-24 11:28:12.365154.365154 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.367428.367428 cuda_h.py:19] end gate cost 0.0013549327850341797 seconds
DEBUG 12-24 11:28:12.367413.367413 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.368740.368740 lmp.py:519] 
DEBUG 12-24 11:28:12.368740.368740 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.368392.368392 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:12.368415.368415 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:12.368980.368980 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:12.368962.368962 lmp.py:523] 
DEBUG 12-24 11:28:12.368962.368962 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.368660.368660 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.368662.368662 lmp.py:530]   Expert 21 |     15 | CPU
DEBUG 12-24 11:28:12.368313.368313 lmp.py:530]   Expert 10 |     25 | CPU
DEBUG 12-24 11:28:12.368580.368580 lmp.py:530]   Expert 44 |     25 | CPU
DEBUG 12-24 11:28:12.368654.368654 lmp.py:530]   Expert 14 |     31 | CPU
DEBUG 12-24 11:28:12.368729.368729 lmp.py:530]   Expert 33 |     31 | CPU
DEBUG 12-24 11:28:12.368327.368327 lmp.py:530]   Expert 15 |     36 | CPU
DEBUG 12-24 11:28:12.369686.369686 lmp.py:530]   Expert 63 |     36 | CPU
DEBUG 12-24 11:28:12.369238.369238 lmp.py:530]   Expert 26 |     38 | CPU
DEBUG 12-24 11:28:12.369313.369313 lmp.py:530]   Expert 52 |     38 | CPU
DEBUG 12-24 11:28:12.369910.369910 lmp.py:530]   Expert 11 |     41 | CPU
DEBUG 12-24 11:28:12.369747.369747 lmp.py:530]   Expert 53 |     45 | CPU
DEBUG 12-24 11:28:12.369298.369298 lmp.py:530]   Expert 31 |     48 | CPU
DEBUG 12-24 11:28:12.369419.369419 lmp.py:530]   Expert 59 |     49 | CPU
DEBUG 12-24 11:28:12.369971.369971 lmp.py:530]   Expert  9 |     50 | CPU
DEBUG 12-24 11:28:12.369615.369615 lmp.py:530]   Expert 62 |     52 | CPU
DEBUG 12-24 11:28:12.369690.369690 lmp.py:530]   Expert 16 |     53 | CPU
DEBUG 12-24 11:28:12.369526.369526 lmp.py:530]   Expert 22 |     54 | CPU
DEBUG 12-24 11:28:12.369124.369124 lmp.py:530]   Expert 35 |     56 | CPU
DEBUG 12-24 11:28:12.369960.369960 lmp.py:530]   Expert 19 |     57 | CPU
DEBUG 12-24 11:28:12.369127.369127 lmp.py:530]   Expert 42 |     59 | CPU
DEBUG 12-24 11:28:12.369725.369725 lmp.py:530]   Expert 45 |     62 | CPU
DEBUG 12-24 11:28:12.369369.369369 lmp.py:530]   Expert 46 |     65 | CPU
DEBUG 12-24 11:28:12.369729.369729 lmp.py:530]   Expert 49 |     66 | CPU
DEBUG 12-24 11:28:12.369135.369135 lmp.py:530]   Expert 50 |     71 | CPU
DEBUG 12-24 11:28:12.369209.369209 lmp.py:530]   Expert 41 |     72 | CPU
DEBUG 12-24 11:28:12.370807.370807 lmp.py:530]   Expert 13 |     73 | CPU
DEBUG 12-24 11:28:12.370643.370643 lmp.py:530]   Expert 23 |     77 | CPU
DEBUG 12-24 11:28:12.370003.370003 lmp.py:530]   Expert 58 |     77 | CPU
DEBUG 12-24 11:28:12.370270.370270 lmp.py:530]   Expert 43 |     80 | CPU
DEBUG 12-24 11:28:12.370629.370629 lmp.py:530]   Expert 40 |     81 | CPU
DEBUG 12-24 11:28:12.370989.370989 lmp.py:530]   Expert 48 |     81 | CPU
DEBUG 12-24 11:28:12.370348.370348 lmp.py:530]   Expert 32 |     82 | CPU
DEBUG 12-24 11:28:12.370184.370184 lmp.py:530]   Expert 51 |     83 | GPU
DEBUG 12-24 11:28:12.370782.370782 lmp.py:530]   Expert 34 |     85 | GPU
DEBUG 12-24 11:28:12.370433.370433 lmp.py:530]   Expert 24 |     87 | GPU
DEBUG 12-24 11:28:12.370793.370793 lmp.py:530]   Expert 25 |     88 | GPU
DEBUG 12-24 11:28:12.370675.370675 lmp.py:530]   Expert 47 |     91 | GPU
DEBUG 12-24 11:28:12.370081.370081 lmp.py:530]   Expert 61 |     92 | GPU
DEBUG 12-24 11:28:12.370440.370440 lmp.py:530]   Expert 30 |     93 | GPU
DEBUG 12-24 11:28:12.370959.370959 lmp.py:530]   Expert 28 |     95 | GPU
DEBUG 12-24 11:28:12.370795.370795 lmp.py:530]   Expert 39 |     95 | GPU
DEBUG 12-24 11:28:12.370201.370201 lmp.py:530]   Expert 29 |     96 | GPU
DEBUG 12-24 11:28:12.370084.370084 lmp.py:530]   Expert 54 |     96 | GPU
DEBUG 12-24 11:28:12.371920.371920 lmp.py:530]   Expert 37 |    100 | GPU
DEBUG 12-24 11:28:12.371803.371803 lmp.py:530]   Expert 60 |    100 | GPU
DEBUG 12-24 11:28:12.371400.371400 lmp.py:530]   Expert  8 |    101 | GPU
DEBUG 12-24 11:28:12.371760.371760 lmp.py:530]   Expert 38 |    103 | GPU
DEBUG 12-24 11:28:12.371596.371596 lmp.py:530]   Expert 20 |    104 | GPU
DEBUG 12-24 11:28:12.371956.371956 lmp.py:530]   Expert 12 |    105 | GPU
DEBUG 12-24 11:28:12.371699.371699 lmp.py:530]   Expert 27 |    105 | GPU
DEBUG 12-24 11:28:12.371774.371774 lmp.py:530]   Expert 57 |    105 | GPU
DEBUG 12-24 11:28:12.371133.371133 lmp.py:530]   Expert 55 |    110 | GPU
DEBUG 12-24 11:28:12.371016.371016 lmp.py:530]   Expert  7 |    114 | GPU
DEBUG 12-24 11:28:12.371852.371852 lmp.py:530]   Expert 17 |    119 | GPU
DEBUG 12-24 11:28:12.371689.371689 lmp.py:530]   Expert  6 |    121 | GPU
DEBUG 12-24 11:28:12.371048.371048 lmp.py:530]   Expert 56 |    154 | GPU
DEBUG 12-24 11:28:12.371361.371361 lmp.py:530]   Expert 18 |    172 | GPU
DEBUG 12-24 11:28:12.371244.371244 lmp.py:530]   Expert 36 |    180 | GPU
DEBUG 12-24 11:28:12.371080.371080 lmp.py:530]   Expert  1 |   1230 | GPU
DEBUG 12-24 11:28:12.371155.371155 lmp.py:530]   Expert  3 |   1247 | GPU
DEBUG 12-24 11:28:12.371991.371991 lmp.py:530]   Expert  4 |   1276 | GPU
DEBUG 12-24 11:28:12.372350.372350 lmp.py:530]   Expert  2 |   1292 | GPU
DEBUG 12-24 11:28:12.372710.372710 lmp.py:530]   Expert  5 |   1323 | GPU
DEBUG 12-24 11:28:12.372949.372949 lmp.py:530]   Expert  0 |   1400 | GPU
DEBUG 12-24 11:28:12.372030.372030 lmp.py:531] 
DEBUG 12-24 11:28:12.372030.372030 lmp.py:531]   CPU total tokens: 1726 (14.0%)
DEBUG 12-24 11:28:12.372348.372348 lmp.py:532]   GPU total tokens: 10562 (86.0%)
DEBUG 12-24 11:28:12.372628.372628 cuda_h.py:19] end experts_map_get cost 0.004752159118652344 seconds
DEBUG 12-24 11:28:12.372185.372185 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.372446.372446 lmp.py:541] 
DEBUG 12-24 11:28:12.372446.372446 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:12.372621.372621 cuda_h.py:19] end cpu_experts_submit cost 0.00015091896057128906 seconds
DEBUG 12-24 11:28:12.372662.372662 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.372187.372187 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.372571.372571 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.373172.373172 cuda_h.py:19] end allocate_cuda_memory cost 0.00029158592224121094 seconds
DEBUG 12-24 11:28:12.373472.373472 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.373103.373103 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.373992.373992 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.373517.373517 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 42d7d05c-aad7-43e4-9eca-27c6f51f87d6
DEBUG 12-24 11:28:12.373472.373472 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:12.389631.389631 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 42d7d05c-aad7-43e4-9eca-27c6f51f87d6
DEBUG 12-24 11:28:12.389841.389841 mlpmodule.py:630] group tensors cost 0.015451431274414062 s
DEBUG 12-24 11:28:12.390548.390548 cuda_h.py:19] end load_into_gpu_async cost 0.01685047149658203 seconds
DEBUG 12-24 11:28:12.390265.390265 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.390261.390261 cuda_h.py:19] end restore_tensors2 cost 0.0005950927734375 seconds
DEBUG 12-24 11:28:12.391669.391669 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.01849985122680664 seconds
DEBUG 12-24 11:28:12.398689.398689 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.025882959365844727 seconds
DEBUG 12-24 11:28:12.398032.398032 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 12-24 11:28:12.398128.398128 cuda_h.py:10] start start_load_qkvogn_s_weight_l_7
DEBUG 12-24 11:28:12.398108.398108 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 5.316734313964844e-05 seconds
DEBUG 12-24 11:28:12.398309.398309 cuda_h.py:19] end start_load_qkvogn_s_weight_l_7 cost 0.00013899803161621094 seconds
DEBUG 12-24 11:28:12.398861.398861 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.399133.399133 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.399896.399896 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.399710.399710 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.400586.400586 cuda_h.py:19] end allocate_cuda_memory cost 0.0011317729949951172 seconds
DEBUG 12-24 11:28:12.401755.401755 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.401780.401780 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.401687.401687 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.401941.401941 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c8706c37-130f-461b-83cc-268ac38916a1
DEBUG 12-24 11:28:12.401207.401207 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.402934.402934 cuda_h.py:19] end gpu_sexperts cost 0.003276824951171875 seconds
DEBUG 12-24 11:28:12.402825.402825 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.402847.402847 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 42d7d05c-aad7-43e4-9eca-27c6f51f87d6
INFO 12-24 11:28:12.403384.403384 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c8706c37-130f-461b-83cc-268ac38916a1
DEBUG 12-24 11:28:12.403377.403377 cuda_h.py:19] end load_into_gpu_async cost 0.0021927356719970703 seconds
DEBUG 12-24 11:28:12.403288.403288 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.403309.403309 cuda_h.py:19] end restore_tensors2 cost 0.0001461505889892578 seconds
DEBUG 12-24 11:28:12.403928.403928 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004384517669677734 seconds
INFO 12-24 11:28:12.405844.405844 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c8706c37-130f-461b-83cc-268ac38916a1
DEBUG 12-24 11:28:12.405082.405082 mlpmodule.py:668] pad cost 0.015656471252441406 s
DEBUG 12-24 11:28:12.405993.405993 mlpmodule.py:674] create cpu tensor cost 4.3392181396484375e-05 s
DEBUG 12-24 11:28:12.405896.405896 mlpmodule.py:679] move to cpu cost 3.147125244140625e-05 s
DEBUG 12-24 11:28:12.417320.417320 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:12.417179.417179 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.417487.417487 mlpmodule.py:700] group_w3 first element: -0.00604248046875
WARNING 12-24 11:28:12.417550.417550 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.426933.426933 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.426298.426298 mlpmodule.py:719] start einsum3
INFO 12-24 11:28:12.434336.434336 client.py:127] Model loaded
DEBUG 12-24 11:28:12.434352.434352 cuda_h.py:19] end wait_experts cost 0.03204607963562012 seconds
DEBUG 12-24 11:28:12.434004.434004 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.434411.434411 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:12.436264.436264 mlpmodule.py:457] gpu group tensors cost 0.0015499591827392578 s
DEBUG 12-24 11:28:12.436322.436322 mlpmodule.py:723] group einsum cost 0.030666112899780273 s
DEBUG 12-24 11:28:12.437236.437236 mlpmodule.py:731] cpy2cputensor cost 0.0003876686096191406 s
INFO 12-24 11:28:12.439829.439829 client.py:127] Model loaded
DEBUG 12-24 11:28:12.439992.439992 cuda_h.py:19] end sllm_worker_task cost 0.04045462608337402 seconds
DEBUG 12-24 11:28:12.443104.443104 mlpmodule.py:490] gpu pad cost 0.006848573684692383 s
DEBUG 12-24 11:28:12.445369.445369 mlpmodule.py:508] gpu group einsum cost 0.0016198158264160156 s
DEBUG 12-24 11:28:12.451393.451393 mlpmodule.py:537] gpu experts func einsum cost 0.016474485397338867 s
DEBUG 12-24 11:28:12.451867.451867 cuda_h.py:19] end gpu_experts cost 0.01682114601135254 seconds
DEBUG 12-24 11:28:12.451651.451651 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.451792.451792 cuda_h.py:19] end wait_cetm_experts cost 2.09808349609375e-05 seconds
DEBUG 12-24 11:28:12.451515.451515 lmp.py:615] gpu end - einsum end = 5.3ms
DEBUG 12-24 11:28:12.451088.451088 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.451766.451766 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.8848648071289062e-05 seconds
DEBUG 12-24 11:28:12.451344.451344 cuda_h.py:19] end layer_moe_generate_6 cost 0.08619213104248047 seconds
DEBUG 12-24 11:28:12.452512.452512 lmp.py:445] -------------------------------- end layer 6 --------------------------------
DEBUG 12-24 11:28:12.452778.452778 lmp.py:418] -------------------------------- start layer 7 --------------------------------
DEBUG 12-24 11:28:12.452018.452018 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.452046.452046 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.456600.456600 cuda_h.py:19] end self_attn cost 0.0033986568450927734 seconds
DEBUG 12-24 11:28:12.456999.456999 cuda_h.py:19] end iln_self_attn_paln cost 0.004330873489379883 seconds
DEBUG 12-24 11:28:12.456624.456624 cuda_h.py:10] start layer_moe_generate_7
DEBUG 12-24 11:28:12.456870.456870 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.457244.457244 cuda_h.py:19] end gate cost 0.0006258487701416016 seconds
DEBUG 12-24 11:28:12.457603.457603 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.457389.457389 lmp.py:519] 
DEBUG 12-24 11:28:12.457389.457389 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.457099.457099 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:12.457802.457802 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:12.457021.457021 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:12.457664.457664 lmp.py:523] 
DEBUG 12-24 11:28:12.457664.457664 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.457830.457830 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.457480.457480 lmp.py:530]   Expert 40 |     19 | CPU
DEBUG 12-24 11:28:12.457646.457646 lmp.py:530]   Expert  8 |     25 | CPU
DEBUG 12-24 11:28:12.458858.458858 lmp.py:530]   Expert 26 |     32 | CPU
DEBUG 12-24 11:28:12.458594.458594 lmp.py:530]   Expert 15 |     33 | CPU
DEBUG 12-24 11:28:12.458330.458330 lmp.py:530]   Expert 33 |     36 | CPU
DEBUG 12-24 11:28:12.458065.458065 lmp.py:530]   Expert 31 |     37 | CPU
DEBUG 12-24 11:28:12.458616.458616 lmp.py:530]   Expert 48 |     37 | CPU
DEBUG 12-24 11:28:12.458212.458212 lmp.py:530]   Expert 35 |     38 | CPU
DEBUG 12-24 11:28:12.458260.458260 lmp.py:530]   Expert 18 |     44 | CPU
DEBUG 12-24 11:28:12.458095.458095 lmp.py:530]   Expert 49 |     47 | CPU
DEBUG 12-24 11:28:12.458691.458691 lmp.py:530]   Expert 41 |     48 | CPU
DEBUG 12-24 11:28:12.458050.458050 lmp.py:530]   Expert 25 |     49 | CPU
DEBUG 12-24 11:28:12.458646.458646 lmp.py:530]   Expert 29 |     55 | CPU
DEBUG 12-24 11:28:12.458958.458958 lmp.py:530]   Expert 55 |     55 | CPU
DEBUG 12-24 11:28:12.458555.458555 lmp.py:530]   Expert  6 |     57 | CPU
DEBUG 12-24 11:28:12.458913.458913 lmp.py:530]   Expert 56 |     57 | CPU
DEBUG 12-24 11:28:12.458271.458271 lmp.py:530]   Expert 32 |     59 | CPU
DEBUG 12-24 11:28:12.458630.458630 lmp.py:530]   Expert 39 |     61 | CPU
DEBUG 12-24 11:28:12.458988.458988 lmp.py:530]   Expert 16 |     64 | CPU
DEBUG 12-24 11:28:12.458300.458300 lmp.py:530]   Expert 20 |     66 | CPU
DEBUG 12-24 11:28:12.458373.458373 lmp.py:530]   Expert 53 |     66 | CPU
DEBUG 12-24 11:28:12.458970.458970 lmp.py:530]   Expert 17 |     67 | CPU
DEBUG 12-24 11:28:12.458566.458566 lmp.py:530]   Expert 22 |     67 | CPU
DEBUG 12-24 11:28:12.458925.458925 lmp.py:530]   Expert 57 |     67 | CPU
DEBUG 12-24 11:28:12.458283.458283 lmp.py:530]   Expert 36 |     68 | CPU
DEBUG 12-24 11:28:12.458403.458403 lmp.py:530]   Expert 45 |     68 | CPU
DEBUG 12-24 11:28:12.458761.458761 lmp.py:530]   Expert 63 |     72 | CPU
DEBUG 12-24 11:28:12.458119.458119 lmp.py:530]   Expert 43 |     75 | CPU
DEBUG 12-24 11:28:12.458716.458716 lmp.py:530]   Expert 13 |     77 | CPU
DEBUG 12-24 11:28:12.458551.458551 lmp.py:530]   Expert 52 |     77 | CPU
DEBUG 12-24 11:28:12.458148.458148 lmp.py:530]   Expert 30 |     78 | CPU
DEBUG 12-24 11:28:12.458698.458698 lmp.py:530]   Expert 42 |     79 | CPU
DEBUG 12-24 11:28:12.458308.458308 lmp.py:530]   Expert 51 |     80 | GPU
DEBUG 12-24 11:28:12.458574.458574 lmp.py:530]   Expert 58 |     80 | GPU
DEBUG 12-24 11:28:12.458170.458170 lmp.py:530]   Expert 34 |     81 | GPU
DEBUG 12-24 11:28:12.458005.458005 lmp.py:530]   Expert 28 |     82 | GPU
DEBUG 12-24 11:28:12.458364.458364 lmp.py:530]   Expert 54 |     82 | GPU
DEBUG 12-24 11:28:12.458722.458722 lmp.py:530]   Expert 61 |     89 | GPU
DEBUG 12-24 11:28:12.458319.458319 lmp.py:530]   Expert 21 |     90 | GPU
DEBUG 12-24 11:28:12.458438.458438 lmp.py:530]   Expert  7 |     93 | GPU
DEBUG 12-24 11:28:12.458558.458558 lmp.py:530]   Expert 59 |     94 | GPU
DEBUG 12-24 11:28:12.458155.458155 lmp.py:530]   Expert 24 |     95 | GPU
DEBUG 12-24 11:28:12.458513.458513 lmp.py:530]   Expert 60 |     96 | GPU
DEBUG 12-24 11:28:12.458110.458110 lmp.py:530]   Expert  9 |     97 | GPU
DEBUG 12-24 11:28:12.458422.458422 lmp.py:530]   Expert 37 |     97 | GPU
DEBUG 12-24 11:28:12.458257.458257 lmp.py:530]   Expert 10 |     98 | GPU
DEBUG 12-24 11:28:12.458853.458853 lmp.py:530]   Expert 27 |    101 | GPU
DEBUG 12-24 11:28:12.458973.458973 lmp.py:530]   Expert 38 |    101 | GPU
DEBUG 12-24 11:28:12.458093.458093 lmp.py:530]   Expert 19 |    103 | GPU
DEBUG 12-24 11:28:12.458690.458690 lmp.py:530]   Expert 47 |    108 | GPU
DEBUG 12-24 11:28:12.458333.458333 lmp.py:530]   Expert 12 |    114 | GPU
DEBUG 12-24 11:28:12.458214.458214 lmp.py:530]   Expert 44 |    118 | GPU
DEBUG 12-24 11:28:12.458572.458572 lmp.py:530]   Expert 23 |    121 | GPU
DEBUG 12-24 11:28:12.458930.458930 lmp.py:530]   Expert 14 |    140 | GPU
DEBUG 12-24 11:28:12.458573.458573 lmp.py:530]   Expert 62 |    159 | GPU
DEBUG 12-24 11:28:12.459647.459647 lmp.py:530]   Expert 50 |    166 | GPU
DEBUG 12-24 11:28:12.459005.459005 lmp.py:530]   Expert 11 |    171 | GPU
DEBUG 12-24 11:28:12.459317.459317 lmp.py:530]   Expert 46 |    224 | GPU
DEBUG 12-24 11:28:12.459390.459390 lmp.py:530]   Expert  1 |   1230 | GPU
DEBUG 12-24 11:28:12.459749.459749 lmp.py:530]   Expert  3 |   1236 | GPU
DEBUG 12-24 11:28:12.459345.459345 lmp.py:530]   Expert  0 |   1261 | GPU
DEBUG 12-24 11:28:12.459180.459180 lmp.py:530]   Expert  5 |   1262 | GPU
DEBUG 12-24 11:28:12.459539.459539 lmp.py:530]   Expert  4 |   1311 | GPU
DEBUG 12-24 11:28:12.459420.459420 lmp.py:530]   Expert  2 |   1328 | GPU
DEBUG 12-24 11:28:12.459494.459494 lmp.py:531] 
DEBUG 12-24 11:28:12.459494.459494 lmp.py:531]   CPU total tokens: 1780 (14.5%)
DEBUG 12-24 11:28:12.459282.459282 lmp.py:532]   GPU total tokens: 10508 (85.5%)
DEBUG 12-24 11:28:12.459839.459839 cuda_h.py:19] end experts_map_get cost 0.0016889572143554688 seconds
DEBUG 12-24 11:28:12.459867.459867 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.459320.459320 lmp.py:541] 
DEBUG 12-24 11:28:12.459320.459320 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:12.459984.459984 cuda_h.py:19] end cpu_experts_submit cost 0.0001220703125 seconds
DEBUG 12-24 11:28:12.459442.459442 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.459894.459894 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.459575.459575 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.465748.465748 cuda_h.py:19] end allocate_cuda_memory cost 0.005721569061279297 seconds
DEBUG 12-24 11:28:12.465937.465937 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.465177.465177 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.465715.465715 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.465133.465133 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f92800b4-4ae1-46f2-8bde-b39e61529530
DEBUG 12-24 11:28:12.465961.465961 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.465728.465728 mlpmodule.py:588]  experts func einsum cost 0.09220576286315918 s
INFO 12-24 11:28:12.473670.473670 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f92800b4-4ae1-46f2-8bde-b39e61529530
DEBUG 12-24 11:28:12.473701.473701 mlpmodule.py:630] group tensors cost 0.007027626037597656 s
DEBUG 12-24 11:28:12.474256.474256 cuda_h.py:19] end load_into_gpu_async cost 0.00847172737121582 seconds
DEBUG 12-24 11:28:12.474413.474413 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.474105.474105 cuda_h.py:19] end restore_tensors2 cost 0.0004324913024902344 seconds
DEBUG 12-24 11:28:12.474901.474901 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.01521921157836914 seconds
DEBUG 12-24 11:28:12.479713.479713 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.020110368728637695 seconds
DEBUG 12-24 11:28:12.479530.479530 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 12-24 11:28:12.479174.479174 cuda_h.py:10] start start_load_qkvogn_s_weight_l_8
DEBUG 12-24 11:28:12.479084.479084 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 3.528594970703125e-05 seconds
DEBUG 12-24 11:28:12.479515.479515 cuda_h.py:19] end start_load_qkvogn_s_weight_l_8 cost 7.557868957519531e-05 seconds
DEBUG 12-24 11:28:12.479887.479887 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.479369.479369 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.480913.480913 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.480880.480880 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.481223.481223 cuda_h.py:19] end allocate_cuda_memory cost 0.0008144378662109375 seconds
DEBUG 12-24 11:28:12.481175.481175 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.481636.481636 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.481739.481739 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.481703.481703 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9fb9913b-5195-4a3b-b47d-45a86b3fbf6f
DEBUG 12-24 11:28:12.482736.482736 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.482879.482879 cuda_h.py:19] end gpu_sexperts cost 0.002679586410522461 seconds
DEBUG 12-24 11:28:12.482221.482221 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.482905.482905 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f92800b4-4ae1-46f2-8bde-b39e61529530
INFO 12-24 11:28:12.483861.483861 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9fb9913b-5195-4a3b-b47d-45a86b3fbf6f
DEBUG 12-24 11:28:12.483767.483767 cuda_h.py:19] end load_into_gpu_async cost 0.0019516944885253906 seconds
DEBUG 12-24 11:28:12.483751.483751 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.483771.483771 cuda_h.py:19] end restore_tensors2 cost 0.00014734268188476562 seconds
DEBUG 12-24 11:28:12.484835.484835 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003668546676635742 seconds
INFO 12-24 11:28:12.485460.485460 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9fb9913b-5195-4a3b-b47d-45a86b3fbf6f
DEBUG 12-24 11:28:12.485990.485990 mlpmodule.py:668] pad cost 0.011787891387939453 s
DEBUG 12-24 11:28:12.485987.485987 mlpmodule.py:674] create cpu tensor cost 4.00543212890625e-05 s
DEBUG 12-24 11:28:12.486930.486930 mlpmodule.py:679] move to cpu cost 2.8371810913085938e-05 s
DEBUG 12-24 11:28:12.496168.496168 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:12.497980.497980 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.497644.497644 mlpmodule.py:700] group_w3 first element: -0.08056640625
WARNING 12-24 11:28:12.497118.497118 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.505789.505789 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.506300.506300 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.515305.515305 mlpmodule.py:723] group einsum cost 0.029139041900634766 s
DEBUG 12-24 11:28:12.515529.515529 mlpmodule.py:731] cpy2cputensor cost 0.00040459632873535156 s
INFO 12-24 11:28:12.527976.527976 client.py:127] Model loaded
DEBUG 12-24 11:28:12.527239.527239 cuda_h.py:19] end wait_experts cost 0.04478931427001953 seconds
DEBUG 12-24 11:28:12.527671.527671 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.527970.527970 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:12.528051.528051 mlpmodule.py:457] gpu group tensors cost 0.0007293224334716797 s
DEBUG 12-24 11:28:12.530879.530879 mlpmodule.py:490] gpu pad cost 0.0017042160034179688 s
DEBUG 12-24 11:28:12.530747.530747 mlpmodule.py:508] gpu group einsum cost 0.0005388259887695312 s
INFO 12-24 11:28:12.533488.533488 client.py:127] Model loaded
DEBUG 12-24 11:28:12.533574.533574 cuda_h.py:19] end sllm_worker_task cost 0.053580522537231445 seconds
DEBUG 12-24 11:28:12.534019.534019 mlpmodule.py:537] gpu experts func einsum cost 0.006665229797363281 s
DEBUG 12-24 11:28:12.534202.534202 cuda_h.py:19] end gpu_experts cost 0.006918668746948242 seconds
DEBUG 12-24 11:28:12.534766.534766 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.534218.534218 cuda_h.py:19] end wait_cetm_experts cost 2.193450927734375e-05 seconds
DEBUG 12-24 11:28:12.534490.534490 lmp.py:615] gpu end - einsum end = 14.5ms
DEBUG 12-24 11:28:12.534969.534969 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.534229.534229 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.0742416381835938e-05 seconds
DEBUG 12-24 11:28:12.534787.534787 cuda_h.py:19] end layer_moe_generate_7 cost 0.07800459861755371 seconds
DEBUG 12-24 11:28:12.534779.534779 lmp.py:445] -------------------------------- end layer 7 --------------------------------
DEBUG 12-24 11:28:12.534966.534966 lmp.py:418] -------------------------------- start layer 8 --------------------------------
DEBUG 12-24 11:28:12.535277.535277 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.535658.535658 cuda_h.py:10] start self_attn
DEBUG 12-24 11:28:12.537653.537653 mlpmodule.py:588]  experts func einsum cost 0.0714726448059082 s
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.538450.538450 cuda_h.py:19] end self_attn cost 0.0034296512603759766 seconds
DEBUG 12-24 11:28:12.539369.539369 cuda_h.py:19] end iln_self_attn_paln cost 0.004075765609741211 seconds
DEBUG 12-24 11:28:12.539165.539165 cuda_h.py:10] start layer_moe_generate_8
DEBUG 12-24 11:28:12.539882.539882 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.539698.539698 cuda_h.py:19] end gate cost 0.0006012916564941406 seconds
DEBUG 12-24 11:28:12.539249.539249 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.540782.540782 lmp.py:519] 
DEBUG 12-24 11:28:12.540782.540782 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.540776.540776 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:12.540473.540473 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:12.540784.540784 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:12.540712.540712 lmp.py:523] 
DEBUG 12-24 11:28:12.540712.540712 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.540117.540117 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.540767.540767 lmp.py:530]   Expert 12 |      5 | CPU
DEBUG 12-24 11:28:12.540933.540933 lmp.py:530]   Expert 59 |      6 | CPU
DEBUG 12-24 11:28:12.540622.540622 lmp.py:530]   Expert  7 |      7 | CPU
DEBUG 12-24 11:28:12.540073.540073 lmp.py:530]   Expert 27 |      7 | CPU
DEBUG 12-24 11:28:12.540762.540762 lmp.py:530]   Expert 30 |      9 | CPU
DEBUG 12-24 11:28:12.540372.540372 lmp.py:530]   Expert 38 |     10 | CPU
DEBUG 12-24 11:28:12.540777.540777 lmp.py:530]   Expert 22 |     13 | CPU
DEBUG 12-24 11:28:12.540466.540466 lmp.py:530]   Expert 39 |     13 | CPU
DEBUG 12-24 11:28:12.540156.540156 lmp.py:530]   Expert 14 |     15 | CPU
DEBUG 12-24 11:28:12.540560.540560 lmp.py:530]   Expert 50 |     22 | CPU
DEBUG 12-24 11:28:12.540249.540249 lmp.py:530]   Expert 33 |     23 | CPU
DEBUG 12-24 11:28:12.540462.540462 lmp.py:530]   Expert 40 |     24 | CPU
DEBUG 12-24 11:28:12.540197.540197 lmp.py:530]   Expert 34 |     25 | CPU
DEBUG 12-24 11:28:12.540172.540172 lmp.py:530]   Expert 13 |     26 | CPU
DEBUG 12-24 11:28:12.540146.540146 lmp.py:530]   Expert 26 |     27 | CPU
DEBUG 12-24 11:28:12.540881.540881 lmp.py:530]   Expert 53 |     28 | CPU
DEBUG 12-24 11:28:12.540378.540378 lmp.py:530]   Expert 17 |     31 | CPU
DEBUG 12-24 11:28:12.540114.540114 lmp.py:530]   Expert 36 |     33 | CPU
DEBUG 12-24 11:28:12.540088.540088 lmp.py:530]   Expert 44 |     34 | CPU
DEBUG 12-24 11:28:12.540062.540062 lmp.py:530]   Expert  8 |     35 | CPU
DEBUG 12-24 11:28:12.540513.540513 lmp.py:530]   Expert 32 |     39 | CPU
DEBUG 12-24 11:28:12.540202.540202 lmp.py:530]   Expert 54 |     40 | CPU
DEBUG 12-24 11:28:12.540653.540653 lmp.py:530]   Expert 60 |     42 | CPU
DEBUG 12-24 11:28:12.540104.540104 lmp.py:530]   Expert 15 |     43 | CPU
DEBUG 12-24 11:28:12.540555.540555 lmp.py:530]   Expert 19 |     46 | CPU
DEBUG 12-24 11:28:12.540006.540006 lmp.py:530]   Expert 58 |     46 | CPU
DEBUG 12-24 11:28:12.540218.540218 lmp.py:530]   Expert 61 |     46 | CPU
DEBUG 12-24 11:28:12.540716.540716 lmp.py:530]   Expert 56 |     48 | CPU
DEBUG 12-24 11:28:12.540690.540690 lmp.py:530]   Expert 23 |     50 | CPU
DEBUG 12-24 11:28:12.540425.540425 lmp.py:530]   Expert 51 |     50 | CPU
DEBUG 12-24 11:28:12.540161.540161 lmp.py:530]   Expert 29 |     52 | CPU
DEBUG 12-24 11:28:12.540658.540658 lmp.py:530]   Expert 20 |     53 | CPU
DEBUG 12-24 11:28:12.540632.540632 lmp.py:530]   Expert 10 |     54 | GPU
DEBUG 12-24 11:28:12.540129.540129 lmp.py:530]   Expert 11 |     54 | GPU
DEBUG 12-24 11:28:12.540103.540103 lmp.py:530]   Expert 37 |     55 | GPU
DEBUG 12-24 11:28:12.540554.540554 lmp.py:530]   Expert 57 |     56 | GPU
DEBUG 12-24 11:28:12.540528.540528 lmp.py:530]   Expert 55 |     61 | GPU
DEBUG 12-24 11:28:12.540979.540979 lmp.py:530]   Expert 21 |     62 | GPU
DEBUG 12-24 11:28:12.540668.540668 lmp.py:530]   Expert 31 |     62 | GPU
DEBUG 12-24 11:28:12.540881.540881 lmp.py:530]   Expert 41 |     62 | GPU
DEBUG 12-24 11:28:12.540855.540855 lmp.py:530]   Expert 18 |     63 | GPU
DEBUG 12-24 11:28:12.540352.540352 lmp.py:530]   Expert 47 |     63 | GPU
DEBUG 12-24 11:28:12.541088.541088 lmp.py:530]   Expert 28 |     67 | GPU
DEBUG 12-24 11:28:12.541300.541300 lmp.py:530]   Expert 42 |     67 | GPU
DEBUG 12-24 11:28:12.541797.541797 lmp.py:530]   Expert 52 |     68 | GPU
DEBUG 12-24 11:28:12.541533.541533 lmp.py:530]   Expert  9 |     70 | GPU
DEBUG 12-24 11:28:12.541792.541792 lmp.py:530]   Expert 16 |     75 | GPU
DEBUG 12-24 11:28:12.541527.541527 lmp.py:530]   Expert 25 |     77 | GPU
DEBUG 12-24 11:28:12.541025.541025 lmp.py:530]   Expert 62 |     83 | GPU
DEBUG 12-24 11:28:12.541237.541237 lmp.py:530]   Expert 43 |     86 | GPU
DEBUG 12-24 11:28:12.541450.541450 lmp.py:530]   Expert 45 |     88 | GPU
DEBUG 12-24 11:28:12.541900.541900 lmp.py:530]   Expert 63 |     91 | GPU
DEBUG 12-24 11:28:12.541113.541113 lmp.py:530]   Expert 24 |     94 | GPU
DEBUG 12-24 11:28:12.541802.541802 lmp.py:530]   Expert 35 |     95 | GPU
DEBUG 12-24 11:28:12.541538.541538 lmp.py:530]   Expert 49 |     96 | GPU
DEBUG 12-24 11:28:12.541274.541274 lmp.py:530]   Expert 46 |    129 | GPU
DEBUG 12-24 11:28:12.541771.541771 lmp.py:530]   Expert  6 |    153 | GPU
DEBUG 12-24 11:28:12.541983.541983 lmp.py:530]   Expert 48 |    164 | GPU
DEBUG 12-24 11:28:12.541719.541719 lmp.py:530]   Expert  2 |   1521 | GPU
DEBUG 12-24 11:28:12.541693.541693 lmp.py:530]   Expert  1 |   1533 | GPU
DEBUG 12-24 11:28:12.541190.541190 lmp.py:530]   Expert  4 |   1535 | GPU
DEBUG 12-24 11:28:12.541403.541403 lmp.py:530]   Expert  0 |   1546 | GPU
DEBUG 12-24 11:28:12.541377.541377 lmp.py:530]   Expert  3 |   1550 | GPU
DEBUG 12-24 11:28:12.541112.541112 lmp.py:530]   Expert  5 |   1560 | GPU
DEBUG 12-24 11:28:12.541801.541801 lmp.py:531] 
DEBUG 12-24 11:28:12.541801.541801 lmp.py:531]   CPU total tokens: 948 (7.7%)
DEBUG 12-24 11:28:12.541491.541491 lmp.py:532]   GPU total tokens: 11340 (92.3%)
DEBUG 12-24 11:28:12.541425.541425 cuda_h.py:19] end experts_map_get cost 0.001468658447265625 seconds
DEBUG 12-24 11:28:12.541545.541545 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.541210.541210 lmp.py:541] 
DEBUG 12-24 11:28:12.541210.541210 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:12.541437.541437 cuda_h.py:19] end cpu_experts_submit cost 0.00010919570922851562 seconds
DEBUG 12-24 11:28:12.541517.541517 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.541254.541254 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.541060.541060 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.542937.542937 cuda_h.py:19] end allocate_cuda_memory cost 0.0002601146697998047 seconds
DEBUG 12-24 11:28:12.542979.542979 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.542643.542643 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.542227.542227 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.542884.542884 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 15f070e9-c166-4ae8-bf39-4363fe8a024f
DEBUG 12-24 11:28:12.542810.542810 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:12.554707.554707 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 15f070e9-c166-4ae8-bf39-4363fe8a024f
DEBUG 12-24 11:28:12.554373.554373 mlpmodule.py:630] group tensors cost 0.011997461318969727 s
DEBUG 12-24 11:28:12.555911.555911 cuda_h.py:19] end load_into_gpu_async cost 0.013348817825317383 seconds
DEBUG 12-24 11:28:12.555250.555250 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.556053.556053 cuda_h.py:19] end restore_tensors2 cost 0.0004088878631591797 seconds
DEBUG 12-24 11:28:12.556134.556134 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.014596223831176758 seconds
DEBUG 12-24 11:28:12.561120.561120 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.019546031951904297 seconds
DEBUG 12-24 11:28:12.561812.561812 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 12-24 11:28:12.561601.561601 cuda_h.py:10] start start_load_qkvogn_s_weight_l_9
DEBUG 12-24 11:28:12.561504.561504 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 3.1948089599609375e-05 seconds
DEBUG 12-24 11:28:12.561029.561029 cuda_h.py:19] end start_load_qkvogn_s_weight_l_9 cost 7.009506225585938e-05 seconds
DEBUG 12-24 11:28:12.561685.561685 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.561993.561993 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.561646.561646 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.561035.561035 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.563966.563966 cuda_h.py:19] end allocate_cuda_memory cost 0.0009188652038574219 seconds
DEBUG 12-24 11:28:12.563218.563218 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.563115.563115 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.563063.563063 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.563146.563146 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 01c1ec24-9a7a-443f-b473-f804f6a39fd9
DEBUG 12-24 11:28:12.563709.563709 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.564968.564968 cuda_h.py:19] end gpu_sexperts cost 0.002687215805053711 seconds
DEBUG 12-24 11:28:12.564384.564384 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.564200.564200 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 15f070e9-c166-4ae8-bf39-4363fe8a024f
INFO 12-24 11:28:12.565590.565590 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 01c1ec24-9a7a-443f-b473-f804f6a39fd9
DEBUG 12-24 11:28:12.565012.565012 cuda_h.py:19] end load_into_gpu_async cost 0.0020608901977539062 seconds
DEBUG 12-24 11:28:12.565361.565361 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.565025.565025 cuda_h.py:19] end restore_tensors2 cost 0.000152587890625 seconds
DEBUG 12-24 11:28:12.565637.565637 mlpmodule.py:668] pad cost 0.010334491729736328 s
DEBUG 12-24 11:28:12.565434.565434 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004060983657836914 seconds
DEBUG 12-24 11:28:12.566048.566048 mlpmodule.py:674] create cpu tensor cost 5.1021575927734375e-05 s
DEBUG 12-24 11:28:12.566225.566225 mlpmodule.py:679] move to cpu cost 0.000152587890625 s
INFO 12-24 11:28:12.567290.567290 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 01c1ec24-9a7a-443f-b473-f804f6a39fd9
DEBUG 12-24 11:28:12.572282.572282 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:12.572092.572092 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.572710.572710 mlpmodule.py:700] group_w3 first element: 0.048583984375
WARNING 12-24 11:28:12.572754.572754 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.577150.577150 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.578856.578856 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.584566.584566 mlpmodule.py:723] group einsum cost 0.017637252807617188 s
DEBUG 12-24 11:28:12.584496.584496 mlpmodule.py:731] cpy2cputensor cost 0.0003898143768310547 s
INFO 12-24 11:28:12.597926.597926 client.py:127] Model loaded
DEBUG 12-24 11:28:12.597336.597336 cuda_h.py:19] end wait_experts cost 0.033300161361694336 seconds
DEBUG 12-24 11:28:12.597244.597244 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.597246.597246 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:12.598047.598047 mlpmodule.py:457] gpu group tensors cost 0.0006992816925048828 s
DEBUG 12-24 11:28:12.603937.603937 mlpmodule.py:490] gpu pad cost 0.0045583248138427734 s
INFO 12-24 11:28:12.603152.603152 client.py:127] Model loaded
DEBUG 12-24 11:28:12.603075.603075 cuda_h.py:19] end sllm_worker_task cost 0.042104244232177734 seconds
DEBUG 12-24 11:28:12.607314.607314 mlpmodule.py:588]  experts func einsum cost 0.06524419784545898 s
DEBUG 12-24 11:28:12.608645.608645 mlpmodule.py:508] gpu group einsum cost 0.004648447036743164 s
DEBUG 12-24 11:28:12.611061.611061 mlpmodule.py:537] gpu experts func einsum cost 0.012964010238647461 s
DEBUG 12-24 11:28:12.611282.611282 cuda_h.py:19] end gpu_experts cost 0.013165950775146484 seconds
DEBUG 12-24 11:28:12.611085.611085 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.611974.611974 cuda_h.py:19] end wait_cetm_experts cost 1.9073486328125e-05 seconds
DEBUG 12-24 11:28:12.611293.611293 lmp.py:615] gpu end - einsum end = 22.2ms
DEBUG 12-24 11:28:12.611341.611341 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.611932.611932 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8835067749023438e-05 seconds
DEBUG 12-24 11:28:12.611582.611582 cuda_h.py:19] end layer_moe_generate_8 cost 0.07215452194213867 seconds
DEBUG 12-24 11:28:12.611019.611019 lmp.py:445] -------------------------------- end layer 8 --------------------------------
DEBUG 12-24 11:28:12.611775.611775 lmp.py:418] -------------------------------- start layer 9 --------------------------------
DEBUG 12-24 11:28:12.611848.611848 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.611837.611837 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.614112.614112 cuda_h.py:19] end self_attn cost 0.002218484878540039 seconds
DEBUG 12-24 11:28:12.614326.614326 cuda_h.py:19] end iln_self_attn_paln cost 0.002782106399536133 seconds
DEBUG 12-24 11:28:12.614163.614163 cuda_h.py:10] start layer_moe_generate_9
DEBUG 12-24 11:28:12.614210.614210 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.615781.615781 cuda_h.py:19] end gate cost 0.0005981922149658203 seconds
DEBUG 12-24 11:28:12.615657.615657 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.615077.615077 lmp.py:519] 
DEBUG 12-24 11:28:12.615077.615077 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.615402.615402 lmp.py:520]   Total experts: 63
DEBUG 12-24 11:28:12.615052.615052 lmp.py:521]   CPU experts: 31 (49%)
DEBUG 12-24 11:28:12.615649.615649 lmp.py:522]   GPU experts: 32 (51%)
DEBUG 12-24 11:28:12.615338.615338 lmp.py:523] 
DEBUG 12-24 11:28:12.615338.615338 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.615789.615789 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.615723.615723 lmp.py:530]   Expert 60 |      9 | CPU
DEBUG 12-24 11:28:12.615413.615413 lmp.py:530]   Expert 17 |     11 | CPU
DEBUG 12-24 11:28:12.615387.615387 lmp.py:530]   Expert 54 |     13 | CPU
DEBUG 12-24 11:28:12.615361.615361 lmp.py:530]   Expert  7 |     14 | CPU
DEBUG 12-24 11:28:12.615619.615619 lmp.py:530]   Expert 26 |     14 | CPU
DEBUG 12-24 11:28:12.615309.615309 lmp.py:530]   Expert  8 |     16 | CPU
DEBUG 12-24 11:28:12.615760.615760 lmp.py:530]   Expert 25 |     18 | CPU
DEBUG 12-24 11:28:12.615926.615926 lmp.py:530]   Expert 13 |     20 | CPU
DEBUG 12-24 11:28:12.615615.615615 lmp.py:530]   Expert 48 |     20 | CPU
DEBUG 12-24 11:28:12.615351.615351 lmp.py:530]   Expert 56 |     20 | CPU
DEBUG 12-24 11:28:12.615086.615086 lmp.py:530]   Expert 39 |     22 | CPU
DEBUG 12-24 11:28:12.615583.615583 lmp.py:530]   Expert 45 |     23 | CPU
DEBUG 12-24 11:28:12.615081.615081 lmp.py:530]   Expert 11 |     25 | CPU
DEBUG 12-24 11:28:12.615339.615339 lmp.py:530]   Expert 42 |     25 | CPU
DEBUG 12-24 11:28:12.615314.615314 lmp.py:530]   Expert 47 |     27 | CPU
DEBUG 12-24 11:28:12.615811.615811 lmp.py:530]   Expert 33 |     29 | CPU
DEBUG 12-24 11:28:12.615070.615070 lmp.py:530]   Expert 29 |     30 | CPU
DEBUG 12-24 11:28:12.615090.615090 lmp.py:530]   Expert 62 |     33 | CPU
DEBUG 12-24 11:28:12.615779.615779 lmp.py:530]   Expert 28 |     34 | CPU
DEBUG 12-24 11:28:12.615038.615038 lmp.py:530]   Expert  6 |     35 | CPU
DEBUG 12-24 11:28:12.615727.615727 lmp.py:530]   Expert 12 |     35 | CPU
DEBUG 12-24 11:28:12.615178.615178 lmp.py:530]   Expert 19 |     36 | CPU
DEBUG 12-24 11:28:12.615391.615391 lmp.py:530]   Expert 27 |     36 | CPU
DEBUG 12-24 11:28:12.615126.615126 lmp.py:530]   Expert 38 |     37 | CPU
DEBUG 12-24 11:28:12.615385.615385 lmp.py:530]   Expert 52 |     38 | CPU
DEBUG 12-24 11:28:12.615644.615644 lmp.py:530]   Expert 23 |     39 | CPU
DEBUG 12-24 11:28:12.615903.615903 lmp.py:530]   Expert 31 |     50 | CPU
DEBUG 12-24 11:28:12.615161.615161 lmp.py:530]   Expert 18 |     51 | CPU
DEBUG 12-24 11:28:12.616897.616897 lmp.py:530]   Expert 16 |     53 | CPU
DEBUG 12-24 11:28:12.616917.616917 lmp.py:530]   Expert 22 |     53 | CPU
DEBUG 12-24 11:28:12.616415.616415 lmp.py:530]   Expert 43 |     58 | CPU
DEBUG 12-24 11:28:12.616435.616435 lmp.py:530]   Expert 14 |     59 | GPU
DEBUG 12-24 11:28:12.616932.616932 lmp.py:530]   Expert 49 |     59 | GPU
DEBUG 12-24 11:28:12.616145.616145 lmp.py:530]   Expert 55 |     59 | GPU
DEBUG 12-24 11:28:12.616596.616596 lmp.py:530]   Expert 20 |     60 | GPU
DEBUG 12-24 11:28:12.616046.616046 lmp.py:530]   Expert 51 |     60 | GPU
DEBUG 12-24 11:28:12.616259.616259 lmp.py:530]   Expert 59 |     61 | GPU
DEBUG 12-24 11:28:12.616710.616710 lmp.py:530]   Expert 30 |     63 | GPU
DEBUG 12-24 11:28:12.616445.616445 lmp.py:530]   Expert 61 |     63 | GPU
DEBUG 12-24 11:28:12.616466.616466 lmp.py:530]   Expert 34 |     64 | GPU
DEBUG 12-24 11:28:12.616725.616725 lmp.py:530]   Expert 44 |     65 | GPU
DEBUG 12-24 11:28:12.616983.616983 lmp.py:530]   Expert 37 |     71 | GPU
DEBUG 12-24 11:28:12.616242.616242 lmp.py:530]   Expert 58 |     71 | GPU
DEBUG 12-24 11:28:12.616501.616501 lmp.py:530]   Expert 40 |     74 | GPU
DEBUG 12-24 11:28:12.616760.616760 lmp.py:530]   Expert 57 |     74 | GPU
DEBUG 12-24 11:28:12.616780.616780 lmp.py:530]   Expert 10 |     75 | GPU
DEBUG 12-24 11:28:12.616039.616039 lmp.py:530]   Expert 46 |     86 | GPU
DEBUG 12-24 11:28:12.616536.616536 lmp.py:530]   Expert 21 |     90 | GPU
DEBUG 12-24 11:28:12.616987.616987 lmp.py:530]   Expert 41 |     90 | GPU
DEBUG 12-24 11:28:12.616676.616676 lmp.py:530]   Expert 53 |     92 | GPU
DEBUG 12-24 11:28:12.616889.616889 lmp.py:530]   Expert  9 |     93 | GPU
DEBUG 12-24 11:28:12.616055.616055 lmp.py:530]   Expert 32 |     94 | GPU
DEBUG 12-24 11:28:12.616420.616420 lmp.py:530]   Expert 50 |     95 | GPU
DEBUG 12-24 11:28:12.616130.616130 lmp.py:530]   Expert 63 |    100 | GPU
DEBUG 12-24 11:28:12.616104.616104 lmp.py:530]   Expert 24 |    107 | GPU
DEBUG 12-24 11:28:12.616316.616316 lmp.py:530]   Expert 36 |    149 | GPU
DEBUG 12-24 11:28:12.616290.616290 lmp.py:530]   Expert 15 |    267 | GPU
DEBUG 12-24 11:28:12.616264.616264 lmp.py:530]   Expert  2 |   1475 | GPU
DEBUG 12-24 11:28:12.616000.616000 lmp.py:530]   Expert  5 |   1489 | GPU
DEBUG 12-24 11:28:12.616212.616212 lmp.py:530]   Expert  3 |   1518 | GPU
DEBUG 12-24 11:28:12.616378.616378 lmp.py:530]   Expert  0 |   1531 | GPU
DEBUG 12-24 11:28:12.616544.616544 lmp.py:530]   Expert  1 |   1544 | GPU
DEBUG 12-24 11:28:12.616472.616472 lmp.py:530]   Expert  4 |   1566 | GPU
DEBUG 12-24 11:28:12.616354.616354 lmp.py:531] 
DEBUG 12-24 11:28:12.616354.616354 lmp.py:531]   CPU total tokens: 924 (7.5%)
DEBUG 12-24 11:28:12.616235.616235 lmp.py:532]   GPU total tokens: 11364 (92.5%)
DEBUG 12-24 11:28:12.616931.616931 cuda_h.py:19] end experts_map_get cost 0.0014383792877197266 seconds
DEBUG 12-24 11:28:12.616097.616097 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.616093.616093 lmp.py:541] 
DEBUG 12-24 11:28:12.616093.616093 lmp.py:541]   Computing 31 experts on CPU...
DEBUG 12-24 11:28:12.616029.616029 cuda_h.py:19] end cpu_experts_submit cost 0.00010371208190917969 seconds
DEBUG 12-24 11:28:12.616632.616632 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.616276.616276 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.616850.616850 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.617772.617772 cuda_h.py:19] end allocate_cuda_memory cost 0.0001888275146484375 seconds
DEBUG 12-24 11:28:12.617761.617761 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.617186.617186 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.617200.617200 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.617619.617619 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4e30e17a-f815-482c-89f4-0747a91528b3
DEBUG 12-24 11:28:12.617347.617347 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:12.625285.625285 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4e30e17a-f815-482c-89f4-0747a91528b3
DEBUG 12-24 11:28:12.625911.625911 mlpmodule.py:630] group tensors cost 0.00768280029296875 s
DEBUG 12-24 11:28:12.626142.626142 cuda_h.py:19] end load_into_gpu_async cost 0.008977174758911133 seconds
DEBUG 12-24 11:28:12.626975.626975 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.626843.626843 cuda_h.py:19] end restore_tensors2 cost 0.00036025047302246094 seconds
DEBUG 12-24 11:28:12.626388.626388 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.010028362274169922 seconds
DEBUG 12-24 11:28:12.631117.631117 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.014264822006225586 seconds
DEBUG 12-24 11:28:12.631252.631252 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 12-24 11:28:12.631207.631207 cuda_h.py:10] start start_load_qkvogn_s_weight_l_10
DEBUG 12-24 11:28:12.631904.631904 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 2.9087066650390625e-05 seconds
DEBUG 12-24 11:28:12.631097.631097 cuda_h.py:19] end start_load_qkvogn_s_weight_l_10 cost 6.842613220214844e-05 seconds
DEBUG 12-24 11:28:12.631509.631509 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.631831.631831 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.631289.631289 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.631156.631156 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.632387.632387 cuda_h.py:19] end allocate_cuda_memory cost 0.0006437301635742188 seconds
DEBUG 12-24 11:28:12.632017.632017 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.632128.632128 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.633179.633179 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.633190.633190 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c8d2d4f1-8866-436e-bd61-3a4c5474d72d
DEBUG 12-24 11:28:12.633416.633416 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.633014.633014 cuda_h.py:19] end gpu_sexperts cost 0.002397298812866211 seconds
DEBUG 12-24 11:28:12.633727.633727 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.633350.633350 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4e30e17a-f815-482c-89f4-0747a91528b3
DEBUG 12-24 11:28:12.634821.634821 mlpmodule.py:668] pad cost 0.00833749771118164 s
DEBUG 12-24 11:28:12.634083.634083 mlpmodule.py:674] create cpu tensor cost 4.57763671875e-05 s
INFO 12-24 11:28:12.634810.634810 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c8d2d4f1-8866-436e-bd61-3a4c5474d72d
DEBUG 12-24 11:28:12.635390.635390 mlpmodule.py:679] move to cpu cost 0.0002167224884033203 s
DEBUG 12-24 11:28:12.635704.635704 cuda_h.py:19] end load_into_gpu_async cost 0.0021753311157226562 seconds
DEBUG 12-24 11:28:12.635888.635888 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.635385.635385 cuda_h.py:19] end restore_tensors2 cost 0.0001533031463623047 seconds
DEBUG 12-24 11:28:12.635985.635985 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0038182735443115234 seconds
INFO 12-24 11:28:12.637700.637700 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c8d2d4f1-8866-436e-bd61-3a4c5474d72d
DEBUG 12-24 11:28:12.641010.641010 mlpmodule.py:694] group_w3: shape=torch.Size([31, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=89391104
DEBUG 12-24 11:28:12.641290.641290 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.641683.641683 mlpmodule.py:700] group_w3 first element: -0.00640869140625
WARNING 12-24 11:28:12.641958.641958 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.645115.645115 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.645138.645138 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.651111.651111 mlpmodule.py:723] group einsum cost 0.015831947326660156 s
DEBUG 12-24 11:28:12.651105.651105 mlpmodule.py:731] cpy2cputensor cost 0.0003368854522705078 s
DEBUG 12-24 11:28:12.672312.672312 mlpmodule.py:588]  experts func einsum cost 0.05449986457824707 s
INFO 12-24 11:28:12.673912.673912 client.py:127] Model loaded
DEBUG 12-24 11:28:12.673381.673381 cuda_h.py:19] end wait_experts cost 0.03946995735168457 seconds
DEBUG 12-24 11:28:12.673881.673881 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.673420.673420 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:12.675320.675320 mlpmodule.py:457] gpu group tensors cost 0.0014066696166992188 s
INFO 12-24 11:28:12.678161.678161 client.py:127] Model loaded
DEBUG 12-24 11:28:12.678319.678319 cuda_h.py:19] end sllm_worker_task cost 0.04697442054748535 seconds
DEBUG 12-24 11:28:12.679946.679946 mlpmodule.py:490] gpu pad cost 0.0045108795166015625 s
DEBUG 12-24 11:28:12.681896.681896 mlpmodule.py:508] gpu group einsum cost 0.0011172294616699219 s
DEBUG 12-24 11:28:12.685954.685954 mlpmodule.py:537] gpu experts func einsum cost 0.012240171432495117 s
DEBUG 12-24 11:28:12.686839.686839 cuda_h.py:19] end gpu_experts cost 0.012540817260742188 seconds
DEBUG 12-24 11:28:12.686298.686298 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.686486.686486 cuda_h.py:19] end wait_cetm_experts cost 2.0265579223632812e-05 seconds
DEBUG 12-24 11:28:12.686494.686494 lmp.py:615] gpu end - einsum end = 30.4ms
DEBUG 12-24 11:28:12.686259.686259 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.686222.686222 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.765655517578125e-05 seconds
DEBUG 12-24 11:28:12.686614.686614 cuda_h.py:19] end layer_moe_generate_9 cost 0.07200169563293457 seconds
DEBUG 12-24 11:28:12.686782.686782 lmp.py:445] -------------------------------- end layer 9 --------------------------------
DEBUG 12-24 11:28:12.686519.686519 lmp.py:418] -------------------------------- start layer 10 --------------------------------
DEBUG 12-24 11:28:12.686851.686851 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.687746.687746 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.691535.691535 cuda_h.py:19] end self_attn cost 0.004464864730834961 seconds
DEBUG 12-24 11:28:12.692029.692029 cuda_h.py:19] end iln_self_attn_paln cost 0.00582432746887207 seconds
DEBUG 12-24 11:28:12.692783.692783 cuda_h.py:10] start layer_moe_generate_10
DEBUG 12-24 11:28:12.692886.692886 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.694590.694590 cuda_h.py:19] end gate cost 0.0013687610626220703 seconds
DEBUG 12-24 11:28:12.694760.694760 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.695014.695014 lmp.py:519] 
DEBUG 12-24 11:28:12.695014.695014 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.695613.695613 lmp.py:520]   Total experts: 63
DEBUG 12-24 11:28:12.695039.695039 lmp.py:521]   CPU experts: 31 (49%)
DEBUG 12-24 11:28:12.695982.695982 lmp.py:522]   GPU experts: 32 (51%)
DEBUG 12-24 11:28:12.695056.695056 lmp.py:523] 
DEBUG 12-24 11:28:12.695056.695056 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.695369.695369 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.695941.695941 lmp.py:530]   Expert 27 |     11 | CPU
DEBUG 12-24 11:28:12.695447.695447 lmp.py:530]   Expert 61 |     11 | CPU
DEBUG 12-24 11:28:12.695521.695521 lmp.py:530]   Expert 19 |     14 | CPU
DEBUG 12-24 11:28:12.696695.696695 lmp.py:530]   Expert 55 |     14 | CPU
DEBUG 12-24 11:28:12.696816.696816 lmp.py:530]   Expert  7 |     16 | CPU
DEBUG 12-24 11:28:12.696699.696699 lmp.py:530]   Expert 48 |     16 | CPU
DEBUG 12-24 11:28:12.696866.696866 lmp.py:530]   Expert 14 |     18 | CPU
DEBUG 12-24 11:28:12.696603.696603 lmp.py:530]   Expert 47 |     18 | CPU
DEBUG 12-24 11:28:12.696697.696697 lmp.py:530]   Expert 20 |     19 | CPU
DEBUG 12-24 11:28:12.696599.696599 lmp.py:530]   Expert 11 |     21 | CPU
DEBUG 12-24 11:28:12.696024.696024 lmp.py:530]   Expert 21 |     21 | CPU
DEBUG 12-24 11:28:12.696449.696449 lmp.py:530]   Expert 50 |     22 | CPU
DEBUG 12-24 11:28:12.696179.696179 lmp.py:530]   Expert 35 |     23 | CPU
DEBUG 12-24 11:28:12.696226.696226 lmp.py:530]   Expert  6 |     24 | CPU
DEBUG 12-24 11:28:12.696558.696558 lmp.py:530]   Expert 44 |     30 | CPU
DEBUG 12-24 11:28:12.696175.696175 lmp.py:530]   Expert 32 |     34 | CPU
DEBUG 12-24 11:28:12.696316.696316 lmp.py:530]   Expert 56 |     34 | CPU
DEBUG 12-24 11:28:12.696264.696264 lmp.py:530]   Expert 13 |     35 | CPU
DEBUG 12-24 11:28:12.696450.696450 lmp.py:530]   Expert 15 |     37 | CPU
DEBUG 12-24 11:28:12.696259.696259 lmp.py:530]   Expert 54 |     37 | CPU
DEBUG 12-24 11:28:12.696353.696353 lmp.py:530]   Expert 60 |     38 | CPU
DEBUG 12-24 11:28:12.696685.696685 lmp.py:530]   Expert 43 |     39 | CPU
DEBUG 12-24 11:28:12.696826.696826 lmp.py:530]   Expert 18 |     41 | CPU
DEBUG 12-24 11:28:12.696489.696489 lmp.py:530]   Expert 12 |     42 | CPU
DEBUG 12-24 11:28:12.696391.696391 lmp.py:530]   Expert 24 |     42 | CPU
DEBUG 12-24 11:28:12.696293.696293 lmp.py:530]   Expert 26 |     42 | CPU
DEBUG 12-24 11:28:12.696194.696194 lmp.py:530]   Expert 46 |     43 | CPU
DEBUG 12-24 11:28:12.696003.696003 lmp.py:530]   Expert 63 |     43 | CPU
DEBUG 12-24 11:28:12.696097.696097 lmp.py:530]   Expert 45 |     44 | CPU
DEBUG 12-24 11:28:12.696714.696714 lmp.py:530]   Expert 10 |     46 | CPU
DEBUG 12-24 11:28:12.697901.697901 lmp.py:530]   Expert 29 |     50 | CPU
DEBUG 12-24 11:28:12.697564.697564 lmp.py:530]   Expert 30 |     50 | GPU
DEBUG 12-24 11:28:12.697512.697512 lmp.py:530]   Expert 28 |     51 | GPU
DEBUG 12-24 11:28:12.697845.697845 lmp.py:530]   Expert 16 |     52 | GPU
DEBUG 12-24 11:28:12.697938.697938 lmp.py:530]   Expert  9 |     53 | GPU
DEBUG 12-24 11:28:12.697271.697271 lmp.py:530]   Expert 41 |     55 | GPU
DEBUG 12-24 11:28:12.697934.697934 lmp.py:530]   Expert 42 |     55 | GPU
DEBUG 12-24 11:28:12.697597.697597 lmp.py:530]   Expert 31 |     56 | GPU
DEBUG 12-24 11:28:12.697261.697261 lmp.py:530]   Expert 57 |     59 | GPU
DEBUG 12-24 11:28:12.697686.697686 lmp.py:530]   Expert 37 |     60 | GPU
DEBUG 12-24 11:28:12.697303.697303 lmp.py:530]   Expert 62 |     60 | GPU
DEBUG 12-24 11:28:12.697396.697396 lmp.py:530]   Expert 53 |     62 | GPU
DEBUG 12-24 11:28:12.697821.697821 lmp.py:530]   Expert 38 |     63 | GPU
DEBUG 12-24 11:28:12.697246.697246 lmp.py:530]   Expert  8 |     66 | GPU
DEBUG 12-24 11:28:12.697148.697148 lmp.py:530]   Expert 17 |     66 | GPU
DEBUG 12-24 11:28:12.697811.697811 lmp.py:530]   Expert 39 |     67 | GPU
DEBUG 12-24 11:28:12.697760.697760 lmp.py:530]   Expert 36 |     68 | GPU
DEBUG 12-24 11:28:12.697377.697377 lmp.py:530]   Expert 25 |     78 | GPU
DEBUG 12-24 11:28:12.697517.697517 lmp.py:530]   Expert 52 |     78 | GPU
DEBUG 12-24 11:28:12.697134.697134 lmp.py:530]   Expert 58 |     82 | GPU
DEBUG 12-24 11:28:12.697036.697036 lmp.py:530]   Expert 40 |     85 | GPU
DEBUG 12-24 11:28:12.697460.697460 lmp.py:530]   Expert 51 |     87 | GPU
DEBUG 12-24 11:28:12.697647.697647 lmp.py:530]   Expert 23 |     99 | GPU
DEBUG 12-24 11:28:12.697072.697072 lmp.py:530]   Expert 33 |    105 | GPU
DEBUG 12-24 11:28:12.697450.697450 lmp.py:530]   Expert 22 |    109 | GPU
DEBUG 12-24 11:28:12.697068.697068 lmp.py:530]   Expert 49 |    121 | GPU
DEBUG 12-24 11:28:12.697685.697685 lmp.py:530]   Expert 59 |    126 | GPU
DEBUG 12-24 11:28:12.697348.697348 lmp.py:530]   Expert  3 |   1487 | GPU
DEBUG 12-24 11:28:12.697058.697058 lmp.py:530]   Expert  2 |   1553 | GPU
DEBUG 12-24 11:28:12.697721.697721 lmp.py:530]   Expert  0 |   1567 | GPU
DEBUG 12-24 11:28:12.698669.698669 lmp.py:530]   Expert  5 |   1583 | GPU
DEBUG 12-24 11:28:12.698855.698855 lmp.py:530]   Expert  1 |   1598 | GPU
DEBUG 12-24 11:28:12.698757.698757 lmp.py:530]   Expert  4 |   1662 | GPU
DEBUG 12-24 11:28:12.698805.698805 lmp.py:531] 
DEBUG 12-24 11:28:12.698805.698805 lmp.py:531]   CPU total tokens: 925 (7.5%)
DEBUG 12-24 11:28:12.698329.698329 lmp.py:532]   GPU total tokens: 11363 (92.5%)
DEBUG 12-24 11:28:12.698006.698006 cuda_h.py:19] end experts_map_get cost 0.0035085678100585938 seconds
DEBUG 12-24 11:28:12.698484.698484 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.698501.698501 lmp.py:541] 
DEBUG 12-24 11:28:12.698501.698501 lmp.py:541]   Computing 31 experts on CPU...
DEBUG 12-24 11:28:12.698174.698174 cuda_h.py:19] end cpu_experts_submit cost 0.00018405914306640625 seconds
DEBUG 12-24 11:28:12.698089.698089 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.698709.698709 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.698180.698180 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.699984.699984 cuda_h.py:19] end allocate_cuda_memory cost 0.0003857612609863281 seconds
DEBUG 12-24 11:28:12.699841.699841 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.699016.699016 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.699502.699502 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.699331.699331 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1dda0097-26a3-455b-be84-9062d12b38a4
DEBUG 12-24 11:28:12.699601.699601 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.716573.716573 mlpmodule.py:630] group tensors cost 0.016629457473754883 s
INFO 12-24 11:28:12.717039.717039 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1dda0097-26a3-455b-be84-9062d12b38a4
DEBUG 12-24 11:28:12.717384.717384 cuda_h.py:19] end load_into_gpu_async cost 0.01852250099182129 seconds
DEBUG 12-24 11:28:12.718804.718804 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.718047.718047 cuda_h.py:19] end restore_tensors2 cost 0.0006806850433349609 seconds
DEBUG 12-24 11:28:12.718421.718421 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.02022838592529297 seconds
DEBUG 12-24 11:28:12.725519.725519 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.027014493942260742 seconds
DEBUG 12-24 11:28:12.725193.725193 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 12-24 11:28:12.725917.725917 cuda_h.py:10] start start_load_qkvogn_s_weight_l_11
DEBUG 12-24 11:28:12.725344.725344 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 4.482269287109375e-05 seconds
DEBUG 12-24 11:28:12.725658.725658 cuda_h.py:19] end start_load_qkvogn_s_weight_l_11 cost 0.00010561943054199219 seconds
DEBUG 12-24 11:28:12.725612.725612 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.726114.726114 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.726607.726607 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.726333.726333 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.728824.728824 cuda_h.py:19] end allocate_cuda_memory cost 0.0015611648559570312 seconds
DEBUG 12-24 11:28:12.728702.728702 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.728486.728486 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.728998.728998 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.728575.728575 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, be16746e-efd9-401c-b07d-20a7f08dacff
DEBUG 12-24 11:28:12.728837.728837 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.729421.729421 cuda_h.py:19] end gpu_sexperts cost 0.003147602081298828 seconds
DEBUG 12-24 11:28:12.729636.729636 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.729975.729975 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1dda0097-26a3-455b-be84-9062d12b38a4
INFO 12-24 11:28:12.730759.730759 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, be16746e-efd9-401c-b07d-20a7f08dacff
DEBUG 12-24 11:28:12.730789.730789 cuda_h.py:19] end load_into_gpu_async cost 0.0017330646514892578 seconds
DEBUG 12-24 11:28:12.730936.730936 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.730901.730901 cuda_h.py:19] end restore_tensors2 cost 0.00011515617370605469 seconds
DEBUG 12-24 11:28:12.730339.730339 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.003801584243774414 seconds
INFO 12-24 11:28:12.731111.731111 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, be16746e-efd9-401c-b07d-20a7f08dacff
DEBUG 12-24 11:28:12.731626.731626 mlpmodule.py:668] pad cost 0.014304637908935547 s
DEBUG 12-24 11:28:12.732929.732929 mlpmodule.py:674] create cpu tensor cost 5.316734313964844e-05 s
DEBUG 12-24 11:28:12.732641.732641 mlpmodule.py:679] move to cpu cost 6.723403930664062e-05 s
DEBUG 12-24 11:28:12.738079.738079 mlpmodule.py:694] group_w3: shape=torch.Size([31, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=89391104
DEBUG 12-24 11:28:12.738373.738373 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.738574.738574 mlpmodule.py:700] group_w3 first element: -0.06689453125
WARNING 12-24 11:28:12.738319.738319 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.742921.742921 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.743461.743461 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.748990.748990 mlpmodule.py:723] group einsum cost 0.01611781120300293 s
DEBUG 12-24 11:28:12.748301.748301 mlpmodule.py:731] cpy2cputensor cost 0.0002751350402832031 s
INFO 12-24 11:28:12.763409.763409 client.py:127] Model loaded
DEBUG 12-24 11:28:12.764208.764208 cuda_h.py:19] end wait_experts cost 0.034816741943359375 seconds
DEBUG 12-24 11:28:12.764825.764825 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.764587.764587 lmp.py:585]   Computing 32 experts on GPU...
INFO 12-24 11:28:12.764440.764440 client.py:127] Model loaded
DEBUG 12-24 11:28:12.765290.765290 cuda_h.py:19] end sllm_worker_task cost 0.03860116004943848 seconds
DEBUG 12-24 11:28:12.765721.765721 mlpmodule.py:457] gpu group tensors cost 0.0011184215545654297 s
DEBUG 12-24 11:28:12.771089.771089 mlpmodule.py:588]  experts func einsum cost 0.07103848457336426 s
DEBUG 12-24 11:28:12.773986.773986 mlpmodule.py:490] gpu pad cost 0.008191108703613281 s
DEBUG 12-24 11:28:12.774860.774860 mlpmodule.py:508] gpu group einsum cost 0.0010216236114501953 s
DEBUG 12-24 11:28:12.779014.779014 mlpmodule.py:537] gpu experts func einsum cost 0.01483011245727539 s
DEBUG 12-24 11:28:12.779661.779661 cuda_h.py:19] end gpu_experts cost 0.015049457550048828 seconds
DEBUG 12-24 11:28:12.779331.779331 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.779121.779121 cuda_h.py:19] end wait_cetm_experts cost 2.09808349609375e-05 seconds
DEBUG 12-24 11:28:12.779685.779685 lmp.py:615] gpu end - einsum end = 26.4ms
DEBUG 12-24 11:28:12.779013.779013 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.779432.779432 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2649765014648438e-05 seconds
DEBUG 12-24 11:28:12.779996.779996 cuda_h.py:19] end layer_moe_generate_10 cost 0.0865943431854248 seconds
DEBUG 12-24 11:28:12.779353.779353 lmp.py:445] -------------------------------- end layer 10 --------------------------------
DEBUG 12-24 11:28:12.779646.779646 lmp.py:418] -------------------------------- start layer 11 --------------------------------
DEBUG 12-24 11:28:12.779488.779488 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.780467.780467 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.783141.783141 cuda_h.py:19] end self_attn cost 0.003242969512939453 seconds
DEBUG 12-24 11:28:12.783721.783721 cuda_h.py:19] end iln_self_attn_paln cost 0.0039632320404052734 seconds
DEBUG 12-24 11:28:12.783333.783333 cuda_h.py:10] start layer_moe_generate_11
DEBUG 12-24 11:28:12.783486.783486 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.784993.784993 cuda_h.py:19] end gate cost 0.0008590221405029297 seconds
DEBUG 12-24 11:28:12.784068.784068 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.785537.785537 lmp.py:519] 
DEBUG 12-24 11:28:12.785537.785537 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.785869.785869 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:12.785049.785049 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:12.785652.785652 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:12.785395.785395 lmp.py:523] 
DEBUG 12-24 11:28:12.785395.785395 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.785614.785614 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.785032.785032 lmp.py:530]   Expert 35 |      4 | CPU
DEBUG 12-24 11:28:12.785728.785728 lmp.py:530]   Expert 19 |      5 | CPU
DEBUG 12-24 11:28:12.785232.785232 lmp.py:530]   Expert 59 |     11 | CPU
DEBUG 12-24 11:28:12.785259.785259 lmp.py:530]   Expert  8 |     14 | CPU
DEBUG 12-24 11:28:12.785286.785286 lmp.py:530]   Expert 17 |     16 | CPU
DEBUG 12-24 11:28:12.785313.785313 lmp.py:530]   Expert 44 |     18 | CPU
DEBUG 12-24 11:28:12.785625.785625 lmp.py:530]   Expert 16 |     19 | CPU
DEBUG 12-24 11:28:12.785467.785467 lmp.py:530]   Expert 62 |     19 | CPU
DEBUG 12-24 11:28:12.785733.785733 lmp.py:530]   Expert 25 |     20 | CPU
DEBUG 12-24 11:28:12.785283.785283 lmp.py:530]   Expert  6 |     23 | CPU
DEBUG 12-24 11:28:12.785833.785833 lmp.py:530]   Expert 49 |     25 | CPU
DEBUG 12-24 11:28:12.785622.785622 lmp.py:530]   Expert 23 |     26 | CPU
DEBUG 12-24 11:28:12.785411.785411 lmp.py:530]   Expert 50 |     30 | CPU
DEBUG 12-24 11:28:12.785438.785438 lmp.py:530]   Expert 41 |     33 | CPU
DEBUG 12-24 11:28:12.785227.785227 lmp.py:530]   Expert 18 |     34 | CPU
DEBUG 12-24 11:28:12.785492.785492 lmp.py:530]   Expert 40 |     34 | CPU
DEBUG 12-24 11:28:12.785281.785281 lmp.py:530]   Expert 38 |     35 | CPU
DEBUG 12-24 11:28:12.785070.785070 lmp.py:530]   Expert 46 |     36 | CPU
DEBUG 12-24 11:28:12.785859.785859 lmp.py:530]   Expert 53 |     36 | CPU
DEBUG 12-24 11:28:12.785886.785886 lmp.py:530]   Expert 28 |     39 | CPU
DEBUG 12-24 11:28:12.785675.785675 lmp.py:530]   Expert 63 |     40 | CPU
DEBUG 12-24 11:28:12.785463.785463 lmp.py:530]   Expert 39 |     41 | CPU
DEBUG 12-24 11:28:12.785252.785252 lmp.py:530]   Expert 10 |     45 | CPU
DEBUG 12-24 11:28:12.785279.785279 lmp.py:530]   Expert 15 |     47 | CPU
DEBUG 12-24 11:28:12.785591.785591 lmp.py:530]   Expert 32 |     47 | CPU
DEBUG 12-24 11:28:12.785380.785380 lmp.py:530]   Expert 47 |     47 | CPU
DEBUG 12-24 11:28:12.785169.785169 lmp.py:530]   Expert 57 |     49 | CPU
DEBUG 12-24 11:28:12.785196.785196 lmp.py:530]   Expert 42 |     51 | CPU
DEBUG 12-24 11:28:12.785984.785984 lmp.py:530]   Expert 61 |     51 | CPU
DEBUG 12-24 11:28:12.785296.785296 lmp.py:530]   Expert 48 |     52 | CPU
DEBUG 12-24 11:28:12.785085.785085 lmp.py:530]   Expert 24 |     55 | CPU
DEBUG 12-24 11:28:12.786635.786635 lmp.py:530]   Expert 31 |     58 | CPU
DEBUG 12-24 11:28:12.786186.786186 lmp.py:530]   Expert 12 |     59 | GPU
DEBUG 12-24 11:28:12.786498.786498 lmp.py:530]   Expert 45 |     60 | GPU
DEBUG 12-24 11:28:12.786286.786286 lmp.py:530]   Expert 52 |     61 | GPU
DEBUG 12-24 11:28:12.786837.786837 lmp.py:530]   Expert 58 |     64 | GPU
DEBUG 12-24 11:28:12.786387.786387 lmp.py:530]   Expert 51 |     67 | GPU
DEBUG 12-24 11:28:12.786937.786937 lmp.py:530]   Expert 55 |     68 | GPU
DEBUG 12-24 11:28:12.786726.786726 lmp.py:530]   Expert 56 |     68 | GPU
DEBUG 12-24 11:28:12.786038.786038 lmp.py:530]   Expert 20 |     72 | GPU
DEBUG 12-24 11:28:12.786827.786827 lmp.py:530]   Expert 43 |     72 | GPU
DEBUG 12-24 11:28:12.786377.786377 lmp.py:530]   Expert  9 |     73 | GPU
DEBUG 12-24 11:28:12.786451.786451 lmp.py:530]   Expert 26 |     74 | GPU
DEBUG 12-24 11:28:12.786239.786239 lmp.py:530]   Expert 60 |     76 | GPU
DEBUG 12-24 11:28:12.786790.786790 lmp.py:530]   Expert  7 |     84 | GPU
DEBUG 12-24 11:28:12.786578.786578 lmp.py:530]   Expert 36 |     84 | GPU
DEBUG 12-24 11:28:12.786367.786367 lmp.py:530]   Expert 14 |     86 | GPU
DEBUG 12-24 11:28:12.786441.786441 lmp.py:530]   Expert 37 |     90 | GPU
DEBUG 12-24 11:28:12.786991.786991 lmp.py:530]   Expert 22 |     91 | GPU
DEBUG 12-24 11:28:12.786541.786541 lmp.py:530]   Expert 21 |     95 | GPU
DEBUG 12-24 11:28:12.786615.786615 lmp.py:530]   Expert 27 |     96 | GPU
DEBUG 12-24 11:28:12.786404.786404 lmp.py:530]   Expert 54 |     97 | GPU
DEBUG 12-24 11:28:12.786477.786477 lmp.py:530]   Expert 33 |    105 | GPU
DEBUG 12-24 11:28:12.786027.786027 lmp.py:530]   Expert 13 |    111 | GPU
DEBUG 12-24 11:28:12.786816.786816 lmp.py:530]   Expert 29 |    111 | GPU
DEBUG 12-24 11:28:12.786366.786366 lmp.py:530]   Expert 11 |    119 | GPU
DEBUG 12-24 11:28:12.786678.786678 lmp.py:530]   Expert 30 |    126 | GPU
DEBUG 12-24 11:28:12.786990.786990 lmp.py:530]   Expert 34 |    127 | GPU
DEBUG 12-24 11:28:12.786302.786302 lmp.py:530]   Expert  1 |   1484 | GPU
DEBUG 12-24 11:28:12.786852.786852 lmp.py:530]   Expert  5 |   1485 | GPU
DEBUG 12-24 11:28:12.786164.786164 lmp.py:530]   Expert  0 |   1492 | GPU
DEBUG 12-24 11:28:12.786953.786953 lmp.py:530]   Expert  4 |   1503 | GPU
DEBUG 12-24 11:28:12.786411.786411 lmp.py:530]   Expert  3 |   1510 | GPU
DEBUG 12-24 11:28:12.786199.786199 lmp.py:530]   Expert  2 |   1518 | GPU
DEBUG 12-24 11:28:12.786703.786703 lmp.py:531] 
DEBUG 12-24 11:28:12.786703.786703 lmp.py:531]   CPU total tokens: 1060 (8.6%)
DEBUG 12-24 11:28:12.786684.786684 lmp.py:532]   GPU total tokens: 11228 (91.4%)
DEBUG 12-24 11:28:12.786672.786672 cuda_h.py:19] end experts_map_get cost 0.0017807483673095703 seconds
DEBUG 12-24 11:28:12.786891.786891 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.786622.786622 lmp.py:541] 
DEBUG 12-24 11:28:12.786622.786622 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:12.786287.786287 cuda_h.py:19] end cpu_experts_submit cost 0.00012493133544921875 seconds
DEBUG 12-24 11:28:12.786559.786559 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.786548.786548 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.787944.787944 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.787767.787767 cuda_h.py:19] end allocate_cuda_memory cost 0.00021576881408691406 seconds
DEBUG 12-24 11:28:12.787008.787008 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.787393.787393 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.787311.787311 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.787736.787736 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a5326772-b9c8-4f05-9348-bc1476cc562a
DEBUG 12-24 11:28:12.787080.787080 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.836695.836695 mlpmodule.py:630] group tensors cost 0.04872870445251465 s
INFO 12-24 11:28:12.837562.837562 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a5326772-b9c8-4f05-9348-bc1476cc562a
DEBUG 12-24 11:28:12.838379.838379 cuda_h.py:19] end load_into_gpu_async cost 0.050563812255859375 seconds
DEBUG 12-24 11:28:12.838554.838554 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.839828.839828 cuda_h.py:19] end restore_tensors2 cost 0.0007846355438232422 seconds
DEBUG 12-24 11:28:12.839435.839435 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.05215787887573242 seconds
DEBUG 12-24 11:28:12.848781.848781 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.06134963035583496 seconds
DEBUG 12-24 11:28:12.851429.851429 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 12-24 11:28:12.851710.851710 cuda_h.py:10] start start_load_qkvogn_s_weight_l_12
DEBUG 12-24 11:28:12.851288.851288 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 3.4809112548828125e-05 seconds
DEBUG 12-24 11:28:12.851627.851627 cuda_h.py:19] end start_load_qkvogn_s_weight_l_12 cost 7.772445678710938e-05 seconds
DEBUG 12-24 11:28:12.851430.851430 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.851481.851481 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.852700.852700 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.852146.852146 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.852962.852962 cuda_h.py:19] end allocate_cuda_memory cost 0.0005316734313964844 seconds
DEBUG 12-24 11:28:12.853701.853701 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.853955.853955 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.853904.853904 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.853680.853680 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a86ae3ca-0649-41c9-b6e3-72b5c146b96f
DEBUG 12-24 11:28:12.853756.853756 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.853736.853736 cuda_h.py:19] end gpu_sexperts cost 0.0021648406982421875 seconds
DEBUG 12-24 11:28:12.854212.854212 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.854137.854137 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a5326772-b9c8-4f05-9348-bc1476cc562a
INFO 12-24 11:28:12.854004.854004 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a86ae3ca-0649-41c9-b6e3-72b5c146b96f
DEBUG 12-24 11:28:12.854053.854053 cuda_h.py:19] end load_into_gpu_async cost 0.001547098159790039 seconds
DEBUG 12-24 11:28:12.854531.854531 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.854634.854634 cuda_h.py:19] end restore_tensors2 cost 7.605552673339844e-05 seconds
DEBUG 12-24 11:28:12.854119.854119 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.00260162353515625 seconds
INFO 12-24 11:28:12.854952.854952 client.py:127] Model loaded
DEBUG 12-24 11:28:12.855976.855976 cuda_h.py:19] end wait_experts cost 0.0011441707611083984 seconds
DEBUG 12-24 11:28:12.855973.855973 cuda_h.py:10] start gpu_experts
INFO 12-24 11:28:12.855763.855763 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a86ae3ca-0649-41c9-b6e3-72b5c146b96f
DEBUG 12-24 11:28:12.855978.855978 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:12.857908.857908 mlpmodule.py:457] gpu group tensors cost 0.0013916492462158203 s
DEBUG 12-24 11:28:12.860298.860298 mlpmodule.py:668] pad cost 0.022951602935791016 s
DEBUG 12-24 11:28:12.860462.860462 mlpmodule.py:674] create cpu tensor cost 5.91278076171875e-05 s
DEBUG 12-24 11:28:12.860420.860420 mlpmodule.py:679] move to cpu cost 5.459785461425781e-05 s
DEBUG 12-24 11:28:12.863148.863148 mlpmodule.py:490] gpu pad cost 0.005194425582885742 s
INFO 12-24 11:28:12.863209.863209 client.py:127] Model loaded
DEBUG 12-24 11:28:12.864460.864460 cuda_h.py:19] end sllm_worker_task cost 0.012017250061035156 seconds
DEBUG 12-24 11:28:12.865352.865352 mlpmodule.py:508] gpu group einsum cost 0.0017719268798828125 s
DEBUG 12-24 11:28:12.866332.866332 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:12.866301.866301 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.866086.866086 mlpmodule.py:700] group_w3 first element: 0.049072265625
WARNING 12-24 11:28:12.866203.866203 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.870391.870391 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.871851.871851 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.871256.871256 mlpmodule.py:537] gpu experts func einsum cost 0.015542745590209961 s
DEBUG 12-24 11:28:12.871137.871137 cuda_h.py:19] end gpu_experts cost 0.016034364700317383 seconds
DEBUG 12-24 11:28:12.872383.872383 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.876544.876544 mlpmodule.py:723] group einsum cost 0.015123367309570312 s
DEBUG 12-24 11:28:12.876816.876816 mlpmodule.py:731] cpy2cputensor cost 0.00031638145446777344 s
DEBUG 12-24 11:28:12.880717.880717 cuda_h.py:19] end wait_cetm_experts cost 0.008716344833374023 seconds
DEBUG 12-24 11:28:12.880323.880323 lmp.py:615] gpu end - einsum end = -8.7ms
DEBUG 12-24 11:28:12.881447.881447 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.881026.881026 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 3.147125244140625e-05 seconds
DEBUG 12-24 11:28:12.881689.881689 cuda_h.py:19] end layer_moe_generate_11 cost 0.0972435474395752 seconds
DEBUG 12-24 11:28:12.881028.881028 lmp.py:445] -------------------------------- end layer 11 --------------------------------
DEBUG 12-24 11:28:12.881850.881850 lmp.py:418] -------------------------------- start layer 12 --------------------------------
DEBUG 12-24 11:28:12.881454.881454 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.881760.881760 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.885028.885028 cuda_h.py:19] end self_attn cost 0.003159046173095703 seconds
DEBUG 12-24 11:28:12.885985.885985 cuda_h.py:19] end iln_self_attn_paln cost 0.003974437713623047 seconds
DEBUG 12-24 11:28:12.885696.885696 cuda_h.py:10] start layer_moe_generate_12
DEBUG 12-24 11:28:12.885611.885611 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.886287.886287 cuda_h.py:19] end gate cost 0.0007390975952148438 seconds
DEBUG 12-24 11:28:12.886408.886408 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.886154.886154 lmp.py:519] 
DEBUG 12-24 11:28:12.886154.886154 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.886394.886394 lmp.py:520]   Total experts: 63
DEBUG 12-24 11:28:12.886381.886381 lmp.py:521]   CPU experts: 31 (49%)
DEBUG 12-24 11:28:12.886316.886316 lmp.py:522]   GPU experts: 32 (51%)
DEBUG 12-24 11:28:12.886581.886581 lmp.py:523] 
DEBUG 12-24 11:28:12.886581.886581 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.886847.886847 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.886742.886742 lmp.py:530]   Expert 11 |      2 | CPU
DEBUG 12-24 11:28:12.886438.886438 lmp.py:530]   Expert 51 |      3 | CPU
DEBUG 12-24 11:28:12.886750.886750 lmp.py:530]   Expert 22 |      4 | CPU
DEBUG 12-24 11:28:12.886585.886585 lmp.py:530]   Expert  8 |      5 | CPU
DEBUG 12-24 11:28:12.886658.886658 lmp.py:530]   Expert 16 |      7 | CPU
DEBUG 12-24 11:28:12.886401.886401 lmp.py:530]   Expert 27 |     11 | CPU
DEBUG 12-24 11:28:12.887620.887620 lmp.py:530]   Expert 34 |     12 | CPU
DEBUG 12-24 11:28:12.887409.887409 lmp.py:530]   Expert 12 |     17 | CPU
DEBUG 12-24 11:28:12.887959.887959 lmp.py:530]   Expert 13 |     19 | CPU
DEBUG 12-24 11:28:12.887748.887748 lmp.py:530]   Expert 49 |     19 | CPU
DEBUG 12-24 11:28:12.887060.887060 lmp.py:530]   Expert 32 |     20 | CPU
DEBUG 12-24 11:28:12.887518.887518 lmp.py:530]   Expert 21 |     22 | CPU
DEBUG 12-24 11:28:12.887068.887068 lmp.py:530]   Expert 37 |     24 | CPU
DEBUG 12-24 11:28:12.887665.887665 lmp.py:530]   Expert 44 |     25 | CPU
DEBUG 12-24 11:28:12.887738.887738 lmp.py:530]   Expert 47 |     27 | CPU
DEBUG 12-24 11:28:12.887096.887096 lmp.py:530]   Expert 10 |     28 | CPU
DEBUG 12-24 11:28:12.887600.887600 lmp.py:530]   Expert 61 |     28 | CPU
DEBUG 12-24 11:28:12.887151.887151 lmp.py:530]   Expert 29 |     33 | CPU
DEBUG 12-24 11:28:12.887701.887701 lmp.py:530]   Expert 41 |     37 | CPU
DEBUG 12-24 11:28:12.887490.887490 lmp.py:530]   Expert 26 |     38 | CPU
DEBUG 12-24 11:28:12.887040.887040 lmp.py:530]   Expert 62 |     38 | CPU
DEBUG 12-24 11:28:12.887113.887113 lmp.py:530]   Expert 30 |     40 | CPU
DEBUG 12-24 11:28:12.887187.887187 lmp.py:530]   Expert 43 |     41 | CPU
DEBUG 12-24 11:28:12.887645.887645 lmp.py:530]   Expert 14 |     44 | CPU
DEBUG 12-24 11:28:12.887480.887480 lmp.py:530]   Expert 45 |     44 | CPU
DEBUG 12-24 11:28:12.887315.887315 lmp.py:530]   Expert 46 |     46 | CPU
DEBUG 12-24 11:28:12.887673.887673 lmp.py:530]   Expert 58 |     47 | CPU
DEBUG 12-24 11:28:12.887270.887270 lmp.py:530]   Expert 23 |     48 | CPU
DEBUG 12-24 11:28:12.887012.887012 lmp.py:530]   Expert 53 |     49 | CPU
DEBUG 12-24 11:28:12.887086.887086 lmp.py:530]   Expert 55 |     49 | CPU
DEBUG 12-24 11:28:12.887921.887921 lmp.py:530]   Expert  7 |     51 | CPU
DEBUG 12-24 11:28:12.887709.887709 lmp.py:530]   Expert 19 |     51 | GPU
DEBUG 12-24 11:28:12.887544.887544 lmp.py:530]   Expert 31 |     53 | GPU
DEBUG 12-24 11:28:12.887386.887386 lmp.py:530]   Expert 57 |     59 | GPU
DEBUG 12-24 11:28:12.887937.887937 lmp.py:530]   Expert 42 |     60 | GPU
DEBUG 12-24 11:28:12.887156.887156 lmp.py:530]   Expert 36 |     62 | GPU
DEBUG 12-24 11:28:12.887752.887752 lmp.py:530]   Expert 48 |     62 | GPU
DEBUG 12-24 11:28:12.887349.887349 lmp.py:530]   Expert 39 |     64 | GPU
DEBUG 12-24 11:28:12.887469.887469 lmp.py:530]   Expert 25 |     65 | GPU
DEBUG 12-24 11:28:12.887066.887066 lmp.py:530]   Expert 50 |     66 | GPU
DEBUG 12-24 11:28:12.887570.887570 lmp.py:530]   Expert 59 |     70 | GPU
DEBUG 12-24 11:28:12.887643.887643 lmp.py:530]   Expert 17 |     72 | GPU
DEBUG 12-24 11:28:12.887717.887717 lmp.py:530]   Expert  6 |     77 | GPU
DEBUG 12-24 11:28:12.887028.887028 lmp.py:530]   Expert 40 |     81 | GPU
DEBUG 12-24 11:28:12.887102.887102 lmp.py:530]   Expert 18 |     84 | GPU
DEBUG 12-24 11:28:12.887891.887891 lmp.py:530]   Expert 33 |     86 | GPU
DEBUG 12-24 11:28:12.887872.887872 lmp.py:530]   Expert 24 |     87 | GPU
DEBUG 12-24 11:28:12.887707.887707 lmp.py:530]   Expert 54 |     99 | GPU
DEBUG 12-24 11:28:12.887542.887542 lmp.py:530]   Expert 28 |    103 | GPU
DEBUG 12-24 11:28:12.887191.887191 lmp.py:530]   Expert 15 |    106 | GPU
DEBUG 12-24 11:28:12.887649.887649 lmp.py:530]   Expert 35 |    106 | GPU
DEBUG 12-24 11:28:12.887253.887253 lmp.py:530]   Expert 20 |    110 | GPU
DEBUG 12-24 11:28:12.887664.887664 lmp.py:530]   Expert  9 |    115 | GPU
DEBUG 12-24 11:28:12.887360.887360 lmp.py:530]   Expert 56 |    139 | GPU
DEBUG 12-24 11:28:12.887294.887294 lmp.py:530]   Expert 60 |    142 | GPU
DEBUG 12-24 11:28:12.887991.887991 lmp.py:530]   Expert 38 |    174 | GPU
DEBUG 12-24 11:28:12.888071.888071 lmp.py:530]   Expert 52 |    221 | GPU
DEBUG 12-24 11:28:12.888005.888005 lmp.py:530]   Expert  4 |   1475 | GPU
DEBUG 12-24 11:28:12.888701.888701 lmp.py:530]   Expert  0 |   1480 | GPU
DEBUG 12-24 11:28:12.888921.888921 lmp.py:530]   Expert  3 |   1480 | GPU
DEBUG 12-24 11:28:12.888286.888286 lmp.py:530]   Expert  2 |   1502 | GPU
DEBUG 12-24 11:28:12.888982.888982 lmp.py:530]   Expert  1 |   1527 | GPU
DEBUG 12-24 11:28:12.888393.888393 lmp.py:530]   Expert  5 |   1532 | GPU
DEBUG 12-24 11:28:12.888043.888043 lmp.py:531] 
DEBUG 12-24 11:28:12.888043.888043 lmp.py:531]   CPU total tokens: 878 (7.1%)
DEBUG 12-24 11:28:12.888408.888408 lmp.py:532]   GPU total tokens: 11410 (92.9%)
DEBUG 12-24 11:28:12.888495.888495 cuda_h.py:19] end experts_map_get cost 0.0017871856689453125 seconds
DEBUG 12-24 11:28:12.888767.888767 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.888890.888890 lmp.py:541] 
DEBUG 12-24 11:28:12.888890.888890 lmp.py:541]   Computing 31 experts on CPU...
DEBUG 12-24 11:28:12.888045.888045 cuda_h.py:19] end cpu_experts_submit cost 0.00013875961303710938 seconds
DEBUG 12-24 11:28:12.888092.888092 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.888127.888127 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.888914.888914 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.895489.895489 cuda_h.py:19] end allocate_cuda_memory cost 0.0070269107818603516 seconds
DEBUG 12-24 11:28:12.895279.895279 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.896777.896777 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.896361.896361 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.896587.896587 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 4b103ca7-0631-4bdb-be59-0f9b7729637f
DEBUG 12-24 11:28:12.896607.896607 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:12.898050.898050 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 4b103ca7-0631-4bdb-be59-0f9b7729637f
DEBUG 12-24 11:28:12.898701.898701 cuda_h.py:19] end load_into_gpu_async cost 0.0024733543395996094 seconds
DEBUG 12-24 11:28:12.898450.898450 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.898159.898159 cuda_h.py:19] end restore_tensors2 cost 0.0003910064697265625 seconds
DEBUG 12-24 11:28:12.899526.899526 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.01048421859741211 seconds
DEBUG 12-24 11:28:12.902566.902566 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.013727664947509766 seconds
DEBUG 12-24 11:28:12.902793.902793 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 12-24 11:28:12.902470.902470 cuda_h.py:10] start start_load_qkvogn_s_weight_l_13
DEBUG 12-24 11:28:12.902089.902089 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 3.504753112792969e-05 seconds
DEBUG 12-24 11:28:12.902613.902613 cuda_h.py:19] end start_load_qkvogn_s_weight_l_13 cost 7.367134094238281e-05 seconds
DEBUG 12-24 11:28:12.902269.902269 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.902789.902789 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.902422.902422 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.902173.902173 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.903769.903769 cuda_h.py:19] end allocate_cuda_memory cost 0.000362396240234375 seconds
DEBUG 12-24 11:28:12.903917.903917 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.903588.903588 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.903318.903318 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.903835.903835 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9453b649-3257-48dc-ba02-8f282cfbe37b
DEBUG 12-24 11:28:12.903229.903229 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.903315.903315 mlpmodule.py:588]  experts func einsum cost 0.1155548095703125 s
DEBUG 12-24 11:28:12.915424.915424 cuda_h.py:19] end gpu_sexperts cost 0.01287698745727539 seconds
INFO 12-24 11:28:12.915771.915771 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9453b649-3257-48dc-ba02-8f282cfbe37b
DEBUG 12-24 11:28:12.915230.915230 mlpmodule.py:630] group tensors cost 0.01160287857055664 s
DEBUG 12-24 11:28:12.915735.915735 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.916385.916385 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 4b103ca7-0631-4bdb-be59-0f9b7729637f
DEBUG 12-24 11:28:12.916365.916365 cuda_h.py:19] end load_into_gpu_async cost 0.013308048248291016 seconds
DEBUG 12-24 11:28:12.916379.916379 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.917780.917780 cuda_h.py:19] end restore_tensors2 cost 7.82012939453125e-05 seconds
DEBUG 12-24 11:28:12.917411.917411 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.014450788497924805 seconds
INFO 12-24 11:28:12.917171.917171 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9453b649-3257-48dc-ba02-8f282cfbe37b
DEBUG 12-24 11:28:12.919806.919806 mlpmodule.py:668] pad cost 0.002660989761352539 s
DEBUG 12-24 11:28:12.919532.919532 mlpmodule.py:674] create cpu tensor cost 4.029273986816406e-05 s
DEBUG 12-24 11:28:12.919263.919263 mlpmodule.py:679] move to cpu cost 4.7206878662109375e-05 s
DEBUG 12-24 11:28:12.925362.925362 mlpmodule.py:694] group_w3: shape=torch.Size([31, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=89391104
DEBUG 12-24 11:28:12.925226.925226 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.925069.925069 mlpmodule.py:700] group_w3 first element: -0.0150146484375
WARNING 12-24 11:28:12.925436.925436 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.929946.929946 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.930035.930035 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:12.935906.935906 mlpmodule.py:723] group einsum cost 0.01577138900756836 s
DEBUG 12-24 11:28:12.935798.935798 mlpmodule.py:731] cpy2cputensor cost 0.00026345252990722656 s
INFO 12-24 11:28:12.952979.952979 client.py:127] Model loaded
DEBUG 12-24 11:28:12.952686.952686 cuda_h.py:19] end wait_experts cost 0.035678863525390625 seconds
DEBUG 12-24 11:28:12.952854.952854 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:12.952683.952683 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:12.953585.953585 mlpmodule.py:457] gpu group tensors cost 0.0008988380432128906 s
DEBUG 12-24 11:28:12.956460.956460 mlpmodule.py:490] gpu pad cost 0.0025222301483154297 s
DEBUG 12-24 11:28:12.957357.957357 mlpmodule.py:508] gpu group einsum cost 0.0007693767547607422 s
INFO 12-24 11:28:12.958305.958305 client.py:127] Model loaded
DEBUG 12-24 11:28:12.958439.958439 cuda_h.py:19] end sllm_worker_task cost 0.05588197708129883 seconds
DEBUG 12-24 11:28:12.961647.961647 mlpmodule.py:537] gpu experts func einsum cost 0.008885622024536133 s
DEBUG 12-24 11:28:12.961034.961034 cuda_h.py:19] end gpu_experts cost 0.009116649627685547 seconds
DEBUG 12-24 11:28:12.961936.961936 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:12.961997.961997 cuda_h.py:19] end wait_cetm_experts cost 1.430511474609375e-05 seconds
DEBUG 12-24 11:28:12.961415.961415 lmp.py:615] gpu end - einsum end = 22.0ms
DEBUG 12-24 11:28:12.961233.961233 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:12.961685.961685 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.8596649169921875e-05 seconds
DEBUG 12-24 11:28:12.961387.961387 cuda_h.py:19] end layer_moe_generate_12 cost 0.07638955116271973 seconds
DEBUG 12-24 11:28:12.962241.962241 lmp.py:445] -------------------------------- end layer 12 --------------------------------
DEBUG 12-24 11:28:12.962818.962818 lmp.py:418] -------------------------------- start layer 13 --------------------------------
DEBUG 12-24 11:28:12.962753.962753 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:12.962253.962253 cuda_h.py:10] start self_attn
DEBUG 12-24 11:28:12.963845.963845 mlpmodule.py:588]  experts func einsum cost 0.06003880500793457 s
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:12.965593.965593 cuda_h.py:19] end self_attn cost 0.0029571056365966797 seconds
DEBUG 12-24 11:28:12.965524.965524 cuda_h.py:19] end iln_self_attn_paln cost 0.003624439239501953 seconds
DEBUG 12-24 11:28:12.965897.965897 cuda_h.py:10] start layer_moe_generate_13
DEBUG 12-24 11:28:12.965905.965905 cuda_h.py:10] start gate
DEBUG 12-24 11:28:12.966716.966716 cuda_h.py:19] end gate cost 0.0006296634674072266 seconds
DEBUG 12-24 11:28:12.966883.966883 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:12.966397.966397 lmp.py:519] 
DEBUG 12-24 11:28:12.966397.966397 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:12.967014.967014 lmp.py:520]   Total experts: 63
DEBUG 12-24 11:28:12.967286.967286 lmp.py:521]   CPU experts: 31 (49%)
DEBUG 12-24 11:28:12.967744.967744 lmp.py:522]   GPU experts: 32 (51%)
DEBUG 12-24 11:28:12.967817.967817 lmp.py:523] 
DEBUG 12-24 11:28:12.967817.967817 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:12.967368.967368 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:12.967309.967309 lmp.py:530]   Expert 53 |      1 | CPU
DEBUG 12-24 11:28:12.967336.967336 lmp.py:530]   Expert  6 |      2 | CPU
DEBUG 12-24 11:28:12.967694.967694 lmp.py:530]   Expert 50 |      8 | CPU
DEBUG 12-24 11:28:12.967337.967337 lmp.py:530]   Expert 20 |     12 | CPU
DEBUG 12-24 11:28:12.967219.967219 lmp.py:530]   Expert  8 |     14 | CPU
DEBUG 12-24 11:28:12.967769.967769 lmp.py:530]   Expert 61 |     16 | CPU
DEBUG 12-24 11:28:12.967366.967366 lmp.py:530]   Expert 16 |     17 | CPU
DEBUG 12-24 11:28:12.967962.967962 lmp.py:530]   Expert 54 |     20 | CPU
DEBUG 12-24 11:28:12.967321.967321 lmp.py:530]   Expert 35 |     22 | CPU
DEBUG 12-24 11:28:12.967964.967964 lmp.py:530]   Expert 26 |     25 | CPU
DEBUG 12-24 11:28:12.967752.967752 lmp.py:530]   Expert 12 |     33 | CPU
DEBUG 12-24 11:28:12.967395.967395 lmp.py:530]   Expert 63 |     33 | CPU
DEBUG 12-24 11:28:12.967038.967038 lmp.py:530]   Expert 13 |     35 | CPU
DEBUG 12-24 11:28:12.967681.967681 lmp.py:530]   Expert 28 |     35 | CPU
DEBUG 12-24 11:28:12.967278.967278 lmp.py:530]   Expert 30 |     36 | CPU
DEBUG 12-24 11:28:12.967875.967875 lmp.py:530]   Expert 18 |     37 | CPU
DEBUG 12-24 11:28:12.967233.967233 lmp.py:530]   Expert 38 |     38 | CPU
DEBUG 12-24 11:28:12.967830.967830 lmp.py:530]   Expert 46 |     38 | CPU
DEBUG 12-24 11:28:12.967903.967903 lmp.py:530]   Expert 11 |     40 | CPU
DEBUG 12-24 11:28:12.967453.967453 lmp.py:530]   Expert  9 |     43 | CPU
DEBUG 12-24 11:28:12.967812.967812 lmp.py:530]   Expert 32 |     44 | CPU
DEBUG 12-24 11:28:12.967978.967978 lmp.py:530]   Expert 48 |     48 | CPU
DEBUG 12-24 11:28:12.967144.967144 lmp.py:530]   Expert 49 |     49 | CPU
DEBUG 12-24 11:28:12.967787.967787 lmp.py:530]   Expert 43 |     51 | CPU
DEBUG 12-24 11:28:12.967430.967430 lmp.py:530]   Expert 17 |     52 | CPU
DEBUG 12-24 11:28:12.967742.967742 lmp.py:530]   Expert 37 |     53 | CPU
DEBUG 12-24 11:28:12.967385.967385 lmp.py:530]   Expert 56 |     53 | CPU
DEBUG 12-24 11:28:12.967743.967743 lmp.py:530]   Expert 36 |     55 | CPU
DEBUG 12-24 11:28:12.967863.967863 lmp.py:530]   Expert 34 |     56 | CPU
DEBUG 12-24 11:28:12.967983.967983 lmp.py:530]   Expert 47 |     56 | CPU
DEBUG 12-24 11:28:12.967341.967341 lmp.py:530]   Expert 29 |     58 | CPU
DEBUG 12-24 11:28:12.967461.967461 lmp.py:530]   Expert  7 |     59 | GPU
DEBUG 12-24 11:28:12.967011.967011 lmp.py:530]   Expert 31 |     59 | GPU
DEBUG 12-24 11:28:12.967415.967415 lmp.py:530]   Expert 55 |     59 | GPU
DEBUG 12-24 11:28:12.967582.967582 lmp.py:530]   Expert 51 |     60 | GPU
DEBUG 12-24 11:28:12.967748.967748 lmp.py:530]   Expert 45 |     63 | GPU
DEBUG 12-24 11:28:12.967391.967391 lmp.py:530]   Expert 52 |     63 | GPU
DEBUG 12-24 11:28:12.967703.967703 lmp.py:530]   Expert 27 |     65 | GPU
DEBUG 12-24 11:28:12.967107.967107 lmp.py:530]   Expert 22 |     68 | GPU
DEBUG 12-24 11:28:12.967465.967465 lmp.py:530]   Expert 33 |     68 | GPU
DEBUG 12-24 11:28:12.967585.967585 lmp.py:530]   Expert 14 |     69 | GPU
DEBUG 12-24 11:28:12.967420.967420 lmp.py:530]   Expert 60 |     69 | GPU
DEBUG 12-24 11:28:12.967540.967540 lmp.py:530]   Expert 25 |     70 | GPU
DEBUG 12-24 11:28:12.967183.967183 lmp.py:530]   Expert 24 |     71 | GPU
DEBUG 12-24 11:28:12.967449.967449 lmp.py:530]   Expert 58 |     71 | GPU
DEBUG 12-24 11:28:12.967853.967853 lmp.py:530]   Expert 40 |     73 | GPU
DEBUG 12-24 11:28:12.967258.967258 lmp.py:530]   Expert 41 |     82 | GPU
DEBUG 12-24 11:28:12.967662.967662 lmp.py:530]   Expert 57 |     82 | GPU
DEBUG 12-24 11:28:12.967828.967828 lmp.py:530]   Expert 39 |     84 | GPU
DEBUG 12-24 11:28:12.968856.968856 lmp.py:530]   Expert 21 |     90 | GPU
DEBUG 12-24 11:28:12.968260.968260 lmp.py:530]   Expert 44 |     90 | GPU
DEBUG 12-24 11:28:12.968142.968142 lmp.py:530]   Expert 62 |     90 | GPU
DEBUG 12-24 11:28:12.968738.968738 lmp.py:530]   Expert 10 |     99 | GPU
DEBUG 12-24 11:28:12.968335.968335 lmp.py:530]   Expert 42 |     99 | GPU
DEBUG 12-24 11:28:12.968455.968455 lmp.py:530]   Expert 23 |    106 | GPU
DEBUG 12-24 11:28:12.968482.968482 lmp.py:530]   Expert 15 |    115 | GPU
DEBUG 12-24 11:28:12.968125.968125 lmp.py:530]   Expert 59 |    117 | GPU
DEBUG 12-24 11:28:12.968529.968529 lmp.py:530]   Expert  2 |   1487 | GPU
DEBUG 12-24 11:28:12.968934.968934 lmp.py:530]   Expert  0 |   1493 | GPU
DEBUG 12-24 11:28:12.968862.968862 lmp.py:530]   Expert  4 |   1499 | GPU
DEBUG 12-24 11:28:12.968266.968266 lmp.py:530]   Expert  3 |   1525 | GPU
DEBUG 12-24 11:28:12.968101.968101 lmp.py:530]   Expert  5 |   1543 | GPU
DEBUG 12-24 11:28:12.968267.968267 lmp.py:530]   Expert  1 |   1620 | GPU
DEBUG 12-24 11:28:12.968387.968387 lmp.py:531] 
DEBUG 12-24 11:28:12.968387.968387 lmp.py:531]   CPU total tokens: 1080 (8.8%)
DEBUG 12-24 11:28:12.968176.968176 lmp.py:532]   GPU total tokens: 11208 (91.2%)
DEBUG 12-24 11:28:12.968733.968733 cuda_h.py:19] end experts_map_get cost 0.0016140937805175781 seconds
DEBUG 12-24 11:28:12.968522.968522 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:12.968803.968803 lmp.py:541] 
DEBUG 12-24 11:28:12.968803.968803 lmp.py:541]   Computing 31 experts on CPU...
DEBUG 12-24 11:28:12.968428.968428 cuda_h.py:19] end cpu_experts_submit cost 0.000152587890625 seconds
DEBUG 12-24 11:28:12.968039.968039 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:12.968690.968690 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.968847.968847 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.969328.969328 cuda_h.py:19] end allocate_cuda_memory cost 0.0002803802490234375 seconds
DEBUG 12-24 11:28:12.969516.969516 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.969848.969848 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.969200.969200 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.969149.969149 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, bfb9b7ce-0603-4fe1-a5d9-dbc21f8edc8b
DEBUG 12-24 11:28:12.969950.969950 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.976792.976792 mlpmodule.py:630] group tensors cost 0.0066945552825927734 s
INFO 12-24 11:28:12.977988.977988 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, bfb9b7ce-0603-4fe1-a5d9-dbc21f8edc8b
DEBUG 12-24 11:28:12.977925.977925 cuda_h.py:19] end load_into_gpu_async cost 0.008022546768188477 seconds
DEBUG 12-24 11:28:12.977542.977542 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.977594.977594 cuda_h.py:19] end restore_tensors2 cost 0.00035071372985839844 seconds
DEBUG 12-24 11:28:12.977417.977417 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.009039878845214844 seconds
DEBUG 12-24 11:28:12.982739.982739 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.013751983642578125 seconds
DEBUG 12-24 11:28:12.982576.982576 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 12-24 11:28:12.982876.982876 cuda_h.py:10] start start_load_qkvogn_s_weight_l_14
DEBUG 12-24 11:28:12.982004.982004 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 2.6702880859375e-05 seconds
DEBUG 12-24 11:28:12.982713.982713 cuda_h.py:19] end start_load_qkvogn_s_weight_l_14 cost 6.127357482910156e-05 seconds
DEBUG 12-24 11:28:12.982032.982032 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:12.982864.982864 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:12.982810.982810 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:12.983928.983928 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:12.983728.983728 cuda_h.py:19] end allocate_cuda_memory cost 0.0006859302520751953 seconds
DEBUG 12-24 11:28:12.983706.983706 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:12.983470.983470 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:12.984276.984276 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:12.984265.984265 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1fdb1696-8d91-4379-9e5b-d1148c949c61
DEBUG 12-24 11:28:12.984077.984077 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:12.984750.984750 cuda_h.py:19] end gpu_sexperts cost 0.0020380020141601562 seconds
DEBUG 12-24 11:28:12.984732.984732 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:12.984025.984025 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, bfb9b7ce-0603-4fe1-a5d9-dbc21f8edc8b
DEBUG 12-24 11:28:12.985530.985530 mlpmodule.py:668] pad cost 0.008181333541870117 s
DEBUG 12-24 11:28:12.985878.985878 mlpmodule.py:674] create cpu tensor cost 4.029273986816406e-05 s
DEBUG 12-24 11:28:12.985635.985635 mlpmodule.py:679] move to cpu cost 2.9802322387695312e-05 s
INFO 12-24 11:28:12.986682.986682 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1fdb1696-8d91-4379-9e5b-d1148c949c61
DEBUG 12-24 11:28:12.986837.986837 cuda_h.py:19] end load_into_gpu_async cost 0.002811908721923828 seconds
DEBUG 12-24 11:28:12.987447.987447 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:12.987543.987543 cuda_h.py:19] end restore_tensors2 cost 0.0003352165222167969 seconds
DEBUG 12-24 11:28:12.987789.987789 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004748344421386719 seconds
INFO 12-24 11:28:12.989344.989344 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1fdb1696-8d91-4379-9e5b-d1148c949c61
DEBUG 12-24 11:28:12.991403.991403 mlpmodule.py:694] group_w3: shape=torch.Size([31, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=89391104
DEBUG 12-24 11:28:12.991180.991180 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:12.991553.991553 mlpmodule.py:700] group_w3 first element: 0.07275390625
WARNING 12-24 11:28:12.991735.991735 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:12.995962.995962 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:12.996959.996959 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.000258.000258 mlpmodule.py:723] group einsum cost 0.015195846557617188 s
DEBUG 12-24 11:28:13.000298.000298 mlpmodule.py:731] cpy2cputensor cost 0.0003082752227783203 s
INFO 12-24 11:28:13.025986.025986 client.py:127] Model loaded
DEBUG 12-24 11:28:13.026746.026746 cuda_h.py:19] end wait_experts cost 0.04127907752990723 seconds
DEBUG 12-24 11:28:13.026555.026555 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.026503.026503 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:13.026221.026221 mlpmodule.py:457] gpu group tensors cost 0.000579833984375 s
DEBUG 12-24 11:28:13.028159.028159 mlpmodule.py:490] gpu pad cost 0.0014736652374267578 s
DEBUG 12-24 11:28:13.028270.028270 mlpmodule.py:588]  experts func einsum cost 0.05897188186645508 s
DEBUG 12-24 11:28:13.029431.029431 mlpmodule.py:508] gpu group einsum cost 0.0007205009460449219 s
INFO 12-24 11:28:13.031615.031615 client.py:127] Model loaded
DEBUG 12-24 11:28:13.031294.031294 cuda_h.py:19] end sllm_worker_task cost 0.04906868934631348 seconds
DEBUG 12-24 11:28:13.032682.032682 mlpmodule.py:537] gpu experts func einsum cost 0.006258487701416016 s
DEBUG 12-24 11:28:13.032123.032123 cuda_h.py:19] end gpu_experts cost 0.006467103958129883 seconds
DEBUG 12-24 11:28:13.032641.032641 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.032748.032748 cuda_h.py:19] end wait_cetm_experts cost 1.621246337890625e-05 seconds
DEBUG 12-24 11:28:13.032352.032352 lmp.py:615] gpu end - einsum end = 27.4ms
DEBUG 12-24 11:28:13.032400.032400 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.032892.032892 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.7404556274414062e-05 seconds
DEBUG 12-24 11:28:13.032495.032495 cuda_h.py:19] end layer_moe_generate_13 cost 0.06687688827514648 seconds
DEBUG 12-24 11:28:13.032342.032342 lmp.py:445] -------------------------------- end layer 13 --------------------------------
DEBUG 12-24 11:28:13.033813.033813 lmp.py:418] -------------------------------- start layer 14 --------------------------------
DEBUG 12-24 11:28:13.033364.033364 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.033737.033737 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.037377.037377 cuda_h.py:19] end self_attn cost 0.00421142578125 seconds
DEBUG 12-24 11:28:13.037220.037220 cuda_h.py:19] end iln_self_attn_paln cost 0.004781484603881836 seconds
DEBUG 12-24 11:28:13.037818.037818 cuda_h.py:10] start layer_moe_generate_14
DEBUG 12-24 11:28:13.037104.037104 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.038999.038999 cuda_h.py:19] end gate cost 0.0005908012390136719 seconds
DEBUG 12-24 11:28:13.038729.038729 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.038765.038765 lmp.py:519] 
DEBUG 12-24 11:28:13.038765.038765 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.038183.038183 lmp.py:520]   Total experts: 59
DEBUG 12-24 11:28:13.038700.038700 lmp.py:521]   CPU experts: 29 (49%)
DEBUG 12-24 11:28:13.038866.038866 lmp.py:522]   GPU experts: 30 (51%)
DEBUG 12-24 11:28:13.038648.038648 lmp.py:523] 
DEBUG 12-24 11:28:13.038648.038648 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.038384.038384 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.038126.038126 lmp.py:530]   Expert 18 |      1 | CPU
DEBUG 12-24 11:28:13.039101.039101 lmp.py:530]   Expert 49 |      1 | CPU
DEBUG 12-24 11:28:13.039359.039359 lmp.py:530]   Expert 34 |      2 | CPU
DEBUG 12-24 11:28:13.039141.039141 lmp.py:530]   Expert 48 |      2 | CPU
DEBUG 12-24 11:28:13.039923.039923 lmp.py:530]   Expert 15 |      3 | CPU
DEBUG 12-24 11:28:13.039705.039705 lmp.py:530]   Expert 54 |      3 | CPU
DEBUG 12-24 11:28:13.039487.039487 lmp.py:530]   Expert 55 |      3 | CPU
DEBUG 12-24 11:28:13.039130.039130 lmp.py:530]   Expert 57 |      3 | CPU
DEBUG 12-24 11:28:13.039150.039150 lmp.py:530]   Expert 62 |      3 | CPU
DEBUG 12-24 11:28:13.039694.039694 lmp.py:530]   Expert 35 |      4 | CPU
DEBUG 12-24 11:28:13.039999.039999 lmp.py:530]   Expert 50 |      4 | CPU
DEBUG 12-24 11:28:13.039781.039781 lmp.py:530]   Expert 51 |      4 | CPU
DEBUG 12-24 11:28:13.039325.039325 lmp.py:530]   Expert 21 |      5 | CPU
DEBUG 12-24 11:28:13.039868.039868 lmp.py:530]   Expert 56 |      5 | CPU
DEBUG 12-24 11:28:13.039412.039412 lmp.py:530]   Expert 39 |      6 | CPU
DEBUG 12-24 11:28:13.039717.039717 lmp.py:530]   Expert 45 |      6 | CPU
DEBUG 12-24 11:28:13.039260.039260 lmp.py:530]   Expert 20 |      7 | CPU
DEBUG 12-24 11:28:13.039804.039804 lmp.py:530]   Expert 29 |      7 | CPU
DEBUG 12-24 11:28:13.039347.039347 lmp.py:530]   Expert 43 |      7 | CPU
DEBUG 12-24 11:28:13.039652.039652 lmp.py:530]   Expert  8 |      8 | CPU
DEBUG 12-24 11:28:13.039196.039196 lmp.py:530]   Expert 16 |      9 | CPU
DEBUG 12-24 11:28:13.039740.039740 lmp.py:530]   Expert 31 |      9 | CPU
DEBUG 12-24 11:28:13.039045.039045 lmp.py:530]   Expert 40 |      9 | CPU
DEBUG 12-24 11:28:13.039065.039065 lmp.py:530]   Expert 44 |      9 | CPU
DEBUG 12-24 11:28:13.039609.039609 lmp.py:530]   Expert 14 |     10 | CPU
DEBUG 12-24 11:28:13.039914.039914 lmp.py:530]   Expert 41 |     10 | CPU
DEBUG 12-24 11:28:13.039457.039457 lmp.py:530]   Expert 27 |     12 | CPU
DEBUG 12-24 11:28:13.039762.039762 lmp.py:530]   Expert 52 |     12 | CPU
DEBUG 12-24 11:28:13.039544.039544 lmp.py:530]   Expert  6 |     13 | CPU
DEBUG 12-24 11:28:13.039088.039088 lmp.py:530]   Expert 11 |     13 | GPU
DEBUG 12-24 11:28:13.039631.039631 lmp.py:530]   Expert 23 |     13 | GPU
DEBUG 12-24 11:28:13.039936.039936 lmp.py:530]   Expert 42 |     13 | GPU
DEBUG 12-24 11:28:13.039242.039242 lmp.py:530]   Expert 19 |     14 | GPU
DEBUG 12-24 11:28:13.039785.039785 lmp.py:530]   Expert 63 |     14 | GPU
DEBUG 12-24 11:28:13.039329.039329 lmp.py:530]   Expert 12 |     15 | GPU
DEBUG 12-24 11:28:13.039872.039872 lmp.py:530]   Expert 37 |     15 | GPU
DEBUG 12-24 11:28:13.039177.039177 lmp.py:530]   Expert 24 |     17 | GPU
DEBUG 12-24 11:28:13.039721.039721 lmp.py:530]   Expert 30 |     17 | GPU
DEBUG 12-24 11:28:13.039264.039264 lmp.py:530]   Expert 53 |     17 | GPU
DEBUG 12-24 11:28:13.039569.039569 lmp.py:530]   Expert 60 |     17 | GPU
DEBUG 12-24 11:28:13.039875.039875 lmp.py:530]   Expert 10 |     19 | GPU
DEBUG 12-24 11:28:13.039180.039180 lmp.py:530]   Expert 32 |     20 | GPU
DEBUG 12-24 11:28:13.039485.039485 lmp.py:530]   Expert 13 |     21 | GPU
DEBUG 12-24 11:28:13.039552.039552 lmp.py:530]   Expert 28 |     22 | GPU
DEBUG 12-24 11:28:13.039857.039857 lmp.py:530]   Expert 22 |     23 | GPU
DEBUG 12-24 11:28:13.039162.039162 lmp.py:530]   Expert 33 |     25 | GPU
DEBUG 12-24 11:28:13.039467.039467 lmp.py:530]   Expert 46 |     25 | GPU
DEBUG 12-24 11:28:13.039772.039772 lmp.py:530]   Expert 36 |     26 | GPU
DEBUG 12-24 11:28:13.039839.039839 lmp.py:530]   Expert 26 |     27 | GPU
DEBUG 12-24 11:28:13.039382.039382 lmp.py:530]   Expert 25 |     28 | GPU
DEBUG 12-24 11:28:13.039687.039687 lmp.py:530]   Expert 47 |     34 | GPU
DEBUG 12-24 11:28:13.039754.039754 lmp.py:530]   Expert 58 |     37 | GPU
DEBUG 12-24 11:28:13.039059.039059 lmp.py:530]   Expert  9 |     43 | GPU
DEBUG 12-24 11:28:13.039603.039603 lmp.py:530]   Expert  2 |   1926 | GPU
DEBUG 12-24 11:28:13.039908.039908 lmp.py:530]   Expert  1 |   1929 | GPU
DEBUG 12-24 11:28:13.039213.039213 lmp.py:530]   Expert  4 |   1929 | GPU
DEBUG 12-24 11:28:13.039995.039995 lmp.py:530]   Expert  0 |   1935 | GPU
DEBUG 12-24 11:28:13.039062.039062 lmp.py:530]   Expert  5 |   1937 | GPU
DEBUG 12-24 11:28:13.039605.039605 lmp.py:530]   Expert  3 |   1940 | GPU
DEBUG 12-24 11:28:13.039625.039625 lmp.py:531] 
DEBUG 12-24 11:28:13.039625.039625 lmp.py:531]   CPU total tokens: 177 (1.4%)
DEBUG 12-24 11:28:13.039123.039123 lmp.py:532]   GPU total tokens: 12111 (98.6%)
DEBUG 12-24 11:28:13.039342.039342 cuda_h.py:19] end experts_map_get cost 0.0012691020965576172 seconds
DEBUG 12-24 11:28:13.039839.039839 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.039874.039874 lmp.py:541] 
DEBUG 12-24 11:28:13.039874.039874 lmp.py:541]   Computing 29 experts on CPU...
DEBUG 12-24 11:28:13.040995.040995 cuda_h.py:19] end cpu_experts_submit cost 9.417533874511719e-05 seconds
DEBUG 12-24 11:28:13.040069.040069 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.040891.040891 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.040644.040644 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.040199.040199 cuda_h.py:19] end allocate_cuda_memory cost 0.00030517578125 seconds
DEBUG 12-24 11:28:13.040241.040241 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.040427.040427 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.040093.040093 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.040425.040425 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1a426968-5ec0-4e58-a40b-2f8bc2ce1a5f
DEBUG 12-24 11:28:13.040875.040875 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:13.079965.079965 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1a426968-5ec0-4e58-a40b-2f8bc2ce1a5f
DEBUG 12-24 11:28:13.080249.080249 mlpmodule.py:630] group tensors cost 0.0387876033782959 s
DEBUG 12-24 11:28:13.081847.081847 cuda_h.py:19] end load_into_gpu_async cost 0.040593624114990234 seconds
DEBUG 12-24 11:28:13.081829.081829 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.082208.082208 cuda_h.py:19] end restore_tensors2 cost 0.0007607936859130859 seconds
DEBUG 12-24 11:28:13.082769.082769 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.042394399642944336 seconds
DEBUG 12-24 11:28:13.091210.091210 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.05163741111755371 seconds
DEBUG 12-24 11:28:13.091275.091275 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 12-24 11:28:13.092121.092121 cuda_h.py:10] start start_load_qkvogn_s_weight_l_15
DEBUG 12-24 11:28:13.092279.092279 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 5.555152893066406e-05 seconds
DEBUG 12-24 11:28:13.092565.092565 cuda_h.py:19] end start_load_qkvogn_s_weight_l_15 cost 0.00010919570922851562 seconds
DEBUG 12-24 11:28:13.092513.092513 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.092470.092470 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.092956.092956 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.092401.092401 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.092674.092674 cuda_h.py:19] end allocate_cuda_memory cost 0.00020313262939453125 seconds
DEBUG 12-24 11:28:13.092193.092193 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.092187.092187 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.092494.092494 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.092958.092958 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a1a3d0b0-1133-442a-8c8c-d321702191fb
DEBUG 12-24 11:28:13.092868.092868 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.093483.093483 cuda_h.py:19] end gpu_sexperts cost 0.0012328624725341797 seconds
DEBUG 12-24 11:28:13.093406.093406 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.093892.093892 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1a426968-5ec0-4e58-a40b-2f8bc2ce1a5f
INFO 12-24 11:28:13.093985.093985 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a1a3d0b0-1133-442a-8c8c-d321702191fb
DEBUG 12-24 11:28:13.093345.093345 cuda_h.py:19] end load_into_gpu_async cost 0.001131296157836914 seconds
DEBUG 12-24 11:28:13.093001.093001 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.094693.094693 cuda_h.py:19] end restore_tensors2 cost 6.031990051269531e-05 seconds
DEBUG 12-24 11:28:13.094210.094210 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0016303062438964844 seconds
INFO 12-24 11:28:13.094012.094012 client.py:127] Model loaded
DEBUG 12-24 11:28:13.094319.094319 cuda_h.py:19] end wait_experts cost 0.0008790493011474609 seconds
DEBUG 12-24 11:28:13.094419.094419 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.094143.094143 lmp.py:585]   Computing 30 experts on GPU...
INFO 12-24 11:28:13.095820.095820 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a1a3d0b0-1133-442a-8c8c-d321702191fb
DEBUG 12-24 11:28:13.095985.095985 mlpmodule.py:457] gpu group tensors cost 0.0007586479187011719 s
DEBUG 12-24 11:28:13.098660.098660 mlpmodule.py:668] pad cost 0.016855955123901367 s
DEBUG 12-24 11:28:13.098614.098614 mlpmodule.py:674] create cpu tensor cost 7.82012939453125e-05 s
DEBUG 12-24 11:28:13.098976.098976 mlpmodule.py:679] move to cpu cost 6.389617919921875e-05 s
DEBUG 12-24 11:28:13.099985.099985 mlpmodule.py:490] gpu pad cost 0.003607034683227539 s
DEBUG 12-24 11:28:13.103939.103939 mlpmodule.py:508] gpu group einsum cost 0.0035653114318847656 s
INFO 12-24 11:28:13.103477.103477 client.py:127] Model loaded
DEBUG 12-24 11:28:13.103951.103951 cuda_h.py:19] end sllm_worker_task cost 0.011510848999023438 seconds
DEBUG 12-24 11:28:13.104206.104206 mlpmodule.py:694] group_w3: shape=torch.Size([29, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=83623936
DEBUG 12-24 11:28:13.104919.104919 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.104797.104797 mlpmodule.py:700] group_w3 first element: 0.064453125
WARNING 12-24 11:28:13.104703.104703 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.107091.107091 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.108187.108187 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.109419.109419 mlpmodule.py:537] gpu experts func einsum cost 0.014293193817138672 s
DEBUG 12-24 11:28:13.109214.109214 cuda_h.py:19] end gpu_experts cost 0.014587163925170898 seconds
DEBUG 12-24 11:28:13.109554.109554 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.112486.112486 mlpmodule.py:723] group einsum cost 0.01357889175415039 s
DEBUG 12-24 11:28:13.112721.112721 mlpmodule.py:731] cpy2cputensor cost 0.00022172927856445312 s
DEBUG 12-24 11:28:13.116277.116277 cuda_h.py:19] end wait_cetm_experts cost 0.007040977478027344 seconds
DEBUG 12-24 11:28:13.116215.116215 lmp.py:615] gpu end - einsum end = -7.0ms
DEBUG 12-24 11:28:13.116518.116518 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.116342.116342 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 3.647804260253906e-05 seconds
DEBUG 12-24 11:28:13.116667.116667 cuda_h.py:19] end layer_moe_generate_14 cost 0.07893228530883789 seconds
DEBUG 12-24 11:28:13.117377.117377 lmp.py:445] -------------------------------- end layer 14 --------------------------------
DEBUG 12-24 11:28:13.117670.117670 lmp.py:418] -------------------------------- start layer 15 --------------------------------
DEBUG 12-24 11:28:13.117512.117512 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.117061.117061 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.120324.120324 cuda_h.py:19] end self_attn cost 0.0026378631591796875 seconds
DEBUG 12-24 11:28:13.120204.120204 cuda_h.py:19] end iln_self_attn_paln cost 0.0034253597259521484 seconds
DEBUG 12-24 11:28:13.120199.120199 cuda_h.py:10] start layer_moe_generate_15
DEBUG 12-24 11:28:13.120492.120492 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.121842.121842 cuda_h.py:19] end gate cost 0.0007100105285644531 seconds
DEBUG 12-24 11:28:13.121778.121778 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.121657.121657 lmp.py:519] 
DEBUG 12-24 11:28:13.121657.121657 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.121228.121228 lmp.py:520]   Total experts: 63
DEBUG 12-24 11:28:13.121646.121646 lmp.py:521]   CPU experts: 31 (49%)
DEBUG 12-24 11:28:13.121727.121727 lmp.py:522]   GPU experts: 32 (51%)
DEBUG 12-24 11:28:13.122661.122661 lmp.py:523] 
DEBUG 12-24 11:28:13.122661.122661 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.122072.122072 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.122636.122636 lmp.py:530]   Expert 15 |      1 | CPU
DEBUG 12-24 11:28:13.122094.122094 lmp.py:530]   Expert 41 |      2 | CPU
DEBUG 12-24 11:28:13.122836.122836 lmp.py:530]   Expert 43 |      2 | CPU
DEBUG 12-24 11:28:13.122340.122340 lmp.py:530]   Expert  6 |      3 | CPU
DEBUG 12-24 11:28:13.122083.122083 lmp.py:530]   Expert 34 |      3 | CPU
DEBUG 12-24 11:28:13.122348.122348 lmp.py:530]   Expert 40 |      3 | CPU
DEBUG 12-24 11:28:13.122091.122091 lmp.py:530]   Expert  9 |      4 | CPU
DEBUG 12-24 11:28:13.122072.122072 lmp.py:530]   Expert 51 |      4 | CPU
DEBUG 12-24 11:28:13.122576.122576 lmp.py:530]   Expert 57 |      4 | CPU
DEBUG 12-24 11:28:13.122841.122841 lmp.py:530]   Expert 32 |      5 | CPU
DEBUG 12-24 11:28:13.122299.122299 lmp.py:530]   Expert 52 |      5 | CPU
DEBUG 12-24 11:28:13.122757.122757 lmp.py:530]   Expert 18 |      6 | CPU
DEBUG 12-24 11:28:13.122022.122022 lmp.py:530]   Expert 22 |      6 | CPU
DEBUG 12-24 11:28:13.122003.122003 lmp.py:530]   Expert 53 |      6 | CPU
DEBUG 12-24 11:28:13.122798.122798 lmp.py:530]   Expert 59 |      6 | CPU
DEBUG 12-24 11:28:13.122495.122495 lmp.py:530]   Expert 10 |      7 | CPU
DEBUG 12-24 11:28:13.122760.122760 lmp.py:530]   Expert 37 |      7 | CPU
DEBUG 12-24 11:28:13.122026.122026 lmp.py:530]   Expert 42 |      7 | CPU
DEBUG 12-24 11:28:13.122530.122530 lmp.py:530]   Expert 45 |      7 | CPU
DEBUG 12-24 11:28:13.122795.122795 lmp.py:530]   Expert 56 |      7 | CPU
DEBUG 12-24 11:28:13.122822.122822 lmp.py:530]   Expert 25 |      8 | CPU
DEBUG 12-24 11:28:13.122519.122519 lmp.py:530]   Expert 36 |      8 | CPU
DEBUG 12-24 11:28:13.122976.122976 lmp.py:530]   Expert  7 |      9 | CPU
DEBUG 12-24 11:28:13.122195.122195 lmp.py:530]   Expert 16 |      9 | CPU
DEBUG 12-24 11:28:13.122892.122892 lmp.py:530]   Expert 48 |      9 | CPU
DEBUG 12-24 11:28:13.122157.122157 lmp.py:530]   Expert 60 |      9 | CPU
DEBUG 12-24 11:28:13.122423.122423 lmp.py:530]   Expert 23 |     10 | CPU
DEBUG 12-24 11:28:13.122211.122211 lmp.py:530]   Expert 35 |     10 | CPU
DEBUG 12-24 11:28:13.122000.122000 lmp.py:530]   Expert 61 |     10 | CPU
DEBUG 12-24 11:28:13.122458.122458 lmp.py:530]   Expert 14 |     12 | CPU
DEBUG 12-24 11:28:13.122916.122916 lmp.py:530]   Expert 31 |     12 | CPU
DEBUG 12-24 11:28:13.122658.122658 lmp.py:530]   Expert 46 |     12 | GPU
DEBUG 12-24 11:28:13.122877.122877 lmp.py:530]   Expert 55 |     12 | GPU
DEBUG 12-24 11:28:13.122904.122904 lmp.py:530]   Expert 12 |     13 | GPU
DEBUG 12-24 11:28:13.122932.122932 lmp.py:530]   Expert 24 |     13 | GPU
DEBUG 12-24 11:28:13.122720.122720 lmp.py:530]   Expert 19 |     14 | GPU
DEBUG 12-24 11:28:13.122747.122747 lmp.py:530]   Expert 21 |     14 | GPU
DEBUG 12-24 11:28:13.122536.122536 lmp.py:530]   Expert 49 |     14 | GPU
DEBUG 12-24 11:28:13.122325.122325 lmp.py:530]   Expert 54 |     14 | GPU
DEBUG 12-24 11:28:13.122352.122352 lmp.py:530]   Expert 62 |     14 | GPU
DEBUG 12-24 11:28:13.122618.122618 lmp.py:530]   Expert 13 |     15 | GPU
DEBUG 12-24 11:28:13.122075.122075 lmp.py:530]   Expert 17 |     15 | GPU
DEBUG 12-24 11:28:13.122295.122295 lmp.py:530]   Expert 29 |     15 | GPU
DEBUG 12-24 11:28:13.122037.122037 lmp.py:530]   Expert 27 |     16 | GPU
DEBUG 12-24 11:28:13.122018.122018 lmp.py:530]   Expert 33 |     16 | GPU
DEBUG 12-24 11:28:13.122283.122283 lmp.py:530]   Expert 38 |     16 | GPU
DEBUG 12-24 11:28:13.123549.123549 lmp.py:530]   Expert 44 |     17 | GPU
DEBUG 12-24 11:28:13.123099.123099 lmp.py:530]   Expert 47 |     18 | GPU
DEBUG 12-24 11:28:13.123365.123365 lmp.py:530]   Expert 20 |     20 | GPU
DEBUG 12-24 11:28:13.123154.123154 lmp.py:530]   Expert 58 |     20 | GPU
DEBUG 12-24 11:28:13.123181.123181 lmp.py:530]   Expert 50 |     22 | GPU
DEBUG 12-24 11:28:13.123208.123208 lmp.py:530]   Expert 26 |     23 | GPU
DEBUG 12-24 11:28:13.123997.123997 lmp.py:530]   Expert 28 |     25 | GPU
DEBUG 12-24 11:28:13.123501.123501 lmp.py:530]   Expert 30 |     28 | GPU
DEBUG 12-24 11:28:13.123482.123482 lmp.py:530]   Expert 11 |     34 | GPU
DEBUG 12-24 11:28:13.123986.123986 lmp.py:530]   Expert 39 |     41 | GPU
DEBUG 12-24 11:28:13.123966.123966 lmp.py:530]   Expert  8 |     43 | GPU
DEBUG 12-24 11:28:13.123470.123470 lmp.py:530]   Expert  2 |   1922 | GPU
DEBUG 12-24 11:28:13.123498.123498 lmp.py:530]   Expert  5 |   1924 | GPU
DEBUG 12-24 11:28:13.123286.123286 lmp.py:530]   Expert  4 |   1926 | GPU
DEBUG 12-24 11:28:13.123075.123075 lmp.py:530]   Expert  0 |   1932 | GPU
DEBUG 12-24 11:28:13.123102.123102 lmp.py:530]   Expert  1 |   1937 | GPU
DEBUG 12-24 11:28:13.123129.123129 lmp.py:530]   Expert  3 |   1942 | GPU
DEBUG 12-24 11:28:13.123110.123110 lmp.py:531] 
DEBUG 12-24 11:28:13.123110.123110 lmp.py:531]   CPU total tokens: 201 (1.6%)
DEBUG 12-24 11:28:13.123568.123568 lmp.py:532]   GPU total tokens: 12087 (98.4%)
DEBUG 12-24 11:28:13.123509.123509 cuda_h.py:19] end experts_map_get cost 0.001840829849243164 seconds
DEBUG 12-24 11:28:13.123258.123258 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.123075.123075 lmp.py:541] 
DEBUG 12-24 11:28:13.123075.123075 lmp.py:541]   Computing 31 experts on CPU...
DEBUG 12-24 11:28:13.123303.123303 cuda_h.py:19] end cpu_experts_submit cost 0.00011134147644042969 seconds
DEBUG 12-24 11:28:13.123145.123145 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.123087.123087 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.123503.123503 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.126190.126190 cuda_h.py:19] end allocate_cuda_memory cost 0.002469778060913086 seconds
DEBUG 12-24 11:28:13.126199.126199 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.126108.126108 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.126136.126136 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.126799.126799 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 9ec6fce2-eb13-4b70-983d-d73f4d8bd704
DEBUG 12-24 11:28:13.126072.126072 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:13.127457.127457 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 9ec6fce2-eb13-4b70-983d-d73f4d8bd704
DEBUG 12-24 11:28:13.128108.128108 cuda_h.py:19] end load_into_gpu_async cost 0.0015807151794433594 seconds
DEBUG 12-24 11:28:13.128810.128810 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.130305.130305 cuda_h.py:19] end restore_tensors2 cost 0.0024726390838623047 seconds
DEBUG 12-24 11:28:13.130076.130076 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.006952047348022461 seconds
DEBUG 12-24 11:28:13.131174.131174 mlpmodule.py:588]  experts func einsum cost 0.09038972854614258 s
DEBUG 12-24 11:28:13.143666.143666 mlpmodule.py:630] group tensors cost 0.011326789855957031 s
DEBUG 12-24 11:28:13.146975.146975 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.0231778621673584 seconds
DEBUG 12-24 11:28:13.146953.146953 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 12-24 11:28:13.147312.147312 cuda_h.py:10] start start_load_qkvogn_s_weight_l_16
DEBUG 12-24 11:28:13.147407.147407 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 3.147125244140625e-05 seconds
DEBUG 12-24 11:28:13.147554.147554 cuda_h.py:19] end start_load_qkvogn_s_weight_l_16 cost 7.319450378417969e-05 seconds
DEBUG 12-24 11:28:13.147258.147258 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.147148.147148 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.147782.147782 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.147750.147750 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.148587.148587 cuda_h.py:19] end allocate_cuda_memory cost 0.0007998943328857422 seconds
DEBUG 12-24 11:28:13.148007.148007 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.148320.148320 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.148932.148932 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.148655.148655 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 57fbf986-8a5f-48b3-aaf6-1c56861637ac
DEBUG 12-24 11:28:13.148804.148804 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.149748.149748 cuda_h.py:19] end gpu_sexperts cost 0.0018413066864013672 seconds
DEBUG 12-24 11:28:13.149856.149856 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.149156.149156 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 9ec6fce2-eb13-4b70-983d-d73f4d8bd704
INFO 12-24 11:28:13.149808.149808 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 57fbf986-8a5f-48b3-aaf6-1c56861637ac
DEBUG 12-24 11:28:13.149017.149017 cuda_h.py:19] end load_into_gpu_async cost 0.0012135505676269531 seconds
DEBUG 12-24 11:28:13.149362.149362 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.150638.150638 cuda_h.py:19] end restore_tensors2 cost 8.749961853027344e-05 seconds
DEBUG 12-24 11:28:13.150806.150806 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0024917125701904297 seconds
INFO 12-24 11:28:13.150593.150593 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 57fbf986-8a5f-48b3-aaf6-1c56861637ac
DEBUG 12-24 11:28:13.151762.151762 mlpmodule.py:668] pad cost 0.007181406021118164 s
DEBUG 12-24 11:28:13.151772.151772 mlpmodule.py:674] create cpu tensor cost 4.0531158447265625e-05 s
DEBUG 12-24 11:28:13.151999.151999 mlpmodule.py:679] move to cpu cost 2.86102294921875e-05 s
DEBUG 12-24 11:28:13.156646.156646 mlpmodule.py:694] group_w3: shape=torch.Size([31, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=89391104
DEBUG 12-24 11:28:13.157841.157841 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.157061.157061 mlpmodule.py:700] group_w3 first element: -0.004180908203125
WARNING 12-24 11:28:13.157084.157084 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.161886.161886 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.161998.161998 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.166175.166175 mlpmodule.py:723] group einsum cost 0.014309883117675781 s
DEBUG 12-24 11:28:13.166572.166572 mlpmodule.py:731] cpy2cputensor cost 0.00010371208190917969 s
INFO 12-24 11:28:13.183337.183337 client.py:127] Model loaded
DEBUG 12-24 11:28:13.184554.184554 cuda_h.py:19] end wait_experts cost 0.034620046615600586 seconds
DEBUG 12-24 11:28:13.184847.184847 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.184417.184417 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:13.184718.184718 mlpmodule.py:457] gpu group tensors cost 0.0005834102630615234 s
DEBUG 12-24 11:28:13.186499.186499 mlpmodule.py:588]  experts func einsum cost 0.054029226303100586 s
INFO 12-24 11:28:13.186602.186602 client.py:127] Model loaded
DEBUG 12-24 11:28:13.186638.186638 cuda_h.py:19] end sllm_worker_task cost 0.039615631103515625 seconds
DEBUG 12-24 11:28:13.188524.188524 mlpmodule.py:490] gpu pad cost 0.0034389495849609375 s
DEBUG 12-24 11:28:13.189519.189519 mlpmodule.py:508] gpu group einsum cost 0.0007410049438476562 s
DEBUG 12-24 11:28:13.191890.191890 mlpmodule.py:537] gpu experts func einsum cost 0.00770115852355957 s
DEBUG 12-24 11:28:13.192350.192350 cuda_h.py:19] end gpu_experts cost 0.007895231246948242 seconds
DEBUG 12-24 11:28:13.192292.192292 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.192439.192439 cuda_h.py:19] end wait_cetm_experts cost 1.3589859008789062e-05 seconds
DEBUG 12-24 11:28:13.192135.192135 lmp.py:615] gpu end - einsum end = 21.6ms
DEBUG 12-24 11:28:13.192806.192806 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.192390.192390 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.6689300537109375e-05 seconds
DEBUG 12-24 11:28:13.192086.192086 cuda_h.py:19] end layer_moe_generate_15 cost 0.07155036926269531 seconds
DEBUG 12-24 11:28:13.192828.192828 lmp.py:445] -------------------------------- end layer 15 --------------------------------
DEBUG 12-24 11:28:13.192783.192783 lmp.py:418] -------------------------------- start layer 16 --------------------------------
DEBUG 12-24 11:28:13.192618.192618 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.192356.192356 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.199537.199537 cuda_h.py:19] end self_attn cost 0.006331682205200195 seconds
DEBUG 12-24 11:28:13.199248.199248 cuda_h.py:19] end iln_self_attn_paln cost 0.0069315433502197266 seconds
DEBUG 12-24 11:28:13.199707.199707 cuda_h.py:10] start layer_moe_generate_16
DEBUG 12-24 11:28:13.199708.199708 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.200590.200590 cuda_h.py:19] end gate cost 0.0005822181701660156 seconds
DEBUG 12-24 11:28:13.200989.200989 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.200959.200959 lmp.py:519] 
DEBUG 12-24 11:28:13.200959.200959 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.200106.200106 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:13.200471.200471 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:13.200545.200545 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:13.200711.200711 lmp.py:523] 
DEBUG 12-24 11:28:13.200711.200711 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.200592.200592 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.200480.200480 lmp.py:530]   Expert 31 |      1 | CPU
DEBUG 12-24 11:28:13.200885.200885 lmp.py:530]   Expert 45 |      1 | CPU
DEBUG 12-24 11:28:13.200097.200097 lmp.py:530]   Expert 58 |      1 | CPU
DEBUG 12-24 11:28:13.200263.200263 lmp.py:530]   Expert  6 |      2 | CPU
DEBUG 12-24 11:28:13.200237.200237 lmp.py:530]   Expert 38 |      2 | CPU
DEBUG 12-24 11:28:13.200688.200688 lmp.py:530]   Expert 49 |      2 | CPU
DEBUG 12-24 11:28:13.200186.200186 lmp.py:530]   Expert 60 |      2 | CPU
DEBUG 12-24 11:28:13.200352.200352 lmp.py:530]   Expert 14 |      3 | CPU
DEBUG 12-24 11:28:13.200279.200279 lmp.py:530]   Expert 18 |      4 | CPU
DEBUG 12-24 11:28:13.200969.200969 lmp.py:530]   Expert 28 |      4 | CPU
DEBUG 12-24 11:28:13.200420.200420 lmp.py:530]   Expert 39 |      4 | CPU
DEBUG 12-24 11:28:13.200347.200347 lmp.py:530]   Expert 54 |      4 | CPU
DEBUG 12-24 11:28:13.200321.200321 lmp.py:530]   Expert 61 |      4 | CPU
DEBUG 12-24 11:28:13.200057.200057 lmp.py:530]   Expert 32 |      5 | CPU
DEBUG 12-24 11:28:13.200031.200031 lmp.py:530]   Expert 34 |      5 | CPU
DEBUG 12-24 11:28:13.200005.200005 lmp.py:530]   Expert 36 |      5 | CPU
DEBUG 12-24 11:28:13.200218.200218 lmp.py:530]   Expert 40 |      5 | CPU
DEBUG 12-24 11:28:13.200953.200953 lmp.py:530]   Expert 21 |      6 | CPU
DEBUG 12-24 11:28:13.200450.200450 lmp.py:530]   Expert 41 |      6 | CPU
DEBUG 12-24 11:28:13.200948.200948 lmp.py:530]   Expert 47 |      6 | CPU
DEBUG 12-24 11:28:13.200922.200922 lmp.py:530]   Expert 13 |      7 | CPU
DEBUG 12-24 11:28:13.201896.201896 lmp.py:530]   Expert 20 |      7 | CPU
DEBUG 12-24 11:28:13.201347.201347 lmp.py:530]   Expert 25 |      7 | CPU
DEBUG 12-24 11:28:13.201797.201797 lmp.py:530]   Expert 26 |      7 | CPU
DEBUG 12-24 11:28:13.201487.201487 lmp.py:530]   Expert 11 |      8 | CPU
DEBUG 12-24 11:28:13.201176.201176 lmp.py:530]   Expert 12 |      8 | CPU
DEBUG 12-24 11:28:13.201388.201388 lmp.py:530]   Expert 19 |      8 | CPU
DEBUG 12-24 11:28:13.201124.201124 lmp.py:530]   Expert 48 |      8 | CPU
DEBUG 12-24 11:28:13.201621.201621 lmp.py:530]   Expert 16 |      9 | CPU
DEBUG 12-24 11:28:13.201357.201357 lmp.py:530]   Expert 35 |      9 | CPU
DEBUG 12-24 11:28:13.201854.201854 lmp.py:530]   Expert 52 |      9 | CPU
DEBUG 12-24 11:28:13.201590.201590 lmp.py:530]   Expert 59 |      9 | CPU
DEBUG 12-24 11:28:13.201325.201325 lmp.py:530]   Expert  7 |     10 | GPU
DEBUG 12-24 11:28:13.201823.201823 lmp.py:530]   Expert 37 |     10 | GPU
DEBUG 12-24 11:28:13.201273.201273 lmp.py:530]   Expert  8 |     11 | GPU
DEBUG 12-24 11:28:13.201724.201724 lmp.py:530]   Expert 10 |     11 | GPU
DEBUG 12-24 11:28:13.201414.201414 lmp.py:530]   Expert 51 |     11 | GPU
DEBUG 12-24 11:28:13.201103.201103 lmp.py:530]   Expert 57 |     11 | GPU
DEBUG 12-24 11:28:13.201031.201031 lmp.py:530]   Expert 24 |     12 | GPU
DEBUG 12-24 11:28:13.201005.201005 lmp.py:530]   Expert 23 |     13 | GPU
DEBUG 12-24 11:28:13.201740.201740 lmp.py:530]   Expert 55 |     13 | GPU
DEBUG 12-24 11:28:13.201238.201238 lmp.py:530]   Expert 53 |     14 | GPU
DEBUG 12-24 11:28:13.201212.201212 lmp.py:530]   Expert 15 |     15 | GPU
DEBUG 12-24 11:28:13.201709.201709 lmp.py:530]   Expert 44 |     15 | GPU
DEBUG 12-24 11:28:13.201444.201444 lmp.py:530]   Expert 22 |     16 | GPU
DEBUG 12-24 11:28:13.201180.201180 lmp.py:530]   Expert 30 |     18 | GPU
DEBUG 12-24 11:28:13.201916.201916 lmp.py:530]   Expert 33 |     20 | GPU
DEBUG 12-24 11:28:13.201174.201174 lmp.py:530]   Expert 42 |     20 | GPU
DEBUG 12-24 11:28:13.201149.201149 lmp.py:530]   Expert 43 |     20 | GPU
DEBUG 12-24 11:28:13.201361.201361 lmp.py:530]   Expert  9 |     21 | GPU
DEBUG 12-24 11:28:13.201812.201812 lmp.py:530]   Expert 46 |     21 | GPU
DEBUG 12-24 11:28:13.201024.201024 lmp.py:530]   Expert 17 |     22 | GPU
DEBUG 12-24 11:28:13.201952.201952 lmp.py:530]   Expert 56 |     22 | GPU
DEBUG 12-24 11:28:13.201449.201449 lmp.py:530]   Expert 63 |     24 | GPU
DEBUG 12-24 11:28:13.201185.201185 lmp.py:530]   Expert 29 |     25 | GPU
DEBUG 12-24 11:28:13.201682.201682 lmp.py:530]   Expert 50 |     25 | GPU
DEBUG 12-24 11:28:13.201179.201179 lmp.py:530]   Expert 27 |     38 | GPU
DEBUG 12-24 11:28:13.201677.201677 lmp.py:530]   Expert 62 |     50 | GPU
DEBUG 12-24 11:28:13.201412.201412 lmp.py:530]   Expert  0 |   1924 | GPU
DEBUG 12-24 11:28:13.201909.201909 lmp.py:530]   Expert  4 |   1924 | GPU
DEBUG 12-24 11:28:13.201407.201407 lmp.py:530]   Expert  1 |   1938 | GPU
DEBUG 12-24 11:28:13.201665.201665 lmp.py:530]   Expert  3 |   1938 | GPU
DEBUG 12-24 11:28:13.201163.201163 lmp.py:530]   Expert  2 |   1942 | GPU
DEBUG 12-24 11:28:13.201137.201137 lmp.py:530]   Expert  5 |   1966 | GPU
DEBUG 12-24 11:28:13.201303.201303 lmp.py:531] 
DEBUG 12-24 11:28:13.201303.201303 lmp.py:531]   CPU total tokens: 168 (1.4%)
DEBUG 12-24 11:28:13.201946.201946 lmp.py:532]   GPU total tokens: 12120 (98.6%)
DEBUG 12-24 11:28:13.201880.201880 cuda_h.py:19] end experts_map_get cost 0.0014712810516357422 seconds
DEBUG 12-24 11:28:13.201238.201238 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.201280.201280 lmp.py:541] 
DEBUG 12-24 11:28:13.201280.201280 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:13.201262.201262 cuda_h.py:19] end cpu_experts_submit cost 9.965896606445312e-05 seconds
DEBUG 12-24 11:28:13.201528.201528 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.201026.201026 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.202309.202309 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.202913.202913 cuda_h.py:19] end allocate_cuda_memory cost 0.0002002716064453125 seconds
DEBUG 12-24 11:28:13.202856.202856 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.202234.202234 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.202523.202523 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.202616.202616 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, c076f322-c587-4bad-b8ad-452d69232ccd
DEBUG 12-24 11:28:13.202252.202252 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:13.224782.224782 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, c076f322-c587-4bad-b8ad-452d69232ccd
DEBUG 12-24 11:28:13.224714.224714 mlpmodule.py:630] group tensors cost 0.02160954475402832 s
DEBUG 12-24 11:28:13.225036.225036 cuda_h.py:19] end load_into_gpu_async cost 0.022937774658203125 seconds
DEBUG 12-24 11:28:13.225858.225858 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.226122.226122 cuda_h.py:19] end restore_tensors2 cost 0.0005221366882324219 seconds
DEBUG 12-24 11:28:13.226708.226708 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.024277448654174805 seconds
DEBUG 12-24 11:28:13.231486.231486 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.029955387115478516 seconds
DEBUG 12-24 11:28:13.231668.231668 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 12-24 11:28:13.232041.232041 cuda_h.py:10] start start_load_qkvogn_s_weight_l_17
DEBUG 12-24 11:28:13.232633.232633 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 3.457069396972656e-05 seconds
DEBUG 12-24 11:28:13.232416.232416 cuda_h.py:19] end start_load_qkvogn_s_weight_l_17 cost 8.845329284667969e-05 seconds
DEBUG 12-24 11:28:13.232610.232610 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.232729.232729 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.232747.232747 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.232651.232651 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.233661.233661 cuda_h.py:19] end allocate_cuda_memory cost 0.0011897087097167969 seconds
DEBUG 12-24 11:28:13.234976.234976 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.234694.234694 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.234061.234061 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.234256.234256 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, aa9846c9-d080-469d-b0c1-95f0cb19099d
DEBUG 12-24 11:28:13.234494.234494 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.234482.234482 cuda_h.py:19] end gpu_sexperts cost 0.0026679039001464844 seconds
DEBUG 12-24 11:28:13.235133.235133 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.235770.235770 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, c076f322-c587-4bad-b8ad-452d69232ccd
INFO 12-24 11:28:13.235016.235016 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, aa9846c9-d080-469d-b0c1-95f0cb19099d
DEBUG 12-24 11:28:13.235525.235525 cuda_h.py:19] end load_into_gpu_async cost 0.0016553401947021484 seconds
DEBUG 12-24 11:28:13.235534.235534 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.236131.236131 cuda_h.py:19] end restore_tensors2 cost 0.00013566017150878906 seconds
DEBUG 12-24 11:28:13.236306.236306 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0035643577575683594 seconds
INFO 12-24 11:28:13.240686.240686 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, aa9846c9-d080-469d-b0c1-95f0cb19099d
DEBUG 12-24 11:28:13.241094.241094 mlpmodule.py:668] pad cost 0.016086578369140625 s
DEBUG 12-24 11:28:13.241304.241304 mlpmodule.py:674] create cpu tensor cost 4.1961669921875e-05 s
DEBUG 12-24 11:28:13.241346.241346 mlpmodule.py:679] move to cpu cost 2.8371810913085938e-05 s
DEBUG 12-24 11:28:13.246130.246130 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:13.247121.247121 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.247753.247753 mlpmodule.py:700] group_w3 first element: -0.007537841796875
WARNING 12-24 11:28:13.247935.247935 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.251103.251103 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.251192.251192 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.256329.256329 mlpmodule.py:723] group einsum cost 0.014840364456176758 s
DEBUG 12-24 11:28:13.256457.256457 mlpmodule.py:731] cpy2cputensor cost 0.0001125335693359375 s
INFO 12-24 11:28:13.260314.260314 client.py:127] Model loaded
DEBUG 12-24 11:28:13.261271.261271 cuda_h.py:19] end wait_experts cost 0.026035070419311523 seconds
DEBUG 12-24 11:28:13.261226.261226 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.261843.261843 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:13.261823.261823 mlpmodule.py:457] gpu group tensors cost 0.0006985664367675781 s
INFO 12-24 11:28:13.263832.263832 client.py:127] Model loaded
DEBUG 12-24 11:28:13.263253.263253 cuda_h.py:19] end sllm_worker_task cost 0.030731916427612305 seconds
DEBUG 12-24 11:28:13.265377.265377 mlpmodule.py:490] gpu pad cost 0.0029935836791992188 s
DEBUG 12-24 11:28:13.273968.273968 mlpmodule.py:508] gpu group einsum cost 0.008022308349609375 s
DEBUG 12-24 11:28:13.275452.275452 mlpmodule.py:537] gpu experts func einsum cost 0.014716625213623047 s
DEBUG 12-24 11:28:13.276382.276382 cuda_h.py:19] end gpu_experts cost 0.01490163803100586 seconds
DEBUG 12-24 11:28:13.276608.276608 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.276285.276285 cuda_h.py:19] end wait_cetm_experts cost 1.6689300537109375e-05 seconds
DEBUG 12-24 11:28:13.276458.276458 lmp.py:615] gpu end - einsum end = 12.1ms
DEBUG 12-24 11:28:13.276838.276838 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.276952.276952 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.09808349609375e-05 seconds
DEBUG 12-24 11:28:13.276886.276886 cuda_h.py:19] end layer_moe_generate_16 cost 0.07671976089477539 seconds
DEBUG 12-24 11:28:13.276052.276052 lmp.py:445] -------------------------------- end layer 16 --------------------------------
DEBUG 12-24 11:28:13.276391.276391 lmp.py:418] -------------------------------- start layer 17 --------------------------------
DEBUG 12-24 11:28:13.276179.276179 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.276474.276474 mlpmodule.py:588]  experts func einsum cost 0.07383537292480469 s
DEBUG 12-24 11:28:13.276709.276709 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.279176.279176 cuda_h.py:19] end self_attn cost 0.002215862274169922 seconds
DEBUG 12-24 11:28:13.279410.279410 cuda_h.py:19] end iln_self_attn_paln cost 0.0029060840606689453 seconds
DEBUG 12-24 11:28:13.279008.279008 cuda_h.py:10] start layer_moe_generate_17
DEBUG 12-24 11:28:13.279625.279625 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.280692.280692 cuda_h.py:19] end gate cost 0.0005791187286376953 seconds
DEBUG 12-24 11:28:13.280276.280276 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.280320.280320 lmp.py:519] 
DEBUG 12-24 11:28:13.280320.280320 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.280507.280507 lmp.py:520]   Total experts: 60
DEBUG 12-24 11:28:13.280679.280679 lmp.py:521]   CPU experts: 30 (50%)
DEBUG 12-24 11:28:13.280515.280515 lmp.py:522]   GPU experts: 30 (50%)
DEBUG 12-24 11:28:13.280204.280204 lmp.py:523] 
DEBUG 12-24 11:28:13.280204.280204 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.280370.280370 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.280020.280020 lmp.py:530]   Expert  7 |      2 | CPU
DEBUG 12-24 11:28:13.280901.280901 lmp.py:530]   Expert 14 |      2 | CPU
DEBUG 12-24 11:28:13.280114.280114 lmp.py:530]   Expert 25 |      2 | CPU
DEBUG 12-24 11:28:13.280564.280564 lmp.py:530]   Expert 39 |      2 | CPU
DEBUG 12-24 11:28:13.280539.280539 lmp.py:530]   Expert 46 |      2 | CPU
DEBUG 12-24 11:28:13.280513.280513 lmp.py:530]   Expert 27 |      3 | CPU
DEBUG 12-24 11:28:13.280725.280725 lmp.py:530]   Expert  8 |      4 | CPU
DEBUG 12-24 11:28:13.280322.280322 lmp.py:530]   Expert 10 |      4 | CPU
DEBUG 12-24 11:28:13.280296.280296 lmp.py:530]   Expert 20 |      4 | CPU
DEBUG 12-24 11:28:13.280793.280793 lmp.py:530]   Expert 49 |      4 | CPU
DEBUG 12-24 11:28:13.280529.280529 lmp.py:530]   Expert 52 |      4 | CPU
DEBUG 12-24 11:28:13.280026.280026 lmp.py:530]   Expert 53 |      4 | CPU
DEBUG 12-24 11:28:13.280761.280761 lmp.py:530]   Expert 60 |      4 | CPU
DEBUG 12-24 11:28:13.280497.280497 lmp.py:530]   Expert 11 |      5 | CPU
DEBUG 12-24 11:28:13.280233.280233 lmp.py:530]   Expert 50 |      5 | CPU
DEBUG 12-24 11:28:13.280730.280730 lmp.py:530]   Expert 59 |      5 | CPU
DEBUG 12-24 11:28:13.280704.280704 lmp.py:530]   Expert 15 |      6 | CPU
DEBUG 12-24 11:28:13.280678.280678 lmp.py:530]   Expert 32 |      6 | CPU
DEBUG 12-24 11:28:13.280175.280175 lmp.py:530]   Expert 40 |      6 | CPU
DEBUG 12-24 11:28:13.280149.280149 lmp.py:530]   Expert 63 |      6 | CPU
DEBUG 12-24 11:28:13.280885.280885 lmp.py:530]   Expert 29 |      7 | CPU
DEBUG 12-24 11:28:13.281620.281620 lmp.py:530]   Expert 57 |      7 | CPU
DEBUG 12-24 11:28:13.281118.281118 lmp.py:530]   Expert 30 |      8 | CPU
DEBUG 12-24 11:28:13.281092.281092 lmp.py:530]   Expert 56 |      8 | CPU
DEBUG 12-24 11:28:13.281827.281827 lmp.py:530]   Expert  9 |      9 | CPU
DEBUG 12-24 11:28:13.281325.281325 lmp.py:530]   Expert 37 |      9 | CPU
DEBUG 12-24 11:28:13.281822.281822 lmp.py:530]   Expert 24 |     10 | CPU
DEBUG 12-24 11:28:13.281319.281319 lmp.py:530]   Expert 34 |     10 | CPU
DEBUG 12-24 11:28:13.281816.281816 lmp.py:530]   Expert 17 |     12 | CPU
DEBUG 12-24 11:28:13.281552.281552 lmp.py:530]   Expert 31 |     13 | CPU
DEBUG 12-24 11:28:13.281287.281287 lmp.py:530]   Expert 33 |     13 | GPU
DEBUG 12-24 11:28:13.281785.281785 lmp.py:530]   Expert 38 |     14 | GPU
DEBUG 12-24 11:28:13.281282.281282 lmp.py:530]   Expert 51 |     14 | GPU
DEBUG 12-24 11:28:13.281779.281779 lmp.py:530]   Expert 61 |     14 | GPU
DEBUG 12-24 11:28:13.281038.281038 lmp.py:530]   Expert 19 |     15 | GPU
DEBUG 12-24 11:28:13.281012.281012 lmp.py:530]   Expert 44 |     15 | GPU
DEBUG 12-24 11:28:13.281509.281509 lmp.py:530]   Expert 18 |     16 | GPU
DEBUG 12-24 11:28:13.281483.281483 lmp.py:530]   Expert 26 |     16 | GPU
DEBUG 12-24 11:28:13.281980.281980 lmp.py:530]   Expert 54 |     16 | GPU
DEBUG 12-24 11:28:13.281716.281716 lmp.py:530]   Expert 16 |     17 | GPU
DEBUG 12-24 11:28:13.281975.281975 lmp.py:530]   Expert 42 |     17 | GPU
DEBUG 12-24 11:28:13.281472.281472 lmp.py:530]   Expert 48 |     18 | GPU
DEBUG 12-24 11:28:13.281969.281969 lmp.py:530]   Expert 62 |     18 | GPU
DEBUG 12-24 11:28:13.281705.281705 lmp.py:530]   Expert  6 |     19 | GPU
DEBUG 12-24 11:28:13.281964.281964 lmp.py:530]   Expert 35 |     21 | GPU
DEBUG 12-24 11:28:13.281222.281222 lmp.py:530]   Expert 55 |     22 | GPU
DEBUG 12-24 11:28:13.281196.281196 lmp.py:530]   Expert 22 |     23 | GPU
DEBUG 12-24 11:28:13.281932.281932 lmp.py:530]   Expert 12 |     25 | GPU
DEBUG 12-24 11:28:13.281191.281191 lmp.py:530]   Expert 43 |     26 | GPU
DEBUG 12-24 11:28:13.281211.281211 lmp.py:530]   Expert 41 |     27 | GPU
DEBUG 12-24 11:28:13.281708.281708 lmp.py:530]   Expert 23 |     29 | GPU
DEBUG 12-24 11:28:13.281967.281967 lmp.py:530]   Expert 58 |     29 | GPU
DEBUG 12-24 11:28:13.281464.281464 lmp.py:530]   Expert 13 |     30 | GPU
DEBUG 12-24 11:28:13.281723.281723 lmp.py:530]   Expert 45 |     54 | GPU
DEBUG 12-24 11:28:13.281982.281982 lmp.py:530]   Expert  3 |   1923 | GPU
DEBUG 12-24 11:28:13.281718.281718 lmp.py:530]   Expert  1 |   1927 | GPU
DEBUG 12-24 11:28:13.281215.281215 lmp.py:530]   Expert  2 |   1928 | GPU
DEBUG 12-24 11:28:13.281950.281950 lmp.py:530]   Expert  0 |   1929 | GPU
DEBUG 12-24 11:28:13.281209.281209 lmp.py:530]   Expert  5 |   1943 | GPU
DEBUG 12-24 11:28:13.281674.281674 lmp.py:530]   Expert  4 |   1957 | GPU
DEBUG 12-24 11:28:13.281615.281615 lmp.py:531] 
DEBUG 12-24 11:28:13.281615.281615 lmp.py:531]   CPU total tokens: 173 (1.4%)
DEBUG 12-24 11:28:13.281066.281066 lmp.py:532]   GPU total tokens: 12115 (98.6%)
DEBUG 12-24 11:28:13.281093.281093 cuda_h.py:19] end experts_map_get cost 0.0014197826385498047 seconds
DEBUG 12-24 11:28:13.281067.281067 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.281486.281486 lmp.py:541] 
DEBUG 12-24 11:28:13.281486.281486 lmp.py:541]   Computing 30 experts on CPU...
DEBUG 12-24 11:28:13.281077.281077 cuda_h.py:19] end cpu_experts_submit cost 9.226799011230469e-05 seconds
DEBUG 12-24 11:28:13.281628.281628 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.281504.281504 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.281143.281143 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.282920.282920 cuda_h.py:19] end allocate_cuda_memory cost 0.0002262592315673828 seconds
DEBUG 12-24 11:28:13.282286.282286 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.282374.282374 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.282905.282905 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.282031.282031 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ae91548d-239f-4b85-9a9f-833b455cd793
DEBUG 12-24 11:28:13.282693.282693 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.318366.318366 mlpmodule.py:630] group tensors cost 0.03588700294494629 s
INFO 12-24 11:28:13.320451.320451 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ae91548d-239f-4b85-9a9f-833b455cd793
DEBUG 12-24 11:28:13.320903.320903 cuda_h.py:19] end load_into_gpu_async cost 0.03832864761352539 seconds
DEBUG 12-24 11:28:13.320004.320004 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.321961.321961 cuda_h.py:19] end restore_tensors2 cost 0.0006046295166015625 seconds
DEBUG 12-24 11:28:13.321096.321096 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.03962826728820801 seconds
DEBUG 12-24 11:28:13.328442.328442 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.04624533653259277 seconds
DEBUG 12-24 11:28:13.328083.328083 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 12-24 11:28:13.328338.328338 cuda_h.py:10] start start_load_qkvogn_s_weight_l_18
DEBUG 12-24 11:28:13.328343.328343 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 4.696846008300781e-05 seconds
DEBUG 12-24 11:28:13.328146.328146 cuda_h.py:19] end start_load_qkvogn_s_weight_l_18 cost 0.00011873245239257812 seconds
DEBUG 12-24 11:28:13.328545.328545 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.328306.328306 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.329077.329077 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.329566.329566 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.329673.329673 cuda_h.py:19] end allocate_cuda_memory cost 0.0005352497100830078 seconds
DEBUG 12-24 11:28:13.330904.330904 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.330887.330887 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.330708.330708 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.330532.330532 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 496fc2a8-3619-4a20-93d2-a4cea9a726e1
DEBUG 12-24 11:28:13.330287.330287 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.331203.331203 cuda_h.py:19] end gpu_sexperts cost 0.0025517940521240234 seconds
DEBUG 12-24 11:28:13.331665.331665 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.331913.331913 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ae91548d-239f-4b85-9a9f-833b455cd793
INFO 12-24 11:28:13.332648.332648 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 496fc2a8-3619-4a20-93d2-a4cea9a726e1
DEBUG 12-24 11:28:13.332575.332575 cuda_h.py:19] end load_into_gpu_async cost 0.002328157424926758 seconds
DEBUG 12-24 11:28:13.332082.332082 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.332806.332806 cuda_h.py:19] end restore_tensors2 cost 0.00017070770263671875 seconds
DEBUG 12-24 11:28:13.332055.332055 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0037758350372314453 seconds
INFO 12-24 11:28:13.333957.333957 client.py:127] Model loaded
DEBUG 12-24 11:28:13.333147.333147 cuda_h.py:19] end wait_experts cost 0.0018541812896728516 seconds
DEBUG 12-24 11:28:13.333467.333467 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.333350.333350 lmp.py:585]   Computing 30 experts on GPU...
INFO 12-24 11:28:13.335852.335852 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 496fc2a8-3619-4a20-93d2-a4cea9a726e1
DEBUG 12-24 11:28:13.335110.335110 mlpmodule.py:457] gpu group tensors cost 0.002244234085083008 s
DEBUG 12-24 11:28:13.337052.337052 mlpmodule.py:668] pad cost 0.016744375228881836 s
DEBUG 12-24 11:28:13.337747.337747 mlpmodule.py:674] create cpu tensor cost 0.00010037422180175781 s
DEBUG 12-24 11:28:13.337758.337758 mlpmodule.py:679] move to cpu cost 4.506111145019531e-05 s
DEBUG 12-24 11:28:13.339814.339814 mlpmodule.py:490] gpu pad cost 0.003773927688598633 s
DEBUG 12-24 11:28:13.341832.341832 mlpmodule.py:508] gpu group einsum cost 0.0010194778442382812 s
INFO 12-24 11:28:13.342903.342903 client.py:127] Model loaded
DEBUG 12-24 11:28:13.342465.342465 cuda_h.py:19] end sllm_worker_task cost 0.013184070587158203 seconds
DEBUG 12-24 11:28:13.342049.342049 mlpmodule.py:694] group_w3: shape=torch.Size([30, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=86507520
DEBUG 12-24 11:28:13.342762.342762 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.342336.342336 mlpmodule.py:700] group_w3 first element: 0.0400390625
WARNING 12-24 11:28:13.343207.343207 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.346296.346296 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.346720.346720 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.347877.347877 mlpmodule.py:537] gpu experts func einsum cost 0.013856887817382812 s
DEBUG 12-24 11:28:13.347315.347315 cuda_h.py:19] end gpu_experts cost 0.014197587966918945 seconds
DEBUG 12-24 11:28:13.347131.347131 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.350120.350120 mlpmodule.py:723] group einsum cost 0.013509988784790039 s
DEBUG 12-24 11:28:13.351525.351525 mlpmodule.py:731] cpy2cputensor cost 0.00012946128845214844 s
DEBUG 12-24 11:28:13.355323.355323 cuda_h.py:19] end wait_cetm_experts cost 0.0074310302734375 seconds
DEBUG 12-24 11:28:13.355738.355738 lmp.py:615] gpu end - einsum end = -7.4ms
DEBUG 12-24 11:28:13.355591.355591 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.355223.355223 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 3.6716461181640625e-05 seconds
DEBUG 12-24 11:28:13.355933.355933 cuda_h.py:19] end layer_moe_generate_17 cost 0.07603979110717773 seconds
DEBUG 12-24 11:28:13.355120.355120 lmp.py:445] -------------------------------- end layer 17 --------------------------------
DEBUG 12-24 11:28:13.355028.355028 lmp.py:418] -------------------------------- start layer 18 --------------------------------
DEBUG 12-24 11:28:13.355301.355301 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.356717.356717 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.359783.359783 cuda_h.py:19] end self_attn cost 0.0026807785034179688 seconds
DEBUG 12-24 11:28:13.359979.359979 cuda_h.py:19] end iln_self_attn_paln cost 0.0034160614013671875 seconds
DEBUG 12-24 11:28:13.359445.359445 cuda_h.py:10] start layer_moe_generate_18
DEBUG 12-24 11:28:13.359453.359453 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.360166.360166 cuda_h.py:19] end gate cost 0.0006949901580810547 seconds
DEBUG 12-24 11:28:13.360671.360671 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.360611.360611 lmp.py:519] 
DEBUG 12-24 11:28:13.360611.360611 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.360089.360089 lmp.py:520]   Total experts: 63
DEBUG 12-24 11:28:13.360554.360554 lmp.py:521]   CPU experts: 31 (49%)
DEBUG 12-24 11:28:13.360872.360872 lmp.py:522]   GPU experts: 32 (51%)
DEBUG 12-24 11:28:13.360807.360807 lmp.py:523] 
DEBUG 12-24 11:28:13.360807.360807 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.360695.360695 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.360544.360544 lmp.py:530]   Expert 37 |      1 | CPU
DEBUG 12-24 11:28:13.360478.360478 lmp.py:530]   Expert  8 |      2 | CPU
DEBUG 12-24 11:28:13.360459.360459 lmp.py:530]   Expert 43 |      2 | CPU
DEBUG 12-24 11:28:13.360201.360201 lmp.py:530]   Expert 53 |      2 | CPU
DEBUG 12-24 11:28:13.360705.360705 lmp.py:530]   Expert  6 |      4 | CPU
DEBUG 12-24 11:28:13.360733.360733 lmp.py:530]   Expert 17 |      4 | CPU
DEBUG 12-24 11:28:13.360475.360475 lmp.py:530]   Expert 35 |      4 | CPU
DEBUG 12-24 11:28:13.360217.360217 lmp.py:530]   Expert 20 |      5 | CPU
DEBUG 12-24 11:28:13.360721.360721 lmp.py:530]   Expert 32 |      5 | CPU
DEBUG 12-24 11:28:13.360948.360948 lmp.py:530]   Expert 40 |      5 | CPU
DEBUG 12-24 11:28:13.361028.361028 lmp.py:530]   Expert 23 |      6 | CPU
DEBUG 12-24 11:28:13.361009.361009 lmp.py:530]   Expert 29 |      6 | CPU
DEBUG 12-24 11:28:13.361513.361513 lmp.py:530]   Expert 34 |      6 | CPU
DEBUG 12-24 11:28:13.361255.361255 lmp.py:530]   Expert 41 |      6 | CPU
DEBUG 12-24 11:28:13.361282.361282 lmp.py:530]   Expert 12 |      7 | CPU
DEBUG 12-24 11:28:13.361548.361548 lmp.py:530]   Expert 14 |      7 | CPU
DEBUG 12-24 11:28:13.361813.361813 lmp.py:530]   Expert 30 |      7 | CPU
DEBUG 12-24 11:28:13.361370.361370 lmp.py:530]   Expert 33 |      7 | CPU
DEBUG 12-24 11:28:13.361828.361828 lmp.py:530]   Expert 46 |      7 | CPU
DEBUG 12-24 11:28:13.361047.361047 lmp.py:530]   Expert 54 |      7 | CPU
DEBUG 12-24 11:28:13.361313.361313 lmp.py:530]   Expert 24 |      8 | CPU
DEBUG 12-24 11:28:13.361340.361340 lmp.py:530]   Expert 38 |      8 | CPU
DEBUG 12-24 11:28:13.361367.361367 lmp.py:530]   Expert 39 |      8 | CPU
DEBUG 12-24 11:28:13.361394.361394 lmp.py:530]   Expert 51 |      8 | CPU
DEBUG 12-24 11:28:13.361945.361945 lmp.py:530]   Expert 44 |      9 | CPU
DEBUG 12-24 11:28:13.361734.361734 lmp.py:530]   Expert 49 |      9 | CPU
DEBUG 12-24 11:28:13.361761.361761 lmp.py:530]   Expert 58 |      9 | CPU
DEBUG 12-24 11:28:13.361218.361218 lmp.py:530]   Expert 60 |      9 | CPU
DEBUG 12-24 11:28:13.361438.361438 lmp.py:530]   Expert 61 |      9 | CPU
DEBUG 12-24 11:28:13.361657.361657 lmp.py:530]   Expert 25 |     10 | CPU
DEBUG 12-24 11:28:13.361399.361399 lmp.py:530]   Expert 45 |     10 | CPU
DEBUG 12-24 11:28:13.361903.361903 lmp.py:530]   Expert 57 |     10 | GPU
DEBUG 12-24 11:28:13.361930.361930 lmp.py:530]   Expert 13 |     11 | GPU
DEBUG 12-24 11:28:13.361481.361481 lmp.py:530]   Expert 50 |     12 | GPU
DEBUG 12-24 11:28:13.361508.361508 lmp.py:530]   Expert 63 |     13 | GPU
DEBUG 12-24 11:28:13.361297.361297 lmp.py:530]   Expert 16 |     14 | GPU
DEBUG 12-24 11:28:13.361324.361324 lmp.py:530]   Expert 27 |     14 | GPU
DEBUG 12-24 11:28:13.361351.361351 lmp.py:530]   Expert 55 |     14 | GPU
DEBUG 12-24 11:28:13.361047.361047 lmp.py:530]   Expert  9 |     15 | GPU
DEBUG 12-24 11:28:13.361505.361505 lmp.py:530]   Expert 11 |     15 | GPU
DEBUG 12-24 11:28:13.361724.361724 lmp.py:530]   Expert 56 |     15 | GPU
DEBUG 12-24 11:28:13.361943.361943 lmp.py:530]   Expert  7 |     16 | GPU
DEBUG 12-24 11:28:13.361447.361447 lmp.py:530]   Expert 22 |     16 | GPU
DEBUG 12-24 11:28:13.361474.361474 lmp.py:530]   Expert 18 |     17 | GPU
DEBUG 12-24 11:28:13.361502.361502 lmp.py:530]   Expert 47 |     18 | GPU
DEBUG 12-24 11:28:13.361529.361529 lmp.py:530]   Expert 48 |     19 | GPU
DEBUG 12-24 11:28:13.361556.361556 lmp.py:530]   Expert 42 |     22 | GPU
DEBUG 12-24 11:28:13.361583.361583 lmp.py:530]   Expert 10 |     24 | GPU
DEBUG 12-24 11:28:13.361133.361133 lmp.py:530]   Expert 21 |     25 | GPU
DEBUG 12-24 11:28:13.361591.361591 lmp.py:530]   Expert 31 |     25 | GPU
DEBUG 12-24 11:28:13.361049.361049 lmp.py:530]   Expert 15 |     26 | GPU
DEBUG 12-24 11:28:13.361506.361506 lmp.py:530]   Expert 52 |     26 | GPU
DEBUG 12-24 11:28:13.361726.361726 lmp.py:530]   Expert 59 |     26 | GPU
DEBUG 12-24 11:28:13.361991.361991 lmp.py:530]   Expert 36 |     27 | GPU
DEBUG 12-24 11:28:13.361257.361257 lmp.py:530]   Expert 28 |     32 | GPU
DEBUG 12-24 11:28:13.361807.361807 lmp.py:530]   Expert 62 |     32 | GPU
DEBUG 12-24 11:28:13.361834.361834 lmp.py:530]   Expert 26 |     40 | GPU
DEBUG 12-24 11:28:13.361623.361623 lmp.py:530]   Expert  5 |   1921 | GPU
DEBUG 12-24 11:28:13.361650.361650 lmp.py:530]   Expert  3 |   1924 | GPU
DEBUG 12-24 11:28:13.362631.362631 lmp.py:530]   Expert  0 |   1927 | GPU
DEBUG 12-24 11:28:13.362612.362612 lmp.py:530]   Expert  4 |   1928 | GPU
DEBUG 12-24 11:28:13.362454.362454 lmp.py:530]   Expert  1 |   1932 | GPU
DEBUG 12-24 11:28:13.362388.362388 lmp.py:530]   Expert  2 |   1935 | GPU
DEBUG 12-24 11:28:13.362369.362369 lmp.py:531] 
DEBUG 12-24 11:28:13.362369.362369 lmp.py:531]   CPU total tokens: 197 (1.6%)
DEBUG 12-24 11:28:13.362350.362350 lmp.py:532]   GPU total tokens: 12091 (98.4%)
DEBUG 12-24 11:28:13.362053.362053 cuda_h.py:19] end experts_map_get cost 0.0018625259399414062 seconds
DEBUG 12-24 11:28:13.362510.362510 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.362135.362135 lmp.py:541] 
DEBUG 12-24 11:28:13.362135.362135 lmp.py:541]   Computing 31 experts on CPU...
DEBUG 12-24 11:28:13.362078.362078 cuda_h.py:19] end cpu_experts_submit cost 0.00011277198791503906 seconds
DEBUG 12-24 11:28:13.362589.362589 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.362670.362670 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.362464.362464 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.365044.365044 cuda_h.py:19] end allocate_cuda_memory cost 0.002460956573486328 seconds
DEBUG 12-24 11:28:13.365160.365160 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.365499.365499 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.365573.365573 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.365667.365667 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, fd65c7b5-f75d-4c71-babd-9a8a390cd64e
DEBUG 12-24 11:28:13.365986.365986 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:13.367093.367093 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, fd65c7b5-f75d-4c71-babd-9a8a390cd64e
DEBUG 12-24 11:28:13.367598.367598 cuda_h.py:19] end load_into_gpu_async cost 0.002566814422607422 seconds
DEBUG 12-24 11:28:13.367155.367155 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.368113.368113 cuda_h.py:19] end restore_tensors2 cost 0.0002949237823486328 seconds
DEBUG 12-24 11:28:13.368743.368743 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.005716562271118164 seconds
DEBUG 12-24 11:28:13.370374.370374 mlpmodule.py:588]  experts func einsum cost 0.08779549598693848 s
DEBUG 12-24 11:28:13.370228.370228 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.008573293685913086 seconds
DEBUG 12-24 11:28:13.371087.371087 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 12-24 11:28:13.371956.371956 cuda_h.py:10] start start_load_qkvogn_s_weight_l_19
DEBUG 12-24 11:28:13.371422.371422 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 2.9802322387695312e-05 seconds
DEBUG 12-24 11:28:13.371794.371794 cuda_h.py:19] end start_load_qkvogn_s_weight_l_19 cost 6.413459777832031e-05 seconds
DEBUG 12-24 11:28:13.371775.371775 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.371920.371920 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.371711.371711 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.371428.371428 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.372600.372600 cuda_h.py:19] end allocate_cuda_memory cost 0.0008072853088378906 seconds
DEBUG 12-24 11:28:13.373023.373023 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.373867.373867 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.373977.373977 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.373701.373701 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ec2e713c-3c2a-48c0-b5d0-d94bea1bbe09
DEBUG 12-24 11:28:13.380355.380355 mlpmodule.py:630] group tensors cost 0.0066983699798583984 s
DEBUG 12-24 11:28:13.381035.381035 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.381789.381789 cuda_h.py:19] end gpu_sexperts cost 0.010079383850097656 seconds
DEBUG 12-24 11:28:13.381016.381016 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.381753.381753 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, fd65c7b5-f75d-4c71-babd-9a8a390cd64e
INFO 12-24 11:28:13.383898.383898 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ec2e713c-3c2a-48c0-b5d0-d94bea1bbe09
DEBUG 12-24 11:28:13.383573.383573 cuda_h.py:19] end load_into_gpu_async cost 0.010204315185546875 seconds
DEBUG 12-24 11:28:13.383597.383597 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.383943.383943 cuda_h.py:19] end restore_tensors2 cost 0.0001704692840576172 seconds
DEBUG 12-24 11:28:13.383357.383357 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.011974573135375977 seconds
INFO 12-24 11:28:13.385990.385990 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ec2e713c-3c2a-48c0-b5d0-d94bea1bbe09
DEBUG 12-24 11:28:13.385319.385319 mlpmodule.py:668] pad cost 0.004275083541870117 s
DEBUG 12-24 11:28:13.385914.385914 mlpmodule.py:674] create cpu tensor cost 4.76837158203125e-05 s
DEBUG 12-24 11:28:13.385108.385108 mlpmodule.py:679] move to cpu cost 2.9325485229492188e-05 s
DEBUG 12-24 11:28:13.391913.391913 mlpmodule.py:694] group_w3: shape=torch.Size([31, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=89391104
DEBUG 12-24 11:28:13.391392.391392 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.391864.391864 mlpmodule.py:700] group_w3 first element: -0.0247802734375
WARNING 12-24 11:28:13.391556.391556 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.395856.395856 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.395954.395954 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.401080.401080 mlpmodule.py:723] group einsum cost 0.015138864517211914 s
DEBUG 12-24 11:28:13.401894.401894 mlpmodule.py:731] cpy2cputensor cost 0.00010228157043457031 s
DEBUG 12-24 11:28:13.419306.419306 mlpmodule.py:588]  experts func einsum cost 0.04642343521118164 s
INFO 12-24 11:28:13.426624.426624 client.py:127] Model loaded
DEBUG 12-24 11:28:13.427358.427358 cuda_h.py:19] end wait_experts cost 0.045157432556152344 seconds
DEBUG 12-24 11:28:13.427076.427076 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.427952.427952 lmp.py:585]   Computing 32 experts on GPU...
INFO 12-24 11:28:13.427837.427837 client.py:127] Model loaded
DEBUG 12-24 11:28:13.428389.428389 cuda_h.py:19] end sllm_worker_task cost 0.05644083023071289 seconds
DEBUG 12-24 11:28:13.428320.428320 mlpmodule.py:457] gpu group tensors cost 0.0011606216430664062 s
DEBUG 12-24 11:28:13.430523.430523 mlpmodule.py:490] gpu pad cost 0.00196075439453125 s
DEBUG 12-24 11:28:13.431950.431950 mlpmodule.py:508] gpu group einsum cost 0.0006203651428222656 s
DEBUG 12-24 11:28:13.434187.434187 mlpmodule.py:537] gpu experts func einsum cost 0.007513284683227539 s
DEBUG 12-24 11:28:13.434899.434899 cuda_h.py:19] end gpu_experts cost 0.007727146148681641 seconds
DEBUG 12-24 11:28:13.434854.434854 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.434644.434644 cuda_h.py:19] end wait_cetm_experts cost 2.0742416381835938e-05 seconds
DEBUG 12-24 11:28:13.435830.435830 lmp.py:615] gpu end - einsum end = 29.5ms
DEBUG 12-24 11:28:13.435800.435800 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.435789.435789 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2649765014648438e-05 seconds
DEBUG 12-24 11:28:13.435022.435022 cuda_h.py:19] end layer_moe_generate_18 cost 0.0757443904876709 seconds
DEBUG 12-24 11:28:13.435710.435710 lmp.py:445] -------------------------------- end layer 18 --------------------------------
DEBUG 12-24 11:28:13.435718.435718 lmp.py:418] -------------------------------- start layer 19 --------------------------------
DEBUG 12-24 11:28:13.435514.435514 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.435564.435564 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.441019.441019 cuda_h.py:19] end self_attn cost 0.005612850189208984 seconds
DEBUG 12-24 11:28:13.441830.441830 cuda_h.py:19] end iln_self_attn_paln cost 0.006238698959350586 seconds
DEBUG 12-24 11:28:13.441620.441620 cuda_h.py:10] start layer_moe_generate_19
DEBUG 12-24 11:28:13.441859.441859 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.442821.442821 cuda_h.py:19] end gate cost 0.0006046295166015625 seconds
DEBUG 12-24 11:28:13.442982.442982 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.442429.442429 lmp.py:519] 
DEBUG 12-24 11:28:13.442429.442429 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.442039.442039 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:13.442212.442212 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:13.442954.442954 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:13.442359.442359 lmp.py:523] 
DEBUG 12-24 11:28:13.442359.442359 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.442002.442002 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.442844.442844 lmp.py:530]   Expert 60 |      1 | CPU
DEBUG 12-24 11:28:13.442248.442248 lmp.py:530]   Expert 63 |      1 | CPU
DEBUG 12-24 11:28:13.442699.442699 lmp.py:530]   Expert 12 |      2 | CPU
DEBUG 12-24 11:28:13.442150.442150 lmp.py:530]   Expert 24 |      2 | CPU
DEBUG 12-24 11:28:13.442601.442601 lmp.py:530]   Expert 55 |      2 | CPU
DEBUG 12-24 11:28:13.443052.443052 lmp.py:530]   Expert  6 |      3 | CPU
DEBUG 12-24 11:28:13.443026.443026 lmp.py:530]   Expert 23 |      3 | CPU
DEBUG 12-24 11:28:13.443238.443238 lmp.py:530]   Expert 30 |      3 | CPU
DEBUG 12-24 11:28:13.443974.443974 lmp.py:530]   Expert 42 |      3 | CPU
DEBUG 12-24 11:28:13.443948.443948 lmp.py:530]   Expert  8 |      4 | CPU
DEBUG 12-24 11:28:13.443922.443922 lmp.py:530]   Expert 48 |      4 | CPU
DEBUG 12-24 11:28:13.443658.443658 lmp.py:530]   Expert 59 |      4 | CPU
DEBUG 12-24 11:28:13.443632.443632 lmp.py:530]   Expert 16 |      5 | CPU
DEBUG 12-24 11:28:13.443606.443606 lmp.py:530]   Expert 21 |      5 | CPU
DEBUG 12-24 11:28:13.443533.443533 lmp.py:530]   Expert 27 |      5 | CPU
DEBUG 12-24 11:28:13.443223.443223 lmp.py:530]   Expert 41 |      5 | CPU
DEBUG 12-24 11:28:13.443389.443389 lmp.py:530]   Expert 47 |      5 | CPU
DEBUG 12-24 11:28:13.443317.443317 lmp.py:530]   Expert 50 |      5 | CPU
DEBUG 12-24 11:28:13.443291.443291 lmp.py:530]   Expert 52 |      5 | CPU
DEBUG 12-24 11:28:13.443026.443026 lmp.py:530]   Expert 56 |      5 | CPU
DEBUG 12-24 11:28:13.443762.443762 lmp.py:530]   Expert 13 |      6 | CPU
DEBUG 12-24 11:28:13.443736.443736 lmp.py:530]   Expert 32 |      6 | CPU
DEBUG 12-24 11:28:13.443472.443472 lmp.py:530]   Expert 34 |      7 | CPU
DEBUG 12-24 11:28:13.443207.443207 lmp.py:530]   Expert 58 |      7 | CPU
DEBUG 12-24 11:28:13.443181.443181 lmp.py:530]   Expert 11 |      8 | CPU
DEBUG 12-24 11:28:13.443917.443917 lmp.py:530]   Expert 18 |      8 | CPU
DEBUG 12-24 11:28:13.443891.443891 lmp.py:530]   Expert 31 |      8 | CPU
DEBUG 12-24 11:28:13.443388.443388 lmp.py:530]   Expert 35 |      8 | CPU
DEBUG 12-24 11:28:13.443124.443124 lmp.py:530]   Expert 62 |      8 | CPU
DEBUG 12-24 11:28:13.443575.443575 lmp.py:530]   Expert 37 |      9 | CPU
DEBUG 12-24 11:28:13.443787.443787 lmp.py:530]   Expert 49 |      9 | CPU
DEBUG 12-24 11:28:13.443476.443476 lmp.py:530]   Expert 33 |     10 | CPU
DEBUG 12-24 11:28:13.443404.443404 lmp.py:530]   Expert 44 |     10 | GPU
DEBUG 12-24 11:28:13.443901.443901 lmp.py:530]   Expert 57 |     10 | GPU
DEBUG 12-24 11:28:13.443637.443637 lmp.py:530]   Expert 10 |     11 | GPU
DEBUG 12-24 11:28:13.443611.443611 lmp.py:530]   Expert 43 |     11 | GPU
DEBUG 12-24 11:28:13.443870.443870 lmp.py:530]   Expert 46 |     11 | GPU
DEBUG 12-24 11:28:13.443367.443367 lmp.py:530]   Expert 22 |     12 | GPU
DEBUG 12-24 11:28:13.443103.443103 lmp.py:530]   Expert 36 |     12 | GPU
DEBUG 12-24 11:28:13.443600.443600 lmp.py:530]   Expert 40 |     12 | GPU
DEBUG 12-24 11:28:13.443335.443335 lmp.py:530]   Expert 15 |     13 | GPU
DEBUG 12-24 11:28:13.443833.443833 lmp.py:530]   Expert 45 |     13 | GPU
DEBUG 12-24 11:28:13.443568.443568 lmp.py:530]   Expert 17 |     14 | GPU
DEBUG 12-24 11:28:13.443827.443827 lmp.py:530]   Expert 53 |     14 | GPU
DEBUG 12-24 11:28:13.443039.443039 lmp.py:530]   Expert  9 |     16 | GPU
DEBUG 12-24 11:28:13.443729.443729 lmp.py:530]   Expert 26 |     16 | GPU
DEBUG 12-24 11:28:13.443180.443180 lmp.py:530]   Expert 54 |     16 | GPU
DEBUG 12-24 11:28:13.443869.443869 lmp.py:530]   Expert 20 |     17 | GPU
DEBUG 12-24 11:28:13.443605.443605 lmp.py:530]   Expert  7 |     19 | GPU
DEBUG 12-24 11:28:13.443863.443863 lmp.py:530]   Expert 14 |     19 | GPU
DEBUG 12-24 11:28:13.443599.443599 lmp.py:530]   Expert 51 |     22 | GPU
DEBUG 12-24 11:28:13.443335.443335 lmp.py:530]   Expert 19 |     23 | GPU
DEBUG 12-24 11:28:13.443832.443832 lmp.py:530]   Expert 25 |     23 | GPU
DEBUG 12-24 11:28:13.443329.443329 lmp.py:530]   Expert 38 |     25 | GPU
DEBUG 12-24 11:28:13.443826.443826 lmp.py:530]   Expert 39 |     26 | GPU
DEBUG 12-24 11:28:13.443323.443323 lmp.py:530]   Expert 61 |     29 | GPU
DEBUG 12-24 11:28:13.443821.443821 lmp.py:530]   Expert 29 |     31 | GPU
DEBUG 12-24 11:28:13.443079.443079 lmp.py:530]   Expert 28 |     56 | GPU
DEBUG 12-24 11:28:13.443054.443054 lmp.py:530]   Expert  0 |   1921 | GPU
DEBUG 12-24 11:28:13.443504.443504 lmp.py:530]   Expert  5 |   1926 | GPU
DEBUG 12-24 11:28:13.443955.443955 lmp.py:530]   Expert  1 |   1927 | GPU
DEBUG 12-24 11:28:13.443883.443883 lmp.py:530]   Expert  4 |   1930 | GPU
DEBUG 12-24 11:28:13.443572.443572 lmp.py:530]   Expert  2 |   1954 | GPU
DEBUG 12-24 11:28:13.443308.443308 lmp.py:530]   Expert  3 |   1983 | GPU
DEBUG 12-24 11:28:13.443759.443759 lmp.py:531] 
DEBUG 12-24 11:28:13.443759.443759 lmp.py:531]   CPU total tokens: 166 (1.4%)
DEBUG 12-24 11:28:13.443448.443448 lmp.py:532]   GPU total tokens: 12122 (98.6%)
DEBUG 12-24 11:28:13.443429.443429 cuda_h.py:19] end experts_map_get cost 0.0014615058898925781 seconds
DEBUG 12-24 11:28:13.444595.444595 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.444114.444114 lmp.py:541] 
DEBUG 12-24 11:28:13.444114.444114 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:13.444050.444050 cuda_h.py:19] end cpu_experts_submit cost 0.00010371208190917969 seconds
DEBUG 12-24 11:28:13.444269.444269 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.444621.444621 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.444719.444719 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.444107.444107 cuda_h.py:19] end allocate_cuda_memory cost 0.0002856254577636719 seconds
DEBUG 12-24 11:28:13.444195.444195 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.444051.444051 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.444770.444770 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.444963.444963 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 850583bd-a30a-41d8-a09f-821c1e8bd7ab
DEBUG 12-24 11:28:13.445943.445943 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:13.461230.461230 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 850583bd-a30a-41d8-a09f-821c1e8bd7ab
DEBUG 12-24 11:28:13.461095.461095 mlpmodule.py:630] group tensors cost 0.016489744186401367 s
DEBUG 12-24 11:28:13.462371.462371 cuda_h.py:19] end load_into_gpu_async cost 0.01806020736694336 seconds
DEBUG 12-24 11:28:13.462628.462628 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.463279.463279 cuda_h.py:19] end restore_tensors2 cost 0.0004057884216308594 seconds
DEBUG 12-24 11:28:13.463830.463830 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.019263744354248047 seconds
DEBUG 12-24 11:28:13.468514.468514 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.024306058883666992 seconds
DEBUG 12-24 11:28:13.468034.468034 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 12-24 11:28:13.468917.468917 cuda_h.py:10] start start_load_qkvogn_s_weight_l_20
DEBUG 12-24 11:28:13.468019.468019 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 3.4809112548828125e-05 seconds
DEBUG 12-24 11:28:13.468358.468358 cuda_h.py:19] end start_load_qkvogn_s_weight_l_20 cost 7.724761962890625e-05 seconds
DEBUG 12-24 11:28:13.468683.468683 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.468734.468734 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.469629.469629 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.469925.469925 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.470126.470126 cuda_h.py:19] end allocate_cuda_memory cost 0.0009565353393554688 seconds
DEBUG 12-24 11:28:13.470194.470194 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.470906.470906 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.470063.470063 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.470595.470595 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 87f9ae81-6e71-4df1-aaa3-1e986eefb103
DEBUG 12-24 11:28:13.471536.471536 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.471237.471237 cuda_h.py:19] end gpu_sexperts cost 0.0028371810913085938 seconds
DEBUG 12-24 11:28:13.471322.471322 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.471723.471723 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 850583bd-a30a-41d8-a09f-821c1e8bd7ab
INFO 12-24 11:28:13.472731.472731 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 87f9ae81-6e71-4df1-aaa3-1e986eefb103
DEBUG 12-24 11:28:13.472506.472506 cuda_h.py:19] end load_into_gpu_async cost 0.0018780231475830078 seconds
DEBUG 12-24 11:28:13.472899.472899 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.472410.472410 cuda_h.py:19] end restore_tensors2 cost 0.00016164779663085938 seconds
DEBUG 12-24 11:28:13.473817.473817 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0037598609924316406 seconds
INFO 12-24 11:28:13.474008.474008 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 87f9ae81-6e71-4df1-aaa3-1e986eefb103
DEBUG 12-24 11:28:13.475748.475748 mlpmodule.py:668] pad cost 0.012182235717773438 s
DEBUG 12-24 11:28:13.475184.475184 mlpmodule.py:674] create cpu tensor cost 4.458427429199219e-05 s
DEBUG 12-24 11:28:13.475126.475126 mlpmodule.py:679] move to cpu cost 2.6941299438476562e-05 s
DEBUG 12-24 11:28:13.480761.480761 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:13.480625.480625 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.480653.480653 mlpmodule.py:700] group_w3 first element: -0.0576171875
WARNING 12-24 11:28:13.480069.480069 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.484015.484015 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.484842.484842 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.489041.489041 mlpmodule.py:723] group einsum cost 0.014219522476196289 s
DEBUG 12-24 11:28:13.489359.489359 mlpmodule.py:731] cpy2cputensor cost 0.000110626220703125 s
INFO 12-24 11:28:13.505127.505127 client.py:127] Model loaded
DEBUG 12-24 11:28:13.505713.505713 cuda_h.py:19] end wait_experts cost 0.033355712890625 seconds
DEBUG 12-24 11:28:13.505761.505761 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.505994.505994 lmp.py:585]   Computing 32 experts on GPU...
INFO 12-24 11:28:13.506333.506333 client.py:127] Model loaded
DEBUG 12-24 11:28:13.506811.506811 cuda_h.py:19] end sllm_worker_task cost 0.03710627555847168 seconds
DEBUG 12-24 11:28:13.506414.506414 mlpmodule.py:457] gpu group tensors cost 0.0009889602661132812 s
DEBUG 12-24 11:28:13.507604.507604 mlpmodule.py:588]  experts func einsum cost 0.062188148498535156 s
DEBUG 12-24 11:28:13.509308.509308 mlpmodule.py:490] gpu pad cost 0.0024547576904296875 s
DEBUG 12-24 11:28:13.509898.509898 mlpmodule.py:508] gpu group einsum cost 0.0007426738739013672 s
DEBUG 12-24 11:28:13.512419.512419 mlpmodule.py:537] gpu experts func einsum cost 0.007081270217895508 s
DEBUG 12-24 11:28:13.512773.512773 cuda_h.py:19] end gpu_experts cost 0.007250547409057617 seconds
DEBUG 12-24 11:28:13.512814.512814 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.512252.512252 cuda_h.py:19] end wait_cetm_experts cost 1.5020370483398438e-05 seconds
DEBUG 12-24 11:28:13.512571.512571 lmp.py:615] gpu end - einsum end = 17.3ms
DEBUG 12-24 11:28:13.512043.512043 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.512919.512919 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.9550323486328125e-05 seconds
DEBUG 12-24 11:28:13.512999.512999 cuda_h.py:19] end layer_moe_generate_19 cost 0.07103729248046875 seconds
DEBUG 12-24 11:28:13.513707.513707 lmp.py:445] -------------------------------- end layer 19 --------------------------------
DEBUG 12-24 11:28:13.513417.513417 lmp.py:418] -------------------------------- start layer 20 --------------------------------
DEBUG 12-24 11:28:13.513490.513490 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.513764.513764 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.519234.519234 cuda_h.py:19] end self_attn cost 0.006474971771240234 seconds
DEBUG 12-24 11:28:13.520455.520455 cuda_h.py:19] end iln_self_attn_paln cost 0.007038116455078125 seconds
DEBUG 12-24 11:28:13.520530.520530 cuda_h.py:10] start layer_moe_generate_20
DEBUG 12-24 11:28:13.520578.520578 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.523033.523033 cuda_h.py:19] end gate cost 0.0032625198364257812 seconds
DEBUG 12-24 11:28:13.523864.523864 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.524697.524697 lmp.py:519] 
DEBUG 12-24 11:28:13.524697.524697 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.524692.524692 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:13.524480.524480 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:13.524123.524123 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:13.524859.524859 lmp.py:523] 
DEBUG 12-24 11:28:13.524859.524859 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.524595.524595 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.524768.524768 lmp.py:530]   Expert  8 |      1 | CPU
DEBUG 12-24 11:28:13.524503.524503 lmp.py:530]   Expert 38 |      1 | CPU
DEBUG 12-24 11:28:13.524762.524762 lmp.py:530]   Expert 45 |      2 | CPU
DEBUG 12-24 11:28:13.524782.524782 lmp.py:530]   Expert 62 |      2 | CPU
DEBUG 12-24 11:28:13.524564.524564 lmp.py:530]   Expert 14 |      3 | CPU
DEBUG 12-24 11:28:13.524823.524823 lmp.py:530]   Expert 28 |      3 | CPU
DEBUG 12-24 11:28:13.524367.524367 lmp.py:530]   Expert 33 |      3 | CPU
DEBUG 12-24 11:28:13.524771.524771 lmp.py:530]   Expert 53 |      3 | CPU
DEBUG 12-24 11:28:13.524553.524553 lmp.py:530]   Expert 10 |      4 | CPU
DEBUG 12-24 11:28:13.524097.524097 lmp.py:530]   Expert 11 |      4 | CPU
DEBUG 12-24 11:28:13.524117.524117 lmp.py:530]   Expert 12 |      4 | CPU
DEBUG 12-24 11:28:13.524137.524137 lmp.py:530]   Expert 13 |      4 | CPU
DEBUG 12-24 11:28:13.524681.524681 lmp.py:530]   Expert 54 |      4 | CPU
DEBUG 12-24 11:28:13.524225.524225 lmp.py:530]   Expert 57 |      4 | CPU
DEBUG 12-24 11:28:13.524483.524483 lmp.py:530]   Expert 43 |      6 | CPU
DEBUG 12-24 11:28:13.524504.524504 lmp.py:530]   Expert 46 |      6 | CPU
DEBUG 12-24 11:28:13.524286.524286 lmp.py:530]   Expert 51 |      6 | CPU
DEBUG 12-24 11:28:13.524697.524697 lmp.py:530]   Expert 30 |      7 | CPU
DEBUG 12-24 11:28:13.524691.524691 lmp.py:530]   Expert 50 |      7 | CPU
DEBUG 12-24 11:28:13.524665.524665 lmp.py:530]   Expert 19 |      8 | CPU
DEBUG 12-24 11:28:13.524163.524163 lmp.py:530]   Expert 22 |      8 | CPU
DEBUG 12-24 11:28:13.524898.524898 lmp.py:530]   Expert 42 |      8 | CPU
DEBUG 12-24 11:28:13.524634.524634 lmp.py:530]   Expert 49 |      8 | CPU
DEBUG 12-24 11:28:13.524131.524131 lmp.py:530]   Expert 61 |      8 | CPU
DEBUG 12-24 11:28:13.524344.524344 lmp.py:530]   Expert  6 |      9 | CPU
DEBUG 12-24 11:28:13.524079.524079 lmp.py:530]   Expert 31 |      9 | CPU
DEBUG 12-24 11:28:13.524815.524815 lmp.py:530]   Expert 52 |      9 | CPU
DEBUG 12-24 11:28:13.524789.524789 lmp.py:530]   Expert 36 |     10 | CPU
DEBUG 12-24 11:28:13.524524.524524 lmp.py:530]   Expert 63 |     10 | CPU
DEBUG 12-24 11:28:13.524260.524260 lmp.py:530]   Expert  9 |     11 | CPU
DEBUG 12-24 11:28:13.524757.524757 lmp.py:530]   Expert 15 |     11 | CPU
DEBUG 12-24 11:28:13.524731.524731 lmp.py:530]   Expert 20 |     11 | CPU
DEBUG 12-24 11:28:13.524229.524229 lmp.py:530]   Expert 23 |     11 | GPU
DEBUG 12-24 11:28:13.524203.524203 lmp.py:530]   Expert 39 |     11 | GPU
DEBUG 12-24 11:28:13.524415.524415 lmp.py:530]   Expert 29 |     12 | GPU
DEBUG 12-24 11:28:13.524151.524151 lmp.py:530]   Expert 35 |     12 | GPU
DEBUG 12-24 11:28:13.524363.524363 lmp.py:530]   Expert  7 |     13 | GPU
DEBUG 12-24 11:28:13.524099.524099 lmp.py:530]   Expert 16 |     13 | GPU
DEBUG 12-24 11:28:13.524834.524834 lmp.py:530]   Expert 32 |     13 | GPU
DEBUG 12-24 11:28:13.524332.524332 lmp.py:530]   Expert 55 |     13 | GPU
DEBUG 12-24 11:28:13.524829.524829 lmp.py:530]   Expert 26 |     14 | GPU
DEBUG 12-24 11:28:13.524326.524326 lmp.py:530]   Expert 59 |     14 | GPU
DEBUG 12-24 11:28:13.524300.524300 lmp.py:530]   Expert 18 |     16 | GPU
DEBUG 12-24 11:28:13.524559.524559 lmp.py:530]   Expert 21 |     16 | GPU
DEBUG 12-24 11:28:13.524295.524295 lmp.py:530]   Expert 47 |     16 | GPU
DEBUG 12-24 11:28:13.524792.524792 lmp.py:530]   Expert 60 |     16 | GPU
DEBUG 12-24 11:28:13.524527.524527 lmp.py:530]   Expert 27 |     18 | GPU
DEBUG 12-24 11:28:13.524786.524786 lmp.py:530]   Expert 48 |     18 | GPU
DEBUG 12-24 11:28:13.524522.524522 lmp.py:530]   Expert 37 |     20 | GPU
DEBUG 12-24 11:28:13.524019.524019 lmp.py:530]   Expert 44 |     21 | GPU
DEBUG 12-24 11:28:13.524755.524755 lmp.py:530]   Expert 56 |     23 | GPU
DEBUG 12-24 11:28:13.524252.524252 lmp.py:530]   Expert 24 |     24 | GPU
DEBUG 12-24 11:28:13.524749.524749 lmp.py:530]   Expert 40 |     30 | GPU
DEBUG 12-24 11:28:13.524485.524485 lmp.py:530]   Expert 34 |     31 | GPU
DEBUG 12-24 11:28:13.524220.524220 lmp.py:530]   Expert 17 |     33 | GPU
DEBUG 12-24 11:28:13.525717.525717 lmp.py:530]   Expert 41 |     35 | GPU
DEBUG 12-24 11:28:13.525453.525453 lmp.py:530]   Expert 58 |     49 | GPU
DEBUG 12-24 11:28:13.525950.525950 lmp.py:530]   Expert 25 |     51 | GPU
DEBUG 12-24 11:28:13.525924.525924 lmp.py:530]   Expert  0 |   1920 | GPU
DEBUG 12-24 11:28:13.525660.525660 lmp.py:530]   Expert  3 |   1921 | GPU
DEBUG 12-24 11:28:13.525396.525396 lmp.py:530]   Expert  2 |   1923 | GPU
DEBUG 12-24 11:28:13.525893.525893 lmp.py:530]   Expert  1 |   1925 | GPU
DEBUG 12-24 11:28:13.525059.525059 lmp.py:530]   Expert  5 |   1927 | GPU
DEBUG 12-24 11:28:13.525271.525271 lmp.py:530]   Expert  4 |   1935 | GPU
DEBUG 12-24 11:28:13.525722.525722 lmp.py:531] 
DEBUG 12-24 11:28:13.525722.525722 lmp.py:531]   CPU total tokens: 194 (1.6%)
DEBUG 12-24 11:28:13.525412.525412 lmp.py:532]   GPU total tokens: 12094 (98.4%)
DEBUG 12-24 11:28:13.525108.525108 cuda_h.py:19] end experts_map_get cost 0.0015213489532470703 seconds
DEBUG 12-24 11:28:13.525512.525512 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.525535.525535 lmp.py:541] 
DEBUG 12-24 11:28:13.525535.525535 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:13.525385.525385 cuda_h.py:19] end cpu_experts_submit cost 0.0001308917999267578 seconds
DEBUG 12-24 11:28:13.525340.525340 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.525196.525196 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.525732.525732 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.526209.526209 cuda_h.py:19] end allocate_cuda_memory cost 0.0003528594970703125 seconds
DEBUG 12-24 11:28:13.526973.526973 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.526113.526113 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.526128.526128 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.526115.526115 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 310aa09e-f8cf-44cc-b828-f147518c6405
DEBUG 12-24 11:28:13.526203.526203 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.541113.541113 mlpmodule.py:630] group tensors cost 0.014789819717407227 s
INFO 12-24 11:28:13.542045.542045 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 310aa09e-f8cf-44cc-b828-f147518c6405
DEBUG 12-24 11:28:13.542745.542745 cuda_h.py:19] end load_into_gpu_async cost 0.016297578811645508 seconds
DEBUG 12-24 11:28:13.542130.542130 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.542491.542491 cuda_h.py:19] end restore_tensors2 cost 0.00043845176696777344 seconds
DEBUG 12-24 11:28:13.543553.543553 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.017545700073242188 seconds
DEBUG 12-24 11:28:13.547524.547524 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.021692514419555664 seconds
DEBUG 12-24 11:28:13.547520.547520 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 12-24 11:28:13.547819.547819 cuda_h.py:10] start start_load_qkvogn_s_weight_l_21
DEBUG 12-24 11:28:13.547875.547875 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 3.528594970703125e-05 seconds
DEBUG 12-24 11:28:13.547784.547784 cuda_h.py:19] end start_load_qkvogn_s_weight_l_21 cost 7.557868957519531e-05 seconds
DEBUG 12-24 11:28:13.547679.547679 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.547546.547546 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.547633.547633 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.548541.548541 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.549429.549429 cuda_h.py:19] end allocate_cuda_memory cost 0.0012650489807128906 seconds
DEBUG 12-24 11:28:13.549050.549050 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.549279.549279 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.549484.549484 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.550873.550873 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 38385c87-c588-43d8-b4e2-81d3a7d173ad
DEBUG 12-24 11:28:13.550549.550549 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.550862.550862 cuda_h.py:19] end gpu_sexperts cost 0.003063201904296875 seconds
DEBUG 12-24 11:28:13.550820.550820 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.550212.550212 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 310aa09e-f8cf-44cc-b828-f147518c6405
DEBUG 12-24 11:28:13.551155.551155 mlpmodule.py:668] pad cost 0.009528636932373047 s
DEBUG 12-24 11:28:13.551397.551397 mlpmodule.py:674] create cpu tensor cost 4.649162292480469e-05 s
DEBUG 12-24 11:28:13.551685.551685 mlpmodule.py:679] move to cpu cost 3.0517578125e-05 s
INFO 12-24 11:28:13.552565.552565 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 38385c87-c588-43d8-b4e2-81d3a7d173ad
DEBUG 12-24 11:28:13.552908.552908 cuda_h.py:19] end load_into_gpu_async cost 0.0025665760040283203 seconds
DEBUG 12-24 11:28:13.552963.552963 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.552231.552231 cuda_h.py:19] end restore_tensors2 cost 0.0001735687255859375 seconds
DEBUG 12-24 11:28:13.552413.552413 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004730224609375 seconds
INFO 12-24 11:28:13.555862.555862 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 38385c87-c588-43d8-b4e2-81d3a7d173ad
DEBUG 12-24 11:28:13.557370.557370 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:13.557615.557615 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.557015.557015 mlpmodule.py:700] group_w3 first element: -0.052734375
WARNING 12-24 11:28:13.557277.557277 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.561296.561296 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.561074.561074 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.567265.567265 mlpmodule.py:723] group einsum cost 0.015281438827514648 s
DEBUG 12-24 11:28:13.567082.567082 mlpmodule.py:731] cpy2cputensor cost 0.00012826919555664062 s
DEBUG 12-24 11:28:13.584641.584641 mlpmodule.py:588]  experts func einsum cost 0.05729818344116211 s
INFO 12-24 11:28:13.586347.586347 client.py:127] Model loaded
DEBUG 12-24 11:28:13.586960.586960 cuda_h.py:19] end wait_experts cost 0.03559279441833496 seconds
DEBUG 12-24 11:28:13.586862.586862 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.586095.586095 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:13.586540.586540 mlpmodule.py:457] gpu group tensors cost 0.0005540847778320312 s
INFO 12-24 11:28:13.587836.587836 client.py:127] Model loaded
DEBUG 12-24 11:28:13.587339.587339 cuda_h.py:19] end sllm_worker_task cost 0.03988838195800781 seconds
DEBUG 12-24 11:28:13.589671.589671 mlpmodule.py:490] gpu pad cost 0.002069234848022461 s
DEBUG 12-24 11:28:13.589835.589835 mlpmodule.py:508] gpu group einsum cost 0.0004475116729736328 s
DEBUG 12-24 11:28:13.592496.592496 mlpmodule.py:537] gpu experts func einsum cost 0.0059642791748046875 s
DEBUG 12-24 11:28:13.592745.592745 cuda_h.py:19] end gpu_experts cost 0.006171226501464844 seconds
DEBUG 12-24 11:28:13.592070.592070 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.592231.592231 cuda_h.py:19] end wait_cetm_experts cost 2.0265579223632812e-05 seconds
DEBUG 12-24 11:28:13.592695.592695 lmp.py:615] gpu end - einsum end = 20.2ms
DEBUG 12-24 11:28:13.592082.592082 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.592409.592409 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 3.24249267578125e-05 seconds
DEBUG 12-24 11:28:13.592873.592873 cuda_h.py:19] end layer_moe_generate_20 cost 0.07252168655395508 seconds
DEBUG 12-24 11:28:13.592044.592044 lmp.py:445] -------------------------------- end layer 20 --------------------------------
DEBUG 12-24 11:28:13.592953.592953 lmp.py:418] -------------------------------- start layer 21 --------------------------------
DEBUG 12-24 11:28:13.592695.592695 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.593281.593281 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.599986.599986 cuda_h.py:19] end self_attn cost 0.006570339202880859 seconds
DEBUG 12-24 11:28:13.600850.600850 cuda_h.py:19] end iln_self_attn_paln cost 0.007178068161010742 seconds
DEBUG 12-24 11:28:13.600117.600117 cuda_h.py:10] start layer_moe_generate_21
DEBUG 12-24 11:28:13.600641.600641 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.600174.600174 cuda_h.py:19] end gate cost 0.000640869140625 seconds
DEBUG 12-24 11:28:13.600334.600334 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.601523.601523 lmp.py:519] 
DEBUG 12-24 11:28:13.601523.601523 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.601563.601563 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:13.601783.601783 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:13.601141.601141 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:13.601545.601545 lmp.py:523] 
DEBUG 12-24 11:28:13.601545.601545 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.601950.601950 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.601838.601838 lmp.py:530]   Expert 54 |      1 | CPU
DEBUG 12-24 11:28:13.601766.601766 lmp.py:530]   Expert 24 |      2 | CPU
DEBUG 12-24 11:28:13.601217.601217 lmp.py:530]   Expert 26 |      2 | CPU
DEBUG 12-24 11:28:13.601906.601906 lmp.py:530]   Expert 48 |      2 | CPU
DEBUG 12-24 11:28:13.601834.601834 lmp.py:530]   Expert 53 |      2 | CPU
DEBUG 12-24 11:28:13.601000.601000 lmp.py:530]   Expert 56 |      2 | CPU
DEBUG 12-24 11:28:13.601404.601404 lmp.py:530]   Expert 13 |      3 | CPU
DEBUG 12-24 11:28:13.601571.601571 lmp.py:530]   Expert 17 |      3 | CPU
DEBUG 12-24 11:28:13.601783.601783 lmp.py:530]   Expert 44 |      3 | CPU
DEBUG 12-24 11:28:13.601996.601996 lmp.py:530]   Expert  9 |      4 | CPU
DEBUG 12-24 11:28:13.601731.601731 lmp.py:530]   Expert 25 |      4 | CPU
DEBUG 12-24 11:28:13.601467.601467 lmp.py:530]   Expert 40 |      4 | CPU
DEBUG 12-24 11:28:13.601202.601202 lmp.py:530]   Expert 12 |      5 | CPU
DEBUG 12-24 11:28:13.601415.601415 lmp.py:530]   Expert 19 |      5 | CPU
DEBUG 12-24 11:28:13.601912.601912 lmp.py:530]   Expert 57 |      5 | CPU
DEBUG 12-24 11:28:13.601648.601648 lmp.py:530]   Expert  8 |      6 | CPU
DEBUG 12-24 11:28:13.601145.601145 lmp.py:530]   Expert 23 |      6 | CPU
DEBUG 12-24 11:28:13.601881.601881 lmp.py:530]   Expert 28 |      6 | CPU
DEBUG 12-24 11:28:13.601378.601378 lmp.py:530]   Expert 30 |      6 | CPU
DEBUG 12-24 11:28:13.601113.601113 lmp.py:530]   Expert 11 |      7 | CPU
DEBUG 12-24 11:28:13.601087.601087 lmp.py:530]   Expert 14 |      7 | CPU
DEBUG 12-24 11:28:13.601300.601300 lmp.py:530]   Expert 43 |      7 | CPU
DEBUG 12-24 11:28:13.601512.601512 lmp.py:530]   Expert 50 |      7 | CPU
DEBUG 12-24 11:28:13.601010.601010 lmp.py:530]   Expert 51 |      7 | CPU
DEBUG 12-24 11:28:13.601222.601222 lmp.py:530]   Expert 15 |      8 | CPU
DEBUG 12-24 11:28:13.601911.601911 lmp.py:530]   Expert 38 |      8 | CPU
DEBUG 12-24 11:28:13.601362.601362 lmp.py:530]   Expert 47 |      8 | CPU
DEBUG 12-24 11:28:13.601813.601813 lmp.py:530]   Expert 49 |      8 | CPU
DEBUG 12-24 11:28:13.601026.601026 lmp.py:530]   Expert 59 |      8 | CPU
DEBUG 12-24 11:28:13.601192.601192 lmp.py:530]   Expert 60 |      8 | CPU
DEBUG 12-24 11:28:13.601166.601166 lmp.py:530]   Expert 32 |      9 | CPU
DEBUG 12-24 11:28:13.601617.601617 lmp.py:530]   Expert 52 |      9 | CPU
DEBUG 12-24 11:28:13.601829.601829 lmp.py:530]   Expert 62 |      9 | GPU
DEBUG 12-24 11:28:13.601565.601565 lmp.py:530]   Expert 35 |     10 | GPU
DEBUG 12-24 11:28:13.601539.601539 lmp.py:530]   Expert 31 |     11 | GPU
DEBUG 12-24 11:28:13.601513.601513 lmp.py:530]   Expert  6 |     12 | GPU
DEBUG 12-24 11:28:13.601010.601010 lmp.py:530]   Expert 34 |     12 | GPU
DEBUG 12-24 11:28:13.601984.601984 lmp.py:530]   Expert 20 |     13 | GPU
DEBUG 12-24 11:28:13.602481.602481 lmp.py:530]   Expert 33 |     13 | GPU
DEBUG 12-24 11:28:13.602217.602217 lmp.py:530]   Expert 29 |     14 | GPU
DEBUG 12-24 11:28:13.602952.602952 lmp.py:530]   Expert  7 |     15 | GPU
DEBUG 12-24 11:28:13.602927.602927 lmp.py:530]   Expert 42 |     15 | GPU
DEBUG 12-24 11:28:13.602424.602424 lmp.py:530]   Expert 55 |     15 | GPU
DEBUG 12-24 11:28:13.602921.602921 lmp.py:530]   Expert 58 |     16 | GPU
DEBUG 12-24 11:28:13.602657.602657 lmp.py:530]   Expert 61 |     16 | GPU
DEBUG 12-24 11:28:13.602869.602869 lmp.py:530]   Expert 22 |     17 | GPU
DEBUG 12-24 11:28:13.602320.602320 lmp.py:530]   Expert 41 |     17 | GPU
DEBUG 12-24 11:28:13.602294.602294 lmp.py:530]   Expert 37 |     18 | GPU
DEBUG 12-24 11:28:13.602983.602983 lmp.py:530]   Expert 46 |     18 | GPU
DEBUG 12-24 11:28:13.602196.602196 lmp.py:530]   Expert 10 |     19 | GPU
DEBUG 12-24 11:28:13.602170.602170 lmp.py:530]   Expert 16 |     20 | GPU
DEBUG 12-24 11:28:13.602144.602144 lmp.py:530]   Expert 27 |     20 | GPU
DEBUG 12-24 11:28:13.602879.602879 lmp.py:530]   Expert 45 |     23 | GPU
DEBUG 12-24 11:28:13.602853.602853 lmp.py:530]   Expert 63 |     23 | GPU
DEBUG 12-24 11:28:13.602589.602589 lmp.py:530]   Expert 39 |     27 | GPU
DEBUG 12-24 11:28:13.602848.602848 lmp.py:530]   Expert 18 |     29 | GPU
DEBUG 12-24 11:28:13.602822.602822 lmp.py:530]   Expert 36 |     34 | GPU
DEBUG 12-24 11:28:13.602081.602081 lmp.py:530]   Expert 21 |     45 | GPU
DEBUG 12-24 11:28:13.602055.602055 lmp.py:530]   Expert  1 |   1924 | GPU
DEBUG 12-24 11:28:13.602506.602506 lmp.py:530]   Expert  3 |   1928 | GPU
DEBUG 12-24 11:28:13.602195.602195 lmp.py:530]   Expert  5 |   1928 | GPU
DEBUG 12-24 11:28:13.602884.602884 lmp.py:530]   Expert  2 |   1935 | GPU
DEBUG 12-24 11:28:13.602812.602812 lmp.py:530]   Expert  4 |   1945 | GPU
DEBUG 12-24 11:28:13.602786.602786 lmp.py:530]   Expert  0 |   1975 | GPU
DEBUG 12-24 11:28:13.602237.602237 lmp.py:531] 
DEBUG 12-24 11:28:13.602237.602237 lmp.py:531]   CPU total tokens: 172 (1.4%)
DEBUG 12-24 11:28:13.602926.602926 lmp.py:532]   GPU total tokens: 12116 (98.6%)
DEBUG 12-24 11:28:13.602669.602669 cuda_h.py:19] end experts_map_get cost 0.0014469623565673828 seconds
DEBUG 12-24 11:28:13.602835.602835 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.602731.602731 lmp.py:541] 
DEBUG 12-24 11:28:13.602731.602731 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:13.602759.602759 cuda_h.py:19] end cpu_experts_submit cost 9.751319885253906e-05 seconds
DEBUG 12-24 11:28:13.602932.602932 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.602060.602060 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.602410.602410 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.603059.603059 cuda_h.py:19] end allocate_cuda_memory cost 0.0005843639373779297 seconds
DEBUG 12-24 11:28:13.603820.603820 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.603530.603530 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.603207.603207 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.603241.603241 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 44a37230-729c-4461-b7b9-348067751a14
DEBUG 12-24 11:28:13.603083.603083 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:13.617403.617403 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 44a37230-729c-4461-b7b9-348067751a14
DEBUG 12-24 11:28:13.617889.617889 mlpmodule.py:630] group tensors cost 0.012993574142456055 s
DEBUG 12-24 11:28:13.617488.617488 cuda_h.py:19] end load_into_gpu_async cost 0.014188289642333984 seconds
DEBUG 12-24 11:28:13.617703.617703 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.618999.618999 cuda_h.py:19] end restore_tensors2 cost 0.00029540061950683594 seconds
DEBUG 12-24 11:28:13.618775.618775 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.015600919723510742 seconds
DEBUG 12-24 11:28:13.622502.622502 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.019939899444580078 seconds
DEBUG 12-24 11:28:13.622067.622067 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 12-24 11:28:13.622459.622459 cuda_h.py:10] start start_load_qkvogn_s_weight_l_22
DEBUG 12-24 11:28:13.622547.622547 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 2.9802322387695312e-05 seconds
DEBUG 12-24 11:28:13.622403.622403 cuda_h.py:19] end start_load_qkvogn_s_weight_l_22 cost 6.699562072753906e-05 seconds
DEBUG 12-24 11:28:13.622390.622390 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.622382.622382 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.623057.623057 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.623097.623097 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.624551.624551 cuda_h.py:19] end allocate_cuda_memory cost 0.0009975433349609375 seconds
DEBUG 12-24 11:28:13.624757.624757 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.624007.624007 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.624693.624693 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.625869.625869 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 5fcb730f-1b06-4428-ba3f-97aa36d068a8
DEBUG 12-24 11:28:13.625186.625186 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.625887.625887 cuda_h.py:19] end gpu_sexperts cost 0.0026998519897460938 seconds
DEBUG 12-24 11:28:13.625904.625904 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.625608.625608 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 44a37230-729c-4461-b7b9-348067751a14
DEBUG 12-24 11:28:13.626063.626063 mlpmodule.py:668] pad cost 0.008428335189819336 s
DEBUG 12-24 11:28:13.626729.626729 mlpmodule.py:674] create cpu tensor cost 4.4345855712890625e-05 s
INFO 12-24 11:28:13.626602.626602 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 5fcb730f-1b06-4428-ba3f-97aa36d068a8
DEBUG 12-24 11:28:13.626116.626116 mlpmodule.py:679] move to cpu cost 0.00023651123046875 s
DEBUG 12-24 11:28:13.626054.626054 cuda_h.py:19] end load_into_gpu_async cost 0.0019927024841308594 seconds
DEBUG 12-24 11:28:13.626556.626556 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.627348.627348 cuda_h.py:19] end restore_tensors2 cost 0.0001919269561767578 seconds
DEBUG 12-24 11:28:13.627286.627286 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004115581512451172 seconds
INFO 12-24 11:28:13.629000.629000 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 5fcb730f-1b06-4428-ba3f-97aa36d068a8
DEBUG 12-24 11:28:13.631213.631213 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:13.632079.632079 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.632664.632664 mlpmodule.py:700] group_w3 first element: 0.0174560546875
WARNING 12-24 11:28:13.632661.632661 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.636839.636839 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.637152.637152 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.641056.641056 mlpmodule.py:723] group einsum cost 0.014999866485595703 s
DEBUG 12-24 11:28:13.642920.642920 mlpmodule.py:731] cpy2cputensor cost 0.00012183189392089844 s
DEBUG 12-24 11:28:13.658633.658633 mlpmodule.py:588]  experts func einsum cost 0.054808855056762695 s
INFO 12-24 11:28:13.664391.664391 client.py:127] Model loaded
DEBUG 12-24 11:28:13.665033.665033 cuda_h.py:19] end wait_experts cost 0.03937840461730957 seconds
DEBUG 12-24 11:28:13.665016.665016 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.665231.665231 lmp.py:585]   Computing 32 experts on GPU...
INFO 12-24 11:28:13.666649.666649 client.py:127] Model loaded
DEBUG 12-24 11:28:13.666747.666747 cuda_h.py:19] end sllm_worker_task cost 0.043770551681518555 seconds
DEBUG 12-24 11:28:13.667058.667058 mlpmodule.py:457] gpu group tensors cost 0.0017573833465576172 s
DEBUG 12-24 11:28:13.671555.671555 mlpmodule.py:490] gpu pad cost 0.004223823547363281 s
DEBUG 12-24 11:28:13.672261.672261 mlpmodule.py:508] gpu group einsum cost 0.001142263412475586 s
DEBUG 12-24 11:28:13.676107.676107 mlpmodule.py:537] gpu experts func einsum cost 0.011145830154418945 s
DEBUG 12-24 11:28:13.676501.676501 cuda_h.py:19] end gpu_experts cost 0.011450052261352539 seconds
DEBUG 12-24 11:28:13.676463.676463 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.676352.676352 cuda_h.py:19] end wait_cetm_experts cost 2.0265579223632812e-05 seconds
DEBUG 12-24 11:28:13.676969.676969 lmp.py:615] gpu end - einsum end = 29.8ms
DEBUG 12-24 11:28:13.676423.676423 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.677650.677650 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.193450927734375e-05 seconds
DEBUG 12-24 11:28:13.677075.677075 cuda_h.py:19] end layer_moe_generate_21 cost 0.07683563232421875 seconds
DEBUG 12-24 11:28:13.677015.677015 lmp.py:445] -------------------------------- end layer 21 --------------------------------
DEBUG 12-24 11:28:13.677262.677262 lmp.py:418] -------------------------------- start layer 22 --------------------------------
DEBUG 12-24 11:28:13.677157.677157 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.677102.677102 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.682530.682530 cuda_h.py:19] end self_attn cost 0.005202531814575195 seconds
DEBUG 12-24 11:28:13.683705.683705 cuda_h.py:19] end iln_self_attn_paln cost 0.0058476924896240234 seconds
DEBUG 12-24 11:28:13.683971.683971 cuda_h.py:10] start layer_moe_generate_22
DEBUG 12-24 11:28:13.683589.683589 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.683332.683332 cuda_h.py:19] end gate cost 0.0006208419799804688 seconds
DEBUG 12-24 11:28:13.684923.684923 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.684264.684264 lmp.py:519] 
DEBUG 12-24 11:28:13.684264.684264 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.684782.684782 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:13.684908.684908 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:13.684803.684803 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:13.684208.684208 lmp.py:523] 
DEBUG 12-24 11:28:13.684208.684208 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.684612.684612 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.684977.684977 lmp.py:530]   Expert 17 |      1 | CPU
DEBUG 12-24 11:28:13.684667.684667 lmp.py:530]   Expert 26 |      1 | CPU
DEBUG 12-24 11:28:13.684356.684356 lmp.py:530]   Expert 41 |      1 | CPU
DEBUG 12-24 11:28:13.684807.684807 lmp.py:530]   Expert 11 |      2 | CPU
DEBUG 12-24 11:28:13.684019.684019 lmp.py:530]   Expert  7 |      3 | CPU
DEBUG 12-24 11:28:13.684470.684470 lmp.py:530]   Expert 15 |      3 | CPU
DEBUG 12-24 11:28:13.684921.684921 lmp.py:530]   Expert 36 |      3 | CPU
DEBUG 12-24 11:28:13.684610.684610 lmp.py:530]   Expert 44 |      3 | CPU
DEBUG 12-24 11:28:13.684253.684253 lmp.py:530]   Expert 45 |      3 | CPU
DEBUG 12-24 11:28:13.684804.684804 lmp.py:530]   Expert 54 |      3 | CPU
DEBUG 12-24 11:28:13.684208.684208 lmp.py:530]   Expert  6 |      4 | CPU
DEBUG 12-24 11:28:13.684613.684613 lmp.py:530]   Expert 10 |      4 | CPU
DEBUG 12-24 11:28:13.684686.684686 lmp.py:530]   Expert 32 |      4 | CPU
DEBUG 12-24 11:28:13.684429.684429 lmp.py:530]   Expert 46 |      4 | CPU
DEBUG 12-24 11:28:13.684641.684641 lmp.py:530]   Expert 52 |      4 | CPU
DEBUG 12-24 11:28:13.684377.684377 lmp.py:530]   Expert 55 |      4 | CPU
DEBUG 12-24 11:28:13.684351.684351 lmp.py:530]   Expert 63 |      4 | CPU
DEBUG 12-24 11:28:13.684848.684848 lmp.py:530]   Expert  8 |      5 | CPU
DEBUG 12-24 11:28:13.684822.684822 lmp.py:530]   Expert 13 |      5 | CPU
DEBUG 12-24 11:28:13.684034.684034 lmp.py:530]   Expert 24 |      5 | CPU
DEBUG 12-24 11:28:13.684770.684770 lmp.py:530]   Expert 28 |      5 | CPU
DEBUG 12-24 11:28:13.684744.684744 lmp.py:530]   Expert 19 |      6 | CPU
DEBUG 12-24 11:28:13.684241.684241 lmp.py:530]   Expert 27 |      6 | CPU
DEBUG 12-24 11:28:13.684692.684692 lmp.py:530]   Expert 47 |      6 | CPU
DEBUG 12-24 11:28:13.684382.684382 lmp.py:530]   Expert 62 |      6 | CPU
DEBUG 12-24 11:28:13.684309.684309 lmp.py:530]   Expert 48 |      7 | CPU
DEBUG 12-24 11:28:13.684475.684475 lmp.py:530]   Expert 21 |      8 | CPU
DEBUG 12-24 11:28:13.684211.684211 lmp.py:530]   Expert 42 |      8 | CPU
DEBUG 12-24 11:28:13.684423.684423 lmp.py:530]   Expert 56 |      8 | CPU
DEBUG 12-24 11:28:13.684159.684159 lmp.py:530]   Expert 60 |      8 | CPU
DEBUG 12-24 11:28:13.684895.684895 lmp.py:530]   Expert 61 |      8 | CPU
DEBUG 12-24 11:28:13.684630.684630 lmp.py:530]   Expert 22 |      9 | CPU
DEBUG 12-24 11:28:13.684604.684604 lmp.py:530]   Expert 34 |      9 | GPU
DEBUG 12-24 11:28:13.685578.685578 lmp.py:530]   Expert 39 |      9 | GPU
DEBUG 12-24 11:28:13.685076.685076 lmp.py:530]   Expert 50 |      9 | GPU
DEBUG 12-24 11:28:13.685811.685811 lmp.py:530]   Expert  9 |     10 | GPU
DEBUG 12-24 11:28:13.685309.685309 lmp.py:530]   Expert 18 |     10 | GPU
DEBUG 12-24 11:28:13.685283.685283 lmp.py:530]   Expert 12 |     11 | GPU
DEBUG 12-24 11:28:13.685780.685780 lmp.py:530]   Expert 31 |     11 | GPU
DEBUG 12-24 11:28:13.685277.685277 lmp.py:530]   Expert 30 |     12 | GPU
DEBUG 12-24 11:28:13.685966.685966 lmp.py:530]   Expert 53 |     13 | GPU
DEBUG 12-24 11:28:13.685755.685755 lmp.py:530]   Expert 33 |     14 | GPU
DEBUG 12-24 11:28:13.685729.685729 lmp.py:530]   Expert 35 |     14 | GPU
DEBUG 12-24 11:28:13.685465.685465 lmp.py:530]   Expert 58 |     14 | GPU
DEBUG 12-24 11:28:13.685962.685962 lmp.py:530]   Expert 59 |     14 | GPU
DEBUG 12-24 11:28:13.685459.685459 lmp.py:530]   Expert 37 |     17 | GPU
DEBUG 12-24 11:28:13.685195.685195 lmp.py:530]   Expert 38 |     17 | GPU
DEBUG 12-24 11:28:13.685930.685930 lmp.py:530]   Expert 49 |     18 | GPU
DEBUG 12-24 11:28:13.685666.685666 lmp.py:530]   Expert 29 |     19 | GPU
DEBUG 12-24 11:28:13.685163.685163 lmp.py:530]   Expert 20 |     20 | GPU
DEBUG 12-24 11:28:13.685376.685376 lmp.py:530]   Expert 51 |     20 | GPU
DEBUG 12-24 11:28:13.685873.685873 lmp.py:530]   Expert 40 |     21 | GPU
DEBUG 12-24 11:28:13.685608.685608 lmp.py:530]   Expert 23 |     26 | GPU
DEBUG 12-24 11:28:13.685106.685106 lmp.py:530]   Expert 16 |     28 | GPU
DEBUG 12-24 11:28:13.685080.685080 lmp.py:530]   Expert 43 |     32 | GPU
DEBUG 12-24 11:28:13.685054.685054 lmp.py:530]   Expert 14 |     53 | GPU
DEBUG 12-24 11:28:13.685127.685127 lmp.py:530]   Expert 25 |     55 | GPU
DEBUG 12-24 11:28:13.685532.685532 lmp.py:530]   Expert 57 |     61 | GPU
DEBUG 12-24 11:28:13.685698.685698 lmp.py:530]   Expert  0 |   1922 | GPU
DEBUG 12-24 11:28:13.685295.685295 lmp.py:530]   Expert  3 |   1928 | GPU
DEBUG 12-24 11:28:13.685745.685745 lmp.py:530]   Expert  1 |   1929 | GPU
DEBUG 12-24 11:28:13.685481.685481 lmp.py:530]   Expert  2 |   1935 | GPU
DEBUG 12-24 11:28:13.685217.685217 lmp.py:530]   Expert  4 |   1942 | GPU
DEBUG 12-24 11:28:13.685191.685191 lmp.py:530]   Expert  5 |   1944 | GPU
DEBUG 12-24 11:28:13.685403.685403 lmp.py:531] 
DEBUG 12-24 11:28:13.685403.685403 lmp.py:531]   CPU total tokens: 151 (1.2%)
DEBUG 12-24 11:28:13.685331.685331 lmp.py:532]   GPU total tokens: 12137 (98.8%)
DEBUG 12-24 11:28:13.685550.685550 cuda_h.py:19] end experts_map_get cost 0.0014870166778564453 seconds
DEBUG 12-24 11:28:13.685478.685478 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.685202.685202 lmp.py:541] 
DEBUG 12-24 11:28:13.685202.685202 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:13.685516.685516 cuda_h.py:19] end cpu_experts_submit cost 0.00011181831359863281 seconds
DEBUG 12-24 11:28:13.685563.685563 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.685869.685869 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.685059.685059 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.686726.686726 cuda_h.py:19] end allocate_cuda_memory cost 0.0002810955047607422 seconds
DEBUG 12-24 11:28:13.686337.686337 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.686524.686524 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.686916.686916 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.686619.686619 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 03ea6a5a-f582-464c-af9b-e2f81446a238
DEBUG 12-24 11:28:13.686890.686890 client.py:106] call stub.LoadModelAsync
INFO 12-24 11:28:13.704414.704414 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 03ea6a5a-f582-464c-af9b-e2f81446a238
DEBUG 12-24 11:28:13.704419.704419 mlpmodule.py:630] group tensors cost 0.017897367477416992 s
DEBUG 12-24 11:28:13.705132.705132 cuda_h.py:19] end load_into_gpu_async cost 0.01914381980895996 seconds
DEBUG 12-24 11:28:13.705629.705629 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.706002.706002 cuda_h.py:19] end restore_tensors2 cost 0.0007815361022949219 seconds
DEBUG 12-24 11:28:13.706458.706458 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.020901918411254883 seconds
DEBUG 12-24 11:28:13.715740.715740 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.02957463264465332 seconds
DEBUG 12-24 11:28:13.715222.715222 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 12-24 11:28:13.715173.715173 cuda_h.py:10] start start_load_qkvogn_s_weight_l_23
DEBUG 12-24 11:28:13.715496.715496 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 5.4836273193359375e-05 seconds
DEBUG 12-24 11:28:13.715817.715817 cuda_h.py:19] end start_load_qkvogn_s_weight_l_23 cost 0.00015687942504882812 seconds
DEBUG 12-24 11:28:13.715311.715311 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.716219.716219 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.716014.716014 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.716505.716505 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.717996.717996 cuda_h.py:19] end allocate_cuda_memory cost 0.0008139610290527344 seconds
DEBUG 12-24 11:28:13.717493.717493 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.717483.717483 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.718208.718208 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.718075.718075 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 81605d58-e676-4c59-bbb4-e33e56d7ae2b
DEBUG 12-24 11:28:13.718016.718016 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.718906.718906 cuda_h.py:19] end gpu_sexperts cost 0.002835512161254883 seconds
DEBUG 12-24 11:28:13.719096.719096 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.719153.719153 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 03ea6a5a-f582-464c-af9b-e2f81446a238
INFO 12-24 11:28:13.720341.720341 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 81605d58-e676-4c59-bbb4-e33e56d7ae2b
DEBUG 12-24 11:28:13.720472.720472 cuda_h.py:19] end load_into_gpu_async cost 0.002656698226928711 seconds
DEBUG 12-24 11:28:13.720018.720018 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.720524.720524 cuda_h.py:19] end restore_tensors2 cost 0.0001494884490966797 seconds
DEBUG 12-24 11:28:13.721269.721269 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0045223236083984375 seconds
DEBUG 12-24 11:28:13.721024.721024 mlpmodule.py:668] pad cost 0.015900850296020508 s
DEBUG 12-24 11:28:13.721341.721341 mlpmodule.py:674] create cpu tensor cost 5.626678466796875e-05 s
DEBUG 12-24 11:28:13.721275.721275 mlpmodule.py:679] move to cpu cost 0.00014400482177734375 s
INFO 12-24 11:28:13.723084.723084 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 81605d58-e676-4c59-bbb4-e33e56d7ae2b
DEBUG 12-24 11:28:13.726832.726832 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:13.726129.726129 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.727675.727675 mlpmodule.py:700] group_w3 first element: -0.025390625
WARNING 12-24 11:28:13.727023.727023 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.731568.731568 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.731372.731372 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.736743.736743 mlpmodule.py:723] group einsum cost 0.014093399047851562 s
DEBUG 12-24 11:28:13.736057.736057 mlpmodule.py:731] cpy2cputensor cost 0.0001220703125 s
INFO 12-24 11:28:13.742086.742086 client.py:127] Model loaded
DEBUG 12-24 11:28:13.742639.742639 cuda_h.py:19] end wait_experts cost 0.023824453353881836 seconds
DEBUG 12-24 11:28:13.743302.743302 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.743535.743535 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:13.743381.743381 mlpmodule.py:457] gpu group tensors cost 0.0006403923034667969 s
DEBUG 12-24 11:28:13.745286.745286 mlpmodule.py:490] gpu pad cost 0.001661539077758789 s
DEBUG 12-24 11:28:13.746727.746727 mlpmodule.py:508] gpu group einsum cost 0.0006353855133056641 s
INFO 12-24 11:28:13.747999.747999 client.py:127] Model loaded
DEBUG 12-24 11:28:13.747953.747953 cuda_h.py:19] end sllm_worker_task cost 0.03143024444580078 seconds
DEBUG 12-24 11:28:13.750694.750694 mlpmodule.py:537] gpu experts func einsum cost 0.0070035457611083984 s
DEBUG 12-24 11:28:13.750407.750407 cuda_h.py:19] end gpu_experts cost 0.0072443485260009766 seconds
DEBUG 12-24 11:28:13.750024.750024 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.750668.750668 cuda_h.py:19] end wait_cetm_experts cost 2.09808349609375e-05 seconds
DEBUG 12-24 11:28:13.750802.750802 lmp.py:615] gpu end - einsum end = 9.3ms
DEBUG 12-24 11:28:13.750811.750811 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.750223.750223 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.09808349609375e-05 seconds
DEBUG 12-24 11:28:13.750026.750026 cuda_h.py:19] end layer_moe_generate_22 cost 0.06725263595581055 seconds
DEBUG 12-24 11:28:13.750523.750523 lmp.py:445] -------------------------------- end layer 22 --------------------------------
DEBUG 12-24 11:28:13.750677.750677 lmp.py:418] -------------------------------- start layer 23 --------------------------------
DEBUG 12-24 11:28:13.750996.750996 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.751423.751423 cuda_h.py:10] start self_attn
DEBUG 12-24 11:28:13.753603.753603 mlpmodule.py:588]  experts func einsum cost 0.06714510917663574 s
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.756288.756288 cuda_h.py:19] end self_attn cost 0.0051610469818115234 seconds
DEBUG 12-24 11:28:13.756741.756741 cuda_h.py:19] end iln_self_attn_paln cost 0.005819082260131836 seconds
DEBUG 12-24 11:28:13.756723.756723 cuda_h.py:10] start layer_moe_generate_23
DEBUG 12-24 11:28:13.756725.756725 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.757011.757011 cuda_h.py:19] end gate cost 0.0005981922149658203 seconds
DEBUG 12-24 11:28:13.757125.757125 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.757690.757690 lmp.py:519] 
DEBUG 12-24 11:28:13.757690.757690 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.757585.757585 lmp.py:520]   Total experts: 61
DEBUG 12-24 11:28:13.757089.757089 lmp.py:521]   CPU experts: 30 (49%)
DEBUG 12-24 11:28:13.757209.757209 lmp.py:522]   GPU experts: 31 (51%)
DEBUG 12-24 11:28:13.757422.757422 lmp.py:523] 
DEBUG 12-24 11:28:13.757422.757422 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.757873.757873 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.757522.757522 lmp.py:530]   Expert  9 |      2 | CPU
DEBUG 12-24 11:28:13.757212.757212 lmp.py:530]   Expert 16 |      2 | CPU
DEBUG 12-24 11:28:13.757947.757947 lmp.py:530]   Expert 44 |      2 | CPU
DEBUG 12-24 11:28:13.757921.757921 lmp.py:530]   Expert 45 |      2 | CPU
DEBUG 12-24 11:28:13.757657.757657 lmp.py:530]   Expert 36 |      3 | CPU
DEBUG 12-24 11:28:13.757393.757393 lmp.py:530]   Expert 22 |      4 | CPU
DEBUG 12-24 11:28:13.757367.757367 lmp.py:530]   Expert  7 |      5 | CPU
DEBUG 12-24 11:28:13.758678.758678 lmp.py:530]   Expert 25 |      5 | CPU
DEBUG 12-24 11:28:13.758414.758414 lmp.py:530]   Expert 28 |      5 | CPU
DEBUG 12-24 11:28:13.758150.758150 lmp.py:530]   Expert 47 |      5 | CPU
DEBUG 12-24 11:28:13.758885.758885 lmp.py:530]   Expert 59 |      5 | CPU
DEBUG 12-24 11:28:13.758383.758383 lmp.py:530]   Expert  6 |      6 | CPU
DEBUG 12-24 11:28:13.758357.758357 lmp.py:530]   Expert 27 |      6 | CPU
DEBUG 12-24 11:28:13.758615.758615 lmp.py:530]   Expert 58 |      6 | CPU
DEBUG 12-24 11:28:13.758351.758351 lmp.py:530]   Expert 34 |      7 | CPU
DEBUG 12-24 11:28:13.758610.758610 lmp.py:530]   Expert 38 |      7 | CPU
DEBUG 12-24 11:28:13.758869.758869 lmp.py:530]   Expert 40 |      7 | CPU
DEBUG 12-24 11:28:13.758366.758366 lmp.py:530]   Expert 63 |      7 | CPU
DEBUG 12-24 11:28:13.758386.758386 lmp.py:530]   Expert 13 |      8 | CPU
DEBUG 12-24 11:28:13.758883.758883 lmp.py:530]   Expert 21 |      8 | CPU
DEBUG 12-24 11:28:13.758142.758142 lmp.py:530]   Expert 30 |      8 | CPU
DEBUG 12-24 11:28:13.758639.758639 lmp.py:530]   Expert 32 |      8 | CPU
DEBUG 12-24 11:28:13.758660.758660 lmp.py:530]   Expert 19 |      9 | CPU
DEBUG 12-24 11:28:13.758680.758680 lmp.py:530]   Expert 51 |      9 | CPU
DEBUG 12-24 11:28:13.758177.758177 lmp.py:530]   Expert 52 |     10 | CPU
DEBUG 12-24 11:28:13.758675.758675 lmp.py:530]   Expert 12 |     11 | CPU
DEBUG 12-24 11:28:13.758933.758933 lmp.py:530]   Expert 26 |     11 | CPU
DEBUG 12-24 11:28:13.758192.758192 lmp.py:530]   Expert 35 |     11 | CPU
DEBUG 12-24 11:28:13.758451.758451 lmp.py:530]   Expert 43 |     11 | CPU
DEBUG 12-24 11:28:13.758948.758948 lmp.py:530]   Expert 11 |     12 | CPU
DEBUG 12-24 11:28:13.758207.758207 lmp.py:530]   Expert 20 |     12 | GPU
DEBUG 12-24 11:28:13.758943.758943 lmp.py:530]   Expert 42 |     12 | GPU
DEBUG 12-24 11:28:13.758201.758201 lmp.py:530]   Expert 62 |     12 | GPU
DEBUG 12-24 11:28:13.758699.758699 lmp.py:530]   Expert 15 |     13 | GPU
DEBUG 12-24 11:28:13.758719.758719 lmp.py:530]   Expert 37 |     13 | GPU
DEBUG 12-24 11:28:13.758978.758978 lmp.py:530]   Expert 39 |     14 | GPU
DEBUG 12-24 11:28:13.758236.758236 lmp.py:530]   Expert 46 |     14 | GPU
DEBUG 12-24 11:28:13.758495.758495 lmp.py:530]   Expert 50 |     14 | GPU
DEBUG 12-24 11:28:13.758754.758754 lmp.py:530]   Expert 53 |     14 | GPU
DEBUG 12-24 11:28:13.758774.758774 lmp.py:530]   Expert 23 |     16 | GPU
DEBUG 12-24 11:28:13.758033.758033 lmp.py:530]   Expert 33 |     16 | GPU
DEBUG 12-24 11:28:13.758530.758530 lmp.py:530]   Expert  8 |     17 | GPU
DEBUG 12-24 11:28:13.758551.758551 lmp.py:530]   Expert 31 |     17 | GPU
DEBUG 12-24 11:28:13.758048.758048 lmp.py:530]   Expert 41 |     17 | GPU
DEBUG 12-24 11:28:13.758068.758068 lmp.py:530]   Expert 54 |     17 | GPU
DEBUG 12-24 11:28:13.758566.758566 lmp.py:530]   Expert 10 |     18 | GPU
DEBUG 12-24 11:28:13.758824.758824 lmp.py:530]   Expert 61 |     18 | GPU
DEBUG 12-24 11:28:13.758606.758606 lmp.py:530]   Expert 24 |     21 | GPU
DEBUG 12-24 11:28:13.758627.758627 lmp.py:530]   Expert 29 |     21 | GPU
DEBUG 12-24 11:28:13.758124.758124 lmp.py:530]   Expert 18 |     23 | GPU
DEBUG 12-24 11:28:13.758383.758383 lmp.py:530]   Expert 14 |     25 | GPU
DEBUG 12-24 11:28:13.758880.758880 lmp.py:530]   Expert 57 |     26 | GPU
DEBUG 12-24 11:28:13.758900.758900 lmp.py:530]   Expert 48 |     35 | GPU
DEBUG 12-24 11:28:13.758921.758921 lmp.py:530]   Expert 60 |     52 | GPU
DEBUG 12-24 11:28:13.758179.758179 lmp.py:530]   Expert 56 |     59 | GPU
DEBUG 12-24 11:28:13.758438.758438 lmp.py:530]   Expert  5 |   1922 | GPU
DEBUG 12-24 11:28:13.758935.758935 lmp.py:530]   Expert  0 |   1924 | GPU
DEBUG 12-24 11:28:13.758194.758194 lmp.py:530]   Expert  1 |   1925 | GPU
DEBUG 12-24 11:28:13.758215.758215 lmp.py:530]   Expert  3 |   1926 | GPU
DEBUG 12-24 11:28:13.758712.758712 lmp.py:530]   Expert  4 |   1931 | GPU
DEBUG 12-24 11:28:13.758971.758971 lmp.py:530]   Expert  2 |   1942 | GPU
DEBUG 12-24 11:28:13.758183.758183 lmp.py:531] 
DEBUG 12-24 11:28:13.758183.758183 lmp.py:531]   CPU total tokens: 202 (1.6%)
DEBUG 12-24 11:28:13.758826.758826 lmp.py:532]   GPU total tokens: 12086 (98.4%)
DEBUG 12-24 11:28:13.758568.758568 cuda_h.py:19] end experts_map_get cost 0.0013530254364013672 seconds
DEBUG 12-24 11:28:13.758019.758019 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.758915.758915 lmp.py:541] 
DEBUG 12-24 11:28:13.758915.758915 lmp.py:541]   Computing 30 experts on CPU...
DEBUG 12-24 11:28:13.759613.759613 cuda_h.py:19] end cpu_experts_submit cost 9.846687316894531e-05 seconds
DEBUG 12-24 11:28:13.759594.759594 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.759946.759946 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.759653.759653 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.759586.759586 cuda_h.py:19] end allocate_cuda_memory cost 0.0003390312194824219 seconds
DEBUG 12-24 11:28:13.759767.759767 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.759569.759569 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.759769.759769 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.759326.759326 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 1e9381e7-ff82-45a7-959a-af26a028f8a2
DEBUG 12-24 11:28:13.759485.759485 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.770489.770489 mlpmodule.py:630] group tensors cost 0.010436534881591797 s
INFO 12-24 11:28:13.771351.771351 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 1e9381e7-ff82-45a7-959a-af26a028f8a2
DEBUG 12-24 11:28:13.771065.771065 cuda_h.py:19] end load_into_gpu_async cost 0.01163172721862793 seconds
DEBUG 12-24 11:28:13.771834.771834 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.771907.771907 cuda_h.py:19] end restore_tensors2 cost 0.00036787986755371094 seconds
DEBUG 12-24 11:28:13.771750.771750 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.012717723846435547 seconds
DEBUG 12-24 11:28:13.775885.775885 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.01679205894470215 seconds
DEBUG 12-24 11:28:13.775881.775881 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 12-24 11:28:13.775227.775227 cuda_h.py:10] start start_load_qkvogn_s_weight_l_24
DEBUG 12-24 11:28:13.776322.776322 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 3.457069396972656e-05 seconds
DEBUG 12-24 11:28:13.776515.776515 cuda_h.py:19] end start_load_qkvogn_s_weight_l_24 cost 7.43865966796875e-05 seconds
DEBUG 12-24 11:28:13.776172.776172 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.776137.776137 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.776430.776430 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.776879.776879 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.777321.777321 cuda_h.py:19] end allocate_cuda_memory cost 0.0005834102630615234 seconds
DEBUG 12-24 11:28:13.777864.777864 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.777139.777139 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.777964.777964 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.777075.777075 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a0e308cb-af8f-4b96-91c2-45b09fc41b20
DEBUG 12-24 11:28:13.778051.778051 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.778653.778653 cuda_h.py:19] end gpu_sexperts cost 0.0024411678314208984 seconds
DEBUG 12-24 11:28:13.778107.778107 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.778745.778745 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 1e9381e7-ff82-45a7-959a-af26a028f8a2
INFO 12-24 11:28:13.779433.779433 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a0e308cb-af8f-4b96-91c2-45b09fc41b20
DEBUG 12-24 11:28:13.779995.779995 cuda_h.py:19] end load_into_gpu_async cost 0.0018198490142822266 seconds
DEBUG 12-24 11:28:13.779309.779309 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.779067.779067 cuda_h.py:19] end restore_tensors2 cost 0.00016307830810546875 seconds
DEBUG 12-24 11:28:13.780839.780839 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0033893585205078125 seconds
INFO 12-24 11:28:13.781256.781256 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a0e308cb-af8f-4b96-91c2-45b09fc41b20
DEBUG 12-24 11:28:13.782221.782221 mlpmodule.py:668] pad cost 0.010833501815795898 s
DEBUG 12-24 11:28:13.782682.782682 mlpmodule.py:674] create cpu tensor cost 3.981590270996094e-05 s
DEBUG 12-24 11:28:13.782147.782147 mlpmodule.py:679] move to cpu cost 2.765655517578125e-05 s
DEBUG 12-24 11:28:13.787958.787958 mlpmodule.py:694] group_w3: shape=torch.Size([30, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=86507520
DEBUG 12-24 11:28:13.787373.787373 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.787389.787389 mlpmodule.py:700] group_w3 first element: -0.0054931640625
WARNING 12-24 11:28:13.787247.787247 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.791452.791452 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.792289.792289 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.796738.796738 mlpmodule.py:723] group einsum cost 0.014590024948120117 s
DEBUG 12-24 11:28:13.797800.797800 mlpmodule.py:731] cpy2cputensor cost 0.0001399517059326172 s
DEBUG 12-24 11:28:13.813433.813433 mlpmodule.py:588]  experts func einsum cost 0.05365753173828125 s
INFO 12-24 11:28:13.821787.821787 client.py:127] Model loaded
DEBUG 12-24 11:28:13.821215.821215 cuda_h.py:19] end wait_experts cost 0.04251527786254883 seconds
DEBUG 12-24 11:28:13.821693.821693 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.821171.821171 lmp.py:585]   Computing 31 experts on GPU...
INFO 12-24 11:28:13.822333.822333 client.py:127] Model loaded
DEBUG 12-24 11:28:13.822691.822691 cuda_h.py:19] end sllm_worker_task cost 0.04576873779296875 seconds
DEBUG 12-24 11:28:13.822930.822930 mlpmodule.py:457] gpu group tensors cost 0.0010263919830322266 s
DEBUG 12-24 11:28:13.824337.824337 mlpmodule.py:490] gpu pad cost 0.001775503158569336 s
DEBUG 12-24 11:28:13.825161.825161 mlpmodule.py:508] gpu group einsum cost 0.0005886554718017578 s
DEBUG 12-24 11:28:13.828033.828033 mlpmodule.py:537] gpu experts func einsum cost 0.007081031799316406 s
DEBUG 12-24 11:28:13.828553.828553 cuda_h.py:19] end gpu_experts cost 0.0072710514068603516 seconds
DEBUG 12-24 11:28:13.828131.828131 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.828205.828205 cuda_h.py:19] end wait_cetm_experts cost 1.8835067749023438e-05 seconds
DEBUG 12-24 11:28:13.828915.828915 lmp.py:615] gpu end - einsum end = 26.9ms
DEBUG 12-24 11:28:13.828838.828838 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:13.828304.828304 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 2.2649765014648438e-05 seconds
DEBUG 12-24 11:28:13.828537.828537 cuda_h.py:19] end layer_moe_generate_23 cost 0.07212948799133301 seconds
DEBUG 12-24 11:28:13.829198.829198 lmp.py:445] -------------------------------- end layer 23 --------------------------------
DEBUG 12-24 11:28:13.829590.829590 lmp.py:418] -------------------------------- start layer 24 --------------------------------
DEBUG 12-24 11:28:13.829532.829532 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.829735.829735 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.834791.834791 cuda_h.py:19] end self_attn cost 0.005177021026611328 seconds
DEBUG 12-24 11:28:13.835939.835939 cuda_h.py:19] end iln_self_attn_paln cost 0.005814313888549805 seconds
DEBUG 12-24 11:28:13.835206.835206 cuda_h.py:10] start layer_moe_generate_24
DEBUG 12-24 11:28:13.835207.835207 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.835083.835083 cuda_h.py:19] end gate cost 0.0006122589111328125 seconds
DEBUG 12-24 11:28:13.835436.835436 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.836717.836717 lmp.py:519] 
DEBUG 12-24 11:28:13.836717.836717 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.836042.836042 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:13.836738.836738 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:13.836620.836620 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:13.836786.836786 lmp.py:523] 
DEBUG 12-24 11:28:13.836786.836786 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.836906.836906 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.836032.836032 lmp.py:530]   Expert 16 |      1 | CPU
DEBUG 12-24 11:28:13.836437.836437 lmp.py:530]   Expert 46 |      1 | CPU
DEBUG 12-24 11:28:13.836365.836365 lmp.py:530]   Expert 54 |      1 | CPU
DEBUG 12-24 11:28:13.836339.836339 lmp.py:530]   Expert 44 |      2 | CPU
DEBUG 12-24 11:28:13.836790.836790 lmp.py:530]   Expert 55 |      2 | CPU
DEBUG 12-24 11:28:13.836002.836002 lmp.py:530]   Expert 62 |      2 | CPU
DEBUG 12-24 11:28:13.836691.836691 lmp.py:530]   Expert 17 |      3 | CPU
DEBUG 12-24 11:28:13.836665.836665 lmp.py:530]   Expert 20 |      3 | CPU
DEBUG 12-24 11:28:13.836070.836070 lmp.py:530]   Expert 25 |      3 | CPU
DEBUG 12-24 11:28:13.836951.836951 lmp.py:530]   Expert 53 |      3 | CPU
DEBUG 12-24 11:28:13.836985.836985 lmp.py:530]   Expert 56 |      3 | CPU
DEBUG 12-24 11:28:13.836198.836198 lmp.py:530]   Expert 22 |      4 | CPU
DEBUG 12-24 11:28:13.836649.836649 lmp.py:530]   Expert 30 |      4 | CPU
DEBUG 12-24 11:28:13.836861.836861 lmp.py:530]   Expert 47 |      4 | CPU
DEBUG 12-24 11:28:13.836603.836603 lmp.py:530]   Expert  6 |      5 | CPU
DEBUG 12-24 11:28:13.836816.836816 lmp.py:530]   Expert 35 |      5 | CPU
DEBUG 12-24 11:28:13.836552.836552 lmp.py:530]   Expert 38 |      5 | CPU
DEBUG 12-24 11:28:13.836764.836764 lmp.py:530]   Expert 42 |      5 | CPU
DEBUG 12-24 11:28:13.836261.836261 lmp.py:530]   Expert 57 |      5 | CPU
DEBUG 12-24 11:28:13.836997.836997 lmp.py:530]   Expert 23 |      7 | CPU
DEBUG 12-24 11:28:13.836732.836732 lmp.py:530]   Expert 32 |      7 | CPU
DEBUG 12-24 11:28:13.836468.836468 lmp.py:530]   Expert 28 |      8 | CPU
DEBUG 12-24 11:28:13.836965.836965 lmp.py:530]   Expert 50 |      8 | CPU
DEBUG 12-24 11:28:13.836701.836701 lmp.py:530]   Expert 60 |      8 | CPU
DEBUG 12-24 11:28:13.836437.836437 lmp.py:530]   Expert 29 |      9 | CPU
DEBUG 12-24 11:28:13.836411.836411 lmp.py:530]   Expert 48 |      9 | CPU
DEBUG 12-24 11:28:13.836908.836908 lmp.py:530]   Expert 59 |      9 | CPU
DEBUG 12-24 11:28:13.836074.836074 lmp.py:530]   Expert 63 |      9 | CPU
DEBUG 12-24 11:28:13.836240.836240 lmp.py:530]   Expert 14 |     10 | CPU
DEBUG 12-24 11:28:13.836790.836790 lmp.py:530]   Expert 43 |     10 | CPU
DEBUG 12-24 11:28:13.836764.836764 lmp.py:530]   Expert 51 |     10 | CPU
DEBUG 12-24 11:28:13.836738.836738 lmp.py:530]   Expert 52 |     10 | CPU
DEBUG 12-24 11:28:13.836713.836713 lmp.py:530]   Expert 27 |     11 | GPU
DEBUG 12-24 11:28:13.836210.836210 lmp.py:530]   Expert 41 |     11 | GPU
DEBUG 12-24 11:28:13.836945.836945 lmp.py:530]   Expert 61 |     11 | GPU
DEBUG 12-24 11:28:13.836204.836204 lmp.py:530]   Expert 15 |     12 | GPU
DEBUG 12-24 11:28:13.836701.836701 lmp.py:530]   Expert 40 |     12 | GPU
DEBUG 12-24 11:28:13.836437.836437 lmp.py:530]   Expert 36 |     13 | GPU
DEBUG 12-24 11:28:13.836934.836934 lmp.py:530]   Expert 37 |     13 | GPU
DEBUG 12-24 11:28:13.836431.836431 lmp.py:530]   Expert 49 |     13 | GPU
DEBUG 12-24 11:28:13.836929.836929 lmp.py:530]   Expert 58 |     13 | GPU
DEBUG 12-24 11:28:13.836664.836664 lmp.py:530]   Expert 26 |     15 | GPU
DEBUG 12-24 11:28:13.836115.836115 lmp.py:530]   Expert 18 |     17 | GPU
DEBUG 12-24 11:28:13.836566.836566 lmp.py:530]   Expert 34 |     17 | GPU
DEBUG 12-24 11:28:13.836116.836116 lmp.py:530]   Expert 10 |     18 | GPU
DEBUG 12-24 11:28:13.836044.836044 lmp.py:530]   Expert 13 |     20 | GPU
DEBUG 12-24 11:28:13.836018.836018 lmp.py:530]   Expert 24 |     21 | GPU
DEBUG 12-24 11:28:13.837992.837992 lmp.py:530]   Expert 31 |     22 | GPU
DEBUG 12-24 11:28:13.837728.837728 lmp.py:530]   Expert 45 |     22 | GPU
DEBUG 12-24 11:28:13.837987.837987 lmp.py:530]   Expert  7 |     24 | GPU
DEBUG 12-24 11:28:13.837484.837484 lmp.py:530]   Expert  8 |     24 | GPU
DEBUG 12-24 11:28:13.837981.837981 lmp.py:530]   Expert 19 |     25 | GPU
DEBUG 12-24 11:28:13.837478.837478 lmp.py:530]   Expert 21 |     26 | GPU
DEBUG 12-24 11:28:13.837975.837975 lmp.py:530]   Expert 12 |     28 | GPU
DEBUG 12-24 11:28:13.837234.837234 lmp.py:530]   Expert 11 |     30 | GPU
DEBUG 12-24 11:28:13.837970.837970 lmp.py:530]   Expert 39 |     30 | GPU
DEBUG 12-24 11:28:13.837705.837705 lmp.py:530]   Expert 33 |     31 | GPU
DEBUG 12-24 11:28:13.837441.837441 lmp.py:530]   Expert  9 |     50 | GPU
DEBUG 12-24 11:28:13.837607.837607 lmp.py:530]   Expert  0 |   1924 | GPU
DEBUG 12-24 11:28:13.837581.837581 lmp.py:530]   Expert  4 |   1924 | GPU
DEBUG 12-24 11:28:13.837555.837555 lmp.py:530]   Expert  2 |   1929 | GPU
DEBUG 12-24 11:28:13.837052.837052 lmp.py:530]   Expert  5 |   1929 | GPU
DEBUG 12-24 11:28:13.837788.837788 lmp.py:530]   Expert  1 |   1935 | GPU
DEBUG 12-24 11:28:13.837285.837285 lmp.py:530]   Expert  3 |   1943 | GPU
DEBUG 12-24 11:28:13.837975.837975 lmp.py:531] 
DEBUG 12-24 11:28:13.837975.837975 lmp.py:531]   CPU total tokens: 175 (1.4%)
DEBUG 12-24 11:28:13.837425.837425 lmp.py:532]   GPU total tokens: 12113 (98.6%)
DEBUG 12-24 11:28:13.837645.837645 cuda_h.py:19] end experts_map_get cost 0.0014605522155761719 seconds
DEBUG 12-24 11:28:13.837572.837572 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.837807.837807 lmp.py:541] 
DEBUG 12-24 11:28:13.837807.837807 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:13.837881.837881 cuda_h.py:19] end cpu_experts_submit cost 0.00010180473327636719 seconds
DEBUG 12-24 11:28:13.837531.837531 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.837029.837029 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.837405.837405 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.837369.837369 cuda_h.py:19] end allocate_cuda_memory cost 0.0002911090850830078 seconds
DEBUG 12-24 11:28:13.838981.838981 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.838167.838167 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.838559.838559 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.838785.838785 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 2038df12-13d1-4c17-9cee-9c9a70d823a7
DEBUG 12-24 11:28:13.838620.838620 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.854932.854932 mlpmodule.py:630] group tensors cost 0.015649080276489258 s
INFO 12-24 11:28:13.854898.854898 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 2038df12-13d1-4c17-9cee-9c9a70d823a7
DEBUG 12-24 11:28:13.855344.855344 cuda_h.py:19] end load_into_gpu_async cost 0.016945362091064453 seconds
DEBUG 12-24 11:28:13.855299.855299 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.855233.855233 cuda_h.py:19] end restore_tensors2 cost 0.00037288665771484375 seconds
DEBUG 12-24 11:28:13.855269.855269 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.01802349090576172 seconds
DEBUG 12-24 11:28:13.859779.859779 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.022412538528442383 seconds
DEBUG 12-24 11:28:13.859497.859497 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 12-24 11:28:13.860988.860988 cuda_h.py:10] start start_load_qkvogn_s_weight_l_25
DEBUG 12-24 11:28:13.860845.860845 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 3.314018249511719e-05 seconds
DEBUG 12-24 11:28:13.860323.860323 cuda_h.py:19] end start_load_qkvogn_s_weight_l_25 cost 7.200241088867188e-05 seconds
DEBUG 12-24 11:28:13.860741.860741 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.860177.860177 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.860065.860065 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.860635.860635 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.862338.862338 cuda_h.py:19] end allocate_cuda_memory cost 0.001209259033203125 seconds
DEBUG 12-24 11:28:13.862094.862094 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.862066.862066 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.862011.862011 cuda_h.py:19] end gpu_sexperts cost 0.0024614334106445312 seconds
DEBUG 12-24 11:28:13.862863.862863 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.863554.863554 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 770a094e-14d7-4fd1-a8c5-1f5a71ba48b5
DEBUG 12-24 11:28:13.863309.863309 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.862364.862364 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.863869.863869 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 2038df12-13d1-4c17-9cee-9c9a70d823a7
DEBUG 12-24 11:28:13.864970.864970 mlpmodule.py:668] pad cost 0.009377002716064453 s
DEBUG 12-24 11:28:13.864298.864298 mlpmodule.py:674] create cpu tensor cost 3.838539123535156e-05 s
DEBUG 12-24 11:28:13.864923.864923 mlpmodule.py:679] move to cpu cost 3.886222839355469e-05 s
INFO 12-24 11:28:13.865623.865623 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 770a094e-14d7-4fd1-a8c5-1f5a71ba48b5
DEBUG 12-24 11:28:13.866201.866201 cuda_h.py:19] end load_into_gpu_async cost 0.0037841796875 seconds
DEBUG 12-24 11:28:13.866252.866252 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.866021.866021 cuda_h.py:19] end restore_tensors2 cost 0.0003426074981689453 seconds
DEBUG 12-24 11:28:13.867270.867270 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.006346225738525391 seconds
INFO 12-24 11:28:13.868180.868180 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 770a094e-14d7-4fd1-a8c5-1f5a71ba48b5
DEBUG 12-24 11:28:13.869383.869383 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:13.869355.869355 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.869517.869517 mlpmodule.py:700] group_w3 first element: -0.0108642578125
WARNING 12-24 11:28:13.869381.869381 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.873141.873141 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.874866.874866 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.878331.878331 mlpmodule.py:723] group einsum cost 0.013892412185668945 s
DEBUG 12-24 11:28:13.878089.878089 mlpmodule.py:731] cpy2cputensor cost 0.00012683868408203125 s
INFO 12-24 11:28:13.894419.894419 client.py:127] Model loaded
DEBUG 12-24 11:28:13.894965.894965 cuda_h.py:19] end wait_experts cost 0.030559062957763672 seconds
DEBUG 12-24 11:28:13.894052.894052 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.894186.894186 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:13.894545.894545 mlpmodule.py:457] gpu group tensors cost 0.0005700588226318359 s
DEBUG 12-24 11:28:13.896030.896030 mlpmodule.py:588]  experts func einsum cost 0.05755734443664551 s
DEBUG 12-24 11:28:13.896692.896692 mlpmodule.py:490] gpu pad cost 0.001558065414428711 s
DEBUG 12-24 11:28:13.897004.897004 mlpmodule.py:508] gpu group einsum cost 0.0005402565002441406 s
DEBUG 12-24 11:28:13.899967.899967 mlpmodule.py:537] gpu experts func einsum cost 0.005657672882080078 s
DEBUG 12-24 11:28:13.900890.900890 cuda_h.py:19] end gpu_experts cost 0.005823373794555664 seconds
DEBUG 12-24 11:28:13.900554.900554 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.900337.900337 cuda_h.py:19] end wait_cetm_experts cost 1.52587890625e-05 seconds
DEBUG 12-24 11:28:13.900132.900132 lmp.py:615] gpu end - einsum end = 16.6ms
DEBUG 12-24 11:28:13.900565.900565 cuda_h.py:10] start wait_load_qkvogn_s_weight
INFO 12-24 11:28:13.900270.900270 client.py:127] Model loaded
DEBUG 12-24 11:28:13.900919.900919 cuda_h.py:19] end sllm_worker_task cost 0.04013490676879883 seconds
DEBUG 12-24 11:28:13.900381.900381 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 0.00058746337890625 seconds
DEBUG 12-24 11:28:13.900152.900152 cuda_h.py:19] end layer_moe_generate_24 cost 0.06580209732055664 seconds
DEBUG 12-24 11:28:13.901707.901707 lmp.py:445] -------------------------------- end layer 24 --------------------------------
DEBUG 12-24 11:28:13.901523.901523 lmp.py:418] -------------------------------- start layer 25 --------------------------------
DEBUG 12-24 11:28:13.901312.901312 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.901354.901354 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.907009.907009 cuda_h.py:19] end self_attn cost 0.005828857421875 seconds
DEBUG 12-24 11:28:13.907336.907336 cuda_h.py:19] end iln_self_attn_paln cost 0.0064127445220947266 seconds
DEBUG 12-24 11:28:13.907748.907748 cuda_h.py:10] start layer_moe_generate_25
DEBUG 12-24 11:28:13.907749.907749 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.908022.908022 cuda_h.py:19] end gate cost 0.0005891323089599609 seconds
DEBUG 12-24 11:28:13.908898.908898 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.908099.908099 lmp.py:519] 
DEBUG 12-24 11:28:13.908099.908099 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.908809.908809 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:13.908982.908982 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:13.908532.908532 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:13.908698.908698 lmp.py:523] 
DEBUG 12-24 11:28:13.908698.908698 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.908103.908103 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.908660.908660 lmp.py:530]   Expert  9 |      1 | CPU
DEBUG 12-24 11:28:13.908542.908542 lmp.py:530]   Expert 23 |      1 | CPU
DEBUG 12-24 11:28:13.908469.908469 lmp.py:530]   Expert 20 |      2 | CPU
DEBUG 12-24 11:28:13.908920.908920 lmp.py:530]   Expert 34 |      2 | CPU
DEBUG 12-24 11:28:13.908371.908371 lmp.py:530]   Expert 22 |      3 | CPU
DEBUG 12-24 11:28:13.908583.908583 lmp.py:530]   Expert 33 |      3 | CPU
DEBUG 12-24 11:28:13.908796.908796 lmp.py:530]   Expert  6 |      4 | CPU
DEBUG 12-24 11:28:13.908532.908532 lmp.py:530]   Expert 10 |      4 | CPU
DEBUG 12-24 11:28:13.908506.908506 lmp.py:530]   Expert 13 |      4 | CPU
DEBUG 12-24 11:28:13.908718.908718 lmp.py:530]   Expert 16 |      4 | CPU
DEBUG 12-24 11:28:13.908454.908454 lmp.py:530]   Expert 27 |      4 | CPU
DEBUG 12-24 11:28:13.908381.908381 lmp.py:530]   Expert 36 |      4 | CPU
DEBUG 12-24 11:28:13.908071.908071 lmp.py:530]   Expert 59 |      4 | CPU
DEBUG 12-24 11:28:13.908475.908475 lmp.py:530]   Expert 15 |      5 | CPU
DEBUG 12-24 11:28:13.909403.909403 lmp.py:530]   Expert 24 |      5 | CPU
DEBUG 12-24 11:28:13.909854.909854 lmp.py:530]   Expert 31 |      5 | CPU
DEBUG 12-24 11:28:13.909590.909590 lmp.py:530]   Expert 38 |      5 | CPU
DEBUG 12-24 11:28:13.909325.909325 lmp.py:530]   Expert 44 |      5 | CPU
DEBUG 12-24 11:28:13.909061.909061 lmp.py:530]   Expert 46 |      5 | CPU
DEBUG 12-24 11:28:13.909035.909035 lmp.py:530]   Expert 50 |      5 | CPU
DEBUG 12-24 11:28:13.909770.909770 lmp.py:530]   Expert 18 |      6 | CPU
DEBUG 12-24 11:28:13.909744.909744 lmp.py:530]   Expert 21 |      6 | CPU
DEBUG 12-24 11:28:13.909242.909242 lmp.py:530]   Expert 32 |      6 | CPU
DEBUG 12-24 11:28:13.909977.909977 lmp.py:530]   Expert  8 |      7 | CPU
DEBUG 12-24 11:28:13.909713.909713 lmp.py:530]   Expert 56 |      7 | CPU
DEBUG 12-24 11:28:13.909449.909449 lmp.py:530]   Expert 61 |      7 | CPU
DEBUG 12-24 11:28:13.909184.909184 lmp.py:530]   Expert 14 |      8 | CPU
DEBUG 12-24 11:28:13.909874.909874 lmp.py:530]   Expert 42 |      8 | CPU
DEBUG 12-24 11:28:13.909324.909324 lmp.py:530]   Expert 47 |      8 | CPU
DEBUG 12-24 11:28:13.909014.909014 lmp.py:530]   Expert 48 |      8 | CPU
DEBUG 12-24 11:28:13.909226.909226 lmp.py:530]   Expert 26 |     10 | CPU
DEBUG 12-24 11:28:13.909677.909677 lmp.py:530]   Expert 28 |     10 | CPU
DEBUG 12-24 11:28:13.909651.909651 lmp.py:530]   Expert 41 |     10 | GPU
DEBUG 12-24 11:28:13.909387.909387 lmp.py:530]   Expert 43 |     11 | GPU
DEBUG 12-24 11:28:13.909122.909122 lmp.py:530]   Expert 55 |     11 | GPU
DEBUG 12-24 11:28:13.909858.909858 lmp.py:530]   Expert 62 |     11 | GPU
DEBUG 12-24 11:28:13.909832.909832 lmp.py:530]   Expert 51 |     12 | GPU
DEBUG 12-24 11:28:13.909329.909329 lmp.py:530]   Expert 52 |     12 | GPU
DEBUG 12-24 11:28:13.909303.909303 lmp.py:530]   Expert 53 |     12 | GPU
DEBUG 12-24 11:28:13.909800.909800 lmp.py:530]   Expert 63 |     12 | GPU
DEBUG 12-24 11:28:13.909775.909775 lmp.py:530]   Expert 25 |     15 | GPU
DEBUG 12-24 11:28:13.909510.909510 lmp.py:530]   Expert 39 |     16 | GPU
DEBUG 12-24 11:28:13.909007.909007 lmp.py:530]   Expert 60 |     16 | GPU
DEBUG 12-24 11:28:13.909220.909220 lmp.py:530]   Expert  7 |     18 | GPU
DEBUG 12-24 11:28:13.909432.909432 lmp.py:530]   Expert 11 |     18 | GPU
DEBUG 12-24 11:28:13.909883.909883 lmp.py:530]   Expert 45 |     19 | GPU
DEBUG 12-24 11:28:13.909049.909049 lmp.py:530]   Expert 40 |     21 | GPU
DEBUG 12-24 11:28:13.909500.909500 lmp.py:530]   Expert 49 |     21 | GPU
DEBUG 12-24 11:28:13.909666.909666 lmp.py:530]   Expert 58 |     21 | GPU
DEBUG 12-24 11:28:13.909640.909640 lmp.py:530]   Expert 57 |     23 | GPU
DEBUG 12-24 11:28:13.909376.909376 lmp.py:530]   Expert 12 |     25 | GPU
DEBUG 12-24 11:28:13.909635.909635 lmp.py:530]   Expert 37 |     25 | GPU
DEBUG 12-24 11:28:13.909370.909370 lmp.py:530]   Expert 29 |     29 | GPU
DEBUG 12-24 11:28:13.909868.909868 lmp.py:530]   Expert 30 |     31 | GPU
DEBUG 12-24 11:28:13.909603.909603 lmp.py:530]   Expert 54 |     31 | GPU
DEBUG 12-24 11:28:13.909862.909862 lmp.py:530]   Expert 19 |     32 | GPU
DEBUG 12-24 11:28:13.909074.909074 lmp.py:530]   Expert 17 |     40 | GPU
DEBUG 12-24 11:28:13.909810.909810 lmp.py:530]   Expert 35 |     40 | GPU
DEBUG 12-24 11:28:13.909546.909546 lmp.py:530]   Expert  2 |   1925 | GPU
DEBUG 12-24 11:28:13.909805.909805 lmp.py:530]   Expert  5 |   1928 | GPU
DEBUG 12-24 11:28:13.909494.909494 lmp.py:530]   Expert  1 |   1929 | GPU
DEBUG 12-24 11:28:13.909945.909945 lmp.py:530]   Expert  3 |   1929 | GPU
DEBUG 12-24 11:28:13.909634.909634 lmp.py:530]   Expert  0 |   1935 | GPU
DEBUG 12-24 11:28:13.909846.909846 lmp.py:530]   Expert  4 |   1944 | GPU
DEBUG 12-24 11:28:13.909489.909489 lmp.py:531] 
DEBUG 12-24 11:28:13.909489.909489 lmp.py:531]   CPU total tokens: 166 (1.4%)
DEBUG 12-24 11:28:13.909179.909179 lmp.py:532]   GPU total tokens: 12122 (98.6%)
DEBUG 12-24 11:28:13.909875.909875 cuda_h.py:19] end experts_map_get cost 0.0014653205871582031 seconds
DEBUG 12-24 11:28:13.909326.909326 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.909083.909083 lmp.py:541] 
DEBUG 12-24 11:28:13.909083.909083 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:13.909158.909158 cuda_h.py:19] end cpu_experts_submit cost 0.00010132789611816406 seconds
DEBUG 12-24 11:28:13.910377.910377 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.910875.910875 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.910059.910059 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.910540.910540 cuda_h.py:19] end allocate_cuda_memory cost 0.00028514862060546875 seconds
DEBUG 12-24 11:28:13.910914.910914 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.910783.910783 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.910937.910937 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.910970.910970 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f9ecac10-6f1d-49f9-b80c-bfa862bef6b3
DEBUG 12-24 11:28:13.910918.910918 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.923158.923158 mlpmodule.py:630] group tensors cost 0.012325763702392578 s
INFO 12-24 11:28:13.924553.924553 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f9ecac10-6f1d-49f9-b80c-bfa862bef6b3
DEBUG 12-24 11:28:13.924895.924895 cuda_h.py:19] end load_into_gpu_async cost 0.013639211654663086 seconds
DEBUG 12-24 11:28:13.924565.924565 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.924215.924215 cuda_h.py:19] end restore_tensors2 cost 0.00037407875061035156 seconds
DEBUG 12-24 11:28:13.924203.924203 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.01471853256225586 seconds
DEBUG 12-24 11:28:13.929478.929478 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.019003868103027344 seconds
DEBUG 12-24 11:28:13.929408.929408 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 12-24 11:28:13.929423.929423 cuda_h.py:10] start start_load_qkvogn_s_weight_l_26
DEBUG 12-24 11:28:13.929134.929134 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 3.2901763916015625e-05 seconds
DEBUG 12-24 11:28:13.929274.929274 cuda_h.py:19] end start_load_qkvogn_s_weight_l_26 cost 6.937980651855469e-05 seconds
DEBUG 12-24 11:28:13.929308.929308 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:13.929306.929306 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:13.929643.929643 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.929603.929603 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.930766.930766 cuda_h.py:19] end allocate_cuda_memory cost 0.0007710456848144531 seconds
DEBUG 12-24 11:28:13.930668.930668 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.931281.931281 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.931679.931679 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.931026.931026 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, e1d92084-14ff-4c44-b353-173a8c183143
DEBUG 12-24 11:28:13.931775.931775 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.931435.931435 cuda_h.py:19] end gpu_sexperts cost 0.002356290817260742 seconds
DEBUG 12-24 11:28:13.932021.932021 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:13.932427.932427 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f9ecac10-6f1d-49f9-b80c-bfa862bef6b3
INFO 12-24 11:28:13.932421.932421 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, e1d92084-14ff-4c44-b353-173a8c183143
DEBUG 12-24 11:28:13.933195.933195 mlpmodule.py:668] pad cost 0.008949756622314453 s
DEBUG 12-24 11:28:13.933246.933246 cuda_h.py:19] end load_into_gpu_async cost 0.0021076202392578125 seconds
DEBUG 12-24 11:28:13.933369.933369 mlpmodule.py:674] create cpu tensor cost 5.14984130859375e-05 s
DEBUG 12-24 11:28:13.933757.933757 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.933159.933159 cuda_h.py:19] end restore_tensors2 cost 0.000164031982421875 seconds
DEBUG 12-24 11:28:13.933864.933864 mlpmodule.py:679] move to cpu cost 0.0003266334533691406 s
DEBUG 12-24 11:28:13.933695.933695 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004182100296020508 seconds
INFO 12-24 11:28:13.935028.935028 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, e1d92084-14ff-4c44-b353-173a8c183143
DEBUG 12-24 11:28:13.939534.939534 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:13.939923.939923 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:13.939031.939031 mlpmodule.py:700] group_w3 first element: -0.01397705078125
WARNING 12-24 11:28:13.939651.939651 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:13.943253.943253 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:13.943150.943150 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:13.948294.948294 mlpmodule.py:723] group einsum cost 0.01421356201171875 s
DEBUG 12-24 11:28:13.948371.948371 mlpmodule.py:731] cpy2cputensor cost 0.00012993812561035156 s
DEBUG 12-24 11:28:13.965009.965009 mlpmodule.py:588]  experts func einsum cost 0.054778099060058594 s
INFO 12-24 11:28:13.966373.966373 client.py:127] Model loaded
DEBUG 12-24 11:28:13.966535.966535 cuda_h.py:19] end wait_experts cost 0.03487586975097656 seconds
DEBUG 12-24 11:28:13.966622.966622 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:13.966232.966232 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:13.967471.967471 mlpmodule.py:457] gpu group tensors cost 0.0005242824554443359 s
DEBUG 12-24 11:28:13.969341.969341 mlpmodule.py:490] gpu pad cost 0.0014333724975585938 s
DEBUG 12-24 11:28:13.969881.969881 mlpmodule.py:508] gpu group einsum cost 0.0004413127899169922 s
DEBUG 12-24 11:28:13.972484.972484 mlpmodule.py:537] gpu experts func einsum cost 0.005273342132568359 s
DEBUG 12-24 11:28:13.972156.972156 cuda_h.py:19] end gpu_experts cost 0.0054585933685302734 seconds
DEBUG 12-24 11:28:13.972197.972197 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:13.972682.972682 cuda_h.py:19] end wait_cetm_experts cost 1.4066696166992188e-05 seconds
DEBUG 12-24 11:28:13.972716.972716 lmp.py:615] gpu end - einsum end = 19.1ms
DEBUG 12-24 11:28:13.972480.972480 cuda_h.py:10] start wait_load_qkvogn_s_weight
INFO 12-24 11:28:13.972614.972614 client.py:127] Model loaded
DEBUG 12-24 11:28:13.973494.973494 cuda_h.py:19] end sllm_worker_task cost 0.0434267520904541 seconds
DEBUG 12-24 11:28:13.973089.973089 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 0.0005426406860351562 seconds
DEBUG 12-24 11:28:13.973237.973237 cuda_h.py:19] end layer_moe_generate_25 cost 0.06560611724853516 seconds
DEBUG 12-24 11:28:13.973256.973256 lmp.py:445] -------------------------------- end layer 25 --------------------------------
DEBUG 12-24 11:28:13.973734.973734 lmp.py:418] -------------------------------- start layer 26 --------------------------------
DEBUG 12-24 11:28:13.973238.973238 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:13.973942.973942 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:13.979074.979074 cuda_h.py:19] end self_attn cost 0.006042957305908203 seconds
DEBUG 12-24 11:28:13.980725.980725 cuda_h.py:19] end iln_self_attn_paln cost 0.006615638732910156 seconds
DEBUG 12-24 11:28:13.980753.980753 cuda_h.py:10] start layer_moe_generate_26
DEBUG 12-24 11:28:13.980755.980755 cuda_h.py:10] start gate
DEBUG 12-24 11:28:13.980479.980479 cuda_h.py:19] end gate cost 0.0006420612335205078 seconds
DEBUG 12-24 11:28:13.980832.980832 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:13.981636.981636 lmp.py:519] 
DEBUG 12-24 11:28:13.981636.981636 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:13.981200.981200 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:13.981088.981088 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:13.981161.981161 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:13.981043.981043 lmp.py:523] 
DEBUG 12-24 11:28:13.981043.981043 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:13.981447.981447 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:13.981574.981574 lmp.py:530]   Expert  6 |      2 | CPU
DEBUG 12-24 11:28:13.981740.981740 lmp.py:530]   Expert  7 |      2 | CPU
DEBUG 12-24 11:28:13.981429.981429 lmp.py:530]   Expert 12 |      4 | CPU
DEBUG 12-24 11:28:13.981642.981642 lmp.py:530]   Expert 30 |      4 | CPU
DEBUG 12-24 11:28:13.981854.981854 lmp.py:530]   Expert 36 |      4 | CPU
DEBUG 12-24 11:28:13.981828.981828 lmp.py:530]   Expert 53 |      4 | CPU
DEBUG 12-24 11:28:13.981326.981326 lmp.py:530]   Expert 61 |      4 | CPU
DEBUG 12-24 11:28:13.981538.981538 lmp.py:530]   Expert 21 |      5 | CPU
DEBUG 12-24 11:28:13.981274.981274 lmp.py:530]   Expert 22 |      5 | CPU
DEBUG 12-24 11:28:13.981725.981725 lmp.py:530]   Expert 29 |      5 | CPU
DEBUG 12-24 11:28:13.981937.981937 lmp.py:530]   Expert 58 |      5 | CPU
DEBUG 12-24 11:28:13.981626.981626 lmp.py:530]   Expert 59 |      5 | CPU
DEBUG 12-24 11:28:13.981600.981600 lmp.py:530]   Expert  8 |      6 | CPU
DEBUG 12-24 11:28:13.981574.981574 lmp.py:530]   Expert 11 |      6 | CPU
DEBUG 12-24 11:28:13.981310.981310 lmp.py:530]   Expert 19 |      6 | CPU
DEBUG 12-24 11:28:13.981807.981807 lmp.py:530]   Expert 41 |      6 | CPU
DEBUG 12-24 11:28:13.981304.981304 lmp.py:530]   Expert 43 |      6 | CPU
DEBUG 12-24 11:28:13.981040.981040 lmp.py:530]   Expert 48 |      6 | CPU
DEBUG 12-24 11:28:13.981537.981537 lmp.py:530]   Expert 62 |      6 | CPU
DEBUG 12-24 11:28:13.981988.981988 lmp.py:530]   Expert 17 |      7 | CPU
DEBUG 12-24 11:28:13.981962.981962 lmp.py:530]   Expert 25 |      7 | CPU
DEBUG 12-24 11:28:13.981221.981221 lmp.py:530]   Expert 34 |      7 | CPU
DEBUG 12-24 11:28:13.981195.981195 lmp.py:530]   Expert 51 |      7 | CPU
DEBUG 12-24 11:28:13.981931.981931 lmp.py:530]   Expert 57 |      7 | CPU
DEBUG 12-24 11:28:13.981666.981666 lmp.py:530]   Expert 49 |      8 | CPU
DEBUG 12-24 11:28:13.981832.981832 lmp.py:530]   Expert  9 |      9 | CPU
DEBUG 12-24 11:28:13.981045.981045 lmp.py:530]   Expert 39 |      9 | CPU
DEBUG 12-24 11:28:13.981734.981734 lmp.py:530]   Expert 42 |      9 | CPU
DEBUG 12-24 11:28:13.981423.981423 lmp.py:530]   Expert 45 |      9 | CPU
DEBUG 12-24 11:28:13.981113.981113 lmp.py:530]   Expert 56 |      9 | CPU
DEBUG 12-24 11:28:13.981087.981087 lmp.py:530]   Expert 38 |     10 | CPU
DEBUG 12-24 11:28:13.981822.981822 lmp.py:530]   Expert 47 |     10 | CPU
DEBUG 12-24 11:28:13.981558.981558 lmp.py:530]   Expert 26 |     11 | GPU
DEBUG 12-24 11:28:13.981532.981532 lmp.py:530]   Expert 27 |     11 | GPU
DEBUG 12-24 11:28:13.981268.981268 lmp.py:530]   Expert 10 |     12 | GPU
DEBUG 12-24 11:28:13.981480.981480 lmp.py:530]   Expert 13 |     12 | GPU
DEBUG 12-24 11:28:13.981454.981454 lmp.py:530]   Expert 28 |     12 | GPU
DEBUG 12-24 11:28:13.981951.981951 lmp.py:530]   Expert 31 |     12 | GPU
DEBUG 12-24 11:28:13.981164.981164 lmp.py:530]   Expert 37 |     12 | GPU
DEBUG 12-24 11:28:13.981092.981092 lmp.py:530]   Expert 32 |     13 | GPU
DEBUG 12-24 11:28:13.982019.982019 lmp.py:530]   Expert 24 |     14 | GPU
DEBUG 12-24 11:28:13.982470.982470 lmp.py:530]   Expert 40 |     14 | GPU
DEBUG 12-24 11:28:13.982636.982636 lmp.py:530]   Expert 54 |     14 | GPU
DEBUG 12-24 11:28:13.982087.982087 lmp.py:530]   Expert 60 |     14 | GPU
DEBUG 12-24 11:28:13.982823.982823 lmp.py:530]   Expert 15 |     15 | GPU
DEBUG 12-24 11:28:13.982559.982559 lmp.py:530]   Expert 33 |     15 | GPU
DEBUG 12-24 11:28:13.982533.982533 lmp.py:530]   Expert 23 |     17 | GPU
DEBUG 12-24 11:28:13.982268.982268 lmp.py:530]   Expert 50 |     18 | GPU
DEBUG 12-24 11:28:13.982242.982242 lmp.py:530]   Expert 52 |     18 | GPU
DEBUG 12-24 11:28:13.982739.982739 lmp.py:530]   Expert 46 |     19 | GPU
DEBUG 12-24 11:28:13.982714.982714 lmp.py:530]   Expert 16 |     20 | GPU
DEBUG 12-24 11:28:13.982449.982449 lmp.py:530]   Expert 55 |     20 | GPU
DEBUG 12-24 11:28:13.982615.982615 lmp.py:530]   Expert 18 |     22 | GPU
DEBUG 12-24 11:28:13.982066.982066 lmp.py:530]   Expert 44 |     26 | GPU
DEBUG 12-24 11:28:13.982232.982232 lmp.py:530]   Expert 35 |     32 | GPU
DEBUG 12-24 11:28:13.982398.982398 lmp.py:530]   Expert 14 |     35 | GPU
DEBUG 12-24 11:28:13.982326.982326 lmp.py:530]   Expert 20 |     36 | GPU
DEBUG 12-24 11:28:13.982539.982539 lmp.py:530]   Expert 63 |     36 | GPU
DEBUG 12-24 11:28:13.982274.982274 lmp.py:530]   Expert  3 |   1922 | GPU
DEBUG 12-24 11:28:13.982248.982248 lmp.py:530]   Expert  0 |   1930 | GPU
DEBUG 12-24 11:28:13.982745.982745 lmp.py:530]   Expert  1 |   1931 | GPU
DEBUG 12-24 11:28:13.982720.982720 lmp.py:530]   Expert  2 |   1937 | GPU
DEBUG 12-24 11:28:13.982217.982217 lmp.py:530]   Expert  4 |   1937 | GPU
DEBUG 12-24 11:28:13.982191.982191 lmp.py:530]   Expert  5 |   1952 | GPU
DEBUG 12-24 11:28:13.982119.982119 lmp.py:531] 
DEBUG 12-24 11:28:13.982119.982119 lmp.py:531]   CPU total tokens: 199 (1.6%)
DEBUG 12-24 11:28:13.982238.982238 lmp.py:532]   GPU total tokens: 12089 (98.4%)
DEBUG 12-24 11:28:13.982173.982173 cuda_h.py:19] end experts_map_get cost 0.0014526844024658203 seconds
DEBUG 12-24 11:28:13.982816.982816 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:13.982858.982858 lmp.py:541] 
DEBUG 12-24 11:28:13.982858.982858 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:13.982694.982694 cuda_h.py:19] end cpu_experts_submit cost 9.679794311523438e-05 seconds
DEBUG 12-24 11:28:13.982960.982960 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:13.982835.982835 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:13.982403.982403 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:13.983102.983102 cuda_h.py:19] end allocate_cuda_memory cost 0.0002713203430175781 seconds
DEBUG 12-24 11:28:13.983608.983608 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:13.983325.983325 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:13.983677.983677 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:13.983665.983665 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b25f8250-c08d-4d45-936e-b0e06f1303ff
DEBUG 12-24 11:28:13.983777.983777 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:13.996678.996678 mlpmodule.py:630] group tensors cost 0.012679815292358398 s
INFO 12-24 11:28:13.997247.997247 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b25f8250-c08d-4d45-936e-b0e06f1303ff
DEBUG 12-24 11:28:13.997739.997739 cuda_h.py:19] end load_into_gpu_async cost 0.013960599899291992 seconds
DEBUG 12-24 11:28:13.997217.997217 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:13.997237.997237 cuda_h.py:19] end restore_tensors2 cost 0.0003657341003417969 seconds
DEBUG 12-24 11:28:13.997173.997173 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.01500558853149414 seconds
DEBUG 12-24 11:28:14.001730.001730 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.019217967987060547 seconds
DEBUG 12-24 11:28:14.001693.001693 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 12-24 11:28:14.001900.001900 cuda_h.py:10] start start_load_qkvogn_s_weight_l_27
DEBUG 12-24 11:28:14.002611.002611 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 3.170967102050781e-05 seconds
DEBUG 12-24 11:28:14.002420.002420 cuda_h.py:19] end start_load_qkvogn_s_weight_l_27 cost 6.937980651855469e-05 seconds
DEBUG 12-24 11:28:14.002407.002407 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:14.002863.002863 cuda_h.py:10] start sllm_worker_task
DEBUG 12-24 11:28:14.002400.002400 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:14.002812.002812 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:14.003085.003085 cuda_h.py:19] end allocate_cuda_memory cost 0.0007054805755615234 seconds
DEBUG 12-24 11:28:14.003919.003919 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:14.003724.003724 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:14.004432.004432 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:14.004825.004825 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, 59c5eb39-c95d-4e52-9c39-3872374898e6
DEBUG 12-24 11:28:14.004182.004182 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:14.004306.004306 cuda_h.py:19] end gpu_sexperts cost 0.0022513866424560547 seconds
DEBUG 12-24 11:28:14.004911.004911 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:14.004164.004164 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b25f8250-c08d-4d45-936e-b0e06f1303ff
DEBUG 12-24 11:28:14.005799.005799 mlpmodule.py:668] pad cost 0.008678197860717773 s
DEBUG 12-24 11:28:14.005504.005504 mlpmodule.py:674] create cpu tensor cost 3.886222839355469e-05 s
DEBUG 12-24 11:28:14.005301.005301 mlpmodule.py:679] move to cpu cost 2.6226043701171875e-05 s
INFO 12-24 11:28:14.006578.006578 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, 59c5eb39-c95d-4e52-9c39-3872374898e6
DEBUG 12-24 11:28:14.006597.006597 cuda_h.py:19] end load_into_gpu_async cost 0.002314329147338867 seconds
DEBUG 12-24 11:28:14.006367.006367 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:14.006635.006635 cuda_h.py:19] end restore_tensors2 cost 0.00038743019104003906 seconds
DEBUG 12-24 11:28:14.006758.006758 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.004186391830444336 seconds
INFO 12-24 11:28:14.009837.009837 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, 59c5eb39-c95d-4e52-9c39-3872374898e6
DEBUG 12-24 11:28:14.010175.010175 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:14.010888.010888 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:14.010427.010427 mlpmodule.py:700] group_w3 first element: 0.0145263671875
WARNING 12-24 11:28:14.011192.011192 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:14.015807.015807 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:14.015347.015347 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:14.020811.020811 mlpmodule.py:723] group einsum cost 0.014183282852172852 s
DEBUG 12-24 11:28:14.020715.020715 mlpmodule.py:731] cpy2cputensor cost 0.000133514404296875 s
DEBUG 12-24 11:28:14.037016.037016 mlpmodule.py:588]  experts func einsum cost 0.05422830581665039 s
INFO 12-24 11:28:14.038945.038945 client.py:127] Model loaded
DEBUG 12-24 11:28:14.038961.038961 cuda_h.py:19] end wait_experts cost 0.03369736671447754 seconds
DEBUG 12-24 11:28:14.038856.038856 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:14.038990.038990 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:14.039420.039420 mlpmodule.py:457] gpu group tensors cost 0.0005300045013427734 s
DEBUG 12-24 11:28:14.040073.040073 mlpmodule.py:490] gpu pad cost 0.0016238689422607422 s
DEBUG 12-24 11:28:14.041364.041364 mlpmodule.py:508] gpu group einsum cost 0.0005326271057128906 s
INFO 12-24 11:28:14.044279.044279 client.py:127] Model loaded
DEBUG 12-24 11:28:14.044394.044394 cuda_h.py:19] end sllm_worker_task cost 0.042128562927246094 seconds
DEBUG 12-24 11:28:14.044652.044652 mlpmodule.py:537] gpu experts func einsum cost 0.006220817565917969 s
DEBUG 12-24 11:28:14.045686.045686 cuda_h.py:19] end gpu_experts cost 0.006540775299072266 seconds
DEBUG 12-24 11:28:14.045047.045047 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:14.045110.045110 cuda_h.py:19] end wait_cetm_experts cost 3.170967102050781e-05 seconds
DEBUG 12-24 11:28:14.045974.045974 lmp.py:615] gpu end - einsum end = 19.9ms
DEBUG 12-24 11:28:14.045209.045209 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:14.045515.045515 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 1.9550323486328125e-05 seconds
DEBUG 12-24 11:28:14.045264.045264 cuda_h.py:19] end layer_moe_generate_26 cost 0.06527233123779297 seconds
DEBUG 12-24 11:28:14.045899.045899 lmp.py:445] -------------------------------- end layer 26 --------------------------------
DEBUG 12-24 11:28:14.045708.045708 lmp.py:418] -------------------------------- start layer 27 --------------------------------
DEBUG 12-24 11:28:14.045973.045973 cuda_h.py:10] start iln_self_attn_paln
DEBUG 12-24 11:28:14.045294.045294 cuda_h.py:10] start self_attn
cos shape torch.Size([64, 128]) sin shape torch.Size([64, 128]) position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]], device='cuda:1')
cos shape torch.Size([1, 1, 64, 128]) sin shape torch.Size([1, 1, 64, 128])
q_embed shape torch.Size([32, 16, 64, 128]) k_embed shape torch.Size([32, 16, 64, 128])
DEBUG 12-24 11:28:14.051367.051367 cuda_h.py:19] end self_attn cost 0.0055084228515625 seconds
DEBUG 12-24 11:28:14.051331.051331 cuda_h.py:19] end iln_self_attn_paln cost 0.006098747253417969 seconds
DEBUG 12-24 11:28:14.051359.051359 cuda_h.py:10] start layer_moe_generate_27
DEBUG 12-24 11:28:14.051360.051360 cuda_h.py:10] start gate
DEBUG 12-24 11:28:14.052091.052091 cuda_h.py:19] end gate cost 0.0006098747253417969 seconds
DEBUG 12-24 11:28:14.052443.052443 cuda_h.py:10] start experts_map_get
DEBUG 12-24 11:28:14.052439.052439 lmp.py:519] 
DEBUG 12-24 11:28:14.052439.052439 lmp.py:519] Expert Token Distribution & Device Allocation:
DEBUG 12-24 11:28:14.052718.052718 lmp.py:520]   Total experts: 64
DEBUG 12-24 11:28:14.052176.052176 lmp.py:521]   CPU experts: 32 (50%)
DEBUG 12-24 11:28:14.052442.052442 lmp.py:522]   GPU experts: 32 (50%)
DEBUG 12-24 11:28:14.052561.052561 lmp.py:523] 
DEBUG 12-24 11:28:14.052561.052561 lmp.py:523]   Expert ID | Tokens | Device
DEBUG 12-24 11:28:14.052489.052489 lmp.py:524]   -----------------------------------
DEBUG 12-24 11:28:14.053331.053331 lmp.py:530]   Expert 20 |      2 | CPU
DEBUG 12-24 11:28:14.053736.053736 lmp.py:530]   Expert 31 |      3 | CPU
DEBUG 12-24 11:28:14.053425.053425 lmp.py:530]   Expert 42 |      3 | CPU
DEBUG 12-24 11:28:14.053637.053637 lmp.py:530]   Expert 25 |      4 | CPU
DEBUG 12-24 11:28:14.053611.053611 lmp.py:530]   Expert 57 |      4 | CPU
DEBUG 12-24 11:28:14.053824.053824 lmp.py:530]   Expert 58 |      4 | CPU
DEBUG 12-24 11:28:14.053321.053321 lmp.py:530]   Expert 18 |      5 | CPU
DEBUG 12-24 11:28:14.053057.053057 lmp.py:530]   Expert 32 |      5 | CPU
DEBUG 12-24 11:28:14.053269.053269 lmp.py:530]   Expert 52 |      5 | CPU
DEBUG 12-24 11:28:14.053005.053005 lmp.py:530]   Expert 12 |      6 | CPU
DEBUG 12-24 11:28:14.053740.053740 lmp.py:530]   Expert 13 |      6 | CPU
DEBUG 12-24 11:28:14.053238.053238 lmp.py:530]   Expert 16 |      6 | CPU
DEBUG 12-24 11:28:14.053165.053165 lmp.py:530]   Expert 24 |      6 | CPU
DEBUG 12-24 11:28:14.053431.053431 lmp.py:530]   Expert 47 |      6 | CPU
DEBUG 12-24 11:28:14.053597.053597 lmp.py:530]   Expert 48 |      6 | CPU
DEBUG 12-24 11:28:14.053763.053763 lmp.py:530]   Expert 11 |      7 | CPU
DEBUG 12-24 11:28:14.053737.053737 lmp.py:530]   Expert 17 |      7 | CPU
DEBUG 12-24 11:28:14.053711.053711 lmp.py:530]   Expert 39 |      7 | CPU
DEBUG 12-24 11:28:14.053208.053208 lmp.py:530]   Expert 40 |      7 | CPU
DEBUG 12-24 11:28:14.053183.053183 lmp.py:530]   Expert 41 |      7 | CPU
DEBUG 12-24 11:28:14.053680.053680 lmp.py:530]   Expert 44 |      7 | CPU
DEBUG 12-24 11:28:14.053415.053415 lmp.py:530]   Expert 45 |      7 | CPU
DEBUG 12-24 11:28:14.053913.053913 lmp.py:530]   Expert 54 |      7 | CPU
DEBUG 12-24 11:28:14.053648.053648 lmp.py:530]   Expert 56 |      7 | CPU
DEBUG 12-24 11:28:14.053145.053145 lmp.py:530]   Expert 61 |      7 | CPU
DEBUG 12-24 11:28:14.053881.053881 lmp.py:530]   Expert  6 |      8 | CPU
DEBUG 12-24 11:28:14.053140.053140 lmp.py:530]   Expert  7 |      8 | CPU
DEBUG 12-24 11:28:14.053114.053114 lmp.py:530]   Expert  9 |      8 | CPU
DEBUG 12-24 11:28:14.053042.053042 lmp.py:530]   Expert 46 |      8 | CPU
DEBUG 12-24 11:28:14.053353.053353 lmp.py:530]   Expert 59 |      8 | CPU
DEBUG 12-24 11:28:14.053520.053520 lmp.py:530]   Expert 14 |      9 | CPU
DEBUG 12-24 11:28:14.053494.053494 lmp.py:530]   Expert 30 |      9 | CPU
DEBUG 12-24 11:28:14.053991.053991 lmp.py:530]   Expert 38 |      9 | GPU
DEBUG 12-24 11:28:14.053727.053727 lmp.py:530]   Expert 10 |     10 | GPU
DEBUG 12-24 11:28:14.053224.053224 lmp.py:530]   Expert 15 |     10 | GPU
DEBUG 12-24 11:28:14.053198.053198 lmp.py:530]   Expert 50 |     10 | GPU
DEBUG 12-24 11:28:14.053457.053457 lmp.py:530]   Expert 51 |     10 | GPU
DEBUG 12-24 11:28:14.053431.053431 lmp.py:530]   Expert 29 |     11 | GPU
DEBUG 12-24 11:28:14.053689.053689 lmp.py:530]   Expert 62 |     11 | GPU
DEBUG 12-24 11:28:14.053187.053187 lmp.py:530]   Expert 36 |     12 | GPU
DEBUG 12-24 11:28:14.053922.053922 lmp.py:530]   Expert 22 |     13 | GPU
DEBUG 12-24 11:28:14.053658.053658 lmp.py:530]   Expert 34 |     13 | GPU
DEBUG 12-24 11:28:14.053917.053917 lmp.py:530]   Expert 37 |     16 | GPU
DEBUG 12-24 11:28:14.053606.053606 lmp.py:530]   Expert 49 |     16 | GPU
DEBUG 12-24 11:28:14.053342.053342 lmp.py:530]   Expert 28 |     18 | GPU
DEBUG 12-24 11:28:14.053600.053600 lmp.py:530]   Expert 35 |     19 | GPU
DEBUG 12-24 11:28:14.053859.053859 lmp.py:530]   Expert  8 |     20 | GPU
DEBUG 12-24 11:28:14.053595.053595 lmp.py:530]   Expert 19 |     20 | GPU
DEBUG 12-24 11:28:14.053854.053854 lmp.py:530]   Expert 23 |     20 | GPU
DEBUG 12-24 11:28:14.053112.053112 lmp.py:530]   Expert 53 |     20 | GPU
DEBUG 12-24 11:28:14.053610.053610 lmp.py:530]   Expert 60 |     23 | GPU
DEBUG 12-24 11:28:14.053107.053107 lmp.py:530]   Expert 55 |     25 | GPU
DEBUG 12-24 11:28:14.053604.053604 lmp.py:530]   Expert 27 |     26 | GPU
DEBUG 12-24 11:28:14.053101.053101 lmp.py:530]   Expert 21 |     27 | GPU
DEBUG 12-24 11:28:14.053598.053598 lmp.py:530]   Expert 26 |     28 | GPU
DEBUG 12-24 11:28:14.053334.053334 lmp.py:530]   Expert 33 |     32 | GPU
DEBUG 12-24 11:28:14.053831.053831 lmp.py:530]   Expert 43 |     35 | GPU
DEBUG 12-24 11:28:14.053805.053805 lmp.py:530]   Expert 63 |     43 | GPU
DEBUG 12-24 11:28:14.053640.053640 lmp.py:530]   Expert  0 |   1928 | GPU
DEBUG 12-24 11:28:14.053045.053045 lmp.py:530]   Expert  2 |   1930 | GPU
DEBUG 12-24 11:28:14.053118.053118 lmp.py:530]   Expert  4 |   1931 | GPU
DEBUG 12-24 11:28:14.054285.054285 lmp.py:530]   Expert  5 |   1931 | GPU
DEBUG 12-24 11:28:14.054259.054259 lmp.py:530]   Expert  1 |   1936 | GPU
DEBUG 12-24 11:28:14.054994.054994 lmp.py:530]   Expert  3 |   1936 | GPU
DEBUG 12-24 11:28:14.054207.054207 lmp.py:531] 
DEBUG 12-24 11:28:14.054207.054207 lmp.py:531]   CPU total tokens: 199 (1.6%)
DEBUG 12-24 11:28:14.054658.054658 lmp.py:532]   GPU total tokens: 12089 (98.4%)
DEBUG 12-24 11:28:14.054162.054162 cuda_h.py:19] end experts_map_get cost 0.0014481544494628906 seconds
DEBUG 12-24 11:28:14.054328.054328 cuda_h.py:10] start cpu_experts_submit
DEBUG 12-24 11:28:14.054953.054953 lmp.py:541] 
DEBUG 12-24 11:28:14.054953.054953 lmp.py:541]   Computing 32 experts on CPU...
DEBUG 12-24 11:28:14.054312.054312 cuda_h.py:19] end cpu_experts_submit cost 0.00010848045349121094 seconds
DEBUG 12-24 11:28:14.054836.054836 cuda_h.py:10] start allocate_experts_cuda_memory_and_restore_model
DEBUG 12-24 11:28:14.054712.054712 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu
DEBUG 12-24 11:28:14.054916.054916 cuda_h.py:10] start allocate_cuda_memory
DEBUG 12-24 11:28:14.054635.054635 cuda_h.py:19] end allocate_cuda_memory cost 0.00028395652770996094 seconds
DEBUG 12-24 11:28:14.054447.054447 cuda_h.py:10] start load_into_gpu_async
DEBUG 12-24 11:28:14.054071.054071 sllm_store_c.py:27] get device uuid map
DEBUG 12-24 11:28:14.054224.054224 sllm_store_c.py:29] call client load into gpu
DEBUG 12-24 11:28:14.055450.055450 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, b3e7c2d7-ade5-4324-b12f-91379fa5d4c1
DEBUG 12-24 11:28:14.055954.055954 client.py:106] call stub.LoadModelAsync
DEBUG 12-24 11:28:14.067307.067307 mlpmodule.py:630] group tensors cost 0.012550115585327148 s
INFO 12-24 11:28:14.068892.068892 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, b3e7c2d7-ade5-4324-b12f-91379fa5d4c1
DEBUG 12-24 11:28:14.068379.068379 cuda_h.py:19] end load_into_gpu_async cost 0.013952970504760742 seconds
DEBUG 12-24 11:28:14.068003.068003 cuda_h.py:10] start restore_tensors2
DEBUG 12-24 11:28:14.069268.069268 cuda_h.py:19] end restore_tensors2 cost 0.000370025634765625 seconds
DEBUG 12-24 11:28:14.069185.069185 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.015068769454956055 seconds
DEBUG 12-24 11:28:14.073064.073064 cuda_h.py:19] end allocate_experts_cuda_memory_and_restore_model cost 0.019238948822021484 seconds
DEBUG 12-24 11:28:14.073106.073106 cuda_h.py:10] start start_load_qkvogn_s_weight_l_28
DEBUG 12-24 11:28:14.073174.073174 cuda_h.py:19] end start_load_qkvogn_s_weight_l_28 cost 1.0967254638671875e-05 seconds
DEBUG 12-24 11:28:14.073215.073215 cuda_h.py:10] start gpu_sexperts
DEBUG 12-24 11:28:14.074872.074872 cuda_h.py:19] end gpu_sexperts cost 0.0004096031188964844 seconds
DEBUG 12-24 11:28:14.074345.074345 cuda_h.py:10] start wait_experts
INFO 12-24 11:28:14.074452.074452 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, b3e7c2d7-ade5-4324-b12f-91379fa5d4c1
DEBUG 12-24 11:28:14.075449.075449 mlpmodule.py:668] pad cost 0.006432771682739258 s
DEBUG 12-24 11:28:14.075869.075869 mlpmodule.py:674] create cpu tensor cost 3.886222839355469e-05 s
DEBUG 12-24 11:28:14.075381.075381 mlpmodule.py:679] move to cpu cost 2.6226043701171875e-05 s
DEBUG 12-24 11:28:14.080044.080044 mlpmodule.py:694] group_w3: shape=torch.Size([32, 1408, 2048]), device=cpu, dtype=torch.bfloat16, numel=92274688
DEBUG 12-24 11:28:14.080446.080446 mlpmodule.py:695] group_w3 strides: (2883584, 2048, 1), is_contiguous: True
DEBUG 12-24 11:28:14.080362.080362 mlpmodule.py:700] group_w3 first element: -0.000606536865234375
WARNING 12-24 11:28:14.080035.080035 mlpmodule.py:710] start einsum2
WARNING 12-24 11:28:14.085239.085239 mlpmodule.py:715] intermediate
WARNING 12-24 11:28:14.085453.085453 mlpmodule.py:719] start einsum3
DEBUG 12-24 11:28:14.090327.090327 mlpmodule.py:723] group einsum cost 0.014923810958862305 s
DEBUG 12-24 11:28:14.090376.090376 mlpmodule.py:731] cpy2cputensor cost 0.00012969970703125 s
DEBUG 12-24 11:28:14.108206.108206 mlpmodule.py:588]  experts func einsum cost 0.05265474319458008 s
INFO 12-24 11:28:14.109565.109565 client.py:127] Model loaded
DEBUG 12-24 11:28:14.109104.109104 cuda_h.py:19] end wait_experts cost 0.035486459732055664 seconds
DEBUG 12-24 11:28:14.109284.109284 cuda_h.py:10] start gpu_experts
DEBUG 12-24 11:28:14.109894.109894 lmp.py:585]   Computing 32 experts on GPU...
DEBUG 12-24 11:28:14.110080.110080 mlpmodule.py:457] gpu group tensors cost 0.0005216598510742188 s
DEBUG 12-24 11:28:14.111393.111393 mlpmodule.py:490] gpu pad cost 0.0014107227325439453 s
DEBUG 12-24 11:28:14.112599.112599 mlpmodule.py:508] gpu group einsum cost 0.0005433559417724609 s
DEBUG 12-24 11:28:14.115045.115045 mlpmodule.py:537] gpu experts func einsum cost 0.005410194396972656 s
DEBUG 12-24 11:28:14.115565.115565 cuda_h.py:19] end gpu_experts cost 0.0055887699127197266 seconds
DEBUG 12-24 11:28:14.115367.115367 cuda_h.py:10] start wait_cetm_experts
DEBUG 12-24 11:28:14.115236.115236 cuda_h.py:19] end wait_cetm_experts cost 1.430511474609375e-05 seconds
DEBUG 12-24 11:28:14.115508.115508 lmp.py:615] gpu end - einsum end = 20.1ms
DEBUG 12-24 11:28:14.115888.115888 cuda_h.py:10] start wait_load_qkvogn_s_weight
DEBUG 12-24 11:28:14.115658.115658 cuda_h.py:19] end wait_load_qkvogn_s_weight cost 9.298324584960938e-06 seconds
DEBUG 12-24 11:28:14.115215.115215 cuda_h.py:19] end layer_moe_generate_27 cost 0.06370019912719727 seconds
DEBUG 12-24 11:28:14.115611.115611 lmp.py:445] -------------------------------- end layer 27 --------------------------------
DEBUG 12-24 11:28:14.115612.115612 cuda_h.py:19] end multi_layer cost 2.271243095397949 seconds
Collecting data...
Generating '/tmp/nsys-report-6687.qdstrm'
[1/1] [0%                          ] report1.nsys-rep[1/1] [0%                          ] report1.nsys-rep[1/1] [7%                          ] report1.nsys-rep[1/1] [11%                         ] report1.nsys-rep[1/1] [==18%                       ] report1.nsys-rep[1/1] [====26%                     ] report1.nsys-rep[1/1] [======34%                   ] report1.nsys-rep[1/1] [========42%                 ] report1.nsys-rep[1/1] [===========50%              ] report1.nsys-rep[1/1] [========================100%] report1.nsys-rep[1/1] [========================100%] report1.nsys-rep
Generated:
	/mnt/zhengcf3/lmp/examples/report1.nsys-rep
