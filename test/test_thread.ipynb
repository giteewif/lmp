{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2f15d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:02<00:00,  7.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import MixtralForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# 加载 Mixtral 模型（只加载一个 expert）\n",
    "#Mixtral-8x22B-Instruct-v0.1\n",
    "model_path = \"/mnt/zhengcf3/models/Mixtral-8x7B\"\n",
    "print(\"正在加载模型...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",  # 加载到 CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9dcda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerComputeThread 类已定义，可以使用 create_layer_workers() 创建线程\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class LayerComputeThread:\n",
    "    \"\"\"独立的layer计算线程，使用独立的CUDA流\"\"\"\n",
    "    \n",
    "    def __init__(self, layer, device: str, thread_id: int):\n",
    "        self.layer = layer\n",
    "        self.device = device\n",
    "        self.thread_id = thread_id\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        \n",
    "        # 创建独立的CUDA流\n",
    "        self.stream = torch.cuda.Stream(device=device)\n",
    "        \n",
    "        # 输入队列：接收待处理的任务 (task_id, inputs)\n",
    "        self.input_queue = queue.Queue()\n",
    "        \n",
    "        # 输出队列：返回处理结果 (task_id, outputs)\n",
    "        self.output_queue = queue.Queue()\n",
    "        \n",
    "    def _worker(self):\n",
    "        \"\"\"工作线程主循环\"\"\"\n",
    "        print(f\"线程 {self.thread_id} (设备 {self.device}) 启动，CUDA流已创建\")\n",
    "        \n",
    "        while self.running:\n",
    "            task = None\n",
    "            task_id = None\n",
    "            # 从队列获取任务，超时1秒\n",
    "            task = self.input_queue.get()\n",
    "            \n",
    "            if task is None:  # 停止信号\n",
    "                break\n",
    "            \n",
    "\n",
    "            task_id, inputs, next_device = task\n",
    "\n",
    "            \n",
    "            # 在独立的CUDA流上执行计算\n",
    "            outputs = None\n",
    "\n",
    "            with torch.cuda.stream(self.stream):\n",
    "                time_start_stream = time.time()\n",
    "                # 执行layer计算\n",
    "                outputs, _ = self.layer.block_sparse_moe(inputs)\n",
    "                time_start_move = time.time()\n",
    "                outputs = outputs.to(next_device, non_blocking=False)\n",
    "                time_end_move = time.time()\n",
    "                self.output_queue.put((task_id, outputs))\n",
    "                time_end_stream = time.time()\n",
    "                print(f\"线程 {self.thread_id} (设备 {self.device}) 流计算时间: {time_end_stream - time_start_stream:.6f}s 移动时间: {time_end_move - time_start_move:.6f}s\")\n",
    "                \n",
    "    def start(self):\n",
    "        \"\"\"启动线程\"\"\"\n",
    "        if self.running:\n",
    "            return\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._worker, daemon=True)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"停止线程\"\"\"\n",
    "        if not self.running:\n",
    "            return\n",
    "        self.running = False\n",
    "        self.input_queue.put(None)  # 发送停止信号\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=5.0)\n",
    "        # 清理CUDA缓存\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def submit(self, task_id: int, inputs: torch.Tensor, next_device: str) -> None:\n",
    "        \"\"\"提交任务到队列\"\"\"\n",
    "        self.input_queue.put((task_id, inputs, next_device))\n",
    "    \n",
    "    def get_result(self, timeout: Optional[float] = None) -> Tuple[int, Optional[torch.Tensor]]:\n",
    "        \"\"\"从输出队列获取结果\"\"\"\n",
    "        return self.output_queue.get(timeout=timeout)\n",
    "    \n",
    "    def has_result(self) -> bool:\n",
    "        \"\"\"检查是否有结果可用\"\"\"\n",
    "        return not self.output_queue.empty()\n",
    "\n",
    "\n",
    "# 使用示例和测试函数\n",
    "def create_layer_workers(layer1, layer2, device1: str, device2: str):\n",
    "    \"\"\"创建两个layer计算线程\"\"\"\n",
    "    worker1 = LayerComputeThread(layer1, device1, thread_id=1)\n",
    "    worker2 = LayerComputeThread(layer2, device2, thread_id=2)\n",
    "    \n",
    "    worker1.start()\n",
    "    worker2.start()\n",
    "    \n",
    "    return worker1, worker2\n",
    "\n",
    "def stop_layer_workers(worker1, worker2):\n",
    "    \"\"\"停止两个线程\"\"\"\n",
    "    worker1.stop()\n",
    "    worker2.stop()\n",
    "\n",
    "print(\"LayerComputeThread 类已定义，可以使用 create_layer_workers() 创建线程\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf8ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建多线程计算系统...\n",
      "线程 1 (设备 cuda:1) 启动，CUDA流已创建\n",
      "线程 2 (设备 cuda:2) 启动，CUDA流已创建\n",
      "\n",
      "开始测试多线程计算...\n",
      "GPU内存状态 - Device1: 3.47 GB, Device2: 2.72 GB\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.450553s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.135839s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.171958s\n",
      "任务 0: 完成，耗时 0.843655s\n",
      "  GPU内存 - Device1: 11.99 GB, Device2: 2.97 GB\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.181164s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.260222s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.172610s\n",
      "任务 1: 完成，耗时 0.616037s\n",
      "  GPU内存 - Device1: 11.99 GB, Device2: 2.97 GB\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.173243s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.172922s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.173233s\n",
      "任务 2: 完成，耗时 0.521625s\n",
      "  GPU内存 - Device1: 11.99 GB, Device2: 2.97 GB\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.172616s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.265846s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.172649s\n",
      "任务 3: 完成，耗时 0.613066s\n",
      "  GPU内存 - Device1: 11.99 GB, Device2: 2.97 GB\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.172956s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.172574s\n",
      "提交任务到worker1\n",
      "线程 1 (设备 cuda:1) 流计算时间: 0.173621s\n",
      "任务 4: 完成，耗时 0.521373s\n",
      "  GPU内存 - Device1: 11.99 GB, Device2: 2.97 GB\n",
      "\n",
      "总时间: 3.250486 秒\n",
      "每次时间: [0.843655, 0.616037, 0.521625, 0.613066, 0.521373]\n",
      "平均时间: 0.623151 秒\n",
      "\n",
      "最终GPU内存状态:\n",
      "  Device1: 11.99 GB / 20.76 GB (峰值)\n",
      "  Device2: 2.97 GB / 3.22 GB (峰值)\n",
      "\n",
      "停止线程...\n",
      "所有线程已停止\n",
      "内存清理完成\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = 5\n",
    "h1 = 14336\n",
    "h2 = 4096\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "layer1 = model.model.layers[1]\n",
    "layer2 = model.model.layers[2]\n",
    "layer1 = layer1.to(device1)\n",
    "layer2 = layer2.to(device2)\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 512\n",
    "\n",
    "inputsb0 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "inputsb1 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "inputsb2 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "\n",
    "inputs_list = [inputsb0, inputsb1, inputsb2]\n",
    "# 定义layer计算函数（用于线程中）\n",
    "def layer_cal(layer, inputs):\n",
    "    bmoe = layer.block_sparse_moe\n",
    "    out, _ = bmoe(inputs)\n",
    "    return out\n",
    "\n",
    "# 创建包装类，使layer可以被线程调用\n",
    "class LayerWrapper:\n",
    "    def __init__(self, layer):\n",
    "        self.layer = layer\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        return layer_cal(self.layer, inputs)\n",
    "\n",
    "# 创建两个独立的计算线程\n",
    "print(\"创建多线程计算系统...\")\n",
    "worker1 = LayerComputeThread(layer1, device1, thread_id=1)\n",
    "worker2 = LayerComputeThread(layer2, device2, thread_id=2)\n",
    "\n",
    "worker1.start()\n",
    "worker2.start()\n",
    "\n",
    "# 等待线程启动\n",
    "time.sleep(0.5)\n",
    "\n",
    "# 测试：提交任务并获取结果\n",
    "print(\"\\n开始测试多线程计算...\")\n",
    "print(f\"GPU内存状态 - Device1: {torch.cuda.memory_allocated(device1)/1024**3:.2f} GB, Device2: {torch.cuda.memory_allocated(device2)/1024**3:.2f} GB\")\n",
    "\n",
    "# 清理CUDA缓存\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "times_list = []\n",
    "time_start = time.time()\n",
    "\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    \n",
    "    # 处理每个输入\n",
    "    results = []\n",
    "    for idx, inputs in enumerate(inputs_list):\n",
    "        # 提交任务到worker1\n",
    "        print(f\"提交任务到worker1\")\n",
    "        worker1.submit(task_id=i*len(inputs_list)*2 + idx*2, inputs=inputs, next_device=device2)\n",
    "        task_id1, result1 = worker1.get_result()\n",
    "        \n",
    "        if result1 is None:\n",
    "            print(f\"任务 {i}-{idx}: worker1 失败\")\n",
    "            break\n",
    "        \n",
    "        # 提交任务到worker2\n",
    "\n",
    "        # worker2.submit(task_id=i*len(inputs_list)*2 + idx*2 + 1, inputs=result1, next_device=device1)\n",
    "        # task_id2, result2 = worker2.get_result()\n",
    "        \n",
    "        # if result2 is None:\n",
    "        #     print(f\"任务 {i}-{idx}: worker2 失败\")\n",
    "        #     break\n",
    "        \n",
    "        # results.append(result1)\n",
    "        \n",
    "        # # 清理中间结果，释放内存\n",
    "        # del result1\n",
    "        # if idx > 0:  # 保留最后一个结果\n",
    "        #     del results[idx-1]\n",
    "    \n",
    "    # 同步所有设备\n",
    "    # torch.cuda.synchronize(device=device1)\n",
    "    # torch.cuda.synchronize(device=device2)\n",
    " \n",
    "    # 定期清理CUDA缓存\n",
    "    # torch.cuda.empty_cache()\n",
    "    \n",
    "    time_end_once = time.time()\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "    \n",
    "    print(f\"任务 {i}: 完成，耗时 {times_list[-1]:.6f}s\")\n",
    "    print(f\"  GPU内存 - Device1: {torch.cuda.memory_allocated(device1)/1024**3:.2f} GB, Device2: {torch.cuda.memory_allocated(device2)/1024**3:.2f} GB\")\n",
    "torch.cuda.empty_cache()\n",
    "time_end = time.time()\n",
    "print(f\"\\n总时间: {time_end - time_start:.6f} 秒\")\n",
    "print(f\"每次时间: {times_list}\")\n",
    "print(f\"平均时间: {sum(times_list)/len(times_list):.6f} 秒\")\n",
    "\n",
    "# 最终内存状态\n",
    "print(f\"\\n最终GPU内存状态:\")\n",
    "print(f\"  Device1: {torch.cuda.memory_allocated(device1)/1024**3:.2f} GB / {torch.cuda.max_memory_allocated(device1)/1024**3:.2f} GB (峰值)\")\n",
    "print(f\"  Device2: {torch.cuda.memory_allocated(device2)/1024**3:.2f} GB / {torch.cuda.max_memory_allocated(device2)/1024**3:.2f} GB (峰值)\")\n",
    "\n",
    "# 清理所有中间变量\n",
    "del inputs_list\n",
    "if 'results' in locals():\n",
    "    del results\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 停止线程\n",
    "print(\"\\n停止线程...\")\n",
    "worker1.stop()\n",
    "worker2.stop()\n",
    "print(\"所有线程已停止\")\n",
    "torch.cuda.empty_cache()\n",
    "# 最终清理\n",
    "torch.cuda.empty_cache()\n",
    "print(\"内存清理完成\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
