{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69eaea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.17659449577331543 secondsEvery time [0.105594, 0.017791, 0.017636, 0.017633, 0.017664]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "times = 5\n",
    "# 14336 4096\n",
    "h1 = 16384\n",
    "h2 = 6144\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "times_list = []\n",
    "expert1 = torch.randn(h1, h2, dtype=dtype, device=device1)\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    out = expert1.to(device2)\n",
    "    torch.cuda.synchronize(device=device1)\n",
    "    torch.cuda.synchronize(device=device2)\n",
    "    time_end_once = time.time()\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start} seconds\"\n",
    "    f\"Every time {times_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30e833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.08314394950866699 secondsEvery time [0.016779, 0.016552, 0.016543, 0.016538, 0.016537]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "times = 5\n",
    "h1 = 16384\n",
    "h2 = 6144\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "\n",
    "expert1 = torch.randn(h1, h2, dtype=dtype, device=\"cpu\", pin_memory=True)\n",
    "\n",
    "times_list = []\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    expert1.to(device2)\n",
    "    time_end_once = time.time()\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start} seconds\"\n",
    "    f\"Every time {times_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb686f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:02<00:00,  9.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import MixtralForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# 加载 Mixtral 模型（只加载一个 expert）\n",
    "#Mixtral-8x22B-Instruct-v0.1\n",
    "model_path = \"/mnt/zhengcf3/models/Mixtral-8x7B\"\n",
    "print(\"正在加载模型...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",  # 加载到 CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "136b7eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "Time taken: 0.11647605895996094 secondsEvery time [0.031299, 0.021477, 0.020552, 0.020748, 0.021886]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "times = 5\n",
    "h1 = 14336\n",
    "h2 = 4096\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "layer1 = model.model.layers[1]\n",
    "layer2 = model.model.layers[2]\n",
    "layer1 = layer1.to(device1)\n",
    "layer2 = layer2.to(device2)\n",
    "\n",
    "layer3 = model.model.layers[3]\n",
    "\n",
    "layer1.eval()\n",
    "layer2.eval()\n",
    "batch_size = 64\n",
    "seq_len = 8\n",
    "\n",
    "inputs_cpu = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=\"cpu\")\n",
    "inputsb0 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "inputsb1 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "inputsb2 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "\n",
    "\n",
    "inputs_list = [inputsb0]\n",
    "\n",
    "w1 = torch.nn.Linear(h2, h1, bias=False, device=device1, dtype=dtype)\n",
    "w2 = torch.nn.Linear(h1, h2, bias=False, device=device1, dtype=dtype)\n",
    "w3 = torch.nn.Linear(h2, h1, bias=False, device=device1, dtype=dtype)\n",
    "act_fn = torch.nn.SiLU()\n",
    "\n",
    "def experts_cal(layer, inputs):\n",
    "    for i in range(1):\n",
    "        expert = layer.block_sparse_moe.experts[i]\n",
    "        # for expert in layer.block_sparse_moe.experts:\n",
    "        out = expert(inputs)\n",
    "        print(out.shape)\n",
    "    return out\n",
    "def layer_cal(layer, inputs):\n",
    "    bmoe = layer.block_sparse_moe\n",
    "    out, _ = bmoe(inputs)\n",
    "    return out\n",
    "\n",
    "def move(tensor, device):\n",
    "    return tensor.to(device)\n",
    "\n",
    "times_list = []\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    for input in inputs_list:\n",
    "        print(f\"提交任务到worker1\")\n",
    "        out = experts_cal(layer3, inputs_cpu)\n",
    "        torch.cuda.synchronize(device=device1)\n",
    "        \n",
    "    time_end_once = time.time()\n",
    "    del out\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start} seconds\"\n",
    "    f\"Every time {times_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e6054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_experts load time in Deepseek: 0.04655790328979492 seconds\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "k = torch.randn(1408, 2048, dtype=torch.bfloat16, device=\"cpu\", pin_memory=True)\n",
    "time_start_torch = time.time()\n",
    "for i in range(32):\n",
    "    for i in range(3):\n",
    "        k_g = k.to(\"cuda:1\")\n",
    "time_end_torch = time.time()\n",
    "\n",
    "print(f\"one_experts load time in Deepseek: {time_end_torch - time_start_torch} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37e3d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start load_config\n",
      "end load_config cost 0.003858327865600586\n",
      "start init_layers\n",
      "start init_2\n",
      "end init_2 cost 0.025965213775634766\n",
      "start copy_1\n",
      "end copy_1 cost 0.6324846744537354\n"
     ]
    }
   ],
   "source": [
    "import concurrent\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "timings = {}\n",
    "def time_hook(name):\n",
    "    start = time.time()\n",
    "    print(f\"start {name}\")\n",
    "    timings[name] = start\n",
    "\n",
    "def time_hook_end(name):\n",
    "    end = time.time()\n",
    "    \n",
    "    cost = end - timings[name]\n",
    "    print(f\"end {name} cost {cost}\")\n",
    "# 获取项目根目录和必要的路径\n",
    "project_root = \"/mnt/zhengcf3/lmp\"\n",
    "src_dir = os.path.join(project_root, 'src')\n",
    "sllm_store_dir = os.path.join(project_root, 'src', 'sllm_store')\n",
    "\n",
    "# 添加必要的目录到 Python 路径\n",
    "# 1. 添加 sllm_store 目录（必须在 src 之前，这样 sllm_store 可以被找到）\n",
    "sys.path.insert(0, sllm_store_dir)\n",
    "# 2. 添加 src 目录（用于导入 lmp, utils 等）\n",
    "sys.path.insert(0, src_dir)\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "from models.Deepseek.deepseek_moe_16b_base.modeling_deepseek import DeepseekDecoderLayer\n",
    "from accelerate import init_empty_weights\n",
    "path = \"/mnt/zhengcf3/lmp/src/models/Deepseek/deepseek_moe_16b_base\"\n",
    "time_hook(\"load_config\")\n",
    "config = AutoConfig.from_pretrained(path , trust_remote_code=True)\n",
    "config._attn_implementation = \"sdpa\"\n",
    "time_hook_end(\"load_config\")\n",
    "\n",
    "time_hook(\"init_layers\")\n",
    "layers = []\n",
    "\n",
    "\n",
    "\n",
    "def init_layers(layer_idx):\n",
    "    with init_empty_weights():\n",
    "        for i in range(28):\n",
    "            layer_c = DeepseekDecoderLayer(config, i)\n",
    "            layer_h = copy.deepcopy(layer_c)  # 深拷贝\n",
    "            layers.append(layer_h)\n",
    "            layers.append(layer_c)\n",
    "def init_layer(layer_idx):\n",
    "    with init_empty_weights():\n",
    "        layer_c = DeepseekDecoderLayer(config, layer_idx)\n",
    "        layers.append(layer_c)\n",
    "\n",
    "time_hook(\"init_2\")\n",
    "with init_empty_weights():\n",
    "    layer_cc = DeepseekDecoderLayer(config, 2)\n",
    "time_hook_end(\"init_2\")\n",
    "\n",
    "def copy_c():\n",
    "    layer_h = copy.deepcopy(layer_cc)  # 深拷贝\n",
    "    layers.append(layer_h)\n",
    "\n",
    "time_hook(\"copy_1\")\n",
    "for i in range(27):\n",
    "    copy_c()\n",
    "time_hook_end(\"copy_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556a9944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型...\n",
      "加载模型时间: 2.415902614593506 秒\n",
      "深拷贝模型时间: [0.8905317783355713, 0.5729832649230957, 0.871990442276001, 0.897723913192749] 秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import MixtralForCausalLM, AutoModelForCausalLM\n",
    "import copy\n",
    "from accelerate import init_empty_weights\n",
    "from models.Deepseek.deepseek_moe_16b_base.modeling_deepseek import DeepseekForCausalLM, DeepseekRMSNorm\n",
    "# 加载 Mixtral 模型（只加载一个 expert）\n",
    "#Mixtral-8x22B-Instruct-v0.1\n",
    "model_path = \"/mnt/zhengcf3/models/deepseek-moe-16b-base\"\n",
    "print(\"正在加载模型...\")\n",
    "time_start = time.time()\n",
    "with init_empty_weights():\n",
    "    model = DeepseekForCausalLM(config)\n",
    "print(f\"加载模型时间: {time.time() - time_start} 秒\")\n",
    "\n",
    "time_list = []\n",
    "for i in range(4):\n",
    "    time_start=time.time()\n",
    "    model_cpy = copy.deepcopy(model)\n",
    "    time_list.append(time.time() - time_start)\n",
    "print(f\"深拷贝模型时间: {time_list} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fe215fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 01-05 17:07:53.380369.380369 cuda_memory_view.py:260] \n",
      "DEBUG 01-05 17:07:53.380369.380369 cuda_memory_view.py:260] restore_tensors_from_shared_memory_names time: 0.01531219482421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 01-05 17:07:55.571406.571406 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu\n",
      "DEBUG 01-05 17:07:55.573758.573758 cuda_h.py:10] start allocate_cuda_memory\n",
      "DEBUG 01-05 17:07:55.575225.575225 cuda_h.py:19] end allocate_cuda_memory cost 0.0005710124969482422 seconds\n",
      "DEBUG 01-05 17:07:55.576239.576239 cuda_h.py:10] start load_into_gpu_async\n",
      "DEBUG 01-05 17:07:55.577194.577194 sllm_store_c.py:27] get device uuid map\n",
      "DEBUG 01-05 17:07:55.578105.578105 sllm_store_c.py:29] call client load into gpu\n",
      "DEBUG 01-05 17:07:55.578336.578336 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, ae6739da-a811-45a6-bf1e-d2a8fcc8334e\n",
      "DEBUG 01-05 17:07:55.579325.579325 client.py:106] call stub.LoadModelAsync\n",
      "INFO 01-05 17:07:55.581081.581081 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, ae6739da-a811-45a6-bf1e-d2a8fcc8334e\n",
      "DEBUG 01-05 17:07:55.582140.582140 cuda_h.py:19] end load_into_gpu_async cost 0.0055119991302490234 seconds\n",
      "DEBUG 01-05 17:07:55.583342.583342 cuda_h.py:10] start restore_tensors2\n",
      "DEBUG 01-05 17:07:55.584724.584724 cuda_h.py:19] end restore_tensors2 cost 8.034706115722656e-05 seconds\n",
      "DEBUG 01-05 17:07:55.584368.584368 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.011123180389404297 seconds\n",
      "INFO 01-05 17:07:55.586787.586787 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, ae6739da-a811-45a6-bf1e-d2a8fcc8334e\n",
      "INFO 01-05 17:07:55.657578.657578 client.py:127] Model loaded\n",
      "DEBUG 01-05 17:07:55.658770.658770 cuda_h.py:10] start load_qkvogns_weight_l_0\n",
      "DEBUG 01-05 17:07:55.659372.659372 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu\n",
      "DEBUG 01-05 17:07:55.660230.660230 cuda_h.py:10] start allocate_cuda_memory\n",
      "DEBUG 01-05 17:07:55.662192.662192 cuda_h.py:19] end allocate_cuda_memory cost 0.0003712177276611328 seconds\n",
      "DEBUG 01-05 17:07:55.662092.662092 cuda_h.py:10] start load_into_gpu_async\n",
      "DEBUG 01-05 17:07:55.663032.663032 sllm_store_c.py:27] get device uuid map\n",
      "DEBUG 01-05 17:07:55.664611.664611 sllm_store_c.py:29] call client load into gpu\n",
      "DEBUG 01-05 17:07:55.665713.665713 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, a1ec9200-6882-48e5-81b1-22c7deb94110\n",
      "DEBUG 01-05 17:07:55.666425.666425 client.py:106] call stub.LoadModelAsync\n",
      "INFO 01-05 17:07:55.669072.669072 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, a1ec9200-6882-48e5-81b1-22c7deb94110\n",
      "DEBUG 01-05 17:07:55.670797.670797 cuda_h.py:19] end load_into_gpu_async cost 0.006424427032470703 seconds\n",
      "DEBUG 01-05 17:07:55.670419.670419 cuda_h.py:10] start restore_tensors2\n",
      "DEBUG 01-05 17:07:55.671846.671846 cuda_h.py:19] end restore_tensors2 cost 0.00012254714965820312 seconds\n",
      "DEBUG 01-05 17:07:55.672217.672217 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.011301040649414062 seconds\n",
      "INFO 01-05 17:07:55.672521.672521 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, a1ec9200-6882-48e5-81b1-22c7deb94110\n",
      "INFO 01-05 17:07:55.685450.685450 client.py:127] Model loaded\n",
      "DEBUG 01-05 17:07:55.687000.687000 cuda_h.py:19] end load_qkvogns_weight_l_0 cost 0.027875185012817383 seconds\n",
      "DEBUG 01-05 17:07:55.688999.688999 cuda_h.py:10] start start_load_qkvogn_s_weight_l_1\n",
      "DEBUG 01-05 17:07:55.689393.689393 cuda_h.py:19] end start_load_qkvogn_s_weight_l_1 cost 8.273124694824219e-05 seconds\n",
      "DEBUG 01-05 17:07:55.689103.689103 cuda_h.py:10] start sllm_worker_task\n",
      "DEBUG 01-05 17:07:55.692508.692508 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu\n",
      "DEBUG 01-05 17:07:55.692424.692424 cuda_h.py:10] start allocate_cuda_memory\n",
      "DEBUG 01-05 17:07:55.694002.694002 cuda_h.py:19] end allocate_cuda_memory cost 0.0003352165222167969 seconds\n",
      "DEBUG 01-05 17:07:55.695678.695678 cuda_h.py:10] start load_into_gpu_async\n",
      "DEBUG 01-05 17:07:55.696717.696717 sllm_store_c.py:27] get device uuid map\n",
      "DEBUG 01-05 17:07:55.696763.696763 sllm_store_c.py:29] call client load into gpu\n",
      "DEBUG 01-05 17:07:55.697813.697813 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, dd68dce2-375d-449c-9dc2-a89176c331ac\n",
      "DEBUG 01-05 17:07:55.698158.698158 client.py:106] call stub.LoadModelAsync\n",
      "INFO 01-05 17:07:55.701857.701857 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, dd68dce2-375d-449c-9dc2-a89176c331ac\n",
      "DEBUG 01-05 17:07:55.702797.702797 cuda_h.py:19] end load_into_gpu_async cost 0.005955934524536133 seconds\n",
      "DEBUG 01-05 17:07:55.702206.702206 cuda_h.py:10] start restore_tensors2\n",
      "DEBUG 01-05 17:07:55.703364.703364 cuda_h.py:19] end restore_tensors2 cost 0.000225067138671875 seconds\n",
      "DEBUG 01-05 17:07:55.703032.703032 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.0110626220703125 seconds\n",
      "INFO 01-05 17:07:55.705495.705495 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, dd68dce2-375d-449c-9dc2-a89176c331ac\n",
      "INFO 01-05 17:07:55.709790.709790 client.py:127] Model loaded\n",
      "DEBUG 01-05 17:07:55.710592.710592 cuda_h.py:19] end sllm_worker_task cost 0.01832866668701172 seconds\n",
      "DEBUG 01-05 17:07:55.712800.712800 cuda_h.py:10] start allocate_cuda_memory_and_load_into_gpu\n",
      "DEBUG 01-05 17:07:55.713868.713868 cuda_h.py:10] start allocate_cuda_memory\n",
      "DEBUG 01-05 17:07:55.714684.714684 cuda_h.py:19] end allocate_cuda_memory cost 0.0004725456237792969 seconds\n",
      "DEBUG 01-05 17:07:55.715317.715317 cuda_h.py:10] start load_into_gpu_async\n",
      "DEBUG 01-05 17:07:55.716305.716305 sllm_store_c.py:27] get device uuid map\n",
      "DEBUG 01-05 17:07:55.717844.717844 sllm_store_c.py:29] call client load into gpu\n",
      "DEBUG 01-05 17:07:55.718816.718816 client.py:72] load_into_gpu: deepseek-moe-16b-base-bfloat16, f38d7d8c-59c9-42f2-9f83-9a73e72d7b88\n",
      "DEBUG 01-05 17:07:55.719784.719784 client.py:106] call stub.LoadModelAsync\n",
      "INFO 01-05 17:07:55.721154.721154 client.py:115] Model loaded: deepseek-moe-16b-base-bfloat16, f38d7d8c-59c9-42f2-9f83-9a73e72d7b88\n",
      "DEBUG 01-05 17:07:55.722236.722236 cuda_h.py:19] end load_into_gpu_async cost 0.006247282028198242 seconds\n",
      "DEBUG 01-05 17:07:55.723407.723407 cuda_h.py:10] start restore_tensors2\n",
      "DEBUG 01-05 17:07:55.724404.724404 cuda_h.py:19] end restore_tensors2 cost 0.0005676746368408203 seconds\n",
      "DEBUG 01-05 17:07:55.725817.725817 cuda_h.py:19] end allocate_cuda_memory_and_load_into_gpu cost 0.012323617935180664 seconds\n",
      "INFO 01-05 17:07:55.731737.731737 client.py:119] confirm_model_loaded: deepseek-moe-16b-base-bfloat16, f38d7d8c-59c9-42f2-9f83-9a73e72d7b88\n",
      "INFO 01-05 17:07:55.763036.763036 client.py:127] Model loaded\n",
      "start layer_1\n",
      "end layer_1 cost 0.022078752517700195\n",
      "start layer_1\n",
      "end layer_1 cost 0.011878252029418945\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import MixtralForCausalLM, AutoModelForCausalLM\n",
    "import copy\n",
    "from accelerate import init_empty_weights\n",
    "from models.Deepseek.deepseek_moe_16b_base.modeling_deepseek import DeepseekForCausalLM, DeepseekRMSNorm\n",
    "\n",
    "timings = {}\n",
    "def time_hook(name):\n",
    "    start = time.time()\n",
    "    print(f\"start {name}\")\n",
    "    timings[name] = start\n",
    "\n",
    "def time_hook_end(name):\n",
    "    end = time.time()\n",
    "    \n",
    "    cost = end - timings[name]\n",
    "    print(f\"end {name} cost {cost}\")\n",
    "# 获取项目根目录和必要的路径\n",
    "project_root = \"/mnt/zhengcf3/lmp\"\n",
    "src_dir = os.path.join(project_root, 'src')\n",
    "sllm_store_dir = os.path.join(project_root, 'src', 'sllm_store')\n",
    "\n",
    "# 添加必要的目录到 Python 路径\n",
    "# 1. 添加 sllm_store 目录（必须在 src 之前，这样 sllm_store 可以被找到）\n",
    "sys.path.insert(0, sllm_store_dir)\n",
    "# 2. 添加 src 目录（用于导入 lmp, utils 等）\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "from lmp.lmp import MLPLLM\n",
    "# 加载 Mixtral 模型（只加载一个 expert）\n",
    "#Mixtral-8x22B-Instruct-v0.1\n",
    "model_path = \"deepseek-moe-16b-base-bfloat16\"\n",
    "model_name_type = \"Deepseek\"\n",
    "mlpllm = MLPLLM( model_name_type=model_name_type, model_path=model_path )\n",
    "device1 = \"cuda:1\"\n",
    "device1_index = int(device1.split(\":\")[1])\n",
    "\n",
    "mlpllm.cmv.start_init_meta_model(hmv=mlpllm.hmv)\n",
    "mlpllm.cmv.load_general_and_init()\n",
    "mlpllm.cmv.init_load_qkvogn_es_weight(layer_idx=0)\n",
    "\n",
    "layer_idx = 1\n",
    "mlpllm.cmv.start_load_qkvogn_s_weight(layer_idx=1, device=device1)\n",
    "mlpllm.cmv.wait_load_qkvogn_s_weight(layer_idx=1)\n",
    "\n",
    "gpu_expert_ids = [i for i in range(mlpllm.mlpm.config.num_hidden_layers)]\n",
    "gpu_expert_names = mlpllm.mlpm.get_experts_names(layer_idx=1, expert_idx_list=list(gpu_expert_ids))\n",
    "gpu_expert_names = gpu_expert_names\n",
    "ret1, replica_uuid1, state_dict1 = \\\n",
    "    mlpllm.cmv.allocate_cuda_memory_and_load_into_gpu(\n",
    "    gpu_expert_names, device_index_int=device1_index)\n",
    "mlpllm.cmv.restore2model(state_dict1, mlpllm.cmv.mlpm_ci)\n",
    "mlpllm.cmv.wait_load_into_gpu(replica_uuid1)\n",
    "\n",
    "input_hidden_states = torch.randn(32, 1, 2048, dtype=torch.bfloat16, device=device1)\n",
    "\n",
    "layer = mlpllm.cmv.mlpm_ci.model.layers[layer_idx]\n",
    "time_hook(\"layer_1\")\n",
    "out = layer.mlp(input_hidden_states)\n",
    "time_hook_end(\"layer_1\")\n",
    "\n",
    "time_hook(\"layer_1\")\n",
    "out = layer.mlp(input_hidden_states)\n",
    "time_hook_end(\"layer_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
