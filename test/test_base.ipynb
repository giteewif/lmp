{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69eaea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.17659449577331543 secondsEvery time [0.105594, 0.017791, 0.017636, 0.017633, 0.017664]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "times = 5\n",
    "# 14336 4096\n",
    "h1 = 16384\n",
    "h2 = 6144\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "times_list = []\n",
    "expert1 = torch.randn(h1, h2, dtype=dtype, device=device1)\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    out = expert1.to(device2)\n",
    "    torch.cuda.synchronize(device=device1)\n",
    "    torch.cuda.synchronize(device=device2)\n",
    "    time_end_once = time.time()\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start} seconds\"\n",
    "    f\"Every time {times_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30e833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.08314394950866699 secondsEvery time [0.016779, 0.016552, 0.016543, 0.016538, 0.016537]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "times = 5\n",
    "h1 = 16384\n",
    "h2 = 6144\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "\n",
    "expert1 = torch.randn(h1, h2, dtype=dtype, device=\"cpu\", pin_memory=True)\n",
    "\n",
    "times_list = []\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    expert1.to(device2)\n",
    "    time_end_once = time.time()\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start} seconds\"\n",
    "    f\"Every time {times_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb686f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:02<00:00,  9.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import MixtralForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# 加载 Mixtral 模型（只加载一个 expert）\n",
    "#Mixtral-8x22B-Instruct-v0.1\n",
    "model_path = \"/mnt/zhengcf3/models/Mixtral-8x7B\"\n",
    "print(\"正在加载模型...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",  # 加载到 CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "136b7eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "Time taken: 0.11647605895996094 secondsEvery time [0.031299, 0.021477, 0.020552, 0.020748, 0.021886]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "times = 5\n",
    "h1 = 14336\n",
    "h2 = 4096\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "layer1 = model.model.layers[1]\n",
    "layer2 = model.model.layers[2]\n",
    "layer1 = layer1.to(device1)\n",
    "layer2 = layer2.to(device2)\n",
    "\n",
    "layer3 = model.model.layers[3]\n",
    "\n",
    "layer1.eval()\n",
    "layer2.eval()\n",
    "batch_size = 64\n",
    "seq_len = 8\n",
    "\n",
    "inputs_cpu = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=\"cpu\")\n",
    "inputsb0 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "inputsb1 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "inputsb2 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "\n",
    "\n",
    "inputs_list = [inputsb0]\n",
    "\n",
    "w1 = torch.nn.Linear(h2, h1, bias=False, device=device1, dtype=dtype)\n",
    "w2 = torch.nn.Linear(h1, h2, bias=False, device=device1, dtype=dtype)\n",
    "w3 = torch.nn.Linear(h2, h1, bias=False, device=device1, dtype=dtype)\n",
    "act_fn = torch.nn.SiLU()\n",
    "\n",
    "def experts_cal(layer, inputs):\n",
    "    for i in range(1):\n",
    "        expert = layer.block_sparse_moe.experts[i]\n",
    "        # for expert in layer.block_sparse_moe.experts:\n",
    "        out = expert(inputs)\n",
    "        print(out.shape)\n",
    "    return out\n",
    "def layer_cal(layer, inputs):\n",
    "    bmoe = layer.block_sparse_moe\n",
    "    out, _ = bmoe(inputs)\n",
    "    return out\n",
    "\n",
    "def move(tensor, device):\n",
    "    return tensor.to(device)\n",
    "\n",
    "times_list = []\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    for input in inputs_list:\n",
    "        print(f\"提交任务到worker1\")\n",
    "        out = experts_cal(layer3, inputs_cpu)\n",
    "        torch.cuda.synchronize(device=device1)\n",
    "        \n",
    "    time_end_once = time.time()\n",
    "    del out\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start} seconds\"\n",
    "    f\"Every time {times_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocate torch tensor time: 0.0009953975677490234 seconds\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "k = torch.randn(1408, 2048, dtype=torch.bfloat16, device=\"cpu\", pin_memory=True)\n",
    "time_start_torch = time.time()\n",
    "for i in range(3):\n",
    "    k_g = k.to(\"cuda:1\")\n",
    "time_end_torch = time.time()\n",
    "\n",
    "print(f\"allocate torch tensor time: {time_end_torch - time_start_torch} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
