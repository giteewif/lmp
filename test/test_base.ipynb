{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69eaea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.17659449577331543 secondsEvery time [0.105594, 0.017791, 0.017636, 0.017633, 0.017664]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "times = 5\n",
    "# 14336 4096\n",
    "h1 = 16384\n",
    "h2 = 6144\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "times_list = []\n",
    "expert1 = torch.randn(h1, h2, dtype=dtype, device=device1)\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    out = expert1.to(device2)\n",
    "    torch.cuda.synchronize(device=device1)\n",
    "    torch.cuda.synchronize(device=device2)\n",
    "    time_end_once = time.time()\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start} seconds\"\n",
    "    f\"Every time {times_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30e833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.08314394950866699 secondsEvery time [0.016779, 0.016552, 0.016543, 0.016538, 0.016537]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "times = 5\n",
    "h1 = 16384\n",
    "h2 = 6144\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "\n",
    "expert1 = torch.randn(h1, h2, dtype=dtype, device=\"cpu\", pin_memory=True)\n",
    "\n",
    "times_list = []\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    expert1.to(device2)\n",
    "    time_end_once = time.time()\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start} seconds\"\n",
    "    f\"Every time {times_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbb686f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/mnt/zhengcf3/env/lmp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:02<00:00,  9.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import MixtralForCausalLM, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# 加载 Mixtral 模型（只加载一个 expert）\n",
    "#Mixtral-8x22B-Instruct-v0.1\n",
    "model_path = \"/mnt/zhengcf3/models/Mixtral-8x7B\"\n",
    "print(\"正在加载模型...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cpu\",  # 加载到 CPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "136b7eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "提交任务到worker1\n",
      "torch.Size([64, 8, 4096])\n",
      "Time taken: 0.11647605895996094 secondsEvery time [0.031299, 0.021477, 0.020552, 0.020748, 0.021886]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "times = 5\n",
    "h1 = 14336\n",
    "h2 = 4096\n",
    "dtype=torch.bfloat16\n",
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "device3 = \"cuda:3\"\n",
    "\n",
    "layer1 = model.model.layers[1]\n",
    "layer2 = model.model.layers[2]\n",
    "layer1 = layer1.to(device1)\n",
    "layer2 = layer2.to(device2)\n",
    "\n",
    "layer3 = model.model.layers[3]\n",
    "\n",
    "layer1.eval()\n",
    "layer2.eval()\n",
    "batch_size = 64\n",
    "seq_len = 8\n",
    "\n",
    "inputs_cpu = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=\"cpu\")\n",
    "inputsb0 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "inputsb1 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "inputsb2 = torch.randn(batch_size, seq_len, h2, dtype=dtype, device=device1)\n",
    "\n",
    "\n",
    "inputs_list = [inputsb0]\n",
    "\n",
    "w1 = torch.nn.Linear(h2, h1, bias=False, device=device1, dtype=dtype)\n",
    "w2 = torch.nn.Linear(h1, h2, bias=False, device=device1, dtype=dtype)\n",
    "w3 = torch.nn.Linear(h2, h1, bias=False, device=device1, dtype=dtype)\n",
    "act_fn = torch.nn.SiLU()\n",
    "\n",
    "def experts_cal(layer, inputs):\n",
    "    for i in range(1):\n",
    "        expert = layer.block_sparse_moe.experts[i]\n",
    "        # for expert in layer.block_sparse_moe.experts:\n",
    "        out = expert(inputs)\n",
    "        print(out.shape)\n",
    "    return out\n",
    "def layer_cal(layer, inputs):\n",
    "    bmoe = layer.block_sparse_moe\n",
    "    out, _ = bmoe(inputs)\n",
    "    return out\n",
    "\n",
    "def move(tensor, device):\n",
    "    return tensor.to(device)\n",
    "\n",
    "times_list = []\n",
    "time_start = time.time()\n",
    "for i in range(times):\n",
    "    time_start_once = time.time()\n",
    "    for input in inputs_list:\n",
    "        print(f\"提交任务到worker1\")\n",
    "        out = experts_cal(layer3, inputs_cpu)\n",
    "        torch.cuda.synchronize(device=device1)\n",
    "        \n",
    "    time_end_once = time.time()\n",
    "    del out\n",
    "    times_list.append(round(time_end_once - time_start_once, 6))\n",
    "time_end = time.time()\n",
    "print(f\"Time taken: {time_end - time_start} seconds\"\n",
    "    f\"Every time {times_list}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b93f825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move time in Deepseek: 0.0007386207580566406 seconds\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "k = torch.randn(32, 64, 2048, dtype=torch.bfloat16, device=\"cuda:0\")\n",
    "time_start_torch = time.time()\n",
    "\n",
    "k_g = k.to(\"cuda:1\")\n",
    "k_t = k_g.to(\"cuda:0\")\n",
    "time_end_torch = time.time()\n",
    "\n",
    "print(f\"move time in Deepseek: {time_end_torch - time_start_torch} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e6054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_experts load time in Deepseek: 0.04655790328979492 seconds\n"
     ]
    }
   ],
   "source": [
    "import time, torch\n",
    "k = torch.randn(1408, 2048, dtype=torch.bfloat16, device=\"cpu\", pin_memory=True)\n",
    "time_start_torch = time.time()\n",
    "for i in range(32):\n",
    "    for i in range(3):\n",
    "        k_g = k.to(\"cuda:1\")\n",
    "time_end_torch = time.time()\n",
    "\n",
    "print(f\"one_experts load time in Deepseek: {time_end_torch - time_start_torch} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37e3d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start load_config\n",
      "end load_config cost 0.003858327865600586\n",
      "start init_layers\n",
      "start init_2\n",
      "end init_2 cost 0.025965213775634766\n",
      "start copy_1\n",
      "end copy_1 cost 0.6324846744537354\n"
     ]
    }
   ],
   "source": [
    "import concurrent\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "timings = {}\n",
    "def time_hook(name):\n",
    "    start = time.time()\n",
    "    print(f\"start {name}\")\n",
    "    timings[name] = start\n",
    "\n",
    "def time_hook_end(name):\n",
    "    end = time.time()\n",
    "    \n",
    "    cost = end - timings[name]\n",
    "    print(f\"end {name} cost {cost}\")\n",
    "# 获取项目根目录和必要的路径\n",
    "project_root = \"/mnt/zhengcf3/lmp\"\n",
    "src_dir = os.path.join(project_root, 'src')\n",
    "sllm_store_dir = os.path.join(project_root, 'src', 'sllm_store')\n",
    "\n",
    "# 添加必要的目录到 Python 路径\n",
    "# 1. 添加 sllm_store 目录（必须在 src 之前，这样 sllm_store 可以被找到）\n",
    "sys.path.insert(0, sllm_store_dir)\n",
    "# 2. 添加 src 目录（用于导入 lmp, utils 等）\n",
    "sys.path.insert(0, src_dir)\n",
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "from models.Deepseek.deepseek_moe_16b_base.modeling_deepseek import DeepseekDecoderLayer\n",
    "from accelerate import init_empty_weights\n",
    "path = \"/mnt/zhengcf3/lmp/src/models/Deepseek/deepseek_moe_16b_base\"\n",
    "time_hook(\"load_config\")\n",
    "config = AutoConfig.from_pretrained(path , trust_remote_code=True)\n",
    "config._attn_implementation = \"sdpa\"\n",
    "time_hook_end(\"load_config\")\n",
    "\n",
    "time_hook(\"init_layers\")\n",
    "layers = []\n",
    "\n",
    "\n",
    "\n",
    "def init_layers(layer_idx):\n",
    "    with init_empty_weights():\n",
    "        for i in range(28):\n",
    "            layer_c = DeepseekDecoderLayer(config, i)\n",
    "            layer_h = copy.deepcopy(layer_c)  # 深拷贝\n",
    "            layers.append(layer_h)\n",
    "            layers.append(layer_c)\n",
    "def init_layer(layer_idx):\n",
    "    with init_empty_weights():\n",
    "        layer_c = DeepseekDecoderLayer(config, layer_idx)\n",
    "        layers.append(layer_c)\n",
    "\n",
    "time_hook(\"init_2\")\n",
    "with init_empty_weights():\n",
    "    layer_cc = DeepseekDecoderLayer(config, 2)\n",
    "time_hook_end(\"init_2\")\n",
    "\n",
    "def copy_c():\n",
    "    layer_h = copy.deepcopy(layer_cc)  # 深拷贝\n",
    "    layers.append(layer_h)\n",
    "\n",
    "time_hook(\"copy_1\")\n",
    "for i in range(27):\n",
    "    copy_c()\n",
    "time_hook_end(\"copy_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556a9944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型...\n",
      "加载模型时间: 2.415902614593506 秒\n",
      "深拷贝模型时间: [0.8905317783355713, 0.5729832649230957, 0.871990442276001, 0.897723913192749] 秒\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import MixtralForCausalLM, AutoModelForCausalLM\n",
    "import copy\n",
    "from accelerate import init_empty_weights\n",
    "from models.Deepseek.deepseek_moe_16b_base.modeling_deepseek import DeepseekForCausalLM, DeepseekRMSNorm\n",
    "# 加载 Mixtral 模型（只加载一个 expert）\n",
    "#Mixtral-8x22B-Instruct-v0.1\n",
    "model_path = \"/mnt/zhengcf3/models/deepseek-moe-16b-base\"\n",
    "print(\"正在加载模型...\")\n",
    "time_start = time.time()\n",
    "with init_empty_weights():\n",
    "    model = DeepseekForCausalLM(config)\n",
    "print(f\"加载模型时间: {time.time() - time_start} 秒\")\n",
    "\n",
    "time_list = []\n",
    "for i in range(4):\n",
    "    time_start=time.time()\n",
    "    model_cpy = copy.deepcopy(model)\n",
    "    time_list.append(time.time() - time_start)\n",
    "print(f\"深拷贝模型时间: {time_list} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe215fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 01-13 09:31:50.808408.808408 cuda_memory_view.py:567] \n",
      "DEBUG 01-13 09:31:50.808408.808408 cuda_memory_view.py:567] restore_tensors_from_shared_memory_names time: 0.014753103256225586\n",
      "start init_emodel\n",
      "end init_emodel cost 0.0031054019927978516\n",
      "start init_emodel_cpy\n",
      "end init_emodel_cpy cost 0.0018086433410644531\n",
      "start init_layer\n",
      "end init_layer cost 0.0277559757232666\n"
     ]
    }
   ],
   "source": [
    "import torch,os, sys\n",
    "import time\n",
    "from transformers import MixtralForCausalLM, AutoModelForCausalLM\n",
    "import copy\n",
    "from accelerate import init_empty_weights\n",
    "# from models.Deepseek.deepseek_moe_16b_base.modeling_deepseek import DeepseekForCausalLM, DeepseekRMSNorm\n",
    "\n",
    "timings = {}\n",
    "def time_hook(name):\n",
    "    start = time.time()\n",
    "    print(f\"start {name}\")\n",
    "    timings[name] = start\n",
    "\n",
    "def time_hook_end(name):\n",
    "    end = time.time()\n",
    "    \n",
    "    cost = end - timings[name]\n",
    "    print(f\"end {name} cost {cost}\")\n",
    "# 获取项目根目录和必要的路径\n",
    "project_root = \"/mnt/zhengcf3/lmp\"\n",
    "src_dir = os.path.join(project_root, 'src')\n",
    "sllm_store_dir = os.path.join(project_root, 'src', 'sllm_store')\n",
    "\n",
    "# 添加必要的目录到 Python 路径\n",
    "# 1. 添加 sllm_store 目录（必须在 src 之前，这样 sllm_store 可以被找到）\n",
    "sys.path.insert(0, sllm_store_dir)\n",
    "# 2. 添加 src 目录（用于导入 lmp, utils 等）\n",
    "sys.path.insert(0, src_dir)\n",
    "\n",
    "from lmp.lmp import MLPLLM\n",
    "from models.Deepseek.mlpmodule import DeepseekModule, DeepseekOCalModel\n",
    "from models.Deepseek.deepseek_moe_16b_base.modeling_deepseek import DeepseekForCausalLM, DeepseekRMSNorm, DeepseekDecoderLayer\n",
    "\n",
    "# 加载 Mixtral 模型（只加载一个 expert）\n",
    "#Mixtral-8x22B-Instruct-v0.1\n",
    "model_path = \"deepseek-moe-16b-base-bfloat16\"\n",
    "model_name_type = \"Deepseek\"\n",
    "mlpllm = MLPLLM( model_name_type=model_name_type, model_path=model_path )\n",
    "\n",
    "with init_empty_weights():\n",
    "    time_hook(\"init_emodel\")\n",
    "    cm = DeepseekOCalModel(mlpllm.mlpm.config)\n",
    "    time_hook_end(\"init_emodel\")\n",
    "\n",
    "    time_hook(\"init_emodel_cpy\")\n",
    "    cm = copy.deepcopy(cm)\n",
    "    time_hook_end(\"init_emodel_cpy\")\n",
    "\n",
    "    time_hook(\"init_layer\")\n",
    "    dl = DeepseekDecoderLayer(mlpllm.mlpm.config, layer_idx=1)\n",
    "    time_hook_end(\"init_layer\")\n",
    "\n",
    "    # time_hook(\"init_emodel\")\n",
    "    # cm = DeepseekForCausalLM(mlpllm.mlpm.config)\n",
    "    # time_hook_end(\"init_emodel\")\n",
    "\n",
    "# device1 = \"cuda:1\"\n",
    "# device1_index = int(device1.split(\":\")[1])\n",
    "\n",
    "# mlpllm.cmv.start_init_meta_model(hmv=mlpllm.hmv)\n",
    "# mlpllm.cmv.load_general_and_init()\n",
    "# mlpllm.cmv.init_load_qkvogn_es_weight(layer_idx=0)\n",
    "\n",
    "# layer_idx = 1\n",
    "# mlpllm.cmv.start_load_qkvogn_s_weight(layer_idx=1, device=device1)\n",
    "# mlpllm.cmv.wait_load_qkvogn_s_weight(layer_idx=1)\n",
    "\n",
    "# gpu_expert_ids = [i for i in range(mlpllm.mlpm.config.num_hidden_layers)]\n",
    "# gpu_expert_names = mlpllm.mlpm.get_experts_names(layer_idx=1, expert_idx_list=list(gpu_expert_ids))\n",
    "# gpu_expert_names = gpu_expert_names\n",
    "# ret1, replica_uuid1, state_dict1 = \\\n",
    "#     mlpllm.cmv.allocate_cuda_memory_and_load_into_gpu(\n",
    "#     gpu_expert_names, device_index_int=device1_index)\n",
    "# mlpllm.cmv.restore2model(state_dict1, mlpllm.cmv.mlpm_ci)\n",
    "# mlpllm.cmv.wait_load_into_gpu(replica_uuid1)\n",
    "\n",
    "# input_hidden_states = torch.randn(32, 1, 2048, dtype=torch.bfloat16, device=device1)\n",
    "\n",
    "# layer = mlpllm.cmv.mlpm_ci.model.layers[layer_idx]\n",
    "# time_hook(\"layer_1\")\n",
    "# out = layer.mlp(input_hidden_states)\n",
    "# time_hook_end(\"layer_1\")\n",
    "\n",
    "# time_hook(\"layer_1\")\n",
    "# out = layer.mlp(input_hidden_states)\n",
    "# time_hook_end(\"layer_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "\n",
    "device1 = torch.device(\"cuda:1\")\n",
    "\n",
    "print(device1.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
