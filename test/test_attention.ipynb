{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72abb8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import threading\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from math import *\n",
    "dev = \"cpu\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# 设置线程数\n",
    "torch.set_num_threads(192)\n",
    "\n",
    "# 准备 SDPA 数据\n",
    "batch_size = 512\n",
    "num_heads = 32\n",
    "seq_len = 512\n",
    "head_dim = 128\n",
    "q_seq_len = 1  # decode 阶段\n",
    "\n",
    "query = torch.randn(batch_size, num_heads, q_seq_len, head_dim, device=dev, dtype=dtype)\n",
    "key = torch.randn(batch_size, 8, seq_len, head_dim, device=dev, dtype=dtype)\n",
    "value = torch.randn(batch_size, 8, seq_len, head_dim, device=dev, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82035d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move time 0.10378456115722656\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "time_start_move = time.time()\n",
    "key_pin = key.pin_memory()\n",
    "key_gpu = key_pin.to(\"cuda:1\")\n",
    "time_end_move = time.time()\n",
    "del key_gpu\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"move time {time_end_move - time_start_move}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48f06fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single group 0 real attn out cost 0.14505553245544434 s\n",
      "write to output tensor cost 0.028368234634399414 s\n",
      "single group 1 real attn out cost 0.058889150619506836 s\n",
      "write to output tensor cost 0.0022513866424560547 s\n",
      "single group 2 real attn out cost 0.009257316589355469 s\n",
      "write to output tensor cost 0.00022125244140625 s\n",
      "single group 3 real attn out cost 0.009157419204711914 s\n",
      "write to output tensor cost 0.00022602081298828125 s\n",
      "dot attn help cost 0.396262 seconds\n",
      "single group 0 real attn out cost 0.009003877639770508 s\n",
      "write to output tensor cost 0.00022411346435546875 s\n",
      "single group 1 real attn out cost 0.00862884521484375 s\n",
      "write to output tensor cost 0.00022935867309570312 s\n",
      "single group 2 real attn out cost 0.008588790893554688 s\n",
      "write to output tensor cost 0.00022411346435546875 s\n",
      "single group 3 real attn out cost 0.00838160514831543 s\n",
      "write to output tensor cost 0.00020956993103027344 s\n",
      "dot attn help cost 0.037035 seconds\n",
      "single group 0 real attn out cost 0.008901834487915039 s\n",
      "write to output tensor cost 0.0002505779266357422 s\n",
      "single group 1 real attn out cost 0.009194612503051758 s\n",
      "write to output tensor cost 0.00021600723266601562 s\n",
      "single group 2 real attn out cost 0.009172439575195312 s\n",
      "write to output tensor cost 0.0002014636993408203 s\n",
      "single group 3 real attn out cost 0.008719682693481445 s\n",
      "write to output tensor cost 0.00018143653869628906 s\n",
      "dot attn help cost 0.038393 seconds\n",
      "sdpa time 0.4722158908843994\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(160)\n",
    "from torch.nn.attention import  sdpa_kernel\n",
    "from torch.nn.attention import  SDPBackend\n",
    "import time\n",
    "def compute_sdpa(query, key, value, num_runs=1):\n",
    "    \"\"\"执行 SDPA 计算\"\"\"\n",
    "    results = []\n",
    "    time_list = []\n",
    "    for _ in range(num_runs):\n",
    "        time_start_single = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(\n",
    "                query, key, value,\n",
    "                is_causal=True,\n",
    "                dropout_p=0.0\n",
    "            )\n",
    "        time_end_single = time.time()\n",
    "        time_list.append(round(time_end_single - time_start_single,6))\n",
    "        results.append(output)\n",
    "    print(f\"sdpa time {time_list}\")\n",
    "    return results\n",
    "@torch.no_grad()\n",
    "def scaled_dot_product_attention_help(\n",
    "    query_states, \n",
    "    key_states, \n",
    "    value_states, \n",
    "    attn_mask=None, dropout_p=0.0, enable_gqa=False, is_causal=False, output_tensor=None):\n",
    "\n",
    "    time_start = time.time()\n",
    "   \n",
    "    num_query_heads = query_states.shape[1]     # e.g. 32\n",
    "    num_key_heads = key_states.shape[1]\n",
    "    num_groups = int(num_query_heads//num_key_heads)   # 4 组\n",
    "    \n",
    "    query_states = query_states\n",
    "   \n",
    "    if output_tensor is None:\n",
    "        output_tensor = torch.zeros(\n",
    "            query_states.shape, dtype=query_states.dtype, device=query_states.device, pin_memory=False\n",
    "        )\n",
    "    else:\n",
    "        output_tensor = output_tensor.contiguous()\n",
    "    \n",
    "    query_groups = []\n",
    "    query_indices_list = []\n",
    "    \n",
    "    for group_idx in range(num_groups):\n",
    "        query_indices = torch.arange(group_idx, num_query_heads, num_groups)\n",
    "        query_group = query_states[:, query_indices, :, :]  # 确保连续内存\n",
    "        query_groups.append(query_group)\n",
    "        query_indices_list.append(query_indices)\n",
    "    \n",
    "    for group_idx in range(num_groups):\n",
    "        query_group = query_groups[group_idx]\n",
    "        query_indices = query_indices_list[group_idx]\n",
    "        \n",
    "        \n",
    "        # 优化5: 使用预取的数据，避免重复内存访问\n",
    "        key_group = key_states    # (batch, 8, seq_len, head_dim)\n",
    "        value_group = value_states # (batch, 8, seq_len, head_dim)\n",
    "\n",
    "        time_start_tmp = time.time()\n",
    "        with sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n",
    "            attn_out = torch.nn.functional.scaled_dot_product_attention(\n",
    "                query_group, key_group, value_group,\n",
    "                attn_mask=attn_mask,\n",
    "                dropout_p=dropout_p,\n",
    "                enable_gqa=enable_gqa,\n",
    "                is_causal=is_causal\n",
    "            )\n",
    "        print(f\"single group {group_idx} real attn out cost {time.time() - time_start_tmp} s\")\n",
    "        \n",
    "        time_start_cpy = time.time()\n",
    "        output_tensor[:, query_indices, :, :] = attn_out\n",
    "        print(f\"write to output tensor cost {time.time() - time_start_cpy} s\")\n",
    "    print(f\"dot attn help cost {time.time()-time_start:.6f} seconds\")\n",
    "    return output_tensor\n",
    "\n",
    "@torch.no_grad()\n",
    "def scaled_dot_product_attention_help_split_kv(\n",
    "    query_states, \n",
    "    key_states, \n",
    "    value_states, \n",
    "    attn_mask=None, dropout_p=0.0, enable_gqa=False, is_causal=False, output_tensor=None):\n",
    "    \"\"\"\n",
    "    将 key_group 和 value_group 拆分成两份，query_group 也相应拆分成两份，分别计算\n",
    "    确保计算结果准确，符合原函数的根本计算逻辑\n",
    "    \"\"\"\n",
    "    time_start = time.time()\n",
    "   \n",
    "    num_query_heads = query_states.shape[1]     # e.g. 32\n",
    "    num_key_heads = key_states.shape[1]         # e.g. 8\n",
    "    num_groups = int(num_query_heads // num_key_heads)   # 4 组\n",
    "    \n",
    "    query_states = query_states.contiguous()\n",
    "    key_states = key_states.contiguous()\n",
    "    value_states = value_states.contiguous()\n",
    "   \n",
    "    if output_tensor is None:\n",
    "        output_tensor = torch.zeros(\n",
    "            query_states.shape, dtype=query_states.dtype, device=query_states.device, pin_memory=False\n",
    "        )\n",
    "    else:\n",
    "        output_tensor = output_tensor.contiguous()\n",
    "    \n",
    "    query_groups = []\n",
    "    query_indices_list = []\n",
    "    \n",
    "    for group_idx in range(num_groups):\n",
    "        query_indices = torch.arange(group_idx, num_query_heads, num_groups)\n",
    "        query_group = query_states[:, query_indices, :, :] # 确保连续内存\n",
    "        query_groups.append(query_group)\n",
    "        query_indices_list.append(query_indices)\n",
    "    \n",
    "    for group_idx in range(num_groups):\n",
    "        query_group = query_groups[group_idx]\n",
    "        query_indices = query_indices_list[group_idx]\n",
    "        \n",
    "        # 将 key_group 和 value_group 拆分成两份\n",
    "        # key_group: (batch, 8, seq_len, head_dim) -> 拆成两份，每份 (batch, 4, seq_len, head_dim)\n",
    "        num_kv_heads = key_states.shape[1]\n",
    "        num_kv_splits = 4  # 拆分成两份\n",
    "        kv_heads_per_split = num_kv_heads // num_kv_splits  # 每份的 head 数\n",
    "        \n",
    "        # 将 query_group 也拆分成两份，对应 KV 的拆分\n",
    "        num_query_heads_in_group = query_group.shape[1]\n",
    "        query_heads_per_split = num_query_heads_in_group // num_kv_splits  # 每份的 query head 数\n",
    "        \n",
    "        # 存储每份的计算结果\n",
    "        attn_out_parts = []\n",
    "        \n",
    "        for split_idx in range(num_kv_splits):\n",
    "            # 拆分 KV: 第一份 [0:4], 第二份 [4:8]\n",
    "            kv_start = split_idx * kv_heads_per_split\n",
    "            kv_end = kv_start + kv_heads_per_split\n",
    "            key_split = key_states[:, kv_start:kv_end, :, :]  # (batch, 4, seq_len, head_dim)\n",
    "            value_split = value_states[:, kv_start:kv_end, :, :]  # (batch, 4, seq_len, head_dim)\n",
    "            \n",
    "            # 拆分 query: 需要按照原始 stride 方式分组\n",
    "            # query_group 的 heads 是通过 stride=num_groups 方式选择的\n",
    "            # 例如 group 0: [0, 4, 8, 12, 16, 20, 24, 28]\n",
    "            # 拆分成两份：第一份 [0, 4, 8, 12]，第二份 [16, 20, 24, 28]\n",
    "            # 在 query_group 中，这些 heads 的索引是 [0, 1, 2, 3] 和 [4, 5, 6, 7]\n",
    "            query_start = split_idx * query_heads_per_split\n",
    "            query_end = query_start + query_heads_per_split\n",
    "            query_split = query_group[:, query_start:query_end, :, :]  # (batch, 4, q_seq_len, head_dim)\n",
    "            \n",
    "            # 计算这一份的 attention\n",
    "            time_start_tmp = time.time()\n",
    "            with sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n",
    "                attn_out_split = torch.nn.functional.scaled_dot_product_attention(\n",
    "                    query_split, key_split, value_split,\n",
    "                    attn_mask=attn_mask,\n",
    "                    dropout_p=dropout_p,\n",
    "                    enable_gqa=enable_gqa,\n",
    "                    is_causal=is_causal\n",
    "                )\n",
    "            print(f\"single group {group_idx} split {split_idx} real attn out cost {time.time() - time_start_tmp} s\")\n",
    "            attn_out_parts.append(attn_out_split)\n",
    "        \n",
    "        # 合并两份的计算结果\n",
    "        time_start_concat = time.time()\n",
    "        attn_out = torch.cat(attn_out_parts, dim=1)  # 在 head 维度上拼接\n",
    "        print(f\"single group {group_idx} concat cost {time.time() - time_start_concat} s\")\n",
    "        \n",
    "        # 将结果写入 output_tensor\n",
    "        time_start_cpy = time.time()\n",
    "        output_tensor[:, query_indices, :, :] = attn_out\n",
    "        print(f\"write to output tensor cost {time.time() - time_start_cpy} s\")\n",
    "    \n",
    "    print(f\"dot attn help split kv cost {time.time()-time_start:.6f} seconds\")\n",
    "    return output_tensor\n",
    "\n",
    "\n",
    "\n",
    "num_runs = 3\n",
    "time_start = time.time()\n",
    "for i in range(num_runs):\n",
    "    # sdpa_results1 = scaled_dot_product_attention_help_split_kv(query, key, value)\n",
    "    sdpa_results2 = scaled_dot_product_attention_help(query, key, value)\n",
    "    \n",
    "    # 验证两个计算结果是否相同\n",
    "    # if torch.equal(sdpa_results1, sdpa_results2):\n",
    "    #     print(f\"Run {i+1}: Results are exactly equal ✓\")\n",
    "    # elif torch.allclose(sdpa_results1, sdpa_results2, rtol=1e-5, atol=1e-8):\n",
    "    #     max_diff = (sdpa_results1 - sdpa_results2).abs().max().item()\n",
    "    #     print(f\"Run {i+1}: Results are close (max diff: {max_diff:.2e}) ✓\")\n",
    "    # else:\n",
    "    #     max_diff = (sdpa_results1 - sdpa_results2).abs().max().item()\n",
    "    #     mean_diff = (sdpa_results1 - sdpa_results2).abs().mean().item()\n",
    "    #     print(f\"Run {i+1}: Results differ! Max diff: {max_diff:.2e}, Mean diff: {mean_diff:.2e} ✗\")\n",
    "    #     assert False, f\"Results don't match: max_diff={max_diff:.2e}, mean_diff={mean_diff:.2e}\"\n",
    "sdpa_time = time.time() - time_start\n",
    "print(f\"sdpa time {sdpa_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move time 0.009891986846923828\n",
      "move time 0.00965428352355957\n",
      "move time 0.017072439193725586\n",
      "move time 0.009665727615356445\n"
     ]
    }
   ],
   "source": [
    "device1 = \"cuda:1\"\n",
    "device2 = \"cuda:2\"\n",
    "\n",
    "expert1 = torch.randn(14336, 4096, dtype=torch.bfloat16, device=\"cpu\", pin_memory=True)\n",
    "expert2 = torch.randn(14336, 4096, dtype=torch.bfloat16, device=\"cpu\", pin_memory=True)\n",
    "\n",
    "time_start = time.time()\n",
    "expert1.to(device1, non_blocking=True)\n",
    "# expert1.to(device2, non_blocking=True)\n",
    "torch.cuda.synchronize(device=device1)\n",
    "# torch.cuda.synchronize(device=device2)\n",
    "time_end = time.time()\n",
    "print(f\"move time {time_end - time_start}\")\n",
    "\n",
    "time_start = time.time()\n",
    "expert1.to(device1, non_blocking=True)\n",
    "# expert1.to(device2, non_blocking=True)\n",
    "torch.cuda.synchronize(device=device1)\n",
    "# torch.cuda.synchronize(device=device2)\n",
    "time_end = time.time()\n",
    "print(f\"move time {time_end - time_start}\")\n",
    "\n",
    "time_start = time.time()\n",
    "expert1.to(device1, non_blocking=True)\n",
    "expert1.to(device2, non_blocking=True)\n",
    "torch.cuda.synchronize(device=device1)\n",
    "torch.cuda.synchronize(device=device2)\n",
    "time_end = time.time()\n",
    "print(f\"move time {time_end - time_start}\")\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "expert1.to(device1, non_blocking=True)\n",
    "expert2.to(device2, non_blocking=True)\n",
    "torch.cuda.synchronize(device=device1)\n",
    "torch.cuda.synchronize(device=device2)\n",
    "time_end = time.time()\n",
    "print(f\"move time {time_end - time_start}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbdba32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocate torch tensor time: 0.0935208797454834 seconds\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 14ms 10 experts\n",
    "# all   93 ms\n",
    "import time, torch\n",
    "k = torch.randn(1408, 2048, dtype=torch.bfloat16, device=\"cpu\", pin_memory=True)\n",
    "num = 64\n",
    "time_start_torch = time.time()\n",
    "for i in range(num*3):\n",
    "    k_g = k.to(\"cuda:1\")\n",
    "time_end_torch = time.time()\n",
    "\n",
    "print(f\"allocate torch tensor time: {time_end_torch - time_start_torch} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
