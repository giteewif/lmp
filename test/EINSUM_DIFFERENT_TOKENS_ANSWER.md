# torch.einsumèƒ½å¦ç”¨äºä¸åŒæ•°é‡tokençš„CPU expertè®¡ç®—ï¼Ÿ

## ç­”æ¡ˆï¼š**å¯ä»¥ï¼** âœ…

`torch.einsum`å¯ä»¥å¤„ç†ä¸åŒæ•°é‡tokençš„æƒ…å†µï¼Œä½†éœ€è¦é€šè¿‡**padding**æˆ–**åˆ†ç»„**ç­–ç•¥ã€‚

## ä¸‰ç§å®ç°ç­–ç•¥

### ç­–ç•¥1: å…¨éƒ¨Paddingåˆ°æœ€å¤§ï¼ˆæœ€å¿«ï¼‰â­â­â­â­â­

**å®ç°æ–¹å¼**ï¼š
```python
# å°†æ‰€æœ‰expertçš„token paddingåˆ°æœ€å¤§æ•°é‡
max_tokens = max(tokens.shape[0] for tokens in expert_tokens_map.values())

# Paddingå¹¶å †å 
stacked_inputs = []
masks = []
for expert_idx, tokens in expert_tokens_map.items():
    num_tokens = tokens.shape[0]
    if num_tokens < max_tokens:
        padding = torch.zeros(max_tokens - num_tokens, hidden_size)
        padded = torch.cat([tokens, padding], dim=0)
    else:
        padded = tokens
    stacked_inputs.append(padded)
    
    # åˆ›å»ºmask
    mask = torch.zeros(max_tokens, dtype=torch.bool)
    mask[:num_tokens] = True
    masks.append(mask)

# ä½¿ç”¨einsumæ‰¹é‡è®¡ç®—
stacked_inputs = torch.stack(stacked_inputs)  # [E, max_tokens, H]
w1_weights = torch.stack([w1 for ...])  # [E, I, H]

outputs = torch.einsum('eth,eih->eti', stacked_inputs, w1_weights)

# ç”¨maskæå–æœ‰æ•ˆç»“æœ
for i, expert_idx in enumerate(expert_indices):
    valid_outputs = outputs[i][masks[i]]
```

**ä¼˜ç‚¹**ï¼š
- âœ… æœ€å¤§åŒ–BLASä¼˜åŒ–ï¼ˆå¤§batchï¼‰
- âœ… æ€§èƒ½æœ€å¥½ï¼ˆæµ‹è¯•æ˜¾ç¤ºæ¯”æ™ºèƒ½åˆ†ç»„å¿«çº¦1.7xï¼‰
- âœ… å®ç°ç›¸å¯¹ç®€å•

**ç¼ºç‚¹**ï¼š
- âš ï¸ å†…å­˜æµªè´¹ï¼ˆpaddingéƒ¨åˆ†ï¼‰
- âš ï¸ å¦‚æœtokenæ•°é‡å·®å¼‚å¤§ï¼Œæµªè´¹ä¸¥é‡

**é€‚ç”¨åœºæ™¯**ï¼š
- tokenæ•°é‡å·®å¼‚ä¸å¤§ï¼ˆ<50%ï¼‰
- å†…å­˜å……è¶³
- è¿½æ±‚æœ€é«˜æ€§èƒ½

---

### ç­–ç•¥2: æ™ºèƒ½åˆ†ç»„ï¼ˆæ¨èï¼‰â­â­â­â­

**å®ç°æ–¹å¼**ï¼š
```python
# å…è®¸ä¸€å®šæ¯”ä¾‹çš„paddingï¼ˆå¦‚30%ï¼‰
max_padding_ratio = 0.3

# æŒ‰tokenæ•°é‡æ’åºå¹¶åˆ†ç»„
sorted_experts = sorted(expert_tokens_map.items(), 
                       key=lambda x: x[1].shape[0], reverse=True)

groups = []
current_group = []
current_max = 0

for expert_idx, tokens in sorted_experts:
    num_tokens = tokens.shape[0]
    if not current_group:
        current_group.append(expert_idx)
        current_max = num_tokens
    else:
        padding_ratio = (current_max - num_tokens) / current_max
        if padding_ratio <= max_padding_ratio:
            # å¯ä»¥åˆå¹¶
            current_group.append(expert_idx)
        else:
            # å¼€å§‹æ–°ç»„
            groups.append((current_group, current_max))
            current_group = [expert_idx]
            current_max = num_tokens

# å¯¹æ¯ç»„ä½¿ç”¨einsumè®¡ç®—ï¼ˆç±»ä¼¼ç­–ç•¥1ï¼‰
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¹³è¡¡æ€§èƒ½å’Œå†…å­˜
- âœ… å‡å°‘paddingæµªè´¹
- âœ… è‡ªé€‚åº”åˆ†ç»„ç­–ç•¥

**ç¼ºç‚¹**ï¼š
- âš ï¸ éœ€è¦è°ƒä¼˜paddingæ¯”ä¾‹å‚æ•°
- âš ï¸ å®ç°è¾ƒå¤æ‚

**é€‚ç”¨åœºæ™¯**ï¼š
- tokenæ•°é‡å·®å¼‚è¾ƒå¤§
- éœ€è¦å¹³è¡¡æ€§èƒ½å’Œå†…å­˜
- **æ¨èç”¨äºå¤§å¤šæ•°åœºæ™¯**

---

### ç­–ç•¥3: æ— Paddingï¼ˆæœ€çœå†…å­˜ï¼‰â­â­â­

**å®ç°æ–¹å¼**ï¼š
```python
# åªåˆå¹¶ç›¸åŒtokenæ•°é‡çš„expert
groups = {}
for expert_idx, tokens in expert_tokens_map.items():
    num_tokens = tokens.shape[0]
    if num_tokens not in groups:
        groups[num_tokens] = []
    groups[num_tokens].append(expert_idx)

# ç›¸åŒtokenæ•°é‡çš„ä½¿ç”¨einsumåˆå¹¶
# ä¸åŒtokenæ•°é‡çš„å•ç‹¬è®¡ç®—
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ— å†…å­˜æµªè´¹
- âœ… å®ç°ç®€å•

**ç¼ºç‚¹**ï¼š
- âš ï¸ å¯èƒ½æ— æ³•å……åˆ†åˆ©ç”¨BLASä¼˜åŒ–
- âš ï¸ å¦‚æœtokenæ•°é‡éƒ½ä¸åŒï¼Œæ— æ³•åˆå¹¶

**é€‚ç”¨åœºæ™¯**ï¼š
- å†…å­˜ç´§å¼ 
- tokenæ•°é‡å·®å¼‚å¾ˆå¤§
- å¯ä»¥æ¥å—æ€§èƒ½æŸå¤±

---

## æ€§èƒ½å¯¹æ¯”ï¼ˆå®æµ‹æ•°æ®ï¼‰

åŸºäºæµ‹è¯•åœºæ™¯ï¼šExpert 0æœ‰64ä¸ªtokenï¼ŒExpert 1æœ‰32ä¸ªtokenï¼ŒExpert 2æœ‰50ä¸ªtoken

| ç­–ç•¥ | æ—¶é—´ | Paddingæµªè´¹ | è¯´æ˜ |
|------|------|-------------|------|
| **å…¨éƒ¨padding** | **11.078 ms** | 24.0% | **æœ€å¿«** |
| æ™ºèƒ½åˆ†ç»„ | 19.145 ms | 18.0% | å¹³è¡¡ |
| æ— padding | ~19 ms | 0% | æœ€çœå†…å­˜ |

**å…³é”®å‘ç°**ï¼š
- å…¨éƒ¨paddingåˆ°æœ€å¤§åè€Œ**æ›´å¿«**ï¼ˆå› ä¸ºBLASä¼˜åŒ–ï¼‰
- å³ä½¿æœ‰24%çš„paddingæµªè´¹ï¼Œæ€§èƒ½ä»ç„¶æœ€å¥½
- è¿™è¯´æ˜**BLASåº“å¯¹å¤§batchçš„ä¼˜åŒ–æ•ˆæœæ˜¾è‘—**

---

## å®é™…åº”ç”¨å»ºè®®

### æ¨èæ–¹æ¡ˆï¼š**å…¨éƒ¨Paddingç­–ç•¥**

**åŸå› **ï¼š
1. æ€§èƒ½æœ€å¥½ï¼ˆå¿«1.7xï¼‰
2. å®ç°ç®€å•
3. å†…å­˜æµªè´¹é€šå¸¸å¯æ¥å—ï¼ˆç‰¹åˆ«æ˜¯CPUè®¡ç®—æ—¶ï¼‰

**å®ç°è¦ç‚¹**ï¼š
```python
def einsum_different_tokens(expert_tokens_map, expert_weights):
    # 1. æ‰¾åˆ°æœ€å¤§tokenæ•°é‡
    max_tokens = max(t.shape[0] for t in expert_tokens_map.values())
    
    # 2. Paddingå¹¶å †å 
    stacked_inputs = []
    masks = []
    for tokens in expert_tokens_map.values():
        num_tokens = tokens.shape[0]
        padded = F.pad(tokens, (0, 0, 0, max_tokens - num_tokens))
        stacked_inputs.append(padded)
        mask = torch.zeros(max_tokens, dtype=torch.bool)
        mask[:num_tokens] = True
        masks.append(mask)
    
    # 3. ä½¿ç”¨einsumæ‰¹é‡è®¡ç®—
    stacked_inputs = torch.stack(stacked_inputs)
    outputs = torch.einsum('eth,eih->eti', stacked_inputs, w1_weights)
    
    # 4. æå–æœ‰æ•ˆç»“æœ
    for i, mask in enumerate(masks):
        valid_outputs = outputs[i][mask]
```

### å¦‚æœå†…å­˜ç´§å¼ ï¼šä½¿ç”¨æ™ºèƒ½åˆ†ç»„

è®¾ç½®`max_padding_ratio=0.2`ï¼ˆå…è®¸20%çš„paddingï¼‰ï¼Œåœ¨æ€§èƒ½å’Œå†…å­˜ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

---

## å…³é”®ä»£ç ç¤ºä¾‹

### å®Œæ•´å®ç°ï¼ˆå…¨éƒ¨paddingç­–ç•¥ï¼‰

```python
def optimized_cpu_experts_flexible(
    layer,
    expert_tokens_map: Dict[int, torch.Tensor],
    routing_weights: Dict[int, torch.Tensor]
):
    """
    ä¼˜åŒ–çš„CPU expertè®¡ç®—ï¼ˆæ”¯æŒä¸åŒtokenæ•°é‡ï¼‰
    ä½¿ç”¨å…¨éƒ¨paddingç­–ç•¥
    """
    if not expert_tokens_map:
        return {}
    
    expert_indices = list(expert_tokens_map.keys())
    max_tokens = max(tokens.shape[0] for tokens in expert_tokens_map.values())
    
    # Paddingå¹¶å †å 
    stacked_inputs = []
    masks = []
    w1_list, w2_list, w3_list = [], [], []
    
    for expert_idx in expert_indices:
        tokens = expert_tokens_map[expert_idx]
        num_tokens = tokens.shape[0]
        
        # Padding
        if num_tokens < max_tokens:
            padding = torch.zeros(
                max_tokens - num_tokens, tokens.shape[1],
                dtype=tokens.dtype, device=tokens.device
            )
            padded_tokens = torch.cat([tokens, padding], dim=0)
        else:
            padded_tokens = tokens
        
        stacked_inputs.append(padded_tokens)
        
        # åˆ›å»ºmask
        mask = torch.zeros(max_tokens, dtype=torch.bool, device=tokens.device)
        mask[:num_tokens] = True
        masks.append(mask)
        
        # æ”¶é›†æƒé‡
        expert = layer.block_sparse_moe.experts[expert_idx]
        w1_list.append(expert.w1.weight)
        w2_list.append(expert.w2.weight)
        w3_list.append(expert.w3.weight)
    
    # å †å 
    stacked_inputs = torch.stack(stacked_inputs)  # [E, max_tokens, H]
    w1_weights = torch.stack(w1_list)  # [E, I, H]
    w2_weights = torch.stack(w2_list)  # [E, H, I]
    w3_weights = torch.stack(w3_list)  # [E, I, H]
    
    # ä½¿ç”¨einsumæ‰¹é‡è®¡ç®—
    w1_out = torch.einsum('eth,eih->eti', stacked_inputs, w1_weights)
    w1_out = F.silu(w1_out)
    w3_out = torch.einsum('eth,eih->eti', stacked_inputs, w3_weights)
    intermediate = w1_out * w3_out
    outputs = torch.einsum('eti,ehi->eth', intermediate, w2_weights)
    
    # æå–æœ‰æ•ˆç»“æœ
    results = {}
    for i, expert_idx in enumerate(expert_indices):
        expert_outputs = outputs[i][masks[i]]
        if expert_idx in routing_weights:
            expert_outputs = expert_outputs * routing_weights[expert_idx]
        results[expert_idx] = expert_outputs
    
    return results
```

---

## æ€»ç»“

### âœ… ç­”æ¡ˆï¼šå¯ä»¥ï¼

`torch.einsum`**å¯ä»¥**ç”¨äºä¸åŒæ•°é‡tokençš„CPU expertè®¡ç®—ï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼ï¼š

1. **Paddingç­–ç•¥**ï¼šå°†ä¸åŒtokenæ•°é‡paddingåˆ°ç›¸åŒå¤§å°ï¼Œä½¿ç”¨einsumæ‰¹é‡è®¡ç®—ï¼Œç„¶åç”¨maskæå–æœ‰æ•ˆç»“æœ
2. **æ™ºèƒ½åˆ†ç»„**ï¼šå…è®¸ä¸€å®špaddingæ¯”ä¾‹ï¼Œå°†ç›¸è¿‘tokenæ•°é‡çš„expertåˆ†ç»„
3. **æ— padding**ï¼šåªåˆå¹¶ç›¸åŒtokenæ•°é‡çš„expert

### ğŸ† æ¨èï¼šå…¨éƒ¨Paddingç­–ç•¥

- **æ€§èƒ½æœ€å¥½**ï¼ˆå®æµ‹å¿«1.7xï¼‰
- **å®ç°ç®€å•**
- **å†…å­˜æµªè´¹é€šå¸¸å¯æ¥å—**

### å…³é”®ç‚¹

- âœ… einsumæœ¬èº«æ”¯æŒä¸åŒå½¢çŠ¶çš„è¾“å…¥ï¼ˆé€šè¿‡paddingï¼‰
- âœ… ä½¿ç”¨maskæå–æœ‰æ•ˆç»“æœ
- âœ… ä¿æŒexpertç‹¬ç«‹æ€§ï¼ˆæ¯ä¸ªexpertå¤„ç†å„è‡ªçš„tokenï¼‰
- âœ… æœ€å¤§åŒ–BLASä¼˜åŒ–ï¼ˆå¤§batchæ€§èƒ½æ›´å¥½ï¼‰

### ä½¿ç”¨å»ºè®®

```python
# æ¨èä½¿ç”¨å…¨éƒ¨paddingç­–ç•¥
results = einsum_with_padding_strategy(
    expert_tokens_map,
    expert_weights,
    routing_weights,
    strategy="full_padding"  # æˆ– "smart"
)
```

