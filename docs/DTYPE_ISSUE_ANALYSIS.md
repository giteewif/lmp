# Dtype é—®é¢˜åˆ†æ

## ğŸ” é—®é¢˜æè¿°

Expert æƒé‡ä»å…±äº«å†…å­˜åŠ è½½åæ˜¾ç¤ºä¸º `torch.float32`ï¼Œä½† config ä¸­é…ç½®çš„æ˜¯ `bfloat16`ã€‚

## âœ… å·²ç¡®è®¤æ­£ç¡®çš„éƒ¨åˆ†

1. **tensor_index_resize.json æ ¼å¼æ­£ç¡®**
   - dtype å­—æ®µæ ¼å¼ï¼š`"torch.bfloat16"` âœ“
   - C++ `stringToScalarType` æ”¯æŒæ­¤æ ¼å¼ âœ“

2. **C++ ä»£ç é€»è¾‘æ­£ç¡®**
   - `RestoreTensorsFromSharedMemoryNames` æ­£ç¡®è¯»å– dtype âœ“
   - `torch::from_blob` ä½¿ç”¨æ­£ç¡®çš„ dtype âœ“

## âš ï¸ å¯èƒ½çš„é—®é¢˜ç‚¹

### 1. æ¨¡å‹åˆ›å»ºæ—¶æœªä½¿ç”¨æ­£ç¡®çš„ dtype

**ä½ç½®**: `DeepseekModule.create_empty_model()`

```python
def create_empty_model(self, config: AutoConfig):
    config._attn_implementation = "sdpa"
    with init_empty_weights():
        model = DeepseekForCausalLM(config)  # âš ï¸ å¯èƒ½æœªä½¿ç”¨ config.torch_dtype
        return model
```

**é—®é¢˜**: `init_empty_weights()` åˆ›å»ºçš„æ¨¡å‹å¯èƒ½ä½¿ç”¨é»˜è®¤ dtype (float32)ï¼Œè€Œä¸æ˜¯ config.torch_dtypeã€‚

**è§£å†³æ–¹æ¡ˆ**: éœ€è¦åœ¨åˆ›å»ºæ¨¡å‹æ—¶æ˜¾å¼æŒ‡å®š dtypeï¼š

```python
def create_empty_model(self, config: AutoConfig):
    config._attn_implementation = "sdpa"
    
    # è·å–ç›®æ ‡ dtype
    target_dtype = config.torch_dtype
    if isinstance(target_dtype, str):
        dtype_map = {
            'float32': torch.float32,
            'float16': torch.float16,
            'bfloat16': torch.bfloat16,
        }
        target_dtype = dtype_map.get(target_dtype, torch.float32)
    
    with init_empty_weights():
        model = DeepseekForCausalLM(config)
        # å°†æ¨¡å‹è½¬æ¢ä¸ºç›®æ ‡ dtype
        model = model.to(dtype=target_dtype)
        return model
```

### 2. set_module_tensor_to_device å¯èƒ½è¦†ç›– dtype

**ä½ç½®**: `restore2model()` å’Œ `restore_hm_state_dict2model()`

**é—®é¢˜**: `set_module_tensor_to_device` å¯èƒ½æ ¹æ®ç›®æ ‡æ¨¡å—çš„ dtype è‡ªåŠ¨è½¬æ¢ï¼Œè€Œä¸æ˜¯ä¿æŒ tensor çš„åŸå§‹ dtypeã€‚

**æ£€æŸ¥æ–¹æ³•**: åœ¨è®¾ç½®å‰æ‰“å° tensor çš„ dtypeï¼š

```python
def restore2model(self, model_state_dict, model):
    with torch.no_grad():
        for name, param in model_state_dict.items():
            print(f"Setting {name}: tensor dtype={param.dtype}, model dtype={getattr(model, name.split('.')[0], None)}")
            set_module_tensor_to_device(model, name, param.device, param, clear_cache=False)
```

### 3. å…±äº«å†…å­˜ä¸­çš„æ•°æ®æ ¼å¼é—®é¢˜

**é—®é¢˜**: å³ä½¿ tensor_index_resize.json æ ‡è®°ä¸º bfloat16ï¼Œå…±äº«å†…å­˜ä¸­çš„å®é™…æ•°æ®å¯èƒ½æ˜¯ float32ã€‚

**æ£€æŸ¥æ–¹æ³•**: åœ¨ C++ ä»£ç ä¸­æ·»åŠ è°ƒè¯•è¾“å‡ºï¼š

```cpp
at::ScalarType dtype = stringToScalarType(dtype_str);
std::cerr << "Tensor " << name << ": dtype_str=" << dtype_str 
          << ", ScalarType=" << dtype << std::endl;
```

## ğŸ”§ è°ƒè¯•æ­¥éª¤

### Step 1: æ£€æŸ¥ä»å…±äº«å†…å­˜åŠ è½½çš„ tensor dtype

åœ¨ `cuda_memory_view.py` ä¸­æ·»åŠ ï¼š

```python
self.hm_state_dict = restore_tensors_from_shared_memory_names(...)

# æ£€æŸ¥ dtype
for name, tensor in list(self.hm_state_dict.items())[:5]:
    if 'expert' in name:
        print(f"{name}: dtype={tensor.dtype}, shape={tensor.shape}")
```

### Step 2: æ£€æŸ¥æ¨¡å‹åˆ›å»ºæ—¶çš„ dtype

åœ¨ `mlpmodule.py` ä¸­æ·»åŠ ï¼š

```python
def create_empty_model(self):
    model = self.model_class.create_empty_model(self.config)
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ª expert çš„ dtype
    if hasattr(model, 'model') and hasattr(model.model, 'layers'):
        layer0 = model.model.layers[0]
        if hasattr(layer0, 'mlp') and hasattr(layer0.mlp, 'experts'):
            expert0 = layer0.mlp.experts[0]
            if hasattr(expert0, 'gate_proj'):
                print(f"Model expert dtype: {expert0.gate_proj.weight.dtype}")
    
    return model
```

### Step 3: æ£€æŸ¥ restore åçš„ dtype

åœ¨ `restore2model` ä¸­æ·»åŠ ï¼š

```python
def restore2model(self, model_state_dict, model):
    with torch.no_grad():
        for name, param in model_state_dict.items():
            if 'expert' in name and 'weight' in name:
                print(f"Before restore: {name} dtype={param.dtype}")
                set_module_tensor_to_device(model, name, param.device, param, clear_cache=False)
                
                # æ£€æŸ¥ restore å
                module_param = get_module_tensor(model, name)
                if module_param is not None:
                    print(f"After restore: {name} dtype={module_param.dtype}")
```

## ğŸ“‹ æ£€æŸ¥æ¸…å•

- [ ] tensor_index_resize.json ä¸­ dtype æ ¼å¼æ­£ç¡® âœ“ (å·²ç¡®è®¤)
- [ ] C++ ä»£ç æ­£ç¡®è¯»å– dtype âœ“ (å·²ç¡®è®¤)
- [ ] ä»å…±äº«å†…å­˜åŠ è½½çš„ tensor dtype æ­£ç¡®ï¼Ÿ
- [ ] æ¨¡å‹åˆ›å»ºæ—¶ä½¿ç”¨æ­£ç¡®çš„ dtypeï¼Ÿ
- [ ] restore åæ¨¡å‹ä¸­çš„ dtype æ­£ç¡®ï¼Ÿ

## ğŸ¯ æœ€å¯èƒ½çš„åŸå› 

æ ¹æ®è¯Šæ–­ç»“æœï¼Œ**æœ€å¯èƒ½çš„é—®é¢˜æ˜¯æ¨¡å‹åˆ›å»ºæ—¶æœªä½¿ç”¨ config.torch_dtype**ã€‚

`init_empty_weights()` åˆ›å»ºçš„æ¨¡å‹é»˜è®¤ä½¿ç”¨ float32ï¼Œå³ä½¿ config ä¸­æŒ‡å®šäº† bfloat16ã€‚

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

ä¿®æ”¹ `DeepseekModule.create_empty_model()`:

```python
def create_empty_model(self, config: AutoConfig):
    config._attn_implementation = "sdpa"
    
    # è·å–ç›®æ ‡ dtype
    target_dtype = getattr(config, 'torch_dtype', None)
    if target_dtype is None:
        target_dtype = torch.float32
    elif isinstance(target_dtype, str):
        dtype_map = {
            'float32': torch.float32,
            'float16': torch.float16,
            'bfloat16': torch.bfloat16,
        }
        target_dtype = dtype_map.get(target_dtype, torch.float32)
    
    with init_empty_weights():
        model = DeepseekForCausalLM(config)
        # å°†æ¨¡å‹è½¬æ¢ä¸ºç›®æ ‡ dtype
        model = model.to(dtype=target_dtype)
        return model
```

è¿™æ ·åˆ›å»ºçš„æ¨¡å‹å°±æ˜¯ bfloat16ï¼Œåç»­ä»å…±äº«å†…å­˜åŠ è½½çš„ bfloat16 tensor å°±èƒ½æ­£ç¡®åŒ¹é…ã€‚

