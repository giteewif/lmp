# Dtype è½¬æ¢ä¿®å¤è¯´æ˜

## ğŸ› é—®é¢˜æè¿°

ä»å…±äº«å†…å­˜åŠ è½½çš„ expert æƒé‡é»˜è®¤æ˜¯ `torch.float32`ï¼Œä½† config ä¸­é…ç½®çš„ `torch_dtype` æ˜¯ `bfloat16`ï¼Œå¯¼è‡´æ¨¡å‹æƒé‡ç±»å‹ä¸åŒ¹é…ã€‚

### é—®é¢˜è¡¨ç°

```
mlp.experts.55.gate_proj.weight: meta (shape: torch.Size([1408, 2048])) (dtype: torch.float32)
mlp.experts.55.up_proj.weight: meta (shape: torch.Size([1408, 2048])) (dtype: torch.float32)
mlp.experts.55.down_proj.weight: meta (shape: torch.Size([2048, 1408])) (dtype: torch.float32)
```

ä½† config ä¸­ï¼š
```json
{
  "torch_dtype": "bfloat16"
}
```

### æ ¹æœ¬åŸå› 

åœ¨ `restore_hm_state_dict2model` å’Œ `restore2model` å‡½æ•°ä¸­ï¼Œç›´æ¥ä½¿ç”¨ä»å…±äº«å†…å­˜åŠ è½½çš„ tensorï¼Œæ²¡æœ‰æ ¹æ® `config.torch_dtype` è¿›è¡Œ dtype è½¬æ¢ã€‚

## âœ… ä¿®å¤æ–¹æ¡ˆ

### 1. ä¿®å¤ `restore_hm_state_dict2model` (`mlpmodule.py`)

åœ¨å‡½æ•°å¼€å§‹æ—¶è·å–ç›®æ ‡ dtypeï¼š

```python
# è·å–ç›®æ ‡ dtypeï¼ˆä» config ä¸­è¯»å–ï¼‰
target_dtype = self.config.torch_dtype
if isinstance(target_dtype, str):
    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸º torch.dtype
    dtype_map = {
        'float32': torch.float32,
        'float16': torch.float16,
        'bfloat16': torch.bfloat16,
    }
    target_dtype = dtype_map.get(target_dtype, torch.float32)
```

åœ¨è®¾ç½® tensor å‰è¿›è¡Œè½¬æ¢ï¼š

```python
# è½¬æ¢ tensor åˆ°ç›®æ ‡ dtypeï¼ˆå¦‚æœä¸ç›®æ ‡ dtype ä¸åŒï¼‰
if tensor.dtype != target_dtype:
    tensor = tensor.to(target_dtype)
    logger.debug(f"Converted {name} from {tensor.dtype} to {target_dtype}")

# ä½¿ç”¨ accelerate çš„å·¥å…·å‡½æ•°è®¾ç½® tensor
set_module_tensor_to_device(
    model,
    name,
    tensor.device,
    tensor,
    clear_cache=False,
)
```

### 2. ä¿®å¤ `restore2model` (`cuda_memory_view.py`)

åŒæ ·æ·»åŠ  dtype è½¬æ¢é€»è¾‘ï¼š

```python
def restore2model(self, model_state_dict, model):
    """
    å°† state_dict æ¢å¤åˆ°æ¨¡å‹ä¸­ï¼Œå¹¶æ ¹æ® config.torch_dtype è½¬æ¢ dtype
    """
    # è·å–ç›®æ ‡ dtypeï¼ˆä» config ä¸­è¯»å–ï¼‰
    target_dtype = self.mlpm.config.torch_dtype
    if isinstance(target_dtype, str):
        dtype_map = {
            'float32': torch.float32,
            'float16': torch.float16,
            'bfloat16': torch.bfloat16,
        }
        target_dtype = dtype_map.get(target_dtype, torch.float32)
    
    with torch.no_grad():
        for name, param in model_state_dict.items():
            # è½¬æ¢ tensor åˆ°ç›®æ ‡ dtypeï¼ˆå¦‚æœä¸ç›®æ ‡ dtype ä¸åŒï¼‰
            if param.dtype != target_dtype:
                param = param.to(target_dtype)
            
            set_module_tensor_to_device(model, name, param.device, param, clear_cache=False)
```

## ğŸ“Š ä¿®å¤æ•ˆæœ

### ä¿®å¤å‰

```python
# ä»å…±äº«å†…å­˜åŠ è½½çš„ tensor
tensor.dtype = torch.float32  # âŒ é”™è¯¯

# ç›´æ¥è®¾ç½®åˆ°æ¨¡å‹
set_module_tensor_to_device(model, name, device, tensor)

# ç»“æœï¼šæ¨¡å‹ä¸­çš„æƒé‡æ˜¯ float32ï¼Œä½† config è¦æ±‚ bfloat16
```

### ä¿®å¤å

```python
# ä»å…±äº«å†…å­˜åŠ è½½çš„ tensor
tensor.dtype = torch.float32

# æ ¹æ® config è½¬æ¢
target_dtype = config.torch_dtype  # "bfloat16"
tensor = tensor.to(torch.bfloat16)  # âœ… è½¬æ¢

# è®¾ç½®åˆ°æ¨¡å‹
set_module_tensor_to_device(model, name, device, tensor)

# ç»“æœï¼šæ¨¡å‹ä¸­çš„æƒé‡æ˜¯ bfloat16ï¼Œä¸ config ä¸€è‡´ âœ…
```

## ğŸ” éªŒè¯æ–¹æ³•

### 1. æ£€æŸ¥æƒé‡ dtype

```python
# åœ¨ restore åæ£€æŸ¥
for name, param in model.named_parameters():
    if "expert" in name and "weight" in name:
        print(f"{name}: {param.dtype}")
        assert param.dtype == config.torch_dtype, f"Expected {config.torch_dtype}, got {param.dtype}"
```

### 2. æ£€æŸ¥æ—¥å¿—

ä¿®å¤åä¼šè¾“å‡ºè½¬æ¢æ—¥å¿—ï¼š

```
DEBUG: Converted model.layers.1.mlp.experts.55.gate_proj.weight from torch.float32 to torch.bfloat16
DEBUG: Converted model.layers.1.mlp.experts.55.up_proj.weight from torch.float32 to torch.bfloat16
DEBUG: Converted model.layers.1.mlp.experts.55.down_proj.weight from torch.float32 to torch.bfloat16
```

### 3. éªŒè¯é…ç½®

```python
# æ£€æŸ¥ config
print(f"Config torch_dtype: {config.torch_dtype}")

# æ£€æŸ¥å®é™…æƒé‡
expert_weight = model.layers[1].mlp.experts[55].gate_proj.weight
print(f"Expert weight dtype: {expert_weight.dtype}")

# åº”è¯¥åŒ¹é…
assert str(expert_weight.dtype) == str(config.torch_dtype)
```

## ğŸ¯ æ”¯æŒçš„ Dtype

ä¿®å¤åçš„ä»£ç æ”¯æŒä»¥ä¸‹ dtypeï¼š

| Config å€¼ | PyTorch dtype |
|-----------|--------------|
| `"float32"` | `torch.float32` |
| `"float16"` | `torch.float16` |
| `"bfloat16"` | `torch.bfloat16` |

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æ€§èƒ½å½±å“

- **è½¬æ¢å¼€é”€**: dtype è½¬æ¢ä¼šæ¶ˆè€—å°‘é‡ CPU æ—¶é—´
- **å†…å­˜å½±å“**: bfloat16 æ¯” float32 èŠ‚çœ 50% å†…å­˜
- **ç²¾åº¦å½±å“**: bfloat16 å¯èƒ½ç•¥å¾®é™ä½ç²¾åº¦ï¼Œä½†é€šå¸¸å¯å¿½ç•¥

### 2. å…¼å®¹æ€§

- âœ… æ”¯æŒä» float32 è½¬æ¢åˆ° bfloat16/float16
- âœ… æ”¯æŒä» bfloat16/float16 è½¬æ¢åˆ° float32
- âš ï¸ å¦‚æœå…±äº«å†…å­˜ä¸­çš„ tensor å·²ç»æ˜¯ç›®æ ‡ dtypeï¼Œä¸ä¼šé‡å¤è½¬æ¢

### 3. è°ƒè¯•

å¦‚æœé‡åˆ° dtype ä¸åŒ¹é…é—®é¢˜ï¼š

1. æ£€æŸ¥ config.torch_dtype æ˜¯å¦æ­£ç¡®
2. æŸ¥çœ‹æ—¥å¿—ä¸­çš„è½¬æ¢ä¿¡æ¯
3. éªŒè¯æœ€ç»ˆæƒé‡çš„ dtype

## ğŸ“š ç›¸å…³æ–‡ä»¶

- `lmp/src/models/mlpmodule.py` - `restore_hm_state_dict2model` å‡½æ•°
- `lmp/src/lmp/cuda_memory_view.py` - `restore2model` å‡½æ•°
- `lmp/src/models/Deepseek/mlpmodule.py` - Deepseek æ¨¡å‹å®ç°
- `lmp/src/models/Mixtral/mlpmodule.py` - Mixtral æ¨¡å‹å®ç°

## ğŸ“ æ€»ç»“

ä¿®å¤åçš„ä»£ç ä¼šï¼š

1. âœ… è‡ªåŠ¨ä» config è¯»å–ç›®æ ‡ dtype
2. âœ… åœ¨è®¾ç½® tensor åˆ°æ¨¡å‹å‰è¿›è¡Œ dtype è½¬æ¢
3. âœ… è¾“å‡ºè°ƒè¯•æ—¥å¿—ä¾¿äºæ’æŸ¥é—®é¢˜
4. âœ… æ”¯æŒ Deepseek å’Œ Mixtral ä¸¤ç§æ¨¡å‹

ç°åœ¨ expert æƒé‡ä¼šæ­£ç¡®ä½¿ç”¨ config ä¸­æŒ‡å®šçš„ dtypeï¼ˆå¦‚ bfloat16ï¼‰ï¼ğŸš€

